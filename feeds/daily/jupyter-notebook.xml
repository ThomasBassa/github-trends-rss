<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Jupyter Notebook, Today</title><link>https://github.com/trending/jupyter-notebook?since=daily</link><description>The top repositories on GitHub for jupyter-notebook, measured daily</description><pubDate>Fri, 27 Dec 2019 20:10:10 GMT</pubDate><lastBuildDate>Fri, 27 Dec 2019 20:10:10 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>trekhleb/homemade-machine-learning #1 in Jupyter Notebook, Today</title><link>https://github.com/trekhleb/homemade-machine-learning</link><description>&lt;p&gt;&lt;i&gt;ü§ñ Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-homemade-machine-learning" class="anchor" aria-hidden="true" href="#homemade-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Homemade Machine Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://mybinder.org/v2/gh/trekhleb/homemade-machine-learning/master?filepath=notebooks" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/trekhleb/homemade-machine-learning" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ec56d7f6b55050cd6e8ee599c29b1436223ebebc/68747470733a2f2f7472617669732d63692e6f72672f7472656b686c65622f686f6d656d6164652d6d616368696e652d6c6561726e696e672e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/trekhleb/homemade-machine-learning.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For Octave/MatLab version of this repository please check &lt;a href="https://github.com/trekhleb/machine-learning-octave"&gt;machine-learning-octave&lt;/a&gt; project.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This repository contains examples of popular machine learning algorithms implemented in &lt;strong&gt;Python&lt;/strong&gt; with mathematics behind them being explained. Each algorithm has interactive &lt;strong&gt;Jupyter Notebook&lt;/strong&gt; demo that allows you to play with training data, algorithms configurations and immediately see the results, charts and predictions &lt;strong&gt;right in your browser&lt;/strong&gt;. In most cases the explanations are based on &lt;a href="https://www.coursera.org/learn/machine-learning" rel="nofollow"&gt;this great machine learning course&lt;/a&gt; by Andrew Ng.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The purpose of this repository is &lt;em&gt;not&lt;/em&gt; to implement machine learning algorithms by using 3&lt;sup&gt;rd&lt;/sup&gt; party library one-liners &lt;em&gt;but&lt;/em&gt; rather to practice implementing these algorithms from scratch and get better understanding of the mathematics behind each algorithm. That's why all algorithms implementations are called "homemade" and not intended to be used for production.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-supervised-learning" class="anchor" aria-hidden="true" href="#supervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supervised Learning&lt;/h2&gt;
&lt;p&gt;In supervised learning we have a set of training data as an input and a set of labels or "correct answers" for each training set as an output. Then we're training our model (machine learning algorithm parameters) to map the input to the output correctly (to do correct prediction). The ultimate purpose is to find such model parameters that will successfully continue correct &lt;em&gt;input‚Üíoutput&lt;/em&gt; mapping (predictions) even for new input examples.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-regression" class="anchor" aria-hidden="true" href="#regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Regression&lt;/h3&gt;
&lt;p&gt;In regression problems we do real value predictions. Basically we try to draw a line/plane/n-dimensional plane along the training examples.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Usage examples: stock price forecast, sales analysis, dependency of any number, etc.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content--linear-regression" class="anchor" aria-hidden="true" href="#-linear-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;ü§ñ&lt;/g-emoji&gt; Linear Regression&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;üìó&lt;/g-emoji&gt; &lt;a href="homemade/linear_regression"&gt;Math | Linear Regression&lt;/a&gt; - theory and links for further readings&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;‚öôÔ∏è&lt;/g-emoji&gt; &lt;a href="homemade/linear_regression/linear_regression.py"&gt;Code | Linear Regression&lt;/a&gt; - implementation example&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/univariate_linear_regression_demo.ipynb" rel="nofollow"&gt;Demo | Univariate Linear Regression&lt;/a&gt; - predict &lt;code&gt;country happiness&lt;/code&gt; score by &lt;code&gt;economy GDP&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/multivariate_linear_regression_demo.ipynb" rel="nofollow"&gt;Demo | Multivariate Linear Regression&lt;/a&gt; - predict &lt;code&gt;country happiness&lt;/code&gt; score by &lt;code&gt;economy GDP&lt;/code&gt; and &lt;code&gt;freedom index&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/non_linear_regression_demo.ipynb" rel="nofollow"&gt;Demo | Non-linear Regression&lt;/a&gt; - use linear regression with &lt;em&gt;polynomial&lt;/em&gt; and &lt;em&gt;sinusoid&lt;/em&gt; features to predict non-linear dependencies&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-classification" class="anchor" aria-hidden="true" href="#classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Classification&lt;/h3&gt;
&lt;p&gt;In classification problems we split input examples by certain characteristic.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Usage examples: spam-filters, language detection, finding similar documents, handwritten letters recognition, etc.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content--logistic-regression" class="anchor" aria-hidden="true" href="#-logistic-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;ü§ñ&lt;/g-emoji&gt; Logistic Regression&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;üìó&lt;/g-emoji&gt; &lt;a href="homemade/logistic_regression"&gt;Math | Logistic Regression&lt;/a&gt; - theory and links for further readings&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;‚öôÔ∏è&lt;/g-emoji&gt; &lt;a href="homemade/logistic_regression/logistic_regression.py"&gt;Code | Logistic Regression&lt;/a&gt; - implementation example&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/logistic_regression_with_linear_boundary_demo.ipynb" rel="nofollow"&gt;Demo | Logistic Regression (Linear Boundary)&lt;/a&gt; - predict Iris flower &lt;code&gt;class&lt;/code&gt; based on &lt;code&gt;petal_length&lt;/code&gt; and &lt;code&gt;petal_width&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/logistic_regression_with_non_linear_boundary_demo.ipynb" rel="nofollow"&gt;Demo | Logistic Regression (Non-Linear Boundary)&lt;/a&gt; - predict microchip &lt;code&gt;validity&lt;/code&gt; based on &lt;code&gt;param_1&lt;/code&gt; and &lt;code&gt;param_2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/multivariate_logistic_regression_demo.ipynb" rel="nofollow"&gt;Demo | Multivariate Logistic Regression | MNIST&lt;/a&gt; - recognize handwritten digits from &lt;code&gt;28x28&lt;/code&gt; pixel images&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/multivariate_logistic_regression_fashion_demo.ipynb" rel="nofollow"&gt;Demo | Multivariate Logistic Regression | Fashion MNIST&lt;/a&gt; - recognize clothes types from &lt;code&gt;28x28&lt;/code&gt; pixel images&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-unsupervised-learning" class="anchor" aria-hidden="true" href="#unsupervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised Learning&lt;/h2&gt;
&lt;p&gt;Unsupervised learning is a branch of machine learning that learns from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-clustering" class="anchor" aria-hidden="true" href="#clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Clustering&lt;/h3&gt;
&lt;p&gt;In clustering problems we split the training examples by unknown characteristics. The algorithm itself decides what characteristic to use for splitting.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Usage examples: market segmentation, social networks analysis, organize computing clusters, astronomical data analysis, image compression, etc.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content--k-means-algorithm" class="anchor" aria-hidden="true" href="#-k-means-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;ü§ñ&lt;/g-emoji&gt; K-means Algorithm&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;üìó&lt;/g-emoji&gt; &lt;a href="homemade/k_means"&gt;Math | K-means Algorithm&lt;/a&gt; - theory and links for further readings&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;‚öôÔ∏è&lt;/g-emoji&gt; &lt;a href="homemade/k_means/k_means.py"&gt;Code | K-means Algorithm&lt;/a&gt; - implementation example&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/k_means/k_means_demo.ipynb" rel="nofollow"&gt;Demo | K-means Algorithm&lt;/a&gt; - split Iris flowers into clusters based on &lt;code&gt;petal_length&lt;/code&gt; and &lt;code&gt;petal_width&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-anomaly-detection" class="anchor" aria-hidden="true" href="#anomaly-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Anomaly Detection&lt;/h3&gt;
&lt;p&gt;Anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Usage examples: intrusion detection, fraud detection, system health monitoring, removing anomalous data from the dataset etc.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content--anomaly-detection-using-gaussian-distribution" class="anchor" aria-hidden="true" href="#-anomaly-detection-using-gaussian-distribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;ü§ñ&lt;/g-emoji&gt; Anomaly Detection using Gaussian Distribution&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;üìó&lt;/g-emoji&gt; &lt;a href="homemade/anomaly_detection"&gt;Math | Anomaly Detection using Gaussian Distribution&lt;/a&gt; - theory and links for further readings&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;‚öôÔ∏è&lt;/g-emoji&gt; &lt;a href="homemade/anomaly_detection/gaussian_anomaly_detection.py"&gt;Code | Anomaly Detection using Gaussian Distribution&lt;/a&gt; - implementation example&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/anomaly_detection/anomaly_detection_gaussian_demo.ipynb" rel="nofollow"&gt;Demo | Anomaly Detection&lt;/a&gt; - find anomalies in server operational parameters like &lt;code&gt;latency&lt;/code&gt; and &lt;code&gt;threshold&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-neural-network-nn" class="anchor" aria-hidden="true" href="#neural-network-nn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Neural Network (NN)&lt;/h2&gt;
&lt;p&gt;The neural network itself isn't an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Usage examples: as a substitute of all other algorithms in general, image recognition, voice recognition, image processing (applying specific style), language translation, etc.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content--multilayer-perceptron-mlp" class="anchor" aria-hidden="true" href="#-multilayer-perceptron-mlp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;ü§ñ&lt;/g-emoji&gt; Multilayer Perceptron (MLP)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;üìó&lt;/g-emoji&gt; &lt;a href="homemade/neural_network"&gt;Math | Multilayer Perceptron&lt;/a&gt; - theory and links for further readings&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;‚öôÔ∏è&lt;/g-emoji&gt; &lt;a href="homemade/neural_network/multilayer_perceptron.py"&gt;Code | Multilayer Perceptron&lt;/a&gt; - implementation example&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/neural_network/multilayer_perceptron_demo.ipynb" rel="nofollow"&gt;Demo | Multilayer Perceptron | MNIST&lt;/a&gt; - recognize handwritten digits from &lt;code&gt;28x28&lt;/code&gt; pixel images&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/neural_network/multilayer_perceptron_fashion_demo.ipynb" rel="nofollow"&gt;Demo | Multilayer Perceptron | Fashion MNIST&lt;/a&gt; - recognize the type of clothes from &lt;code&gt;28x28&lt;/code&gt; pixel images&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-machine-learning-map" class="anchor" aria-hidden="true" href="#machine-learning-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Map&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/machine-learning-map.png"&gt;&lt;img src="images/machine-learning-map.png" alt="Machine Learning Map" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The source of the following machine learning topics map is &lt;a href="https://vas3k.ru/blog/machine_learning/" rel="nofollow"&gt;this wonderful blog post&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-installing-python" class="anchor" aria-hidden="true" href="#installing-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing Python&lt;/h4&gt;
&lt;p&gt;Make sure that you have &lt;a href="https://realpython.com/installing-python/" rel="nofollow"&gt;Python installed&lt;/a&gt; on your machine.&lt;/p&gt;
&lt;p&gt;You might want to use &lt;a href="https://docs.python.org/3/library/venv.html" rel="nofollow"&gt;venv&lt;/a&gt; standard Python library
to create virtual environments and have Python, &lt;code&gt;pip&lt;/code&gt; and all dependent packages to be installed and
served from the local project directory to avoid messing with system wide packages and their
versions.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-installing-dependencies" class="anchor" aria-hidden="true" href="#installing-dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing Dependencies&lt;/h4&gt;
&lt;p&gt;Install all dependencies that are required for the project by running:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-launching-jupyter-locally" class="anchor" aria-hidden="true" href="#launching-jupyter-locally"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Launching Jupyter Locally&lt;/h4&gt;
&lt;p&gt;All demos in the project may be run directly in your browser without installing Jupyter locally. But if you want to launch &lt;a href="http://jupyter.org/" rel="nofollow"&gt;Jupyter Notebook&lt;/a&gt; locally you may do it by running the following command from the root folder of the project:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;jupyter notebook&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After this Jupyter Notebook will be accessible by &lt;code&gt;http://localhost:8888&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-launching-jupyter-remotely" class="anchor" aria-hidden="true" href="#launching-jupyter-remotely"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Launching Jupyter Remotely&lt;/h4&gt;
&lt;p&gt;Each algorithm section contains demo links to &lt;a href="http://nbviewer.jupyter.org/" rel="nofollow"&gt;Jupyter NBViewer&lt;/a&gt;. This is fast online previewer for Jupyter notebooks where you may see demo code, charts and data right in your browser without installing anything locally. In case if you want to &lt;em&gt;change&lt;/em&gt; the code and &lt;em&gt;experiment&lt;/em&gt; with demo notebook you need to launch the notebook in &lt;a href="https://mybinder.org/" rel="nofollow"&gt;Binder&lt;/a&gt;. You may do it by simply clicking the &lt;em&gt;"Execute on Binder"&lt;/em&gt; link in top right corner of the NBViewer.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/binder-button-place.png"&gt;&lt;img src="./images/binder-button-place.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h2&gt;
&lt;p&gt;The list of datasets that is being used for Jupyter Notebook demos may be found in &lt;a href="data"&gt;data folder&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>trekhleb</author><guid isPermaLink="false">https://github.com/trekhleb/homemade-machine-learning</guid><pubDate>Fri, 27 Dec 2019 00:01:00 GMT</pubDate></item><item><title>Pierian-Data/Complete-Python-3-Bootcamp #2 in Jupyter Notebook, Today</title><link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link><description>&lt;p&gt;&lt;i&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-complete-python-3-bootcamp" class="anchor" aria-hidden="true" href="#complete-python-3-bootcamp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete-Python-3-Bootcamp&lt;/h1&gt;
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;
&lt;p&gt;Get it now for 95% off with the link:
&lt;a href="https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB" rel="nofollow"&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pierian-Data</author><guid isPermaLink="false">https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</guid><pubDate>Fri, 27 Dec 2019 00:02:00 GMT</pubDate></item><item><title>lexfridman/mit-deep-learning #3 in Jupyter Notebook, Today</title><link>https://github.com/lexfridman/mit-deep-learning</link><description>&lt;p&gt;&lt;i&gt;Tutorials, assignments, and competitions for MIT Deep Learning related courses.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-mit-deep-learning" class="anchor" aria-hidden="true" href="#mit-deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MIT Deep Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://deeplearning.mit.edu/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/42d3d635cdb0e88dee786c6edc1f41e0902faa2b/68747470733a2f2f646565706c6561726e696e672e6d69742e6564752f66696c65732f696d616765732f6d69745f646565705f6c6561726e696e672e706e67" data-canonical-src="https://deeplearning.mit.edu/files/images/mit_deep_learning.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository is a collection of tutorials for &lt;a href="https://deeplearning.mit.edu/" rel="nofollow"&gt;MIT Deep Learning&lt;/a&gt; courses. More added as courses progress.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tutorial-deep-learning-basics" class="anchor" aria-hidden="true" href="#tutorial-deep-learning-basics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorial: Deep Learning Basics&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb"&gt;&lt;img src="https://camo.githubusercontent.com/0d2d7920619e3df257f1c677431968ff491a5e80/68747470733a2f2f692e696d6775722e636f6d2f6a3446714275522e676966" data-canonical-src="https://i.imgur.com/j4FqBuR.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tutorial accompanies the &lt;a href="https://www.youtube.com/watch?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf&amp;amp;v=O5xeyoRL95U" rel="nofollow"&gt;lecture on Deep Learning Basics&lt;/a&gt;. It presents several concepts in deep learning, demonstrating the first two (feed forward and convolutional neural networks) and providing pointers to tutorials on the others. This is a good place to start.&lt;/p&gt;
&lt;p&gt;Links: [ &lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb"&gt;Jupyter Notebook&lt;/a&gt; ]
[ &lt;a href="https://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt; ]
[ &lt;a href="https://medium.com/tensorflow/mit-deep-learning-basics-introduction-and-overview-with-tensorflow-355bcd26baf0" rel="nofollow"&gt;Blog Post&lt;/a&gt; ]
[ &lt;a href="https://www.youtube.com/watch?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf&amp;amp;v=O5xeyoRL95U" rel="nofollow"&gt;Lecture Video&lt;/a&gt; ]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tutorial-driving-scene-segmentation" class="anchor" aria-hidden="true" href="#tutorial-driving-scene-segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorial: Driving Scene Segmentation&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb"&gt;&lt;img src="images/thumb_driving_scene_segmentation.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tutorial demostrates semantic segmentation with a state-of-the-art model (DeepLab) on a sample video from the MIT Driving Scene Segmentation Dataset.&lt;/p&gt;
&lt;p&gt;Links: [ &lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb"&gt;Jupyter Notebook&lt;/a&gt; ]
[ &lt;a href="https://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt; ]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tutorial-generative-adversarial-networks-gans" class="anchor" aria-hidden="true" href="#tutorial-generative-adversarial-networks-gans"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorial: Generative Adversarial Networks (GANs)&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_gans/tutorial_gans.ipynb"&gt;&lt;img src="images/thumb_mushroom_biggan.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tutorial explores generative adversarial networks (GANs) starting with BigGAN, the state-of-the-art conditional GAN.&lt;/p&gt;
&lt;p&gt;Links: [ &lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_gans/tutorial_gans.ipynb"&gt;Jupyter Notebook&lt;/a&gt; ]
[ &lt;a href="https://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_gans/tutorial_gans.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt; ]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deeptraffic-deep-reinforcement-learning-competition" class="anchor" aria-hidden="true" href="#deeptraffic-deep-reinforcement-learning-competition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepTraffic Deep Reinforcement Learning Competition&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://selfdrivingcars.mit.edu/deeptraffic" rel="nofollow"&gt;&lt;img src="images/thumb_deeptraffic.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;DeepTraffic is a deep reinforcement learning competition. The goal is to create a neural network that drives a vehicle (or multiple vehicles) as fast as possible through dense highway traffic.&lt;/p&gt;
&lt;p&gt;Links: [ &lt;a href="https://github.com/lexfridman/deeptraffic"&gt;GitHub&lt;/a&gt; ] [ &lt;a href="https://selfdrivingcars.mit.edu/deeptraffic" rel="nofollow"&gt;Website&lt;/a&gt; ] [ &lt;a href="https://arxiv.org/abs/1801.02805" rel="nofollow"&gt;Paper&lt;/a&gt; ]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-team" class="anchor" aria-hidden="true" href="#team"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Team&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://lexfridman.com" rel="nofollow"&gt;Lex Fridman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~liding/" rel="nofollow"&gt;Li Ding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~jterwill/" rel="nofollow"&gt;Jack Terwilliger&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~glazermi/" rel="nofollow"&gt;Michael Glazer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~patsekin/" rel="nofollow"&gt;Aleksandr Patsekin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~aishni/" rel="nofollow"&gt;Aishni Parab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~aladawy/" rel="nofollow"&gt;Dina AlAdawy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~henris/" rel="nofollow"&gt;Henri Schmidt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>lexfridman</author><guid isPermaLink="false">https://github.com/lexfridman/mit-deep-learning</guid><pubDate>Fri, 27 Dec 2019 00:03:00 GMT</pubDate></item><item><title>chiphuyen/python-is-cool #4 in Jupyter Notebook, Today</title><link>https://github.com/chiphuyen/python-is-cool</link><description>&lt;p&gt;&lt;i&gt;Cool Python features for machine learning that I used to be too afraid to use. Will be updated as I have more time / learn more.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python-is-cool" class="anchor" aria-hidden="true" href="#python-is-cool"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python-is-cool&lt;/h1&gt;
&lt;p&gt;A gentle guide to the Python features that I didn't know existed or was too afraid to use. This will be updated as I learn more and become less lazy.&lt;/p&gt;
&lt;p&gt;This uses &lt;code&gt;python &amp;gt;= 3.6&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;GitHub has problem rendering Jupyter notebook so I copied the content here. I still keep the notebook in case you want to clone and run it on your machine, but you can also click the Binder badge below and run it in your browser.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://mybinder.org/v2/gh/chiphuyen/python-is-cool/master?urlpath=lab/tree/cool-python-tips.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-1-lambda-map-filter-reduce" class="anchor" aria-hidden="true" href="#1-lambda-map-filter-reduce"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Lambda, map, filter, reduce&lt;/h2&gt;
&lt;p&gt;The lambda keyword is used to create inline functions. The functions&lt;code&gt;square_fn&lt;/code&gt; and &lt;code&gt;square_ld&lt;/code&gt; below are identical.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;square_fn&lt;/span&gt;(&lt;span class="pl-smi"&gt;x&lt;/span&gt;):
    &lt;span class="pl-k"&gt;return&lt;/span&gt; x &lt;span class="pl-k"&gt;*&lt;/span&gt; x

square_ld &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x &lt;span class="pl-k"&gt;*&lt;/span&gt; x

&lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;10&lt;/span&gt;):
    &lt;span class="pl-k"&gt;assert&lt;/span&gt; square_fn(i) &lt;span class="pl-k"&gt;==&lt;/span&gt; square_ld(i)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Its quick declaration makes &lt;code&gt;lambda&lt;/code&gt; functions ideal for use in callbacks, and when functions are to be passed as arguments to other functions. They are especially useful when used in conjunction with functions like &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;, and &lt;code&gt;reduce&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;map(fn, iterable)&lt;/code&gt; applies the &lt;code&gt;fn&lt;/code&gt; to all elements of the &lt;code&gt;iterable&lt;/code&gt; (e.g. list, set, dictionary, tuple, string) and returns a map object.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;nums &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;333&lt;/span&gt;&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;2323&lt;/span&gt;&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;2230&lt;/span&gt;, &lt;span class="pl-c1"&gt;40&lt;/span&gt;&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;34&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;3&lt;/span&gt;]
nums_squared &lt;span class="pl-k"&gt;=&lt;/span&gt; [num &lt;span class="pl-k"&gt;*&lt;/span&gt; num &lt;span class="pl-k"&gt;for&lt;/span&gt; num &lt;span class="pl-k"&gt;in&lt;/span&gt; nums]
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(nums_squared)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0.1111111&lt;/span&gt;, &lt;span class="pl-c1"&gt;2263.04081632&lt;/span&gt;, &lt;span class="pl-c1"&gt;1.085147&lt;/span&gt;, &lt;span class="pl-c1"&gt;1.384083&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.44444444&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the same as calling using &lt;code&gt;map&lt;/code&gt; with a callback function.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;nums_squared_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;map&lt;/span&gt;(square_fn, nums)
nums_squared_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;map&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x &lt;span class="pl-k"&gt;*&lt;/span&gt; x, nums)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-c1"&gt;list&lt;/span&gt;(nums_squared_1))

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0.1111111&lt;/span&gt;, &lt;span class="pl-c1"&gt;2263.04081632&lt;/span&gt;, &lt;span class="pl-c1"&gt;1.085147&lt;/span&gt;, &lt;span class="pl-c1"&gt;1.384083&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.44444444&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can also use &lt;code&gt;map&lt;/code&gt; with more than one iterable. For example, if you want to calculate the mean squared error of a simple linear function &lt;code&gt;f(x) = ax + b&lt;/code&gt; with the true label &lt;code&gt;labels&lt;/code&gt;, these two methods are equivalent:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;a, b &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;0.5&lt;/span&gt;
xs &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;]
labels &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;6.4&lt;/span&gt;, &lt;span class="pl-c1"&gt;8.9&lt;/span&gt;, &lt;span class="pl-c1"&gt;10.9&lt;/span&gt;, &lt;span class="pl-c1"&gt;15.3&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Method 1: using a loop&lt;/span&gt;
errors &lt;span class="pl-k"&gt;=&lt;/span&gt; []
&lt;span class="pl-k"&gt;for&lt;/span&gt; i, x &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;enumerate&lt;/span&gt;(xs):
    errors.append((a &lt;span class="pl-k"&gt;*&lt;/span&gt; x &lt;span class="pl-k"&gt;+&lt;/span&gt; b &lt;span class="pl-k"&gt;-&lt;/span&gt; labels[i]) &lt;span class="pl-k"&gt;**&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt;)
result1 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;sum&lt;/span&gt;(errors) &lt;span class="pl-k"&gt;**&lt;/span&gt; &lt;span class="pl-c1"&gt;0.5&lt;/span&gt; &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(xs)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Method 2: using map&lt;/span&gt;
diffs &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;map&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;y&lt;/span&gt;: (a &lt;span class="pl-k"&gt;*&lt;/span&gt; x &lt;span class="pl-k"&gt;+&lt;/span&gt; b &lt;span class="pl-k"&gt;-&lt;/span&gt; y) &lt;span class="pl-k"&gt;**&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt;, xs, labels)
result2 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;sum&lt;/span&gt;(diffs) &lt;span class="pl-k"&gt;**&lt;/span&gt; &lt;span class="pl-c1"&gt;0.5&lt;/span&gt; &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(xs)

&lt;span class="pl-c1"&gt;print&lt;/span&gt;(result1, result2)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.35089172119045514&lt;/span&gt; &lt;span class="pl-c1"&gt;0.35089172119045514&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that objects returned by &lt;code&gt;map&lt;/code&gt; and &lt;code&gt;filter&lt;/code&gt; are iterators, which means that their values aren't stored but generated as needed. After you've called &lt;code&gt;sum(diffs)&lt;/code&gt;, &lt;code&gt;diffs&lt;/code&gt; becomes empty. If you want to keep all elements in &lt;code&gt;diffs&lt;/code&gt;, convert it to a list using &lt;code&gt;list(diffs)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;filter(fn, iterable)&lt;/code&gt; works the same way as &lt;code&gt;map&lt;/code&gt;, except that &lt;code&gt;fn&lt;/code&gt; returns a boolean value and &lt;code&gt;filter&lt;/code&gt; returns all the elements of the &lt;code&gt;iterable&lt;/code&gt; for which the &lt;code&gt;fn&lt;/code&gt; returns True.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bad_preds &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;filter&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.5&lt;/span&gt;, errors)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-c1"&gt;list&lt;/span&gt;(bad_preds))

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0.8100000000000006&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.6400000000000011&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;reduce(fn, iterable, initializer)&lt;/code&gt; is used when we want to iteratively apply an operator to all elements in a list. For example, if we want to calculate the product of all elements in a list:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;product &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; num &lt;span class="pl-k"&gt;in&lt;/span&gt; nums:
    product &lt;span class="pl-k"&gt;*=&lt;/span&gt; num
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(product)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;12.95564683272412&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is equivalent to:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; functools &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-v"&gt;reduce&lt;/span&gt;
product &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-v"&gt;reduce&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;y&lt;/span&gt;: x &lt;span class="pl-k"&gt;*&lt;/span&gt; y, nums)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(product)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;12.95564683272412&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-note-on-the-performance-of-lambda-functions" class="anchor" aria-hidden="true" href="#note-on-the-performance-of-lambda-functions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note on the performance of lambda functions&lt;/h3&gt;
&lt;p&gt;Lambda functions are meant for one time use. Each time &lt;code&gt;lambda x: dosomething(x)&lt;/code&gt; is called, the function has to be created, which hurts the performance if you call &lt;code&gt;lambda x: dosomething(x)&lt;/code&gt; multiple times (e.g. when you pass it inside &lt;code&gt;reduce&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;When you assign a name to the lambda function as in &lt;code&gt;fn = lambda x: dosomething(x)&lt;/code&gt;, its performance is slightly slower than the same function defined using &lt;code&gt;def&lt;/code&gt;, but the difference is negligible. See &lt;a href="https://stackoverflow.com/questions/26540885/lambda-is-slower-than-function-call-in-python-why" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Even though I find lambdas cool, I personally recommend using named functions when you can for the sake of clarity.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-2-list-manipulation" class="anchor" aria-hidden="true" href="#2-list-manipulation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. List manipulation&lt;/h2&gt;
&lt;p&gt;Python lists are super cool.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-21-unpacking" class="anchor" aria-hidden="true" href="#21-unpacking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.1 Unpacking&lt;/h3&gt;
&lt;p&gt;We can unpack a list by each element like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;elems &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;]
a, b, c, d &lt;span class="pl-k"&gt;=&lt;/span&gt; elems
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(a, b, c, d)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt; &lt;span class="pl-c1"&gt;3&lt;/span&gt; &lt;span class="pl-c1"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can also unpack a list like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;a, &lt;span class="pl-k"&gt;*&lt;/span&gt;new_elems, d &lt;span class="pl-k"&gt;=&lt;/span&gt; elems
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(a)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(new_elems)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(d)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;
    [&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;]
    &lt;span class="pl-c1"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-22-slicing" class="anchor" aria-hidden="true" href="#22-slicing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.2 Slicing&lt;/h3&gt;
&lt;p&gt;We know that we can reverse a list using &lt;code&gt;[::-1]&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;elems &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;10&lt;/span&gt;))
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(elems)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-c1"&gt;9&lt;/span&gt;]

&lt;span class="pl-c1"&gt;print&lt;/span&gt;(elems[::&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;])

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;9&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The syntax &lt;code&gt;[x:y:z]&lt;/code&gt; means "take every &lt;code&gt;z&lt;/code&gt;th element of a list from index &lt;code&gt;x&lt;/code&gt; to index &lt;code&gt;y&lt;/code&gt;". When &lt;code&gt;z&lt;/code&gt; is negative, it indicates going backwards. When &lt;code&gt;x&lt;/code&gt; isn't specified, it defaults to the first element of the list in the direction you are traversing the list. When &lt;code&gt;y&lt;/code&gt; isn't specified, it defaults to the last element of the list. So if we want to take every 2th element of a list, we use &lt;code&gt;[::2]&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;evens &lt;span class="pl-k"&gt;=&lt;/span&gt; elems[::&lt;span class="pl-c1"&gt;2&lt;/span&gt;]
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(evens)

reversed_evens &lt;span class="pl-k"&gt;=&lt;/span&gt; elems[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;::&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;]
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(reversed_evens)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;]
    [&lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can also use slicing to delete all the even numbers in the list.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;del&lt;/span&gt; elems[::&lt;span class="pl-c1"&gt;2&lt;/span&gt;]
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(elems)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;9&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-23-insertion" class="anchor" aria-hidden="true" href="#23-insertion"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.3 Insertion&lt;/h3&gt;
&lt;p&gt;We can change the value of an element in a list to another value.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;elems &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;10&lt;/span&gt;))
elems[&lt;span class="pl-c1"&gt;1&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(elems)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;10&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-c1"&gt;9&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we want to replace the element at an index with multiple elements, e.g. replace the value &lt;code&gt;1&lt;/code&gt; with 3 values &lt;code&gt;20, 30, 40&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;elems &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;10&lt;/span&gt;))
elems[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;2&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;20&lt;/span&gt;, &lt;span class="pl-c1"&gt;30&lt;/span&gt;, &lt;span class="pl-c1"&gt;40&lt;/span&gt;]
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(elems)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;20&lt;/span&gt;, &lt;span class="pl-c1"&gt;30&lt;/span&gt;, &lt;span class="pl-c1"&gt;40&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-c1"&gt;9&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we want to insert 3 values &lt;code&gt;0.2, 0.3, 0.5&lt;/code&gt; between element at index 0 and element at index 1:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;elems &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;10&lt;/span&gt;))
elems[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;1&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;0.2&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.3&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.5&lt;/span&gt;]
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(elems)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.2&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.3&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.5&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-c1"&gt;9&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-24-flattening" class="anchor" aria-hidden="true" href="#24-flattening"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.4 Flattening&lt;/h3&gt;
&lt;p&gt;We can flatten a list of lists using &lt;code&gt;sum&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;list_of_lists &lt;span class="pl-k"&gt;=&lt;/span&gt; [[&lt;span class="pl-c1"&gt;1&lt;/span&gt;], [&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;]]
&lt;span class="pl-c1"&gt;sum&lt;/span&gt;(list_of_lists, [])

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we have nested lists, we can recursively flatten it. That's another beauty of lambda functions -- we can use it in the same line as its creation.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;nested_lists &lt;span class="pl-k"&gt;=&lt;/span&gt; [[&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;], [[&lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;], [&lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;], [[&lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;], [&lt;span class="pl-c1"&gt;9&lt;/span&gt;, &lt;span class="pl-c1"&gt;10&lt;/span&gt;], [[&lt;span class="pl-c1"&gt;11&lt;/span&gt;, [&lt;span class="pl-c1"&gt;12&lt;/span&gt;, &lt;span class="pl-c1"&gt;13&lt;/span&gt;]]]]]]
flatten &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: [y &lt;span class="pl-k"&gt;for&lt;/span&gt; l &lt;span class="pl-k"&gt;in&lt;/span&gt; x &lt;span class="pl-k"&gt;for&lt;/span&gt; y &lt;span class="pl-k"&gt;in&lt;/span&gt; flatten(l)] &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;type&lt;/span&gt;(x) &lt;span class="pl-k"&gt;is&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt; &lt;span class="pl-k"&gt;else&lt;/span&gt; [x]
flatten(nested_lists)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This line of code is from&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; https://github.com/sahands/python-by-example/blob/master/python-by-example.rst#flattening-lists&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-25-list-vs-generator" class="anchor" aria-hidden="true" href="#25-list-vs-generator"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.5 List vs generator&lt;/h3&gt;
&lt;p&gt;To illustrate the difference between a list and a generator, let's look at an example of creating n-grams out of a list of tokens.&lt;/p&gt;
&lt;p&gt;One way to create n-grams is to use a sliding window.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;i&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;school&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;ngrams&lt;/span&gt;(&lt;span class="pl-smi"&gt;tokens&lt;/span&gt;, &lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    length &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(tokens)
    grams &lt;span class="pl-k"&gt;=&lt;/span&gt; []
    &lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(length &lt;span class="pl-k"&gt;-&lt;/span&gt; n &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;):
        grams.append(tokens[i:i&lt;span class="pl-k"&gt;+&lt;/span&gt;n])
    &lt;span class="pl-k"&gt;return&lt;/span&gt; grams

&lt;span class="pl-c1"&gt;print&lt;/span&gt;(ngrams(tokens, &lt;span class="pl-c1"&gt;3&lt;/span&gt;))

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;i&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
     [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
     [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
     [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;school&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the above example, we have to store all the n-grams at the same time. If the text has m tokens, then the memory requirement is &lt;code&gt;O(nm)&lt;/code&gt;, which can be problematic when m is large.&lt;/p&gt;
&lt;p&gt;Instead of using a list to store all n-grams, we can use a generator that generates the next n-gram when it's asked for. This is known as lazy evaluation. We can make the function &lt;code&gt;ngrams&lt;/code&gt; returns a generator using the keyword &lt;code&gt;yield&lt;/code&gt;. Then the memory requirement is &lt;code&gt;O(m+n)&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;ngrams&lt;/span&gt;(&lt;span class="pl-smi"&gt;tokens&lt;/span&gt;, &lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    length &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(tokens)
    &lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(length &lt;span class="pl-k"&gt;-&lt;/span&gt; n &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;):
        &lt;span class="pl-k"&gt;yield&lt;/span&gt; tokens[i:i&lt;span class="pl-k"&gt;+&lt;/span&gt;n]

ngrams_generator &lt;span class="pl-k"&gt;=&lt;/span&gt; ngrams(tokens, &lt;span class="pl-c1"&gt;3&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(ngrams_generator)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;generator &lt;span class="pl-c1"&gt;object&lt;/span&gt; ngrams at &lt;span class="pl-c1"&gt;&lt;span class="pl-k"&gt;0x&lt;/span&gt;1069b26d0&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt;

&lt;span class="pl-k"&gt;for&lt;/span&gt; ngram &lt;span class="pl-k"&gt;in&lt;/span&gt; ngrams_generator:
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(ngram)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;i&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;school&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Another way to generate n-grams is to use slices to create lists: &lt;code&gt;[0, 1, ..., -n]&lt;/code&gt;, &lt;code&gt;[1, 2, ..., -n+1]&lt;/code&gt;, ..., &lt;code&gt;[n-1, n, ..., -1]&lt;/code&gt;, and then &lt;code&gt;zip&lt;/code&gt; them together.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;ngrams&lt;/span&gt;(&lt;span class="pl-smi"&gt;tokens&lt;/span&gt;, &lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    length &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(tokens)
    slices &lt;span class="pl-k"&gt;=&lt;/span&gt; (tokens[i:length&lt;span class="pl-k"&gt;-&lt;/span&gt;n&lt;span class="pl-k"&gt;+&lt;/span&gt;i&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;] &lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(n))
    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;zip&lt;/span&gt;(&lt;span class="pl-k"&gt;*&lt;/span&gt;slices)

ngrams_generator &lt;span class="pl-k"&gt;=&lt;/span&gt; ngrams(tokens, &lt;span class="pl-c1"&gt;3&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(ngrams_generator)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;&lt;span class="pl-c1"&gt;zip&lt;/span&gt; &lt;span class="pl-c1"&gt;object&lt;/span&gt; at &lt;span class="pl-c1"&gt;&lt;span class="pl-k"&gt;0x&lt;/span&gt;1069a7dc8&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; zip objects are generators&lt;/span&gt;

&lt;span class="pl-k"&gt;for&lt;/span&gt; ngram &lt;span class="pl-k"&gt;in&lt;/span&gt; ngrams_generator:
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(ngram)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;i&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;school&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that to create slices, we use &lt;code&gt;(tokens[...] for i in range(n))&lt;/code&gt; instead of &lt;code&gt;[tokens[...] for i in range(n)]&lt;/code&gt;. &lt;code&gt;[]&lt;/code&gt; is the normal list comprehension that returns a list. &lt;code&gt;()&lt;/code&gt; returns a generator.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-3-classes-and-magic-methods" class="anchor" aria-hidden="true" href="#3-classes-and-magic-methods"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. Classes and magic methods&lt;/h2&gt;
&lt;p&gt;In Python, magic methods are prefixed and suffixed with the double underscore &lt;code&gt;__&lt;/code&gt;, also known as dunder. The most wellknown magic method is probably &lt;code&gt;__init__&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Node&lt;/span&gt;:
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt; A struct to denote the node of a binary tree.&lt;/span&gt;
&lt;span class="pl-s"&gt;    It contains a value and pointers to left and right children.&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;value&lt;/span&gt;, &lt;span class="pl-smi"&gt;left&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;, &lt;span class="pl-smi"&gt;right&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;=&lt;/span&gt; value
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.left &lt;span class="pl-k"&gt;=&lt;/span&gt; left
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.right &lt;span class="pl-k"&gt;=&lt;/span&gt; right&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When we try to print out a Node object, however, it's not very interpretable.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;root &lt;span class="pl-k"&gt;=&lt;/span&gt; Node(&lt;span class="pl-c1"&gt;5&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(root) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; &amp;lt;__main__.Node object at 0x1069c4518&amp;gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ideally, when user prints out a node, we want to print out the node's value and the values of its children if it has children. To do so, we use the magic method &lt;code&gt;__repr__&lt;/code&gt;, which must return a printable object, like string.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Node&lt;/span&gt;:
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt; A struct to denote the node of a binary tree.&lt;/span&gt;
&lt;span class="pl-s"&gt;    It contains a value and pointers to left and right children.&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;value&lt;/span&gt;, &lt;span class="pl-smi"&gt;left&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;, &lt;span class="pl-smi"&gt;right&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;=&lt;/span&gt; value
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.left &lt;span class="pl-k"&gt;=&lt;/span&gt; left
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.right &lt;span class="pl-k"&gt;=&lt;/span&gt; right

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__repr__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        strings &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;value: &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;&lt;span class="pl-c1"&gt;self&lt;/span&gt;.value&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;]
        strings.append(&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;left: &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;&lt;span class="pl-c1"&gt;self&lt;/span&gt;.left.value&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.left &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;left: None&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
        strings.append(&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;right: &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;&lt;span class="pl-c1"&gt;self&lt;/span&gt;.right.value&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.right &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;right: None&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;, &lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.join(strings)

left &lt;span class="pl-k"&gt;=&lt;/span&gt; Node(&lt;span class="pl-c1"&gt;4&lt;/span&gt;)
root &lt;span class="pl-k"&gt;=&lt;/span&gt; Node(&lt;span class="pl-c1"&gt;5&lt;/span&gt;, left)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(root) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; value: 5, left: 4, right: None&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We'd also like to compare two nodes by comparing their values. To do so, we overload the operator &lt;code&gt;==&lt;/code&gt; with &lt;code&gt;__eq__&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt; with &lt;code&gt;__lt__&lt;/code&gt;, and &lt;code&gt;&amp;gt;=&lt;/code&gt; with &lt;code&gt;__ge__&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Node&lt;/span&gt;:
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt; A struct to denote the node of a binary tree.&lt;/span&gt;
&lt;span class="pl-s"&gt;    It contains a value and pointers to left and right children.&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;value&lt;/span&gt;, &lt;span class="pl-smi"&gt;left&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;, &lt;span class="pl-smi"&gt;right&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;=&lt;/span&gt; value
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.left &lt;span class="pl-k"&gt;=&lt;/span&gt; left
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.right &lt;span class="pl-k"&gt;=&lt;/span&gt; right

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__eq__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;other&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;==&lt;/span&gt; other.value

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__lt__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;other&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; other.value

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__ge__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;other&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;&amp;gt;=&lt;/span&gt; other.value


left &lt;span class="pl-k"&gt;=&lt;/span&gt; Node(&lt;span class="pl-c1"&gt;4&lt;/span&gt;)
root &lt;span class="pl-k"&gt;=&lt;/span&gt; Node(&lt;span class="pl-c1"&gt;5&lt;/span&gt;, left)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(left &lt;span class="pl-k"&gt;==&lt;/span&gt; root) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; False&lt;/span&gt;
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(left &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; root) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; True&lt;/span&gt;
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(left &lt;span class="pl-k"&gt;&amp;gt;=&lt;/span&gt; root) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; False&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For a comprehensive list of supported magic methods &lt;a href="https://www.tutorialsteacher.com/python/magic-methods-in-python" rel="nofollow"&gt;here&lt;/a&gt; or see the official Python documentation &lt;a href="https://docs.python.org/3/reference/datamodel.html#special-method-names" rel="nofollow"&gt;here&lt;/a&gt; (slightly harder to read).&lt;/p&gt;
&lt;p&gt;Some of the methods that I highly recommend:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;__len__&lt;/code&gt;: to overload the &lt;code&gt;len()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__str__&lt;/code&gt;: to overload the &lt;code&gt;str()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__iter__&lt;/code&gt;: if you want to your objects to be iterators. This also allows you to call &lt;code&gt;next()&lt;/code&gt; on your object.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For classes like Node where we know for sure all the attributes they can support (in the case of Node, they are &lt;code&gt;value&lt;/code&gt;, &lt;code&gt;left&lt;/code&gt;, and &lt;code&gt;right&lt;/code&gt;), we might want to use &lt;code&gt;__slots__&lt;/code&gt; to denote those values for both performance boost and memory saving. For a comprehensive understanding of pros and cons of &lt;code&gt;__slots__&lt;/code&gt;, see this &lt;a href="https://stackoverflow.com/a/28059785/5029595" rel="nofollow"&gt;absolutely amazing answer by Aaron Hall on StackOverflow&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Node&lt;/span&gt;:
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt; A struct to denote the node of a binary tree.&lt;/span&gt;
&lt;span class="pl-s"&gt;    It contains a value and pointers to left and right children.&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-c1"&gt;__slots__&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;value&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;left&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;right&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;value&lt;/span&gt;, &lt;span class="pl-smi"&gt;left&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;, &lt;span class="pl-smi"&gt;right&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;=&lt;/span&gt; value
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.left &lt;span class="pl-k"&gt;=&lt;/span&gt; left
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.right &lt;span class="pl-k"&gt;=&lt;/span&gt; right&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-4-local-namespace-objects-attributes" class="anchor" aria-hidden="true" href="#4-local-namespace-objects-attributes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4. local namespace, object's attributes&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;locals()&lt;/code&gt; function returns a dictionary containing the variables defined in the local namespace.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Model1&lt;/span&gt;:
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;hidden_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-smi"&gt;num_layers&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-smi"&gt;learning_rate&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3e-4&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-c1"&gt;locals&lt;/span&gt;())
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.hidden_size &lt;span class="pl-k"&gt;=&lt;/span&gt; hidden_size
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.num_layers &lt;span class="pl-k"&gt;=&lt;/span&gt; num_layers
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.learning_rate &lt;span class="pl-k"&gt;=&lt;/span&gt; learning_rate

model1 &lt;span class="pl-k"&gt;=&lt;/span&gt; Model1()

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.0003&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;num_layers&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hidden_size&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;self&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;__main__.Model1 &lt;span class="pl-c1"&gt;object&lt;/span&gt; at &lt;span class="pl-c1"&gt;&lt;span class="pl-k"&gt;0x&lt;/span&gt;1069b1470&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt;}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All attributes of an object are stored in its &lt;code&gt;__dict__&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;print&lt;/span&gt;(model1.&lt;span class="pl-c1"&gt;__dict__&lt;/span&gt;)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hidden_size&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;num_layers&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.0003&lt;/span&gt;}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that manually assigning each of the arguments to an attribute can be quite tiring when the list of the arguments is large. To avoid this, we can directly assign the list of arguments to the object's &lt;code&gt;__dict__&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Model2&lt;/span&gt;:
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;hidden_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-smi"&gt;num_layers&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-smi"&gt;learning_rate&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3e-4&lt;/span&gt;):
        params &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;locals&lt;/span&gt;()
        &lt;span class="pl-k"&gt;del&lt;/span&gt; params[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;self&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.&lt;span class="pl-c1"&gt;__dict__&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; params

model2 &lt;span class="pl-k"&gt;=&lt;/span&gt; Model2()
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(model2.&lt;span class="pl-c1"&gt;__dict__&lt;/span&gt;)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.0003&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;num_layers&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hidden_size&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;100&lt;/span&gt;}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This can be especially convenient when the object is initiated using the catch-all &lt;code&gt;**kwargs&lt;/code&gt;, though the use of &lt;code&gt;**kwargs&lt;/code&gt; should be reduced to the minimum.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Model3&lt;/span&gt;:
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-k"&gt;**&lt;/span&gt;&lt;span class="pl-smi"&gt;kwargs&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.&lt;span class="pl-c1"&gt;__dict__&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; kwargs

model3 &lt;span class="pl-k"&gt;=&lt;/span&gt; Model3(&lt;span class="pl-v"&gt;hidden_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-v"&gt;num_layers&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-v"&gt;learning_rate&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3e-4&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(model3.&lt;span class="pl-c1"&gt;__dict__&lt;/span&gt;)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hidden_size&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;num_layers&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.0003&lt;/span&gt;}&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-5-wild-import" class="anchor" aria-hidden="true" href="#5-wild-import"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5. Wild import&lt;/h2&gt;
&lt;p&gt;Often, you run into this wild import &lt;code&gt;*&lt;/code&gt; that looks something like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;file.py&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;    &lt;span class="pl-k"&gt;from&lt;/span&gt; parts &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is irresponsible because it will import everything in module, even the imports of that module. For example, if &lt;code&gt;parts.py&lt;/code&gt; looks like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;parts.py&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow

&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Encoder&lt;/span&gt;:
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Decoder&lt;/span&gt;:
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Loss&lt;/span&gt;:
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;helper&lt;/span&gt;(&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;args&lt;/span&gt;, &lt;span class="pl-k"&gt;**&lt;/span&gt;&lt;span class="pl-smi"&gt;kwargs&lt;/span&gt;):
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;utils&lt;/span&gt;(&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;args&lt;/span&gt;, &lt;span class="pl-k"&gt;**&lt;/span&gt;&lt;span class="pl-smi"&gt;kwargs&lt;/span&gt;):
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since &lt;code&gt;parts.py&lt;/code&gt; doesn't have &lt;code&gt;__all__&lt;/code&gt; specified, &lt;code&gt;file.py&lt;/code&gt; will import Encoder, Decoder, Loss, utils, helper together with numpy and tensorflow.&lt;/p&gt;
&lt;p&gt;If we intend that only Encoder, Decoder, and Loss are ever to be imported and used in another module, we should specify that in &lt;code&gt;parts.py&lt;/code&gt; using the &lt;code&gt;__all__&lt;/code&gt; keyword.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;parts.py&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt; &lt;span class="pl-c1"&gt;__all__&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Encoder&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Decoder&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Loss&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow

&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Encoder&lt;/span&gt;:
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, if some user irresponsibly does a wild import with &lt;code&gt;parts&lt;/code&gt;, they can only import Encoder, Decoder, Loss. Personally, I also find &lt;code&gt;__all__&lt;/code&gt; helpful as it gives me an overview of the module.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-6-decorator-to-time-your-functions" class="anchor" aria-hidden="true" href="#6-decorator-to-time-your-functions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;6. Decorator to time your functions&lt;/h2&gt;
&lt;p&gt;It's often useful to know how long it takes a function to run, e.g. when you need to compare the performance of two algorithms that do the same thing. One naive way is to call &lt;code&gt;time.time()&lt;/code&gt; at the begin and end of each function and print out the difference.&lt;/p&gt;
&lt;p&gt;For example: compare two algorithms to calculate the n-th Fibonacci number, one uses memoization and one doesn't.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;fib_helper&lt;/span&gt;(&lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    &lt;span class="pl-k"&gt;if&lt;/span&gt; n &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt;:
        &lt;span class="pl-k"&gt;return&lt;/span&gt; n
    &lt;span class="pl-k"&gt;return&lt;/span&gt; fib_helper(n &lt;span class="pl-k"&gt;-&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;) &lt;span class="pl-k"&gt;+&lt;/span&gt; fib_helper(n &lt;span class="pl-k"&gt;-&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt;)

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;fib&lt;/span&gt;(&lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt; fib is a wrapper function so that later we can change its behavior&lt;/span&gt;
&lt;span class="pl-s"&gt;    at the top level without affecting the behavior at every recursion step.&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;return&lt;/span&gt; fib_helper(n)

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;fib_m_helper&lt;/span&gt;(&lt;span class="pl-smi"&gt;n&lt;/span&gt;, &lt;span class="pl-smi"&gt;computed&lt;/span&gt;):
    &lt;span class="pl-k"&gt;if&lt;/span&gt; n &lt;span class="pl-k"&gt;in&lt;/span&gt; computed:
        &lt;span class="pl-k"&gt;return&lt;/span&gt; computed[n]
    computed[n] &lt;span class="pl-k"&gt;=&lt;/span&gt; fib_m_helper(n &lt;span class="pl-k"&gt;-&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;, computed) &lt;span class="pl-k"&gt;+&lt;/span&gt; fib_m_helper(n &lt;span class="pl-k"&gt;-&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt;, computed)
    &lt;span class="pl-k"&gt;return&lt;/span&gt; computed[n]

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;fib_m&lt;/span&gt;(&lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    &lt;span class="pl-k"&gt;return&lt;/span&gt; fib_m_helper(n, {&lt;span class="pl-c1"&gt;0&lt;/span&gt;: &lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;: &lt;span class="pl-c1"&gt;1&lt;/span&gt;})&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let's make sure that &lt;code&gt;fib&lt;/code&gt; and &lt;code&gt;fib_m&lt;/code&gt; are functionally equivalent.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;for&lt;/span&gt; n &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;20&lt;/span&gt;):
    &lt;span class="pl-k"&gt;assert&lt;/span&gt; fib(n) &lt;span class="pl-k"&gt;==&lt;/span&gt; fib_m(n)&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; time

start &lt;span class="pl-k"&gt;=&lt;/span&gt; time.time()
fib(&lt;span class="pl-c1"&gt;30&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;Without memoization, it takes &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;time.time() &lt;span class="pl-k"&gt;-&lt;/span&gt; start&lt;span class="pl-k"&gt;:7f&lt;/span&gt;&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-s"&gt; seconds.&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; Without memoization, it takes &lt;span class="pl-c1"&gt;0.267569&lt;/span&gt; seconds.

start &lt;span class="pl-k"&gt;=&lt;/span&gt; time.time()
fib_m(&lt;span class="pl-c1"&gt;30&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;With memoization, it takes &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;time.time() &lt;span class="pl-k"&gt;-&lt;/span&gt; start&lt;span class="pl-k"&gt;:.7f&lt;/span&gt;&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-s"&gt; seconds.&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; With memoization, it takes &lt;span class="pl-c1"&gt;0.0000713&lt;/span&gt; seconds.&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you want to time multiple functions, it can be a drag having to write the same code over and over again. It'd be nice to have a way to specify how to change any function in the same way. In this case would be to call time.time() at the beginning and the end of each function, and print out the time difference.&lt;/p&gt;
&lt;p&gt;This is exactly what decorators do. They allow programmers to change the behavior of a function or class. Here's an example to create a decorator &lt;code&gt;timeit&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;timeit&lt;/span&gt;(&lt;span class="pl-smi"&gt;fn&lt;/span&gt;): 
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; *args and **kwargs are to support positional and named arguments of fn&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;get_time&lt;/span&gt;(&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;args&lt;/span&gt;, &lt;span class="pl-k"&gt;**&lt;/span&gt;&lt;span class="pl-smi"&gt;kwargs&lt;/span&gt;): 
        start &lt;span class="pl-k"&gt;=&lt;/span&gt; time.time() 
        output &lt;span class="pl-k"&gt;=&lt;/span&gt; fn(&lt;span class="pl-k"&gt;*&lt;/span&gt;args, &lt;span class="pl-k"&gt;**&lt;/span&gt;kwargs)
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;Time taken in &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;fn.&lt;span class="pl-c1"&gt;__name__&lt;/span&gt;&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-s"&gt;: &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;time.time() &lt;span class="pl-k"&gt;-&lt;/span&gt; start&lt;span class="pl-k"&gt;:.7f&lt;/span&gt;&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;)
        &lt;span class="pl-k"&gt;return&lt;/span&gt; output  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; make sure that the decorator returns the output of fn&lt;/span&gt;
    &lt;span class="pl-k"&gt;return&lt;/span&gt; get_time &lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Add the decorator &lt;code&gt;@timeit&lt;/code&gt; to your functions.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-en"&gt;@timeit&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;fib&lt;/span&gt;(&lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    &lt;span class="pl-k"&gt;return&lt;/span&gt; fib_helper(n)

&lt;span class="pl-en"&gt;@timeit&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;fib_m&lt;/span&gt;(&lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    &lt;span class="pl-k"&gt;return&lt;/span&gt; fib_m_helper(n, {&lt;span class="pl-c1"&gt;0&lt;/span&gt;: &lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;: &lt;span class="pl-c1"&gt;1&lt;/span&gt;})

fib(&lt;span class="pl-c1"&gt;30&lt;/span&gt;)
fib_m(&lt;span class="pl-c1"&gt;30&lt;/span&gt;)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; Time taken &lt;span class="pl-k"&gt;in&lt;/span&gt; fib: &lt;span class="pl-c1"&gt;0.2787242&lt;/span&gt;
&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; Time taken &lt;span class="pl-k"&gt;in&lt;/span&gt; fib_m: &lt;span class="pl-c1"&gt;0.0000138&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-7-caching-with-functoolslru_cache" class="anchor" aria-hidden="true" href="#7-caching-with-functoolslru_cache"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;7. Caching with @functools.lru_cache&lt;/h2&gt;
&lt;p&gt;Memoization is a form of cache: we cache the previously calculated Fibonacci numbers so that we don't have to calculate them again.&lt;/p&gt;
&lt;p&gt;Caching is such an important technique that Python provides a built-in decorator to give your function the caching capacity. If you want &lt;code&gt;fib_helper&lt;/code&gt; to reuse the previously calculated Fibonacci numbers, you can just add the decorator &lt;code&gt;lru_cache&lt;/code&gt; from &lt;code&gt;functools&lt;/code&gt;. &lt;code&gt;lru&lt;/code&gt; stands for "least recently used". For more information on cache, see &lt;a href="https://docs.python.org/3/library/functools.html" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; functools

&lt;span class="pl-en"&gt;@functools.lru_cache&lt;/span&gt;()
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;fib_helper&lt;/span&gt;(&lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    &lt;span class="pl-k"&gt;if&lt;/span&gt; n &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt;:
        &lt;span class="pl-k"&gt;return&lt;/span&gt; n
    &lt;span class="pl-k"&gt;return&lt;/span&gt; fib_helper(n &lt;span class="pl-k"&gt;-&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;) &lt;span class="pl-k"&gt;+&lt;/span&gt; fib_helper(n &lt;span class="pl-k"&gt;-&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt;)

&lt;span class="pl-en"&gt;@timeit&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;fib&lt;/span&gt;(&lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt; fib is a wrapper function so that later we can change its behavior&lt;/span&gt;
&lt;span class="pl-s"&gt;    at the top level without affecting the behavior at every recursion step.&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;return&lt;/span&gt; fib_helper(n)

fib(&lt;span class="pl-c1"&gt;50&lt;/span&gt;)
fib_m(&lt;span class="pl-c1"&gt;50&lt;/span&gt;)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; Time taken &lt;span class="pl-k"&gt;in&lt;/span&gt; fib: &lt;span class="pl-c1"&gt;0.0000412&lt;/span&gt;
&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; Time taken &lt;span class="pl-k"&gt;in&lt;/span&gt; fib_m: &lt;span class="pl-c1"&gt;0.0000281&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>chiphuyen</author><guid isPermaLink="false">https://github.com/chiphuyen/python-is-cool</guid><pubDate>Fri, 27 Dec 2019 00:04:00 GMT</pubDate></item><item><title>iamtrask/Grokking-Deep-Learning #5 in Jupyter Notebook, Today</title><link>https://github.com/iamtrask/Grokking-Deep-Learning</link><description>&lt;p&gt;&lt;i&gt;this repository accompanies the book "Grokking Deep Learning"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-grokking-deep-learning" class="anchor" aria-hidden="true" href="#grokking-deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grokking-Deep-Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://floydhub.com/run" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2208430861b539afa7c55a34278f45d4aed96d0b/68747470733a2f2f7374617469632e666c6f79646875622e636f6d2f627574746f6e2f627574746f6e2d736d616c6c2e737667" alt="Run on FloydHub" data-canonical-src="https://static.floydhub.com/button/button-small.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository accompanies the book "Grokking Deep Learning", &lt;a href="https://manning.com/books/grokking-deep-learning?a_aid=grokkingdl&amp;amp;a_bid=32715258" title="Grokking Deep Learning" rel="nofollow"&gt;available here&lt;/a&gt;. Also, the coupon code "trask40" is good for a 40% discount.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter3%20-%20%20Forward%20Propagation%20-%20Intro%20to%20Neural%20Prediction.ipynb"&gt;Chapter 3 - Forward Propagation - Intro to Neural Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter4%20-%20Gradient%20Descent%20-%20Intro%20to%20Neural%20Learning.ipynb"&gt;Chapter 4 - Gradient Descent - Into to Neural Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter5%20-%20Generalizing%20Gradient%20Descent%20-%20Learning%20Multiple%20Weights%20at%20a%20Time.ipynb"&gt;Chapter 5 - Generalizing Gradient Descent - Learning Multiple Weights at a Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter6%20-%20Intro%20to%20Backpropagation%20-%20Building%20Your%20First%20DEEP%20Neural%20Network.ipynb"&gt;Chapter 6 - Intro to Backpropagation - Building your first DEEP Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter8%20-%20Intro%20to%20Regularization%20-%20Learning%20Signal%20and%20Ignoring%20Noise.ipynb"&gt;Chapter 8 - Intro to Regularization - Learning Signal and Ignoring Noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter9%20-%20Intro%20to%20Activation%20Functions%20-%20Modeling%20Probabilities.ipynb"&gt;Chapter 9 - Intro to Activation Functions - Learning to Model Probabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter10%20-%20Intro%20to%20Convolutional%20Neural%20Networks%20-%20Learning%20Edges%20and%20Corners.ipynb"&gt;Chapter 10 - Intro to Convolutional Neural Networks - Learning Edges and Corners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter11%20-%20Intro%20to%20Word%20Embeddings%20-%20Neural%20Networks%20that%20Understand%20Language.ipynb"&gt;Chapter 11 - Intro to Word Embeddings - Neural Networks which Understand Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter12%20-%20Intro%20to%20Recurrence%20-%20Predicting%20the%20Next%20Word.ipynb"&gt;Chapter 12 - Intro to Recurrence (RNNs) - Predicting the Next Word&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter13%20-%20Intro%20to%20Automatic%20Differentiation%20-%20Let's%20Build%20A%20Deep%20Learning%20Framework.ipynb"&gt;Chapter 13 - Intro to Automatic Differentiation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter14%20-%20Exploding%20Gradients%20Examples.ipynb"&gt;Chapter 14 - Exploding Gradients Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter14%20-%20Intro%20to%20LSTMs%20-%20Learn%20to%20Write%20Like%20Shakespeare.ipynb"&gt;Chapter 14 - Intro to LSTMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter14%20-%20Intro%20to%20LSTMs%20-%20Part%202%20-%20Learn%20to%20Write%20Like%20Shakespeare.ipynb"&gt;Chapter 14 - Intro to LSTMs - Part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter15%20-%20Intro%20to%20Federated%20Learning%20-%20Deep%20Learning%20on%20Unseen%20Data.ipynb"&gt;Chapter 15 - Intro to Federated Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>iamtrask</author><guid isPermaLink="false">https://github.com/iamtrask/Grokking-Deep-Learning</guid><pubDate>Fri, 27 Dec 2019 00:05:00 GMT</pubDate></item><item><title>ShusenTang/Dive-into-DL-PyTorch #6 in Jupyter Notebook, Today</title><link>https://github.com/ShusenTang/Dive-into-DL-PyTorch</link><description>&lt;p&gt;&lt;i&gt;Êú¨È°πÁõÆÂ∞Ü„ÄäÂä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†„Äã(Dive into Deep Learning)Âéü‰π¶‰∏≠ÁöÑMXNetÂÆûÁé∞Êîπ‰∏∫PyTorchÂÆûÁé∞„ÄÇ&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="docs/README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="img/cover.png"&gt;&lt;img width="500" src="img/cover.png" alt="Â∞ÅÈù¢" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch" rel="nofollow"&gt;Êú¨È°πÁõÆ&lt;/a&gt;Â∞Ü&lt;a href="http://zh.d2l.ai/" rel="nofollow"&gt;„ÄäÂä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†„Äã&lt;/a&gt;¬†Âéü‰π¶‰∏≠MXNet‰ª£Á†ÅÂÆûÁé∞Êîπ‰∏∫PyTorchÂÆûÁé∞„ÄÇÂéü‰π¶‰ΩúËÄÖÔºöÈòøÊñØÈ°ø¬∑Âº†„ÄÅÊùéÊ≤ê„ÄÅÊâéÂç°Èáå C. Á´ãÈ°ø„ÄÅ‰∫öÂéÜÂ±±Â§ß J. ÊñØËé´Êãâ‰ª•ÂèäÂÖ∂‰ªñÁ§æÂå∫Ë¥°ÁåÆËÄÖÔºåGitHubÂú∞ÂùÄÔºö&lt;a href="https://github.com/d2l-ai/d2l-zh"&gt;https://github.com/d2l-ai/d2l-zh&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ê≠§‰π¶ÁöÑ&lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;‰∏≠&lt;/a&gt;&lt;a href="https://d2l.ai/" rel="nofollow"&gt;Ëã±&lt;/a&gt;ÁâàÊú¨Â≠òÂú®‰∏Ä‰∫õ‰∏çÂêåÔºåÈíàÂØπÊ≠§‰π¶Ëã±ÊñáÁâàÁöÑPyTorchÈáçÊûÑÂèØÂèÇËÄÉ&lt;a href="https://github.com/dsgiitr/d2l-pytorch"&gt;Ëøô‰∏™È°πÁõÆ&lt;/a&gt;„ÄÇ
There are some differences between the &lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;Chinese&lt;/a&gt; and &lt;a href="https://d2l.ai/" rel="nofollow"&gt;English&lt;/a&gt; versions of this book. For the PyTorch modifying of the English version, you can refer to &lt;a href="https://github.com/dsgiitr/d2l-pytorch"&gt;this repo&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ÁÆÄ‰ªã" class="anchor" aria-hidden="true" href="#ÁÆÄ‰ªã"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÁÆÄ‰ªã&lt;/h2&gt;
&lt;p&gt;Êú¨‰ªìÂ∫ì‰∏ªË¶ÅÂåÖÂê´codeÂíådocs‰∏§‰∏™Êñá‰ª∂Â§πÔºàÂ§ñÂä†‰∏Ä‰∫õÊï∞ÊçÆÂ≠òÊîæÂú®data‰∏≠Ôºâ„ÄÇÂÖ∂‰∏≠codeÊñá‰ª∂Â§πÂ∞±ÊòØÊØèÁ´†Áõ∏ÂÖ≥jupyter notebook‰ª£Á†ÅÔºàÂü∫‰∫éPyTorchÔºâÔºõdocsÊñá‰ª∂Â§πÂ∞±ÊòØmarkdownÊ†ºÂºèÁöÑ„ÄäÂä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†„Äã‰π¶‰∏≠ÁöÑÁõ∏ÂÖ≥ÂÜÖÂÆπÔºåÁÑ∂ÂêéÂà©Áî®&lt;a href="https://docsify.js.org/#/zh-cn/" rel="nofollow"&gt;docsify&lt;/a&gt;Â∞ÜÁΩëÈ°µÊñáÊ°£ÈÉ®ÁΩ≤Âà∞GitHub Pages‰∏äÔºåÁî±‰∫éÂéü‰π¶‰ΩøÁî®ÁöÑÊòØMXNetÊ°ÜÊû∂ÔºåÊâÄ‰ª•docsÂÜÖÂÆπÂèØËÉΩ‰∏éÂéü‰π¶Áï•Êúâ‰∏çÂêåÔºå‰ΩÜÊòØÊï¥‰ΩìÂÜÖÂÆπÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇÊ¨¢ËøéÂØπÊú¨È°πÁõÆÂÅöÂá∫Ë¥°ÁåÆÊàñÊèêÂá∫issue„ÄÇ&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-Èù¢Âêë‰∫∫Áæ§" class="anchor" aria-hidden="true" href="#Èù¢Âêë‰∫∫Áæ§"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Èù¢Âêë‰∫∫Áæ§&lt;/h2&gt;
&lt;p&gt;Êú¨È°πÁõÆÈù¢ÂêëÂØπÊ∑±Â∫¶Â≠¶‰π†ÊÑüÂÖ¥Ë∂£ÔºåÂ∞§ÂÖ∂ÊòØÊÉ≥‰ΩøÁî®PyTorchËøõË°åÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁ´•Èûã„ÄÇÊú¨È°πÁõÆÂπ∂‰∏çË¶ÅÊ±Ç‰Ω†Êúâ‰ªª‰ΩïÊ∑±Â∫¶Â≠¶‰π†ÊàñËÄÖÊú∫Âô®Â≠¶‰π†ÁöÑËÉåÊôØÁü•ËØÜÔºå‰Ω†Âè™ÈúÄ‰∫ÜËß£Âü∫Á°ÄÁöÑÊï∞Â≠¶ÂíåÁºñÁ®ãÔºåÂ¶ÇÂü∫Á°ÄÁöÑÁ∫øÊÄß‰ª£Êï∞„ÄÅÂæÆÂàÜÂíåÊ¶ÇÁéáÔºå‰ª•ÂèäÂü∫Á°ÄÁöÑPythonÁºñÁ®ã„ÄÇ&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-È£üÁî®ÊñπÊ≥ï" class="anchor" aria-hidden="true" href="#È£üÁî®ÊñπÊ≥ï"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;È£üÁî®ÊñπÊ≥ï&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-ÊñπÊ≥ï‰∏Ä" class="anchor" aria-hidden="true" href="#ÊñπÊ≥ï‰∏Ä"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÊñπÊ≥ï‰∏Ä&lt;/h3&gt;
&lt;p&gt;Êú¨‰ªìÂ∫ìÂåÖÂê´‰∏Ä‰∫õlatexÂÖ¨ÂºèÔºå‰ΩÜgithubÁöÑmarkdownÂéüÁîüÊòØ‰∏çÊîØÊåÅÂÖ¨ÂºèÊòæÁ§∫ÁöÑÔºåËÄådocsÊñá‰ª∂Â§πÂ∑≤ÁªèÂà©Áî®&lt;a href="https://docsify.js.org/#/zh-cn/" rel="nofollow"&gt;docsify&lt;/a&gt;Ë¢´ÈÉ®ÁΩ≤Âà∞‰∫ÜGitHub Pages‰∏äÔºåÊâÄ‰ª•Êü•ÁúãÊñáÊ°£ÊúÄÁÆÄ‰æøÁöÑÊñπÊ≥ïÂ∞±ÊòØÁõ¥Êé•ËÆøÈóÆ&lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch" rel="nofollow"&gt;Êú¨È°πÁõÆÁΩëÈ°µÁâà&lt;/a&gt;„ÄÇÂΩìÁÑ∂Â¶ÇÊûú‰Ω†ËøòÊÉ≥Ë∑ë‰∏Ä‰∏ãËøêË°åÁõ∏ÂÖ≥‰ª£Á†ÅÁöÑËØùËøòÊòØÂæóÊääÊú¨È°πÁõÆclone‰∏ãÊù•ÔºåÁÑ∂ÂêéËøêË°åcodeÊñá‰ª∂Â§π‰∏ãÁõ∏ÂÖ≥‰ª£Á†Å„ÄÇ&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-ÊñπÊ≥ï‰∫å" class="anchor" aria-hidden="true" href="#ÊñπÊ≥ï‰∫å"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÊñπÊ≥ï‰∫å&lt;/h3&gt;
&lt;p&gt;‰Ω†ËøòÂèØ‰ª•Âú®Êú¨Âú∞ËÆøÈóÆÊñáÊ°£ÔºåÂÖàÂÆâË£Ö&lt;code&gt;docsify-cli&lt;/code&gt;Â∑•ÂÖ∑:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;npm i docsify-cli -g&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;ÁÑ∂ÂêéÂ∞ÜÊú¨È°πÁõÆcloneÂà∞Êú¨Âú∞:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/ShusenTang/Dive-into-DL-PyTorch.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; Dive-into-DL-PyTorch&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;ÁÑ∂ÂêéËøêË°å‰∏Ä‰∏™Êú¨Âú∞ÊúçÂä°Âô®ÔºåËøôÊ†∑Â∞±ÂèØ‰ª•ÂæàÊñπ‰æøÁöÑÂú®&lt;code&gt;http://localhost:3000&lt;/code&gt;ÂÆûÊó∂ËÆøÈóÆÊñáÊ°£ÁΩëÈ°µÊ∏≤ÊüìÊïàÊûú„ÄÇ&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docsify serve docs&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-ÁõÆÂΩï" class="anchor" aria-hidden="true" href="#ÁõÆÂΩï"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÁõÆÂΩï&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;ÁÆÄ‰ªã&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="read_guide.md"&gt;ÈòÖËØªÊåáÂçó&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter01_DL-intro/deep-learning-intro.md"&gt;1. Ê∑±Â∫¶Â≠¶‰π†ÁÆÄ‰ªã&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2. È¢ÑÂ§áÁü•ËØÜ
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.1_install.md"&gt;2.1 ÁéØÂ¢ÉÈÖçÁΩÆ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.2_tensor.md"&gt;2.2 Êï∞ÊçÆÊìç‰Ωú&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.3_autograd.md"&gt;2.3 Ëá™Âä®Ê±ÇÊ¢ØÂ∫¶&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3. Ê∑±Â∫¶Â≠¶‰π†Âü∫Á°Ä
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.1_linear-regression.md"&gt;3.1 Á∫øÊÄßÂõûÂΩí&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.2_linear-regression-scratch.md"&gt;3.2 Á∫øÊÄßÂõûÂΩíÁöÑ‰ªéÈõ∂ÂºÄÂßãÂÆûÁé∞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.3_linear-regression-pytorch.md"&gt;3.3 Á∫øÊÄßÂõûÂΩíÁöÑÁÆÄÊ¥ÅÂÆûÁé∞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.4_softmax-regression.md"&gt;3.4 softmaxÂõûÂΩí&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.5_fashion-mnist.md"&gt;3.5 ÂõæÂÉèÂàÜÁ±ªÊï∞ÊçÆÈõÜÔºàFashion-MNISTÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.6_softmax-regression-scratch.md"&gt;3.6 softmaxÂõûÂΩíÁöÑ‰ªéÈõ∂ÂºÄÂßãÂÆûÁé∞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.7_softmax-regression-pytorch.md"&gt;3.7 softmaxÂõûÂΩíÁöÑÁÆÄÊ¥ÅÂÆûÁé∞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.8_mlp.md"&gt;3.8 Â§öÂ±ÇÊÑüÁü•Êú∫&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.9_mlp-scratch.md"&gt;3.9 Â§öÂ±ÇÊÑüÁü•Êú∫ÁöÑ‰ªéÈõ∂ÂºÄÂßãÂÆûÁé∞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.10_mlp-pytorch.md"&gt;3.10 Â§öÂ±ÇÊÑüÁü•Êú∫ÁöÑÁÆÄÊ¥ÅÂÆûÁé∞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.11_underfit-overfit.md"&gt;3.11 Ê®°ÂûãÈÄâÊã©„ÄÅÊ¨†ÊãüÂêàÂíåËøáÊãüÂêà&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.12_weight-decay.md"&gt;3.12 ÊùÉÈáçË°∞Âáè&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.13_dropout.md"&gt;3.13 ‰∏¢ÂºÉÊ≥ï&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.14_backprop.md"&gt;3.14 Ê≠£Âêë‰º†Êí≠„ÄÅÂèçÂêë‰º†Êí≠ÂíåËÆ°ÁÆóÂõæ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.15_numerical-stability-and-init.md"&gt;3.15 Êï∞ÂÄºÁ®≥ÂÆöÊÄßÂíåÊ®°ÂûãÂàùÂßãÂåñ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.16_kaggle-house-price.md"&gt;3.16 ÂÆûÊàòKaggleÊØîËµõÔºöÊàø‰ª∑È¢ÑÊµã&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;4. Ê∑±Â∫¶Â≠¶‰π†ËÆ°ÁÆó
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.1_model-construction.md"&gt;4.1 Ê®°ÂûãÊûÑÈÄ†&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.2_parameters.md"&gt;4.2 Ê®°ÂûãÂèÇÊï∞ÁöÑËÆøÈóÆ„ÄÅÂàùÂßãÂåñÂíåÂÖ±‰∫´&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.3_deferred-init.md"&gt;4.3 Ê®°ÂûãÂèÇÊï∞ÁöÑÂª∂ÂêéÂàùÂßãÂåñ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.4_custom-layer.md"&gt;4.4 Ëá™ÂÆö‰πâÂ±Ç&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.5_read-write.md"&gt;4.5 ËØªÂèñÂíåÂ≠òÂÇ®&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.6_use-gpu.md"&gt;4.6 GPUËÆ°ÁÆó&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;5. Âç∑ÁßØÁ•ûÁªèÁΩëÁªú
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.1_conv-layer.md"&gt;5.1 ‰∫åÁª¥Âç∑ÁßØÂ±Ç&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.2_padding-and-strides.md"&gt;5.2 Â°´ÂÖÖÂíåÊ≠•ÂπÖ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.3_channels.md"&gt;5.3 Â§öËæìÂÖ•ÈÄöÈÅìÂíåÂ§öËæìÂá∫ÈÄöÈÅì&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.4_pooling.md"&gt;5.4 Ê±†ÂåñÂ±Ç&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.5_lenet.md"&gt;5.5 Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàLeNetÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.6_alexnet.md"&gt;5.6 Ê∑±Â∫¶Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàAlexNetÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.7_vgg.md"&gt;5.7 ‰ΩøÁî®ÈáçÂ§çÂÖÉÁ¥†ÁöÑÁΩëÁªúÔºàVGGÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.8_nin.md"&gt;5.8 ÁΩëÁªú‰∏≠ÁöÑÁΩëÁªúÔºàNiNÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.9_googlenet.md"&gt;5.9 Âê´Âπ∂Ë°åËøûÁªìÁöÑÁΩëÁªúÔºàGoogLeNetÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.10_batch-norm.md"&gt;5.10 ÊâπÈáèÂΩí‰∏ÄÂåñ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.11_resnet.md"&gt;5.11 ÊÆãÂ∑ÆÁΩëÁªúÔºàResNetÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.12_densenet.md"&gt;5.12 Á®†ÂØÜËøûÊé•ÁΩëÁªúÔºàDenseNetÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;6. Âæ™ÁéØÁ•ûÁªèÁΩëÁªú
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.1_lang-model.md"&gt;6.1 ËØ≠Ë®ÄÊ®°Âûã&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.2_rnn.md"&gt;6.2 Âæ™ÁéØÁ•ûÁªèÁΩëÁªú&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.3_lang-model-dataset.md"&gt;6.3 ËØ≠Ë®ÄÊ®°ÂûãÊï∞ÊçÆÈõÜÔºàÂë®Êù∞‰º¶‰∏ìËæëÊ≠åËØçÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.4_rnn-scratch.md"&gt;6.4 Âæ™ÁéØÁ•ûÁªèÁΩëÁªúÁöÑ‰ªéÈõ∂ÂºÄÂßãÂÆûÁé∞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.5_rnn-pytorch.md"&gt;6.5 Âæ™ÁéØÁ•ûÁªèÁΩëÁªúÁöÑÁÆÄÊ¥ÅÂÆûÁé∞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.6_bptt.md"&gt;6.6 ÈÄöËøáÊó∂Èó¥ÂèçÂêë‰º†Êí≠&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.7_gru.md"&gt;6.7 Èó®ÊéßÂæ™ÁéØÂçïÂÖÉÔºàGRUÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.8_lstm.md"&gt;6.8 ÈïøÁü≠ÊúüËÆ∞ÂøÜÔºàLSTMÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.9_deep-rnn.md"&gt;6.9 Ê∑±Â∫¶Âæ™ÁéØÁ•ûÁªèÁΩëÁªú&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.10_bi-rnn.md"&gt;6.10 ÂèåÂêëÂæ™ÁéØÁ•ûÁªèÁΩëÁªú&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;7. ‰ºòÂåñÁÆóÊ≥ï
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.1_optimization-intro.md"&gt;7.1 ‰ºòÂåñ‰∏éÊ∑±Â∫¶Â≠¶‰π†&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.2_gd-sgd.md"&gt;7.2 Ê¢ØÂ∫¶‰∏ãÈôçÂíåÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.3_minibatch-sgd.md"&gt;7.3 Â∞èÊâπÈáèÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.4_momentum.md"&gt;7.4 Âä®ÈáèÊ≥ï&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.5_adagrad.md"&gt;7.5 AdaGradÁÆóÊ≥ï&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.6_rmsprop.md"&gt;7.6 RMSPropÁÆóÊ≥ï&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.7_adadelta.md"&gt;7.7 AdaDeltaÁÆóÊ≥ï&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.8_adam.md"&gt;7.8 AdamÁÆóÊ≥ï&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;8. ËÆ°ÁÆóÊÄßËÉΩ
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.1_hybridize.md"&gt;8.1 ÂëΩ‰ª§ÂºèÂíåÁ¨¶Âè∑ÂºèÊ∑∑ÂêàÁºñÁ®ã&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.2_async-computation.md"&gt;8.2 ÂºÇÊ≠•ËÆ°ÁÆó&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.3_auto-parallelism.md"&gt;8.3 Ëá™Âä®Âπ∂Ë°åËÆ°ÁÆó&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.4_multiple-gpus.md"&gt;8.4 Â§öGPUËÆ°ÁÆó&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;9. ËÆ°ÁÆóÊú∫ËßÜËßâ
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.1_image-augmentation.md"&gt;9.1 ÂõæÂÉèÂ¢ûÂπø&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.2_fine-tuning.md"&gt;9.2 ÂæÆË∞É&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.3_bounding-box.md"&gt;9.3 ÁõÆÊ†áÊ£ÄÊµãÂíåËæπÁïåÊ°Ü&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.4_anchor.md"&gt;9.4 ÈîöÊ°Ü&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.5_multiscale-object-detection.md"&gt;9.5 Â§öÂ∞∫Â∫¶ÁõÆÊ†áÊ£ÄÊµã&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.6_object-detection-dataset.md"&gt;9.6 ÁõÆÊ†áÊ£ÄÊµãÊï∞ÊçÆÈõÜÔºàÁöÆÂç°‰∏òÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 9.7 ÂçïÂèëÂ§öÊ°ÜÊ£ÄÊµãÔºàSSDÔºâ&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.8_rcnn.md"&gt;9.8 Âå∫ÂüüÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàR-CNNÔºâÁ≥ªÂàó&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 9.9 ËØ≠‰πâÂàÜÂâ≤ÂíåÊï∞ÊçÆÈõÜ&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 9.10 ÂÖ®Âç∑ÁßØÁΩëÁªúÔºàFCNÔºâ&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 9.11 Ê†∑ÂºèËøÅÁßª&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 9.12 ÂÆûÊàòKaggleÊØîËµõÔºöÂõæÂÉèÂàÜÁ±ªÔºàCIFAR-10Ôºâ&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 9.13 ÂÆûÊàòKaggleÊØîËµõÔºöÁãóÁöÑÂìÅÁßçËØÜÂà´ÔºàImageNet DogsÔºâ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;10. Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.1_word2vec.md"&gt;10.1 ËØçÂµåÂÖ•Ôºàword2vecÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.2_approx-training.md"&gt;10.2 Ëøë‰ººËÆ≠ÁªÉ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.3_word2vec-pytorch.md"&gt;10.3 word2vecÁöÑÂÆûÁé∞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.4_fasttext.md"&gt;10.4 Â≠êËØçÂµåÂÖ•ÔºàfastTextÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.5_glove.md"&gt;10.5 ÂÖ®Â±ÄÂêëÈáèÁöÑËØçÂµåÂÖ•ÔºàGloVeÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.6_similarity-analogy.md"&gt;10.6 Ê±ÇËøë‰πâËØçÂíåÁ±ªÊØîËØç&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.7_sentiment-analysis-rnn.md"&gt;10.7 ÊñáÊú¨ÊÉÖÊÑüÂàÜÁ±ªÔºö‰ΩøÁî®Âæ™ÁéØÁ•ûÁªèÁΩëÁªú&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.8_sentiment-analysis-cnn.md"&gt;10.8 ÊñáÊú¨ÊÉÖÊÑüÂàÜÁ±ªÔºö‰ΩøÁî®Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàtextCNNÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.9_seq2seq.md"&gt;10.9 ÁºñÁ†ÅÂô®‚ÄîËß£Á†ÅÂô®Ôºàseq2seqÔºâ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.10_beam-search.md"&gt;10.10 ÊùüÊêúÁ¥¢&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.11_attention.md"&gt;10.11 Ê≥®ÊÑèÂäõÊú∫Âà∂&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.12_machine-translation.md"&gt;10.12 Êú∫Âô®ÁøªËØë&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ÊåÅÁª≠Êõ¥Êñ∞‰∏≠......&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-Âéü‰π¶Âú∞ÂùÄ" class="anchor" aria-hidden="true" href="#Âéü‰π¶Âú∞ÂùÄ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Âéü‰π¶Âú∞ÂùÄ&lt;/h2&gt;
&lt;p&gt;‰∏≠ÊñáÁâàÔºö&lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;Âä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†&lt;/a&gt; | &lt;a href="https://github.com/d2l-ai/d2l-zh"&gt;Github‰ªìÂ∫ì&lt;/a&gt;&lt;br&gt;
English Version: &lt;a href="https://d2l.ai/" rel="nofollow"&gt;Dive into Deep Learning&lt;/a&gt; | &lt;a href="https://github.com/d2l-ai/d2l-en"&gt;Github Repo&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ÂºïÁî®" class="anchor" aria-hidden="true" href="#ÂºïÁî®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÂºïÁî®&lt;/h2&gt;
&lt;p&gt;Â¶ÇÊûúÊÇ®Âú®Á†îÁ©∂‰∏≠‰ΩøÁî®‰∫ÜËøô‰∏™È°πÁõÆËØ∑ÂºïÁî®Âéü‰π¶:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@book{zhang2019dive,
    title={Dive into Deep Learning},
    author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
    note={\url{http://www.d2l.ai}},
    year={2020}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ShusenTang</author><guid isPermaLink="false">https://github.com/ShusenTang/Dive-into-DL-PyTorch</guid><pubDate>Fri, 27 Dec 2019 00:06:00 GMT</pubDate></item><item><title>fastai/fastai #7 in Jupyter Notebook, Today</title><link>https://github.com/fastai/fastai</link><description>&lt;p&gt;&lt;i&gt;The fastai deep learning library, plus lessons and tutorials&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://dev.azure.com/fastdotai/fastai/_build/latest?definitionId=1" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a1b1234cce0c844f75224d1df07b4f236f78aee7/68747470733a2f2f6465762e617a7572652e636f6d2f66617374646f7461692f6661737461692f5f617069732f6275696c642f7374617475732f6661737461692e666173746169" alt="Build Status" data-canonical-src="https://dev.azure.com/fastdotai/fastai/_apis/build/status/fastai.fastai" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/26f7b20369ea7a096cfb30bdf0d14bc6ceda0275/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6661737461692e737667" alt="pypi fastai version" data-canonical-src="https://img.shields.io/pypi/v/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://anaconda.org/fastai/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c20a6e61cb1c612b644253db1e3c1f7972d7f0e/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f762f6661737461692f6661737461692e737667" alt="Conda fastai version" data-canonical-src="https://img.shields.io/conda/v/fastai/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://anaconda.org/fastai/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/39f258e6563b60eef6a88589cd194d3a85033747/68747470733a2f2f616e61636f6e64612e6f72672f6661737461692f6661737461692f6261646765732f706c6174666f726d732e737667" alt="Anaconda-Server Badge" data-canonical-src="https://anaconda.org/fastai/fastai/badges/platforms.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6dc643192dbbbd8edda826d1be289ba06ef2b57f/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6661737461692e737667" alt="fastai python compatibility" data-canonical-src="https://img.shields.io/pypi/pyversions/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f600abfa75b593d49643c1710ed42865373f9d75/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f6661737461692e737667" alt="fastai license" data-canonical-src="https://img.shields.io/pypi/l/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-fastai" class="anchor" aria-hidden="true" href="#fastai"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;fastai&lt;/h1&gt;
&lt;p&gt;The fastai library simplifies training fast and accurate neural nets using modern best practices. See the &lt;a href="https://docs.fast.ai" rel="nofollow"&gt;fastai website&lt;/a&gt; to get started. The library is based on research into deep learning best practices undertaken at &lt;a href="http://www.fast.ai" rel="nofollow"&gt;fast.ai&lt;/a&gt;, and includes "out of the box" support for &lt;a href="https://docs.fast.ai/vision.html#vision" rel="nofollow"&gt;&lt;code&gt;vision&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://docs.fast.ai/text.html#text" rel="nofollow"&gt;&lt;code&gt;text&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://docs.fast.ai/tabular.html#tabular" rel="nofollow"&gt;&lt;code&gt;tabular&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://docs.fast.ai/collab.html#collab" rel="nofollow"&gt;&lt;code&gt;collab&lt;/code&gt;&lt;/a&gt; (collaborative filtering) models. For brief examples, see the &lt;a href="https://github.com/fastai/fastai/tree/master/examples"&gt;examples&lt;/a&gt; folder; detailed examples are provided in the full &lt;a href="https://docs.fast.ai/" rel="nofollow"&gt;documentation&lt;/a&gt;. For instance, here's how to train an MNIST model using &lt;a href="https://arxiv.org/abs/1512.03385" rel="nofollow"&gt;resnet18&lt;/a&gt; (from the &lt;a href="https://github.com/fastai/fastai/blob/master/examples/vision.ipynb"&gt;vision example&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; fastai.vision &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;
path &lt;span class="pl-k"&gt;=&lt;/span&gt; untar_data(&lt;span class="pl-c1"&gt;MNIST_PATH&lt;/span&gt;)
data &lt;span class="pl-k"&gt;=&lt;/span&gt; image_data_from_folder(path)
learn &lt;span class="pl-k"&gt;=&lt;/span&gt; cnn_learner(data, models.resnet18, &lt;span class="pl-v"&gt;metrics&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;accuracy)
learn.fit(&lt;span class="pl-c1"&gt;1&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-note-for-coursefastai-students" class="anchor" aria-hidden="true" href="#note-for-coursefastai-students"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note for &lt;a href="http://course.fast.ai" rel="nofollow"&gt;course.fast.ai&lt;/a&gt; students&lt;/h2&gt;
&lt;p&gt;This document is written for &lt;code&gt;fastai v1&lt;/code&gt;, which we use for the current version the &lt;a href="http://course.fast.ai" rel="nofollow"&gt;course.fast.ai&lt;/a&gt; deep learning courses. If you're following along with a course at &lt;a href="http://course18.fast.ai" rel="nofollow"&gt;course18.fast.ai&lt;/a&gt; (i.e. the machine learning course, which isn't updated for v1) you need to use &lt;code&gt;fastai 0.7&lt;/code&gt;;  please follow the installation instructions &lt;a href="https://forums.fast.ai/t/fastai-v0-install-issues-thread/24652" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; &lt;em&gt;fastai v1 currently supports Linux only, and requires &lt;strong&gt;PyTorch v1&lt;/strong&gt; and &lt;strong&gt;Python 3.6&lt;/strong&gt; or later. Windows support is at an experimental stage: it should work fine but it's much slower and less well tested. Since Macs don't currently have good Nvidia GPU support, we do not currently prioritize Mac development.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;fastai-1.x&lt;/code&gt; can be installed with either &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;pip&lt;/code&gt; package managers and also from source. At the moment you can't just run &lt;em&gt;install&lt;/em&gt;, since you first need to get the correct &lt;code&gt;pytorch&lt;/code&gt; version installed - thus to get &lt;code&gt;fastai-1.x&lt;/code&gt; installed choose one of the installation recipes below using your favorite python package manager. Note that &lt;strong&gt;PyTorch v1&lt;/strong&gt; and &lt;strong&gt;Python 3.6&lt;/strong&gt; are the minimal version requirements.&lt;/p&gt;
&lt;p&gt;It's highly recommended you install &lt;code&gt;fastai&lt;/code&gt; and its dependencies in a virtual environment (&lt;a href="https://conda.io/docs/user-guide/tasks/manage-environments.html" rel="nofollow"&gt;&lt;code&gt;conda&lt;/code&gt;&lt;/a&gt; or others), so that you don't interfere with system-wide python packages. It's not that you must, but if you experience problems with any dependency packages, please consider using a fresh virtual environment just for &lt;code&gt;fastai&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Starting with pytorch-1.x you no longer need to install a special pytorch-cpu version. Instead use the normal pytorch and it works with and without GPU. But &lt;a href="https://docs.fast.ai/install.html#cpu-build" rel="nofollow"&gt;you can install the cpu build too&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you experience installation problems, please read about &lt;a href="https://github.com/fastai/fastai/blob/master/README.md#installation-issues"&gt;installation issues&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are planning on using &lt;code&gt;fastai&lt;/code&gt; in the jupyter notebook environment, make sure to also install the corresponding &lt;a href="https://docs.fast.ai/install.html#jupyter-notebook-dependencies" rel="nofollow"&gt;packages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;More advanced installation issues, such as installing only partial dependencies are covered in a dedicated &lt;a href="https://docs.fast.ai/install.html" rel="nofollow"&gt;installation doc&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-conda-install" class="anchor" aria-hidden="true" href="#conda-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conda Install&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c pytorch -c fastai fastai&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will install the &lt;code&gt;pytorch&lt;/code&gt; build with the latest &lt;code&gt;cudatoolkit&lt;/code&gt; version. If you need a higher or lower &lt;code&gt;CUDA XX&lt;/code&gt; build (e.g. CUDA 9.0), following the instructions &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;here&lt;/a&gt;, to install the desired &lt;code&gt;pytorch&lt;/code&gt; build.&lt;/p&gt;
&lt;p&gt;Note that JPEG decoding can be a bottleneck, particularly if you have a fast GPU. You can optionally install an optimized JPEG decoder as follows (Linux):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda uninstall --force jpeg libtiff -y
conda install -c conda-forge libjpeg-turbo pillow==6.0.0
CC=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;cc -mavx2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; pip install --no-cache-dir -U --force-reinstall --no-binary :all: --compile pillow-simd&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you only care about faster JPEG decompression, it can be &lt;code&gt;pillow&lt;/code&gt; or &lt;code&gt;pillow-simd&lt;/code&gt; in the last command above, the latter speeds up other image processing operations. For the full story see &lt;a href="https://docs.fast.ai/performance.html#faster-image-processing" rel="nofollow"&gt;Pillow-SIMD&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pypi-install" class="anchor" aria-hidden="true" href="#pypi-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyPI Install&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install fastai&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By default pip will install the latest &lt;code&gt;pytorch&lt;/code&gt; with the latest &lt;code&gt;cudatoolkit&lt;/code&gt;. If your hardware doesn't support the latest &lt;code&gt;cudatoolkit&lt;/code&gt;, follow the instructions &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;here&lt;/a&gt;, to install a &lt;code&gt;pytorch&lt;/code&gt; build that fits your hardware.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-bug-fix-install" class="anchor" aria-hidden="true" href="#bug-fix-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bug Fix Install&lt;/h3&gt;
&lt;p&gt;If a bug fix was made in git and you can't wait till a new release is made, you can install the bleeding edge version of &lt;code&gt;fastai&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/fastai/fastai.git
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-developer-install" class="anchor" aria-hidden="true" href="#developer-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Developer Install&lt;/h3&gt;
&lt;p&gt;The following instructions will result in a &lt;a href="https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs" rel="nofollow"&gt;pip editable install&lt;/a&gt;, so that you can &lt;code&gt;git pull&lt;/code&gt; at any time and your environment will automatically get the updates:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/fastai/fastai
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; fastai
tools/run-after-git-clone
pip install -e &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;.[dev]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, you can test that the build works by starting the jupyter notebook:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;jupyter notebook&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and executing an example notebook. For example load &lt;code&gt;examples/tabular.ipynb&lt;/code&gt; and run it.&lt;/p&gt;
&lt;p&gt;Please refer to &lt;a href="https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; and &lt;a href="https://docs.fast.ai/dev/develop.html" rel="nofollow"&gt;Notes For Developers&lt;/a&gt; for more details on how to contribute to the &lt;code&gt;fastai&lt;/code&gt; project.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-building-from-source" class="anchor" aria-hidden="true" href="#building-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building From Source&lt;/h3&gt;
&lt;p&gt;If for any reason you can't use the prepackaged packages and have to build from source, this section is for you.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To build &lt;code&gt;pytorch&lt;/code&gt; from source follow the &lt;a href="https://github.com/pytorch/pytorch#from-source"&gt;complete instructions&lt;/a&gt;. Remember to first install CUDA, CuDNN, and other required libraries as suggested - everything will be very slow without those libraries built into &lt;code&gt;pytorch&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, you will also need to build &lt;code&gt;torchvision&lt;/code&gt; from source:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/pytorch/vision
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; vision
python setup.py install&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When both &lt;code&gt;pytorch&lt;/code&gt; and &lt;code&gt;torchvision&lt;/code&gt; are installed, first test that you can load each of these libraries:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;import torch
import torchvision&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;to validate that they were installed correctly&lt;/p&gt;
&lt;p&gt;Finally, proceed with &lt;code&gt;fastai&lt;/code&gt; installation as normal, either through prepackaged pip or conda builds or installing from source ("the developer install") as explained in the sections above.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-installation-issues" class="anchor" aria-hidden="true" href="#installation-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation Issues&lt;/h2&gt;
&lt;p&gt;If the installation process fails, first make sure &lt;a href="https://github.com/fastai/fastai/blob/master/README.md#is-my-system-supported"&gt;your system is supported&lt;/a&gt;. And if the problem is still not addressed, please refer to the &lt;a href="https://docs.fast.ai/troubleshoot.html" rel="nofollow"&gt;troubleshooting document&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you encounter installation problems with conda, make sure you have the latest &lt;code&gt;conda&lt;/code&gt; client (&lt;code&gt;conda install&lt;/code&gt; will do an update too):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install conda&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-is-my-system-supported" class="anchor" aria-hidden="true" href="#is-my-system-supported"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is My System Supported?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Python: You need to have python 3.6 or higher&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CPU or GPU&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;pytorch&lt;/code&gt; binary package comes with its own CUDA, CuDNN, NCCL, MKL, and other libraries so you don't have to install system-wide NVIDIA's CUDA and related libraries if you don't need them for something else. If you have them installed already it doesn't matter which NVIDIA's CUDA version library you have installed system-wide. Your system could have CUDA 9.0 libraries, and you can still use &lt;code&gt;pytorch&lt;/code&gt; build with CUDA 10.0 libraries without any problem, since the &lt;code&gt;pytorch&lt;/code&gt; binary package is self-contained.&lt;/p&gt;
&lt;p&gt;The only requirement is that you have installed and configured the NVIDIA driver correctly. Usually you can test that by running &lt;code&gt;nvidia-smi&lt;/code&gt;. While it's possible that this application is not available on your system, it's very likely that if it doesn't work, then you don't have your NVIDIA drivers configured properly. And remember that a reboot is always required after installing NVIDIA drivers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Operating System:&lt;/p&gt;
&lt;p&gt;Since fastai-1.0 relies on pytorch-1.0, you need to be able to install pytorch-1.0 first.&lt;/p&gt;
&lt;p&gt;As of this moment pytorch.org's 1.0 version supports:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Platform&lt;/th&gt;
&lt;th&gt;GPU&lt;/th&gt;
&lt;th&gt;CPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;linux&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mac&lt;/td&gt;
&lt;td&gt;source&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;windows&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Legend: &lt;code&gt;binary&lt;/code&gt; = can be installed directly, &lt;code&gt;source&lt;/code&gt; = needs to be built from source.&lt;/p&gt;
&lt;p&gt;If there is no &lt;code&gt;pytorch&lt;/code&gt; preview conda or pip package available for your system, you may still be able to &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;build it from source&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How do you know which pytorch cuda version build to choose?&lt;/p&gt;
&lt;p&gt;It depends on the version of the installed NVIDIA driver. Here are the requirements for CUDA versions supported by pre-built &lt;code&gt;pytorch&lt;/code&gt; releases:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CUDA Toolkit&lt;/th&gt;
&lt;th&gt;NVIDIA (Linux x86_64)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CUDA 10.0&lt;/td&gt;
&lt;td&gt;&amp;gt;= 410.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CUDA 9.0&lt;/td&gt;
&lt;td&gt;&amp;gt;= 384.81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CUDA 8.0&lt;/td&gt;
&lt;td&gt;&amp;gt;= 367.48&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So if your NVIDIA driver is less than 384, then you can only use CUDA 8.0. Of course, you can upgrade your drivers to more recent ones if your card supports it.&lt;/p&gt;
&lt;p&gt;You can find a complete table with all variations &lt;a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you use NVIDIA driver 410+, you most likely want to install the &lt;code&gt;cudatoolkit=10.0&lt;/code&gt; pytorch variant, via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c pytorch pytorch cudatoolkit=10.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or if you need a lower version, use one of:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c pytorch pytorch cudatoolkit=8.0
conda install -c pytorch pytorch cudatoolkit=9.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For other options refer to the complete list of &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;the available pytorch variants&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;In order to update your environment, simply install &lt;code&gt;fastai&lt;/code&gt; in exactly the same way you did the initial installation.&lt;/p&gt;
&lt;p&gt;Top level files &lt;code&gt;environment.yml&lt;/code&gt; and &lt;code&gt;environment-cpu.yml&lt;/code&gt; belong to the old fastai (0.7). &lt;code&gt;conda env update&lt;/code&gt; is no longer the way to update your &lt;code&gt;fastai-1.x&lt;/code&gt; environment. These files remain because the fastai course-v2 video instructions rely on this setup. Eventually, once fastai course-v3 p1 and p2 will be completed, they will probably be moved to where they belong - under &lt;code&gt;old/&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-guidelines" class="anchor" aria-hidden="true" href="#contribution-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution guidelines&lt;/h2&gt;
&lt;p&gt;If you want to contribute to &lt;code&gt;fastai&lt;/code&gt;, be sure to review the &lt;a href="https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;. This project adheres to fastai's &lt;a href="https://github.com/fastai/fastai/blob/master/CODE-OF-CONDUCT.md"&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code.&lt;/p&gt;
&lt;p&gt;We use GitHub issues for tracking requests and bugs, so please see &lt;a href="https://forums.fast.ai/" rel="nofollow"&gt;fastai forum&lt;/a&gt; for general questions and discussion.&lt;/p&gt;
&lt;p&gt;The fastai project strives to abide by generally accepted best practices in open-source software development:&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-history" class="anchor" aria-hidden="true" href="#history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;History&lt;/h2&gt;
&lt;p&gt;A detailed history of changes can be found &lt;a href="https://github.com/fastai/fastai/blob/master/CHANGES.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-copyright" class="anchor" aria-hidden="true" href="#copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Copyright&lt;/h2&gt;
&lt;p&gt;Copyright 2017 onwards, fast.ai, Inc. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this project's files except in compliance with the License. A copy of the License is provided in the LICENSE file in this repository.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fastai</author><guid isPermaLink="false">https://github.com/fastai/fastai</guid><pubDate>Fri, 27 Dec 2019 00:07:00 GMT</pubDate></item><item><title>udacity/deep-learning-v2-pytorch #8 in Jupyter Notebook, Today</title><link>https://github.com/udacity/deep-learning-v2-pytorch</link><description>&lt;p&gt;&lt;i&gt;Projects and exercises for the latest Deep Learning ND program https://www.udacity.com/course/deep-learning-nanodegree--nd101&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-pytorch" class="anchor" aria-hidden="true" href="#deep-learning-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning (PyTorch)&lt;/h1&gt;
&lt;p&gt;This repository contains material related to Udacity's &lt;a href="https://www.udacity.com/course/deep-learning-nanodegree--nd101" rel="nofollow"&gt;Deep Learning Nanodegree program&lt;/a&gt;. It consists of a bunch of tutorial notebooks for various deep learning topics. In most cases, the notebooks lead you through implementing models such as convolutional networks, recurrent networks, and GANs. There are other topics covered such as weight initialization and batch normalization.&lt;/p&gt;
&lt;p&gt;There are also notebooks used as projects for the Nanodegree program. In the program itself, the projects are reviewed by real people (Udacity reviewers), but the starting code is available here, as well.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table Of Contents&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-neural-networks" class="anchor" aria-hidden="true" href="#introduction-to-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-neural-networks"&gt;Introduction to Neural Networks&lt;/a&gt;: Learn how to implement gradient descent and apply it to predicting patterns in student admissions data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-analysis-network"&gt;Sentiment Analysis with NumPy&lt;/a&gt;: &lt;a href="http://iamtrask.github.io/" rel="nofollow"&gt;Andrew Trask&lt;/a&gt; leads you through building a sentiment analysis model, predicting if some text is positive or negative.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch"&gt;Introduction to PyTorch&lt;/a&gt;: Learn how to build neural networks in PyTorch and use pre-trained networks for state-of-the-art image classifiers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/convolutional-neural-networks"&gt;Convolutional Neural Networks&lt;/a&gt;: Visualize the output of layers that make up a CNN. Learn how to define and train a CNN for classifying &lt;a href="https://en.wikipedia.org/wiki/MNIST_database" rel="nofollow"&gt;MNIST data&lt;/a&gt;, a handwritten digit database that is notorious in the fields of machine and deep learning. Also, define and train a CNN for classifying images in the &lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow"&gt;CIFAR10 dataset&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/transfer-learning"&gt;Transfer Learning&lt;/a&gt;. In practice, most people don't train their own networks on huge datasets; they use &lt;strong&gt;pre-trained&lt;/strong&gt; networks such as VGGnet. Here you'll use VGGnet to help classify images of flowers without training an end-to-end network from scratch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/weight-initialization"&gt;Weight Initialization&lt;/a&gt;: Explore how initializing network weights affects performance.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/autoencoder"&gt;Autoencoders&lt;/a&gt;: Build models for image compression and de-noising, using feedforward and convolutional networks in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/style-transfer"&gt;Style Transfer&lt;/a&gt;: Extract style and content features from images, using a pre-trained network. Implement style transfer according to the paper, &lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="nofollow"&gt;Image Style Transfer Using Convolutional Neural Networks&lt;/a&gt; by Gatys et. al. Define appropriate losses for iteratively creating a target, style-transferred image of your own design!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-recurrent-neural-networks" class="anchor" aria-hidden="true" href="#recurrent-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/recurrent-neural-networks"&gt;Intro to Recurrent Networks (Time series &amp;amp; Character-level RNN)&lt;/a&gt;: Recurrent neural networks are able to use information about the sequence of data, such as the sequence of characters in text; learn how to implement these in PyTorch for a variety of tasks.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/word2vec-embeddings"&gt;Embeddings (Word2Vec)&lt;/a&gt;: Implement the Word2Vec model to find semantic representations of words for use in natural language processing.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-rnn"&gt;Sentiment Analysis RNN&lt;/a&gt;: Implement a recurrent neural network that can predict if the text of a moview review is positive or negative.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/attention"&gt;Attention&lt;/a&gt;: Implement attention and apply it to annotation vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-generative-adversarial-networks" class="anchor" aria-hidden="true" href="#generative-adversarial-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/gan-mnist"&gt;Generative Adversarial Network on MNIST&lt;/a&gt;: Train a simple generative adversarial network on the MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/batch-norm"&gt;Batch Normalization&lt;/a&gt;: Learn how to improve training rates and network stability with batch normalizations.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/dcgan-svhn"&gt;Deep Convolutional GAN (DCGAN)&lt;/a&gt;: Implement a DCGAN to generate new images based on the Street View House Numbers (SVHN) dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/cycle-gan"&gt;CycleGAN&lt;/a&gt;: Implement a CycleGAN that is designed to learn from unpaired and unlabeled data; use trained generators to transform images from summer to winter and vice versa.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deploying-a-model-with-aws-sagemaker" class="anchor" aria-hidden="true" href="#deploying-a-model-with-aws-sagemaker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deploying a Model (with AWS SageMaker)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/sagemaker-deployment"&gt;All exercise and project notebooks&lt;/a&gt; for the lessons on model deployment can be found in the linked, Github repo. Learn to deploy pre-trained models using AWS SageMaker.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-projects" class="anchor" aria-hidden="true" href="#projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-bikesharing"&gt;Predicting Bike-Sharing Patterns&lt;/a&gt;: Implement a neural network in NumPy to predict bike rentals.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-dog-classification"&gt;Dog Breed Classifier&lt;/a&gt;: Build a convolutional neural network with PyTorch to classify any image (even an image of a face) as a specific dog breed.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-tv-script-generation"&gt;TV Script Generation&lt;/a&gt;: Train a recurrent neural network to generate scripts in the style of dialogue from Seinfeld.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-face-generation"&gt;Face Generation&lt;/a&gt;: Use a DCGAN on the CelebA dataset to generate images of new and realistic human faces.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-elective-material" class="anchor" aria-hidden="true" href="#elective-material"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Elective Material&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/tensorflow/intro-to-tensorflow"&gt;Intro to TensorFlow&lt;/a&gt;: Starting building neural networks with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/keras"&gt;Keras&lt;/a&gt;: Learn to build neural networks and convolutional neural networks with Keras.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-configure-and-manage-your-environment-with-anaconda" class="anchor" aria-hidden="true" href="#configure-and-manage-your-environment-with-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configure and Manage Your Environment with Anaconda&lt;/h2&gt;
&lt;p&gt;Per the Anaconda &lt;a href="http://conda.pydata.org/docs" rel="nofollow"&gt;docs&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Conda is an open source package management system and environment management system
for installing multiple versions of software packages and their dependencies and
switching easily between them. It works on Linux, OS X and Windows, and was created
for Python programs but can package and distribute any software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h2&gt;
&lt;p&gt;Using Anaconda consists of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt; on your computer, by selecting the latest Python version for your operating system. If you already have &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;miniconda&lt;/code&gt; installed, you should be able to skip this step and move on to step 2.&lt;/li&gt;
&lt;li&gt;Create and activate * a new &lt;code&gt;conda&lt;/code&gt; &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;environment&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;* Each time you wish to work on any exercises, activate your &lt;code&gt;conda&lt;/code&gt; environment!&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-1-installation" class="anchor" aria-hidden="true" href="#1-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; the latest version of &lt;code&gt;miniconda&lt;/code&gt; that matches your system.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Linux&lt;/th&gt;
&lt;th&gt;Mac&lt;/th&gt;
&lt;th&gt;Windows&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;64-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86_64.exe" rel="nofollow"&gt;64-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;32-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86.sh" rel="nofollow"&gt;32-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86.exe" rel="nofollow"&gt;32-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt; &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;miniconda&lt;/a&gt; on your machine. Detailed instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-2-create-and-activate-the-environment" class="anchor" aria-hidden="true" href="#2-create-and-activate-the-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Create and Activate the Environment&lt;/h2&gt;
&lt;p&gt;For Windows users, these following commands need to be executed from the &lt;strong&gt;Anaconda prompt&lt;/strong&gt; as opposed to a Windows terminal window. For Mac, a normal terminal window will work.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-git-and-version-control" class="anchor" aria-hidden="true" href="#git-and-version-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Git and version control&lt;/h4&gt;
&lt;p&gt;These instructions also assume you have &lt;code&gt;git&lt;/code&gt; installed for working with Github from a terminal window, but if you do not, you can download that first with the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you'd like to learn more about version control and using &lt;code&gt;git&lt;/code&gt; from the command line, take a look at our &lt;a href="https://www.udacity.com/course/version-control-with-git--ud123" rel="nofollow"&gt;free course: Version Control with Git&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now, we're ready to create our local environment!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clone the repository, and navigate to the downloaded folder. This may take a minute or two to clone due to the included image data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/udacity/deep-learning-v2-pytorch.git
cd deep-learning-v2-pytorch
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;
&lt;p&gt;Create (and activate) a new environment, named &lt;code&gt;deep-learning&lt;/code&gt; with Python 3.6. If prompted to proceed with the install &lt;code&gt;(Proceed [y]/n)&lt;/code&gt; type y.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create -n deep-learning python=3.6
source activate deep-learning
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create --name deep-learning python=3.6
activate deep-learning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point your command line should look something like: &lt;code&gt;(deep-learning) &amp;lt;User&amp;gt;:deep-learning-v2-pytorch &amp;lt;user&amp;gt;$&lt;/code&gt;. The &lt;code&gt;(deep-learning)&lt;/code&gt; indicates that your environment has been activated, and you can proceed with further package installations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install PyTorch and torchvision; this should install the latest version of PyTorch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision -c pytorch 
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch -c pytorch
pip install torchvision
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install a few required pip packages, which are specified in the requirements text file (including OpenCV).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="7"&gt;
&lt;li&gt;That's it!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now most of the &lt;code&gt;deep-learning&lt;/code&gt; libraries are available to you. Very occasionally, you will see a repository with an addition requirements file, which exists should you want to use TensorFlow and Keras, for example. In this case, you're encouraged to install another library to your existing environment, or create a new environment for a specific project.&lt;/p&gt;
&lt;p&gt;Now, assuming your &lt;code&gt;deep-learning&lt;/code&gt; environment is still activated, you can navigate to the main repo and start looking at the notebooks:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd
cd deep-learning-v2-pytorch
jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To exit the environment when you have completed your work session, simply close the terminal window.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>udacity</author><guid isPermaLink="false">https://github.com/udacity/deep-learning-v2-pytorch</guid><pubDate>Fri, 27 Dec 2019 00:08:00 GMT</pubDate></item><item><title>ageron/handson-ml #9 in Jupyter Notebook, Today</title><link>https://github.com/ageron/handson-ml</link><description>&lt;p&gt;&lt;i&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in python using Scikit-Learn and TensorFlow.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-notebooks" class="anchor" aria-hidden="true" href="#machine-learning-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Notebooks&lt;/h1&gt;
&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in
python. It contains the example code and solutions to the exercises in my O'Reilly book &lt;a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/" rel="nofollow"&gt;Hands-on Machine Learning with Scikit-Learn and TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8e10a44b0ddbb9530cc27d877f06db68d9fa1c7d/687474703a2f2f616b616d6169636f766572732e6f7265696c6c792e636f6d2f696d616765732f393738313439313936323238322f6361742e676966" alt="book" data-canonical-src="http://akamaicovers.oreilly.com/images/9781491962282/cat.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simply open the &lt;a href="http://jupyter.org/" rel="nofollow"&gt;Jupyter&lt;/a&gt; notebooks you are interested in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using &lt;a href="http://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/index.ipynb" rel="nofollow"&gt;jupyter.org's notebook viewer&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;note: &lt;a href="https://github.com/ageron/handson-ml/blob/master/index.ipynb"&gt;github.com's notebook viewer&lt;/a&gt; also works but it is slower and the math formulas are not displayed correctly,&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;by cloning this repository and running Jupyter locally. This option lets you play around with the code. In this case, follow the installation instructions below,&lt;/li&gt;
&lt;li&gt;or by running the notebooks in &lt;a href="https://beta.deepnote.com" rel="nofollow"&gt;Deepnote&lt;/a&gt;. This allows you to play around with the code online in your browser. For example, here's a link to the first chapter: &lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb" rel="nofollow"&gt;&lt;img height="22" src="https://camo.githubusercontent.com/c3b9bd12a99f8de3301018192105256209bcf800/68747470733a2f2f626574612e646565706e6f74652e636f6d2f627574746f6e732f6c61756e63682d696e2d646565706e6f74652e737667" data-canonical-src="https://beta.deepnote.com/buttons/launch-in-deepnote.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;p&gt;First, you will need to install &lt;a href="https://git-scm.com/" rel="nofollow"&gt;git&lt;/a&gt;, if you don't have it already.&lt;/p&gt;
&lt;p&gt;Next, clone this repository by opening a terminal and typing the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd $HOME  # or any other development directory you prefer
$ git clone https://github.com/ageron/handson-ml.git
$ cd handson-ml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do not want to install git, you can instead download &lt;a href="https://github.com/ageron/handson-ml/archive/master.zip"&gt;master.zip&lt;/a&gt;, unzip it, rename the resulting directory to &lt;code&gt;handson-ml&lt;/code&gt; and move it to your development directory.&lt;/p&gt;
&lt;p&gt;If you want to go through chapter 16 on Reinforcement Learning, you will need to &lt;a href="https://gym.openai.com/docs" rel="nofollow"&gt;install OpenAI gym&lt;/a&gt; and its dependencies for Atari simulations.&lt;/p&gt;
&lt;p&gt;If you are familiar with Python and you know how to install Python libraries, go ahead and install the libraries listed in &lt;code&gt;requirements.txt&lt;/code&gt; and jump to the &lt;a href="#starting-jupyter"&gt;Starting Jupyter&lt;/a&gt; section. If you need detailed instructions, please read on.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-python--required-libraries" class="anchor" aria-hidden="true" href="#python--required-libraries"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python &amp;amp; Required Libraries&lt;/h2&gt;
&lt;p&gt;Of course, you obviously need Python. Python 3 is already preinstalled on many systems nowadays. You can check which version you have by typing the following command (you may need to replace &lt;code&gt;python3&lt;/code&gt; with &lt;code&gt;python&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 --version  # for Python 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Any Python 3 version should be fine, preferably 3.5 or above. If you don't have Python 3, I recommend installing it. To do so, you have several options: on Windows or MacOSX, you can just download it from &lt;a href="https://www.python.org/downloads/" rel="nofollow"&gt;python.org&lt;/a&gt;. On MacOSX, you can alternatively use &lt;a href="https://www.macports.org/" rel="nofollow"&gt;MacPorts&lt;/a&gt; or &lt;a href="https://brew.sh/" rel="nofollow"&gt;Homebrew&lt;/a&gt;. If you are using Python 3.6 on MacOSX, you need to run the following command to install the &lt;code&gt;certifi&lt;/code&gt; package of certificates because Python 3.6 on MacOSX has no certificates to validate SSL connections (see this &lt;a href="https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error" rel="nofollow"&gt;StackOverflow question&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ /Applications/Python\ 3.6/Install\ Certificates.command
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On Linux, unless you know what you are doing, you should use your system's packaging system. For example, on Debian or Ubuntu, type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo apt-get update
$ sudo apt-get install python3 python3-pip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another option is to download and install &lt;a href="https://www.continuum.io/downloads" rel="nofollow"&gt;Anaconda&lt;/a&gt;. This is a package that includes both Python and many scientific libraries. You should prefer the Python 3 version.&lt;/p&gt;
&lt;p&gt;If you choose to use Anaconda, read the next section, or else jump to the &lt;a href="#using-pip"&gt;Using pip&lt;/a&gt; section.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-anaconda" class="anchor" aria-hidden="true" href="#using-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Anaconda&lt;/h2&gt;
&lt;p&gt;Once you have &lt;a href="https://docs.anaconda.com/anaconda/install/" rel="nofollow"&gt;installed Anaconda&lt;/a&gt; (or Miniconda), you can run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will give you a conda environment named &lt;code&gt;mlbook&lt;/code&gt;, ready to use! Just activate it and you will have everything setup
for you:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda activate mlbook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You are all set! Next, jump to the &lt;a href="#starting-jupyter"&gt;Starting Jupyter&lt;/a&gt; section.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-pip" class="anchor" aria-hidden="true" href="#using-pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using pip&lt;/h2&gt;
&lt;p&gt;If you are not using Anaconda, you need to install several scientific Python libraries that are necessary for this project, in particular NumPy, Matplotlib, Pandas, Jupyter and TensorFlow (and a few others). For this, you can either use Python's integrated packaging system, pip, or you may prefer to use your system's own packaging system (if available, e.g. on Linux, or on MacOSX when using MacPorts or Homebrew). The advantage of using pip is that it is easy to create multiple isolated Python environments with different libraries and different library versions (e.g. one environment for each project). The advantage of using your system's packaging system is that there is less risk of having conflicts between your Python libraries and your system's other packages. Since I have many projects with different library requirements, I prefer to use pip with isolated environments. Moreover, the pip packages are usually the most recent ones available, while Anaconda and system packages often lag behind a bit.&lt;/p&gt;
&lt;p&gt;These are the commands you need to type in a terminal if you want to use pip to install the required libraries. Note: in all the following commands, if you chose to use Python 2 rather than Python 3, you must replace &lt;code&gt;pip3&lt;/code&gt; with &lt;code&gt;pip&lt;/code&gt;, and &lt;code&gt;python3&lt;/code&gt; with &lt;code&gt;python&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First you need to make sure you have the latest version of pip installed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --user --upgrade pip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;--user&lt;/code&gt; option will install the latest version of pip only for the current user. If you prefer to install it system wide (i.e. for all users), you must have administrator rights (e.g. use &lt;code&gt;sudo python3&lt;/code&gt; instead of &lt;code&gt;python3&lt;/code&gt; on Linux), and you should remove the &lt;code&gt;--user&lt;/code&gt; option. The same is true of the command below that uses the &lt;code&gt;--user&lt;/code&gt; option.&lt;/p&gt;
&lt;p&gt;Next, you can optionally create an isolated environment. This is recommended as it makes it possible to have a different environment for each project (e.g. one for this project), with potentially very different libraries, and different versions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --user --upgrade virtualenv
$ python3 -m virtualenv -p `which python3` env
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a new directory called &lt;code&gt;env&lt;/code&gt; in the current directory, containing an isolated Python environment based on Python 3. If you installed multiple versions of Python 3 on your system, you can replace &lt;code&gt;`which python3`&lt;/code&gt; with the path to the Python executable you prefer to use.&lt;/p&gt;
&lt;p&gt;Now you must activate this environment. You will need to run this command every time you want to use this environment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ source ./env/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On Windows, the command is slightly different:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ .\env\Scripts\activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, use pip to install the required python packages. If you are not using virtualenv, you should add the &lt;code&gt;--user&lt;/code&gt; option (alternatively you could install the libraries system-wide, but this will probably require administrator rights, e.g. using &lt;code&gt;sudo pip3&lt;/code&gt; instead of &lt;code&gt;pip3&lt;/code&gt; on Linux).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! You're all set, you just need to start Jupyter now.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-starting-jupyter" class="anchor" aria-hidden="true" href="#starting-jupyter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting Jupyter&lt;/h2&gt;
&lt;p&gt;Okay! You can now start Jupyter, simply type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should open up your browser, and you should see Jupyter's tree view, with the contents of the current directory. If your browser does not open automatically, visit &lt;a href="http://127.0.0.1:8888/tree" rel="nofollow"&gt;127.0.0.1:8888&lt;/a&gt;. Click on &lt;code&gt;index.ipynb&lt;/code&gt; to get started!&lt;/p&gt;
&lt;p&gt;Congrats! You are ready to learn Machine Learning, hands on!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h1&gt;
&lt;p&gt;I would like to thank everyone who contributed to this project, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ageron</author><guid isPermaLink="false">https://github.com/ageron/handson-ml</guid><pubDate>Fri, 27 Dec 2019 00:09:00 GMT</pubDate></item><item><title>Kulbear/deep-learning-coursera #10 in Jupyter Notebook, Today</title><link>https://github.com/Kulbear/deep-learning-coursera</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Specialization by Andrew Ng on Coursera.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-specialization-on-coursera" class="anchor" aria-hidden="true" href="#deep-learning-specialization-on-coursera"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning Specialization on Coursera&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Master Deep Learning, and Break into AI&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Instructor: &lt;a href="http://www.andrewng.org/" rel="nofollow"&gt;Andrew Ng&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This repo contains all my work for this specialization. All the code base, quiz questions, screenshot, and images, are taken from, unless specified, &lt;a href="https://www.coursera.org/specializations/deep-learning" rel="nofollow"&gt;Deep Learning Specialization on Coursera&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-i-want-to-say" class="anchor" aria-hidden="true" href="#what-i-want-to-say"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What I want to say&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;VERBOSE CONTENT WARNING: YOU CAN JUMP TO THE NEXT SECTION IF YOU WANT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a CS major student and a long-time self-taught learner, I have completed many CS related MOOCs on Coursera, Udacity, Udemy, and Edx. I do understand the hard time you spend on understanding new concepts and debugging your program. There are discussion forums on most MOOC platforms, however, even a question with detailed description may need some time to be answered. Here I released these solutions, which are &lt;strong&gt;only for your reference purpose&lt;/strong&gt;. It may help you to save some time. And I hope you don't copy any part of the code (the programming assignments are fairly easy if you read the instructions carefully), see the quiz solutions before you start your own adventure. This course is almost the simplest deep learning course I have ever taken, but the simplicity is based on the fabulous course content and structure. It's a treasure given by deeplearning.ai team.&lt;/p&gt;
&lt;p&gt;Currently, this repo has 3 major parts you may be interested in and I will give a list here.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-programming-assignments" class="anchor" aria-hidden="true" href="#programming-assignments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Programming Assignments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Course 1: Neural Networks and Deep Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb"&gt;Week 2 - PA 1 - Logistic Regression with a Neural Network mindset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Planar%20data%20classification%20with%20one%20hidden%20layer.ipynb"&gt;Week 3 - PA 2 - Planar data classification with one hidden layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb"&gt;Week 4 - PA 3 - Building your Deep Neural Network: Step by Step¬∂&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Deep%20Neural%20Network%20-%20Application.ipynb"&gt;Week 4 - PA 4 - Deep Neural Network for Image Classification: Application&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Initialization.ipynb"&gt;Week 1 - PA 1 - Initialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Regularization.ipynb"&gt;Week 1 - PA 2 - Regularization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Gradient%20Checking.ipynb"&gt;Week 1 - PA 3 - Gradient Checking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Optimization%20methods.ipynb"&gt;Week 2 - PA 4 - Optimization Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Tensorflow%20Tutorial.ipynb"&gt;Week 3 - PA 5 - TensorFlow Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Course 3: Structuring Machine Learning Projects&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is no PA for this course. But this course comes with very interesting case study quizzes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Course 4: Convolutional Neural Networks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Convolution%20model%20-%20Step%20by%20Step%20-%20v1.ipynb"&gt;Week 1 - PA 1 - Convolutional Model: step by step&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Convolution%20model%20-%20Application%20-%20v1.ipynb"&gt;Week 1 - PA 2 - Convolutional Model: application&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Keras%20-%20Tutorial%20-%20Happy%20House%20v1.ipynb"&gt;Week 2 - PA 1 - Keras - Tutorial - Happy House&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Residual%20Networks%20-%20v1.ipynb"&gt;Week 2 - PA 2 - Residual Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Course 5: Sequence Models&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Building%20a%20Recurrent%20Neural%20Network%20-%20Step%20by%20Step%20-%20v2.ipynb"&gt;Week 1 - PA 1 - Building a Recurrent Neural Network - Step by Step&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Dinosaurus%20Island%20--%20Character%20level%20language%20model%20final%20-%20v3.ipynb"&gt;Week 1 - PA 2 - Character level language model - Dinosaurus land&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-quiz-solutions" class="anchor" aria-hidden="true" href="#quiz-solutions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quiz Solutions&lt;/h2&gt;
&lt;p&gt;There are concerns that some people may use the content here to quickly ace the course so I'll no longer update any quiz solution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Course 1: Neural Networks and Deep Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md"&gt;Week 1 Quiz - Introduction to deep learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%202%20Quiz%20-%20Neural%20Network%20Basics.md"&gt;Week 2 Quiz - Neural Network Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%203%20Quiz%20-%20%20Shallow%20Neural%20Networks.md"&gt;Week 3 Quiz - Shallow Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.md"&gt;Week 4 Quiz - Key concepts on Deep Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.md"&gt;Week 1 Quiz - Practical aspects of deep learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202%20Quiz%20-%20Optimization%20algorithms.md"&gt;Week 2 Quiz - Optimization algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.md"&gt;Week 3 Quiz - Hyperparameter tuning, Batch Normalization, Programming Frameworks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Course 3: Structuring Machine Learning Projects&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).md"&gt;Week 1 Quiz - Bird recognition in the city of Peacetopia (case study)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kulbear/deep-learning-coursera/blob/master/Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).md"&gt;Week 2 Quiz - Autonomous driving (case study)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;del&gt;- Course 4: Convolutional Neural Networks&lt;/del&gt;
&lt;del&gt;- Course 5: Sequence Models&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;del&gt;## Important Slide Notes&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;del&gt;I screenshotted some important slide page and store them into GitHub issues. It seems not very helpful for everyone since I only keep those I think may be useful to me.&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;del&gt;- &lt;a href="https://github.com/Kulbear/deep-learning-coursera/issues/1"&gt;Screenshots for Course 1: Neural Networks and Deep Learning&lt;/a&gt;&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;del&gt;- &lt;a href="https://github.com/Kulbear/deep-learning-coursera/issues/2"&gt;Screenshots for Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization&lt;/a&gt;&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;del&gt;- &lt;a href="https://github.com/Kulbear/deep-learning-coursera/issues/3"&gt;Screenshots for Course 3: Structuring Machine Learning Projects&lt;/a&gt;&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;del&gt;- &lt;a href="https://github.com/Kulbear/deep-learning-coursera/issues/14"&gt;Screenshots for Course 4: Convolutional Neural Networks&lt;/a&gt;&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;del&gt;- Screenshots for Course 5: Sequence Models&lt;/del&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-milestones" class="anchor" aria-hidden="true" href="#milestones"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Milestones&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;2017-08-17&lt;/strong&gt;: Finished the first-released 3 courses, YAY! &lt;g-emoji class="g-emoji" alias="smiling_imp" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f608.png"&gt;üòà&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Kulbear</author><guid isPermaLink="false">https://github.com/Kulbear/deep-learning-coursera</guid><pubDate>Fri, 27 Dec 2019 00:10:00 GMT</pubDate></item><item><title>guipsamora/pandas_exercises #11 in Jupyter Notebook, Today</title><link>https://github.com/guipsamora/pandas_exercises</link><description>&lt;p&gt;&lt;i&gt;Practice your pandas skills!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pandas-exercises" class="anchor" aria-hidden="true" href="#pandas-exercises"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pandas Exercises&lt;/h1&gt;
&lt;p&gt;Fed up with a ton of tutorials but no easy way to find exercises I decided to create a repo just with exercises to practice pandas.
Don't get me wrong, tutorials are great resources, but to learn is to do. So unless you practice you won't learn.&lt;/p&gt;
&lt;p&gt;There will be three different types of files:&lt;br&gt;
¬†¬†¬†¬†¬†¬†1. Exercise instructions&lt;br&gt;
¬†¬†¬†¬†¬†¬†2. Solutions without code&lt;br&gt;
¬†¬†¬†¬†¬†¬†3. Solutions with code and comments&lt;/p&gt;
&lt;p&gt;My suggestion is that you learn a topic in a tutorial, video or documentation and then do the first exercises.
Learn one more topic and do more exercises. If you are stuck, don't go directly to the solution with code files. Check the solutions only and try to get the correct answer.&lt;/p&gt;
&lt;p&gt;Suggestions and collaborations are more than welcome.&lt;g-emoji class="g-emoji" alias="slightly_smiling_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f642.png"&gt;üôÇ&lt;/g-emoji&gt; Please open an issue or make a PR indicating the exercise and your problem/solution.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-lessons" class="anchor" aria-hidden="true" href="#lessons"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lessons&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="#getting-and-knowing"&gt;Getting and knowing&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#merge"&gt;Merge&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#time-series"&gt;Time Series&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="#filtering-and-sorting"&gt;Filtering and Sorting&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#stats"&gt;Stats&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#deleting"&gt;Deleting&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="#grouping"&gt;Grouping&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#visualization"&gt;Visualization&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Indexing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="#apply"&gt;Apply&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#creating-series-and-dataframes"&gt;Creating Series and DataFrames&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Exporting&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-getting-and-knowing" class="anchor" aria-hidden="true" href="#getting-and-knowing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data"&gt;Getting and knowing&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/Chipotle"&gt;Chipotle&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/Occupation"&gt;Occupation&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/World%20Food%20Facts"&gt;World Food Facts&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-filtering-and-sorting" class="anchor" aria-hidden="true" href="#filtering-and-sorting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting"&gt;Filtering and Sorting&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Chipotle"&gt;Chipotle&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Euro12"&gt;Euro12&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Fictional%20Army"&gt;Fictional Army&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-grouping" class="anchor" aria-hidden="true" href="#grouping"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping"&gt;Grouping&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Alcohol_Consumption"&gt;Alcohol Consumption&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Occupation"&gt;Occupation&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Regiment"&gt;Regiment&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-apply" class="anchor" aria-hidden="true" href="#apply"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply"&gt;Apply&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply/Students_Alcohol_Consumption"&gt;Students Alcohol Consumption&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply/US_Crime_Rates"&gt;US_Crime_Rates&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-merge" class="anchor" aria-hidden="true" href="#merge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge"&gt;Merge&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Auto_MPG"&gt;Auto_MPG&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Fictitous%20Names"&gt;Fictitious Names&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Housing%20Market"&gt;House Market&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-stats" class="anchor" aria-hidden="true" href="#stats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats"&gt;Stats&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats/US_Baby_Names"&gt;US_Baby_Names&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats/Wind_Stats"&gt;Wind_Stats&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-visualization" class="anchor" aria-hidden="true" href="#visualization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization"&gt;Visualization&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Chipotle"&gt;Chipotle&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Titanic_Desaster"&gt;Titanic Disaster&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Scores"&gt;Scores&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Online_Retail"&gt;Online Retail&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Tips"&gt;Tips&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-creating-series-and-dataframes" class="anchor" aria-hidden="true" href="#creating-series-and-dataframes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/08_Creating_Series_and_DataFrames"&gt;Creating Series and DataFrames&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/08_Creating_Series_and_DataFrames/Pokemon"&gt;Pokemon&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-time-series" class="anchor" aria-hidden="true" href="#time-series"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series"&gt;Time Series&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Apple_Stock"&gt;Apple_Stock&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Getting_Financial_Data"&gt;Getting_Financial_Data&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Getting_Financial_Data"&gt;Investor_Flow_of_Funds_US&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-deleting" class="anchor" aria-hidden="true" href="#deleting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting"&gt;Deleting&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting/Iris"&gt;Iris&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting/Wine"&gt;Wine&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-video-solutions" class="anchor" aria-hidden="true" href="#video-solutions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video Solutions&lt;/h1&gt;
&lt;p&gt;Video tutorials of data scientists working through the above exercises:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=pu3IpU937xs&amp;amp;list=PLgJhDSE2ZLxaY_DigHeiIDC1cD09rXgJv" rel="nofollow"&gt;Data Talks - Pandas Learning By Doing&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>guipsamora</author><guid isPermaLink="false">https://github.com/guipsamora/pandas_exercises</guid><pubDate>Fri, 27 Dec 2019 00:11:00 GMT</pubDate></item><item><title>chenyuntc/pytorch-book #12 in Jupyter Notebook, Today</title><link>https://github.com/chenyuntc/pytorch-book</link><description>&lt;p&gt;&lt;i&gt;PyTorch tutorials and fun projects including neural talk, neural style, poem writing, anime generation („ÄäÊ∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂PyTorchÔºöÂÖ•Èó®‰∏éÂÆûÊàò„Äã)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-english-version" class="anchor" aria-hidden="true" href="#english-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="README_EN.md"&gt;English Version&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;ËøôÊòØ‰π¶Á±ç„ÄäÊ∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂PyTorchÔºöÂÖ•Èó®‰∏éÂÆûË∑µ„ÄãÁöÑÂØπÂ∫î‰ª£Á†ÅÔºå‰ΩÜÊòØ‰πüÂèØ‰ª•‰Ωú‰∏∫‰∏Ä‰∏™Áã¨Á´ãÁöÑPyTorchÂÖ•Èó®ÊåáÂçóÂíåÊïôÁ®ã„ÄÇ&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-Êõ¥Êñ∞ËØ¥Êòé" class="anchor" aria-hidden="true" href="#Êõ¥Êñ∞ËØ¥Êòé"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Êõ¥Êñ∞ËØ¥Êòé&lt;/h2&gt;
&lt;p&gt;Working on migration to Pytorch 1.0, stay tuned!&lt;/p&gt;
&lt;p&gt;ÂΩìÂâçÁâàÊú¨ÁöÑ‰ª£Á†ÅÊòØÂü∫‰∫épytorch 1.0.1Ôºå Â¶ÇÊûúÊÉ≥‰ΩøÁî®ÊóßÁâàÁöÑ ËØ∑ &lt;code&gt;git checkout v0.4&lt;/code&gt; ÊàñËÄÖ &lt;code&gt;git checkout v0.3&lt;/code&gt;„ÄÇÊóßÁâà‰ª£Á†ÅÊúâÊõ¥Â•ΩÁöÑpython2/python3 ÂÖºÂÆπÔºåCPU/GPUÂÖºÂÆπÊµãËØï„ÄÇ Êñ∞ÁâàÁöÑ‰ª£Á†ÅÊú™ÁªèËøáÂÆåÊï¥ÊµãËØïÔºåÂ∑≤Âú®GPUÂíåpython3 ‰∏ãÊµãËØïÈÄöËøá„ÄÇ‰ΩÜÊòØÁêÜËÆ∫‰∏äÂú®python2ÂíåCPU‰∏ä‰∏çÂ∫îËØ•ÊúâÂ§™Â§öÁöÑÈóÆÈ¢ò„ÄÇ&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ÂÜÖÂÆπ" class="anchor" aria-hidden="true" href="#ÂÜÖÂÆπ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÂÜÖÂÆπ&lt;/h2&gt;
&lt;p&gt;ËØ•‰π¶ÔºàÊïôÁ®ã/‰ªìÂ∫ìÔºâÁöÑÂÜÖÂÆπÂ¶ÇÂõæÊâÄÁ§∫Ôºö
&lt;a target="_blank" rel="noopener noreferrer" href="imgs/mindmap.png"&gt;&lt;img src="imgs/mindmap.png" alt="ÊÄùÁª¥ÂØºÂõæ" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ÂèØ‰ª•ÁúãÂá∫Êú¨ÊïôÁ®ãÂèØ‰ª•ÂàÜ‰∏∫‰∏§ÈÉ®ÂàÜÔºö&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Âü∫Á°ÄÈÉ®ÂàÜ&lt;/strong&gt;ÔºàÂâç‰∫îÁ´†ÔºâËÆ≤Ëß£PyTorchÂÜÖÂÆπÔºåËøôÈÉ®‰ªΩ‰ªãÁªç‰∫ÜPyTorch‰∏≠‰∏ªË¶ÅÁöÑÁöÑÊ®°ÂùóÔºåÂíåÊ∑±Â∫¶Â≠¶‰π†‰∏≠Â∏∏Áî®ÁöÑ‰∏Ä‰∫õÂ∑•ÂÖ∑„ÄÇÂØπ‰∫éËøôÈÉ®ÂàÜÂÜÖÂÆπÔºåËøôÈáåÂà©Áî®Jupyter Notebook‰Ωú‰∏∫ÊïôÂ≠¶Â∑•ÂÖ∑ÔºåËØªËÄÖÂèØ‰ª•ÁªìÂêànotebook‰øÆÊîπËøêË°åÔºåÂèçÂ§çÂÆûÈ™å„ÄÇ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Á¨¨‰∫åÁ´†‰ªãÁªçÂ¶Ç‰ΩïÂÆâË£ÖPyTorchÂíåÈÖçÁΩÆÂ≠¶‰π†ÁéØÂ¢É„ÄÇÂêåÊó∂Êèê‰æõ‰∫Ü‰∏Ä‰∏™Âø´ÈÄüÂÖ•Èó®ÊïôÁ®ãÔºåÂü∫‰∫éÂÆòÊñπÁöÑÊïôÁ®ãÁÆÄÂåñÂπ∂Êõ¥Êñ∞ÂÜÖÂÆπÔºåËØªËÄÖÂèØ‰ª•Ëä±Ë¥πÂ§ßÁ∫¶1Âà∞2Â∞èÊó∂ÁöÑÊó∂Èó¥Âø´ÈÄüÂÆåÊàêÂÖ•Èó®‰ªªÂä°ÔºåËÄåÂêéÊ†πÊçÆÈúÄÊ±ÇÂÜçÈÄâÊã©Ê∑±ÂÖ•ÈòÖËØªÂêéÁª≠Áõ∏ÂÖ≥Á´†ËäÇÁöÑÂÜÖÂÆπ„ÄÇ&lt;/li&gt;
&lt;li&gt;Á¨¨‰∏âÁ´†‰ªãÁªç‰∫ÜPyTorch‰∏≠Â§öÁª¥Êï∞ÁªÑTensorÂíåÂä®ÊÄÅÂõæautograd/VariableÁöÑ‰ΩøÁî®ÔºåÂπ∂ÈÖç‰ª•‰æãÂ≠êÔºåËÆ©ËØªËÄÖÂàÜÂà´‰ΩøÁî®TensorÂíåautogradÂÆûÁé∞Á∫øÊÄßÂõûÂΩíÔºåÊØîËæÉ‰∫åËÄÖÁöÑ‰∏çÂêåÁÇπ„ÄÇÈô§‰∫Ü‰ªãÁªçËøô‰∫åËÄÖÁöÑÂü∫Á°Ä‰ΩøÁî®‰πãÂ§ñÔºåÊú¨Á´†ËøòÂØπTensorÁöÑÂ∫ïÂ±ÇËÆæËÆ°Ôºå‰ª•ÂèäautogradÁöÑËÆ°ÁÆóÂõæÂéüÁêÜËøõË°åÊØîËæÉÊ∑±ÂÖ•ÂàÜÊûêÔºåÂ∏åÊúõËÉΩ‰ΩøÂæóËØªËÄÖËÉΩÂØπËøô‰∫õÂ∫ïÂ±ÇÁü•ËØÜÊúâÊõ¥ÂÖ®Èù¢ÁöÑÊéåÊè°„ÄÇ&lt;/li&gt;
&lt;li&gt;Á¨¨ÂõõÁ´†‰ªãÁªç‰∫ÜPyTorch‰∏≠Á•ûÁªèÁΩëÁªúÊ®°ÂùónnÁöÑÂü∫Á°ÄÁî®Ê≥ïÔºåÂêåÊó∂ËÆ≤Ëß£‰∫ÜÁ•ûÁªèÁΩëÁªú‰∏≠‚ÄúÂ±Ç‚ÄùÔºå‚ÄúÊçüÂ§±ÂáΩÊï∞‚ÄùÔºå‚Äú‰ºòÂåñÂô®‚ÄùÁ≠âÔºåÊúÄÂêéÂ∏¶È¢ÜËØªËÄÖÁî®‰∏çÂà∞50Ë°åÁöÑ‰ª£Á†ÅÊê≠Âª∫Âá∫ÊõæÂ§∫ÂæóImageNetÂÜ†ÂÜõÁöÑResNet„ÄÇ&lt;/li&gt;
&lt;li&gt;Á¨¨‰∫îÁ´†‰ªãÁªç‰∫ÜPyTorch‰∏≠Êï∞ÊçÆÂä†ËΩΩÔºåGPUÂä†ÈÄüÔºåÊåÅ‰πÖÂåñÂíåÂèØËßÜÂåñÁ≠âÁõ∏ÂÖ≥Â∑•ÂÖ∑„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;ÂÆûÊàòÈÉ®ÂàÜ&lt;/strong&gt;ÔºàÁ¨¨ÂÖ≠Âà∞ÂçÅÁ´†ÔºâÂà©Áî®PyTorchÂÆûÁé∞‰∫ÜÂá†‰∏™ÈÖ∑ÁÇ´ÊúâË∂£ÁöÑÂ∫îÁî®ÔºåÂØπ‰∫éËøôÈÉ®ÂàÜÁöÑÂÜÖÂÆπÔºåÊú¨‰ªìÂ∫ìÁªôÂá∫ÂÆåÊï¥ÁöÑÂÆûÁé∞‰ª£Á†ÅÔºåÂπ∂Êèê‰æõÈ¢ÑËÆ≠ÁªÉÂ•ΩÁöÑÊ®°Âûã‰Ωú‰∏∫demoÔºå‰æõËØªËÄÖÊµãËØï„ÄÇ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Á¨¨ÂÖ≠Á´†ÊòØÊâø‰∏äÂêØ‰∏ãÁöÑ‰∏ÄÁ´†ÔºåËøô‰∏ÄÁ´†ÁöÑÁõÆÊ†á‰∏çÊòØÊïô‰ºöËØªËÄÖÊñ∞ÂáΩÊï∞ÔºåÊñ∞Áü•ËØÜÔºåËÄåÊòØÁªìÂêàKaggle‰∏≠‰∏Ä‰∏™ÁªèÂÖ∏ÁöÑÊØîËµõÔºåÂÆûÁé∞‰∏Ä‰∏™Ê∑±Â∫¶Â≠¶‰π†‰∏≠ÊØîËæÉÁÆÄÂçïÁöÑÂõæÂÉè‰∫åÂàÜÁ±ªÈóÆÈ¢ò„ÄÇÂú®ÂÆûÁé∞ËøáÁ®ã‰∏≠ÔºåÂ∏¶È¢ÜËØªËÄÖÂ§ç‰π†Ââç‰∫îÁ´†ÁöÑÁü•ËØÜÔºåÂπ∂ÊèêÂá∫‰ª£Á†ÅËßÑËåÉ‰ª•ÂêàÁêÜÁöÑÁªÑÁªáÁ®ãÂ∫èÔºå‰ª£Á†ÅÔºå‰ΩøÂæóÁ®ãÂ∫èÊõ¥Âä†ÂèØËØªÔºåÂèØÁª¥Êä§„ÄÇÁ¨¨ÂÖ≠Á´†Ëøò‰ªãÁªç‰∫ÜÂú®PyTorch‰∏≠Â¶Ç‰ΩïËøõË°ådebug„ÄÇ&lt;/li&gt;
&lt;li&gt;Á¨¨‰∏ÉÁ´†‰∏∫ËØªËÄÖËÆ≤Ëß£‰∫ÜÂΩìÂâçÊúÄÁÅ´ÁàÜÁöÑÁîüÊàêÂØπÊäóÁΩëÁªúÔºàGANÔºâÔºåÂ∏¶È¢ÜËØªËÄÖ‰ªéÂ§¥ÂÆûÁé∞‰∏Ä‰∏™Âä®Êº´Â§¥ÂÉèÁîüÊàêÂô®ÔºåËÉΩÂ§üÂà©Áî®GANÁîüÊàêÈ£éÊ†ºÂ§öÂèòÁöÑÂä®Êº´Â§¥ÂÉè„ÄÇ&lt;/li&gt;
&lt;li&gt;Á¨¨ÂÖ´Á´†‰∏∫ËØªËÄÖËÆ≤Ëß£‰∫ÜÈ£éÊ†ºËøÅÁßªÁöÑÁõ∏ÂÖ≥Áü•ËØÜÔºåÂπ∂Â∏¶È¢ÜËØªËÄÖÂÆûÁé∞È£éÊ†ºËøÅÁßªÁΩëÁªúÔºåÂ∞ÜËá™Â∑±ÁöÑÁÖßÁâáÂèòÊàêÈ´òÂ§ß‰∏äÁöÑÂêçÁîª„ÄÇ&lt;/li&gt;
&lt;li&gt;Á¨¨‰πùÁ´†‰∏∫ËØªËÄÖËÆ≤Ëß£‰∫Ü‰∏Ä‰∫õËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁöÑÂü∫Á°ÄÁü•ËØÜÔºåÂπ∂ËÆ≤Ëß£‰∫ÜCharRNNÁöÑÂéüÁêÜ„ÄÇËÄåÂêéÂà©Áî®Êî∂ÈõÜ‰∫ÜÂá†‰∏áÈ¶ñÂîêËØóÔºåËÆ≠ÁªÉÂá∫‰∫Ü‰∏Ä‰∏™ÂèØ‰ª•Ëá™Âä®ÂÜôËØóÊ≠åÁöÑÂ∞èÁ®ãÂ∫è„ÄÇËøô‰∏™Â∞èÁ®ãÂ∫èÂèØ‰ª•ÊéßÂà∂ÁîüÊàêËØóÊ≠åÁöÑ&lt;strong&gt;Ê†ºÂºè&lt;/strong&gt;Ôºå&lt;strong&gt;ÊÑèÂ¢É&lt;/strong&gt;ÔºåËøòËÉΩÁîüÊàê&lt;strong&gt;ËóèÂ§¥ËØó&lt;/strong&gt;„ÄÇ&lt;/li&gt;
&lt;li&gt;Á¨¨ÂçÅÁ´†‰∏∫ËØªËÄÖ‰ªãÁªç‰∫ÜÂõæÂÉèÊèèËø∞‰ªªÂä°ÔºåÂπ∂‰ª•ÊúÄÊñ∞ÁöÑAI ChallengerÊØîËµõÁöÑÊï∞ÊçÆ‰∏∫‰æãÔºåÂ∏¶È¢ÜËØªËÄÖÂÆûÁé∞‰∫Ü‰∏Ä‰∏™ÂèØ‰ª•ËøõË°åÁÆÄÂçïÂõæÂÉèÊèèËø∞ÁöÑÁöÑÂ∞èÁ®ãÂ∫è„ÄÇ&lt;/li&gt;
&lt;li&gt;Á¨¨ÂçÅ‰∏ÄÁ´†Ôºà&lt;strong&gt;Êñ∞Â¢ûÔºåÂÆûÈ™åÊÄß&lt;/strong&gt;Ôºâ Áî±&lt;a href="https://github.com/Diamondfan"&gt;Diamondfan&lt;/a&gt; ÁºñÂÜôÁöÑËØ≠Èü≥ËØÜÂà´„ÄÇÂÆåÂñÑ‰∫ÜÊú¨È°πÁõÆÔºàÊú¨È°πÁõÆÂ∑≤ÂõäÊã¨ÂõæÂÉèÔºåÊñáÊú¨ÔºåËØ≠Èü≥‰∏âÂ§ßÈ¢ÜÂüüÁöÑ‰æãÂ≠êÔºâ„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Notebook‰∏≠ÁöÑÊñáÂ≠óÊèèËø∞ÂÜÖÂÆπÂ±û‰∫éÊú¨‰π¶ÁöÑÂàùÁ®øÔºåÊúâÊèèËø∞‰∏çÈÄöÈ°∫ÔºåÈîôÂà´Â≠ó‰πãÂ§ÑËøòËØ∑Ë∞ÖËß£&lt;/strong&gt;„ÄÇÊú¨ÊâìÁÆóÂà†Èô§notebook‰∏≠ÊèèËø∞ÁöÑÂÜÖÂÆπÔºåÂè™Áïô‰∏ã‰ª£Á†ÅÔºå‰ΩÜ‰∏∫‰∫ÜÊñπ‰æøËØªËÄÖÈòÖËØªÂ≠¶‰π†ÔºåÊúÄÁªàËøòÊòØÂÜ≥ÂÆöÁïô‰∏ã„ÄÇ Êàë‰ºöÊäΩÁ©∫Ê†πÊçÆ‰π¶‰∏≠ÂÜÖÂÆπÈÄêÂ≠óÊ†°ÂØπËøôÈÉ®ÂàÜÂÜÖÂÆπÔºå‰ΩÜÂπ∂‰∏çÂØπÊ≠§Âπ∂‰∏çÊèê‰æõÂÖ∑‰ΩìÊó∂Èó¥ÁÇπ„ÄÇ&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ÊòØÂê¶ÈúÄË¶Å‰π∞‰π¶" class="anchor" aria-hidden="true" href="#ÊòØÂê¶ÈúÄË¶Å‰π∞‰π¶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÊòØÂê¶ÈúÄË¶Å‰π∞‰π¶&lt;/h2&gt;
&lt;p&gt;‰π¶&lt;strong&gt;‰∏çÊòØÂøÖË¶ÅÁöÑ&lt;/strong&gt;ÔºåËøô‰∏™‰ªìÂ∫ìÂåÖÂê´‰π¶‰∏≠50%‰ª•‰∏äÁöÑÊñáÂ≠óÂÜÖÂÆπÔºå90%‰ª•‰∏äÁöÑ‰ª£Á†ÅÔºåÂ∞§ÂÖ∂ÊòØÂâçÂá†Á´†ÂÖ•Èó®ÂÜÖÂÆπÔºåÂá†‰πéÊòØÂÆåÂÖ®‰øùÁïô‰∫Ü‰π¶‰∏≠ÁöÑËÆ≤Ëß£ÂÜÖÂÆπ„ÄÇËØªËÄÖÂç≥‰Ωø‰∏ç‰π∞‰π¶‰πüËÉΩÊ≠£Â∏∏‰ΩøÁî®Êú¨ÊïôÁ®ã„ÄÇ&lt;/p&gt;
&lt;p&gt;&lt;del&gt;Â¶ÇÊûú‰Ω†ËßâÂæóÁ∫∏Ë¥®‰π¶ÁöÑ‰ºòÂäøÂê∏Âºï‰Ω†Ôºå‰∏çÂ¶®Â∞èÁ†¥Ë¥π‰∏ÄÁ¨îÔºåÊîØÊåÅ‰∏Ä‰∏ã‰ΩúËÄÖËøôÂ§ßÂçäÂπ¥Êù•ÁöÑÂ∑•‰Ωú„ÄÇÂêåÊó∂‰∏∫‰∫ÜÂ∞ΩÂèØËÉΩÁöÑÊñπ‰æøËØªËÄÖÔºåÁ¨îËÄÖËøò‰∏ìÈó®ÂºÄÈÄöËÖæËÆØ‰∫ëÁöÑÊúçÂä°ÔºåÁî®‰ª•‰øùÂ≠òÊïôÁ®ã‰∏≠Áî®Âà∞ÁöÑÈÉ®ÂàÜÊ®°ÂûãÔºåÈ¢ÑÂ§ÑÁêÜÁöÑÊï∞ÊçÆÂíåÈÉ®ÂàÜÂ§ßÊñá‰ª∂„ÄÇ&lt;/del&gt;
‰π¶‰∏≠ÁöÑÈÉ®ÂàÜÂÜÖÂÆπÂ∑≤ÁªèËøáÊó∂Ôºå‰ª•Ê≠§‰ªìÂ∫ìÂÜÖÂÆπ‰∏∫ÂáÜ„ÄÇ&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-‰ª£Á†ÅËØ¥Êòé" class="anchor" aria-hidden="true" href="#‰ª£Á†ÅËØ¥Êòé"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;‰ª£Á†ÅËØ¥Êòé&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;‰ª£Á†Å‰∏ªË¶ÅÂú®python3‰∏ãÊµãËØïÂæóÂà∞ÊúÄÁªàÁªìÊûúÔºåpython2ÊöÇÊú™ÊµãËØï„ÄÇv0.2Âíåv0.3 ÂàÜÊîØÁöÑ‰ª£Á†ÅÂêåÊó∂ÁªèËøá‰∏•Ê†ºÊµãËØïÊîØÊåÅpython2/python3&lt;/li&gt;
&lt;li&gt;ÂÆûÊàòÈÉ®ÂàÜ‰ª£Á†ÅÂêåÊó∂Âú®GPUÂíåCPUÁéØÂ¢É‰∏ãÊµãËØïÈÄöËøá&lt;/li&gt;
&lt;li&gt;‰ª£Á†ÅÂ∑≤Êõ¥Êñ∞ÂÖºÂÆπÂà∞PyTorch &lt;code&gt;0.4.1&lt;/code&gt;, ÂêéÁª≠‰ºöËÄÉËôëÂÖºÂÆπ &lt;code&gt;v1.0&lt;/code&gt;Ôºå‰ΩÜÊöÇÊó†Á°ÆÂàáÊó∂Èó¥ÁÇπ„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Â¶ÇÊûú‰Ω†ÊÉ≥Âú®PyTorch 0.2.0Êàñ0.3‰∏ãËøêË°å,ËØ∑&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git checkout v0.2 # v0.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Â¶ÇÊûúÊúâ‰ªª‰Ωï‰∏çÂΩìÔºåÊàñËÄÖÊúâÂæÖÊîπËøõÁöÑÂú∞ÊñπÔºåÊ¨¢ËøéËØªËÄÖÂºÄissueËÆ®ËÆ∫ÔºåÊàñËÄÖÊèê‰∫§pull request„ÄÇ&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ÁéØÂ¢ÉÈÖçÁΩÆ" class="anchor" aria-hidden="true" href="#ÁéØÂ¢ÉÈÖçÁΩÆ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÁéØÂ¢ÉÈÖçÁΩÆ&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ÂÆâË£Ö&lt;a href="http://pytorch.org" rel="nofollow"&gt;PyTorch&lt;/a&gt;ÔºåËØ∑‰ªéÂÆòÁΩëÈÄâÊã©ÊåáÂÆöÁöÑÁâàÊú¨ÂÆâË£ÖÂç≥ÂèØÔºå‰∏ÄÈîÆÂÆâË£ÖÔºàÂç≥‰Ωø‰Ω†‰ΩøÁî®anacondaÔºå‰πüÂª∫ËÆÆ‰ΩøÁî®pipÔºâ„ÄÇÊõ¥Â§öÁöÑÂÆâË£ÖÊñπÂºèËØ∑ÂèÇÈòÖ‰π¶‰∏≠ËØ¥Êòé„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ÂÖãÈöÜ‰ªìÂ∫ì&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;git clone https:&lt;span class="pl-k"&gt;//&lt;/span&gt;github.com&lt;span class="pl-k"&gt;/&lt;/span&gt;chenyuntc&lt;span class="pl-k"&gt;/&lt;/span&gt;PyTorch&lt;span class="pl-k"&gt;-&lt;/span&gt;book.git&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ÂÆâË£ÖÁ¨¨‰∏âÊñπ‰æùËµñÂåÖ&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;cd pytorch&lt;span class="pl-k"&gt;-&lt;/span&gt;book &lt;span class="pl-ii"&gt;&amp;amp;&amp;amp;&lt;/span&gt; pip install &lt;span class="pl-k"&gt;-&lt;/span&gt;r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-visdomÊâì‰∏çÂºÄÂèäÂÖ∂Ëß£ÂÜ≥ÊñπÊ°à" class="anchor" aria-hidden="true" href="#visdomÊâì‰∏çÂºÄÂèäÂÖ∂Ëß£ÂÜ≥ÊñπÊ°à"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VisdomÊâì‰∏çÂºÄÂèäÂÖ∂Ëß£ÂÜ≥ÊñπÊ°à&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Êñ∞ÁâàÁöÑvisdomÂ∑≤ÁªèËß£ÂÜ≥‰∫ÜËøô‰∏™ÈóÆÈ¢ò,Âè™ÈúÄË¶ÅÂçáÁ∫ßÂç≥ÂèØ&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install --upgrade visdom
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;‰πãÂâçÁöÑ&lt;a href="https://github.com/chenyuntc/pytorch-book/blob/2c8366137b691aaa8fbeeea478cc1611c09e15f5/README.md#visdom%E6%89%93%E4%B8%8D%E5%BC%80%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"&gt;Ëß£ÂÜ≥ÊñπÊ°à&lt;/a&gt; ‰∏çÂÜçÈúÄË¶ÅÔºåÂ∑≤Âà†Èô§„ÄÇ&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-_" class="anchor" aria-hidden="true" href="#_"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;^_^&lt;/h2&gt;
&lt;p&gt;Êúâ‰ªª‰ΩïbugÔºåËß£Èáä‰∏çÊ∏ÖÊ•öÁöÑÂú∞ÊñπÊàñËÄÖÊòØÂõ∞ÊÉëÔºåÊ¨¢ËøéÂºÄissue&lt;/p&gt;
&lt;p&gt;Ê¨¢Ëøépull requests&lt;/p&gt;
&lt;p&gt;Happy Coding!&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0376580818bbc47cd4b2f29ab6ca684122ba6e9f/687474703a2f2f696d6731342e333630627579696d672e636f6d2f6e312f6a66732f7431333333392f33322f323436333733303139382f3231373438332f65383134386336622f35613431323737644e62643134373063312e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/0376580818bbc47cd4b2f29ab6ca684122ba6e9f/687474703a2f2f696d6731342e333630627579696d672e636f6d2f6e312f6a66732f7431333333392f33322f323436333733303139382f3231373438332f65383134386336622f35613431323737644e62643134373063312e6a7067" alt="" data-canonical-src="http://img14.360buyimg.com/n1/jfs/t13339/32/2463730198/217483/e8148c6b/5a41277dNbd1470c1.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://search.jd.com/Search?keyword=pytorch%20%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5&amp;amp;enc=utf-8&amp;amp;wq=pytorch%20%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5&amp;amp;pvid=8b0d91d7108845ad8cbaf596326f3eb3" rel="nofollow"&gt;‰∫¨‰∏úË¥≠‰π∞ÈìæÊé•&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://search.dangdang.com/?key=pytorch%20%C8%EB%C3%C5%D3%EB%CA%B5%BC%F9&amp;amp;act=input" rel="nofollow"&gt;ÂΩìÂΩìË¥≠‰π∞ÈìæÊé•&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>chenyuntc</author><guid isPermaLink="false">https://github.com/chenyuntc/pytorch-book</guid><pubDate>Fri, 27 Dec 2019 00:12:00 GMT</pubDate></item><item><title>hunkim/DeepLearningZeroToAll #13 in Jupyter Notebook, Today</title><link>https://github.com/hunkim/DeepLearningZeroToAll</link><description>&lt;p&gt;&lt;i&gt;TensorFlow Basic Tutorial Labs&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-lab-code-wip-but-call-for-comments" class="anchor" aria-hidden="true" href="#lab-code-wip-but-call-for-comments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lab code (WIP), but call for comments&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/hunkim/DeepLearningZeroToAll" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3a08d3822c942ca48219579a6b3824e009d70ca4/68747470733a2f2f7472617669732d63692e6f72672f68756e6b696d2f446565704c6561726e696e675a65726f546f416c6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/hunkim/DeepLearningZeroToAll.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is code for labs covered in TensorFlow basic tutorials (in Korean) at &lt;a href="https://youtu.be/BS6O0zOGX4E" rel="nofollow"&gt;https://youtu.be/BS6O0zOGX4E&lt;/a&gt;.
(We also have a plan to record videos in English.)&lt;/p&gt;
&lt;p&gt;This is work in progress, and may have bugs.
However, we call for your comments and pull requests. Check out our style guide line:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More TF (1.0) style: use more recent and decent TF APIs.&lt;/li&gt;
&lt;li&gt;More Pythonic: fully leverage the power of python&lt;/li&gt;
&lt;li&gt;Readability (over efficiency): Since it's for instruction purposes, we prefer &lt;em&gt;readability&lt;/em&gt; over others.&lt;/li&gt;
&lt;li&gt;Understandability (over everything): Understanding TF key concepts is the main goal of this code.&lt;/li&gt;
&lt;li&gt;KISS: Keep It Simple Stupid! &lt;a href="https://www.techopedia.com/definition/20262/keep-it-simple-stupid-principle-kiss-principle" rel="nofollow"&gt;https://www.techopedia.com/definition/20262/keep-it-simple-stupid-principle-kiss-principle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-lab-slides" class="anchor" aria-hidden="true" href="#lab-slides"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lab slides:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://goo.gl/jPtWNt" rel="nofollow"&gt;https://goo.gl/jPtWNt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We welcome your comments on slides.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-file-naming-rule" class="anchor" aria-hidden="true" href="#file-naming-rule"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;File naming rule:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;klab-XX-X-[name].py: Keras labs code&lt;/li&gt;
&lt;li&gt;lab-XX-X-[name].py: TensorFlow lab code&lt;/li&gt;
&lt;li&gt;mxlab-XX-X-[name].py: MXNet lab code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-install-requirements" class="anchor" aria-hidden="true" href="#install-requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install requirements&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-run-test-and-autopep8" class="anchor" aria-hidden="true" href="#run-test-and-autopep8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run test and autopep8&lt;/h2&gt;
&lt;p&gt;TODO: Need to add more test cases&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m unittest discover -s tests&lt;span class="pl-k"&gt;;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; http://stackoverflow.com/questions/14328406/&lt;/span&gt;
pip install autopep8 &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; if you haven't install&lt;/span&gt;
autopep8 &lt;span class="pl-c1"&gt;.&lt;/span&gt; --recursive --in-place --pep8-passes 2000 --verbose&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-automatically-create-requirementstxt" class="anchor" aria-hidden="true" href="#automatically-create-requirementstxt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Automatically create requirements.txt&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install pipreqs

pipreqs /path/to/project&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="http://stackoverflow.com/questions/31684375" rel="nofollow"&gt;http://stackoverflow.com/questions/31684375&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributionscomments" class="anchor" aria-hidden="true" href="#contributionscomments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions/Comments&lt;/h2&gt;
&lt;p&gt;We always welcome your comments and pull requests.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-reference-implementations" class="anchor" aria-hidden="true" href="#reference-implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference Implementations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nlintz/TensorFlow-Tutorials/"&gt;https://github.com/nlintz/TensorFlow-Tutorials/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/golbin/TensorFlow-ML-Exercises"&gt;https://github.com/golbin/TensorFlow-ML-Exercises&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/FuZer/Study_TensorFlow"&gt;https://github.com/FuZer/Study_TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fchollet/keras/tree/master/examples"&gt;https://github.com/fchollet/keras/tree/master/examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hunkim</author><guid isPermaLink="false">https://github.com/hunkim/DeepLearningZeroToAll</guid><pubDate>Fri, 27 Dec 2019 00:13:00 GMT</pubDate></item><item><title>jackfrued/Python-100-Days #14 in Jupyter Notebook, Today</title><link>https://github.com/jackfrued/Python-100-Days</link><description>&lt;p&gt;&lt;i&gt;Python - 100Â§©‰ªéÊñ∞ÊâãÂà∞Â§ßÂ∏à&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-python---100Â§©‰ªéÊñ∞ÊâãÂà∞Â§ßÂ∏à" class="anchor" aria-hidden="true" href="#python---100Â§©‰ªéÊñ∞ÊâãÂà∞Â§ßÂ∏à"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python - 100Â§©‰ªéÊñ∞ÊâãÂà∞Â§ßÂ∏à&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;‰ΩúËÄÖÔºöÈ™ÜÊòä&lt;/p&gt;
&lt;p&gt;ÊúÄËøëÊúâÂæàÂ§öÊÉ≥Â≠¶‰π†PythonÁöÑÂ∞è‰ºô‰º¥ÈôÜÈôÜÁª≠Áª≠Âä†ÂÖ•Êàë‰ª¨ÁöÑ‰∫§ÊµÅÁæ§ÔºåÁõÆÂâçÊàë‰ª¨ÁöÑ‰∫§ÊµÅÁæ§‰∫∫Êï∞Â∑≤ÁªèË∂ÖËøá‰∏Ä‰∏á‰∫∫„ÄÇÊàë‰ª¨ÁöÑÁõÆÊ†áÊòØÊâìÈÄ†‰∏Ä‰∏™‰ºòË¥®ÁöÑPython‰∫§ÊµÅÁ§æÂå∫Ôºå‰∏ÄÊñπÈù¢‰∏∫ÊÉ≥Â≠¶‰π†PythonÁöÑÂàùÂ≠¶ËÄÖÊâ´Âπ≥ÂÖ•Èó®ËøáÁ®ã‰∏≠ÁöÑÈáçÈáçÈöúÁ¢çÔºõÂè¶‰∏ÄÊñπ‰∏∫Êñ∞ÂÖ•Ë°åÁöÑÂºÄÂèëËÄÖÊèê‰æõÈóÆÈÅìÁöÑÈÄîÂæÑÔºåÂ∏ÆÂä©‰ªñ‰ª¨ËøÖÈÄüÊàêÈïø‰∏∫‰ºòÁßÄÁöÑËÅå‰∏ö‰∫∫ÔºõÊ≠§Â§ñÔºåÊúâÁªèÈ™åÁöÑÂºÄÂèëËÄÖÂèØ‰ª•Âà©Áî®Ëøô‰∏™Âπ≥Âè∞ÊääËá™Â∑±ÁöÑÂ∑•‰ΩúÁªèÈ™åÊó†ÂÅøÂàÜ‰∫´ÊàñÊúâÂÅøÊèê‰æõÂá∫Êù•ÔºåËÆ©Â§ßÂÆ∂ÈÉΩËÉΩÂ§üÂæóÂà∞ËÅå‰∏öÊäÄËÉΩ‰ª•ÂèäÁªºÂêàÁ¥†Ë¥®ÁöÑÂÖ®Èù¢ÊèêÂçá„ÄÇ‰πãÂâçÁöÑÂÖ¨ÂºÄËØæÂíåÁ∫ø‰∏ãÊäÄÊúØ‰∫§ÊµÅÊ¥ªÂä®Âõ†‰∏∫Â∑•‰ΩúÁöÑÂÖ≥Á≥ªËçíÂ∫ü‰∫Ü‰∏ÄÊÆµÊó∂Èó¥‰∫ÜÔºå‰ΩÜÊòØÂêÑ‰ΩçÂ∞è‰ºô‰º¥‰ªçÁÑ∂Ê¥ªË∑ÉÂú®‰∫§ÊµÅÁæ§Âπ∂‰∏ÄÂ¶ÇÊó¢ÂæÄÁöÑÊîØÊåÅÊàë‰ª¨ÔºåÂú®Ê≠§ÂêëÂ§ßÂÆ∂Ë°®Á§∫ÊÑüË∞¢„ÄÇËøëÊúüÂºÄÂßãÊåÅÁª≠Êõ¥Êñ∞Ââç15Â§©ÂíåÊúÄÂêé10Â§©ÁöÑÂÜÖÂÆπÔºåÂâç15Â§©ÊòØÂÜôÁªôÂàùÂ≠¶ËÄÖÁöÑÔºåÊàëÂ∏åÊúõÊää‰∏äÊâãÁöÑÈöæÂ∫¶Ëøõ‰∏ÄÊ≠•Èôç‰ΩéÔºå‰æãÂ≠êÁ®ãÂ∫èÊõ¥Âä†ÁÆÄÂçïÊ∏ÖÊô∞ÔºõÊúÄÂêé10Â§©ÊòØPythonÈ°πÁõÆÂÆûÊàòÂíåÈù¢ËØïÁõ∏ÂÖ≥ÁöÑ‰∏úË•øÔºåÊàëÂ∏åÊúõÂÜÖÂÆπÊõ¥ËØ¶ÂÆûÂíåÂÆåÊï¥ÔºåÂ∞§ÂÖ∂ÊòØÁ¨¨100Â§©ÁöÑÈù¢ËØïÈ¢òÈÉ®ÂàÜÔºõÂàõ‰Ωú‰∏çÊòìÔºåÊÑüË∞¢Â§ßÂÆ∂ÁöÑÊâìËµèÊîØÊåÅÔºåËøô‰∫õÈí±‰∏ç‰ºöÁî®‰∫éË¥≠‰π∞ÂíñÂï°ËÄåÊòØÈÄöËøáËÖæËÆØÂÖ¨ÁõäÂπ≥Âè∞ÊçêËµ†ÁªôÈúÄË¶ÅÂ∏ÆÂä©ÁöÑ‰∫∫Ôºà&lt;a href="./%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97.md"&gt;ÁÇπÂáª&lt;/a&gt;‰∫ÜËß£ÊçêËµ†ÊÉÖÂÜµÔºâ„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-qq-group.png"&gt;&lt;img src="./res/python-qq-group.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pythonÂ∫îÁî®È¢ÜÂüüÂíåÂ∞±‰∏öÂΩ¢ÂäøÂàÜÊûê" class="anchor" aria-hidden="true" href="#pythonÂ∫îÁî®È¢ÜÂüüÂíåÂ∞±‰∏öÂΩ¢ÂäøÂàÜÊûê"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PythonÂ∫îÁî®È¢ÜÂüüÂíåÂ∞±‰∏öÂΩ¢ÂäøÂàÜÊûê&lt;/h3&gt;
&lt;p&gt;ÁÆÄÂçïÁöÑËØ¥ÔºåPythonÊòØ‰∏Ä‰∏™‚Äú‰ºòÈõÖ‚Äù„ÄÅ‚ÄúÊòéÁ°Æ‚Äù„ÄÅ‚ÄúÁÆÄÂçï‚ÄùÁöÑÁºñÁ®ãËØ≠Ë®Ä„ÄÇ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Â≠¶‰π†Êõ≤Á∫ø‰ΩéÔºåÈùû‰∏ì‰∏ö‰∫∫Â£´‰πüËÉΩ‰∏äÊâã&lt;/li&gt;
&lt;li&gt;ÂºÄÊ∫êÁ≥ªÁªüÔºåÊã•ÊúâÂº∫Â§ßÁöÑÁîüÊÄÅÂúà&lt;/li&gt;
&lt;li&gt;Ëß£ÈáäÂûãËØ≠Ë®ÄÔºåÂÆåÁæéÁöÑÂπ≥Âè∞ÂèØÁßªÊ§çÊÄß&lt;/li&gt;
&lt;li&gt;ÊîØÊåÅÈù¢ÂêëÂØπË±°ÂíåÂáΩÊï∞ÂºèÁºñÁ®ã&lt;/li&gt;
&lt;li&gt;ËÉΩÂ§üÈÄöËøáË∞ÉÁî®C/C++‰ª£Á†ÅÊâ©Â±ïÂäüËÉΩ&lt;/li&gt;
&lt;li&gt;‰ª£Á†ÅËßÑËåÉÁ®ãÂ∫¶È´òÔºåÂèØËØªÊÄßÂº∫&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ÁõÆÂâçÂá†‰∏™ÊØîËæÉÊµÅË°åÁöÑÈ¢ÜÂüüÔºåPythonÈÉΩÊúâÁî®Ê≠¶‰πãÂú∞„ÄÇ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‰∫ëÂü∫Á°ÄËÆæÊñΩ - Python / Java / Go&lt;/li&gt;
&lt;li&gt;DevOps - Python / Shell / Ruby / Go&lt;/li&gt;
&lt;li&gt;ÁΩëÁªúÁà¨Ëô´ - Python / PHP / C++&lt;/li&gt;
&lt;li&gt;Êï∞ÊçÆÂàÜÊûêÊåñÊéò - Python / R / Scala / Matlab&lt;/li&gt;
&lt;li&gt;Êú∫Âô®Â≠¶‰π† - Python / R / Java / Lisp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;‰Ωú‰∏∫‰∏ÄÂêçPythonÂºÄÂèëËÄÖÔºå‰∏ªË¶ÅÁöÑÂ∞±‰∏öÈ¢ÜÂüüÂåÖÊã¨Ôºö&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PythonÊúçÂä°Âô®ÂêéÂè∞ÂºÄÂèë / Ê∏∏ÊàèÊúçÂä°Âô®ÂºÄÂèë / Êï∞ÊçÆÊé•Âè£ÂºÄÂèëÂ∑•Á®ãÂ∏à&lt;/li&gt;
&lt;li&gt;PythonËá™Âä®ÂåñËøêÁª¥Â∑•Á®ãÂ∏à&lt;/li&gt;
&lt;li&gt;PythonÊï∞ÊçÆÂàÜÊûê / Êï∞ÊçÆÂèØËßÜÂåñ / Â§ßÊï∞ÊçÆÂ∑•Á®ãÂ∏à&lt;/li&gt;
&lt;li&gt;PythonÁà¨Ëô´Â∑•Á®ãÂ∏à&lt;/li&gt;
&lt;li&gt;PythonËÅäÂ§©Êú∫Âô®‰∫∫ÂºÄÂèë / ÂõæÂÉèËØÜÂà´ÂíåËßÜËßâÁÆóÊ≥ï / Ê∑±Â∫¶Â≠¶‰π†Â∑•Á®ãÂ∏à&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;‰∏ãÂõæÊòæÁ§∫‰∫Ü‰∏ªË¶ÅÂüéÂ∏ÇPythonÊãõËÅòÈúÄÊ±ÇÈáèÂèäËñ™ËµÑÂæÖÈÅáÊéíË°åÊ¶úÔºàÊà™Ê≠¢Âà∞2018Âπ¥5ÊúàÔºâ„ÄÇ&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-top-10.png"&gt;&lt;img src="./res/python-top-10.png" alt="PythonÊãõËÅòÈúÄÊ±ÇÂèäËñ™ËµÑÂæÖÈÅáTop 10" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-bj-salary.png"&gt;&lt;img src="./res/python-bj-salary.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-salary-chengdu.png"&gt;&lt;img src="./res/python-salary-chengdu.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ÁªôÂàùÂ≠¶ËÄÖÁöÑÂá†‰∏™Âª∫ËÆÆÔºö&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make English as your working language.&lt;/li&gt;
&lt;li&gt;Practice makes perfect.&lt;/li&gt;
&lt;li&gt;All experience comes from mistakes.&lt;/li&gt;
&lt;li&gt;Don't be one of the leeches.&lt;/li&gt;
&lt;li&gt;Either stand out or kicked out.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day0115---pythonËØ≠Ë®ÄÂü∫Á°Ä" class="anchor" aria-hidden="true" href="#day0115---pythonËØ≠Ë®ÄÂü∫Á°Ä"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day01~15 - &lt;a href="./Day01-15"&gt;PythonËØ≠Ë®ÄÂü∫Á°Ä&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day01---ÂàùËØÜpython" class="anchor" aria-hidden="true" href="#day01---ÂàùËØÜpython"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day01 - &lt;a href="./Day01-15/01.%E5%88%9D%E8%AF%86Python.md"&gt;ÂàùËØÜPython&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;PythonÁÆÄ‰ªã - PythonÁöÑÂéÜÂè≤ / PythonÁöÑ‰ºòÁº∫ÁÇπ / PythonÁöÑÂ∫îÁî®È¢ÜÂüü&lt;/li&gt;
&lt;li&gt;Êê≠Âª∫ÁºñÁ®ãÁéØÂ¢É - WindowsÁéØÂ¢É / LinuxÁéØÂ¢É / MacOSÁéØÂ¢É&lt;/li&gt;
&lt;li&gt;‰ªéÁªàÁ´ØËøêË°åPythonÁ®ãÂ∫è - Hello, world / printÂáΩÊï∞ / ËøêË°åÁ®ãÂ∫è&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®IDLE - ‰∫§‰∫íÂºèÁéØÂ¢É(REPL) / ÁºñÂÜôÂ§öË°å‰ª£Á†Å / ËøêË°åÁ®ãÂ∫è / ÈÄÄÂá∫IDLE&lt;/li&gt;
&lt;li&gt;Ê≥®Èáä - Ê≥®ÈáäÁöÑ‰ΩúÁî® / ÂçïË°åÊ≥®Èáä / Â§öË°åÊ≥®Èáä&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day02---ËØ≠Ë®ÄÂÖÉÁ¥†" class="anchor" aria-hidden="true" href="#day02---ËØ≠Ë®ÄÂÖÉÁ¥†"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day02 - &lt;a href="./Day01-15/02.%E8%AF%AD%E8%A8%80%E5%85%83%E7%B4%A0.md"&gt;ËØ≠Ë®ÄÂÖÉÁ¥†&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Á®ãÂ∫èÂíåËøõÂà∂ - Êåá‰ª§ÂíåÁ®ãÂ∫è / ÂÜØËØ∫‰æùÊõºÊú∫ / ‰∫åËøõÂà∂ÂíåÂçÅËøõÂà∂ / ÂÖ´ËøõÂà∂ÂíåÂçÅÂÖ≠ËøõÂà∂&lt;/li&gt;
&lt;li&gt;ÂèòÈáèÂíåÁ±ªÂûã - ÂèòÈáèÁöÑÂëΩÂêç / ÂèòÈáèÁöÑ‰ΩøÁî® / inputÂáΩÊï∞ / Ê£ÄÊü•ÂèòÈáèÁ±ªÂûã / Á±ªÂûãËΩ¨Êç¢&lt;/li&gt;
&lt;li&gt;Êï∞Â≠óÂíåÂ≠óÁ¨¶‰∏≤ - Êï¥Êï∞ / ÊµÆÁÇπÊï∞ / Â§çÊï∞ / Â≠óÁ¨¶‰∏≤ / Â≠óÁ¨¶‰∏≤Âü∫Êú¨Êìç‰Ωú / Â≠óÁ¨¶ÁºñÁ†Å&lt;/li&gt;
&lt;li&gt;ËøêÁÆóÁ¨¶ - Êï∞Â≠¶ËøêÁÆóÁ¨¶ / ËµãÂÄºËøêÁÆóÁ¨¶ / ÊØîËæÉËøêÁÆóÁ¨¶ / ÈÄªËæëËøêÁÆóÁ¨¶ / Ë∫´‰ªΩËøêÁÆóÁ¨¶ / ËøêÁÆóÁ¨¶ÁöÑ‰ºòÂÖàÁ∫ß&lt;/li&gt;
&lt;li&gt;Â∫îÁî®Ê°à‰æã - ÂçéÊ∞èÊ∏©Â∫¶ËΩ¨Êç¢ÊàêÊëÑÊ∞èÊ∏©Â∫¶ / ËæìÂÖ•ÂúÜÁöÑÂçäÂæÑËÆ°ÁÆóÂë®ÈïøÂíåÈù¢ÁßØ / ËæìÂÖ•Âπ¥‰ªΩÂà§Êñ≠ÊòØÂê¶ÊòØÈó∞Âπ¥&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day03---ÂàÜÊîØÁªìÊûÑ" class="anchor" aria-hidden="true" href="#day03---ÂàÜÊîØÁªìÊûÑ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day03 - &lt;a href="./Day01-15/03.%E5%88%86%E6%94%AF%E7%BB%93%E6%9E%84.md"&gt;ÂàÜÊîØÁªìÊûÑ&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ÂàÜÊîØÁªìÊûÑÁöÑÂ∫îÁî®Âú∫ÊôØ - Êù°‰ª∂ / Áº©Ëøõ / ‰ª£Á†ÅÂùó / ÊµÅÁ®ãÂõæ&lt;/li&gt;
&lt;li&gt;ifËØ≠Âè• - ÁÆÄÂçïÁöÑif / if-elseÁªìÊûÑ / if-elif-elseÁªìÊûÑ / ÂµåÂ•óÁöÑif&lt;/li&gt;
&lt;li&gt;Â∫îÁî®Ê°à‰æã - Áî®Êà∑Ë∫´‰ªΩÈ™åËØÅ / Ëã±Âà∂Âçï‰Ωç‰∏éÂÖ¨Âà∂Âçï‰Ωç‰∫íÊç¢ / Êé∑È™∞Â≠êÂÜ≥ÂÆöÂÅö‰ªÄ‰πà / ÁôæÂàÜÂà∂ÊàêÁª©ËΩ¨Á≠âÁ∫ßÂà∂ / ÂàÜÊÆµÂáΩÊï∞Ê±ÇÂÄº / ËæìÂÖ•‰∏âÊù°ËæπÁöÑÈïøÂ∫¶Â¶ÇÊûúËÉΩÊûÑÊàê‰∏âËßíÂΩ¢Â∞±ËÆ°ÁÆóÂë®ÈïøÂíåÈù¢ÁßØ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day04---Âæ™ÁéØÁªìÊûÑ" class="anchor" aria-hidden="true" href="#day04---Âæ™ÁéØÁªìÊûÑ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day04 - &lt;a href="./Day01-15/04.%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84.md"&gt;Âæ™ÁéØÁªìÊûÑ&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Âæ™ÁéØÁªìÊûÑÁöÑÂ∫îÁî®Âú∫ÊôØ - Êù°‰ª∂ / Áº©Ëøõ / ‰ª£Á†ÅÂùó / ÊµÅÁ®ãÂõæ&lt;/li&gt;
&lt;li&gt;whileÂæ™ÁéØ - Âü∫Êú¨ÁªìÊûÑ / breakËØ≠Âè• / continueËØ≠Âè•&lt;/li&gt;
&lt;li&gt;forÂæ™ÁéØ - Âü∫Êú¨ÁªìÊûÑ / rangeÁ±ªÂûã / Âæ™ÁéØ‰∏≠ÁöÑÂàÜÊîØÁªìÊûÑ / ÂµåÂ•óÁöÑÂæ™ÁéØ / ÊèêÂâçÁªìÊùüÁ®ãÂ∫è&lt;/li&gt;
&lt;li&gt;Â∫îÁî®Ê°à‰æã - 1~100Ê±ÇÂíå / Âà§Êñ≠Á¥†Êï∞ / ÁåúÊï∞Â≠óÊ∏∏Êàè / ÊâìÂç∞‰πù‰πùË°® / ÊâìÂç∞‰∏âËßíÂΩ¢ÂõæÊ°à / Áå¥Â≠êÂêÉÊ°É / ÁôæÈí±ÁôæÈ∏°&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day05---ÊûÑÈÄ†Á®ãÂ∫èÈÄªËæë" class="anchor" aria-hidden="true" href="#day05---ÊûÑÈÄ†Á®ãÂ∫èÈÄªËæë"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day05 - &lt;a href="./Day01-15/05.%E6%9E%84%E9%80%A0%E7%A8%8B%E5%BA%8F%E9%80%BB%E8%BE%91.md"&gt;ÊûÑÈÄ†Á®ãÂ∫èÈÄªËæë&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ÁªèÂÖ∏Ê°à‰æãÔºöÊ∞¥‰ªôËä±Êï∞ / ÁôæÈí±ÁôæÈ∏° / CrapsËµåÂçöÊ∏∏Êàè&lt;/li&gt;
&lt;li&gt;ÁªÉ‰π†È¢òÁõÆÔºöÊñêÊ≥¢ÈÇ£Â•ëÊï∞Âàó / ÂÆåÁæéÊï∞ / Á¥†Êï∞&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day06---ÂáΩÊï∞ÂíåÊ®°ÂùóÁöÑ‰ΩøÁî®" class="anchor" aria-hidden="true" href="#day06---ÂáΩÊï∞ÂíåÊ®°ÂùóÁöÑ‰ΩøÁî®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day06 - &lt;a href="./Day01-15/06.%E5%87%BD%E6%95%B0%E5%92%8C%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8.md"&gt;ÂáΩÊï∞ÂíåÊ®°ÂùóÁöÑ‰ΩøÁî®&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ÂáΩÊï∞ÁöÑ‰ΩúÁî® - ‰ª£Á†ÅÁöÑÂùèÂë≥ÈÅì / Áî®ÂáΩÊï∞Â∞ÅË£ÖÂäüËÉΩÊ®°Âùó&lt;/li&gt;
&lt;li&gt;ÂÆö‰πâÂáΩÊï∞ - defËØ≠Âè• / ÂáΩÊï∞Âêç / ÂèÇÊï∞ÂàóË°® / returnËØ≠Âè• / Ë∞ÉÁî®Ëá™ÂÆö‰πâÂáΩÊï∞&lt;/li&gt;
&lt;li&gt;Ë∞ÉÁî®ÂáΩÊï∞ - PythonÂÜÖÁΩÆÂáΩÊï∞ /  ÂØºÂÖ•Ê®°ÂùóÂíåÂáΩÊï∞&lt;/li&gt;
&lt;li&gt;ÂáΩÊï∞ÁöÑÂèÇÊï∞ - ÈªòËÆ§ÂèÇÊï∞ / ÂèØÂèòÂèÇÊï∞ / ÂÖ≥ÈîÆÂ≠óÂèÇÊï∞ / ÂëΩÂêçÂÖ≥ÈîÆÂ≠óÂèÇÊï∞&lt;/li&gt;
&lt;li&gt;ÂáΩÊï∞ÁöÑËøîÂõûÂÄº - Ê≤°ÊúâËøîÂõûÂÄº  / ËøîÂõûÂçï‰∏™ÂÄº / ËøîÂõûÂ§ö‰∏™ÂÄº&lt;/li&gt;
&lt;li&gt;‰ΩúÁî®ÂüüÈóÆÈ¢ò - Â±ÄÈÉ®‰ΩúÁî®Âüü / ÂµåÂ•ó‰ΩúÁî®Âüü / ÂÖ®Â±Ä‰ΩúÁî®Âüü / ÂÜÖÁΩÆ‰ΩúÁî®Âüü / Âíå‰ΩúÁî®ÂüüÁõ∏ÂÖ≥ÁöÑÂÖ≥ÈîÆÂ≠ó&lt;/li&gt;
&lt;li&gt;Áî®Ê®°ÂùóÁÆ°ÁêÜÂáΩÊï∞ - Ê®°ÂùóÁöÑÊ¶ÇÂøµ / Áî®Ëá™ÂÆö‰πâÊ®°ÂùóÁÆ°ÁêÜÂáΩÊï∞ / ÂëΩÂêçÂÜ≤Á™ÅÁöÑÊó∂ÂÄô‰ºöÊÄéÊ†∑ÔºàÂêå‰∏Ä‰∏™Ê®°ÂùóÂíå‰∏çÂêåÁöÑÊ®°ÂùóÔºâ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day07---Â≠óÁ¨¶‰∏≤ÂíåÂ∏∏Áî®Êï∞ÊçÆÁªìÊûÑ" class="anchor" aria-hidden="true" href="#day07---Â≠óÁ¨¶‰∏≤ÂíåÂ∏∏Áî®Êï∞ÊçÆÁªìÊûÑ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day07 - &lt;a href="./Day01-15/07.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.md"&gt;Â≠óÁ¨¶‰∏≤ÂíåÂ∏∏Áî®Êï∞ÊçÆÁªìÊûÑ&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Â≠óÁ¨¶‰∏≤ÁöÑ‰ΩøÁî® - ËÆ°ÁÆóÈïøÂ∫¶ / ‰∏ãÊ†áËøêÁÆó / ÂàáÁâá / Â∏∏Áî®ÊñπÊ≥ï&lt;/li&gt;
&lt;li&gt;ÂàóË°®Âü∫Êú¨Áî®Ê≥ï - ÂÆö‰πâÂàóË°® / Áî®‰∏ãË°®ËÆøÈóÆÂÖÉÁ¥† / ‰∏ãÊ†áË∂äÁïå / Ê∑ªÂä†ÂÖÉÁ¥† / Âà†Èô§ÂÖÉÁ¥† / ‰øÆÊîπÂÖÉÁ¥† / ÂàáÁâá / Âæ™ÁéØÈÅçÂéÜ&lt;/li&gt;
&lt;li&gt;ÂàóË°®Â∏∏Áî®Êìç‰Ωú - ËøûÊé• / Â§çÂà∂(Â§çÂà∂ÂÖÉÁ¥†ÂíåÂ§çÂà∂Êï∞ÁªÑ) / ÈïøÂ∫¶ / ÊéíÂ∫è / ÂÄíËΩ¨ / Êü•Êâæ&lt;/li&gt;
&lt;li&gt;ÁîüÊàêÂàóË°® - ‰ΩøÁî®rangeÂàõÂª∫Êï∞Â≠óÂàóË°® / ÁîüÊàêË°®ËææÂºè / ÁîüÊàêÂô®&lt;/li&gt;
&lt;li&gt;ÂÖÉÁªÑÁöÑ‰ΩøÁî® - ÂÆö‰πâÂÖÉÁªÑ / ‰ΩøÁî®ÂÖÉÁªÑ‰∏≠ÁöÑÂÄº / ‰øÆÊîπÂÖÉÁªÑÂèòÈáè / ÂÖÉÁªÑÂíåÂàóË°®ËΩ¨Êç¢&lt;/li&gt;
&lt;li&gt;ÈõÜÂêàÂü∫Êú¨Áî®Ê≥ï - ÈõÜÂêàÂíåÂàóË°®ÁöÑÂå∫Âà´ /  ÂàõÂª∫ÈõÜÂêà / Ê∑ªÂä†ÂÖÉÁ¥† / Âà†Èô§ÂÖÉÁ¥† /  Ê∏ÖÁ©∫&lt;/li&gt;
&lt;li&gt;ÈõÜÂêàÂ∏∏Áî®Êìç‰Ωú - ‰∫§ÈõÜ / Âπ∂ÈõÜ / Â∑ÆÈõÜ / ÂØπÁß∞Â∑Æ / Â≠êÈõÜ / Ë∂ÖÈõÜ&lt;/li&gt;
&lt;li&gt;Â≠óÂÖ∏ÁöÑÂü∫Êú¨Áî®Ê≥ï - Â≠óÂÖ∏ÁöÑÁâπÁÇπ / ÂàõÂª∫Â≠óÂÖ∏ / Ê∑ªÂä†ÂÖÉÁ¥† / Âà†Èô§ÂÖÉÁ¥† / ÂèñÂÄº / Ê∏ÖÁ©∫&lt;/li&gt;
&lt;li&gt;Â≠óÂÖ∏Â∏∏Áî®Êìç‰Ωú - keys()ÊñπÊ≥ï / values()ÊñπÊ≥ï / items()ÊñπÊ≥ï / setdefault()ÊñπÊ≥ï&lt;/li&gt;
&lt;li&gt;Âü∫Á°ÄÁªÉ‰π† - Ë∑ëÈ©¨ÁÅØÊïàÊûú / ÂàóË°®ÊâæÊúÄÂ§ßÂÖÉÁ¥† / ÁªüËÆ°ËÄÉËØïÊàêÁª©ÁöÑÂπ≥ÂùáÂàÜ / FibonacciÊï∞Âàó / Êù®Ëæâ‰∏âËßí&lt;/li&gt;
&lt;li&gt;ÁªºÂêàÊ°à‰æã - ÂèåËâ≤ÁêÉÈÄâÂè∑ / ‰∫ïÂ≠óÊ£ã&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day08---Èù¢ÂêëÂØπË±°ÁºñÁ®ãÂü∫Á°Ä" class="anchor" aria-hidden="true" href="#day08---Èù¢ÂêëÂØπË±°ÁºñÁ®ãÂü∫Á°Ä"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day08 - &lt;a href="./Day01-15/08.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80.md"&gt;Èù¢ÂêëÂØπË±°ÁºñÁ®ãÂü∫Á°Ä&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Á±ªÂíåÂØπË±° - ‰ªÄ‰πàÊòØÁ±ª / ‰ªÄ‰πàÊòØÂØπË±° / Èù¢ÂêëÂØπË±°ÂÖ∂‰ªñÁõ∏ÂÖ≥Ê¶ÇÂøµ&lt;/li&gt;
&lt;li&gt;ÂÆö‰πâÁ±ª - Âü∫Êú¨ÁªìÊûÑ / Â±ûÊÄßÂíåÊñπÊ≥ï / ÊûÑÈÄ†Âô® / ÊûêÊûÑÂô® / __str__ÊñπÊ≥ï&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®ÂØπË±° - ÂàõÂª∫ÂØπË±° / ÁªôÂØπË±°ÂèëÊ∂àÊÅØ&lt;/li&gt;
&lt;li&gt;Èù¢ÂêëÂØπË±°ÁöÑÂõõÂ§ßÊîØÊü± - ÊäΩË±° / Â∞ÅË£Ö / ÁªßÊâø / Â§öÊÄÅ&lt;/li&gt;
&lt;li&gt;Âü∫Á°ÄÁªÉ‰π† - ÂÆö‰πâÂ≠¶ÁîüÁ±ª / ÂÆö‰πâÊó∂ÈíüÁ±ª / ÂÆö‰πâÂõæÂΩ¢Á±ª / ÂÆö‰πâÊ±ΩËΩ¶Á±ª&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day09---Èù¢ÂêëÂØπË±°ËøõÈò∂" class="anchor" aria-hidden="true" href="#day09---Èù¢ÂêëÂØπË±°ËøõÈò∂"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day09 - &lt;a href="./Day01-15/09.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BF%9B%E9%98%B6.md"&gt;Èù¢ÂêëÂØπË±°ËøõÈò∂&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Â±ûÊÄß - Á±ªÂ±ûÊÄß / ÂÆû‰æãÂ±ûÊÄß / Â±ûÊÄßËÆøÈóÆÂô® / Â±ûÊÄß‰øÆÊîπÂô® / Â±ûÊÄßÂà†Èô§Âô® / ‰ΩøÁî®__slots__&lt;/li&gt;
&lt;li&gt;Á±ª‰∏≠ÁöÑÊñπÊ≥ï - ÂÆû‰æãÊñπÊ≥ï / Á±ªÊñπÊ≥ï / ÈùôÊÄÅÊñπÊ≥ï&lt;/li&gt;
&lt;li&gt;ËøêÁÆóÁ¨¶ÈáçËΩΩ - __add__ / __sub__ / __or__ /__getitem__ / __setitem__ / __len__ / __repr__ / __gt__ / __lt__ / __le__ / __ge__ / __eq__ / __ne__ / __contains__&lt;/li&gt;
&lt;li&gt;Á±ª(ÁöÑÂØπË±°)‰πãÈó¥ÁöÑÂÖ≥Á≥ª - ÂÖ≥ËÅî / ÁªßÊâø / ‰æùËµñ&lt;/li&gt;
&lt;li&gt;ÁªßÊâøÂíåÂ§öÊÄÅ - ‰ªÄ‰πàÊòØÁªßÊâø / ÁªßÊâøÁöÑËØ≠Ê≥ï / Ë∞ÉÁî®Áà∂Á±ªÊñπÊ≥ï / ÊñπÊ≥ïÈáçÂÜô / Á±ªÂûãÂà§ÂÆö / Â§öÈáçÁªßÊâø / Ëè±ÂΩ¢ÁªßÊâø(ÈíªÁü≥ÁªßÊâø)ÂíåC3ÁÆóÊ≥ï&lt;/li&gt;
&lt;li&gt;ÁªºÂêàÊ°à‰æã - Â∑•ËµÑÁªìÁÆóÁ≥ªÁªü / Âõæ‰π¶Ëá™Âä®ÊäòÊâ£Á≥ªÁªü / Ëá™ÂÆö‰πâÂàÜÊï∞Á±ª&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day10---ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÂíåÊ∏∏ÊàèÂºÄÂèë" class="anchor" aria-hidden="true" href="#day10---ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÂíåÊ∏∏ÊàèÂºÄÂèë"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day10 - &lt;a href="./Day01-15/10.%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E5%92%8C%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91.md"&gt;ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÂíåÊ∏∏ÊàèÂºÄÂèë&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;‰ΩøÁî®tkinterÂºÄÂèëGUIÁ®ãÂ∫è&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®pygame‰∏âÊñπÂ∫ìÂºÄÂèëÊ∏∏ÊàèÂ∫îÁî®&lt;/li&gt;
&lt;li&gt;‚ÄúÂ§ßÁêÉÂêÉÂ∞èÁêÉ‚ÄùÊ∏∏Êàè&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day11---Êñá‰ª∂ÂíåÂºÇÂ∏∏" class="anchor" aria-hidden="true" href="#day11---Êñá‰ª∂ÂíåÂºÇÂ∏∏"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day11 - &lt;a href="./Day01-15/11.%E6%96%87%E4%BB%B6%E5%92%8C%E5%BC%82%E5%B8%B8.md"&gt;Êñá‰ª∂ÂíåÂºÇÂ∏∏&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ËØªÊñá‰ª∂ - ËØªÂèñÊï¥‰∏™Êñá‰ª∂ / ÈÄêË°åËØªÂèñ / Êñá‰ª∂Ë∑ØÂæÑ&lt;/li&gt;
&lt;li&gt;ÂÜôÊñá‰ª∂ - Ë¶ÜÁõñÂÜôÂÖ• / ËøΩÂä†ÂÜôÂÖ• / ÊñáÊú¨Êñá‰ª∂ / ‰∫åËøõÂà∂Êñá‰ª∂&lt;/li&gt;
&lt;li&gt;ÂºÇÂ∏∏Â§ÑÁêÜ - ÂºÇÂ∏∏Êú∫Âà∂ÁöÑÈáçË¶ÅÊÄß / try-except‰ª£Á†ÅÂùó / else‰ª£Á†ÅÂùó / finally‰ª£Á†ÅÂùó / ÂÜÖÁΩÆÂºÇÂ∏∏Á±ªÂûã / ÂºÇÂ∏∏Ê†à / raiseËØ≠Âè•&lt;/li&gt;
&lt;li&gt;Êï∞ÊçÆÊåÅ‰πÖÂåñ - CSVÊñá‰ª∂Ê¶ÇËø∞ / csvÊ®°ÂùóÁöÑÂ∫îÁî® / JSONÊï∞ÊçÆÊ†ºÂºè / jsonÊ®°ÂùóÁöÑÂ∫îÁî®&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day12---Â≠óÁ¨¶‰∏≤ÂíåÊ≠£ÂàôË°®ËææÂºè" class="anchor" aria-hidden="true" href="#day12---Â≠óÁ¨¶‰∏≤ÂíåÊ≠£ÂàôË°®ËææÂºè"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day12 - &lt;a href="./Day01-15/12.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.md"&gt;Â≠óÁ¨¶‰∏≤ÂíåÊ≠£ÂàôË°®ËææÂºè&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Â≠óÁ¨¶‰∏≤È´òÁ∫ßÊìç‰Ωú - ËΩ¨‰πâÂ≠óÁ¨¶ / ÂéüÂßãÂ≠óÁ¨¶‰∏≤ / Â§öË°åÂ≠óÁ¨¶‰∏≤ / inÂíå not inËøêÁÆóÁ¨¶ / isÂºÄÂ§¥ÁöÑÊñπÊ≥ï / joinÂíåsplitÊñπÊ≥ï / stripÁõ∏ÂÖ≥ÊñπÊ≥ï / pyperclipÊ®°Âùó / ‰∏çÂèòÂ≠óÁ¨¶‰∏≤ÂíåÂèØÂèòÂ≠óÁ¨¶‰∏≤ / StringIOÁöÑ‰ΩøÁî®&lt;/li&gt;
&lt;li&gt;Ê≠£ÂàôË°®ËææÂºèÂÖ•Èó® - Ê≠£ÂàôË°®ËææÂºèÁöÑ‰ΩúÁî® / ÂÖÉÂ≠óÁ¨¶ / ËΩ¨‰πâ / ÈáèËØç / ÂàÜÁªÑ / Èõ∂ÂÆΩÊñ≠Ë®Ä /Ë¥™Â©™ÂåπÈÖç‰∏éÊÉ∞ÊÄßÂåπÈÖçÊáíÊÉ∞ / ‰ΩøÁî®reÊ®°ÂùóÂÆûÁé∞Ê≠£ÂàôË°®ËææÂºèÊìç‰ΩúÔºàÂåπÈÖç„ÄÅÊêúÁ¥¢„ÄÅÊõøÊç¢„ÄÅÊçïËé∑Ôºâ&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºè - reÊ®°Âùó / compileÂáΩÊï∞ / groupÂíågroupsÊñπÊ≥ï / matchÊñπÊ≥ï / searchÊñπÊ≥ï / findallÂíåfinditerÊñπÊ≥ï / subÂíåsubnÊñπÊ≥ï / splitÊñπÊ≥ï&lt;/li&gt;
&lt;li&gt;Â∫îÁî®Ê°à‰æã - ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÈ™åËØÅËæìÂÖ•ÁöÑÂ≠óÁ¨¶‰∏≤&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day13---ËøõÁ®ãÂíåÁ∫øÁ®ã" class="anchor" aria-hidden="true" href="#day13---ËøõÁ®ãÂíåÁ∫øÁ®ã"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day13 - &lt;a href="./Day01-15/13.%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B.md"&gt;ËøõÁ®ãÂíåÁ∫øÁ®ã&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ËøõÁ®ãÂíåÁ∫øÁ®ãÁöÑÊ¶ÇÂøµ - ‰ªÄ‰πàÊòØËøõÁ®ã / ‰ªÄ‰πàÊòØÁ∫øÁ®ã / Â§öÁ∫øÁ®ãÁöÑÂ∫îÁî®Âú∫ÊôØ&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®ËøõÁ®ã - forkÂáΩÊï∞ / multiprocessingÊ®°Âùó / ËøõÁ®ãÊ±† / ËøõÁ®ãÈó¥ÈÄö‰ø°&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®Á∫øÁ®ã - threadÊ®°Âùó / threadingÊ®°Âùó / ThreadÁ±ª / LockÁ±ª / ConditionÁ±ª / Á∫øÁ®ãÊ±†&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day14---ÁΩëÁªúÁºñÁ®ãÂÖ•Èó®ÂíåÁΩëÁªúÂ∫îÁî®ÂºÄÂèë" class="anchor" aria-hidden="true" href="#day14---ÁΩëÁªúÁºñÁ®ãÂÖ•Èó®ÂíåÁΩëÁªúÂ∫îÁî®ÂºÄÂèë"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day14 - &lt;a href="./Day01-15/14.%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%E5%92%8C%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91.md"&gt;ÁΩëÁªúÁºñÁ®ãÂÖ•Èó®ÂíåÁΩëÁªúÂ∫îÁî®ÂºÄÂèë&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ËÆ°ÁÆóÊú∫ÁΩëÁªúÂü∫Á°Ä - ËÆ°ÁÆóÊú∫ÁΩëÁªúÂèëÂ±ïÂè≤ / ‚ÄúTCP-IP‚ÄùÊ®°Âûã / IPÂú∞ÂùÄ / Á´ØÂè£ / ÂçèËÆÆ / ÂÖ∂‰ªñÁõ∏ÂÖ≥Ê¶ÇÂøµ&lt;/li&gt;
&lt;li&gt;ÁΩëÁªúÂ∫îÁî®Ê®°Âºè - ‚ÄúÂÆ¢Êà∑Á´Ø-ÊúçÂä°Âô®‚ÄùÊ®°Âºè / ‚ÄúÊµèËßàÂô®-ÊúçÂä°Âô®‚ÄùÊ®°Âºè&lt;/li&gt;
&lt;li&gt;Âü∫‰∫éHTTPÂçèËÆÆËÆøÈóÆÁΩëÁªúËµÑÊ∫ê - ÁΩëÁªúAPIÊ¶ÇËø∞ / ËÆøÈóÆURL / requestsÊ®°Âùó / Ëß£ÊûêJSONÊ†ºÂºèÊï∞ÊçÆ&lt;/li&gt;
&lt;li&gt;PythonÁΩëÁªúÁºñÁ®ã - Â•óÊé•Â≠óÁöÑÊ¶ÇÂøµ / socketÊ®°Âùó /  socketÂáΩÊï∞ / ÂàõÂª∫TCPÊúçÂä°Âô® / ÂàõÂª∫TCPÂÆ¢Êà∑Á´Ø / ÂàõÂª∫UDPÊúçÂä°Âô® / ÂàõÂª∫UDPÂÆ¢Êà∑Á´Ø / SocketServerÊ®°Âùó&lt;/li&gt;
&lt;li&gt;ÁîµÂ≠êÈÇÆ‰ª∂ - SMTPÂçèËÆÆ / POP3ÂçèËÆÆ / IMAPÂçèËÆÆ / smtplibÊ®°Âùó / poplibÊ®°Âùó / imaplibÊ®°Âùó&lt;/li&gt;
&lt;li&gt;Áü≠‰ø°ÊúçÂä° - Ë∞ÉÁî®Áü≠‰ø°ÊúçÂä°ÁΩëÂÖ≥&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day15---ÂõæÂÉèÂíåÊñáÊ°£Â§ÑÁêÜ" class="anchor" aria-hidden="true" href="#day15---ÂõæÂÉèÂíåÊñáÊ°£Â§ÑÁêÜ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day15 - &lt;a href="./Day01-15/15.%E5%9B%BE%E5%83%8F%E5%92%8C%E5%8A%9E%E5%85%AC%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86.md"&gt;ÂõæÂÉèÂíåÊñáÊ°£Â§ÑÁêÜ&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Áî®PillowÂ§ÑÁêÜÂõæÁâá - ÂõæÁâáËØªÂÜô / ÂõæÁâáÂêàÊàê / Âá†‰ΩïÂèòÊç¢ / Ëâ≤ÂΩ©ËΩ¨Êç¢ / Êª§ÈïúÊïàÊûú&lt;/li&gt;
&lt;li&gt;ËØªÂÜôWordÊñáÊ°£ - ÊñáÊú¨ÂÜÖÂÆπÁöÑÂ§ÑÁêÜ / ÊÆµËêΩ / È°µÁúâÂíåÈ°µËÑö / Ê†∑ÂºèÁöÑÂ§ÑÁêÜ&lt;/li&gt;
&lt;li&gt;ËØªÂÜôExcelÊñá‰ª∂ - xlrdÊ®°Âùó / xlwtÊ®°Âùó&lt;/li&gt;
&lt;li&gt;ÁîüÊàêPDFÊñá‰ª∂ - pypdf2Ê®°Âùó / reportlabÊ®°Âùó&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day16day20---pythonËØ≠Ë®ÄËøõÈò∂-" class="anchor" aria-hidden="true" href="#day16day20---pythonËØ≠Ë®ÄËøõÈò∂-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day16~Day20 - &lt;a href="./Day16-20/16-20.Python%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6.md"&gt;PythonËØ≠Ë®ÄËøõÈò∂ &lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Â∏∏Áî®Êï∞ÊçÆÁªìÊûÑ&lt;/li&gt;
&lt;li&gt;ÂáΩÊï∞ÁöÑÈ´òÁ∫ßÁî®Ê≥ï - ‚Äú‰∏ÄÁ≠âÂÖ¨Ê∞ë‚Äù / È´òÈò∂ÂáΩÊï∞ / LambdaÂáΩÊï∞ / ‰ΩúÁî®ÂüüÂíåÈó≠ÂåÖ / Ë£ÖÈ•∞Âô®&lt;/li&gt;
&lt;li&gt;Èù¢ÂêëÂØπË±°È´òÁ∫ßÁü•ËØÜ - ‚Äú‰∏âÂ§ßÊîØÊü±‚Äù / Á±ª‰∏éÁ±ª‰πãÈó¥ÁöÑÂÖ≥Á≥ª / ÂûÉÂúæÂõûÊî∂ / È≠îÊúØÂ±ûÊÄßÂíåÊñπÊ≥ï / Ê∑∑ÂÖ• / ÂÖÉÁ±ª / Èù¢ÂêëÂØπË±°ËÆæËÆ°ÂéüÂàô / GoFËÆæËÆ°Ê®°Âºè&lt;/li&gt;
&lt;li&gt;Ëø≠‰ª£Âô®ÂíåÁîüÊàêÂô® - Áõ∏ÂÖ≥È≠îÊúØÊñπÊ≥ï / ÂàõÂª∫ÁîüÊàêÂô®ÁöÑ‰∏§ÁßçÊñπÂºè /&lt;/li&gt;
&lt;li&gt;Âπ∂ÂèëÂíåÂºÇÊ≠•ÁºñÁ®ã - Â§öÁ∫øÁ®ã / Â§öËøõÁ®ã / ÂºÇÊ≠•IO / asyncÂíåawait&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day2130---webÂâçÁ´ØÂÖ•Èó®" class="anchor" aria-hidden="true" href="#day2130---webÂâçÁ´ØÂÖ•Èó®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day21~30 - &lt;a href="./Day21-30/21-30.Web%E5%89%8D%E7%AB%AF%E6%A6%82%E8%BF%B0.md"&gt;WebÂâçÁ´ØÂÖ•Èó®&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Áî®HTMLÊ†áÁ≠æÊâøËΩΩÈ°µÈù¢ÂÜÖÂÆπ&lt;/li&gt;
&lt;li&gt;Áî®CSSÊ∏≤ÊüìÈ°µÈù¢&lt;/li&gt;
&lt;li&gt;Áî®JavaScriptÂ§ÑÁêÜ‰∫§‰∫íÂºèË°å‰∏∫&lt;/li&gt;
&lt;li&gt;jQueryÂÖ•Èó®ÂíåÊèêÈ´ò&lt;/li&gt;
&lt;li&gt;Vue.jsÂÖ•Èó®&lt;/li&gt;
&lt;li&gt;ElementÁöÑ‰ΩøÁî®&lt;/li&gt;
&lt;li&gt;BootstrapÁöÑ‰ΩøÁî®&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day3135---Áé©ËΩ¨linuxÊìç‰ΩúÁ≥ªÁªü" class="anchor" aria-hidden="true" href="#day3135---Áé©ËΩ¨linuxÊìç‰ΩúÁ≥ªÁªü"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day31~35 - &lt;a href="./Day31-35/31-35.%E7%8E%A9%E8%BD%ACLinux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.md"&gt;Áé©ËΩ¨LinuxÊìç‰ΩúÁ≥ªÁªü&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Êìç‰ΩúÁ≥ªÁªüÂèëÂ±ïÂè≤ÂíåLinuxÊ¶ÇËø∞&lt;/li&gt;
&lt;li&gt;LinuxÂü∫Á°ÄÂëΩ‰ª§&lt;/li&gt;
&lt;li&gt;Linux‰∏≠ÁöÑÂÆûÁî®Á®ãÂ∫è&lt;/li&gt;
&lt;li&gt;LinuxÁöÑÊñá‰ª∂Á≥ªÁªü&lt;/li&gt;
&lt;li&gt;VimÁºñËæëÂô®ÁöÑÂ∫îÁî®&lt;/li&gt;
&lt;li&gt;ÁéØÂ¢ÉÂèòÈáèÂíåShellÁºñÁ®ã&lt;/li&gt;
&lt;li&gt;ËΩØ‰ª∂ÁöÑÂÆâË£ÖÂíåÊúçÂä°ÁöÑÈÖçÁΩÆ&lt;/li&gt;
&lt;li&gt;ÁΩëÁªúËÆøÈóÆÂíåÁÆ°ÁêÜ&lt;/li&gt;
&lt;li&gt;ÂÖ∂‰ªñÁõ∏ÂÖ≥ÂÜÖÂÆπ&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day3640---Êï∞ÊçÆÂ∫ìÂü∫Á°ÄÂíåËøõÈò∂" class="anchor" aria-hidden="true" href="#day3640---Êï∞ÊçÆÂ∫ìÂü∫Á°ÄÂíåËøõÈò∂"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day36~40 - &lt;a href="./Day36-40"&gt;Êï∞ÊçÆÂ∫ìÂü∫Á°ÄÂíåËøõÈò∂&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="./Day36-40/36-38.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93MySQL.md"&gt;ÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ìMySQL&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;ÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ìÊ¶ÇËø∞&lt;/li&gt;
&lt;li&gt;MySQLÁöÑÂÆâË£ÖÂíå‰ΩøÁî®&lt;/li&gt;
&lt;li&gt;SQLÁöÑ‰ΩøÁî®
&lt;ul&gt;
&lt;li&gt;DDL - Êï∞ÊçÆÂÆö‰πâËØ≠Ë®Ä - create / drop / alter&lt;/li&gt;
&lt;li&gt;DML - Êï∞ÊçÆÊìç‰ΩúËØ≠Ë®Ä - insert / delete / update / select&lt;/li&gt;
&lt;li&gt;DCL - Êï∞ÊçÆÊéßÂà∂ËØ≠Ë®Ä - grant / revoke&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Áõ∏ÂÖ≥Áü•ËØÜ
&lt;ul&gt;
&lt;li&gt;ËåÉÂºèÁêÜËÆ∫ - ËÆæËÆ°‰∫åÁª¥Ë°®ÁöÑÊåáÂØºÊÄùÊÉ≥&lt;/li&gt;
&lt;li&gt;Êï∞ÊçÆÂÆåÊï¥ÊÄß&lt;/li&gt;
&lt;li&gt;Êï∞ÊçÆ‰∏ÄËá¥ÊÄß&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Âú®Python‰∏≠Êìç‰ΩúMySQL&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="./Day36-40/39-40.NoSQL%E5%85%A5%E9%97%A8.md"&gt;NoSQLÂÖ•Èó®&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;NoSQLÊ¶ÇËø∞&lt;/li&gt;
&lt;li&gt;RedisÊ¶ÇËø∞&lt;/li&gt;
&lt;li&gt;MongoÊ¶ÇËø∞&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day4155---ÂÆûÊàòdjango" class="anchor" aria-hidden="true" href="#day4155---ÂÆûÊàòdjango"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day41~55 - &lt;a href="./Day41-55"&gt;ÂÆûÊàòDjango&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day41---Âø´ÈÄü‰∏äÊâã" class="anchor" aria-hidden="true" href="#day41---Âø´ÈÄü‰∏äÊâã"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day41 - &lt;a href="./Day41-55/41.%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.md"&gt;Âø´ÈÄü‰∏äÊâã&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;WebÂ∫îÁî®Â∑•‰ΩúÂéüÁêÜÂíåHTTPÂçèËÆÆ&lt;/li&gt;
&lt;li&gt;DjangoÊ°ÜÊû∂Ê¶ÇËø∞&lt;/li&gt;
&lt;li&gt;5ÂàÜÈíüÂø´ÈÄü‰∏äÊâã&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®ËßÜÂõæÊ®°Êùø&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day42---Ê∑±ÂÖ•Ê®°Âûã" class="anchor" aria-hidden="true" href="#day42---Ê∑±ÂÖ•Ê®°Âûã"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day42 - &lt;a href="./Day41-55/42.%E6%B7%B1%E5%85%A5%E6%A8%A1%E5%9E%8B.md"&gt;Ê∑±ÂÖ•Ê®°Âûã&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ìÈÖçÁΩÆ&lt;/li&gt;
&lt;li&gt;ÁÆ°ÁêÜÂêéÂè∞ÁöÑ‰ΩøÁî®&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®ORMÂÆåÊàêÂØπÊ®°ÂûãÁöÑCRUDÊìç‰Ωú&lt;/li&gt;
&lt;li&gt;DjangoÊ®°ÂûãÊúÄ‰Ω≥ÂÆûË∑µ&lt;/li&gt;
&lt;li&gt;Ê®°ÂûãÂÆö‰πâÂèÇËÄÉ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day43---ÈùôÊÄÅËµÑÊ∫êÂíåajaxËØ∑Ê±Ç" class="anchor" aria-hidden="true" href="#day43---ÈùôÊÄÅËµÑÊ∫êÂíåajaxËØ∑Ê±Ç"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day43 - &lt;a href="./Day41-55/43.%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%92%8CAjax%E8%AF%B7%E6%B1%82.md"&gt;ÈùôÊÄÅËµÑÊ∫êÂíåAjaxËØ∑Ê±Ç&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Âä†ËΩΩÈùôÊÄÅËµÑÊ∫ê&lt;/li&gt;
&lt;li&gt;Áî®AjaxËØ∑Ê±ÇËé∑ÂèñÊï∞ÊçÆ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day44---Ë°®ÂçïÁöÑÂ∫îÁî®" class="anchor" aria-hidden="true" href="#day44---Ë°®ÂçïÁöÑÂ∫îÁî®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day44 - &lt;a href="./Day41-55/44.%E8%A1%A8%E5%8D%95%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;Ë°®ÂçïÁöÑÂ∫îÁî®&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ë°®ÂçïÂíåË°®ÂçïÊéß‰ª∂&lt;/li&gt;
&lt;li&gt;Ë∑®Á´ôËØ∑Ê±Ç‰º™ÈÄ†ÂíåCSRF‰ª§Áâå&lt;/li&gt;
&lt;li&gt;FormÂíåModelForm&lt;/li&gt;
&lt;li&gt;Ë°®ÂçïÈ™åËØÅ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day45---cookieÂíåsession" class="anchor" aria-hidden="true" href="#day45---cookieÂíåsession"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day45 - &lt;a href="./Day41-55/45.Cookie%E5%92%8CSession.md"&gt;CookieÂíåSession&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ÂÆûÁé∞Áî®Êà∑Ë∑üË∏™&lt;/li&gt;
&lt;li&gt;cookieÂíåsessionÁöÑÂÖ≥Á≥ª&lt;/li&gt;
&lt;li&gt;DjangoÊ°ÜÊû∂ÂØπsessionÁöÑÊîØÊåÅ&lt;/li&gt;
&lt;li&gt;ËßÜÂõæÂáΩÊï∞‰∏≠ÁöÑcookieËØªÂÜôÊìç‰Ωú&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day46---Êä•Ë°®ÂíåÊó•Âøó" class="anchor" aria-hidden="true" href="#day46---Êä•Ë°®ÂíåÊó•Âøó"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day46 - &lt;a href="./Day41-55/46.%E6%8A%A5%E8%A1%A8%E5%92%8C%E6%97%A5%E5%BF%97.md"&gt;Êä•Ë°®ÂíåÊó•Âøó&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ÈÄöËøáHttpResponse‰øÆÊîπÂìçÂ∫îÂ§¥&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®StreamingHttpResponseÂ§ÑÁêÜÂ§ßÊñá‰ª∂&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®xlwtÁîüÊàêExcelÊä•Ë°®&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®reportlabÁîüÊàêPDFÊä•Ë°®&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®EChartsÁîüÊàêÂâçÁ´ØÂõæË°®&lt;/li&gt;
&lt;li&gt;ÈÖçÁΩÆÊó•ÂøóÂíåDjango-Debug-Toolbar&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day47---‰∏≠Èó¥‰ª∂ÁöÑÂ∫îÁî®" class="anchor" aria-hidden="true" href="#day47---‰∏≠Èó¥‰ª∂ÁöÑÂ∫îÁî®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day47 - &lt;a href="./Day41-55/47.%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;‰∏≠Èó¥‰ª∂ÁöÑÂ∫îÁî®&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;‰ªÄ‰πàÊòØ‰∏≠Èó¥‰ª∂&lt;/li&gt;
&lt;li&gt;DjangoÊ°ÜÊû∂ÂÜÖÁΩÆÁöÑ‰∏≠Èó¥‰ª∂&lt;/li&gt;
&lt;li&gt;Ëá™ÂÆö‰πâ‰∏≠Èó¥‰ª∂ÂèäÂÖ∂Â∫îÁî®Âú∫ÊôØ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day48---ÂâçÂêéÁ´ØÂàÜÁ¶ªÂºÄÂèëÂÖ•Èó®" class="anchor" aria-hidden="true" href="#day48---ÂâçÂêéÁ´ØÂàÜÁ¶ªÂºÄÂèëÂÖ•Èó®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day48 - &lt;a href="./Day41-55/48.%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8.md"&gt;ÂâçÂêéÁ´ØÂàÜÁ¶ªÂºÄÂèëÂÖ•Èó®&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ËøîÂõûJSONÊ†ºÂºèÁöÑÊï∞ÊçÆ&lt;/li&gt;
&lt;li&gt;Áî®Vue.jsÊ∏≤ÊüìÈ°µÈù¢&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day49---restfulÊû∂ÊûÑÂíådrfÂÖ•Èó®" class="anchor" aria-hidden="true" href="#day49---restfulÊû∂ÊûÑÂíådrfÂÖ•Èó®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day49 - &lt;a href="./Day41-55/49.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E5%85%A5%E9%97%A8.md"&gt;RESTfulÊû∂ÊûÑÂíåDRFÂÖ•Èó®&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day50---restfulÊû∂ÊûÑÂíådrfËøõÈò∂" class="anchor" aria-hidden="true" href="#day50---restfulÊû∂ÊûÑÂíådrfËøõÈò∂"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day50 - &lt;a href="./Day41-55/50.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E8%BF%9B%E9%98%B6.md"&gt;RESTfulÊû∂ÊûÑÂíåDRFËøõÈò∂&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day51---‰ΩøÁî®ÁºìÂ≠ò" class="anchor" aria-hidden="true" href="#day51---‰ΩøÁî®ÁºìÂ≠ò"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day51 - &lt;a href="./Day41-55/51.%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98.md"&gt;‰ΩøÁî®ÁºìÂ≠ò&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ÁΩëÁ´ô‰ºòÂåñÁ¨¨‰∏ÄÂÆöÂæã&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Âú®DjangoÈ°πÁõÆ‰∏≠‰ΩøÁî®RedisÊèê‰æõÁºìÂ≠òÊúçÂä°&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Âú®ËßÜÂõæÂáΩÊï∞‰∏≠ËØªÂÜôÁºìÂ≠ò&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‰ΩøÁî®Ë£ÖÈ•∞Âô®ÂÆûÁé∞È°µÈù¢ÁºìÂ≠ò&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‰∏∫Êï∞ÊçÆÊé•Âè£Êèê‰æõÁºìÂ≠òÊúçÂä°&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day52---Êñá‰ª∂‰∏ä‰º†ÂíåÂØåÊñáÊú¨ÁºñËæë" class="anchor" aria-hidden="true" href="#day52---Êñá‰ª∂‰∏ä‰º†ÂíåÂØåÊñáÊú¨ÁºñËæë"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day52 - &lt;a href="./Day41-55/52.%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%92%8C%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8.md"&gt;Êñá‰ª∂‰∏ä‰º†ÂíåÂØåÊñáÊú¨ÁºñËæë&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Êñá‰ª∂‰∏ä‰º†Ë°®ÂçïÊéß‰ª∂ÂíåÂõæÁâáÊñá‰ª∂È¢ÑËßà&lt;/li&gt;
&lt;li&gt;ÊúçÂä°Âô®Á´ØÂ¶Ç‰ΩïÂ§ÑÁêÜ‰∏ä‰º†ÁöÑÊñá‰ª∂&lt;/li&gt;
&lt;li&gt;ÂØåÊñáÊú¨ÁºñËæëÂô®Ê¶ÇËø∞&lt;/li&gt;
&lt;li&gt;wangEditorÁöÑ‰ΩøÁî®&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day53---Áü≠‰ø°ÂíåÈÇÆ‰ª∂" class="anchor" aria-hidden="true" href="#day53---Áü≠‰ø°ÂíåÈÇÆ‰ª∂"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day53 - &lt;a href="./Day41-55/53.%E7%9F%AD%E4%BF%A1%E5%92%8C%E9%82%AE%E4%BB%B6.md"&gt;Áü≠‰ø°ÂíåÈÇÆ‰ª∂&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Â∏∏Áî®Áü≠‰ø°ÁΩëÂÖ≥Âπ≥Âè∞‰ªãÁªç&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®Ëû∫‰∏ùÂ∏ΩÂèëÈÄÅÁü≠‰ø°&lt;/li&gt;
&lt;li&gt;DjangoÊ°ÜÊû∂ÂØπÈÇÆ‰ª∂ÊúçÂä°ÁöÑÊîØÊåÅ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day54---ÂºÇÊ≠•‰ªªÂä°ÂíåÂÆöÊó∂‰ªªÂä°" class="anchor" aria-hidden="true" href="#day54---ÂºÇÊ≠•‰ªªÂä°ÂíåÂÆöÊó∂‰ªªÂä°"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day54 - &lt;a href="./Day41-55/54.%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.md"&gt;ÂºÇÊ≠•‰ªªÂä°ÂíåÂÆöÊó∂‰ªªÂä°&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ÁΩëÁ´ô‰ºòÂåñÁ¨¨‰∫åÂÆöÂæã&lt;/li&gt;
&lt;li&gt;ÈÖçÁΩÆÊ∂àÊÅØÈòüÂàóÊúçÂä°&lt;/li&gt;
&lt;li&gt;Âú®È°πÁõÆ‰∏≠‰ΩøÁî®celeryÂÆûÁé∞‰ªªÂä°ÂºÇÊ≠•Âåñ&lt;/li&gt;
&lt;li&gt;Âú®È°πÁõÆ‰∏≠‰ΩøÁî®celeryÂÆûÁé∞ÂÆöÊó∂‰ªªÂä°&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day55---ÂçïÂÖÉÊµãËØïÂíåÈ°πÁõÆ‰∏äÁ∫ø" class="anchor" aria-hidden="true" href="#day55---ÂçïÂÖÉÊµãËØïÂíåÈ°πÁõÆ‰∏äÁ∫ø"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day55 - &lt;a href="./Day41-55/55.%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E5%92%8C%E9%A1%B9%E7%9B%AE%E4%B8%8A%E7%BA%BF.md"&gt;ÂçïÂÖÉÊµãËØïÂíåÈ°πÁõÆ‰∏äÁ∫ø&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Python‰∏≠ÁöÑÂçïÂÖÉÊµãËØï&lt;/li&gt;
&lt;li&gt;DjangoÊ°ÜÊû∂ÂØπÂçïÂÖÉÊµãËØïÁöÑÊîØÊåÅ&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®ÁâàÊú¨ÊéßÂà∂Á≥ªÁªü&lt;/li&gt;
&lt;li&gt;ÈÖçÁΩÆÂíå‰ΩøÁî®uWSGI&lt;/li&gt;
&lt;li&gt;Âä®ÈùôÂàÜÁ¶ªÂíåNginxÈÖçÁΩÆ&lt;/li&gt;
&lt;li&gt;ÈÖçÁΩÆHTTPS&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day5660---ÂÆûÊàòflask" class="anchor" aria-hidden="true" href="#day5660---ÂÆûÊàòflask"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day56~60 - &lt;a href="./Day56-65"&gt;ÂÆûÊàòFlask&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day56---flaskÂÖ•Èó®" class="anchor" aria-hidden="true" href="#day56---flaskÂÖ•Èó®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day56 - &lt;a href="./Day56-60/56.Flask%E5%85%A5%E9%97%A8.md"&gt;FlaskÂÖ•Èó®&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day57---Ê®°ÊùøÁöÑ‰ΩøÁî®" class="anchor" aria-hidden="true" href="#day57---Ê®°ÊùøÁöÑ‰ΩøÁî®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day57 - &lt;a href="./Day56-60/57.%E6%A8%A1%E6%9D%BF%E7%9A%84%E4%BD%BF%E7%94%A8.md"&gt;Ê®°ÊùøÁöÑ‰ΩøÁî®&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day58---Ë°®ÂçïÁöÑÂ§ÑÁêÜ" class="anchor" aria-hidden="true" href="#day58---Ë°®ÂçïÁöÑÂ§ÑÁêÜ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day58 - &lt;a href="./Day56-60/58.%E8%A1%A8%E5%8D%95%E7%9A%84%E5%A4%84%E7%90%86.md"&gt;Ë°®ÂçïÁöÑÂ§ÑÁêÜ&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day59---Êï∞ÊçÆÂ∫ìÊìç‰Ωú" class="anchor" aria-hidden="true" href="#day59---Êï∞ÊçÆÂ∫ìÊìç‰Ωú"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day59 - &lt;a href="./Day56-60/59.%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C.md"&gt;Êï∞ÊçÆÂ∫ìÊìç‰Ωú&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day60---È°πÁõÆÂÆûÊàò" class="anchor" aria-hidden="true" href="#day60---È°πÁõÆÂÆûÊàò"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day60 - &lt;a href="./Day56-60/60.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;È°πÁõÆÂÆûÊàò&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-day6165---ÂÆûÊàòtornado" class="anchor" aria-hidden="true" href="#day6165---ÂÆûÊàòtornado"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day61~65 - &lt;a href="./Day61-65"&gt;ÂÆûÊàòTornado&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day61---È¢ÑÂ§áÁü•ËØÜ" class="anchor" aria-hidden="true" href="#day61---È¢ÑÂ§áÁü•ËØÜ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day61 - &lt;a href="./Day61-65/61.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86.md"&gt;È¢ÑÂ§áÁü•ËØÜ&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Âπ∂ÂèëÁºñÁ®ã&lt;/li&gt;
&lt;li&gt;I/OÊ®°ÂºèÂíå‰∫ã‰ª∂È©±Âä®&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day62---tornadoÂÖ•Èó®" class="anchor" aria-hidden="true" href="#day62---tornadoÂÖ•Èó®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day62 - &lt;a href="./Day61-65/62.Tornado%E5%85%A5%E9%97%A8.md"&gt;TornadoÂÖ•Èó®&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;TornadoÊ¶ÇËø∞&lt;/li&gt;
&lt;li&gt;5ÂàÜÈíü‰∏äÊâãTornado&lt;/li&gt;
&lt;li&gt;Ë∑ØÁî±Ëß£Êûê&lt;/li&gt;
&lt;li&gt;ËØ∑Ê±ÇÂ§ÑÁêÜÂô®&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day63---ÂºÇÊ≠•Âåñ" class="anchor" aria-hidden="true" href="#day63---ÂºÇÊ≠•Âåñ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day63 - &lt;a href="./Day61-65/63.%E5%BC%82%E6%AD%A5%E5%8C%96.md"&gt;ÂºÇÊ≠•Âåñ&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;aiomysqlÂíåaioredisÁöÑ‰ΩøÁî®&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day64---websocketÁöÑÂ∫îÁî®" class="anchor" aria-hidden="true" href="#day64---websocketÁöÑÂ∫îÁî®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day64 - &lt;a href="./Day61-65/64.WebSocket%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;WebSocketÁöÑÂ∫îÁî®&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;WebSocketÁÆÄ‰ªã&lt;/li&gt;
&lt;li&gt;WebSocketÊúçÂä°Âô®Á´ØÁºñÁ®ã&lt;/li&gt;
&lt;li&gt;WebSocketÂÆ¢Êà∑Á´ØÁºñÁ®ã&lt;/li&gt;
&lt;li&gt;È°πÁõÆÔºöWebËÅäÂ§©ÂÆ§&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day65---È°πÁõÆÂÆûÊàò" class="anchor" aria-hidden="true" href="#day65---È°πÁõÆÂÆûÊàò"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day65 - &lt;a href="./Day61-65/65.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;È°πÁõÆÂÆûÊàò&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ÂâçÂêéÁ´ØÂàÜÁ¶ªÂºÄÂèëÂíåÊé•Âè£ÊñáÊ°£ÁöÑÊí∞ÂÜô&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®Vue.jsÂÆûÁé∞ÂâçÁ´ØÊ∏≤Êüì&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®EChartsÂÆûÁé∞Êä•Ë°®ÂäüËÉΩ&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®WebSocketÂÆûÁé∞Êé®ÈÄÅÊúçÂä°&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day6675---Áà¨Ëô´ÂºÄÂèë" class="anchor" aria-hidden="true" href="#day6675---Áà¨Ëô´ÂºÄÂèë"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day66~75 - &lt;a href="./Day66-75"&gt;Áà¨Ëô´ÂºÄÂèë&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day66---ÁΩëÁªúÁà¨Ëô´ÂíåÁõ∏ÂÖ≥Â∑•ÂÖ∑" class="anchor" aria-hidden="true" href="#day66---ÁΩëÁªúÁà¨Ëô´ÂíåÁõ∏ÂÖ≥Â∑•ÂÖ∑"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day66 - &lt;a href="./Day66-75/66.%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%92%8C%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7.md"&gt;ÁΩëÁªúÁà¨Ëô´ÂíåÁõ∏ÂÖ≥Â∑•ÂÖ∑&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ÁΩëÁªúÁà¨Ëô´ÁöÑÊ¶ÇÂøµÂèäÂÖ∂Â∫îÁî®È¢ÜÂüü&lt;/li&gt;
&lt;li&gt;ÁΩëÁªúÁà¨Ëô´ÁöÑÂêàÊ≥ïÊÄßÊé¢ËÆ®&lt;/li&gt;
&lt;li&gt;ÂºÄÂèëÁΩëÁªúÁà¨Ëô´ÁöÑÁõ∏ÂÖ≥Â∑•ÂÖ∑&lt;/li&gt;
&lt;li&gt;‰∏Ä‰∏™Áà¨Ëô´Á®ãÂ∫èÁöÑÊûÑÊàê&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day67---Êï∞ÊçÆÈááÈõÜÂíåËß£Êûê" class="anchor" aria-hidden="true" href="#day67---Êï∞ÊçÆÈááÈõÜÂíåËß£Êûê"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day67 - &lt;a href="./Day66-75/67.%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%92%8C%E8%A7%A3%E6%9E%90.md"&gt;Êï∞ÊçÆÈááÈõÜÂíåËß£Êûê&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Êï∞ÊçÆÈááÈõÜÁöÑÊ†áÂáÜÂíå‰∏âÊñπÂ∫ì&lt;/li&gt;
&lt;li&gt;È°µÈù¢Ëß£ÊûêÁöÑ‰∏âÁßçÊñπÂºèÔºöÊ≠£ÂàôË°®ËææÂºèËß£Êûê / XPathËß£Êûê / CSSÈÄâÊã©Âô®Ëß£Êûê&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day68---Â≠òÂÇ®Êï∞ÊçÆ" class="anchor" aria-hidden="true" href="#day68---Â≠òÂÇ®Êï∞ÊçÆ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day68 - &lt;a href="./Day66-75/68.%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE.md"&gt;Â≠òÂÇ®Êï∞ÊçÆ&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Â¶Ç‰ΩïÂ≠òÂÇ®Êµ∑ÈáèÊï∞ÊçÆ&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞Êï∞ÊçÆÁöÑÁºìÂ≠ò&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day69---Âπ∂Âèë‰∏ãËΩΩ" class="anchor" aria-hidden="true" href="#day69---Âπ∂Âèë‰∏ãËΩΩ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day69 - &lt;a href="./Day66-75/69.%E5%B9%B6%E5%8F%91%E4%B8%8B%E8%BD%BD.md"&gt;Âπ∂Âèë‰∏ãËΩΩ&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Â§öÁ∫øÁ®ãÂíåÂ§öËøõÁ®ã&lt;/li&gt;
&lt;li&gt;ÂºÇÊ≠•I/OÂíåÂçèÁ®ã&lt;/li&gt;
&lt;li&gt;asyncÂíåawaitÂÖ≥ÈîÆÂ≠óÁöÑ‰ΩøÁî®&lt;/li&gt;
&lt;li&gt;‰∏âÊñπÂ∫ìaiohttpÁöÑÂ∫îÁî®&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day70---Ëß£ÊûêÂä®ÊÄÅÂÜÖÂÆπ" class="anchor" aria-hidden="true" href="#day70---Ëß£ÊûêÂä®ÊÄÅÂÜÖÂÆπ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day70 - &lt;a href="./Day66-75/70.%E8%A7%A3%E6%9E%90%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9.md"&gt;Ëß£ÊûêÂä®ÊÄÅÂÜÖÂÆπ&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;JavaScriptÈÄÜÂêëÂ∑•Á®ã&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®SeleniumËé∑ÂèñÂä®ÊÄÅÂÜÖÂÆπ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day71---Ë°®Âçï‰∫§‰∫íÂíåÈ™åËØÅÁ†ÅÂ§ÑÁêÜ" class="anchor" aria-hidden="true" href="#day71---Ë°®Âçï‰∫§‰∫íÂíåÈ™åËØÅÁ†ÅÂ§ÑÁêÜ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day71 - &lt;a href="./Day66-75/71.%E8%A1%A8%E5%8D%95%E4%BA%A4%E4%BA%92%E5%92%8C%E9%AA%8C%E8%AF%81%E7%A0%81%E5%A4%84%E7%90%86.md"&gt;Ë°®Âçï‰∫§‰∫íÂíåÈ™åËØÅÁ†ÅÂ§ÑÁêÜ&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ëá™Âä®Êèê‰∫§Ë°®Âçï&lt;/li&gt;
&lt;li&gt;CookieÊ±†ÁöÑÂ∫îÁî®&lt;/li&gt;
&lt;li&gt;È™åËØÅÁ†ÅÂ§ÑÁêÜ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day72---scrapyÂÖ•Èó®" class="anchor" aria-hidden="true" href="#day72---scrapyÂÖ•Èó®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day72 - &lt;a href="./Day66-75/72.Scrapy%E5%85%A5%E9%97%A8.md"&gt;ScrapyÂÖ•Èó®&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ScrapyÁà¨Ëô´Ê°ÜÊû∂Ê¶ÇËø∞&lt;/li&gt;
&lt;li&gt;ÂÆâË£ÖÂíå‰ΩøÁî®Scrapy&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day73---scrapyÈ´òÁ∫ßÂ∫îÁî®" class="anchor" aria-hidden="true" href="#day73---scrapyÈ´òÁ∫ßÂ∫îÁî®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day73 - &lt;a href="./Day66-75/73.Scrapy%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8.md"&gt;ScrapyÈ´òÁ∫ßÂ∫îÁî®&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;SpiderÁöÑÁî®Ê≥ï&lt;/li&gt;
&lt;li&gt;‰∏≠Èó¥‰ª∂ÁöÑÂ∫îÁî®Ôºö‰∏ãËΩΩ‰∏≠Èó¥‰ª∂ / ËúòËõõ‰∏≠Èó¥‰ª∂&lt;/li&gt;
&lt;li&gt;ScrapyÂØπÊé•SeleniumÊäìÂèñÂä®ÊÄÅÂÜÖÂÆπ&lt;/li&gt;
&lt;li&gt;ScrapyÈÉ®ÁΩ≤Âà∞Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day74---scrapyÂàÜÂ∏ÉÂºèÂÆûÁé∞" class="anchor" aria-hidden="true" href="#day74---scrapyÂàÜÂ∏ÉÂºèÂÆûÁé∞"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day74 - &lt;a href="./Day66-75/74.Scrapy%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0.md"&gt;ScrapyÂàÜÂ∏ÉÂºèÂÆûÁé∞&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ÂàÜÂ∏ÉÂºèÁà¨Ëô´ÁöÑÂéüÁêÜ&lt;/li&gt;
&lt;li&gt;ScrapyÂàÜÂ∏ÉÂºèÂÆûÁé∞&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®ScrapydÂÆûÁé∞ÂàÜÂ∏ÉÂºèÈÉ®ÁΩ≤&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day75---Áà¨Ëô´È°πÁõÆÂÆûÊàò" class="anchor" aria-hidden="true" href="#day75---Áà¨Ëô´È°πÁõÆÂÆûÊàò"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day75 - &lt;a href="./Day66-75/75.%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;Áà¨Ëô´È°πÁõÆÂÆûÊàò&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Áà¨ÂèñÊãõËÅòÁΩëÁ´ôÊï∞ÊçÆ&lt;/li&gt;
&lt;li&gt;Áà¨ÂèñÊàøÂú∞‰∫ßË°å‰∏öÊï∞ÊçÆ&lt;/li&gt;
&lt;li&gt;Áà¨Âèñ‰∫åÊâãËΩ¶‰∫§ÊòìÂπ≥Âè∞Êï∞ÊçÆ&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day7690---Êï∞ÊçÆÂ§ÑÁêÜÂíåÊú∫Âô®Â≠¶‰π†" class="anchor" aria-hidden="true" href="#day7690---Êï∞ÊçÆÂ§ÑÁêÜÂíåÊú∫Âô®Â≠¶‰π†"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day76~90 - &lt;a href="./Day76-90"&gt;Êï∞ÊçÆÂ§ÑÁêÜÂíåÊú∫Âô®Â≠¶‰π†&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day76---Êú∫Âô®Â≠¶‰π†Âü∫Á°Ä" class="anchor" aria-hidden="true" href="#day76---Êú∫Âô®Â≠¶‰π†Âü∫Á°Ä"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day76 - &lt;a href="./Day76-90/76.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md"&gt;Êú∫Âô®Â≠¶‰π†Âü∫Á°Ä&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day77---pandasÁöÑÂ∫îÁî®" class="anchor" aria-hidden="true" href="#day77---pandasÁöÑÂ∫îÁî®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day77 - &lt;a href="./Day76-90/77.Pandas%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;PandasÁöÑÂ∫îÁî®&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day78---numpyÂíåscipyÁöÑÂ∫îÁî®" class="anchor" aria-hidden="true" href="#day78---numpyÂíåscipyÁöÑÂ∫îÁî®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day78 - &lt;a href="./Day76-90/78.NumPy%E5%92%8CSciPy%E7%9A%84%E5%BA%94%E7%94%A8"&gt;NumPyÂíåSciPyÁöÑÂ∫îÁî®&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day79---matplotlibÂíåÊï∞ÊçÆÂèØËßÜÂåñ" class="anchor" aria-hidden="true" href="#day79---matplotlibÂíåÊï∞ÊçÆÂèØËßÜÂåñ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day79 - &lt;a href="./Day76-90/79.Matplotlib%E5%92%8C%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"&gt;MatplotlibÂíåÊï∞ÊçÆÂèØËßÜÂåñ&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day80---kÊúÄËøëÈÇªknnÂàÜÁ±ª" class="anchor" aria-hidden="true" href="#day80---kÊúÄËøëÈÇªknnÂàÜÁ±ª"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day80 - &lt;a href="./Day76-90/80.k%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB.md"&gt;kÊúÄËøëÈÇª(KNN)ÂàÜÁ±ª&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day81---ÂÜ≥Á≠ñÊ†ë" class="anchor" aria-hidden="true" href="#day81---ÂÜ≥Á≠ñÊ†ë"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day81 - &lt;a href="./Day76-90/81.%E5%86%B3%E7%AD%96%E6%A0%91.md"&gt;ÂÜ≥Á≠ñÊ†ë&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day82---Ë¥ùÂè∂ÊñØÂàÜÁ±ª" class="anchor" aria-hidden="true" href="#day82---Ë¥ùÂè∂ÊñØÂàÜÁ±ª"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day82 - &lt;a href="./Day76-90/82.%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB.md"&gt;Ë¥ùÂè∂ÊñØÂàÜÁ±ª&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day83---ÊîØÊåÅÂêëÈáèÊú∫svm" class="anchor" aria-hidden="true" href="#day83---ÊîØÊåÅÂêëÈáèÊú∫svm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day83 - &lt;a href="./Day76-90/83.%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.md"&gt;ÊîØÊåÅÂêëÈáèÊú∫(SVM)&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day84---k-ÂùáÂÄºËÅöÁ±ª" class="anchor" aria-hidden="true" href="#day84---k-ÂùáÂÄºËÅöÁ±ª"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day84 - &lt;a href="./Day76-90/84.K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB.md"&gt;K-ÂùáÂÄºËÅöÁ±ª&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day85---ÂõûÂΩíÂàÜÊûê" class="anchor" aria-hidden="true" href="#day85---ÂõûÂΩíÂàÜÊûê"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day85 - &lt;a href="./Day76-90/85.%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90.md"&gt;ÂõûÂΩíÂàÜÊûê&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day86---Â§ßÊï∞ÊçÆÂàÜÊûêÂÖ•Èó®" class="anchor" aria-hidden="true" href="#day86---Â§ßÊï∞ÊçÆÂàÜÊûêÂÖ•Èó®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day86 - &lt;a href="./Day76-90/86.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%85%A5%E9%97%A8.md"&gt;Â§ßÊï∞ÊçÆÂàÜÊûêÂÖ•Èó®&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day87---Â§ßÊï∞ÊçÆÂàÜÊûêËøõÈò∂" class="anchor" aria-hidden="true" href="#day87---Â§ßÊï∞ÊçÆÂàÜÊûêËøõÈò∂"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day87 - &lt;a href="./Day76-90/87.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%BF%9B%E9%98%B6.md"&gt;Â§ßÊï∞ÊçÆÂàÜÊûêËøõÈò∂&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day88---tensorflowÂÖ•Èó®" class="anchor" aria-hidden="true" href="#day88---tensorflowÂÖ•Èó®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day88 - &lt;a href="./Day76-90/88.Tensorflow%E5%85%A5%E9%97%A8.md"&gt;TensorflowÂÖ•Èó®&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day89---tensorflowÂÆûÊàò" class="anchor" aria-hidden="true" href="#day89---tensorflowÂÆûÊàò"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day89 - &lt;a href="./Day76-90/89.Tensorflow%E5%AE%9E%E6%88%98.md"&gt;TensorflowÂÆûÊàò&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day90---Êé®ËçêÁ≥ªÁªü" class="anchor" aria-hidden="true" href="#day90---Êé®ËçêÁ≥ªÁªü"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day90 - &lt;a href="./Day76-90/90.%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.md"&gt;Êé®ËçêÁ≥ªÁªü&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-day91100---Âõ¢ÈòüÈ°πÁõÆÂºÄÂèë" class="anchor" aria-hidden="true" href="#day91100---Âõ¢ÈòüÈ°πÁõÆÂºÄÂèë"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day91~100 - &lt;a href="./Day91-100"&gt;Âõ¢ÈòüÈ°πÁõÆÂºÄÂèë&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-Á¨¨91Â§©Âõ¢ÈòüÈ°πÁõÆÂºÄÂèëÁöÑÈóÆÈ¢òÂíåËß£ÂÜ≥ÊñπÊ°à" class="anchor" aria-hidden="true" href="#Á¨¨91Â§©Âõ¢ÈòüÈ°πÁõÆÂºÄÂèëÁöÑÈóÆÈ¢òÂíåËß£ÂÜ≥ÊñπÊ°à"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Á¨¨91Â§©Ôºö&lt;a href="./Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md"&gt;Âõ¢ÈòüÈ°πÁõÆÂºÄÂèëÁöÑÈóÆÈ¢òÂíåËß£ÂÜ≥ÊñπÊ°à&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ËΩØ‰ª∂ËøáÁ®ãÊ®°Âûã&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ÁªèÂÖ∏ËøáÁ®ãÊ®°ÂûãÔºàÁÄëÂ∏ÉÊ®°ÂûãÔºâ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ÂèØË°åÊÄßÂàÜÊûêÔºàÁ†îÁ©∂ÂÅöËøòÊòØ‰∏çÂÅöÔºâÔºåËæìÂá∫„ÄäÂèØË°åÊÄßÂàÜÊûêÊä•Âëä„Äã„ÄÇ&lt;/li&gt;
&lt;li&gt;ÈúÄÊ±ÇÂàÜÊûêÔºàÁ†îÁ©∂ÂÅö‰ªÄ‰πàÔºâÔºåËæìÂá∫„ÄäÈúÄÊ±ÇËßÑÊ†ºËØ¥Êòé‰π¶„ÄãÂíå‰∫ßÂìÅÁïåÈù¢ÂéüÂûãÂõæ„ÄÇ&lt;/li&gt;
&lt;li&gt;Ê¶ÇË¶ÅËÆæËÆ°ÂíåËØ¶ÁªÜËÆæËÆ°ÔºåËæìÂá∫Ê¶ÇÂøµÊ®°ÂûãÂõæÔºàERÂõæÔºâ„ÄÅÁâ©ÁêÜÊ®°ÂûãÂõæ„ÄÅÁ±ªÂõæ„ÄÅÊó∂Â∫èÂõæÁ≠â„ÄÇ&lt;/li&gt;
&lt;li&gt;ÁºñÁ†Å / ÊµãËØï„ÄÇ&lt;/li&gt;
&lt;li&gt;‰∏äÁ∫ø / Áª¥Êä§„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ÁÄëÂ∏ÉÊ®°ÂûãÊúÄÂ§ßÁöÑÁº∫ÁÇπÊòØÊó†Ê≥ïÊã•Êä±ÈúÄÊ±ÇÂèòÂåñÔºåÊï¥Â•óÊµÅÁ®ãÁªìÊùüÂêéÊâçËÉΩÁúãÂà∞‰∫ßÂìÅÔºåÂõ¢ÈòüÂ£´Ê∞î‰ΩéËêΩ„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ÊïèÊç∑ÂºÄÂèëÔºàScrumÔºâ- ‰∫ßÂìÅÊâÄÊúâËÄÖ„ÄÅScrum Master„ÄÅÁ†îÂèë‰∫∫Âëò - Sprint&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‰∫ßÂìÅÁöÑBacklogÔºàÁî®Êà∑ÊïÖ‰∫ã„ÄÅ‰∫ßÂìÅÂéüÂûãÔºâ„ÄÇ&lt;/li&gt;
&lt;li&gt;ËÆ°Âàí‰ºöËÆÆÔºàËØÑ‰º∞ÂíåÈ¢ÑÁÆóÔºâ„ÄÇ&lt;/li&gt;
&lt;li&gt;Êó•Â∏∏ÂºÄÂèëÔºàÁ´ôÁ´ã‰ºöËÆÆ„ÄÅÁï™ËåÑÂ∑•‰ΩúÊ≥ï„ÄÅÁªìÂØπÁºñÁ®ã„ÄÅÊµãËØïÂÖàË°å„ÄÅ‰ª£Á†ÅÈáçÊûÑ‚Ä¶‚Ä¶Ôºâ„ÄÇ&lt;/li&gt;
&lt;li&gt;‰øÆÂ§çbugÔºàÈóÆÈ¢òÊèèËø∞„ÄÅÈáçÁé∞Ê≠•È™§„ÄÅÊµãËØï‰∫∫Âëò„ÄÅË¢´ÊåáÊ¥æ‰∫∫Ôºâ„ÄÇ&lt;/li&gt;
&lt;li&gt;ÂèëÂ∏ÉÁâàÊú¨„ÄÇ&lt;/li&gt;
&lt;li&gt;ËØÑÂÆ°‰ºöËÆÆÔºàShowcaseÔºåÁî®Êà∑ÈúÄË¶ÅÂèÇ‰∏éÔºâ„ÄÇ&lt;/li&gt;
&lt;li&gt;ÂõûÈ°æ‰ºöËÆÆÔºàÂØπÂΩìÂâçËø≠‰ª£Âë®ÊúüÂÅö‰∏Ä‰∏™ÊÄªÁªìÔºâ„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ë°•ÂÖÖÔºöÊïèÊç∑ËΩØ‰ª∂ÂºÄÂèëÂÆ£Ë®Ä&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;‰∏™‰ΩìÂíå‰∫íÂä®&lt;/strong&gt; È´ò‰∫é ÊµÅÁ®ãÂíåÂ∑•ÂÖ∑&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Â∑•‰ΩúÁöÑËΩØ‰ª∂&lt;/strong&gt; È´ò‰∫é ËØ¶Â∞ΩÁöÑÊñáÊ°£&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ÂÆ¢Êà∑Âêà‰Ωú&lt;/strong&gt; È´ò‰∫é ÂêàÂêåË∞àÂà§&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ÂìçÂ∫îÂèòÂåñ&lt;/strong&gt; È´ò‰∫é ÈÅµÂæ™ËÆ°Âàí&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/agile-scrum-sprint-cycle.png"&gt;&lt;img src="./res/agile-scrum-sprint-cycle.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ËßíËâ≤Ôºö‰∫ßÂìÅÊâÄÊúâËÄÖÔºàÂÜ≥ÂÆöÂÅö‰ªÄ‰πàÔºåËÉΩÂØπÈúÄÊ±ÇÊãçÊùøÁöÑ‰∫∫Ôºâ„ÄÅÂõ¢ÈòüË¥üË¥£‰∫∫ÔºàËß£ÂÜ≥ÂêÑÁßçÈóÆÈ¢òÔºå‰∏ìÊ≥®Â¶Ç‰ΩïÊõ¥Â•ΩÁöÑÂ∑•‰ΩúÔºåÂ±èËîΩÂ§ñÈÉ®ÂØπÂºÄÂèëÂõ¢ÈòüÁöÑÂΩ±ÂìçÔºâ„ÄÅÂºÄÂèëÂõ¢ÈòüÔºàÈ°πÁõÆÊâßË°å‰∫∫ÂëòÔºåÂÖ∑‰ΩìÊåáÂºÄÂèë‰∫∫ÂëòÂíåÊµãËØï‰∫∫ÂëòÔºâ„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;ÂáÜÂ§áÂ∑•‰ΩúÔºöÂïÜ‰∏öÊ°à‰æãÂíåËµÑÈáë„ÄÅÂêàÂêå„ÄÅÊÜßÊÜ¨„ÄÅÂàùÂßã‰∫ßÂìÅÈúÄÊ±Ç„ÄÅÂàùÂßãÂèëÂ∏ÉËÆ°Âàí„ÄÅÂÖ•ËÇ°„ÄÅÁªÑÂª∫Âõ¢Èòü„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;ÊïèÊç∑Âõ¢ÈòüÈÄöÂ∏∏‰∫∫Êï∞‰∏∫8-10‰∫∫„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Â∑•‰ΩúÈáè‰º∞ÁÆóÔºöÂ∞ÜÂºÄÂèë‰ªªÂä°ÈáèÂåñÔºåÂåÖÊã¨ÂéüÂûã„ÄÅLogoËÆæËÆ°„ÄÅUIËÆæËÆ°„ÄÅÂâçÁ´ØÂºÄÂèëÁ≠âÔºåÂ∞ΩÈáèÊääÊØè‰∏™Â∑•‰ΩúÂàÜËß£Âà∞ÊúÄÂ∞è‰ªªÂä°ÈáèÔºåÊúÄÂ∞è‰ªªÂä°ÈáèÊ†áÂáÜ‰∏∫Â∑•‰ΩúÊó∂Èó¥‰∏çËÉΩË∂ÖËøá‰∏§Â§©ÔºåÁÑ∂Âêé‰º∞ÁÆóÊÄª‰ΩìÈ°πÁõÆÊó∂Èó¥„ÄÇÊääÊØè‰∏™‰ªªÂä°ÈÉΩË¥¥Âú®ÁúãÊùø‰∏äÈù¢ÔºåÁúãÊùø‰∏äÂàÜ‰∏âÈÉ®ÂàÜÔºöto doÔºàÂæÖÂÆåÊàêÔºâ„ÄÅin progressÔºàËøõË°å‰∏≠ÔºâÂíådoneÔºàÂ∑≤ÂÆåÊàêÔºâ„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;È°πÁõÆÂõ¢ÈòüÁªÑÂª∫&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Âõ¢ÈòüÁöÑÊûÑÊàêÂíåËßíËâ≤&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ËØ¥ÊòéÔºöË∞¢Ë∞¢‰ªòÁ••Ëã±Â•≥Â£´ÁªòÂà∂‰∫Ü‰∏ãÈù¢ËøôÂº†Á≤æÁæéÁöÑÂÖ¨Âè∏ÁªÑÁªáÊû∂ÊûÑÂõæ„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/company_architecture.png"&gt;&lt;img src="./res/company_architecture.png" alt="company_architecture" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ÁºñÁ®ãËßÑËåÉÂíå‰ª£Á†ÅÂÆ°Êü•Ôºàflake8„ÄÅpylintÔºâ&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/pylint.png"&gt;&lt;img src="./res/pylint.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python‰∏≠ÁöÑ‰∏Ä‰∫õ‚ÄúÊÉØ‰æã‚ÄùÔºàËØ∑ÂèÇËÄÉ&lt;a href="Python%E6%83%AF%E4%BE%8B.md"&gt;„ÄäPythonÊÉØ‰æã-Â¶Ç‰ΩïÁºñÂÜôPythonicÁöÑ‰ª£Á†Å„Äã&lt;/a&gt;Ôºâ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ÂΩ±Âìç‰ª£Á†ÅÂèØËØªÊÄßÁöÑÂéüÂõ†Ôºö&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‰ª£Á†ÅÊ≥®ÈáäÂ§™Â∞ëÊàñËÄÖÊ≤°ÊúâÊ≥®Èáä&lt;/li&gt;
&lt;li&gt;‰ª£Á†ÅÁ†¥Âùè‰∫ÜËØ≠Ë®ÄÁöÑÊúÄ‰Ω≥ÂÆûË∑µ&lt;/li&gt;
&lt;li&gt;ÂèçÊ®°ÂºèÁºñÁ®ãÔºàÊÑèÂ§ßÂà©Èù¢‰ª£Á†Å„ÄÅÂ§çÂà∂-ÈªèË¥¥ÁºñÁ®ã„ÄÅËá™Ë¥üÁºñÁ®ã„ÄÅ‚Ä¶‚Ä¶Ôºâ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Âõ¢ÈòüÂºÄÂèëÂ∑•ÂÖ∑‰ªãÁªç&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ÁâàÊú¨ÊéßÂà∂ÔºöGit„ÄÅMercury&lt;/li&gt;
&lt;li&gt;Áº∫Èô∑ÁÆ°ÁêÜÔºö&lt;a href="https://about.gitlab.com/" rel="nofollow"&gt;Gitlab&lt;/a&gt;„ÄÅ&lt;a href="http://www.redmine.org.cn/" rel="nofollow"&gt;Redmine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ÊïèÊç∑Èó≠ÁéØÂ∑•ÂÖ∑Ôºö&lt;a href="https://www.zentao.net/" rel="nofollow"&gt;Á¶ÖÈÅì&lt;/a&gt;„ÄÅ&lt;a href="https://www.atlassian.com/software/jira/features" rel="nofollow"&gt;JIRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ÊåÅÁª≠ÈõÜÊàêÔºö&lt;a href="https://jenkins.io/" rel="nofollow"&gt;Jenkins&lt;/a&gt;„ÄÅ&lt;a href="https://travis-ci.org/" rel="nofollow"&gt;Travis-CI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ËØ∑ÂèÇËÄÉ&lt;a href="Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md"&gt;„ÄäÂõ¢ÈòüÈ°πÁõÆÂºÄÂèëÁöÑÈóÆÈ¢òÂíåËß£ÂÜ≥ÊñπÊ°à„Äã&lt;/a&gt;„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-È°πÁõÆÈÄâÈ¢òÂíåÁêÜËß£‰∏öÂä°" class="anchor" aria-hidden="true" href="#È°πÁõÆÈÄâÈ¢òÂíåÁêÜËß£‰∏öÂä°"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;È°πÁõÆÈÄâÈ¢òÂíåÁêÜËß£‰∏öÂä°&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ÈÄâÈ¢òËåÉÂõ¥ËÆæÂÆö&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;CMSÔºàÁî®Êà∑Á´ØÔºâÔºöÊñ∞ÈóªËÅöÂêàÁΩëÁ´ô„ÄÅÈóÆÁ≠î/ÂàÜ‰∫´Á§æÂå∫„ÄÅÂΩ±ËØÑ/‰π¶ËØÑÁΩëÁ´ôÁ≠â„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MISÔºàÁî®Êà∑Á´Ø+ÁÆ°ÁêÜÁ´ØÔºâÔºöKMS„ÄÅKPIËÄÉÊ†∏Á≥ªÁªü„ÄÅHRS„ÄÅCRMÁ≥ªÁªü„ÄÅ‰æõÂ∫îÈìæÁ≥ªÁªü„ÄÅ‰ªìÂÇ®ÁÆ°ÁêÜÁ≥ªÁªüÁ≠â„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AppÂêéÂè∞ÔºàÁÆ°ÁêÜÁ´Ø+Êï∞ÊçÆÊé•Âè£ÔºâÔºö‰∫åÊâã‰∫§ÊòìÁ±ª„ÄÅÊä•ÂàäÊùÇÂøóÁ±ª„ÄÅÂ∞è‰ºóÁîµÂïÜÁ±ª„ÄÅÊñ∞ÈóªËµÑËÆØÁ±ª„ÄÅÊóÖÊ∏∏Á±ª„ÄÅÁ§æ‰∫§Á±ª„ÄÅÈòÖËØªÁ±ªÁ≠â„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ÂÖ∂‰ªñÁ±ªÂûãÔºöËá™Ë∫´Ë°å‰∏öËÉåÊôØÂíåÂ∑•‰ΩúÁªèÈ™å„ÄÅ‰∏öÂä°ÂÆπÊòìÁêÜËß£ÂíåÊääÊéß„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ÈúÄÊ±ÇÁêÜËß£„ÄÅÊ®°ÂùóÂàíÂàÜÂíå‰ªªÂä°ÂàÜÈÖç&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ÈúÄÊ±ÇÁêÜËß£ÔºöÂ§¥ËÑëÈ£éÊö¥ÂíåÁ´ûÂìÅÂàÜÊûê„ÄÇ&lt;/li&gt;
&lt;li&gt;Ê®°ÂùóÂàíÂàÜÔºöÁîªÊÄùÁª¥ÂØºÂõæÔºàXMindÔºâÔºåÊØè‰∏™Ê®°ÂùóÊòØ‰∏Ä‰∏™ÊûùËäÇÁÇπÔºåÊØè‰∏™ÂÖ∑‰ΩìÁöÑÂäüËÉΩÊòØ‰∏Ä‰∏™Âè∂ËäÇÁÇπÔºàÁî®Âä®ËØçË°®Ëø∞ÔºâÔºåÈúÄË¶ÅÁ°Æ‰øùÊØè‰∏™Âè∂ËäÇÁÇπÊó†Ê≥ïÂÜçÁîüÂá∫Êñ∞ËäÇÁÇπÔºåÁ°ÆÂÆöÊØè‰∏™Âè∂Â≠êËäÇÁÇπÁöÑÈáçË¶ÅÊÄß„ÄÅ‰ºòÂÖàÁ∫ßÂíåÂ∑•‰ΩúÈáè„ÄÇ&lt;/li&gt;
&lt;li&gt;‰ªªÂä°ÂàÜÈÖçÔºöÁî±È°πÁõÆË¥üË¥£‰∫∫Ê†πÊçÆ‰∏äÈù¢ÁöÑÊåáÊ†á‰∏∫ÊØè‰∏™Âõ¢ÈòüÊàêÂëòÂàÜÈÖç‰ªªÂä°„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/requirements_by_xmind.png"&gt;&lt;img src="./res/requirements_by_xmind.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Âà∂ÂÆöÈ°πÁõÆËøõÂ∫¶Ë°®ÔºàÊØèÊó•Êõ¥Êñ∞Ôºâ&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Ê®°Âùó&lt;/th&gt;
&lt;th&gt;ÂäüËÉΩ&lt;/th&gt;
&lt;th&gt;‰∫∫Âëò&lt;/th&gt;
&lt;th&gt;Áä∂ÊÄÅ&lt;/th&gt;
&lt;th&gt;ÂÆåÊàê&lt;/th&gt;
&lt;th&gt;Â∑•Êó∂&lt;/th&gt;
&lt;th&gt;ËÆ°ÂàíÂºÄÂßã&lt;/th&gt;
&lt;th&gt;ÂÆûÈôÖÂºÄÂßã&lt;/th&gt;
&lt;th&gt;ËÆ°ÂàíÁªìÊùü&lt;/th&gt;
&lt;th&gt;ÂÆûÈôÖÁªìÊùü&lt;/th&gt;
&lt;th&gt;Â§áÊ≥®&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ËØÑËÆ∫&lt;/td&gt;
&lt;td&gt;Ê∑ªÂä†ËØÑËÆ∫&lt;/td&gt;
&lt;td&gt;ÁéãÂ§ßÈî§&lt;/td&gt;
&lt;td&gt;Ê≠£Âú®ËøõË°å&lt;/td&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Âà†Èô§ËØÑËÆ∫&lt;/td&gt;
&lt;td&gt;ÁéãÂ§ßÈî§&lt;/td&gt;
&lt;td&gt;Á≠âÂæÖ&lt;/td&gt;
&lt;td&gt;0%&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Êü•ÁúãËØÑËÆ∫&lt;/td&gt;
&lt;td&gt;ÁôΩÂÖÉËä≥&lt;/td&gt;
&lt;td&gt;Ê≠£Âú®ËøõË°å&lt;/td&gt;
&lt;td&gt;20%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;ÈúÄË¶ÅËøõË°å‰ª£Á†ÅÂÆ°Êü•&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;ËØÑËÆ∫ÊäïÁ•®&lt;/td&gt;
&lt;td&gt;ÁôΩÂÖÉËä≥&lt;/td&gt;
&lt;td&gt;Á≠âÂæÖ&lt;/td&gt;
&lt;td&gt;0%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/8&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/8&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OOADÂíåÊï∞ÊçÆÂ∫ìËÆæËÆ°&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;UMLÔºàÁªü‰∏ÄÂª∫Ê®°ËØ≠Ë®ÄÔºâÁöÑÁ±ªÂõæ&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/uml-class-diagram.png"&gt;&lt;img src="./res/uml-class-diagram.png" alt="uml" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ÈÄöËøáÊ®°ÂûãÂàõÂª∫Ë°®ÔºàÊ≠£ÂêëÂ∑•Á®ãÔºâ&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python manage.py makemigrations app
python manage.py migrate&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‰ΩøÁî®PowerDesignerÁªòÂà∂Áâ©ÁêÜÊ®°ÂûãÂõæ„ÄÇ&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/power-designer-pdm.png"&gt;&lt;img src="./res/power-designer-pdm.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ÈÄöËøáÊï∞ÊçÆË°®ÂàõÂª∫Ê®°ÂûãÔºàÂèçÂêëÂ∑•Á®ãÔºâ&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python manage.py inspectdb &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; app/models.py&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-Á¨¨92Â§©dockerÂÆπÂô®ËØ¶Ëß£" class="anchor" aria-hidden="true" href="#Á¨¨92Â§©dockerÂÆπÂô®ËØ¶Ëß£"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Á¨¨92Â§©Ôºö&lt;a href="./Day91-100/92.Docker%E5%AE%B9%E5%99%A8%E8%AF%A6%E8%A7%A3.md"&gt;DockerÂÆπÂô®ËØ¶Ëß£&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;DockerÁÆÄ‰ªã&lt;/li&gt;
&lt;li&gt;ÂÆâË£ÖDocker&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®DockerÂàõÂª∫ÂÆπÂô®ÔºàNginx„ÄÅMySQL„ÄÅRedis„ÄÅGitlab„ÄÅJenkinsÔºâ&lt;/li&gt;
&lt;li&gt;ÊûÑÂª∫DockerÈïúÂÉèÔºàDockerfileÁöÑÁºñÂÜôÂíåÁõ∏ÂÖ≥Êåá‰ª§Ôºâ&lt;/li&gt;
&lt;li&gt;ÂÆπÂô®ÁºñÊéíÔºàDocker-composeÔºâ&lt;/li&gt;
&lt;li&gt;ÈõÜÁæ§ÁÆ°ÁêÜ&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-Á¨¨93Â§©mysqlÊÄßËÉΩ‰ºòÂåñ" class="anchor" aria-hidden="true" href="#Á¨¨93Â§©mysqlÊÄßËÉΩ‰ºòÂåñ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Á¨¨93Â§©Ôºö&lt;a href="./Day91-100/93.MySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.md"&gt;MySQLÊÄßËÉΩ‰ºòÂåñ&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-Á¨¨94Â§©ÁΩëÁªúapiÊé•Âè£ËÆæËÆ°" class="anchor" aria-hidden="true" href="#Á¨¨94Â§©ÁΩëÁªúapiÊé•Âè£ËÆæËÆ°"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Á¨¨94Â§©Ôºö&lt;a href="./Day91-100/94.%E7%BD%91%E7%BB%9CAPI%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1.md"&gt;ÁΩëÁªúAPIÊé•Âè£ËÆæËÆ°&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-Á¨¨95Â§©‰ΩøÁî®djangoÂºÄÂèëÂïÜ‰∏öÈ°πÁõÆday91-10095‰ΩøÁî®djangoÂºÄÂèëÂïÜ‰∏öÈ°πÁõÆmd" class="anchor" aria-hidden="true" href="#Á¨¨95Â§©‰ΩøÁî®djangoÂºÄÂèëÂïÜ‰∏öÈ°πÁõÆday91-10095‰ΩøÁî®djangoÂºÄÂèëÂïÜ‰∏öÈ°πÁõÆmd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Á¨¨95Â§©Ôºö[‰ΩøÁî®DjangoÂºÄÂèëÂïÜ‰∏öÈ°πÁõÆ](./Day91-100/95.‰ΩøÁî®DjangoÂºÄÂèëÂïÜ‰∏öÈ°π	ÁõÆ.md)&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-È°πÁõÆÂºÄÂèë‰∏≠ÁöÑÂÖ¨ÂÖ±ÈóÆÈ¢ò" class="anchor" aria-hidden="true" href="#È°πÁõÆÂºÄÂèë‰∏≠ÁöÑÂÖ¨ÂÖ±ÈóÆÈ¢ò"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;È°πÁõÆÂºÄÂèë‰∏≠ÁöÑÂÖ¨ÂÖ±ÈóÆÈ¢ò&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;Êï∞ÊçÆÂ∫ìÁöÑÈÖçÁΩÆÔºàÂ§öÊï∞ÊçÆÂ∫ì„ÄÅ‰∏ª‰ªéÂ§çÂà∂„ÄÅÊï∞ÊçÆÂ∫ìË∑ØÁî±Ôºâ&lt;/li&gt;
&lt;li&gt;ÁºìÂ≠òÁöÑÈÖçÁΩÆÔºàÂàÜÂå∫ÁºìÂ≠ò„ÄÅÈîÆËÆæÁΩÆ„ÄÅË∂ÖÊó∂ËÆæÁΩÆ„ÄÅ‰∏ª‰ªéÂ§çÂà∂„ÄÅÊïÖÈöúÊÅ¢Â§çÔºàÂì®ÂÖµÔºâÔºâ&lt;/li&gt;
&lt;li&gt;Êó•ÂøóÁöÑÈÖçÁΩÆ&lt;/li&gt;
&lt;li&gt;ÂàÜÊûêÂíåË∞ÉËØïÔºàDjango-Debug-ToolBarÔºâ&lt;/li&gt;
&lt;li&gt;Â•ΩÁî®ÁöÑPythonÊ®°ÂùóÔºàÊó•ÊúüËÆ°ÁÆó„ÄÅÂõæÂÉèÂ§ÑÁêÜ„ÄÅÊï∞ÊçÆÂä†ÂØÜ„ÄÅ‰∏âÊñπAPIÔºâ&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-rest-apiËÆæËÆ°" class="anchor" aria-hidden="true" href="#rest-apiËÆæËÆ°"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;REST APIËÆæËÆ°&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;RESTfulÊû∂ÊûÑ
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2011/09/restful.html" rel="nofollow"&gt;ÁêÜËß£RESTfulÊû∂ÊûÑ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2014/05/restful_api.html" rel="nofollow"&gt;RESTful APIËÆæËÆ°ÊåáÂçó&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html" rel="nofollow"&gt;RESTful APIÊúÄ‰Ω≥ÂÆûË∑µ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;APIÊé•Âè£ÊñáÊ°£ÁöÑÊí∞ÂÜô
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://rap2.taobao.org/" rel="nofollow"&gt;RAP2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yapi.demo.qunar.com/" rel="nofollow"&gt;YAPI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.django-rest-framework.org/" rel="nofollow"&gt;django-REST-framework&lt;/a&gt;ÁöÑÂ∫îÁî®&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-È°πÁõÆ‰∏≠ÁöÑÈáçÁÇπÈöæÁÇπÂâñÊûê" class="anchor" aria-hidden="true" href="#È°πÁõÆ‰∏≠ÁöÑÈáçÁÇπÈöæÁÇπÂâñÊûê"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;È°πÁõÆ‰∏≠ÁöÑÈáçÁÇπÈöæÁÇπÂâñÊûê&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;‰ΩøÁî®ÁºìÂ≠òÁºìËß£Êï∞ÊçÆÂ∫ìÂéãÂäõ - Redis&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®Ê∂àÊÅØÈòüÂàóÂÅöËß£ËÄ¶ÂêàÂíåÂâäÂ≥∞ - Celery + RabbitMQ&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-Á¨¨96Â§©ËΩØ‰ª∂ÊµãËØïÂíåËá™Âä®ÂåñÊµãËØï" class="anchor" aria-hidden="true" href="#Á¨¨96Â§©ËΩØ‰ª∂ÊµãËØïÂíåËá™Âä®ÂåñÊµãËØï"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Á¨¨96Â§©Ôºö&lt;a href="Day91-100/96.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%92%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95.md"&gt;ËΩØ‰ª∂ÊµãËØïÂíåËá™Âä®ÂåñÊµãËØï&lt;/a&gt;&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-ÂçïÂÖÉÊµãËØï" class="anchor" aria-hidden="true" href="#ÂçïÂÖÉÊµãËØï"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÂçïÂÖÉÊµãËØï&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;ÊµãËØïÁöÑÁßçÁ±ª&lt;/li&gt;
&lt;li&gt;ÁºñÂÜôÂçïÂÖÉÊµãËØïÔºàunittest„ÄÅpytest„ÄÅnose2„ÄÅtox„ÄÅddt„ÄÅ‚Ä¶‚Ä¶Ôºâ&lt;/li&gt;
&lt;li&gt;ÊµãËØïË¶ÜÁõñÁéáÔºàcoverageÔºâ&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-È°πÁõÆÈÉ®ÁΩ≤" class="anchor" aria-hidden="true" href="#È°πÁõÆÈÉ®ÁΩ≤"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;È°πÁõÆÈÉ®ÁΩ≤&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;ÈÉ®ÁΩ≤ÂâçÁöÑÂáÜÂ§áÂ∑•‰Ωú
&lt;ul&gt;
&lt;li&gt;ÂÖ≥ÈîÆËÆæÁΩÆÔºàSECRET_KEY / DEBUG / ALLOWED_HOSTS / ÁºìÂ≠ò / Êï∞ÊçÆÂ∫ìÔºâ&lt;/li&gt;
&lt;li&gt;HTTPS / CSRF_COOKIE_SECUR  / SESSION_COOKIE_SECURE&lt;/li&gt;
&lt;li&gt;Êó•ÂøóÁõ∏ÂÖ≥ÈÖçÁΩÆ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LinuxÂ∏∏Áî®ÂëΩ‰ª§ÂõûÈ°æ&lt;/li&gt;
&lt;li&gt;LinuxÂ∏∏Áî®ÊúçÂä°ÁöÑÂÆâË£ÖÂíåÈÖçÁΩÆ&lt;/li&gt;
&lt;li&gt;uWSGI/GunicornÂíåNginxÁöÑ‰ΩøÁî®
&lt;ul&gt;
&lt;li&gt;GunicornÂíåuWSGIÁöÑÊØîËæÉ
&lt;ul&gt;
&lt;li&gt;ÂØπ‰∫é‰∏çÈúÄË¶ÅÂ§ßÈáèÂÆöÂà∂ÂåñÁöÑÁÆÄÂçïÂ∫îÁî®Á®ãÂ∫èÔºåGunicornÊòØ‰∏Ä‰∏™‰∏çÈîôÁöÑÈÄâÊã©ÔºåuWSGIÁöÑÂ≠¶‰π†Êõ≤Á∫øÊØîGunicornË¶ÅÈô°Â≥≠ÂæóÂ§öÔºåGunicornÁöÑÈªòËÆ§ÂèÇÊï∞Â∞±Â∑≤ÁªèËÉΩÂ§üÈÄÇÂ∫îÂ§ßÂ§öÊï∞Â∫îÁî®Á®ãÂ∫è„ÄÇ&lt;/li&gt;
&lt;li&gt;uWSGIÊîØÊåÅÂºÇÊûÑÈÉ®ÁΩ≤„ÄÇ&lt;/li&gt;
&lt;li&gt;Áî±‰∫éNginxÊú¨Ë∫´ÊîØÊåÅuWSGIÔºåÂú®Á∫ø‰∏ä‰∏ÄËà¨ÈÉΩÂ∞ÜNginxÂíåuWSGIÊçÜÁªëÂú®‰∏ÄËµ∑ÈÉ®ÁΩ≤ÔºåËÄå‰∏îuWSGIÂ±û‰∫éÂäüËÉΩÈΩêÂÖ®‰∏îÈ´òÂ∫¶ÂÆöÂà∂ÁöÑWSGI‰∏≠Èó¥‰ª∂„ÄÇ&lt;/li&gt;
&lt;li&gt;Âú®ÊÄßËÉΩ‰∏äÔºåGunicornÂíåuWSGIÂÖ∂ÂÆûË°®Áé∞Áõ∏ÂΩì„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®ËôöÊãüÂåñÊäÄÊúØÔºàDockerÔºâÈÉ®ÁΩ≤ÊµãËØïÁéØÂ¢ÉÂíåÁîü‰∫ßÁéØÂ¢É&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-ÊÄßËÉΩÊµãËØï" class="anchor" aria-hidden="true" href="#ÊÄßËÉΩÊµãËØï"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÊÄßËÉΩÊµãËØï&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;ABÁöÑ‰ΩøÁî®&lt;/li&gt;
&lt;li&gt;SQLslapÁöÑ‰ΩøÁî®&lt;/li&gt;
&lt;li&gt;sysbenchÁöÑ‰ΩøÁî®&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-Ëá™Âä®ÂåñÊµãËØï" class="anchor" aria-hidden="true" href="#Ëá™Âä®ÂåñÊµãËØï"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ëá™Âä®ÂåñÊµãËØï&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;‰ΩøÁî®ShellÂíåPythonËøõË°åËá™Âä®ÂåñÊµãËØï&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®SeleniumÂÆûÁé∞Ëá™Âä®ÂåñÊµãËØï
&lt;ul&gt;
&lt;li&gt;Selenium IDE&lt;/li&gt;
&lt;li&gt;Selenium WebDriver&lt;/li&gt;
&lt;li&gt;Selenium Remote Control&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ÊµãËØïÂ∑•ÂÖ∑Robot Framework‰ªãÁªç&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-Á¨¨97Â§©ÁîµÂïÜÁΩëÁ´ôÊäÄÊúØË¶ÅÁÇπÂâñÊûê" class="anchor" aria-hidden="true" href="#Á¨¨97Â§©ÁîµÂïÜÁΩëÁ´ôÊäÄÊúØË¶ÅÁÇπÂâñÊûê"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Á¨¨97Â§©Ôºö&lt;a href="./Day91-100/97.%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E8%A6%81%E7%82%B9%E5%89%96%E6%9E%90.md"&gt;ÁîµÂïÜÁΩëÁ´ôÊäÄÊúØË¶ÅÁÇπÂâñÊûê&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-Á¨¨98Â§©È°πÁõÆÈÉ®ÁΩ≤‰∏äÁ∫øÂíåÊÄßËÉΩË∞É‰ºò" class="anchor" aria-hidden="true" href="#Á¨¨98Â§©È°πÁõÆÈÉ®ÁΩ≤‰∏äÁ∫øÂíåÊÄßËÉΩË∞É‰ºò"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Á¨¨98Â§©Ôºö&lt;a href="./Day91-100/98.%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E5%92%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98.md"&gt;È°πÁõÆÈÉ®ÁΩ≤‰∏äÁ∫øÂíåÊÄßËÉΩË∞É‰ºò&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;MySQLÊï∞ÊçÆÂ∫ìË∞É‰ºò&lt;/li&gt;
&lt;li&gt;WebÊúçÂä°Âô®ÊÄßËÉΩ‰ºòÂåñ
&lt;ul&gt;
&lt;li&gt;NginxË¥üËΩΩÂùáË°°ÈÖçÁΩÆ&lt;/li&gt;
&lt;li&gt;KeepalivedÂÆûÁé∞È´òÂèØÁî®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;‰ª£Á†ÅÊÄßËÉΩË∞É‰ºò
&lt;ul&gt;
&lt;li&gt;Â§öÁ∫øÁ®ã&lt;/li&gt;
&lt;li&gt;ÂºÇÊ≠•Âåñ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ÈùôÊÄÅËµÑÊ∫êËÆøÈóÆ‰ºòÂåñ
&lt;ul&gt;
&lt;li&gt;‰∫ëÂ≠òÂÇ®&lt;/li&gt;
&lt;li&gt;CDN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-Á¨¨99Â§©Èù¢ËØï‰∏≠ÁöÑÂÖ¨ÂÖ±ÈóÆÈ¢ò" class="anchor" aria-hidden="true" href="#Á¨¨99Â§©Èù¢ËØï‰∏≠ÁöÑÂÖ¨ÂÖ±ÈóÆÈ¢ò"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Á¨¨99Â§©Ôºö&lt;a href="./Day91-100/99.%E9%9D%A2%E8%AF%95%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E9%97%AE%E9%A2%98.md"&gt;Èù¢ËØï‰∏≠ÁöÑÂÖ¨ÂÖ±ÈóÆÈ¢ò&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-Á¨¨100Â§©pythonÈù¢ËØïÈ¢òÈõÜ" class="anchor" aria-hidden="true" href="#Á¨¨100Â§©pythonÈù¢ËØïÈ¢òÈõÜ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Á¨¨100Â§©Ôºö&lt;a href="./Day91-100/100.Python%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86.md"&gt;PythonÈù¢ËØïÈ¢òÈõÜ&lt;/a&gt;&lt;/h4&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jackfrued</author><guid isPermaLink="false">https://github.com/jackfrued/Python-100-Days</guid><pubDate>Fri, 27 Dec 2019 00:14:00 GMT</pubDate></item><item><title>lmoroney/dlaicourse #15 in Jupyter Notebook, Today</title><link>https://github.com/lmoroney/dlaicourse</link><description>&lt;p&gt;&lt;i&gt;Notebooks for learning deep learning&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;This repo does not have a README.&lt;/i&gt;&lt;/p&gt;</description><author>lmoroney</author><guid isPermaLink="false">https://github.com/lmoroney/dlaicourse</guid><pubDate>Fri, 27 Dec 2019 00:15:00 GMT</pubDate></item><item><title>fastai/course-nlp #16 in Jupyter Notebook, Today</title><link>https://github.com/fastai/course-nlp</link><description>&lt;p&gt;&lt;i&gt;A Code-First Introduction to NLP course&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-a-code-first-intro-to-natural-language-processing" class="anchor" aria-hidden="true" href="#a-code-first-intro-to-natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A Code-First Intro to Natural Language Processing&lt;/h1&gt;
&lt;p&gt;You can find out about the course in &lt;a href="https://www.fast.ai/2019/07/08/fastai-nlp/" rel="nofollow"&gt;this blog post&lt;/a&gt; and all &lt;a href="https://www.youtube.com/playlist?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9" rel="nofollow"&gt;lecture videos are available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This course was originally taught in the &lt;a href="https://www.usfca.edu/arts-sciences/graduate-programs/analytics" rel="nofollow"&gt;University of San Francisco's Masters of Science in Data Science&lt;/a&gt; program, summer 2019.  The course is taught in Python with Jupyter Notebooks, using libraries such as sklearn, nltk, pytorch, and fastai.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;p&gt;The following topics will be covered:&lt;/p&gt;
&lt;p&gt;1. What is NLP?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A changing field&lt;/li&gt;
&lt;li&gt;Resources&lt;/li&gt;
&lt;li&gt;Tools&lt;/li&gt;
&lt;li&gt;Python libraries&lt;/li&gt;
&lt;li&gt;Example applications&lt;/li&gt;
&lt;li&gt;Ethics issues&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2. Topic Modeling with NMF and SVD&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stop words, stemming, &amp;amp; lemmatization&lt;/li&gt;
&lt;li&gt;Term-document matrix&lt;/li&gt;
&lt;li&gt;Topic Frequency-Inverse Document Frequency (TF-IDF)&lt;/li&gt;
&lt;li&gt;Singular Value Decomposition (SVD)&lt;/li&gt;
&lt;li&gt;Non-negative Matrix Factorization (NMF)&lt;/li&gt;
&lt;li&gt;Truncated SVD, Randomized SVD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3. Sentiment classification with Naive Bayes, Logistic regression, and ngrams&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sparse matrix storage&lt;/li&gt;
&lt;li&gt;Counters&lt;/li&gt;
&lt;li&gt;the fastai library&lt;/li&gt;
&lt;li&gt;Naive Bayes&lt;/li&gt;
&lt;li&gt;Logistic regression&lt;/li&gt;
&lt;li&gt;Ngrams&lt;/li&gt;
&lt;li&gt;Logistic regression with Naive Bayes features, with trigrams&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;4. Regex (and re-visiting tokenization)&lt;/p&gt;
&lt;p&gt;5. Language modeling &amp;amp; sentiment classification with deep learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Language model&lt;/li&gt;
&lt;li&gt;Transfer learning&lt;/li&gt;
&lt;li&gt;Sentiment classification&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;6. Translation with RNNs&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Review Embeddings&lt;/li&gt;
&lt;li&gt;Bleu metric&lt;/li&gt;
&lt;li&gt;Teacher Forcing&lt;/li&gt;
&lt;li&gt;Bidirectional&lt;/li&gt;
&lt;li&gt;Attention&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;7. Translation with the Transformer architecture&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer Model&lt;/li&gt;
&lt;li&gt;Multi-head attention&lt;/li&gt;
&lt;li&gt;Masking&lt;/li&gt;
&lt;li&gt;Label smoothing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;8. Bias &amp;amp; ethics in NLP&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bias in word embeddings&lt;/li&gt;
&lt;li&gt;types of bias&lt;/li&gt;
&lt;li&gt;attention economy&lt;/li&gt;
&lt;li&gt;drowning in fraudulent/fake info&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-why-is-this-course-taught-in-a-weird-order" class="anchor" aria-hidden="true" href="#why-is-this-course-taught-in-a-weird-order"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why is this course taught in a weird order?&lt;/h2&gt;
&lt;p&gt;This course is structured with a &lt;em&gt;top-down&lt;/em&gt; teaching method, which is different from how most math courses operate.  Typically, in a &lt;em&gt;bottom-up&lt;/em&gt; approach, you first learn all the separate components you will be using, and then you gradually build them up into more complex structures.  The problems with this are that students often lose motivation, don't have a sense of the "big picture", and don't know what they'll need.&lt;/p&gt;
&lt;p&gt;Harvard Professor David Perkins has a book, &lt;a href="https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719" rel="nofollow"&gt;Making Learning Whole&lt;/a&gt; in which he uses baseball as an analogy.  We don't require kids to memorize all the rules of baseball and understand all the technical details before we let them play the game.  Rather, they start playing with a just general sense of it, and then gradually learn more rules/details as time goes on.&lt;/p&gt;
&lt;p&gt;If you took the fast.ai deep learning course, that is what we used.  You can hear more about my teaching philosophy &lt;a href="http://www.fast.ai/2016/10/08/teaching-philosophy/" rel="nofollow"&gt;in this blog post&lt;/a&gt; or &lt;a href="https://vimeo.com/214233053" rel="nofollow"&gt;this talk I gave at the San Francisco Machine Learning meetup&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All that to say, don't worry if you don't understand everything at first!  You're not supposed to.  We will start using some "black boxes" and then we'll dig into the lower level details later.&lt;/p&gt;
&lt;p&gt;To start, focus on what things DO, not what they ARE.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fastai</author><guid isPermaLink="false">https://github.com/fastai/course-nlp</guid><pubDate>Fri, 27 Dec 2019 00:16:00 GMT</pubDate></item><item><title>jvns/pandas-cookbook #17 in Jupyter Notebook, Today</title><link>https://github.com/jvns/pandas-cookbook</link><description>&lt;p&gt;&lt;i&gt;Recipes for using Python's pandas library&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pandas-cookbook" class="anchor" aria-hidden="true" href="#pandas-cookbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pandas cookbook&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://mybinder.org/v2/gh/jvns/pandas-cookbook/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://pandas.pydata.org/" rel="nofollow"&gt;pandas&lt;/a&gt; is a Python library for doing
data analysis. It's really fast and lets you do exploratory work
incredibly quickly.&lt;/p&gt;
&lt;p&gt;The goal of this cookbook is to give you some concrete examples for
getting started with pandas. The &lt;a href="http://pandas.pydata.org/pandas-docs/stable/" rel="nofollow"&gt;docs&lt;/a&gt;
are really comprehensive. However, I've often had people
tell me that they have some trouble getting started, so these are
examples with real-world data, and all the bugs and weirdness
that entails.&lt;/p&gt;
&lt;p&gt;I'm working with 3 datasets right now&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;311 calls in New York&lt;/li&gt;
&lt;li&gt;How many people were on Montr√©al's bike paths in 2012&lt;/li&gt;
&lt;li&gt;Montreal's weather for 2012, hourly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It comes with batteries (data) included, so you can try out all the
examples right away.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/jvns/pandas-cookbook/blob/master/cookbook/A%20quick%20tour%20of%20IPython%20Notebook.ipynb" rel="nofollow"&gt;A quick tour of the IPython Notebook&lt;/a&gt;
&lt;br&gt; Shows off IPython's awesome tab completion and magic functions.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/jvns/pandas-cookbook/blob/master/cookbook/Chapter%201%20-%20Reading%20from%20a%20CSV.ipynb" rel="nofollow"&gt;Chapter 1: Reading from a CSV&lt;/a&gt;
&lt;br&gt; Reading your data into pandas is pretty much the easiest thing. Even when the encoding is wrong!&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/jvns/pandas-cookbook/blob/master/cookbook/Chapter%202%20-%20Selecting%20data%20&amp;amp;%20finding%20the%20most%20common%20complaint%20type.ipynb" rel="nofollow"&gt;Chapter 2: Selecting data &amp;amp; finding the most common complaint type&lt;/a&gt;
&lt;br&gt;It's not totally obvious how to select data from a pandas dataframe. Here I explain the basics (how to take slices and get columns)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/jvns/pandas-cookbook/blob/master/cookbook/Chapter%203%20-%20Which%20borough%20has%20the%20most%20noise%20complaints%20%28or%2C%20more%20selecting%20data%29.ipynb" rel="nofollow"&gt;Chapter 3: Which borough has the most noise complaints? (or, more selecting data)&lt;/a&gt;
&lt;br&gt;Here we get into serious slicing and dicing and learn how to filter dataframes in complicated ways, really fast.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/jvns/pandas-cookbook/blob/master/cookbook/Chapter%204%20-%20Find%20out%20on%20which%20weekday%20people%20bike%20the%20most%20with%20groupby%20and%20aggregate.ipynb" rel="nofollow"&gt;Chapter 4: Find out on which weekday people bike the most with groupby and aggregate&lt;/a&gt;
&lt;br&gt; The groupby/aggregate is seriously my favorite thing about pandas and I use it all the time. You should probably read this.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/jvns/pandas-cookbook/blob/master/cookbook/Chapter%205%20-%20Combining%20dataframes%20and%20scraping%20Canadian%20weather%20data.ipynb" rel="nofollow"&gt;Chapter 5: Combining dataframes and scraping Canadian weather data&lt;/a&gt;
&lt;br&gt;Here you get to find out if it's cold in Montreal in the winter (spoiler: yes). Web scraping with pandas is fun!&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/jvns/pandas-cookbook/blob/master/cookbook/Chapter%206%20-%20String%20Operations-%20Which%20month%20was%20the%20snowiest.ipynb" rel="nofollow"&gt;Chapter 6: String operations! Which month was the snowiest?&lt;/a&gt;
&lt;br&gt; Strings with pandas are great. It has all these vectorized string operations and they're the best. We will turn a bunch of strings containing "Snow" into vectors of numbers in a trice.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/jvns/pandas-cookbook/blob/master/cookbook/Chapter%207%20-%20Cleaning%20up%20messy%20data.ipynb" rel="nofollow"&gt;Chapter 7: Cleaning up messy data&lt;/a&gt;
&lt;br&gt; Cleaning up messy data is never a joy, but with pandas it's easier &amp;lt;3&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/jvns/pandas-cookbook/blob/master/cookbook/Chapter%208%20-%20How%20to%20deal%20with%20timestamps.ipynb" rel="nofollow"&gt;Chapter 8: Parsing Unix timestamps&lt;/a&gt;
&lt;br&gt; This is basically a quick trick that took me 2 days to figure out.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/jvns/pandas-cookbook/blob/master/cookbook/Chapter%209%20-%20Loading%20data%20from%20SQL%20databases.ipynb" rel="nofollow"&gt;Chapter 9 - Loading data from SQL databases&lt;/a&gt;
&lt;br&gt; How to load data from an SQL database into Pandas, with examples using SQLite3, PostgreSQL, and MySQL.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-how-to-use-this-cookbook" class="anchor" aria-hidden="true" href="#how-to-use-this-cookbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use this cookbook&lt;/h1&gt;
&lt;p&gt;The easiest way is to try it out instantly online using Binder's awesome service. &lt;strong&gt;&lt;a href="https://mybinder.org/v2/gh/jvns/pandas-cookbook/master" rel="nofollow"&gt;Start by clicking here&lt;/a&gt;&lt;/strong&gt;, wait for it to launch, then click on "cookbook", and you'll be off to the races! It will let you run all the code interactively without having to install anything on your computer.&lt;/p&gt;
&lt;p&gt;To install it locally , you'll need an up-to-date version of IPython Notebook (&amp;gt;= 3.0) and
n your computer
pandas (&amp;gt;=0.13) for this to work properly. It's set up to work with Python 2.7.&lt;/p&gt;
&lt;p&gt;You can get these using &lt;code&gt;pip&lt;/code&gt; (you may want to do this inside a virtual environment to avoid conflicting with your other libraries).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;  pip install -r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This can be difficult to get set up and require you to compile
a whole bunch of things. I instead use and recommend
&lt;a href="https://store.continuum.io/" rel="nofollow"&gt;Anaconda&lt;/a&gt;, which is a Python distribution which
will give you everything you need. It's free and open source.&lt;/p&gt;
&lt;p&gt;Once you have pandas and IPython, you can get going!&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/jvns/pandas-cookbook.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; pandas-cookbook/cookbook
ipython notebook&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A tab should open up in your browser at &lt;code&gt;http://localhost:8888&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Happy pandas!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-running-the-cookbook-inside-docker-container" class="anchor" aria-hidden="true" href="#running-the-cookbook-inside-docker-container"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running the cookbook inside Docker container.&lt;/h1&gt;
&lt;p&gt;This repository contains Dockerfile and can be built into a docker container.
To build the container run following command from inside of the repository directory:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build -t jvns/pandas-cookbook -f Dockerfile-Local .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;run the container:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -d -p 8888:8888 -e "PASSWORD=MakeAPassword" &amp;lt;IMAGE ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;you can find out about the id of the image, by checking&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker images
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After starting the container, you can access ipython notebook with the cookbook
on port 8888. Remember to use https and authenticate with &lt;code&gt;MakeAPassword&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://&amp;lt;docker ip&amp;gt;:8888
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-contribute" class="anchor" aria-hidden="true" href="#contribute"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribute!&lt;/h1&gt;
&lt;p&gt;If you see something wrong, or there's something you'd like to learn that I haven't
explained here, or there's something you know about that you would like to share,
create an issue! Send me email! Send a pull request!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Joining dataframes&lt;/li&gt;
&lt;li&gt;Using stack/unstack&lt;/li&gt;
&lt;li&gt;???&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/bcf3a6ab4e230bba5dccd17c141539d8294228ff/687474703a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d73612f342e302f38387833312e706e67" data-canonical-src="http://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This work is licensed under a &lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h2&gt;
&lt;p&gt;There's &lt;a href="https://github.com/ia-cas/pandas-cookbook"&gt;a translation into Chinese of this repo&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jvns</author><guid isPermaLink="false">https://github.com/jvns/pandas-cookbook</guid><pubDate>Fri, 27 Dec 2019 00:17:00 GMT</pubDate></item><item><title>OpenGenus/cosmos #18 in Jupyter Notebook, Today</title><link>https://github.com/OpenGenus/cosmos</link><description>&lt;p&gt;&lt;i&gt;Algorithms that run our universe | Your personal library of every algorithm and data structure code that you will ever encounter | Ask us anything at our forum | Participate at Hacktoberfest&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cosmos" class="anchor" aria-hidden="true" href="#cosmos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cosmos&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://discourse.opengenus.org/" rel="nofollow"&gt;Join our discussion now&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The universe of algorithm and data structures&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Cosmos&lt;/strong&gt; is your personal offline collection of every algorithm and data structure one will ever encounter and use in a lifetime. This provides solutions in various languages spanning &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;C++&lt;/code&gt;, &lt;code&gt;Java&lt;/code&gt;, &lt;code&gt;JavaScript&lt;/code&gt;, &lt;code&gt;Swift&lt;/code&gt;, &lt;code&gt;Python&lt;/code&gt;, &lt;code&gt;Go&lt;/code&gt; and others.&lt;/p&gt;
&lt;p&gt;This work is maintained by a community of hundreds of people and is a &lt;em&gt;massive collaborative effort&lt;/em&gt; to bring the readily available coding knowledge &lt;strong&gt;offline&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Many coders ask me how to improve their own performances. I cannot say anything except "solve and review and prepare your library"&lt;/strong&gt; - &lt;em&gt;Uwi Tenpen&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-cosmic-structure" class="anchor" aria-hidden="true" href="#cosmic-structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cosmic Structure&lt;/h1&gt;
&lt;p&gt;Following is the high-level structure of cosmos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/code/artificial_intelligence"&gt;Artificial intelligence&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;ü§ñ&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/backtracking"&gt;Backtracking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/bit_manipulation"&gt;Bit manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/cellular_automaton"&gt;Cellular automaton&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="shell" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f41a.png"&gt;üêö&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/compression"&gt;Compression algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/computational_geometry"&gt;Computational geometry&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;‚öôÔ∏è&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/cryptography"&gt;Cryptography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/data_structures"&gt;Data structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/design_pattern"&gt;Design pattern&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/divide_conquer"&gt;Divide conquering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/dynamic_programming"&gt;Dynamic programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/graph_algorithms"&gt;Graph algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/greedy_algorithms"&gt;Greedy algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/mathematical_algorithms"&gt;Mathematical algorithms&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="1234" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f522.png"&gt;üî¢&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/networking"&gt;Networking&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="globe_with_meridians" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f310.png"&gt;üåê&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/numerical_analysis"&gt;Numerical analysis&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="chart_with_upwards_trend" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c8.png"&gt;üìà&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/online_challenges"&gt;Online challenges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/operating_system"&gt;Operating system&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;üíª&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/quantum_algorithms"&gt;Quantum algorithms&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="cyclone" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f300.png"&gt;üåÄ&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/randomized_algorithms"&gt;Randomized algorithms&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="slot_machine" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b0.png"&gt;üé∞&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/search"&gt;Searching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/selection_algorithms"&gt;Selecting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/sorting"&gt;Sorting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/square_root_decomposition"&gt;Square root decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/string_algorithms"&gt;String algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/unclassified"&gt;Unclassified&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="ghost" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f47b.png"&gt;üëª&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each type has several hundreds of problems with solutions in several languages spanning &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;C++&lt;/code&gt;, &lt;code&gt;Java&lt;/code&gt;, &lt;code&gt;Python&lt;/code&gt;, &lt;code&gt;Go&lt;/code&gt; and others.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-maintainers" class="anchor" aria-hidden="true" href="#maintainers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Maintainers&lt;/h1&gt;
&lt;p&gt;This is a massive collaboration. Hence, to keep the quality intact and drive the vision in the proper direction, we have maintainers.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Maintainers are your friends forever. They are vastly different from moderators.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Currently, we have &lt;strong&gt;5 active maintainers&lt;/strong&gt; and we are expanding quickly.&lt;/p&gt;
&lt;p&gt;The task of maintainers is to review pull requests, suggest further quality additions and keep the work up to date with the current state of the world. &lt;g-emoji class="g-emoji" alias="earth_africa" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png"&gt;üåç&lt;/g-emoji&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/OpenGenus/cosmos/wiki/maintainers"&gt;Check out our current maintainers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let us know if you would like to be a maintainer in the Slack channel &lt;em&gt;#algorithms&lt;/em&gt; and we will review and add you upon subsequent contributions. To join our massive community at &lt;a href="https://opengenus.slack.com" rel="nofollow"&gt;Slack&lt;/a&gt; open an issue &lt;a href="https://github.com/OpenGenus/OpenGenus-Slack"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h1&gt;
&lt;p&gt;The success of our vision to bring knowledge offline depends on you. Even a small contribution helps. All forms of contributions are highly welcomed and valued.&lt;/p&gt;
&lt;p&gt;Currently, we have over &lt;strong&gt;700 contributors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you contribute, your name with a link (if available) is added to our &lt;a href="https://github.com/OpenGenus/cosmos/wiki/contributors"&gt;contributors list&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can contribute by writing &lt;code&gt;code&lt;/code&gt;, documentation in the form of &lt;code&gt;installation guides&lt;/code&gt; and &lt;code&gt;style guides&lt;/code&gt;, making Cosmos search friendly and many others. There are endless possibilities.&lt;/p&gt;
&lt;p&gt;Additionally, you might want to take a look at this &lt;a href="https://github.com/OpenGenus/cosmos/wiki/contribute"&gt;contributing guidelines&lt;/a&gt; before you make Cosmos better.&lt;/p&gt;
&lt;p&gt;You may, also, refer to the available &lt;a href="/guides/coding_style"&gt;style guides&lt;/a&gt; before contributing code.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;We believe in freedom and improvement. &lt;a href="https://github.com/OpenGenus/cosmos/blob/master/LICENSE"&gt;GNU General Public License v3.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>OpenGenus</author><guid isPermaLink="false">https://github.com/OpenGenus/cosmos</guid><pubDate>Fri, 27 Dec 2019 00:18:00 GMT</pubDate></item><item><title>dsacademybr/PythonFundamentos #19 in Jupyter Notebook, Today</title><link>https://github.com/dsacademybr/PythonFundamentos</link><description>&lt;p&gt;&lt;i&gt;Reposit√≥rio do Curso Online Python Fundamentos&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python-fundamentos" class="anchor" aria-hidden="true" href="#python-fundamentos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Fundamentos&lt;/h1&gt;
&lt;p&gt;Data Science Academy - Reposit√≥rio dos Arquivos do Curso Gratuito Python Fundamentos para An√°lise de Dados&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.datascienceacademy.com.br" rel="nofollow"&gt;https://www.datascienceacademy.com.br&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dsacademybr</author><guid isPermaLink="false">https://github.com/dsacademybr/PythonFundamentos</guid><pubDate>Fri, 27 Dec 2019 00:19:00 GMT</pubDate></item><item><title>virgili0/Virgilio #20 in Jupyter Notebook, Today</title><link>https://github.com/virgili0/Virgilio</link><description>&lt;p&gt;&lt;i&gt;Your new Mentor for Data Science E-Learning.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-virgilio" class="anchor" aria-hidden="true" href="#virgilio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;Virgilio&lt;/em&gt;&lt;/h1&gt;
&lt;h4&gt;&lt;a id="user-content-your-new-mentor-for-data-science-e-learning" class="anchor" aria-hidden="true" href="#your-new-mentor-for-data-science-e-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Your new Mentor for Data Science E-Learning.&lt;/h4&gt;
&lt;p&gt;Join our community: ¬† &lt;a href="https://www.facebook.com/groups/mathfordatascience/?notif_id=1576071669338330" rel="nofollow"&gt;&lt;img height="24px" src="https://camo.githubusercontent.com/707de57448c727d1086eef95ab883c780b749931/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f66616365626f6f6b2d34354b25323067726f75702532306d656d626572732d626c75652e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/facebook-45K%20group%20members-blue.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/UpQ8bb7" rel="nofollow"&gt;&lt;img height="24px" src="https://camo.githubusercontent.com/9b0e0cc54a5a222ada2528d8b05b933153c66c7b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646973636f72642d35302532306f6e6c696e6525323075736572732d677265656e2e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/discord-50%20online%20users-green.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSeVJ9N7ae8Wr07tfSuHkP5i_5Fa-4Lp5V4fBevsinWyx6t17g/viewform" rel="nofollow"&gt;&lt;img height="24px" src="https://camo.githubusercontent.com/b10f25e6bbf851f7ea83234d92f249103ee6dfae/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e6577736c65747465722d35303025323073756273637269626572732d79656c6c6f772e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/newsletter-500%20subscribers-yellow.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;blockquote&gt;
&lt;p&gt;Virgilio is an open source initiative, aiming to mentor and guide anyone in the world of Data Science and Machine Learning. Our vision is to give &lt;em&gt;everyone&lt;/em&gt; the chance to get involved in this field, get self-started as a practitioner, gain new cutting edge practical skills and learn to navigate through the infinite web of resources and find the ones useful for &lt;em&gt;you&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e652d41e2104dadcc8c6b2d209e00ee6f1caa190/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f632f63652f56697267696c5f2e6a7067"&gt;&lt;img width="480px" src="https://camo.githubusercontent.com/e652d41e2104dadcc8c6b2d209e00ee6f1caa190/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f632f63652f56697267696c5f2e6a7067" data-canonical-src="https://upload.wikimedia.org/wikipedia/commons/c/ce/Virgil_.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
&lt;h3&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-is-virgilio"&gt;&lt;em&gt;What&lt;/em&gt; is Virgilio&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#structure"&gt;Structure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#paradiso"&gt;Paradiso&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#purgatorio"&gt;Purgatorio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#inferno"&gt;Inferno&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#complete-learning-paths"&gt;Complete Learning Paths&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#About"&gt;About&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contribute"&gt;Contribute&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-what-is-virgilio" class="anchor" aria-hidden="true" href="#what-is-virgilio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is Virgilio?&lt;/h1&gt;
&lt;p&gt;Studying and reading through the Internet means swimming in an infinite jungle of chaotic information, even more so in rapidly changing innovative fields.&lt;/p&gt;
&lt;p&gt;Have you ever felt overwhelmed when trying to approach Data Science without a real ‚Äúpath‚Äù to follow?
Are you tired of clicking ‚ÄúRun‚Äù, ‚ÄúRun‚Äù, ‚ÄúRun‚Äù.. on a Jupyter Notebook, with that false sense of confidence given by the comfort zone of the work of others?&lt;/p&gt;
&lt;p&gt;Have you ever got confused because of the several and contradicting names for the same algorithm or approach, from different websites and fragmented tutorials?&lt;/p&gt;
&lt;p&gt;We will address these critical issues for free, for everyone.&lt;/p&gt;
&lt;p&gt;Hi, I'm &lt;a href="https://en.wikipedia.org/wiki/Virgil" rel="nofollow"&gt;Virgilio&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Like I did with &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Dante_Alighieri" rel="nofollow"&gt;Dante&lt;/a&gt;&lt;/em&gt;, just some centuries ago, I'll be your mentor and reference point during your journey through this &lt;em&gt;selva oscura&lt;/em&gt;, providing you complete and organic learning paths for several fields, tools, skills and more.&lt;/p&gt;
&lt;p&gt;The vision of Virgilio is to give everyone the possibility to get into the incredible world of the Data Science and Machine Learning and the business and creative possibilities that they offer, to get self-started as a practitioner, gain new cutting edge practical skills or just learn to discriminate good information from poor information.&lt;/p&gt;
&lt;p&gt;We are doing this by providing only high-quality and coherent content, with clear step-by-step paths and a consistent naming system.&lt;/p&gt;
&lt;p&gt;Imagine Virgilio as an E-Mentor who will tell you what do to get the next step, the next skill, or to apply them in practice to create value.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But what does it mean in practice?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In other words, what's the &lt;strong&gt;target&lt;/strong&gt; of the Virgilio project?&lt;/p&gt;
&lt;p&gt;There are different scenarios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;student&lt;/em&gt; from a different field who wants to explore the intersections and the possibilities offered by the Machine Learning and Statistical methods.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;curious individual&lt;/em&gt; who came in touch with one of the buzzwords related to these fields and wants to discriminate between reliable and unreliable information.&lt;/li&gt;
&lt;li&gt;An &lt;em&gt;experienced practitioner&lt;/em&gt; who wants to have a reference point for the latest techniques, papers and best practices.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;manager&lt;/em&gt; who wants to understand the possibilities of ML applied to their actual problems, like integration with production systems or new solutions from scratch.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;businessman&lt;/em&gt; who wants to understand if his data are suitable for an ML project, and what could be the real business value.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;salesman&lt;/em&gt; who needs to stay up-to-date with the latest technologies and Jargon.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-structure" class="anchor" aria-hidden="true" href="#structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Structure&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;WIP notice: we are in the progress of migrating the content from the old conceptual organisation to the new one. Please be patient while we make Virgilio more awesome!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="map.PNG"&gt;&lt;img src="map.PNG" alt="Figure 1" title="1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As you can imagine it‚Äôs not easy to intercept all these different needs, so our solution ended up in a hierarchical structure which distinguish the content based on different levels of abstraction.&lt;/p&gt;
&lt;p&gt;To do this, we got our inspiration from Dante‚Äôs amazing masterpiece &lt;a href="https://en.wikipedia.org/wiki/Divina_Commedia" rel="nofollow"&gt;‚ÄúLa Divina Commedia‚Äù&lt;/a&gt;, written in over 20 years of work and published for the first time in 1323 d.c.&lt;/p&gt;
&lt;p&gt;Dante‚Äôs journey wouldn‚Äôt have been possible without his companion and guide, Virgilio, a roman famous poet (70 a.c.) who inspired generations of artists since the Roman hegemony in Europe.&lt;/p&gt;
&lt;p&gt;In his journey, Dante travel across the different levels of the catholic conception of the divine world at that time, starting from the Inferno (the prison of the damned), passing through the Purgatorio and reaching eventually the Paradiso (you can call it Valhalla or Nirvana, as you prefer :-) ).&lt;/p&gt;
&lt;p&gt;In your journey, you will start from scratch and eventually reach the theoretical knowledge and solid expendable skill.&lt;/p&gt;
&lt;p&gt;The parallelism is natural:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="order.png"&gt;&lt;img src="order.png" alt="Figure 2" title="1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Going from top to bottom increases the level of detail and decreases the level of abstraction&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;Paradiso&lt;/strong&gt; you won‚Äôt find a single line of code or a math formula, just plain English.
Here‚Äôs the place for introductions, simple explanations, demystifications, and meta-guides (for example a guide about the best way to use Virgilio).
It‚Äôs the best place for the not-techies, beginners and literally everyone who wants to get in touch with Data Science and Machine Learning without getting bored into technical details.
Do you want to communicate these innovative fields? Pass from the Paradiso!&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;Purgatorio&lt;/strong&gt; you can find technical guides for beginners (in the field or in general IT).
For example, you‚Äôll find guides about Python, maths, and statistics.
You will find guides about study techniques, soft skills and you‚Äôll learn how to develop an analytical mindset too.
It‚Äôs an obliged step before the Hell.
Depending on your starting skill, you‚Äôll probably spend here most of the time, learning to code, understand math concepts and more!
&lt;em&gt;If you‚Äôre a complete beginner&lt;/em&gt;, follow the track we proposed, starting from the Fundamentals.&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;Inferno&lt;/strong&gt; you won‚Äôt find gentle introductions or generic explanations, but a lot of different detailed guides, topics, hands-on tutorials and more!
You‚Äôll find an entire section dedicated to research and daily updates from the field! You‚Äôll find guides like ‚Äúhow to train a massive neural network over hundreds of GPUs efficiently‚Äù or ‚ÄúHow to deal with huge datasets‚Äù, or ‚Äúhow to fine-tune a preprocessing pipeline‚Äù. Think about Inferno as the place where you will pick up the sub-field you prefer and dive into that.
It‚Äôs impossible to learn everything at once! One of the Virgilio‚Äôs most important learning strategies is ‚ÄúOne enemy at time‚Äù, "Divide and conquer!".
The three specializations that we provide are Computer Vision, Natural Language Processing, and Agent-based and Reinforcement Learning.
These learning paths are the ‚Äúfinal bosses‚Äù of the Virgilio‚Äôs experience: once you‚Äôll have completed them you will hopefully be skilled enough to land an internship or tackle real business problems!&lt;/p&gt;
&lt;p&gt;Above these you‚Äôll find a plenty of other useful zones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Business Skills: these are necessary to unlock the business value in the real world, and they are probably the most valuable asset for a ‚Äúdata guy‚Äù to have in his pocket.&lt;/li&gt;
&lt;li&gt;Tools: here you find guides about useful tools for programming, or scientific computing in general.&lt;/li&gt;
&lt;li&gt;Research: here you will learn how to find the right papers and digest them. In addition, you‚Äôll discover which teams to follow for your interests.&lt;/li&gt;
&lt;li&gt;Massive computation: here you‚Äôll find hints and resources for computing on clusters, optimize your system and other advanced topics.&lt;/li&gt;
&lt;li&gt;ML for Business applications: here we provide additional resources to select a sector and see what are the today‚Äôs available techniques for the problems of your interest.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-paradiso" class="anchor" aria-hidden="true" href="#paradiso"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Paradiso&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/paradiso/demystification-ai-ml-dl/demystification-ai-ml-dl.md"&gt;Demystification of the key concepts of Artificial Intelligence and Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/paradiso/what-do-you-need-for-ml/what-do-you-need-for-ml.md"&gt;What do you need for ML? &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/paradiso/do-you-really-need-ml/do-you-really-need-ml.md"&gt;Do you really need ML?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/paradiso/use-cases/use-cases.md"&gt;ML use cases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/paradiso/virgilio-teaching-strategy/virgilio-teaching-strategy.md"&gt;Virgilio's Teaching Strategy - Learning to Learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/paradiso/introduction-to-ml/introduction-to-ml.md"&gt;Introduction to ML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-purgatorio" class="anchor" aria-hidden="true" href="#purgatorio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Purgatorio&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fundamentals
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/fundamentals/math-fundamentals/math-fundamentals.md"&gt;Math Fundamentals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/fundamentals/statistics-fundamentals/statistics-fundamentals.md"&gt;Statistics Fundamentals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/fundamentals/python-fundamentals/python-fundamentals.md"&gt;Python Fundamentals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/fundamentals/jupyter-notebook/jupyter-notebook.md"&gt;Jupyter Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/fundamentals/the-data-science-process/the-data-science-process.md"&gt;The Data Science Process&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Define The Scope and Ask Questions
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/define-the-scope-and-ask-questions/frame-the-problem/frame-the-problem.md"&gt;Frame The Problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/define-the-scope-and-ask-questions/usage-and-integration/usage-and-integration.md"&gt;Usage and Integration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/define-the-scope-and-ask-questions/starting-a-data-project/starting-a-data-project.md"&gt;Starting a Data Project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/define-the-scope-and-ask-questions/workspace-setup-and-cloud-computing/workspace-setup-and-cloud-computing.md"&gt;WorkSpace Setup and Cloud Computing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Collect and Prepare Data
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/collect-and-prepare-data/data-preparation/data-preparation.md"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/collect-and-prepare-data/data-visualization/data-visualization.md"&gt;Data Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Select and Train Machine Learning Models
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/select-and-train-machine-learning-models/machine-learning/machine-learning.md"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Evaluate and Fine-Tune&lt;/li&gt;
&lt;li&gt;Launch and Mantain the System&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-inferno" class="anchor" aria-hidden="true" href="#inferno"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inferno&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Computer Vision
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/inferno/computer-vision/introduction-to-computer-vision/introduction-to-computer-vision.ipynb"&gt;Introduction to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/inferno/computer-vision/object-instance-segmentation/object-instance-segmentation.ipynb"&gt;Object Instance Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/inferno/computer-vision/object-tracking/object-tracking.ipynb"&gt;Object Tracking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Natural Language Processing&lt;/li&gt;
&lt;li&gt;Virtual Assistants
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/inferno/virtual-assistants/dialogflow-chatbot/dialogflow-chatbot.md"&gt;Build a Virtual Assistant with DialogFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Soft Skills
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/inferno/soft-skills/impactful-presentations/impactful-presentations.md"&gt;Impactful Presentations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tools
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/inferno/tools/geo-gebra/geo-gebra.md"&gt;Geo Gebra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/inferno/tools/latex/latex.md"&gt;Latex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/inferno/tools/regex/regex.ipynb"&gt;Regex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/inferno/tools/wolfram-alpha/wolfram-alpha.md"&gt;Wolfram Alpha&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Research
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/inferno/research/zotero/zotero.md"&gt;Zotero&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/inferno/research/sota-papers/sota-papers.md"&gt;State-of-Art Papers Explained&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Massive Computation&lt;/li&gt;
&lt;li&gt;Machine Learning for Business Applications&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-complete-learning-paths" class="anchor" aria-hidden="true" href="#complete-learning-paths"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete Learning Paths&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="LearningPaths/Machine%20Learning%20Engineer%20Career%20Path"&gt;Machine Learning Study Path&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;Virgilio is developed and mantained by &lt;a href="docs/contributors.md"&gt;these awesome people&lt;/a&gt;.
You can email us &lt;code&gt;virgilio.datascience (at) gmail.com&lt;/code&gt; or join the &lt;a href="https://discord.gg/UpQ8bb7" rel="nofollow"&gt;Discord chat&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contribute" class="anchor" aria-hidden="true" href="#contribute"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribute&lt;/h3&gt;
&lt;p&gt;That's awesome! Check the &lt;a href="https://github.com/virgili0/Virgilio/blob/master/docs/contributing.md"&gt;contribution guidelines&lt;/a&gt; and get involved in our project!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;The project is licensed under the &lt;a href="LICENSE.md"&gt;GPLv3 terms&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>virgili0</author><guid isPermaLink="false">https://github.com/virgili0/Virgilio</guid><pubDate>Fri, 27 Dec 2019 00:20:00 GMT</pubDate></item><item><title>NVIDIA/tacotron2 #21 in Jupyter Notebook, Today</title><link>https://github.com/NVIDIA/tacotron2</link><description>&lt;p&gt;&lt;i&gt;Tacotron 2 - PyTorch implementation with faster-than-realtime inference&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tacotron-2-without-wavenet" class="anchor" aria-hidden="true" href="#tacotron-2-without-wavenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tacotron 2 (without wavenet)&lt;/h1&gt;
&lt;p&gt;PyTorch implementation of &lt;a href="https://arxiv.org/pdf/1712.05884.pdf" rel="nofollow"&gt;Natural TTS Synthesis By Conditioning
Wavenet On Mel Spectrogram Predictions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This implementation includes &lt;strong&gt;distributed&lt;/strong&gt; and &lt;strong&gt;automatic mixed precision&lt;/strong&gt; support
and uses the &lt;a href="https://keithito.com/LJ-Speech-Dataset/" rel="nofollow"&gt;LJSpeech dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Distributed and Automatic Mixed Precision support relies on NVIDIA's &lt;a href="https://github.com/nvidia/apex"&gt;Apex&lt;/a&gt; and &lt;a href="https://github.com/NVIDIA/apex/tree/master/apex/amp"&gt;AMP&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Visit our &lt;a href="https://nv-adlr.github.io/WaveGlow" rel="nofollow"&gt;website&lt;/a&gt; for audio samples using our published &lt;a href="https://drive.google.com/file/d/1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA/view?usp=sharing" rel="nofollow"&gt;Tacotron 2&lt;/a&gt; and
&lt;a href="https://drive.google.com/file/d/1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx/view?usp=sharing" rel="nofollow"&gt;WaveGlow&lt;/a&gt; models.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="tensorboard.png"&gt;&lt;img src="tensorboard.png" alt="Alignment, Predicted Mel Spectrogram, Target Mel Spectrogram" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-requisites" class="anchor" aria-hidden="true" href="#pre-requisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-requisites&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;NVIDIA GPU + CUDA cuDNN&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download and extract the &lt;a href="https://keithito.com/LJ-Speech-Dataset/" rel="nofollow"&gt;LJ Speech dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Clone this repo: &lt;code&gt;git clone https://github.com/NVIDIA/tacotron2.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;CD into this repo: &lt;code&gt;cd tacotron2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Initialize submodule: &lt;code&gt;git submodule init; git submodule update&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Update .wav paths: &lt;code&gt;sed -i -- 's,DUMMY,ljs_dataset_folder/wavs,g' filelists/*.txt&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Alternatively, set &lt;code&gt;load_mel_from_disk=True&lt;/code&gt; in &lt;code&gt;hparams.py&lt;/code&gt; and update mel-spectrogram paths&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Install &lt;a href="https://github.com/pytorch/pytorch#installation"&gt;PyTorch 1.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install &lt;a href="https://github.com/nvidia/apex"&gt;Apex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install python requirements or build docker image
&lt;ul&gt;
&lt;li&gt;Install python requirements: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;python train.py --output_directory=outdir --log_directory=logdir&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;(OPTIONAL) &lt;code&gt;tensorboard --logdir=outdir/logdir&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-training-using-a-pre-trained-model" class="anchor" aria-hidden="true" href="#training-using-a-pre-trained-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training using a pre-trained model&lt;/h2&gt;
&lt;p&gt;Training using a pre-trained model can lead to faster convergence&lt;br&gt;
By default, the dataset dependent text embedding layers are &lt;a href="https://github.com/NVIDIA/tacotron2/blob/master/hparams.py#L22"&gt;ignored&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download our published &lt;a href="https://drive.google.com/file/d/1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA/view?usp=sharing" rel="nofollow"&gt;Tacotron 2&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;&lt;code&gt;python train.py --output_directory=outdir --log_directory=logdir -c tacotron2_statedict.pt --warm_start&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-multi-gpu-distributed-and-automatic-mixed-precision-training" class="anchor" aria-hidden="true" href="#multi-gpu-distributed-and-automatic-mixed-precision-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-GPU (distributed) and Automatic Mixed Precision Training&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;python -m multiproc train.py --output_directory=outdir --log_directory=logdir --hparams=distributed_run=True,fp16_run=True&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-inference-demo" class="anchor" aria-hidden="true" href="#inference-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inference demo&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download our published &lt;a href="https://drive.google.com/file/d/1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA/view?usp=sharing" rel="nofollow"&gt;Tacotron 2&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;Download our published &lt;a href="https://drive.google.com/file/d/1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx/view?usp=sharing" rel="nofollow"&gt;WaveGlow&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;&lt;code&gt;jupyter notebook --ip=127.0.0.1 --port=31337&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Load inference.ipynb&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;N.b.  When performing Mel-Spectrogram to Audio synthesis, make sure Tacotron 2
and the Mel decoder were trained on the same mel-spectrogram representation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-related-repos" class="anchor" aria-hidden="true" href="#related-repos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related repos&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/WaveGlow"&gt;WaveGlow&lt;/a&gt; Faster than real time Flow-based
Generative Network for Speech Synthesis&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/nv-wavenet/"&gt;nv-wavenet&lt;/a&gt; Faster than real time
WaveNet.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This implementation uses code from the following repos: &lt;a href="https://github.com/keithito/tacotron/"&gt;Keith
Ito&lt;/a&gt;, &lt;a href="https://github.com/pseeth/pytorch-stft"&gt;Prem
Seetharaman&lt;/a&gt; as described in our code.&lt;/p&gt;
&lt;p&gt;We are inspired by &lt;a href="https://github.com/r9y9/tacotron_pytorch"&gt;Ryuchi Yamamoto's&lt;/a&gt;
Tacotron PyTorch implementation.&lt;/p&gt;
&lt;p&gt;We are thankful to the Tacotron 2 paper authors, specially Jonathan Shen, Yuxuan
Wang and Zongheng Yang.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA</author><guid isPermaLink="false">https://github.com/NVIDIA/tacotron2</guid><pubDate>Fri, 27 Dec 2019 00:21:00 GMT</pubDate></item><item><title>randerson112358/Python #22 in Jupyter Notebook, Today</title><link>https://github.com/randerson112358/Python</link><description>&lt;p&gt;&lt;i&gt;:snake: Python Programs&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python" class="anchor" aria-hidden="true" href="#python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python&lt;/h1&gt;
&lt;p&gt;This is a repository that holds my Python programs&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2cffe57e3c8276001c970391a67d67cf3a02f165/68747470733a2f2f7777772e707974686f6e2e6f72672f7374617469632f636f6d6d756e6974795f6c6f676f732f707974686f6e2d6c6f676f2d696e6b73636170652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/2cffe57e3c8276001c970391a67d67cf3a02f165/68747470733a2f2f7777772e707974686f6e2e6f72672f7374617469632f636f6d6d756e6974795f6c6f676f732f707974686f6e2d6c6f676f2d696e6b73636170652e737667" width="400" data-canonical-src="https://www.python.org/static/community_logos/python-logo-inkscape.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
To see me programming in Python checkout the YouTube channel: &lt;a href="https://www.youtube.com/playlist?list=PLBhJnyA0V0uIP6tScPs01FW5WtSpJdmcv" rel="nofollow"&gt;Go To YouTube Channel&lt;/a&gt;
&lt;h1&gt;&lt;a id="user-content-relavent-books-on-amazon" class="anchor" aria-hidden="true" href="#relavent-books-on-amazon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Relavent Books On Amazon&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/gp/product/1449355730/ref=as_li_tl?ie=UTF8&amp;amp;tag=github01d-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=1449355730&amp;amp;linkId=95e6eaf8c12b9fcd483dd06c1dd53e48" rel="nofollow"&gt;Learning Python, 5th Edition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/gp/product/1491962291/ref=as_li_tl?ie=UTF8&amp;amp;tag=github01d-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=1491962291&amp;amp;linkId=9dec6584d63a7cfcbc32af1ff9737bbf" rel="nofollow"&gt;Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/gp/product/1491912057/ref=as_li_tl?ie=UTF8&amp;amp;tag=github01d-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=1491912057&amp;amp;linkId=af650651a6d71fdea49cd5aa95653e1c" rel="nofollow"&gt;Python Data Science Handbook: Essential Tools for Working with Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&amp;amp;tag=github01d-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=1449369413&amp;amp;linkId=7b6ad9375121575c83af505f2a3ed6f3" rel="nofollow"&gt;Introduction to Machine Learning with Python: A Guide for Data Scientists&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-python-data-cleaning-programs" class="anchor" aria-hidden="true" href="#python-data-cleaning-programs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Data Cleaning Programs&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Program Name&lt;/th&gt;
&lt;th&gt;Algorithm Name&lt;/th&gt;
&lt;th&gt;Link to Program&lt;/th&gt;
&lt;th&gt;Blog&lt;/th&gt;
&lt;th&gt;YouTube&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;concatenate_file.py&lt;/td&gt;
&lt;td&gt;Concatenate Multiple CSV files&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/concatenate_file.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://everythingcomputerscience.com/" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.youtube.com/channel/UCbmb5IoBtHZTpYZCDBOC1CA" rel="nofollow"&gt;YouTubeX&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;remove_empty_row.py&lt;/td&gt;
&lt;td&gt;Removes Empty Rows&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/remove_empty_row.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://everythingcomputerscience.com/" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.youtube.com/channel/UCbmb5IoBtHZTpYZCDBOC1CA" rel="nofollow"&gt;YouTubeX&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;replace_strings_with_numbers.py&lt;/td&gt;
&lt;td&gt;Changes Strings in CSV to Numbers&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Replace_Strings_With_Numbers/replace_strings_with_numbers.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://everythingcomputerscience.com/" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/zv_fzW2iA_U" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-web-scraping" class="anchor" aria-hidden="true" href="#web-scraping"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web Scraping&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Program Name&lt;/th&gt;
&lt;th&gt;Algorithm Name&lt;/th&gt;
&lt;th&gt;Link to Program&lt;/th&gt;
&lt;th&gt;Blog&lt;/th&gt;
&lt;th&gt;YouTube&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;scrape.py&lt;/td&gt;
&lt;td&gt;Scrape Website Links&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/scrape.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/scrape-website-using-python-90619cac7c97" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/LGZEn1OYUTk" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;News_Article.py&lt;/td&gt;
&lt;td&gt;Scrape &amp;amp; Summarize Article&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/News_Article.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://everythingcomputerscience.com/" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/YzMA2O_v5co" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-machine-learning-projects--programs" class="anchor" aria-hidden="true" href="#machine-learning-projects--programs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Projects &amp;amp; Programs&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Project Name&lt;/th&gt;
&lt;th&gt;Program Name&lt;/th&gt;
&lt;th&gt;Algorithm Name&lt;/th&gt;
&lt;th&gt;Link to Program&lt;/th&gt;
&lt;th&gt;Blog&lt;/th&gt;
&lt;th&gt;YouTube&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sentiment Analysis&lt;/td&gt;
&lt;td&gt;sentiment.py&lt;/td&gt;
&lt;td&gt;Sentiment Analysis&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/sentiment.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/sentiment-analysis-e2e4442bac13" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/1VHhDSOwJPw" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Simple Linear Regression Ex&lt;/td&gt;
&lt;td&gt;LinearRegression.py&lt;/td&gt;
&lt;td&gt;Linear Regression&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/LinearRegression.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/a-simple-machine-learning-python-program-bf5d156d2cda" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/z7jEJY8FbA8" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Car Classification&lt;/td&gt;
&lt;td&gt;decisionTree.py&lt;/td&gt;
&lt;td&gt;Decision Tree&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/DecisionTree/decisionTree.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/car-classification-89ad60204acf" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/U-Jm8ugN0Ps" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Golf Predictions&lt;/td&gt;
&lt;td&gt;Golf_Predictions.ipynb&lt;/td&gt;
&lt;td&gt;Decision Tree&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Golf_Predictions.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/python-decision-tree-classifier-example-d73bc3aeca6" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/bT-43kgYI3o" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict Boston House Price&lt;/td&gt;
&lt;td&gt;Predict_Boston_Housing_Price.ipynb&lt;/td&gt;
&lt;td&gt;Linear Regression&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Predict_Boston_Housing_Price.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/predict-boston-house-prices-using-python-linear-regression-90469e0a341" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/gOXoFDrseis" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict Stock Price&lt;/td&gt;
&lt;td&gt;stock.ipynb&lt;/td&gt;
&lt;td&gt;Linear Regression &amp;amp; SVR&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/stock.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/predict-stock-prices-using-python-machine-learning-53aa024da20a" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/EYnC4ACIt2g" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Classify Iris Species&lt;/td&gt;
&lt;td&gt;Logistic_Regression.ipynb&lt;/td&gt;
&lt;td&gt;Logistic Regression&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Logistic_Regression.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/python-logistic-regression-program-5e1b32f964db" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/ACdBKML9l4s" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict Median House Price&lt;/td&gt;
&lt;td&gt;Neural_Networks.ipynb&lt;/td&gt;
&lt;td&gt;Deep Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Neural_Networks/Neural_Networks.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/predict-house-median-prices-5f1a768dd256?postPublishedType=repub" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/vSzou5zRwNQ" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Classify Handwritten Digits&lt;/td&gt;
&lt;td&gt;MNIST_ANN.ipynb&lt;/td&gt;
&lt;td&gt;Artificial Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/MNIST_ANN.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/classify-hand-written-digits-5fdbe5d99ee7" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/kOFUQB7u5Ck" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cluster NBA Basketball Players&lt;/td&gt;
&lt;td&gt;Basketball_Data_Exploration.ipynb&lt;/td&gt;
&lt;td&gt;KMeans&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/NBA_Basketball_Exploration/Basketball_Data_Exploration.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/nba-data-analysis-exploration-9293f311e0e8" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/2Pmf6Kqak3w" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict FB Stock Price&lt;/td&gt;
&lt;td&gt;SVM.ipynb&lt;/td&gt;
&lt;td&gt;Support Vector Regression (SVR)&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/SVM_Stock/SVM.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/facebook-stock-prediction-bcfc676bc611" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/tMPfZV_ipOg" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Breast Cancer Detection&lt;/td&gt;
&lt;td&gt;Breast_Cancer_Detection.ipynb&lt;/td&gt;
&lt;td&gt;Random Forest Classifier &amp;amp; Gaussian Naive Bayes &amp;amp; Logistic Regression &amp;amp; Decision Tree Classifier &amp;amp; SVC&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/breast_cancer_detection/Breast_Cancer_Detection.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/breast-cancer-detection-using-machine-learning-38820fe98982" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/NSSOyhJBmWY" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face Detection&lt;/td&gt;
&lt;td&gt;face_detection.py&lt;/td&gt;
&lt;td&gt;Open CV &amp;amp; Adaboost&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/face_detection/face_detection.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/face-detection-using-python-open-cv-d51e27266f7f" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/6klXqQMctPk" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image Classification&lt;/td&gt;
&lt;td&gt;cnn.ipynb&lt;/td&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Classify_Images/cnn.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/classify-images-using-convolutional-neural-networks-python-a89cecc8c679" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/mB7fdy67eFw" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Classify Handwritten Digits CNN&lt;/td&gt;
&lt;td&gt;mnist_cnn.ipynb&lt;/td&gt;
&lt;td&gt;Convolutional Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/mnist_cnn.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/classify-hand-written-digits-using-python-and-convolutional-neural-networks-26ccfc06b95c" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/V4dd2Bt9OHY" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Spam Detection&lt;/td&gt;
&lt;td&gt;Email_Spam_Detection.ipynb&lt;/td&gt;
&lt;td&gt;Naive Bayes&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Email_Spam_Detection/Email_Spam_Detection.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/email-spam-detection-using-python-machine-learning-abe38c889855" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/cNLPt02RwF0" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pima-Indians Diabetes&lt;/td&gt;
&lt;td&gt;Diabetes.ipynb&lt;/td&gt;
&lt;td&gt;Artificial Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Diabetes/Diabetes.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/build-your-own-artificial-neural-network-using-python-f37d16be06bf" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=S2sZNlr-4_4&amp;amp;list=PLBhJnyA0V0uIP6tScPs01FW5WtSpJdmcv&amp;amp;index=28&amp;amp;t=0s" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Movie Recommendation Engine&lt;/td&gt;
&lt;td&gt;Movie_Recommendation.ipynb&lt;/td&gt;
&lt;td&gt;Cosine Similarity&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Movie_Recommender/Movie_Recommendation.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/build-a-movie-recommendation-engine-using-python-scikit-learn-machine-learning-e68ba297e163" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/umSM8rFtVMs" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Article Text To Speech&lt;/td&gt;
&lt;td&gt;Article_Text_To_Speech.py&lt;/td&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Article_Text_To_Speech.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/build-a-text-to-speech-program-using-python-b70de7105383" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/uPSIUjo_Fhw" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AI Smart Dr.Chat Bot&lt;/td&gt;
&lt;td&gt;smartbot.py&lt;/td&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/smartbot.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/build-your-own-ai-chat-bot-using-python-machine-learning-682ddd8acc29" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/QpMsT0WuIuI" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Neural Network Stock Prediction&lt;/td&gt;
&lt;td&gt;lstm2.py&lt;/td&gt;
&lt;td&gt;LSTM&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/LSTM_Stock/lstm2.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/stock-price-prediction-using-python-machine-learning-e82a039ac2bb" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/QIUxPv5PJOY" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>randerson112358</author><guid isPermaLink="false">https://github.com/randerson112358/Python</guid><pubDate>Fri, 27 Dec 2019 00:22:00 GMT</pubDate></item><item><title>makeyourownneuralnetwork/makeyourownneuralnetwork #23 in Jupyter Notebook, Today</title><link>https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork</link><description>&lt;p&gt;&lt;i&gt;Code for the Make Your Own Neural Network book&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-makeyourownneuralnetwork" class="anchor" aria-hidden="true" href="#makeyourownneuralnetwork"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;makeyourownneuralnetwork&lt;/h1&gt;
&lt;p&gt;Code for the Make Your Own Neural Network book&lt;/p&gt;
&lt;p&gt;blog: &lt;a href="https://makeyourownneuralnetwork.blogspot.com/" rel="nofollow"&gt;https://makeyourownneuralnetwork.blogspot.com/&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>makeyourownneuralnetwork</author><guid isPermaLink="false">https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork</guid><pubDate>Fri, 27 Dec 2019 00:23:00 GMT</pubDate></item><item><title>REMitchell/python-scraping #24 in Jupyter Notebook, Today</title><link>https://github.com/REMitchell/python-scraping</link><description>&lt;p&gt;&lt;i&gt;Code samples from the book Web Scraping with Python http://shop.oreilly.com/product/0636920034391.do&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-web-scraping-with-python-code-samples" class="anchor" aria-hidden="true" href="#web-scraping-with-python-code-samples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web Scraping with Python Code Samples&lt;/h1&gt;
&lt;p&gt;These code samples are for the book &lt;a href="http://shop.oreilly.com/product/0636920078067.do" rel="nofollow"&gt;Web Scraping with Python 2nd Edition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you're looking for the first edition code files, they can be found in the &lt;a href="https://github.com/REMitchell/python-scraping/tree/master/v1"&gt;v1&lt;/a&gt; directory.&lt;/p&gt;
&lt;p&gt;Most code for the second edition is contained in &lt;a href="https://jupyter.org/install.html" rel="nofollow"&gt;Jupyter notebooks&lt;/a&gt;. Although these files can be viewed directly in your browser in Github, some formatting changes and oddities may occur. I recommend that you clone the repository, install Jupyter, and view them locally for the best experience.&lt;/p&gt;
&lt;p&gt;The web changes, libraries update, and make mistakes and typos more frequently than I'd like to admit! If you think you've spotted an error, please feel free to make a pull request against this repository.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>REMitchell</author><guid isPermaLink="false">https://github.com/REMitchell/python-scraping</guid><pubDate>Fri, 27 Dec 2019 00:24:00 GMT</pubDate></item><item><title>onnx/models #25 in Jupyter Notebook, Today</title><link>https://github.com/onnx/models</link><description>&lt;p&gt;&lt;i&gt;A collection of pre-trained, state-of-the-art models in the ONNX format &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-onnx-model-zoo" class="anchor" aria-hidden="true" href="#onnx-model-zoo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ONNX Model Zoo&lt;/h1&gt;
&lt;p&gt;Open Neural Network Exchange (ONNX) is an open standard format for representing machine learning models. ONNX is supported by a community of partners who have implemented it in many frameworks and tools.&lt;/p&gt;
&lt;p&gt;The ONNX Model Zoo is a collection of pre-trained, state-of-the-art models in the &lt;a href="http://onnx.ai" rel="nofollow"&gt;ONNX&lt;/a&gt; format contributed by community members like you. Accompanying each model are &lt;a href="http://jupyter.org" rel="nofollow"&gt;Jupyter notebooks&lt;/a&gt; for model training and running inference with the trained model. The notebooks are written in Python and include links to the training dataset as well as references to the original paper that describes the model architecture.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-models" class="anchor" aria-hidden="true" href="#models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-read-the-usage-section-below-for-more-details-on-the-file-formats-in-the-onnx-model-zoo-onnx-pb-npz-and-starter-python-code-for-validating-your-onnx-model-using-test-data" class="anchor" aria-hidden="true" href="#read-the-usage-section-below-for-more-details-on-the-file-formats-in-the-onnx-model-zoo-onnx-pb-npz-and-starter-python-code-for-validating-your-onnx-model-using-test-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Read the &lt;a href="#usage-"&gt;Usage&lt;/a&gt; section below for more details on the file formats in the ONNX Model Zoo (.onnx, .pb, .npz) and starter Python code for validating your ONNX model using test data.&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-vision" class="anchor" aria-hidden="true" href="#vision"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vision&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#image_classification"&gt;Image Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#object_detection"&gt;Object Detection &amp;amp; Image Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#body_analysis"&gt;Body, Face &amp;amp; Gesture Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image_manipulation"&gt;Image Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-language" class="anchor" aria-hidden="true" href="#language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Language&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#machine_comprehension"&gt;Machine Comprehension&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#machine_translation"&gt;Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#language"&gt;Language Modelling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-other" class="anchor" aria-hidden="true" href="#other"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#visual_qna"&gt;Visual Question Answering &amp;amp; Dialog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#speech"&gt;Speech &amp;amp; Audio Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#others"&gt;Other interesting models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-image-classification-" class="anchor" aria-hidden="true" href="#image-classification-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Classification &lt;a name="user-content-image_classification"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-image_classification"&gt;
&lt;p&gt;This collection of models take images as input, then classifies the major objects in the images into 1000 object categories such as keyboard, mouse, pencil, and many animals.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/mobilenet"&gt;MobileNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1801.04381" rel="nofollow"&gt;Sandler et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Light-weight deep neural network best suited for mobile and embedded vision applications. &lt;br&gt;Top-5 error from paper - ~10%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/resnet"&gt;ResNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1512.03385" rel="nofollow"&gt;He et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A CNN model (up to 152 layers). Uses shortcut connections to achieve higher accuracy when classifying images. &lt;br&gt; Top-5 error from paper - ~3.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/squeezenet"&gt;SqueezeNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1602.07360" rel="nofollow"&gt;Iandola et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A light-weight CNN model providing AlexNet level accuracy with 50x fewer parameters. &lt;br&gt;Top-5 error from paper - ~20%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/vgg"&gt;VGG&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1409.1556" rel="nofollow"&gt;Simonyan et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN model(up to 19 layers). Similar to AlexNet but uses multiple smaller kernel-sized filters that provides more accuracy when classifying images. &lt;br&gt;Top-5 error from paper - ~8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/alexnet"&gt;AlexNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="nofollow"&gt;Krizhevsky et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A Deep CNN model (up to 8 layers) where the input is an image and the output is a vector of 1000 numbers. &lt;br&gt; Top-5 error from paper - ~15%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/inception_and_googlenet/googlenet"&gt;GoogleNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1409.4842.pdf" rel="nofollow"&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN model(up to 22 layers). Comparatively smaller and faster than VGG and more accurate in detailing than AlexNet. &lt;br&gt; Top-5 error from paper - ~6.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/caffenet"&gt;CaffeNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucb-icsi-vision-group.github.io/caffe-paper/caffe.pdf" rel="nofollow"&gt;Krizhevsky et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN variation of AlexNet for Image Classification in Caffe where the max pooling precedes the local response normalization (LRN) so that the LRN takes less compute and memory.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/rcnn_ilsvrc13"&gt;RCNN_ILSVRC13&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1311.2524" rel="nofollow"&gt;Girshick et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pure Caffe implementation of R-CNN for image classification. This model uses localization of regions to classify and extract features from images.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/densenet-121"&gt;DenseNet-121&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1608.06993" rel="nofollow"&gt;Huang et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Model that has every layer connected to every other layer and passes on its own feature providing strong gradient flow and more diversified features.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/inception_and_googlenet/inception_v1"&gt;Inception_V1&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1409.4842" rel="nofollow"&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;This model is same as GoogLeNet, implemented through Caffe2 that has improved utilization of the computing resources inside the network and helps with the vanishing gradient problem. &lt;br&gt; Top-5 error from paper - ~6.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/inception_and_googlenet/inception_v2"&gt;Inception_V2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1512.00567" rel="nofollow"&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN model for Image Classification as an adaptation to Inception v1 with batch normalization. This model has reduced computational cost and improved image resolution compared to Inception v1. &lt;br&gt; Top-5 error from paper ~4.82%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/shufflenet"&gt;ShuffleNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1707.01083" rel="nofollow"&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Extremely computation efficient CNN model that is designed specifically for mobile devices. This model greatly reduces the computational cost and provides a ~13x speedup over AlexNet on ARM-based mobile devices. Compared to MobileNet, ShuffleNet achieves superior performance by a significant margin due to it's efficient structure. &lt;br&gt; Top-1 error from paper - ~7.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/zfnet-512"&gt;ZFNet-512&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1311.2901" rel="nofollow"&gt;Zeiler et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN model (up to 8 layers) that increased the number of features that the network is capable of detecting that helps to pick image features at a finer level of resolution. &lt;br&gt; Top-5 error from paper - ~14.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h4&gt;&lt;a id="user-content-domain-based-image-classification-" class="anchor" aria-hidden="true" href="#domain-based-image-classification-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-image_classification"&gt;Domain-based Image Classification &lt;/a&gt;&lt;a name="user-content-domain_based_image"&gt;&lt;/a&gt;&lt;/h4&gt;&lt;a name="user-content-domain_based_image"&gt;
&lt;p&gt;This subset of models classify images for specific domains and datasets.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/mnist"&gt;MNIST-Handwritten Digit Recognition&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_103D_MNIST_ConvolutionalNeuralNetwork.ipynb"&gt;Convolutional Neural Network with MNIST&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN model for handwritten digit identification&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-object-detection--image-segmentation-" class="anchor" aria-hidden="true" href="#object-detection--image-segmentation-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-domain_based_image"&gt;Object Detection &amp;amp; Image Segmentation &lt;/a&gt;&lt;a name="user-content-object_detection"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-object_detection"&gt;
&lt;p&gt;Object detection models detect the presence of multiple objects in an image and segment out areas of the image where the objects are detected. Semantic segmentation models partition an input image by labeling each pixel into a set of pre-defined categories.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/tiny_yolov2"&gt;Tiny YOLOv2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1612.08242.pdf" rel="nofollow"&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A real-time CNN for object detection that detects 20 different classes. A smaller version of the more complex full YOLOv2 network.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/ssd"&gt;SSD&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1512.02325" rel="nofollow"&gt;Liu et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Single Stage Detector: real-time CNN for object detection that detects 80 different classes.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/faster-rcnn"&gt;Faster-RCNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1506.01497" rel="nofollow"&gt;Ren et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Increases efficiency from R-CNN by connecting a RPN with a CNN to create a single, unified network for object detection that detects 80 different classes.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/mask-rcnn"&gt;Mask-RCNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1703.06870" rel="nofollow"&gt;He et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A real-time neural network for object instance segmentation that detects 80 different classes. Extends Faster R-CNN as each of the 300 elected ROIs go through 3 parallel branches of the network: label prediction, bounding box prediction and mask prediction.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLO v2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1612.08242" rel="nofollow"&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A CNN model for real-time object detection system that can detect over 9000 object categories. It uses a single network evaluation, enabling it to be more than 1000x faster than R-CNN and 100x faster than Faster R-CNN. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/yolov3"&gt;YOLO v3&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1804.02767.pdf" rel="nofollow"&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A deep CNN model for real-time object detection that detects 80 different classes. A little bigger than YOLOv2 but still very fast. As accurate as SSD but 3 times faster.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/tiny_yolov3"&gt;Tiny YOLOv3&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1804.02767.pdf" rel="nofollow"&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A smaller version of YOLOv3 model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/duc"&gt;DUC&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1702.08502" rel="nofollow"&gt;Wang et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN based pixel-wise semantic segmentation model with &amp;gt;80% &lt;a href="/models/semantic_segmentation/DUC/README.md/#metric"&gt;mIOU&lt;/a&gt; (mean Intersection Over Union). Trained on cityscapes dataset, which can be effectively implemented in self driving vehicle systems.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FCN&lt;/td&gt;
&lt;td&gt;&lt;a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" rel="nofollow"&gt;Long et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN based segmentation model trained end-to-end, pixel-to-pixel that produces efficient inference and learning. Built off of AlexNet, VGG net, GoogLeNet classification methods. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-body-face--gesture-analysis-" class="anchor" aria-hidden="true" href="#body-face--gesture-analysis-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-object_detection"&gt;Body, Face &amp;amp; Gesture Analysis &lt;/a&gt;&lt;a name="user-content-body_analysis"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-body_analysis"&gt;
&lt;p&gt;Face detection models identify and/or recognize human faces and emotions in given images. Body and Gesture Analysis models identify gender and age in given image.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/body_analysis/arcface"&gt;ArcFace&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1801.07698" rel="nofollow"&gt;Deng et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A CNN based model for face recognition which learns discriminative features of faces and produces embeddings for input face images.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN Cascade&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Li_A_Convolutional_Neural_2015_CVPR_paper.pdf" rel="nofollow"&gt;Li et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The model operates at multiple resolutions, quickly rejecting the background regions in the fast low resolution stages in an image and carefully evaluates a small number of challenging candidates in the last high resolution stage. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/body_analysis/emotion_ferplus"&gt;Emotion FerPlus&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1608.01041" rel="nofollow"&gt;Barsoum et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN for emotion recognition trained on images of faces.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Age and Gender Classification using Convolutional Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.openu.ac.il/home/hassner/projects/cnn_agegender/CNN_AgeGenderEstimation.pdf" rel="nofollow"&gt;Levi et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;This model accurately classifies gender and age even the amount of learning data is limited.&lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-image-manipulation-" class="anchor" aria-hidden="true" href="#image-manipulation-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-body_analysis"&gt;Image Manipulation &lt;/a&gt;&lt;a name="user-content-image_manipulation"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-image_manipulation"&gt;
&lt;p&gt;Image manipulation models use neural networks to transform input images to modified output images. Some popular models in this category involve style transfer or enhancing images by increasing resolution.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Unpaired Image to Image Translation using Cycle consistent Adversarial Network&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1703.10593" rel="nofollow"&gt;Zhu et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The model uses learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/super_resolution/sub_pixel_cnn_2016"&gt;Super Resolution with sub-pixel CNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1609.05158" rel="nofollow"&gt;Shi et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A deep CNN that uses sub-pixel convolution layers to upscale the input image.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/style_transfer/fast_neural_style"&gt;Fast Neural Style Transfer&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1603.08155" rel="nofollow"&gt;Johnson et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;This method uses a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-speech--audio-processing-" class="anchor" aria-hidden="true" href="#speech--audio-processing-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-image_manipulation"&gt;Speech &amp;amp; Audio Processing &lt;/a&gt;&lt;a name="user-content-speech"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-speech"&gt;
&lt;p&gt;This class of models uses audio data to train models that can identify voice, generate music, or even read text out loud.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speech recognition with deep recurrent neural networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.cs.toronto.edu/~fritz/absps/RNN13.pdf" rel="nofollow"&gt;Graves et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A RNN model for sequential data for speech recognition. Labels problems where the input-output alignment is unknown&lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Deep voice: Real time neural text to speech&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1702.07825" rel="nofollow"&gt;Arik et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A DNN model that performs end-to-end neural speech synthesis. Requires fewer parameters and it is faster than other systems. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sound Generative models&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1609.03499" rel="nofollow"&gt;WaveNet: A Generative Model for Raw Audio &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A CNN model that generates raw audio waveforms. Has predictive distribution for each audio sample. Generates realistic music fragments. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-machine-comprehension-" class="anchor" aria-hidden="true" href="#machine-comprehension-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-speech"&gt;Machine Comprehension &lt;/a&gt;&lt;a name="user-content-machine_comprehension"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-machine_comprehension"&gt;
&lt;p&gt;This subset of natural language processing models that answer questions about a given context paragraph.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="text/machine_comprehension/bidirectional_attention_flow"&gt;Bidirectional Attention Flow&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1611.01603" rel="nofollow"&gt;Seo et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A model that answers a query about a given context paragraph.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="text/machine_comprehension/bert-squad"&gt;BERT-Squad&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1810.04805.pdf" rel="nofollow"&gt;Devlin et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;This model answers questions based on the context of the given input paragraph.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="text/machine_comprehension/gpt-2"&gt;GPT-2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="nofollow"&gt;Radford et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A large transformer-based language model that given a sequence of words within some text, predicts the next word.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-machine-translation-" class="anchor" aria-hidden="true" href="#machine-translation-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-machine_comprehension"&gt;Machine Translation &lt;/a&gt;&lt;a name="user-content-machine_translation"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-machine_translation"&gt;
&lt;p&gt;This class of natural language processing models learns how to translate input text to another language.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Neural Machine Translation by jointly learning to align and translate&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1409.0473" rel="nofollow"&gt;Bahdanau et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Aims to build a single neural network that can be jointly tuned to maximize the translation performance. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Google's Neural Machine Translation System&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1609.08144" rel="nofollow"&gt;Wu et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;This model helps to improve issues faced by the Neural Machine Translation (NMT) systems like parallelism that helps accelerate the final translation speed.&lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-language-modelling-" class="anchor" aria-hidden="true" href="#language-modelling-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-machine_translation"&gt;Language Modelling &lt;/a&gt;&lt;a name="user-content-language"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-language"&gt;
&lt;p&gt;This subset of natural language processing models learns representations of language from large corpuses of text.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Deep Neural Network Language Models&lt;/td&gt;
&lt;td&gt;&lt;a href="https://pdfs.semanticscholar.org/a177/45f1d7045636577bcd5d513620df5860e9e5.pdf" rel="nofollow"&gt;Arisoy et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A DNN acoustic model. Used in many natural language technologies. Represents a probability distribution over all possible word strings in a language. &lt;br&gt; &lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-visual-question-answering--dialog-" class="anchor" aria-hidden="true" href="#visual-question-answering--dialog-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-language"&gt;Visual Question Answering &amp;amp; Dialog &lt;/a&gt;&lt;a name="user-content-visual_qna"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-visual_qna"&gt;
&lt;p&gt;This subset of natural language processing models uses input images to answer questions about those images.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;VQA: Visual Question Answering&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1505.00468v6.pdf" rel="nofollow"&gt;Agrawal et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A model that takes an image and a free-form, open-ended natural language question about the image and outputs a natural-language answer. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Yin and Yang: Balancing and Answering Binary Visual Questions&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1511.05099.pdf" rel="nofollow"&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Addresses VQA by converting the question to a tuple that concisely summarizes the visual concept to be detected in the image. Next, if the concept can be found in the image, it provides a ‚Äúyes‚Äù or ‚Äúno‚Äù answer. Its performance matches the traditional VQA approach on unbalanced dataset, and outperforms it on the balanced dataset. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Making the V in VQA Matter&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1612.00837.pdf" rel="nofollow"&gt;Goyal et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Balances the VQA dataset by collecting complementary images such that every question is associated with a pair of similar images that result in two different answers to the question, providing a unique interpretable model that provides a counter-example based explanation.  &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Visual Dialog&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1611.08669" rel="nofollow"&gt;Das et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;An AI agent that holds a meaningful dialog with humans in natural, conversational language about visual content. Curates a large-scale Visual Dialog dataset (VisDial). &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-other-interesting-models-" class="anchor" aria-hidden="true" href="#other-interesting-models-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-visual_qna"&gt;Other interesting models &lt;/a&gt;&lt;a name="user-content-others"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-others"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-others"&gt;There are many interesting deep learning models that do not fit into the categories described above. The ONNX team would like to highly encourage users and researchers to &lt;/a&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt; their models to the growing model zoo.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Text to Image&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1605.05396" rel="nofollow"&gt;Generative Adversarial Text to image Synthesis &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Effectively bridges the advances in text and image modeling, translating visual concepts from characters to pixels. Generates plausible images of birds and flowers from detailed text descriptions. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Time Series Forecasting&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1703.07015.pdf" rel="nofollow"&gt;Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The model extracts short-term local dependency patterns among variables and to discover long-term patterns for time series trends. It helps to predict solar plant energy output, electricity consumption, and traffic jam situations. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recommender systems&lt;/td&gt;
&lt;td&gt;&lt;a href="http://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf" rel="nofollow"&gt;DropoutNet: Addressing Cold Start in Recommender Systems&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A collaborative filtering method that makes predictions about an individual‚Äôs preference based on preference information from other users.&lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Collaborative filtering&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1708.05031.pdf" rel="nofollow"&gt;Neural Collaborative Filtering&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A DNN model based on the interaction between user and item features using matrix factorization. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Autoencoders&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1506.01057" rel="nofollow"&gt;A Hierarchical Neural Autoencoder for Paragraphs and Documents&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;An LSTM (long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs.&lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-usage-" class="anchor" aria-hidden="true" href="#usage-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage &lt;a name="user-content-usage-"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-usage-"&gt;
&lt;p&gt;Every ONNX backend should support running the models out of the box. After downloading and extracting the tarball of each model, you will find:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A protobuf file &lt;code&gt;model.onnx&lt;/code&gt; that represents the serialized ONNX model.&lt;/li&gt;
&lt;li&gt;Test data (in the form of serialized protobuf TensorProto files or serialized NumPy archives).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The test data files can be used to validate ONNX models from the Model Zoo. We have provided the following interface examples for you to get started. Please replace &lt;code&gt;onnx_backend&lt;/code&gt; in your code with the appropriate framework of your choice that provides ONNX inferencing support, and likewise replace &lt;code&gt;backend.run_model&lt;/code&gt; with the framework's model evaluation logic.&lt;/p&gt;
&lt;p&gt;There are two different formats for the test data files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Serialized protobuf TensorProtos (.pb), stored in folders with the naming convention &lt;code&gt;test_data_set_*&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;import&lt;/span&gt; onnx
&lt;span class="pl-k"&gt;import&lt;/span&gt; os
&lt;span class="pl-k"&gt;import&lt;/span&gt; glob
&lt;span class="pl-k"&gt;import&lt;/span&gt; onnx_backend &lt;span class="pl-k"&gt;as&lt;/span&gt; backend

&lt;span class="pl-k"&gt;from&lt;/span&gt; onnx &lt;span class="pl-k"&gt;import&lt;/span&gt; numpy_helper

model &lt;span class="pl-k"&gt;=&lt;/span&gt; onnx.load(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;model.onnx&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
test_data_dir &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;test_data_set_0&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load inputs&lt;/span&gt;
inputs &lt;span class="pl-k"&gt;=&lt;/span&gt; []
inputs_num &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(glob.glob(os.path.join(test_data_dir, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_*.pb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)))
&lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(inputs_num):
    input_file &lt;span class="pl-k"&gt;=&lt;/span&gt; os.path.join(test_data_dir, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_&lt;span class="pl-c1"&gt;{}&lt;/span&gt;.pb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.format(i))
    tensor &lt;span class="pl-k"&gt;=&lt;/span&gt; onnx.TensorProto()
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(input_file, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;rb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        tensor.ParseFromString(f.read())
    inputs.append(numpy_helper.to_array(tensor))

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load reference outputs&lt;/span&gt;
ref_outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; []
ref_outputs_num &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(glob.glob(os.path.join(test_data_dir, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;output_*.pb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)))
&lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(ref_outputs_num):
    output_file &lt;span class="pl-k"&gt;=&lt;/span&gt; os.path.join(test_data_dir, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;output_&lt;span class="pl-c1"&gt;{}&lt;/span&gt;.pb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.format(i))
    tensor &lt;span class="pl-k"&gt;=&lt;/span&gt; onnx.TensorProto()
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(output_file, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;rb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        tensor.ParseFromString(f.read())
    ref_outputs.append(numpy_helper.to_array(tensor))

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run the model on the backend&lt;/span&gt;
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(backend.run_model(model, inputs))

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Compare the results with reference outputs.&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; ref_o, o &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;zip&lt;/span&gt;(ref_outputs, outputs):
    np.testing.assert_almost_equal(ref_o, o)&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Serialized Numpy archives, stored in files with the naming convention &lt;code&gt;test_data_*.npz&lt;/code&gt;. Each file contains one set of test inputs and outputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;import&lt;/span&gt; onnx
&lt;span class="pl-k"&gt;import&lt;/span&gt; onnx_backend &lt;span class="pl-k"&gt;as&lt;/span&gt; backend

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load the model and sample inputs and outputs&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; onnx.load(model_pb_path)
sample &lt;span class="pl-k"&gt;=&lt;/span&gt; np.load(npz_path, &lt;span class="pl-v"&gt;encoding&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bytes&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
inputs &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(sample[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;inputs&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(sample[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;outputs&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run the model with an onnx backend and verify the results&lt;/span&gt;
np.testing.assert_almost_equal(outputs, backend.run_model(model, inputs))&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-model-visualization" class="anchor" aria-hidden="true" href="#model-visualization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model Visualization&lt;/h2&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-usage-"&gt;You can see visualizations of each model's network architecture by using &lt;/a&gt;&lt;a href="https://lutzroeder.github.io/Netron" rel="nofollow"&gt;Netron&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributions" class="anchor" aria-hidden="true" href="#contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions&lt;/h2&gt;
&lt;p&gt;Do you want to contribute a model? To get started, pick any model presented above with the &lt;a href="contribute.md"&gt;contribute&lt;/a&gt; link under the Description column. The links point to a page containing guidelines for making a contribution.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>onnx</author><guid isPermaLink="false">https://github.com/onnx/models</guid><pubDate>Fri, 27 Dec 2019 00:25:00 GMT</pubDate></item></channel></rss>