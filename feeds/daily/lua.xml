<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Lua, Today</title><link>https://github.com/trending/lua?since=daily</link><description>The top repositories on GitHub for lua, measured daily</description><pubDate>Wed, 06 Nov 2019 01:06:05 GMT</pubDate><lastBuildDate>Wed, 06 Nov 2019 01:06:05 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>karpathy/char-rnn #1 in Lua, Today</title><link>https://github.com/karpathy/char-rnn</link><description>&lt;p&gt;&lt;i&gt;Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="Readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-char-rnn" class="anchor" aria-hidden="true" href="#char-rnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;char-rnn&lt;/h1&gt;
&lt;p&gt;This code implements &lt;strong&gt;multi-layer Recurrent Neural Network&lt;/strong&gt; (RNN, LSTM, and GRU) for training/sampling from character-level language models. In other words the model takes one text file as input and trains a Recurrent Neural Network that learns to predict the next character in a sequence. The RNN can then be used to generate text character by character that will look like the original training data. The context of this code base is described in detail in my &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="nofollow"&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are new to Torch/Lua/Neural Nets, it might be helpful to know that this code is really just a slightly more fancy version of this &lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086"&gt;100-line gist&lt;/a&gt; that I wrote in Python/numpy. The code in this repo additionally: allows for multiple layers, uses an LSTM instead of a vanilla RNN, has more supporting code for model checkpointing, and is of course much more efficient since it uses mini-batches and can run on a GPU.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-update-torch-rnn" class="anchor" aria-hidden="true" href="#update-torch-rnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Update: torch-rnn&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://cs.stanford.edu/people/jcjohns/" rel="nofollow"&gt;Justin Johnson&lt;/a&gt; (@jcjohnson) recently re-implemented char-rnn from scratch with a much nicer/smaller/cleaner/faster Torch code base. It's under the name &lt;a href="https://github.com/jcjohnson/torch-rnn"&gt;torch-rnn&lt;/a&gt;. It uses Adam for optimization and hard-codes the RNN/LSTM forward/backward passes for space/time efficiency. This also avoids headaches with cloning models in this repo. In other words, torch-rnn should be the default char-rnn implemention to use now instead of the one in this code base.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h2&gt;
&lt;p&gt;This code is written in Lua and requires &lt;a href="http://torch.ch/" rel="nofollow"&gt;Torch&lt;/a&gt;. If you're on Ubuntu, installing Torch in your home directory may look something like:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps &lt;span class="pl-k"&gt;|&lt;/span&gt; bash
$ git clone https://github.com/torch/distro.git &lt;span class="pl-k"&gt;~&lt;/span&gt;/torch --recursive
$ &lt;span class="pl-c1"&gt;cd&lt;/span&gt; &lt;span class="pl-k"&gt;~&lt;/span&gt;/torch&lt;span class="pl-k"&gt;;&lt;/span&gt; 
$ ./install.sh      &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; and enter "yes" at the end to modify your bashrc&lt;/span&gt;
$ &lt;span class="pl-c1"&gt;source&lt;/span&gt; &lt;span class="pl-k"&gt;~&lt;/span&gt;/.bashrc&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See the Torch installation documentation for more details. After Torch is installed we need to get a few more packages using &lt;a href="https://luarocks.org/" rel="nofollow"&gt;LuaRocks&lt;/a&gt; (which already came with the Torch install). In particular:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ luarocks install nngraph 
$ luarocks install optim
$ luarocks install nn&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you'd like to train on an NVIDIA GPU using CUDA (this can be to about 15x faster), you'll of course need the GPU, and you will have to install the &lt;a href="https://developer.nvidia.com/cuda-toolkit" rel="nofollow"&gt;CUDA Toolkit&lt;/a&gt;. Then get the &lt;code&gt;cutorch&lt;/code&gt; and &lt;code&gt;cunn&lt;/code&gt; packages:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ luarocks install cutorch
$ luarocks install cunn&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you'd like to use OpenCL GPU instead (e.g. ATI cards), you will instead need to install the &lt;code&gt;cltorch&lt;/code&gt; and &lt;code&gt;clnn&lt;/code&gt; packages, and then use the option &lt;code&gt;-opencl 1&lt;/code&gt; during training (&lt;a href="https://github.com/hughperkins/cltorch/issues"&gt;cltorch issues&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ luarocks install cltorch
$ luarocks install clnn&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-data" class="anchor" aria-hidden="true" href="#data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data&lt;/h3&gt;
&lt;p&gt;All input data is stored inside the &lt;code&gt;data/&lt;/code&gt; directory. You'll notice that there is an example dataset included in the repo (in folder &lt;code&gt;data/tinyshakespeare&lt;/code&gt;) which consists of a subset of works of Shakespeare. I'm providing a few more datasets on &lt;a href="http://cs.stanford.edu/people/karpathy/char-rnn/" rel="nofollow"&gt;this page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Your own data&lt;/strong&gt;: If you'd like to use your own data then create a single file &lt;code&gt;input.txt&lt;/code&gt; and place it into a folder in the &lt;code&gt;data/&lt;/code&gt; directory. For example, &lt;code&gt;data/some_folder/input.txt&lt;/code&gt;. The first time you run the training script it will do some preprocessing and write two more convenience cache files into &lt;code&gt;data/some_folder&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dataset sizes&lt;/strong&gt;: Note that if your data is too small (1MB is already considered very small) the RNN won't learn very effectively. Remember that it has to learn everything completely from scratch. Conversely if your data is large (more than about 2MB), feel confident to increase &lt;code&gt;rnn_size&lt;/code&gt; and train a bigger model (see details of training below). It will work &lt;em&gt;significantly better&lt;/em&gt;. For example with 6MB you can easily go up to &lt;code&gt;rnn_size&lt;/code&gt; 300 or even more. The biggest that fits on my GPU and that I've trained with this code is &lt;code&gt;rnn_size&lt;/code&gt; 700 with &lt;code&gt;num_layers&lt;/code&gt; 3 (2 is default).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h3&gt;
&lt;p&gt;Start training the model using &lt;code&gt;train.lua&lt;/code&gt;. As a sanity check, to run on the included example dataset simply try:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ th train.lua -gpuid -1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that here we are setting the flag &lt;code&gt;gpuid&lt;/code&gt; to -1, which tells the code to train using CPU, otherwise it defaults to GPU 0.  There are many other flags for various options. Consult &lt;code&gt;$ th train.lua -help&lt;/code&gt; for comprehensive settings. Here's another example that trains a bigger network and also shows how you can run on your own custom dataset (this already assumes that &lt;code&gt;data/some_folder/input.txt&lt;/code&gt; exists):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ th train.lua -data_dir data/some_folder -rnn_size 512 -num_layers 2 -dropout 0.5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Checkpoints.&lt;/strong&gt; While the model is training it will periodically write checkpoint files to the &lt;code&gt;cv&lt;/code&gt; folder. The frequency with which these checkpoints are written is controlled with number of iterations, as specified with the &lt;code&gt;eval_val_every&lt;/code&gt; option (e.g. if this is 1 then a checkpoint is written every iteration). The filename of these checkpoints contains a very important number: the &lt;strong&gt;loss&lt;/strong&gt;. For example, a checkpoint with filename &lt;code&gt;lm_lstm_epoch0.95_2.0681.t7&lt;/code&gt; indicates that at this point the model was on epoch 0.95 (i.e. it has almost done one full pass over the training data), and the loss on validation data was 2.0681. This number is very important because the lower it is, the better the checkpoint works. Once you start to generate data (discussed below), you will want to use the model checkpoint that reports the lowest validation loss. Notice that this might not necessarily be the last checkpoint at the end of training (due to possible overfitting).&lt;/p&gt;
&lt;p&gt;Another important quantities to be aware of are &lt;code&gt;batch_size&lt;/code&gt; (call it B), &lt;code&gt;seq_length&lt;/code&gt; (call it S), and the &lt;code&gt;train_frac&lt;/code&gt; and &lt;code&gt;val_frac&lt;/code&gt; settings. The batch size specifies how many streams of data are processed in parallel at one time. The sequence length specifies the length of each stream, which is also the limit at which the gradients can propagate backwards in time. For example, if &lt;code&gt;seq_length&lt;/code&gt; is 20, then the gradient signal will never backpropagate more than 20 time steps, and the model might not &lt;em&gt;find&lt;/em&gt; dependencies longer than this length in number of characters. Thus, if you have a very difficult dataset where there are a lot of long-term dependencies you will want to increase this setting. Now, if at runtime your input text file has N characters, these first all get split into chunks of size &lt;code&gt;BxS&lt;/code&gt;. These chunks then get allocated across three splits: train/val/test according to the &lt;code&gt;frac&lt;/code&gt; settings. By default &lt;code&gt;train_frac&lt;/code&gt; is 0.95 and &lt;code&gt;val_frac&lt;/code&gt; is 0.05, which means that 95% of our data chunks will be trained on and 5% of the chunks will be used to estimate the validation loss (and hence the generalization). If your data is small, it's possible that with the default settings you'll only have very few chunks in total (for example 100). This is bad: In these cases you may want to decrease batch size or sequence length.&lt;/p&gt;
&lt;p&gt;Note that you can also initialize parameters from a previously saved checkpoint using &lt;code&gt;init_from&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-sampling" class="anchor" aria-hidden="true" href="#sampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sampling&lt;/h3&gt;
&lt;p&gt;Given a checkpoint file (such as those written to &lt;code&gt;cv&lt;/code&gt;) we can generate new text. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ th sample.lua cv/some_checkpoint.t7 -gpuid -1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make sure that if your checkpoint was trained with GPU it is also sampled from with GPU, or vice versa. Otherwise the code will (currently) complain. As with the train script, see &lt;code&gt;$ th sample.lua -help&lt;/code&gt; for full options. One important one is (for example) &lt;code&gt;-length 10000&lt;/code&gt; which would generate 10,000 characters (default = 2000).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Temperature&lt;/strong&gt;. An important parameter you may want to play with is &lt;code&gt;-temperature&lt;/code&gt;, which takes a number in range (0, 1] (0 not included), default = 1. The temperature is dividing the predicted log probabilities before the Softmax, so lower temperature will cause the model to make more likely, but also more boring and conservative predictions. Higher temperatures cause the model to take more chances and increase diversity of results, but at a cost of more mistakes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Priming&lt;/strong&gt;. It's also possible to prime the model with some starting text using &lt;code&gt;-primetext&lt;/code&gt;. This starts out the RNN with some hardcoded characters to &lt;em&gt;warm&lt;/em&gt; it up with some context before it starts generating text. E.g. a fun primetext might be &lt;code&gt;-primetext "the meaning of life is "&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Training with GPU but sampling on CPU&lt;/strong&gt;. Right now the solution is to use the &lt;code&gt;convert_gpu_cpu_checkpoint.lua&lt;/code&gt; script to convert your GPU checkpoint to a CPU checkpoint. In near future you will not have to do this explicitly. E.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ th convert_gpu_cpu_checkpoint.lua cv/lm_lstm_epoch30.00_1.3950.t7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will create a new file &lt;code&gt;cv/lm_lstm_epoch30.00_1.3950.t7_cpu.t7&lt;/code&gt; that you can use with the sample script and with &lt;code&gt;-gpuid -1&lt;/code&gt; for CPU mode.&lt;/p&gt;
&lt;p&gt;Happy sampling!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tips-and-tricks" class="anchor" aria-hidden="true" href="#tips-and-tricks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tips and Tricks&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-monitoring-validation-loss-vs-training-loss" class="anchor" aria-hidden="true" href="#monitoring-validation-loss-vs-training-loss"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Monitoring Validation Loss vs. Training Loss&lt;/h3&gt;
&lt;p&gt;If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If your training loss is much lower than validation loss then this means the network might be &lt;strong&gt;overfitting&lt;/strong&gt;. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.&lt;/li&gt;
&lt;li&gt;If your training/validation loss are about equal then your model is &lt;strong&gt;underfitting&lt;/strong&gt;. Increase the size of your model (either number of layers or the raw number of neurons per layer)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-approximate-number-of-parameters" class="anchor" aria-hidden="true" href="#approximate-number-of-parameters"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Approximate number of parameters&lt;/h3&gt;
&lt;p&gt;The two most important parameters that control the model are &lt;code&gt;rnn_size&lt;/code&gt; and &lt;code&gt;num_layers&lt;/code&gt;. I would advise that you always use &lt;code&gt;num_layers&lt;/code&gt; of either 2/3. The &lt;code&gt;rnn_size&lt;/code&gt; can be adjusted based on how much data you have. The two important quantities to keep track of here are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The number of parameters in your model. This is printed when you start training.&lt;/li&gt;
&lt;li&gt;The size of your dataset. 1MB file is approximately 1 million characters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil &amp;gt;&amp;gt; 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make &lt;code&gt;rnn_size&lt;/code&gt; larger.&lt;/li&gt;
&lt;li&gt;I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that heps the validation loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-best-models-strategy" class="anchor" aria-hidden="true" href="#best-models-strategy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Best models strategy&lt;/h3&gt;
&lt;p&gt;The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.&lt;/p&gt;
&lt;p&gt;It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.&lt;/p&gt;
&lt;p&gt;By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-additional-pointers-and-acknowledgements" class="anchor" aria-hidden="true" href="#additional-pointers-and-acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Additional Pointers and Acknowledgements&lt;/h2&gt;
&lt;p&gt;This code was originally based on Oxford University Machine Learning class &lt;a href="https://github.com/oxford-cs-ml-2015/practical6"&gt;practical 6&lt;/a&gt;, which is in turn based on &lt;a href="https://github.com/wojciechz/learning_to_execute"&gt;learning to execute&lt;/a&gt; code from Wojciech Zaremba. Chunks of it were also developed in collaboration with my labmate &lt;a href="http://cs.stanford.edu/people/jcjohns/" rel="nofollow"&gt;Justin Johnson&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To learn more about RNN language models I recommend looking at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks" rel="nofollow"&gt;My recent talk&lt;/a&gt; on char-rnn&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1308.0850" rel="nofollow"&gt;Generating Sequences With Recurrent Neural Networks&lt;/a&gt; by Alex Graves&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf" rel="nofollow"&gt;Generating Text with Recurrent Neural Networks&lt;/a&gt; by Ilya Sutskever&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf" rel="nofollow"&gt;Tomas Mikolov's Thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>karpathy</author><guid isPermaLink="false">https://github.com/karpathy/char-rnn</guid><pubDate>Wed, 06 Nov 2019 00:01:00 GMT</pubDate></item><item><title>Ls-Modcompany/FS19_GlobalCompany #2 in Lua, Today</title><link>https://github.com/Ls-Modcompany/FS19_GlobalCompany</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-fs19_globalcompany" class="anchor" aria-hidden="true" href="#fs19_globalcompany"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FS19_GlobalCompany&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-wichtig-lade-dir-nur-relase-versionen-runter" class="anchor" aria-hidden="true" href="#wichtig-lade-dir-nur-relase-versionen-runter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WICHTIG: Lade dir NUR Relase-Versionen runter!&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Klicke auf "release" oder folge diesem Link: &lt;a href="https://github.com/Ls-Modcompany/FS19_GlobalCompany/releases"&gt;https://github.com/Ls-Modcompany/FS19_GlobalCompany/releases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dort gibt es nun Versionen, die spielbereit sind.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;-&amp;gt; Wenn das komplette Repository geladen wird, kann nicht sichergestellt werden, dass der Mod ohne Probleme funktionert!&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Wollt Ihr dann z.B. die Version 1.1.1.0, dann kann dort der Bereich 'Asset' aufgeklappt werden. Dort dann auf 'Source Code (zip)' klicken.&lt;/li&gt;
&lt;li&gt;Die geladene ZIP-Datei entpacken.&lt;/li&gt;
&lt;li&gt;Dort befindet sich ein weiteren Ordner. Der Inhalt von diesem Ordner dann wieder als ZIP-Datei mit dem Titel 'FS19_GlobalCompany' packen (D.h. auf der ersten Ebene muss sich dann die modDesc.xml befinden).&lt;/li&gt;
&lt;li&gt;Die gepackte ZIP kann nun in den Modordner geschoben werden&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-bug-melden" class="anchor" aria-hidden="true" href="#bug-melden"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bug melden&lt;/h1&gt;
&lt;p&gt;Fehler können entweder unter Issues gemeldet werden oder bei uns im Forum: &lt;a href="https://ls-modcompany.com/forum/board/119-globalcompany/" rel="nofollow"&gt;https://ls-modcompany.com/forum/board/119-globalcompany/&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Ls-Modcompany</author><guid isPermaLink="false">https://github.com/Ls-Modcompany/FS19_GlobalCompany</guid><pubDate>Wed, 06 Nov 2019 00:02:00 GMT</pubDate></item><item><title>jcjohnson/neural-style #3 in Lua, Today</title><link>https://github.com/jcjohnson/neural-style</link><description>&lt;p&gt;&lt;i&gt;Torch implementation of neural style algorithm&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-neural-style" class="anchor" aria-hidden="true" href="#neural-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;neural-style&lt;/h1&gt;
&lt;p&gt;This is a torch implementation of the paper &lt;a href="http://arxiv.org/abs/1508.06576" rel="nofollow"&gt;A Neural Algorithm of Artistic Style&lt;/a&gt;
by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.&lt;/p&gt;
&lt;p&gt;The paper presents an algorithm for combining the content of one image with the style of another image using
convolutional neural networks. Here's an example that maps the artistic style of
&lt;a href="https://en.wikipedia.org/wiki/The_Starry_Night" rel="nofollow"&gt;The Starry Night&lt;/a&gt;
onto a night-time photograph of the Stanford campus:&lt;/p&gt;
&lt;div align="center"&gt;
 &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/starry_night_google.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/starry_night_google.jpg" height="223px" style="max-width:100%;"&gt;&lt;/a&gt;
 &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/hoovertowernight.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/hoovertowernight.jpg" height="223px" style="max-width:100%;"&gt;&lt;/a&gt;
 &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/starry_stanford_bigger.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/starry_stanford_bigger.png" width="710px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Applying the style of different images to the same content image gives interesting results.
Here we reproduce Figure 2 from the paper, which renders a photograph of the Tubingen in Germany in a
variety of styles:&lt;/p&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/tubingen.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/tubingen.jpg" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_shipwreck.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_shipwreck.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_scream.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_scream.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_seated_nude.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_seated_nude.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_composition_vii.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_composition_vii.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Here are the results of applying the style of various pieces of artwork to this photograph of the
golden gate bridge:&lt;/p&gt;
&lt;div align="center" height="200px"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/frida_kahlo.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/frida_kahlo.jpg" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_kahlo.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_kahlo.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/escher_sphere.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/escher_sphere.jpg" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_escher.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_escher.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/woman-with-hat-matisse.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/woman-with-hat-matisse.jpg" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_matisse.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_matisse.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/the_scream.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/the_scream.jpg" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_scream.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_scream.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/starry_night_crop.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/starry_night_crop.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/seated-nude.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/seated-nude.jpg" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_seated.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_seated.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-content--style-tradeoff" class="anchor" aria-hidden="true" href="#content--style-tradeoff"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Content / Style Tradeoff&lt;/h3&gt;
&lt;p&gt;The algorithm allows the user to trade-off the relative weight of the style and content reconstruction terms,
as shown in this example where we port the style of &lt;a href="http://www.wikiart.org/en/pablo-picasso/self-portrait-1907" rel="nofollow"&gt;Picasso's 1907 self-portrait&lt;/a&gt; onto Brad Pitt:&lt;/p&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/picasso_selfport1907.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/picasso_selfport1907.jpg" height="220px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/brad_pitt.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/brad_pitt.jpg" height="220px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_5_style_10.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_5_style_10.png" height="220px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_1_style_10.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_1_style_10.png" height="220px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_01_style_10.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_01_style_10.png" height="220px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_0025_style_10.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_0025_style_10.png" height="220px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-style-scale" class="anchor" aria-hidden="true" href="#style-scale"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Style Scale&lt;/h3&gt;
&lt;p&gt;By resizing the style image before extracting style features, we can control the types of artistic
features that are transfered from the style image; you can control this behavior with the &lt;code&gt;-style_scale&lt;/code&gt; flag.
Below we see three examples of rendering the Golden Gate Bridge in the style of The Starry Night.
From left to right, &lt;code&gt;-style_scale&lt;/code&gt; is 2.0, 1.0, and 0.5.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scale2.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scale2.png" height="175px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scale1.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scale1.png" height="175px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scale05.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scale05.png" height="175px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-multiple-style-images" class="anchor" aria-hidden="true" href="#multiple-style-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multiple Style Images&lt;/h3&gt;
&lt;p&gt;You can use more than one style image to blend multiple artistic styles.&lt;/p&gt;
&lt;p&gt;Clockwise from upper left: "The Starry Night" + "The Scream", "The Scream" + "Composition VII",
"Seated Nude" + "Composition VII", and "Seated Nude" + "The Starry Night"&lt;/p&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry_scream.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry_scream.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_scream_composition_vii.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_scream_composition_vii.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry_seated.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry_seated.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_seated_nude_composition_vii.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_seated_nude_composition_vii.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-style-interpolation" class="anchor" aria-hidden="true" href="#style-interpolation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Style Interpolation&lt;/h3&gt;
&lt;p&gt;When using multiple style images, you can control the degree to which they are blended:&lt;/p&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scream_3_7.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scream_3_7.png" height="175px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scream_5_5.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scream_5_5.png" height="175px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scream_7_3.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scream_7_3.png" height="175px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-transfer-style-but-not-color" class="anchor" aria-hidden="true" href="#transfer-style-but-not-color"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transfer style but not color&lt;/h3&gt;
&lt;p&gt;If you add the flag &lt;code&gt;-original_colors 1&lt;/code&gt; then the output image will retain the colors of the original image;
this is similar to &lt;a href="http://blog.deepart.io/2016/06/04/color-independent-style-transfer/" rel="nofollow"&gt;the recent blog post by deepart.io&lt;/a&gt;.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry.png" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_scream.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_scream.png" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_composition_vii.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_composition_vii.png" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/original_color/tubingen_starry.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/original_color/tubingen_starry.png" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/original_color/tubingen_scream.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/original_color/tubingen_scream.png" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/original_color/tubingen_composition_vii.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/original_color/tubingen_composition_vii.png" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup:&lt;/h2&gt;
&lt;p&gt;Dependencies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/torch/torch7"&gt;torch7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/szagoruyko/loadcaffe"&gt;loadcaffe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Optional dependencies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For CUDA backend:
&lt;ul&gt;
&lt;li&gt;CUDA 6.5+&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/torch/cunn"&gt;cunn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For cuDNN backend:
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/soumith/cudnn.torch"&gt;cudnn.torch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For OpenCL backend:
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/hughperkins/cltorch"&gt;cltorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hughperkins/clnn"&gt;clnn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After installing dependencies, you'll need to run the following script to download the VGG model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sh models/download_models.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will download the original &lt;a href="https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md"&gt;VGG-19 model&lt;/a&gt;.
Leon Gatys has graciously provided the modified version of the VGG-19 model that was used in their paper;
this will also be downloaded. By default the original VGG-19 model is used.&lt;/p&gt;
&lt;p&gt;If you have a smaller memory GPU then using NIN Imagenet model will be better and gives slightly worse yet comparable results. You can get the details on the model from &lt;a href="https://github.com/BVLC/caffe/wiki/Model-Zoo"&gt;BVLC Caffe ModelZoo&lt;/a&gt; and can download the files from &lt;a href="https://drive.google.com/folderview?id=0B0IedYUunOQINEFtUi1QNWVhVVU&amp;amp;usp=drive_web" rel="nofollow"&gt;NIN-Imagenet Download Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can find detailed installation instructions for Ubuntu in the &lt;a href="INSTALL.md"&gt;installation guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;Basic usage:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;th neural_style.lua -style_image &amp;lt;image.jpg&amp;gt; -content_image &amp;lt;image.jpg&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OpenCL usage with NIN Model (This requires you download the NIN Imagenet model files as described above):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;th neural_style.lua -style_image examples/inputs/picasso_selfport1907.jpg -content_image examples/inputs/brad_pitt.jpg -output_image profile.png -model_file models/nin_imagenet_conv.caffemodel -proto_file models/train_val.prototxt -gpu 0 -backend clnn -num_iterations 1000 -seed 123 -content_layers relu0,relu3,relu7,relu12 -style_layers relu0,relu3,relu7,relu12 -content_weight 10 -style_weight 1000 -image_size 512 -optimizer adam
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/examples/outputs/pitt_picasso_nin_opencl.png"&gt;&lt;img src="/examples/outputs/pitt_picasso_nin_opencl.png" alt="OpenCL NIN Model Picasso Brad Pitt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To use multiple style images, pass a comma-separated list like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-style_image starry_night.jpg,the_scream.jpg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note that paths to images should not contain the &lt;code&gt;~&lt;/code&gt; character to represent your home directory; you should instead use a relative
path or a full absolute path.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Options&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-image_size&lt;/code&gt;: Maximum side length (in pixels) of of the generated image. Default is 512.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-style_blend_weights&lt;/code&gt;: The weight for blending the style of multiple style images, as a
comma-separated list, such as &lt;code&gt;-style_blend_weights 3,7&lt;/code&gt;. By default all style images
are equally weighted.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-gpu&lt;/code&gt;: Zero-indexed ID of the GPU to use; for CPU mode set &lt;code&gt;-gpu&lt;/code&gt; to -1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Optimization options&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-content_weight&lt;/code&gt;: How much to weight the content reconstruction term. Default is 5e0.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-style_weight&lt;/code&gt;: How much to weight the style reconstruction term. Default is 1e2.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-tv_weight&lt;/code&gt;: Weight of total-variation (TV) regularization; this helps to smooth the image.
Default is 1e-3. Set to 0 to disable TV regularization.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-num_iterations&lt;/code&gt;: Default is 1000.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-init&lt;/code&gt;: Method for generating the generated image; one of &lt;code&gt;random&lt;/code&gt; or &lt;code&gt;image&lt;/code&gt;.
Default is &lt;code&gt;random&lt;/code&gt; which uses a noise initialization as in the paper; &lt;code&gt;image&lt;/code&gt;
initializes with the content image.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-optimizer&lt;/code&gt;: The optimization algorithm to use; either &lt;code&gt;lbfgs&lt;/code&gt; or &lt;code&gt;adam&lt;/code&gt;; default is &lt;code&gt;lbfgs&lt;/code&gt;.
L-BFGS tends to give better results, but uses more memory. Switching to ADAM will reduce memory usage;
when using ADAM you will probably need to play with other parameters to get good results, especially
the style weight, content weight, and learning rate; you may also want to normalize gradients when
using ADAM.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-learning_rate&lt;/code&gt;: Learning rate to use with the ADAM optimizer. Default is 1e1.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-normalize_gradients&lt;/code&gt;: If this flag is present, style and content gradients from each layer will be
L1 normalized. Idea from &lt;a href="https://github.com/andersbll/neural_artistic_style"&gt;andersbll/neural_artistic_style&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Output options&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-output_image&lt;/code&gt;: Name of the output image. Default is &lt;code&gt;out.png&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-print_iter&lt;/code&gt;: Print progress every &lt;code&gt;print_iter&lt;/code&gt; iterations. Set to 0 to disable printing.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-save_iter&lt;/code&gt;: Save the image every &lt;code&gt;save_iter&lt;/code&gt; iterations. Set to 0 to disable saving intermediate results.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Layer options&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-content_layers&lt;/code&gt;: Comma-separated list of layer names to use for content reconstruction.
Default is &lt;code&gt;relu4_2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-style_layers&lt;/code&gt;: Comma-separated list of layer names to use for style reconstruction.
Default is &lt;code&gt;relu1_1,relu2_1,relu3_1,relu4_1,relu5_1&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Other options&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-style_scale&lt;/code&gt;: Scale at which to extract features from the style image. Default is 1.0.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-original_colors&lt;/code&gt;: If you set this to 1, then the output image will keep the colors of the content image.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-proto_file&lt;/code&gt;: Path to the &lt;code&gt;deploy.txt&lt;/code&gt; file for the VGG Caffe model.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-model_file&lt;/code&gt;: Path to the &lt;code&gt;.caffemodel&lt;/code&gt; file for the VGG Caffe model.
Default is the original VGG-19 model; you can also try the normalized VGG-19 model used in the paper.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-pooling&lt;/code&gt;: The type of pooling layers to use; one of &lt;code&gt;max&lt;/code&gt; or &lt;code&gt;avg&lt;/code&gt;. Default is &lt;code&gt;max&lt;/code&gt;.
The VGG-19 models uses max pooling layers, but the paper mentions that replacing these layers with average
pooling layers can improve the results. I haven't been able to get good results using average pooling, but
the option is here.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend&lt;/code&gt;: &lt;code&gt;nn&lt;/code&gt;, &lt;code&gt;cudnn&lt;/code&gt;, or &lt;code&gt;clnn&lt;/code&gt;. Default is &lt;code&gt;nn&lt;/code&gt;. &lt;code&gt;cudnn&lt;/code&gt; requires
&lt;a href="https://github.com/soumith/cudnn.torch"&gt;cudnn.torch&lt;/a&gt; and may reduce memory usage.
&lt;code&gt;clnn&lt;/code&gt; requires &lt;a href="https://github.com/hughperkins/cltorch"&gt;cltorch&lt;/a&gt; and &lt;a href="https://github.com/hughperkins/clnn"&gt;clnn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-cudnn_autotune&lt;/code&gt;: When using the cuDNN backend, pass this flag to use the built-in cuDNN autotuner to select
the best convolution algorithms for your architecture. This will make the first iteration a bit slower and can
take a bit more memory, but may significantly speed up the cuDNN backend.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-frequently-asked-questions" class="anchor" aria-hidden="true" href="#frequently-asked-questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Frequently Asked Questions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Generated image has saturation artifacts:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://cloud.githubusercontent.com/assets/1310570/9694690/fa8e8782-5328-11e5-9c91-11f7b215ad19.png"&gt;&lt;img src="https://cloud.githubusercontent.com/assets/1310570/9694690/fa8e8782-5328-11e5-9c91-11f7b215ad19.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Update the &lt;code&gt;image&lt;/code&gt; packge to the latest version: &lt;code&gt;luarocks install image&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Running without a GPU gives an error message complaining about &lt;code&gt;cutorch&lt;/code&gt; not found&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;
Pass the flag &lt;code&gt;-gpu -1&lt;/code&gt; when running in CPU-only mode&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; The program runs out of memory and dies&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Try reducing the image size: &lt;code&gt;-image_size 256&lt;/code&gt; (or lower). Note that different image sizes will likely
require non-default values for &lt;code&gt;-style_weight&lt;/code&gt; and &lt;code&gt;-content_weight&lt;/code&gt; for optimal results.
If you are running on a GPU, you can also try running with &lt;code&gt;-backend cudnn&lt;/code&gt; to reduce memory usage.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Get the following error message:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;models/VGG_ILSVRC_19_layers_deploy.prototxt.cpu.lua:7: attempt to call method 'ceil' (a nil value)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Update &lt;code&gt;nn&lt;/code&gt; package to the latest version: &lt;code&gt;luarocks install nn&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Get an error message complaining about &lt;code&gt;paths.extname&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Update &lt;code&gt;torch.paths&lt;/code&gt; package to the latest version: &lt;code&gt;luarocks install paths&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; NIN Imagenet model is not giving good results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Make sure the correct &lt;code&gt;-proto_file&lt;/code&gt; is selected. Also make sure the correct parameters for &lt;code&gt;-content_layers&lt;/code&gt; and &lt;code&gt;-style_layers&lt;/code&gt; are set. (See OpenCL usage example above.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; &lt;code&gt;-backend cudnn&lt;/code&gt; is slower than default NN backend&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Add the flag &lt;code&gt;-cudnn_autotune&lt;/code&gt;; this will use the built-in cuDNN autotuner to select the best convolution algorithms.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-memory-usage" class="anchor" aria-hidden="true" href="#memory-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Memory Usage&lt;/h2&gt;
&lt;p&gt;By default, &lt;code&gt;neural-style&lt;/code&gt; uses the &lt;code&gt;nn&lt;/code&gt; backend for convolutions and L-BFGS for optimization.
These give good results, but can both use a lot of memory. You can reduce memory usage with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Use cuDNN&lt;/strong&gt;: Add the flag &lt;code&gt;-backend cudnn&lt;/code&gt; to use the cuDNN backend. This will only work in GPU mode.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use ADAM&lt;/strong&gt;: Add the flag &lt;code&gt;-optimizer adam&lt;/code&gt; to use ADAM instead of L-BFGS. This should significantly
reduce memory usage, but may require tuning of other parameters for good results; in particular you should
play with the learning rate, content weight, style weight, and also consider using gradient normalization.
This should work in both CPU and GPU modes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduce image size&lt;/strong&gt;: If the above tricks are not enough, you can reduce the size of the generated image;
pass the flag &lt;code&gt;-image_size 256&lt;/code&gt; to generate an image at half the default size.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the default settings, &lt;code&gt;neural-style&lt;/code&gt; uses about 3.5GB of GPU memory on my system;
switching to ADAM and cuDNN reduces the GPU memory footprint to about 1GB.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-speed" class="anchor" aria-hidden="true" href="#speed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed&lt;/h2&gt;
&lt;p&gt;Speed can vary a lot depending on the backend and the optimizer.
Here are some times for running 500 iterations with &lt;code&gt;-image_size=512&lt;/code&gt; on a Maxwell Titan X with different settings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-backend nn -optimizer lbfgs&lt;/code&gt;: 62 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend nn -optimizer adam&lt;/code&gt;: 49 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend cudnn -optimizer lbfgs&lt;/code&gt;: 79 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend cudnn -cudnn_autotune -optimizer lbfgs&lt;/code&gt;: 58 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend cudnn -cudnn_autotune -optimizer adam&lt;/code&gt;: 44 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend clnn -optimizer lbfgs&lt;/code&gt;: 169 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend clnn -optimizer adam&lt;/code&gt;: 106 seconds&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are the same benchmarks on a Pascal Titan X with cuDNN 5.0 on CUDA 8.0 RC:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-backend nn -optimizer lbfgs&lt;/code&gt;: 43 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend nn -optimizer adam&lt;/code&gt;: 36 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend cudnn -optimizer lbfgs&lt;/code&gt;: 45 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend cudnn -cudnn_autotune -optimizer lbfgs&lt;/code&gt;: 30 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend cudnn -cudnn_autotune -optimizer adam&lt;/code&gt;: 22 seconds&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-multi-gpu-scaling" class="anchor" aria-hidden="true" href="#multi-gpu-scaling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-GPU scaling&lt;/h2&gt;
&lt;p&gt;You can use multiple GPUs to process images at higher resolutions; different layers of the network will be
computed on different GPUs. You can control which GPUs are used with the &lt;code&gt;-gpu&lt;/code&gt; flag, and you can control
how to split layers across GPUs using the &lt;code&gt;-multigpu_strategy&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;For example in a server with four GPUs, you can give the flag &lt;code&gt;-gpu 0,1,2,3&lt;/code&gt; to process on GPUs 0, 1, 2, and
3 in that order; by also giving the flag &lt;code&gt;-multigpu_strategy 3,6,12&lt;/code&gt; you indicate that the first two layers
should be computed on GPU 0, layers 3 to 5 should be computed on GPU 1, layers 6 to 11 should be computed on
GPU 2, and the remaining layers should be computed on GPU 3. You will need to tune the &lt;code&gt;-multigpu_strategy&lt;/code&gt;
for your setup in order to achieve maximal resolution.&lt;/p&gt;
&lt;p&gt;We can achieve very high quality results at high resolution by combining multi-GPU processing with multiscale
generation as described in the paper
&lt;a href="https://arxiv.org/abs/1611.07865" rel="nofollow"&gt;&lt;strong&gt;Controlling Perceptual Factors in Neural Style Transfer&lt;/strong&gt;&lt;/a&gt; by Leon A. Gatys,
Alexander S. Ecker, Matthias Bethge, Aaron Hertzmann and Eli Shechtman.&lt;/p&gt;
&lt;p&gt;Here is a 3620 x 1905 image generated on a server with four Pascal Titan X GPUs:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/starry_stanford_bigger.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/starry_stanford_bigger.png" height="400px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The script used to generate this image &lt;a href="examples/multigpu_scripts/starry_stanford.sh"&gt;can be found here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-implementation-details" class="anchor" aria-hidden="true" href="#implementation-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementation details&lt;/h2&gt;
&lt;p&gt;Images are initialized with white noise and optimized using L-BFGS.&lt;/p&gt;
&lt;p&gt;We perform style reconstructions using the &lt;code&gt;conv1_1&lt;/code&gt;, &lt;code&gt;conv2_1&lt;/code&gt;, &lt;code&gt;conv3_1&lt;/code&gt;, &lt;code&gt;conv4_1&lt;/code&gt;, and &lt;code&gt;conv5_1&lt;/code&gt; layers
and content reconstructions using the &lt;code&gt;conv4_2&lt;/code&gt; layer. As in the paper, the five style reconstruction losses have
equal weights.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you find this code useful for your research, please cite:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@misc{Johnson2015,
  author = {Johnson, Justin},
  title = {neural-style},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jcjohnson/neural-style}},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jcjohnson</author><guid isPermaLink="false">https://github.com/jcjohnson/neural-style</guid><pubDate>Wed, 06 Nov 2019 00:03:00 GMT</pubDate></item><item><title>Stephan-S/FS19_AutoDrive #4 in Lua, Today</title><link>https://github.com/Stephan-S/FS19_AutoDrive</link><description>&lt;p&gt;&lt;i&gt;FS19 version of AutoDrive - Developer Version&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-fs19_autodrive" class="anchor" aria-hidden="true" href="#fs19_autodrive"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FS19_AutoDrive&lt;/h1&gt;
&lt;p&gt;FS19 version of AutoDrive&lt;/p&gt;
&lt;p&gt;If you want to support my development effort, the best way is to open issues on any bugs you encounter or for features you would like to be added to the mod.&lt;/p&gt;
&lt;p&gt;Wer die Weiterentwicklung des Mods unterstützen möchte, kann dies am Besten durch fleißiges Erstellen von Issues zu gefundenen Bugs und/oder gewünschten Erweiterungen zum Mod tun.&lt;/p&gt;
&lt;p&gt;If you like my work, feel free to buy me a coffee (of which I drink quite a lot :D )
&lt;a href="https://www.buymeacoffee.com/9Di7EUSI2" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/031fc5a134cdca5ae3460822aba371e63f794233/68747470733a2f2f7777772e6275796d6561636f666665652e636f6d2f6173736574732f696d672f637573746f6d5f696d616765732f6f72616e67655f696d672e706e67" alt="Buy Me A Coffee" data-canonical-src="https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.paypal.me/StephanSchlosser" rel="nofollow"&gt;https://www.paypal.me/StephanSchlosser&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Stephan-S</author><guid isPermaLink="false">https://github.com/Stephan-S/FS19_AutoDrive</guid><pubDate>Wed, 06 Nov 2019 00:04:00 GMT</pubDate></item><item><title>Kong/kong #5 in Lua, Today</title><link>https://github.com/Kong/kong</link><description>&lt;p&gt;&lt;i&gt;🦍 The Cloud-Native API Gateway &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://konghq.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9e4fe7914c7357861223aa535d7ca9858253c96e/68747470733a2f2f6b6f6e6768712e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031382f30352f6b6f6e672d6c6f676f2d6769746875622d726561646d652e706e67" alt="" data-canonical-src="https://konghq.com/wp-content/uploads/2018/05/kong-logo-github-readme.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/Kong/kong/branches" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/032b58c2a2e0a2a8dbb0c1fe60a0236e1042b7ad/68747470733a2f2f7472617669732d63692e6f72672f4b6f6e672f6b6f6e672e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/Kong/kong.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/Kong/kong/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/8051e9938a1ab39cf002818dfceb6b6092f34d68/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/intent/follow?screen_name=thekonginc" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/295bb78a3be8393e728bb4ad7470bd98a1c5062d/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f7468656b6f6e67696e632e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/follow/thekonginc.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kong is a cloud-native, fast, scalable, and distributed Microservice
Abstraction Layer &lt;em&gt;(also known as an API Gateway, API Middleware or in some
cases Service Mesh)&lt;/em&gt;. Made available as an open-source project in 2015, its
core values are high performance and extensibility.&lt;/p&gt;
&lt;p&gt;Actively maintained, Kong is widely used in production at companies ranging
from startups to Global 5000 as well as government organizations.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://konghq.com/install" rel="nofollow"&gt;Installation&lt;/a&gt; |
&lt;a href="https://docs.konghq.com" rel="nofollow"&gt;Documentation&lt;/a&gt; |
&lt;a href="https://discuss.konghq.com" rel="nofollow"&gt;Forum&lt;/a&gt; |
&lt;a href="https://konghq.com/blog" rel="nofollow"&gt;Blog&lt;/a&gt; |
IRC (freenode): &lt;a href="https://webchat.freenode.net/?channels=kong" rel="nofollow"&gt;#kong&lt;/a&gt; |
&lt;a href="https://bintray.com/kong/kong-nightly/master" rel="nofollow"&gt;Nightly Builds&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-summary" class="anchor" aria-hidden="true" href="#summary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#why-kong"&gt;&lt;strong&gt;Why Kong?&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#features"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#distributions"&gt;&lt;strong&gt;Distributions&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#development"&gt;&lt;strong&gt;Development&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#enterprise-support--demo"&gt;&lt;strong&gt;Enterprise Support &amp;amp; Demo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-why-kong" class="anchor" aria-hidden="true" href="#why-kong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why Kong?&lt;/h2&gt;
&lt;p&gt;If you are building for the web, mobile, or IoT (Internet of Things) you will
likely end up needing common functionality to run your actual software. Kong
can help by acting as a gateway (or a sidecar) for microservices requests while
providing load balancing, logging, authentication, rate-limiting,
transformations, and more through plugins.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://konghq.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d4d0dcb22c223db0bf2e301aab0dddb3015f1729/68747470733a2f2f6b6f6e6768712e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031382f30352f6b6f6e672d62656e65666974732d6769746875622d726561646d652e706e67" alt="" data-canonical-src="https://konghq.com/wp-content/uploads/2018/05/kong-benefits-github-readme.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cloud-Native&lt;/strong&gt;: Platform agnostic, Kong can run from bare metal to
Kubernetes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Load Balancing&lt;/strong&gt;: Load balance traffic across multiple upstream
services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hash-based Load Balancing&lt;/strong&gt;: Load balance with consistent hashing/sticky
sessions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Circuit-Breaker&lt;/strong&gt;: Intelligent tracking of unhealthy upstream services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Health Checks:&lt;/strong&gt; Active and passive monitoring of your upstream services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service Discovery&lt;/strong&gt;: Resolve SRV records in third-party DNS resolvers like
Consul.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Serverless&lt;/strong&gt;: Invoke and secure AWS Lambda or OpenWhisk functions directly
from Kong.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WebSockets&lt;/strong&gt;: Communicate to your upstream services via WebSockets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;gRPC&lt;/strong&gt;: Communicate to your gRPC services and observe your traffic with logging
and observability plugins&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OAuth2.0&lt;/strong&gt;: Easily add OAuth2.0 authentication to your APIs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logging&lt;/strong&gt;: Log requests and responses to your system over HTTP, TCP, UDP,
or to disk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: ACL, Bot detection, whitelist/blacklist IPs, etc...&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Syslog&lt;/strong&gt;: Logging to System log.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSL&lt;/strong&gt;: Setup a Specific SSL Certificate for an underlying service or API.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt;: Live monitoring provides key load and performance server
metrics.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forward Proxy&lt;/strong&gt;: Make Kong connect to intermediary transparent HTTP proxies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Authentications&lt;/strong&gt;: HMAC, JWT, Basic, and more.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rate-limiting&lt;/strong&gt;: Block and throttle requests based on many variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformations&lt;/strong&gt;: Add, remove, or manipulate HTTP requests and responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Caching&lt;/strong&gt;: Cache and serve responses at the proxy layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CLI&lt;/strong&gt;: Control your Kong cluster from the command line.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;REST API&lt;/strong&gt;: Kong can be operated with its RESTful API for maximum
flexibility.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Geo-Replicated&lt;/strong&gt;: Configs are always up-to-date across different regions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Failure Detection &amp;amp; Recovery&lt;/strong&gt;: Kong is unaffected if one of your Cassandra
nodes goes down.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clustering&lt;/strong&gt;: All Kong nodes auto-join the cluster keeping their config
updated across nodes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Distributed by nature, Kong scales horizontally by simply
adding nodes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Kong handles load with ease by scaling and using NGINX at
the core.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugins&lt;/strong&gt;: Extendable architecture for adding functionality to Kong and
APIs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more info about plugins and integrations, you can check out the &lt;a href="https://docs.konghq.com/hub/" rel="nofollow"&gt;Kong
Hub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-distributions" class="anchor" aria-hidden="true" href="#distributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Distributions&lt;/h2&gt;
&lt;p&gt;Kong comes in many shapes. While this repository contains its core's source
code, other repos are also under active development:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/docker-kong"&gt;Kong Docker&lt;/a&gt;: A Dockerfile for
running Kong in Docker.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kong/releases"&gt;Kong Packages&lt;/a&gt;: Pre-built packages
for Debian, Red Hat, and OS X distributions (shipped with each release).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kong-vagrant"&gt;Kong Vagrant&lt;/a&gt;: A Vagrantfile for
provisioning a development-ready environment for Kong.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/homebrew-kong"&gt;Kong Homebrew&lt;/a&gt;: Homebrew Formula
for Kong.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kong-dist-cloudformation"&gt;Kong CloudFormation&lt;/a&gt;:
Kong in a 1-click deployment for AWS EC2.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/marketplace/pp/B06WP4TNKL" rel="nofollow"&gt;Kong AWS AMI&lt;/a&gt;: Kong AMI on
the AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kong-dist-azure"&gt;Kong on Microsoft Azure&lt;/a&gt;: Run Kong
using Azure Resource Manager.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/heroku/heroku-kong"&gt;Kong on Heroku&lt;/a&gt;: Deploy Kong on
Heroku in one click.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.instaclustr.com/solutions/managed-cassandra-for-kong/" rel="nofollow"&gt;Kong and Instaclustr&lt;/a&gt;: Let
Instaclustr manage your Cassandra cluster.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kubernetes-ingress-controller"&gt;Kubernetes Ingress Controller for Kong&lt;/a&gt;:
Use Kong for Kubernetes Ingress.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bintray.com/kong/kong-nightly/master" rel="nofollow"&gt;Nightly Builds&lt;/a&gt;: Builds of the master branch available
every morning at about 9AM PST.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h2&gt;
&lt;p&gt;If you are planning on developing on Kong, you'll need a development
installation. The &lt;code&gt;next&lt;/code&gt; branch holds the latest unreleased source code.&lt;/p&gt;
&lt;p&gt;You can read more about writing your own plugins in the &lt;a href="https://docs.konghq.com/latest/plugin-development/" rel="nofollow"&gt;Plugin Development
Guide&lt;/a&gt;, or browse an
online version of Kong's source code documentation in the &lt;a href="https://docs.konghq.com/latest/pdk/" rel="nofollow"&gt;Plugin Development
Kit (PDK) Reference&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-docker" class="anchor" aria-hidden="true" href="#docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker&lt;/h4&gt;
&lt;p&gt;You can use Docker / docker-compose and a mounted volume to develop Kong by
following the instructions on &lt;a href="https://github.com/Kong/kong-build-tools#developing-kong"&gt;Kong/kong-build-tools&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-vagrant" class="anchor" aria-hidden="true" href="#vagrant"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vagrant&lt;/h4&gt;
&lt;p&gt;You can use a Vagrant box running Kong and Postgres that you can find at
&lt;a href="https://github.com/Kong/kong-vagrant"&gt;Kong/kong-vagrant&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-source-install" class="anchor" aria-hidden="true" href="#source-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source Install&lt;/h4&gt;
&lt;p&gt;Kong mostly is an OpenResty application made of Lua source files, but also
requires some additional third-party dependencies. We recommend installing
those by following the source install instructions at
&lt;a href="https://docs.konghq.com/install/source/" rel="nofollow"&gt;https://docs.konghq.com/install/source/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Instead of following the second step (Install Kong), clone this repository
and install the latest Lua sources instead of the currently released ones:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/Kong/kong
$ &lt;span class="pl-c1"&gt;cd&lt;/span&gt; kong/

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; you might want to switch to the development branch. See CONTRIBUTING.md&lt;/span&gt;
$ git checkout next

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install the Lua sources&lt;/span&gt;
$ luarocks make&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-running-for-development" class="anchor" aria-hidden="true" href="#running-for-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running for development&lt;/h4&gt;
&lt;p&gt;Check out the &lt;a href="https://github.com/Kong/kong/blob/next/kong.conf.default#L244"&gt;development section&lt;/a&gt;
of the default configuration file for properties to tweak in order to ease
the development process for Kong.&lt;/p&gt;
&lt;p&gt;Modifying the &lt;a href="https://github.com/openresty/lua-nginx-module#lua_package_path"&gt;&lt;code&gt;lua_package_path&lt;/code&gt;&lt;/a&gt;
and &lt;a href="https://github.com/openresty/lua-nginx-module#lua_package_cpath"&gt;&lt;code&gt;lua_package_cpath&lt;/code&gt;&lt;/a&gt;
directives will allow Kong to find your custom plugin's source code wherever it
might be in your system.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h4&gt;
&lt;p&gt;Install the development dependencies (&lt;a href="https://github.com/Olivine-Labs/busted"&gt;busted&lt;/a&gt;, &lt;a href="https://github.com/mpeterv/luacheck"&gt;luacheck&lt;/a&gt;) with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make dev&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Kong relies on three test suites using the &lt;a href="https://github.com/Olivine-Labs/busted"&gt;busted&lt;/a&gt; testing library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unit tests&lt;/li&gt;
&lt;li&gt;Integration tests, which require Postgres and Cassandra to be up and running&lt;/li&gt;
&lt;li&gt;Plugins tests, which require Postgres to be running&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first can simply be run after installing busted and running:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, the integration and plugins tests will spawn a Kong instance and
perform their tests against it. As so, consult/edit the &lt;code&gt;spec/kong_tests.conf&lt;/code&gt;
configuration file to make your test instance point to your Postgres/Cassandra
servers, depending on your needs.&lt;/p&gt;
&lt;p&gt;You can run the integration tests (assuming &lt;strong&gt;both&lt;/strong&gt; Postgres and Cassandra are
running and configured according to &lt;code&gt;spec/kong_tests.conf&lt;/code&gt;) with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make test-integration
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the plugins tests with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make test-plugins
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, all suites can be run at once by simply using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make test-all
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Consult the &lt;a href=".ci/run_tests.sh"&gt;run_tests.sh&lt;/a&gt; script for a more advanced example
usage of the tests suites and the Makefile.&lt;/p&gt;
&lt;p&gt;Finally, a very useful tool in Lua development (as with many other dynamic
languages) is performing static linting of your code. You can use &lt;a href="https://github.com/mpeterv/luacheck"&gt;luacheck&lt;/a&gt;
(installed with &lt;code&gt;make dev&lt;/code&gt;) for this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make lint
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-makefile" class="anchor" aria-hidden="true" href="#makefile"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Makefile&lt;/h4&gt;
&lt;p&gt;When developing, you can use the &lt;code&gt;Makefile&lt;/code&gt; for doing the following operations:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="right"&gt;Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;install&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Install the Kong luarock globally&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;dev&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Install development dependencies&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;lint&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Lint Lua files in &lt;code&gt;kong/&lt;/code&gt; and &lt;code&gt;spec/&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;test&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Run the unit tests suite&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;test-integration&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Run the integration tests suite&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;test-plugins&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Run the plugins test suite&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;test-all&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Run all unit + integration + plugins tests at once&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-enterprise-support--demo" class="anchor" aria-hidden="true" href="#enterprise-support--demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Enterprise Support &amp;amp; Demo&lt;/h2&gt;
&lt;p&gt;If you are working in a large organization you should learn more about &lt;a href="https://konghq.com/kong-enterprise-edition/" rel="nofollow"&gt;Kong
Enterprise&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Copyright 2016-2019 Kong Inc.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Kong</author><guid isPermaLink="false">https://github.com/Kong/kong</guid><pubDate>Wed, 06 Nov 2019 00:05:00 GMT</pubDate></item><item><title>ledgetech/lua-resty-http #6 in Lua, Today</title><link>https://github.com/ledgetech/lua-resty-http</link><description>&lt;p&gt;&lt;i&gt;Lua HTTP client cosocket driver for OpenResty / ngx_lua.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-lua-resty-http" class="anchor" aria-hidden="true" href="#lua-resty-http"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;lua-resty-http&lt;/h1&gt;
&lt;p&gt;Lua HTTP client cosocket driver for &lt;a href="http://openresty.org/" rel="nofollow"&gt;OpenResty&lt;/a&gt; / &lt;a href="https://github.com/openresty/lua-nginx-module"&gt;ngx_lua&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-status" class="anchor" aria-hidden="true" href="#status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Status&lt;/h1&gt;
&lt;p&gt;Production ready.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;HTTP 1.0 and 1.1&lt;/li&gt;
&lt;li&gt;SSL&lt;/li&gt;
&lt;li&gt;Streaming interface to the response body, for predictable memory usage&lt;/li&gt;
&lt;li&gt;Alternative simple interface for singleshot requests without manual connection step&lt;/li&gt;
&lt;li&gt;Chunked and non-chunked transfer encodings&lt;/li&gt;
&lt;li&gt;Keepalive&lt;/li&gt;
&lt;li&gt;Pipelining&lt;/li&gt;
&lt;li&gt;Trailers&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-api" class="anchor" aria-hidden="true" href="#api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;API&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#new"&gt;new&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#connect"&gt;connect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#connect_proxy"&gt;connect_proxy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#set_proxy_options"&gt;set_proxy_options&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#set_timeout"&gt;set_timeout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#set_timeouts"&gt;set_timeouts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ssl_handshake"&gt;ssl_handshake&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#set_keepalive"&gt;set_keepalive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#get_reused_times"&gt;get_reused_times&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#close"&gt;close&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#request"&gt;request&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#request_uri"&gt;request_uri&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#request_pipeline"&gt;request_pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#response"&gt;Response&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#resbody_reader"&gt;body_reader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#resread_body"&gt;read_body&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#resread_trailers"&gt;read_trailers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#proxy"&gt;Proxy&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#proxy_request"&gt;proxy_request&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#proxy_response"&gt;proxy_response&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#utility"&gt;Utility&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#parse_uri"&gt;parse_uri&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#get_client_body_reader"&gt;get_client_body_reader&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-synopsis" class="anchor" aria-hidden="true" href="#synopsis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Synopsis&lt;/h2&gt;
&lt;div class="highlight highlight-source-lua"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;lua_package_path&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/path/to/lua-resty-http/lib/?.lua;;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;;

&lt;span class="pl-c1"&gt;server&lt;/span&gt; {


  location &lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;simpleinterface&lt;/span&gt; {
    resolver &lt;span class="pl-c1"&gt;8.8&lt;/span&gt;.8.8;  &lt;span class="pl-k"&gt;#&lt;/span&gt; use &lt;span class="pl-c1"&gt;Google&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;s open DNS server for an example&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;    content_by_lua_block {&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      -- For simple singleshot requests, use the URI interface.&lt;/span&gt;
&lt;span class="pl-s"&gt;      local http = require "resty.http"&lt;/span&gt;
&lt;span class="pl-s"&gt;      local httpc = http.new()&lt;/span&gt;
&lt;span class="pl-s"&gt;      local res, err = httpc:request_uri("http://example.com/helloworld", {&lt;/span&gt;
&lt;span class="pl-s"&gt;        method = "POST",&lt;/span&gt;
&lt;span class="pl-s"&gt;        body = "a=1&amp;amp;b=2",&lt;/span&gt;
&lt;span class="pl-s"&gt;        headers = {&lt;/span&gt;
&lt;span class="pl-s"&gt;          ["Content-Type"] = "application/x-www-form-urlencoded",&lt;/span&gt;
&lt;span class="pl-s"&gt;        },&lt;/span&gt;
&lt;span class="pl-s"&gt;        keepalive_timeout = 60,&lt;/span&gt;
&lt;span class="pl-s"&gt;        keepalive_pool = 10&lt;/span&gt;
&lt;span class="pl-s"&gt;      })&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      if not res then&lt;/span&gt;
&lt;span class="pl-s"&gt;        ngx.say("failed to request: ", err)&lt;/span&gt;
&lt;span class="pl-s"&gt;        return&lt;/span&gt;
&lt;span class="pl-s"&gt;      end&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      -- In this simple form, there is no manual connection step, so the body is read&lt;/span&gt;
&lt;span class="pl-s"&gt;      -- all in one go, including any trailers, and the connection closed or keptalive&lt;/span&gt;
&lt;span class="pl-s"&gt;      -- for you.&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      ngx.status = res.status&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      for k,v in pairs(res.headers) do&lt;/span&gt;
&lt;span class="pl-s"&gt;          --&lt;/span&gt;
&lt;span class="pl-s"&gt;      end&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      ngx.say(res.body)&lt;/span&gt;
&lt;span class="pl-s"&gt;    }&lt;/span&gt;
&lt;span class="pl-s"&gt;  }&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;  location /genericinterface {&lt;/span&gt;
&lt;span class="pl-s"&gt;    content_by_lua_block {&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      local http = require "resty.http"&lt;/span&gt;
&lt;span class="pl-s"&gt;      local httpc = http.new()&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      -- The generic form gives us more control. We must connect manually.&lt;/span&gt;
&lt;span class="pl-s"&gt;      httpc:set_timeout(500)&lt;/span&gt;
&lt;span class="pl-s"&gt;      httpc:connect("127.0.0.1", 80)&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      -- And request using a path, rather than a full URI.&lt;/span&gt;
&lt;span class="pl-s"&gt;      local res, err = httpc:request({&lt;/span&gt;
&lt;span class="pl-s"&gt;          path = "/helloworld",&lt;/span&gt;
&lt;span class="pl-s"&gt;          headers = {&lt;/span&gt;
&lt;span class="pl-s"&gt;              ["Host"] = "example.com",&lt;/span&gt;
&lt;span class="pl-s"&gt;          },&lt;/span&gt;
&lt;span class="pl-s"&gt;      })&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      if not res then&lt;/span&gt;
&lt;span class="pl-s"&gt;        ngx.say("failed to request: ", err)&lt;/span&gt;
&lt;span class="pl-s"&gt;        return&lt;/span&gt;
&lt;span class="pl-s"&gt;      end&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      -- Now we can use the body_reader iterator, to stream the body according to our desired chunk size.&lt;/span&gt;
&lt;span class="pl-s"&gt;      local reader = res.body_reader&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      repeat&lt;/span&gt;
&lt;span class="pl-s"&gt;        local chunk, err = reader(8192)&lt;/span&gt;
&lt;span class="pl-s"&gt;        if err then&lt;/span&gt;
&lt;span class="pl-s"&gt;          ngx.log(ngx.ERR, err)&lt;/span&gt;
&lt;span class="pl-s"&gt;          break&lt;/span&gt;
&lt;span class="pl-s"&gt;        end&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;        if chunk then&lt;/span&gt;
&lt;span class="pl-s"&gt;          -- process&lt;/span&gt;
&lt;span class="pl-s"&gt;        end&lt;/span&gt;
&lt;span class="pl-s"&gt;      until not chunk&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;      local ok, err = httpc:set_keepalive()&lt;/span&gt;
&lt;span class="pl-s"&gt;      if not ok then&lt;/span&gt;
&lt;span class="pl-s"&gt;        ngx.say("failed to set keepalive: ", err)&lt;/span&gt;
&lt;span class="pl-s"&gt;        return&lt;/span&gt;
&lt;span class="pl-s"&gt;      end&lt;/span&gt;
&lt;span class="pl-s"&gt;    }&lt;/span&gt;
&lt;span class="pl-s"&gt;  }&lt;/span&gt;
&lt;span class="pl-s"&gt;}&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-connection" class="anchor" aria-hidden="true" href="#connection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Connection&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-new" class="anchor" aria-hidden="true" href="#new"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;new&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: httpc = http.new()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Creates the http object. In case of failures, returns &lt;code&gt;nil&lt;/code&gt; and a string describing the error.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-connect" class="anchor" aria-hidden="true" href="#connect"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;connect&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: ok, err = httpc:connect(host, port, options_table?)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;syntax: ok, err = httpc:connect("unix:/path/to/unix.sock", options_table?)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Attempts to connect to the web server.&lt;/p&gt;
&lt;p&gt;Before actually resolving the host name and connecting to the remote backend, this method will always look up the connection pool for matched idle connections created by previous calls of this method.&lt;/p&gt;
&lt;p&gt;An optional Lua table can be specified as the last argument to this method to specify various connect options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pool&lt;/code&gt;
: Specifies a custom name for the connection pool being used. If omitted, then the connection pool name will be generated from the string template &lt;code&gt;&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;lt;unix-socket-path&amp;gt;&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-connect_proxy" class="anchor" aria-hidden="true" href="#connect_proxy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;connect_proxy&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: ok, err = httpc:connect_proxy(proxy_uri, scheme, host, port, proxy_authorization)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Attempts to connect to the web server through the given proxy server. The method accepts the following arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;proxy_uri&lt;/code&gt; - Full URI of the proxy server to use (e.g. &lt;code&gt;http://proxy.example.com:3128/&lt;/code&gt;). Note: Only &lt;code&gt;http&lt;/code&gt; protocol is supported.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scheme&lt;/code&gt; - The protocol to use between the proxy server and the remote host (&lt;code&gt;http&lt;/code&gt; or &lt;code&gt;https&lt;/code&gt;). If &lt;code&gt;https&lt;/code&gt; is specified as the scheme, &lt;code&gt;connect_proxy()&lt;/code&gt; makes a &lt;code&gt;CONNECT&lt;/code&gt; request to establish a TCP tunnel to the remote host through the proxy server.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;host&lt;/code&gt; - The hostname of the remote host to connect to.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;port&lt;/code&gt; - The port of the remote host to connect to.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;proxy_authorization&lt;/code&gt; - The &lt;code&gt;Proxy-Authorization&lt;/code&gt; header value sent to the proxy server via &lt;code&gt;CONNECT&lt;/code&gt; when the &lt;code&gt;scheme&lt;/code&gt; is &lt;code&gt;https&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If an error occurs during the connection attempt, this method returns &lt;code&gt;nil&lt;/code&gt; with a string describing the error. If the connection was successfully established, the method returns &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There's a few key points to keep in mind when using this api:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the scheme is &lt;code&gt;https&lt;/code&gt;, you need to perform the TLS handshake with the remote server manually using the &lt;code&gt;ssl_handshake()&lt;/code&gt; method before sending any requests through the proxy tunnel.&lt;/li&gt;
&lt;li&gt;If the scheme is &lt;code&gt;http&lt;/code&gt;, you need to ensure that the requests you send through the connections conforms to &lt;a href="https://tools.ietf.org/html/rfc7230" rel="nofollow"&gt;RFC 7230&lt;/a&gt; and especially &lt;a href="https://tools.ietf.org/html/rfc7230#section-5.3.2" rel="nofollow"&gt;Section 5.3.2.&lt;/a&gt; which states that the request target must be in absolute form. In practice, this means that when you use &lt;code&gt;send_request()&lt;/code&gt;, the &lt;code&gt;path&lt;/code&gt; must be an absolute URI to the resource (e.g. &lt;code&gt;http://example.com/index.html&lt;/code&gt; instead of just &lt;code&gt;/index.html&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-set_timeout" class="anchor" aria-hidden="true" href="#set_timeout"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;set_timeout&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: httpc:set_timeout(time)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Sets the timeout (in ms) protection for subsequent operations, including the &lt;code&gt;connect&lt;/code&gt; method.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-set_timeouts" class="anchor" aria-hidden="true" href="#set_timeouts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;set_timeouts&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: httpc:set_timeouts(connect_timeout, send_timeout, read_timeout)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Sets the connect timeout threshold, send timeout threshold, and read timeout threshold, respectively, in milliseconds, for subsequent socket operations (connect, send, receive, and iterators returned from receiveuntil).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ssl_handshake" class="anchor" aria-hidden="true" href="#ssl_handshake"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ssl_handshake&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: session, err = httpc:ssl_handshake(session, host, verify)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Performs an SSL handshake on the TCP connection, only available in ngx_lua &amp;gt; v0.9.11&lt;/p&gt;
&lt;p&gt;See docs for &lt;a href="https://github.com/openresty/lua-nginx-module#ngxsockettcp"&gt;ngx.socket.tcp&lt;/a&gt; for details.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-set_keepalive" class="anchor" aria-hidden="true" href="#set_keepalive"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;set_keepalive&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: ok, err = httpc:set_keepalive(max_idle_timeout, pool_size)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Attempts to puts the current connection into the ngx_lua cosocket connection pool.&lt;/p&gt;
&lt;p&gt;You can specify the max idle timeout (in ms) when the connection is in the pool and the maximal size of the pool every nginx worker process.&lt;/p&gt;
&lt;p&gt;Only call this method in the place you would have called the &lt;code&gt;close&lt;/code&gt; method instead. Calling this method will immediately turn the current http object into the &lt;code&gt;closed&lt;/code&gt; state. Any subsequent operations other than &lt;code&gt;connect()&lt;/code&gt; on the current object will return the &lt;code&gt;closed&lt;/code&gt; error.&lt;/p&gt;
&lt;p&gt;Note that calling this instead of &lt;code&gt;close&lt;/code&gt; is "safe" in that it will conditionally close depending on the type of request. Specifically, a &lt;code&gt;1.0&lt;/code&gt; request without &lt;code&gt;Connection: Keep-Alive&lt;/code&gt; will be closed, as will a &lt;code&gt;1.1&lt;/code&gt; request with &lt;code&gt;Connection: Close&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In case of success, returns &lt;code&gt;1&lt;/code&gt;. In case of errors, returns &lt;code&gt;nil, err&lt;/code&gt;. In the case where the connection is conditionally closed as described above, returns &lt;code&gt;2&lt;/code&gt; and the error string &lt;code&gt;connection must be closed&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-set_proxy_options" class="anchor" aria-hidden="true" href="#set_proxy_options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;set_proxy_options&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: httpc:set_proxy_options(opts)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Configure an http proxy to be used with this client instance. The &lt;code&gt;opts&lt;/code&gt; is a table that accepts the following fields:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;http_proxy&lt;/code&gt; - an URI to a proxy server to be used with http requests&lt;/li&gt;
&lt;li&gt;&lt;code&gt;http_proxy_authorization&lt;/code&gt; - a default &lt;code&gt;Proxy-Authorization&lt;/code&gt; header value to be used with &lt;code&gt;http_proxy&lt;/code&gt;, e.g. &lt;code&gt;Basic ZGVtbzp0ZXN0&lt;/code&gt;, which will be overriden if the &lt;code&gt;Proxy-Authorization&lt;/code&gt; request header is present.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;https_proxy&lt;/code&gt; - an URI to a proxy server to be used with https requests&lt;/li&gt;
&lt;li&gt;&lt;code&gt;https_proxy_authorization&lt;/code&gt; - as &lt;code&gt;http_proxy_authorization&lt;/code&gt; but for use with &lt;code&gt;https_proxy&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;no_proxy&lt;/code&gt; - a comma separated list of hosts that should not be proxied.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that proxy options are only applied when using the high-level &lt;code&gt;request_uri()&lt;/code&gt; API.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-get_reused_times" class="anchor" aria-hidden="true" href="#get_reused_times"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;get_reused_times&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: times, err = httpc:get_reused_times()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This method returns the (successfully) reused times for the current connection. In case of error, it returns &lt;code&gt;nil&lt;/code&gt; and a string describing the error.&lt;/p&gt;
&lt;p&gt;If the current connection does not come from the built-in connection pool, then this method always returns &lt;code&gt;0&lt;/code&gt;, that is, the connection has never been reused (yet). If the connection comes from the connection pool, then the return value is always non-zero. So this method can also be used to determine if the current connection comes from the pool.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-close" class="anchor" aria-hidden="true" href="#close"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;close&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: ok, err = http:close()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Closes the current connection and returns the status.&lt;/p&gt;
&lt;p&gt;In case of success, returns &lt;code&gt;1&lt;/code&gt;. In case of errors, returns &lt;code&gt;nil&lt;/code&gt; with a string describing the error.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-requesting" class="anchor" aria-hidden="true" href="#requesting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requesting&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-request" class="anchor" aria-hidden="true" href="#request"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;request&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: res, err = httpc:request(params)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Returns a &lt;code&gt;res&lt;/code&gt; table or &lt;code&gt;nil&lt;/code&gt; and an error message.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;params&lt;/code&gt; table accepts the following fields:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;version&lt;/code&gt; The HTTP version number, currently supporting 1.0 or 1.1.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;method&lt;/code&gt; The HTTP method string.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;path&lt;/code&gt; The path string.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;query&lt;/code&gt; The query string, presented as either a literal string or Lua table..&lt;/li&gt;
&lt;li&gt;&lt;code&gt;headers&lt;/code&gt; A table of request headers.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;body&lt;/code&gt; The request body as a string, or an iterator function (see &lt;a href="#get_client_body_reader"&gt;get_client_body_reader&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ssl_verify&lt;/code&gt; Verify SSL cert matches hostname&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When the request is successful, &lt;code&gt;res&lt;/code&gt; will contain the following fields:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;status&lt;/code&gt; The status code.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reason&lt;/code&gt; The status reason phrase.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;headers&lt;/code&gt; A table of headers. Multiple headers with the same field name will be presented as a table of values.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;has_body&lt;/code&gt; A boolean flag indicating if there is a body to be read.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;body_reader&lt;/code&gt; An iterator function for reading the body in a streaming fashion.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;read_body&lt;/code&gt; A method to read the entire body into a string.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;read_trailers&lt;/code&gt; A method to merge any trailers underneath the headers, after reading the body.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-request_uri" class="anchor" aria-hidden="true" href="#request_uri"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;request_uri&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: res, err = httpc:request_uri(uri, params)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The simple interface. Options supplied in the &lt;code&gt;params&lt;/code&gt; table are the same as in the generic interface, and will override components found in the uri itself.&lt;/p&gt;
&lt;p&gt;There are 3 additional parameters for controlling keepalives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;keepalive&lt;/code&gt; Set to &lt;code&gt;false&lt;/code&gt; to disable keepalives and immediately close the connection.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;keepalive_timeout&lt;/code&gt; The maximal idle timeout (ms). Defaults to &lt;code&gt;lua_socket_keepalive_timeout&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;keepalive_pool&lt;/code&gt; The maximum number of connections in the pool. Defaults to &lt;code&gt;lua_socket_pool_size&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this mode, there is no need to connect manually first. The connection is made on your behalf, suiting cases where you simply need to grab a URI without too much hassle.&lt;/p&gt;
&lt;p&gt;Additionally there is no ability to stream the response body in this mode. If the request is successful, &lt;code&gt;res&lt;/code&gt; will contain the following fields:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;status&lt;/code&gt; The status code.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;headers&lt;/code&gt; A table of headers.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;body&lt;/code&gt; The response body as a string.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-request_pipeline" class="anchor" aria-hidden="true" href="#request_pipeline"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;request_pipeline&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: responses, err = httpc:request_pipeline(params)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This method works as per the &lt;a href="#request"&gt;request&lt;/a&gt; method above, but &lt;code&gt;params&lt;/code&gt; is instead a table of param tables. Each request is sent in order, and &lt;code&gt;responses&lt;/code&gt; is returned as a table of response handles. For example:&lt;/p&gt;
&lt;div class="highlight highlight-source-lua"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;local&lt;/span&gt; responses &lt;span class="pl-k"&gt;=&lt;/span&gt; httpc:&lt;span class="pl-c1"&gt;request_pipeline&lt;/span&gt;{
  {
    path &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/b&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
  },
  {
    path &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/c&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
  },
  {
    path &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/d&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
  }
}

&lt;span class="pl-k"&gt;for&lt;/span&gt; i,r &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;ipairs&lt;/span&gt;(responses) &lt;span class="pl-k"&gt;do&lt;/span&gt;
  &lt;span class="pl-k"&gt;if&lt;/span&gt; r.&lt;span class="pl-smi"&gt;status&lt;/span&gt; &lt;span class="pl-k"&gt;then&lt;/span&gt;
    ngx.&lt;span class="pl-c1"&gt;say&lt;/span&gt;(r.&lt;span class="pl-smi"&gt;status&lt;/span&gt;)
    ngx.&lt;span class="pl-c1"&gt;say&lt;/span&gt;(r:&lt;span class="pl-c1"&gt;read_body&lt;/span&gt;())
  &lt;span class="pl-k"&gt;end&lt;/span&gt;
&lt;span class="pl-k"&gt;end&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Due to the nature of pipelining, no responses are actually read until you attempt to use the response fields (status / headers etc). And since the responses are read off in order, you must read the entire body (and any trailers if you have them), before attempting to read the next response.&lt;/p&gt;
&lt;p&gt;Note this doesn't preclude the use of the streaming response body reader. Responses can still be streamed, so long as the entire body is streamed before attempting to access the next response.&lt;/p&gt;
&lt;p&gt;Be sure to test at least one field (such as status) before trying to use the others, in case a socket read error has occurred.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-response" class="anchor" aria-hidden="true" href="#response"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Response&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-resbody_reader" class="anchor" aria-hidden="true" href="#resbody_reader"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;res.body_reader&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;body_reader&lt;/code&gt; iterator can be used to stream the response body in chunk sizes of your choosing, as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-lua"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;local&lt;/span&gt; reader &lt;span class="pl-k"&gt;=&lt;/span&gt; res.&lt;span class="pl-smi"&gt;body_reader&lt;/span&gt;

&lt;span class="pl-k"&gt;repeat&lt;/span&gt;
  &lt;span class="pl-k"&gt;local&lt;/span&gt; chunk, err &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;reader&lt;/span&gt;(&lt;span class="pl-c1"&gt;8192&lt;/span&gt;)
  &lt;span class="pl-k"&gt;if&lt;/span&gt; err &lt;span class="pl-k"&gt;then&lt;/span&gt;
    ngx.&lt;span class="pl-c1"&gt;log&lt;/span&gt;(ngx.&lt;span class="pl-smi"&gt;ERR&lt;/span&gt;, err)
    &lt;span class="pl-k"&gt;break&lt;/span&gt;
  &lt;span class="pl-k"&gt;end&lt;/span&gt;

  &lt;span class="pl-k"&gt;if&lt;/span&gt; chunk &lt;span class="pl-k"&gt;then&lt;/span&gt;
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; process&lt;/span&gt;
  &lt;span class="pl-k"&gt;end&lt;/span&gt;
&lt;span class="pl-k"&gt;until&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; chunk&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If the reader is called with no arguments, the behaviour depends on the type of connection. If the response is encoded as chunked, then the iterator will return the chunks as they arrive. If not, it will simply return the entire body.&lt;/p&gt;
&lt;p&gt;Note that the size provided is actually a &lt;strong&gt;maximum&lt;/strong&gt; size. So in the chunked transfer case, you may get chunks smaller than the size you ask, as a remainder of the actual HTTP chunks.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-resread_body" class="anchor" aria-hidden="true" href="#resread_body"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;res:read_body&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: body, err = res:read_body()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Reads the entire body into a local string.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-resread_trailers" class="anchor" aria-hidden="true" href="#resread_trailers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;res:read_trailers&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: res:read_trailers()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This merges any trailers underneath the &lt;code&gt;res.headers&lt;/code&gt; table itself. Must be called after reading the body.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-proxy" class="anchor" aria-hidden="true" href="#proxy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Proxy&lt;/h1&gt;
&lt;p&gt;There are two convenience methods for when one simply wishes to proxy the current request to the connected upstream, and safely send it downstream to the client, as a reverse proxy. A complete example:&lt;/p&gt;
&lt;div class="highlight highlight-source-lua"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;local&lt;/span&gt; http &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;require&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;resty.http&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-k"&gt;local&lt;/span&gt; httpc &lt;span class="pl-k"&gt;=&lt;/span&gt; http.&lt;span class="pl-c1"&gt;new&lt;/span&gt;()

httpc:&lt;span class="pl-c1"&gt;set_timeout&lt;/span&gt;(&lt;span class="pl-c1"&gt;500&lt;/span&gt;)
&lt;span class="pl-k"&gt;local&lt;/span&gt; ok, err &lt;span class="pl-k"&gt;=&lt;/span&gt; httpc:&lt;span class="pl-c1"&gt;connect&lt;/span&gt;(HOST, PORT)

&lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; ok &lt;span class="pl-k"&gt;then&lt;/span&gt;
  ngx.&lt;span class="pl-c1"&gt;log&lt;/span&gt;(ngx.&lt;span class="pl-smi"&gt;ERR&lt;/span&gt;, err)
  &lt;span class="pl-k"&gt;return&lt;/span&gt;
&lt;span class="pl-k"&gt;end&lt;/span&gt;

httpc:&lt;span class="pl-c1"&gt;set_timeout&lt;/span&gt;(&lt;span class="pl-c1"&gt;2000&lt;/span&gt;)
httpc:&lt;span class="pl-c1"&gt;proxy_response&lt;/span&gt;(httpc:&lt;span class="pl-c1"&gt;proxy_request&lt;/span&gt;())
httpc:&lt;span class="pl-c1"&gt;set_keepalive&lt;/span&gt;()&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-proxy_request" class="anchor" aria-hidden="true" href="#proxy_request"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;proxy_request&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: local res, err = httpc:proxy_request(request_body_chunk_size?)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Performs a request using the current client request arguments, effectively proxying to the connected upstream. The request body will be read in a streaming fashion, according to &lt;code&gt;request_body_chunk_size&lt;/code&gt; (see &lt;a href="#get_client_body_reader"&gt;documentation on the client body reader&lt;/a&gt; below).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-proxy_response" class="anchor" aria-hidden="true" href="#proxy_response"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;proxy_response&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: httpc:proxy_response(res, chunksize?)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Sets the current response based on the given &lt;code&gt;res&lt;/code&gt;. Ensures that hop-by-hop headers are not sent downstream, and will read the response according to &lt;code&gt;chunksize&lt;/code&gt; (see &lt;a href="#resbody_reader"&gt;documentation on the body reader&lt;/a&gt; above).&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-utility" class="anchor" aria-hidden="true" href="#utility"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Utility&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-parse_uri" class="anchor" aria-hidden="true" href="#parse_uri"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;parse_uri&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: local scheme, host, port, path, query? = unpack(httpc:parse_uri(uri, query_in_path?))&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This is a convenience function allowing one to more easily use the generic interface, when the input data is a URI.&lt;/p&gt;
&lt;p&gt;As of version &lt;code&gt;0.10&lt;/code&gt;, the optional &lt;code&gt;query_in_path&lt;/code&gt; parameter was added, which specifies whether the querystring is to be included in the &lt;code&gt;path&lt;/code&gt; return value, or separately as its own return value. This defaults to &lt;code&gt;true&lt;/code&gt; in order to maintain backwards compatibility. When set to &lt;code&gt;false&lt;/code&gt;, &lt;code&gt;path&lt;/code&gt; will only include the path, and &lt;code&gt;query&lt;/code&gt; will contain the URI args, not including the &lt;code&gt;?&lt;/code&gt; delimiter.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-get_client_body_reader" class="anchor" aria-hidden="true" href="#get_client_body_reader"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;get_client_body_reader&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;syntax: reader, err = httpc:get_client_body_reader(chunksize?, sock?)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Returns an iterator function which can be used to read the downstream client request body in a streaming fashion. You may also specify an optional default chunksize (default is &lt;code&gt;65536&lt;/code&gt;), or an already established socket in
place of the client request.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;div class="highlight highlight-source-lua"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;local&lt;/span&gt; req_reader &lt;span class="pl-k"&gt;=&lt;/span&gt; httpc:&lt;span class="pl-c1"&gt;get_client_body_reader&lt;/span&gt;()

&lt;span class="pl-k"&gt;repeat&lt;/span&gt;
  &lt;span class="pl-k"&gt;local&lt;/span&gt; chunk, err &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;req_reader&lt;/span&gt;(&lt;span class="pl-c1"&gt;8192&lt;/span&gt;)
  &lt;span class="pl-k"&gt;if&lt;/span&gt; err &lt;span class="pl-k"&gt;then&lt;/span&gt;
    ngx.&lt;span class="pl-c1"&gt;log&lt;/span&gt;(ngx.&lt;span class="pl-smi"&gt;ERR&lt;/span&gt;, err)
    &lt;span class="pl-k"&gt;break&lt;/span&gt;
  &lt;span class="pl-k"&gt;end&lt;/span&gt;

  &lt;span class="pl-k"&gt;if&lt;/span&gt; chunk &lt;span class="pl-k"&gt;then&lt;/span&gt;
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; process&lt;/span&gt;
  &lt;span class="pl-k"&gt;end&lt;/span&gt;
&lt;span class="pl-k"&gt;until&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; chunk&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This iterator can also be used as the value for the body field in request params, allowing one to stream the request body into a proxied upstream request.&lt;/p&gt;
&lt;div class="highlight highlight-source-lua"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;local&lt;/span&gt; client_body_reader, err &lt;span class="pl-k"&gt;=&lt;/span&gt; httpc:&lt;span class="pl-c1"&gt;get_client_body_reader&lt;/span&gt;()

&lt;span class="pl-k"&gt;local&lt;/span&gt; res, err &lt;span class="pl-k"&gt;=&lt;/span&gt; httpc:&lt;span class="pl-c1"&gt;request&lt;/span&gt;{
   path &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/helloworld&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
   body &lt;span class="pl-k"&gt;=&lt;/span&gt; client_body_reader,
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If &lt;code&gt;sock&lt;/code&gt; is specified,&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-author" class="anchor" aria-hidden="true" href="#author"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Author&lt;/h1&gt;
&lt;p&gt;James Hurst &lt;a href="mailto:james@pintsized.co.uk"&gt;james@pintsized.co.uk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Originally started life based on &lt;a href="https://github.com/bakins/lua-resty-http-simple"&gt;https://github.com/bakins/lua-resty-http-simple&lt;/a&gt;. Cosocket docs and implementation borrowed from the other lua-resty-* cosocket modules.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-licence" class="anchor" aria-hidden="true" href="#licence"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Licence&lt;/h1&gt;
&lt;p&gt;This module is licensed under the 2-clause BSD license.&lt;/p&gt;
&lt;p&gt;Copyright (c) 2013-2016, James Hurst &lt;a href="mailto:james@pintsized.co.uk"&gt;james@pintsized.co.uk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All rights reserved.&lt;/p&gt;
&lt;p&gt;Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ledgetech</author><guid isPermaLink="false">https://github.com/ledgetech/lua-resty-http</guid><pubDate>Wed, 06 Nov 2019 00:06:00 GMT</pubDate></item><item><title>cmusatyalab/openface #7 in Lua, Today</title><link>https://github.com/cmusatyalab/openface</link><description>&lt;p&gt;&lt;i&gt;Face recognition with deep neural networks.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-openface-----" class="anchor" aria-hidden="true" href="#openface-----"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenFace • &lt;a href="http://travis-ci.org/cmusatyalab/openface" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/7649857bc22a4061aea8a38706cc64b6bca51e45/68747470733a2f2f7472617669732d63692e6f72672f636d7573617479616c61622f6f70656e666163652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/cmusatyalab/openface.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://github.com/cmusatyalab/openface/releases"&gt;&lt;img src="https://camo.githubusercontent.com/009f288da0baa589849c22cdbc1185bfeaf924ab/687474703a2f2f696d672e736869656c64732e696f2f62616467652f72656c656173652d302e322e312d626c75652e7376673f7374796c653d666c6174" alt="Release" data-canonical-src="http://img.shields.io/badge/release-0.2.1-blue.svg?style=flat" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/0257a158db7f15a3a2b76dfd75be916fda130867/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4170616368652d2d322d626c75652e7376673f7374796c653d666c6174" alt="License" data-canonical-src="http://img.shields.io/badge/license-Apache--2-blue.svg?style=flat" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://gitter.im/cmusatyalab/openface" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" alt="Gitter" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Free and open source face recognition with
deep neural networks.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;Website: &lt;a href="http://cmusatyalab.github.io/openface/" rel="nofollow"&gt;http://cmusatyalab.github.io/openface/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://openface-api.readthedocs.org/en/latest/index.html" rel="nofollow"&gt;API Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the
&lt;a href="https://groups.google.com/forum/#!forum/cmu-openface" rel="nofollow"&gt;cmu-openface group&lt;/a&gt;
or the
&lt;a href="https://gitter.im/cmusatyalab/openface" rel="nofollow"&gt;gitter chat&lt;/a&gt;
for discussions and installation issues.&lt;/li&gt;
&lt;li&gt;Development discussions and bugs reports are on the
&lt;a href="https://github.com/cmusatyalab/openface/issues"&gt;issue tracker&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;This research was supported by the National Science Foundation (NSF)
under grant number CNS-1518865.  Additional support
was provided by the Intel Corporation, Google, Vodafone, NVIDIA, and the
Conklin Kistler family fund.  Any opinions, findings, conclusions or
recommendations expressed in this material are those of the authors
and should not be attributed to their employers or funding sources.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-whats-in-this-repository" class="anchor" aria-hidden="true" href="#whats-in-this-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's in this repository?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface/tree/master/batch-represent"&gt;batch-represent&lt;/a&gt;: Generate representations from
a batch of images. &lt;a href="https://gist.github.com/bamos/f03037f5df7e05ad0cc8"&gt;Example directory structure.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface/tree/master/demos/web"&gt;demos/web&lt;/a&gt;: Real-time web demo.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface/tree/master/demos/compare.py"&gt;demos/compare.py&lt;/a&gt;: Demo to compare two images.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface/tree/master/demos/vis-outputs.lua"&gt;demos/vis-outputs.lua&lt;/a&gt;: Demo to
visualize the network's outputs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface/tree/master/demos/classifier.py"&gt;demos/classifier.py&lt;/a&gt;: Demo to train and use classifiers.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface/blob/master/demos/classifier_webcam.py"&gt;demos/classifier_webcam.py&lt;/a&gt;: Demo to use a trained classifier on a webcam stream.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface/blob/master/evaluation"&gt;evaluation&lt;/a&gt;: LFW accuracy evaluation scripts.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface/tree/master/openface"&gt;openface&lt;/a&gt;: Python library code.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface/tree/master/models"&gt;models&lt;/a&gt;: Model directory for openface and 3rd party libraries.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface/tree/master/tests"&gt;tests&lt;/a&gt;: Tests for scripts and library code, including neural network training.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface/tree/master/training"&gt;training&lt;/a&gt;: Scripts to train new OpenFace neural network models.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface/tree/master/util"&gt;util&lt;/a&gt;: Utility scripts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-citations" class="anchor" aria-hidden="true" href="#citations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citations&lt;/h1&gt;
&lt;p&gt;Please cite OpenFace in your publications if it helps your research.
The following is a &lt;a href="http://www.bibtex.org/" rel="nofollow"&gt;BibTeX&lt;/a&gt; and plaintext reference for our
&lt;a href="http://reports-archive.adm.cs.cmu.edu/anon/anon/2016/CMU-CS-16-118.pdf" rel="nofollow"&gt;OpenFace tech report&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@techreport{amos2016openface,
  title={OpenFace: A general-purpose face recognition
    library with mobile applications},
  author={Amos, Brandon and Bartosz Ludwiczuk and Satyanarayanan, Mahadev},
  year={2016},
  institution={CMU-CS-16-118, CMU School of Computer Science},
}

B. Amos, B. Ludwiczuk, M. Satyanarayanan,
"Openface: A general-purpose face recognition library with mobile applications,"
CMU-CS-16-118, CMU School of Computer Science, Tech. Rep., 2016.
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-licensing" class="anchor" aria-hidden="true" href="#licensing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Licensing&lt;/h1&gt;
&lt;p&gt;Unless otherwise stated, the source code and trained Torch and Python
model files are copyright Carnegie Mellon University and licensed
under the &lt;a href="./LICENSE"&gt;Apache 2.0 License&lt;/a&gt;.
Portions from the following third party sources have
been modified and are included in this repository.
These portions are noted in the source files and are
copyright their respective authors with
the licenses listed.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Project&lt;/th&gt;
&lt;th&gt;Modified&lt;/th&gt;
&lt;th&gt;License&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/Atcold/torch-TripletEmbedding"&gt;Atcold/torch-TripletEmbedding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;MIT&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/facebook/fbnn"&gt;facebook/fbnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;BSD&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>cmusatyalab</author><guid isPermaLink="false">https://github.com/cmusatyalab/openface</guid><pubDate>Wed, 06 Nov 2019 00:07:00 GMT</pubDate></item><item><title>rubbertoe98/FiveM-Scripts #8 in Lua, Today</title><link>https://github.com/rubbertoe98/FiveM-Scripts</link><description>&lt;p&gt;&lt;i&gt;Compilation of my publically released FiveM code&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-fivem-scripts" class="anchor" aria-hidden="true" href="#fivem-scripts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FiveM-Scripts&lt;/h1&gt;
&lt;p&gt;Compilation of my publicly released code&lt;/p&gt;
&lt;p&gt;Feel free to make improvements with PRs&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rubbertoe98</author><guid isPermaLink="false">https://github.com/rubbertoe98/FiveM-Scripts</guid><pubDate>Wed, 06 Nov 2019 00:08:00 GMT</pubDate></item></channel></rss>