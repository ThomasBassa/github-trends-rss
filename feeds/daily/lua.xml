<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Lua, Today</title><link>https://github.com/trending/lua?since=daily</link><description>The top repositories on GitHub for lua, measured daily</description><pubDate>Thu, 12 Dec 2019 01:06:13 GMT</pubDate><lastBuildDate>Thu, 12 Dec 2019 01:06:13 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>Openarl/PathOfBuilding #1 in Lua, Today</title><link>https://github.com/Openarl/PathOfBuilding</link><description>&lt;p&gt;&lt;i&gt;Offline build planner for Path of Exile.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-path-of-building" class="anchor" aria-hidden="true" href="#path-of-building"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Path of Building&lt;/h1&gt;
&lt;p&gt;Welcome to Path of Building, an offline build planner for Path of Exile!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Comprehensive offence + defence calculations:
&lt;ul&gt;
&lt;li&gt;Calculate your skill DPS, damage over time, life/mana/ES totals and much more!&lt;/li&gt;
&lt;li&gt;Can factor in auras, buffs, charges, curses, monster resistances and more, to estimate your effective DPS&lt;/li&gt;
&lt;li&gt;Also calculates life/mana reservations&lt;/li&gt;
&lt;li&gt;Shows a summary of character stats in the side bar, as well as a detailed calculations breakdown tab which can show you how the stats were derived&lt;/li&gt;
&lt;li&gt;Supports all skills and support gems, and most passives and item modifiers
&lt;ul&gt;
&lt;li&gt;Throughout the program, supported modifiers will show in blue and unsupported ones in red&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Full support for minions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Passive skill tree planner:
&lt;ul&gt;
&lt;li&gt;Support for jewels including most radius/conversion jewels&lt;/li&gt;
&lt;li&gt;Features alternate path tracing (mouse over a sequence of nodes while holding shift, then click to allocate them all)&lt;/li&gt;
&lt;li&gt;Fully intergrated with the offence/defence calculations; see exactly how each node will affect your character!&lt;/li&gt;
&lt;li&gt;Can import PathOfExile.com and PoEPlanner.com passive tree links; links shortened with PoEURL.com also work&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Skill planner:
&lt;ul&gt;
&lt;li&gt;Add any number of main or supporting skills to your build&lt;/li&gt;
&lt;li&gt;Supporting skills (auras, curses, buffs) can be toggled on and off&lt;/li&gt;
&lt;li&gt;Automatically applies Socketed Gem modifiers from the item a skill is socketed into&lt;/li&gt;
&lt;li&gt;Automatically applies support gems granted by items&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Item planner:
&lt;ul&gt;
&lt;li&gt;Add items from in game by copying and pasting them straight into the program!&lt;/li&gt;
&lt;li&gt;Automatically adds quality to non-corrupted items&lt;/li&gt;
&lt;li&gt;Fully intergrated with the offence/defence calculations; see exactly how much of an upgrade a given item is!&lt;/li&gt;
&lt;li&gt;Contains a searchable database of all uniques that are currently in game (and some that aren't yet!)
&lt;ul&gt;
&lt;li&gt;You can choose the modifier rolls when you add a unique to your build&lt;/li&gt;
&lt;li&gt;Includes all league-specific items and legacy variants&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Features an item crafting system:
&lt;ul&gt;
&lt;li&gt;You can select from any of the game's base item types&lt;/li&gt;
&lt;li&gt;You can select prefix/suffix modifiers from lists&lt;/li&gt;
&lt;li&gt;Custom modifiers can be added, with Master and Essence modifiers available&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Also contains a database of rare item templates:
&lt;ul&gt;
&lt;li&gt;Allows you to create rare items for your build to approximate the gear you will be using&lt;/li&gt;
&lt;li&gt;Choose which modifiers appear on each item, and the rolls for each modifier, to suit your needs&lt;/li&gt;
&lt;li&gt;Has templates that should cover the majority of builds (inb4 'why is there no coral amulet?')&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Other features:
&lt;ul&gt;
&lt;li&gt;You can import passive tree, items, and skills from existing characters&lt;/li&gt;
&lt;li&gt;Share builds with other users by generating a share code&lt;/li&gt;
&lt;li&gt;Automatic updating; most updates will only take a couple of seconds to apply&lt;/li&gt;
&lt;li&gt;Somewhat more open source than usual (look in %ProgramData%\Path of Building if you're interested)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-download" class="anchor" aria-hidden="true" href="#download"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download&lt;/h2&gt;
&lt;p&gt;Head over to the &lt;a href="https://github.com/Openarl/PathOfBuilding/releases"&gt;Releases&lt;/a&gt; page to download the installer.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-donate" class="anchor" aria-hidden="true" href="#donate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Donate&lt;/h2&gt;
&lt;p&gt;If you'd like to help support the development of Path of Building, I have a &lt;a href="https://www.patreon.com/openarl" rel="nofollow"&gt;Patreon page&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-screenshots" class="anchor" aria-hidden="true" href="#screenshots"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Screenshots&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://cloud.githubusercontent.com/assets/19189971/18089779/f0fe23fa-6f04-11e6-8ed7-ff7d5b9f867a.png"&gt;&lt;img src="https://cloud.githubusercontent.com/assets/19189971/18089779/f0fe23fa-6f04-11e6-8ed7-ff7d5b9f867a.png" alt="ss1" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://cloud.githubusercontent.com/assets/19189971/18089778/f0f923f0-6f04-11e6-89c2-b2c1410d3583.png"&gt;&lt;img src="https://cloud.githubusercontent.com/assets/19189971/18089778/f0f923f0-6f04-11e6-89c2-b2c1410d3583.png" alt="ss2" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://cloud.githubusercontent.com/assets/19189971/18089780/f0ff234a-6f04-11e6-8c88-6193fe59a5c4.png"&gt;&lt;img src="https://cloud.githubusercontent.com/assets/19189971/18089780/f0ff234a-6f04-11e6-8c88-6193fe59a5c4.png" alt="ss3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h2&gt;
&lt;p&gt;You can find the full version history &lt;a href="CHANGELOG.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Openarl</author><guid isPermaLink="false">https://github.com/Openarl/PathOfBuilding</guid><pubDate>Thu, 12 Dec 2019 00:01:00 GMT</pubDate></item><item><title>DiscworldZA/gta-resources #2 in Lua, Today</title><link>https://github.com/DiscworldZA/gta-resources</link><description>&lt;p&gt;&lt;i&gt;All the DiscworldZA GTA Resources&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Hi, my name is &lt;a href="https://twitter.com/DiscworldZA" rel="nofollow"&gt;DiscworldZA&lt;/a&gt;. I create GTA V mods for FiveM.&lt;/p&gt;
&lt;p&gt;I create mods while streaming. Give me a follow here for notifications &lt;a href="https://www.twitch.tv/DiscworldZA" rel="nofollow"&gt;Twitch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is my repo for all the mods I create.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-feedback" class="anchor" aria-hidden="true" href="#feedback"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Feedback.&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Highly recommended&lt;/li&gt;
&lt;li&gt;Report Issues &lt;a href="https://github.com/DiscworldZA/gta-resources/issues"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Want a Mod or Feature? Request &lt;a href="https://github.com/DiscworldZA/gta-resources/issues"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Changelog &lt;a href="https://github.com/DiscworldZA/gta-resources/blob/master/changelog.md"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will create almost anything you can think of. Feel free to request it and I will see about creating it.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-configuration" class="anchor" aria-hidden="true" href="#configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration&lt;/h1&gt;
&lt;p&gt;Most server owners are not developers but understand the basics. My mods are created highly configurable. Every location or price for the mods will be able to be configured. If you want more configuration options create an &lt;a href="https://github.com/DiscworldZA/gta-resources/issues"&gt;issue&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All ReadMe contains Configuration sections&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-explanation" class="anchor" aria-hidden="true" href="#explanation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Explanation&lt;/h1&gt;
&lt;p&gt;Most of my mods work with the &lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-base"&gt;Base&lt;/a&gt; mod and &lt;a href="https://github.com/ESX-Org/es_extended"&gt;ESX&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The reasoning behind this is to simplify my process and have all my mods up to the same standard and address bug fixing over the whole platform rather individual broken sections.
It is the same concept that &lt;a href="https://github.com/ESX-Org/es_extended"&gt;ESX&lt;/a&gt; works on but for all my needs.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-requirements-for-all-mods" class="anchor" aria-hidden="true" href="#requirements-for-all-mods"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements for all mods&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ESX-Org/es_extended"&gt;ESX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mythicrp/mythic_notify"&gt;Mythic Notify&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ESX-Org/esx_addonaccount"&gt;ESX Add on Account&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-mod-list" class="anchor" aria-hidden="true" href="#mod-list"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mod List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-base"&gt;Disc-Base&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-ammo"&gt;Disc-Ammo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-armory"&gt;Disc-Armory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-autorepair"&gt;Disc-AutoRepair&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-billing"&gt;Disc-Billing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-boot"&gt;Disc-Boot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-compensation"&gt;Disc-Compensation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-carthief"&gt;Disc-CarThief&lt;/a&gt; (my version of &lt;a href="https://github.com/KlibrDM/esx_carthief"&gt;esx_carthief&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-dragme"&gt;Disc-DragMe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-drugruns"&gt;Disc-DrugRuns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-drugsales"&gt;Disc-DrugSales&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-gcphone"&gt;Disc-GcPhone&lt;/a&gt; (add-on to &lt;a href="https://github.com/N3MTV/gcphone"&gt;gcphone&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-hotwire"&gt;Disc-HotWire&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-hud"&gt;Disc-HUD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-identification"&gt;Disc-Identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-import"&gt;Disc-Import&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-inventoryhud"&gt;Disc-InventoryHUD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-jobcars"&gt;Disc-JobCars&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-panic"&gt;Disc-Panic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-property"&gt;Disc-Property&lt;/a&gt; (BETA)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-showid"&gt;Disc-ShowId&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-social"&gt;Disc-Social&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-tax"&gt;Disc-Tax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-teleport"&gt;Disc-Teleport&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-vehiclepick"&gt;Disc-VehiclePick&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-vehiclesales"&gt;Disc-VehicleSales&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-warrant"&gt;Disc-Warrant&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-ideas" class="anchor" aria-hidden="true" href="#ideas"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ideas&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;NPC Jobs (These are all mods to add depth to the ESX versions)
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-cops"&gt;Disc-Cops&lt;/a&gt; (WIP)&lt;/li&gt;
&lt;li&gt;Disc-EMS&lt;/li&gt;
&lt;li&gt;Disc-Mechanic&lt;/li&gt;
&lt;li&gt;Disc-CarSales&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unique Garages&lt;/li&gt;
&lt;li&gt;Server Wide Robbery Mechanic&lt;/li&gt;
&lt;li&gt;Search Warrants for Disc-Properties via Disc-Warrants&lt;/li&gt;
&lt;li&gt;Automatic BOLO/Facial Detection&lt;/li&gt;
&lt;li&gt;Handheld Police and EMS devices&lt;/li&gt;
&lt;li&gt;Variable Drug Import and Selling (Allow Drug importing via ship or truck) (Allow Large shipments of drugs)&lt;/li&gt;
&lt;li&gt;Expanding EMS and Cop Procedure (Require documentation to be filed)&lt;/li&gt;
&lt;li&gt;Mechanic Repair Expansion&lt;/li&gt;
&lt;li&gt;Vehicle Shop Documentation (License and Registration)&lt;/li&gt;
&lt;li&gt;Cop Evidence Mechanic&lt;/li&gt;
&lt;li&gt;Judge Job (Allow search warrant authorization, Allow cases to be handled)&lt;/li&gt;
&lt;li&gt;Daily Login Reward System&lt;/li&gt;
&lt;li&gt;Vehicle Keys (Compatible With Hotwire)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h1&gt;
&lt;p&gt;Feel free to use my &lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-base"&gt;Base&lt;/a&gt; mod to create your own! Just credit me in some way. Share your mods you created with my base mod with me!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-credit" class="anchor" aria-hidden="true" href="#credit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credit&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Did I forget to credit you for your code?&lt;/li&gt;
&lt;li&gt;Am I using your code without permission?&lt;/li&gt;
&lt;li&gt;Do you want to use my code?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Message me on Discord. I will help you out.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-discord" class="anchor" aria-hidden="true" href="#discord"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Discord&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://discord.gg/S2SckF6" rel="nofollow"&gt;https://discord.gg/S2SckF6&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>DiscworldZA</author><guid isPermaLink="false">https://github.com/DiscworldZA/gta-resources</guid><pubDate>Thu, 12 Dec 2019 00:02:00 GMT</pubDate></item><item><title>Kong/kong #3 in Lua, Today</title><link>https://github.com/Kong/kong</link><description>&lt;p&gt;&lt;i&gt;ü¶ç The Cloud-Native API Gateway &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://konghq.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9e4fe7914c7357861223aa535d7ca9858253c96e/68747470733a2f2f6b6f6e6768712e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031382f30352f6b6f6e672d6c6f676f2d6769746875622d726561646d652e706e67" alt="" data-canonical-src="https://konghq.com/wp-content/uploads/2018/05/kong-logo-github-readme.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/Kong/kong/branches" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/032b58c2a2e0a2a8dbb0c1fe60a0236e1042b7ad/68747470733a2f2f7472617669732d63692e6f72672f4b6f6e672f6b6f6e672e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/Kong/kong.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/Kong/kong/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/8051e9938a1ab39cf002818dfceb6b6092f34d68/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/intent/follow?screen_name=thekonginc" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/295bb78a3be8393e728bb4ad7470bd98a1c5062d/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f7468656b6f6e67696e632e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/follow/thekonginc.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kong is a cloud-native, fast, scalable, and distributed Microservice
Abstraction Layer &lt;em&gt;(also known as an API Gateway, API Middleware or in some
cases Service Mesh)&lt;/em&gt;. Made available as an open-source project in 2015, its
core values are high performance and extensibility.&lt;/p&gt;
&lt;p&gt;Actively maintained, Kong is widely used in production at companies ranging
from startups to Global 5000 as well as government organizations.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://konghq.com/install" rel="nofollow"&gt;Installation&lt;/a&gt; |
&lt;a href="https://docs.konghq.com" rel="nofollow"&gt;Documentation&lt;/a&gt; |
&lt;a href="https://discuss.konghq.com" rel="nofollow"&gt;Forum&lt;/a&gt; |
&lt;a href="https://konghq.com/blog" rel="nofollow"&gt;Blog&lt;/a&gt; |
IRC (freenode): &lt;a href="https://webchat.freenode.net/?channels=kong" rel="nofollow"&gt;#kong&lt;/a&gt; |
&lt;a href="https://bintray.com/kong/kong-nightly/master" rel="nofollow"&gt;Nightly Builds&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-summary" class="anchor" aria-hidden="true" href="#summary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#why-kong"&gt;&lt;strong&gt;Why Kong?&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#features"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#distributions"&gt;&lt;strong&gt;Distributions&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#development"&gt;&lt;strong&gt;Development&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#enterprise-support--demo"&gt;&lt;strong&gt;Enterprise Support &amp;amp; Demo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-why-kong" class="anchor" aria-hidden="true" href="#why-kong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why Kong?&lt;/h2&gt;
&lt;p&gt;If you are building for the web, mobile, or IoT (Internet of Things) you will
likely end up needing common functionality to run your actual software. Kong
can help by acting as a gateway (or a sidecar) for microservices requests while
providing load balancing, logging, authentication, rate-limiting,
transformations, and more through plugins.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://konghq.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d4d0dcb22c223db0bf2e301aab0dddb3015f1729/68747470733a2f2f6b6f6e6768712e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031382f30352f6b6f6e672d62656e65666974732d6769746875622d726561646d652e706e67" alt="" data-canonical-src="https://konghq.com/wp-content/uploads/2018/05/kong-benefits-github-readme.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cloud-Native&lt;/strong&gt;: Platform agnostic, Kong can run from bare metal to
Kubernetes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Load Balancing&lt;/strong&gt;: Load balance traffic across multiple upstream
services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hash-based Load Balancing&lt;/strong&gt;: Load balance with consistent hashing/sticky
sessions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Circuit-Breaker&lt;/strong&gt;: Intelligent tracking of unhealthy upstream services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Health Checks:&lt;/strong&gt; Active and passive monitoring of your upstream services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service Discovery&lt;/strong&gt;: Resolve SRV records in third-party DNS resolvers like
Consul.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Serverless&lt;/strong&gt;: Invoke and secure AWS Lambda or OpenWhisk functions directly
from Kong.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WebSockets&lt;/strong&gt;: Communicate to your upstream services via WebSockets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;gRPC&lt;/strong&gt;: Communicate to your gRPC services and observe your traffic with logging
and observability plugins&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OAuth2.0&lt;/strong&gt;: Easily add OAuth2.0 authentication to your APIs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logging&lt;/strong&gt;: Log requests and responses to your system over HTTP, TCP, UDP,
or to disk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: ACL, Bot detection, whitelist/blacklist IPs, etc...&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Syslog&lt;/strong&gt;: Logging to System log.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSL&lt;/strong&gt;: Setup a Specific SSL Certificate for an underlying service or API.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt;: Live monitoring provides key load and performance server
metrics.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forward Proxy&lt;/strong&gt;: Make Kong connect to intermediary transparent HTTP proxies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Authentications&lt;/strong&gt;: HMAC, JWT, Basic, and more.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rate-limiting&lt;/strong&gt;: Block and throttle requests based on many variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformations&lt;/strong&gt;: Add, remove, or manipulate HTTP requests and responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Caching&lt;/strong&gt;: Cache and serve responses at the proxy layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CLI&lt;/strong&gt;: Control your Kong cluster from the command line.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;REST API&lt;/strong&gt;: Kong can be operated with its RESTful API for maximum
flexibility.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Geo-Replicated&lt;/strong&gt;: Configs are always up-to-date across different regions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Failure Detection &amp;amp; Recovery&lt;/strong&gt;: Kong is unaffected if one of your Cassandra
nodes goes down.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clustering&lt;/strong&gt;: All Kong nodes auto-join the cluster keeping their config
updated across nodes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Distributed by nature, Kong scales horizontally by simply
adding nodes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Kong handles load with ease by scaling and using NGINX at
the core.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugins&lt;/strong&gt;: Extendable architecture for adding functionality to Kong and
APIs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more info about plugins and integrations, you can check out the &lt;a href="https://docs.konghq.com/hub/" rel="nofollow"&gt;Kong
Hub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-distributions" class="anchor" aria-hidden="true" href="#distributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Distributions&lt;/h2&gt;
&lt;p&gt;Kong comes in many shapes. While this repository contains its core's source
code, other repos are also under active development:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/docker-kong"&gt;Kong Docker&lt;/a&gt;: A Dockerfile for
running Kong in Docker.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kong/releases"&gt;Kong Packages&lt;/a&gt;: Pre-built packages
for Debian, Red Hat, and OS X distributions (shipped with each release).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kong-vagrant"&gt;Kong Vagrant&lt;/a&gt;: A Vagrantfile for
provisioning a development-ready environment for Kong.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/homebrew-kong"&gt;Kong Homebrew&lt;/a&gt;: Homebrew Formula
for Kong.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kong-dist-cloudformation"&gt;Kong CloudFormation&lt;/a&gt;:
Kong in a 1-click deployment for AWS EC2.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/marketplace/pp/B06WP4TNKL" rel="nofollow"&gt;Kong AWS AMI&lt;/a&gt;: Kong AMI on
the AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kong-dist-azure"&gt;Kong on Microsoft Azure&lt;/a&gt;: Run Kong
using Azure Resource Manager.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/heroku/heroku-kong"&gt;Kong on Heroku&lt;/a&gt;: Deploy Kong on
Heroku in one click.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.instaclustr.com/solutions/managed-cassandra-for-kong/" rel="nofollow"&gt;Kong and Instaclustr&lt;/a&gt;: Let
Instaclustr manage your Cassandra cluster.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kubernetes-ingress-controller"&gt;Kubernetes Ingress Controller for Kong&lt;/a&gt;:
Use Kong for Kubernetes Ingress.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bintray.com/kong/kong-nightly/master" rel="nofollow"&gt;Nightly Builds&lt;/a&gt;: Builds of the master branch available
every morning at about 9AM PST.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h2&gt;
&lt;p&gt;If you are planning on developing on Kong, you'll need a development
installation. The &lt;code&gt;next&lt;/code&gt; branch holds the latest unreleased source code.&lt;/p&gt;
&lt;p&gt;You can read more about writing your own plugins in the &lt;a href="https://docs.konghq.com/latest/plugin-development/" rel="nofollow"&gt;Plugin Development
Guide&lt;/a&gt;, or browse an
online version of Kong's source code documentation in the &lt;a href="https://docs.konghq.com/latest/pdk/" rel="nofollow"&gt;Plugin Development
Kit (PDK) Reference&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-docker" class="anchor" aria-hidden="true" href="#docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker&lt;/h4&gt;
&lt;p&gt;You can use Docker / docker-compose and a mounted volume to develop Kong by
following the instructions on &lt;a href="https://github.com/Kong/kong-build-tools#developing-kong"&gt;Kong/kong-build-tools&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-vagrant" class="anchor" aria-hidden="true" href="#vagrant"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vagrant&lt;/h4&gt;
&lt;p&gt;You can use a Vagrant box running Kong and Postgres that you can find at
&lt;a href="https://github.com/Kong/kong-vagrant"&gt;Kong/kong-vagrant&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-source-install" class="anchor" aria-hidden="true" href="#source-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source Install&lt;/h4&gt;
&lt;p&gt;Kong mostly is an OpenResty application made of Lua source files, but also
requires some additional third-party dependencies. We recommend installing
those by following the source install instructions at
&lt;a href="https://docs.konghq.com/install/source/" rel="nofollow"&gt;https://docs.konghq.com/install/source/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Instead of following the second step (Install Kong), clone this repository
and install the latest Lua sources instead of the currently released ones:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/Kong/kong
$ &lt;span class="pl-c1"&gt;cd&lt;/span&gt; kong/

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; you might want to switch to the development branch. See CONTRIBUTING.md&lt;/span&gt;
$ git checkout next

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install the Lua sources&lt;/span&gt;
$ luarocks make&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-running-for-development" class="anchor" aria-hidden="true" href="#running-for-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running for development&lt;/h4&gt;
&lt;p&gt;Check out the &lt;a href="https://github.com/Kong/kong/blob/next/kong.conf.default#L244"&gt;development section&lt;/a&gt;
of the default configuration file for properties to tweak in order to ease
the development process for Kong.&lt;/p&gt;
&lt;p&gt;Modifying the &lt;a href="https://github.com/openresty/lua-nginx-module#lua_package_path"&gt;&lt;code&gt;lua_package_path&lt;/code&gt;&lt;/a&gt;
and &lt;a href="https://github.com/openresty/lua-nginx-module#lua_package_cpath"&gt;&lt;code&gt;lua_package_cpath&lt;/code&gt;&lt;/a&gt;
directives will allow Kong to find your custom plugin's source code wherever it
might be in your system.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h4&gt;
&lt;p&gt;Install the development dependencies (&lt;a href="https://github.com/Olivine-Labs/busted"&gt;busted&lt;/a&gt;, &lt;a href="https://github.com/mpeterv/luacheck"&gt;luacheck&lt;/a&gt;) with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make dev&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Kong relies on three test suites using the &lt;a href="https://github.com/Olivine-Labs/busted"&gt;busted&lt;/a&gt; testing library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unit tests&lt;/li&gt;
&lt;li&gt;Integration tests, which require Postgres and Cassandra to be up and running&lt;/li&gt;
&lt;li&gt;Plugins tests, which require Postgres to be running&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first can simply be run after installing busted and running:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, the integration and plugins tests will spawn a Kong instance and
perform their tests against it. As so, consult/edit the &lt;code&gt;spec/kong_tests.conf&lt;/code&gt;
configuration file to make your test instance point to your Postgres/Cassandra
servers, depending on your needs.&lt;/p&gt;
&lt;p&gt;You can run the integration tests (assuming &lt;strong&gt;both&lt;/strong&gt; Postgres and Cassandra are
running and configured according to &lt;code&gt;spec/kong_tests.conf&lt;/code&gt;) with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make test-integration
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the plugins tests with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make test-plugins
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, all suites can be run at once by simply using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make test-all
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Consult the &lt;a href=".ci/run_tests.sh"&gt;run_tests.sh&lt;/a&gt; script for a more advanced example
usage of the tests suites and the Makefile.&lt;/p&gt;
&lt;p&gt;Finally, a very useful tool in Lua development (as with many other dynamic
languages) is performing static linting of your code. You can use &lt;a href="https://github.com/mpeterv/luacheck"&gt;luacheck&lt;/a&gt;
(installed with &lt;code&gt;make dev&lt;/code&gt;) for this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make lint
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-makefile" class="anchor" aria-hidden="true" href="#makefile"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Makefile&lt;/h4&gt;
&lt;p&gt;When developing, you can use the &lt;code&gt;Makefile&lt;/code&gt; for doing the following operations:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="right"&gt;Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;install&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Install the Kong luarock globally&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;dev&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Install development dependencies&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;lint&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Lint Lua files in &lt;code&gt;kong/&lt;/code&gt; and &lt;code&gt;spec/&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;test&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Run the unit tests suite&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;test-integration&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Run the integration tests suite&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;test-plugins&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Run the plugins test suite&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;test-all&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Run all unit + integration + plugins tests at once&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-enterprise-support--demo" class="anchor" aria-hidden="true" href="#enterprise-support--demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Enterprise Support &amp;amp; Demo&lt;/h2&gt;
&lt;p&gt;If you are working in a large organization you should learn more about &lt;a href="https://konghq.com/kong-enterprise-edition/" rel="nofollow"&gt;Kong
Enterprise&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Copyright 2016-2019 Kong Inc.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Kong</author><guid isPermaLink="false">https://github.com/Kong/kong</guid><pubDate>Thu, 12 Dec 2019 00:03:00 GMT</pubDate></item><item><title>junyanz/CycleGAN #4 in Lua, Today</title><link>https://github.com/junyanz/CycleGAN</link><description>&lt;p&gt;&lt;i&gt;Software that can generate photos from paintings,  turn horses into zebras,  perform style transfer, and more.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="imgs/horse2zebra.gif"&gt;&lt;img src="imgs/horse2zebra.gif" align="right" width="384" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-cyclegan" class="anchor" aria-hidden="true" href="#cyclegan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CycleGAN&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-pytorch--project-page----paper" class="anchor" aria-hidden="true" href="#pytorch--project-page----paper"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"&gt;PyTorch&lt;/a&gt; | &lt;a href="https://junyanz.github.io/CycleGAN/" rel="nofollow"&gt;project page&lt;/a&gt; |   &lt;a href="https://arxiv.org/pdf/1703.10593.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Torch implementation for learning an image-to-image translation (i.e. &lt;a href="https://github.com/phillipi/pix2pix"&gt;pix2pix&lt;/a&gt;) &lt;strong&gt;without&lt;/strong&gt; input-output pairs, for example:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/69cbc0371777fba5d251a564e2f8a8f38d1bf43f/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f7465617365725f686967685f7265732e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/69cbc0371777fba5d251a564e2f8a8f38d1bf43f/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f7465617365725f686967685f7265732e6a7067" width="1000px" data-canonical-src="https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://junyanz.github.io/CycleGAN/" rel="nofollow"&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&lt;/a&gt;&lt;br&gt;
&lt;a href="https://people.eecs.berkeley.edu/~junyanz/" rel="nofollow"&gt;Jun-Yan Zhu&lt;/a&gt;*,  &lt;a href="https://taesung.me/" rel="nofollow"&gt;Taesung Park&lt;/a&gt;*, &lt;a href="http://web.mit.edu/phillipi/" rel="nofollow"&gt;Phillip Isola&lt;/a&gt;, &lt;a href="https://people.eecs.berkeley.edu/~efros/" rel="nofollow"&gt;Alexei A. Efros&lt;/a&gt;&lt;br&gt;
Berkeley AI Research Lab, UC Berkeley&lt;br&gt;
In ICCV 2017. (* equal contributions)&lt;/p&gt;
&lt;p&gt;This package includes CycleGAN, &lt;a href="https://github.com/phillipi/pix2pix"&gt;pix2pix&lt;/a&gt;, as well as other methods like &lt;a href="https://arxiv.org/abs/1605.09782" rel="nofollow"&gt;BiGAN&lt;/a&gt;/&lt;a href="https://ishmaelbelghazi.github.io/ALI/" rel="nofollow"&gt;ALI&lt;/a&gt; and Apple's paper &lt;a href="https://arxiv.org/pdf/1612.07828.pdf" rel="nofollow"&gt;S+U learning&lt;/a&gt;.&lt;br&gt;
The code was written by &lt;a href="https://github.com/junyanz"&gt;Jun-Yan Zhu&lt;/a&gt; and &lt;a href="https://github.com/taesung"&gt;Taesung Park&lt;/a&gt;.&lt;br&gt;
&lt;strong&gt;Update&lt;/strong&gt;: Please check out &lt;a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"&gt;PyTorch&lt;/a&gt; implementation for CycleGAN and pix2pix.
The PyTorch version is under active development and can produce results comparable or better than this Torch version.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-other-implementations" class="anchor" aria-hidden="true" href="#other-implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other implementations:&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/leehomyc/cyclegan-1"&gt; [Tensorflow]&lt;/a&gt; (by Harry Yang),
&lt;a href="https://github.com/architrathore/CycleGAN/"&gt;[Tensorflow]&lt;/a&gt; (by Archit Rathore),
&lt;a href="https://github.com/vanhuyz/CycleGAN-TensorFlow"&gt;[Tensorflow]&lt;/a&gt; (by Van Huy),
&lt;a href="https://github.com/XHUJOY/CycleGAN-tensorflow"&gt;[Tensorflow]&lt;/a&gt; (by Xiaowei Hu), 
&lt;a href="https://github.com/LynnHo/CycleGAN-Tensorflow-Simple"&gt; [Tensorflow-simple]&lt;/a&gt; (by Zhenliang He),
&lt;a href="https://github.com/luoxier/CycleGAN_Tensorlayer"&gt; [TensorLayer]&lt;/a&gt; (by luoxier),
&lt;a href="https://github.com/Aixile/chainer-cyclegan"&gt;[Chainer]&lt;/a&gt; (by Yanghua Jin),
&lt;a href="https://github.com/yunjey/mnist-svhn-transfer"&gt;[Minimal PyTorch]&lt;/a&gt; (by yunjey),
&lt;a href="https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN"&gt;[Mxnet]&lt;/a&gt; (by Ldpe2G),
&lt;a href="https://github.com/tjwei/GANotebooks"&gt;[lasagne/Keras]&lt;/a&gt; (by tjwei), 
&lt;a href="https://github.com/simontomaskarlsson/CycleGAN-Keras"&gt;[Keras]&lt;/a&gt; (by Simon Karlsson)&lt;/p&gt;

&lt;h2&gt;&lt;a id="user-content-applications" class="anchor" aria-hidden="true" href="#applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Applications&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-monet-paintings-to-photos" class="anchor" aria-hidden="true" href="#monet-paintings-to-photos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Monet Paintings to Photos&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2296236e17ff15eb5a077fdb62df498b9d000a19/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f7061696e74696e673270686f746f2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/2296236e17ff15eb5a077fdb62df498b9d000a19/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f7061696e74696e673270686f746f2e6a7067" width="1000px" data-canonical-src="https://junyanz.github.io/CycleGAN/images/painting2photo.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-collection-style-transfer" class="anchor" aria-hidden="true" href="#collection-style-transfer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Collection Style Transfer&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1862adecd202ba420847653d9a119f9fed9a3abd/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f70686f746f327061696e74696e672e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/1862adecd202ba420847653d9a119f9fed9a3abd/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f70686f746f327061696e74696e672e6a7067" width="1000px" data-canonical-src="https://junyanz.github.io/CycleGAN/images/photo2painting.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-object-transfiguration" class="anchor" aria-hidden="true" href="#object-transfiguration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Object Transfiguration&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2fadde78dccf4d61f1294933c3e8083c07a303c7/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f6f626a656374732e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/2fadde78dccf4d61f1294933c3e8083c07a303c7/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f6f626a656374732e6a7067" width="1000px" data-canonical-src="https://junyanz.github.io/CycleGAN/images/objects.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-season-transfer" class="anchor" aria-hidden="true" href="#season-transfer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Season Transfer&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e3625979058468277d346b0ee3e7ef8cc399fd51/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f736561736f6e2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/e3625979058468277d346b0ee3e7ef8cc399fd51/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f736561736f6e2e6a7067" width="1000px" data-canonical-src="https://junyanz.github.io/CycleGAN/images/season.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-photo-enhancement-narrow-depth-of-field" class="anchor" aria-hidden="true" href="#photo-enhancement-narrow-depth-of-field"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Photo Enhancement: Narrow depth of field&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/b0610154e3c2a959c0d1acd43cfff304d1df88bd/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f70686f746f5f656e68616e63656d656e742e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/b0610154e3c2a959c0d1acd43cfff304d1df88bd/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f70686f746f5f656e68616e63656d656e742e6a7067" width="1000px" data-canonical-src="https://junyanz.github.io/CycleGAN/images/photo_enhancement.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Linux or OSX&lt;/li&gt;
&lt;li&gt;NVIDIA GPU + CUDA CuDNN (CPU mode and CUDA without CuDNN may work with minimal modification, but untested)&lt;/li&gt;
&lt;li&gt;For MAC users, you need the Linux/GNU commands &lt;code&gt;gfind&lt;/code&gt; and &lt;code&gt;gwc&lt;/code&gt;, which can be installed with &lt;code&gt;brew install findutils coreutils&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Install torch and dependencies from &lt;a href="https://github.com/torch/distro"&gt;https://github.com/torch/distro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install torch packages &lt;code&gt;nngraph&lt;/code&gt;, &lt;code&gt;class&lt;/code&gt;, &lt;code&gt;display&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;luarocks install nngraph
luarocks install class
luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Clone this repo:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/junyanz/CycleGAN
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; CycleGAN&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-apply-a-pre-trained-model" class="anchor" aria-hidden="true" href="#apply-a-pre-trained-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Apply a Pre-trained Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Download the test photos (taken by &lt;a href="https://www.flickr.com/photos/aaefros" rel="nofollow"&gt;Alexei Efros&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;bash ./datasets/download_dataset.sh ae_photos
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Download the pre-trained model &lt;code&gt;style_cezanne&lt;/code&gt; (For CPU model, use &lt;code&gt;style_cezanne_cpu&lt;/code&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;bash ./pretrained_models/download_model.sh style_cezanne
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Now, let's generate Paul C√©zanne style images:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;DATA_ROOT=./datasets/ae_photos name=style_cezanne_pretrained model=one_direction_test phase=test loadSize=256 fineSize=256 resize_or_crop="scale_width" th test.lua
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The test results will be saved to &lt;code&gt;./results/style_cezanne_pretrained/latest_test/index.html&lt;/code&gt;.&lt;br&gt;
Please refer to &lt;a href="#model-zoo"&gt;Model Zoo&lt;/a&gt; for more pre-trained models.
&lt;code&gt;./examples/test_vangogh_style_on_ae_photos.sh&lt;/code&gt; is an example script that downloads the pretrained Van Gogh style network and runs it on Efros's photos.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-train" class="anchor" aria-hidden="true" href="#train"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Train&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Download a dataset (e.g. zebra and horse images from ImageNet):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./datasets/download_dataset.sh horse2zebra&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Train a model:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;DATA_ROOT=./datasets/horse2zebra name=horse2zebra_model th train.lua&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables &lt;code&gt;gpu=0 cudnn=0&lt;/code&gt; forces CPU only&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;DATA_ROOT=./datasets/horse2zebra name=horse2zebra_model gpu=0 cudnn=0 th train.lua&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(Optionally) start the display server to view results as the model trains. (See &lt;a href="#display-ui"&gt;Display UI&lt;/a&gt; for more details):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;th -ldisplay.start 8000 0.0.0.0&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-test" class="anchor" aria-hidden="true" href="#test"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Finally, test the model:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;DATA_ROOT=./datasets/horse2zebra name=horse2zebra_model phase=test th test.lua&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The test results will be saved to an HTML file here: &lt;code&gt;./results/horse2zebra_model/latest_test/index.html&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-zoo" class="anchor" aria-hidden="true" href="#model-zoo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model Zoo&lt;/h2&gt;
&lt;p&gt;Download the pre-trained models with the following script. The model will be saved to &lt;code&gt;./checkpoints/model_name/latest_net_G.t7&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./pretrained_models/download_model.sh model_name&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;orange2apple&lt;/code&gt; (orange -&amp;gt; apple) and &lt;code&gt;apple2orange&lt;/code&gt;: trained on ImageNet categories &lt;code&gt;apple&lt;/code&gt; and &lt;code&gt;orange&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;horse2zebra&lt;/code&gt; (horse -&amp;gt; zebra) and &lt;code&gt;zebra2horse&lt;/code&gt; (zebra -&amp;gt; horse): trained on ImageNet categories &lt;code&gt;horse&lt;/code&gt; and &lt;code&gt;zebra&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;style_monet&lt;/code&gt; (landscape photo -&amp;gt; Monet painting style),  &lt;code&gt;style_vangogh&lt;/code&gt; (landscape photo  -&amp;gt; Van Gogh painting style), &lt;code&gt;style_ukiyoe&lt;/code&gt; (landscape photo  -&amp;gt; Ukiyo-e painting style), &lt;code&gt;style_cezanne&lt;/code&gt; (landscape photo  -&amp;gt; Cezanne painting style): trained on paintings and Flickr landscape photos.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;monet2photo&lt;/code&gt; (Monet paintings -&amp;gt; real landscape): trained on paintings and Flickr landscape photographs.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cityscapes_photo2label&lt;/code&gt; (street scene -&amp;gt; label) and &lt;code&gt;cityscapes_label2photo&lt;/code&gt; (label -&amp;gt; street scene): trained on the Cityscapes dataset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;map2sat&lt;/code&gt; (map -&amp;gt; aerial photo) and &lt;code&gt;sat2map&lt;/code&gt; (aerial photo -&amp;gt; map): trained on Google maps.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iphone2dslr_flower&lt;/code&gt; (iPhone photos of flowers -&amp;gt; DSLR photos of flowers): trained on Flickr photos.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CPU models can be downloaded using:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash pretrained_models/download_model.sh &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;name&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt;_cpu&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;, where &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt; can be &lt;code&gt;horse2zebra&lt;/code&gt;, &lt;code&gt;style_monet&lt;/code&gt;, etc. You just need to append &lt;code&gt;_cpu&lt;/code&gt; to the target model.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-training-and-test-details" class="anchor" aria-hidden="true" href="#training-and-test-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Test Details&lt;/h2&gt;
&lt;p&gt;To train a model,&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;DATA_ROOT=/path/to/data/ name=expt_name th train.lua&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Models are saved to &lt;code&gt;./checkpoints/expt_name&lt;/code&gt; (can be changed by passing &lt;code&gt;checkpoint_dir=your_dir&lt;/code&gt; in train.lua).&lt;br&gt;
See &lt;code&gt;opt_train&lt;/code&gt; in &lt;code&gt;options.lua&lt;/code&gt; for additional training options.&lt;/p&gt;
&lt;p&gt;To test the model,&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;DATA_ROOT=/path/to/data/ name=expt_name phase=test th test.lua&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will run the model named &lt;code&gt;expt_name&lt;/code&gt; in both directions on all images in &lt;code&gt;/path/to/data/testA&lt;/code&gt; and &lt;code&gt;/path/to/data/testB&lt;/code&gt;.&lt;br&gt;
A webpage with result images will be saved to &lt;code&gt;./results/expt_name&lt;/code&gt; (can be changed by passing &lt;code&gt;results_dir=your_dir&lt;/code&gt; in test.lua).&lt;br&gt;
See &lt;code&gt;opt_test&lt;/code&gt; in &lt;code&gt;options.lua&lt;/code&gt; for additional test options. Please use &lt;code&gt;model=one_direction_test&lt;/code&gt; if you only would like to generate outputs of the trained network in only one direction, and specify &lt;code&gt;which_direction=AtoB&lt;/code&gt; or &lt;code&gt;which_direction=BtoA&lt;/code&gt; to set the direction.&lt;/p&gt;
&lt;p&gt;There are other options that can be used. For example, you can specify &lt;code&gt;resize_or_crop=crop&lt;/code&gt; option to avoid resizing the image to squares. This is indeed how we trained GTA2Cityscapes model in the projet &lt;a href="https://junyanz.github.io/CycleGAN/" rel="nofollow"&gt;webpage&lt;/a&gt; and &lt;a href="https://arxiv.org/pdf/1711.03213.pdf" rel="nofollow"&gt;Cycada&lt;/a&gt; model. We prepared the images at 1024px resolution, and used &lt;code&gt;resize_or_crop=crop fineSize=360&lt;/code&gt; to work with the cropped images of size 360x360. We also used &lt;code&gt;lambda_identity=1.0&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h2&gt;
&lt;p&gt;Download the datasets using the following script. Many of the datasets were collected by other researchers. Please cite their papers if you use the data.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./datasets/download_dataset.sh dataset_name&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;facades&lt;/code&gt;: 400 images from the &lt;a href="http://cmp.felk.cvut.cz/~tylecr1/facade/" rel="nofollow"&gt;CMP Facades dataset&lt;/a&gt;. [&lt;a href="datasets/bibtex/facades.tex"&gt;Citation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cityscapes&lt;/code&gt;: 2975 images from the &lt;a href="https://www.cityscapes-dataset.com/" rel="nofollow"&gt;Cityscapes training set&lt;/a&gt;. [&lt;a href="datasets/bibtex/cityscapes.tex"&gt;Citation&lt;/a&gt;]. Note: Due to license issue, we do not host the dataset on our repo. Please download the dataset directly from the Cityscapes webpage. Please refer to &lt;code&gt;./datasets/prepare_cityscapes_dataset.py&lt;/code&gt; for more detail.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;maps&lt;/code&gt;: 1096 training images scraped from Google Maps.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;horse2zebra&lt;/code&gt;: 939 horse images and 1177 zebra images downloaded from &lt;a href="http://www.image-net.org/" rel="nofollow"&gt;ImageNet&lt;/a&gt; using the keywords &lt;code&gt;wild horse&lt;/code&gt; and &lt;code&gt;zebra&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;apple2orange&lt;/code&gt;: 996 apple images and 1020 orange images downloaded from &lt;a href="http://www.image-net.org/" rel="nofollow"&gt;ImageNet&lt;/a&gt; using the keywords &lt;code&gt;apple&lt;/code&gt; and &lt;code&gt;navel orange&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;summer2winter_yosemite&lt;/code&gt;: 1273 summer Yosemite images and 854 winter Yosemite images were downloaded using Flickr API. See more details in our paper.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;monet2photo&lt;/code&gt;, &lt;code&gt;vangogh2photo&lt;/code&gt;, &lt;code&gt;ukiyoe2photo&lt;/code&gt;, &lt;code&gt;cezanne2photo&lt;/code&gt;: The art images were downloaded from &lt;a href="https://www.wikiart.org/" rel="nofollow"&gt;Wikiart&lt;/a&gt;. The real photos are downloaded from Flickr using the combination of the tags &lt;em&gt;landscape&lt;/em&gt; and &lt;em&gt;landscapephotography&lt;/em&gt;. The training set size of each class is Monet:1074, Cezanne:584, Van Gogh:401, Ukiyo-e:1433, Photographs:6853.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iphone2dslr_flower&lt;/code&gt;: both classes of images were downloaded from Flickr. The training set size of each class is iPhone:1813, DSLR:3316. See more details in our paper.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-display-ui" class="anchor" aria-hidden="true" href="#display-ui"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Display UI&lt;/h2&gt;
&lt;p&gt;Optionally, for displaying images during training and test, use the &lt;a href="https://github.com/szym/display"&gt;display package&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install it with: &lt;code&gt;luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Then start the server with: &lt;code&gt;th -ldisplay.start&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Open this URL in your browser: &lt;a href="http://localhost:8000" rel="nofollow"&gt;http://localhost:8000&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By default, the server listens on localhost. Pass &lt;code&gt;0.0.0.0&lt;/code&gt; to allow external connections on any interface:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;th -ldisplay.start 8000 0.0.0.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then open &lt;code&gt;http://(hostname):(port)/&lt;/code&gt; in your browser to load the remote desktop.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-setup-training-and-test-data" class="anchor" aria-hidden="true" href="#setup-training-and-test-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup Training and Test data&lt;/h2&gt;
&lt;p&gt;To train CycleGAN model on your own datasets, you need to create a data folder with two subdirectories &lt;code&gt;trainA&lt;/code&gt; and &lt;code&gt;trainB&lt;/code&gt; that contain images from domain A and B. You can test your model on your training set by setting &lt;code&gt;phase='train'&lt;/code&gt; in  &lt;code&gt;test.lua&lt;/code&gt;. You can also create subdirectories &lt;code&gt;testA&lt;/code&gt; and &lt;code&gt;testB&lt;/code&gt; if you have test data.&lt;/p&gt;
&lt;p&gt;You should &lt;strong&gt;not&lt;/strong&gt; expect our method to work on just any random combination of input and output datasets (e.g. &lt;code&gt;cats&amp;lt;-&amp;gt;keyboards&lt;/code&gt;). From our experiments, we find it works better if two datasets share similar visual content. For example, &lt;code&gt;landscape painting&amp;lt;-&amp;gt;landscape photographs&lt;/code&gt; works much better than &lt;code&gt;portrait painting &amp;lt;-&amp;gt; landscape photographs&lt;/code&gt;. &lt;code&gt;zebras&amp;lt;-&amp;gt;horses&lt;/code&gt; achieves compelling results while &lt;code&gt;cats&amp;lt;-&amp;gt;dogs&lt;/code&gt; completely fails.  See the following section for more discussion.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-failure-cases" class="anchor" aria-hidden="true" href="#failure-cases"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Failure cases&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/757b691307b52fe8a0806dde3a560dc068dbf5b3/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f6661696c7572655f707574696e2e6a7067"&gt;&lt;img align="left" src="https://camo.githubusercontent.com/757b691307b52fe8a0806dde3a560dc068dbf5b3/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f6661696c7572655f707574696e2e6a7067" width="320" data-canonical-src="https://junyanz.github.io/CycleGAN/images/failure_putin.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Our model does not work well when the test image is rather different from the images on which the model is trained, as is the case in the figure to the left (we trained on horses and zebras without riders, but test here one a horse with a rider).  See additional typical failure cases &lt;a href="https://junyanz.github.io/CycleGAN/images/failures.jpg" rel="nofollow"&gt;here&lt;/a&gt;. On translation tasks that involve color and texture changes, like many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of &lt;code&gt;dog&amp;lt;-&amp;gt;cat&lt;/code&gt; transfiguration, the learned translation degenerates into making minimal changes to the input. We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method. In some cases, this gap may be very hard -- or even impossible,-- to close: for example, our method sometimes permutes the labels for tree and building in the output of the cityscapes photos-&amp;gt;labels task.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use this code for your research, please cite our &lt;a href="https://junyanz.github.io/CycleGAN/" rel="nofollow"&gt;paper&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{CycleGAN2017,
  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networkss},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},
  year={2017}
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-related-projects" class="anchor" aria-hidden="true" href="#related-projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related Projects:&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/phillipi/pix2pix"&gt;pix2pix&lt;/a&gt;: Image-to-image translation using conditional adversarial nets&lt;br&gt;
&lt;a href="https://github.com/junyanz/iGAN"&gt;iGAN&lt;/a&gt;: Interactive Image Generation via Generative Adversarial Networks&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-cat-paper-collection" class="anchor" aria-hidden="true" href="#cat-paper-collection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cat Paper Collection&lt;/h2&gt;
&lt;p&gt;If you love cats, and love reading cool graphics, vision, and ML papers, please check out the Cat Paper &lt;a href="https://github.com/junyanz/CatPapers"&gt;Collection&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgments" class="anchor" aria-hidden="true" href="#acknowledgments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;Code borrows from &lt;a href="https://github.com/phillipi/pix2pix"&gt;pix2pix&lt;/a&gt; and &lt;a href="https://github.com/soumith/dcgan.torch"&gt;DCGAN&lt;/a&gt;. The data loader is modified from &lt;a href="https://github.com/soumith/dcgan.torch"&gt;DCGAN&lt;/a&gt; and  &lt;a href="https://github.com/pathak22/context-encoder"&gt;Context-Encoder&lt;/a&gt;. The generative network is adopted from &lt;a href="https://github.com/jcjohnson/neural-style"&gt;neural-style&lt;/a&gt; with &lt;a href="https://github.com/DmitryUlyanov/texture_nets/blob/master/InstanceNormalization.lua"&gt;Instance Normalization&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>junyanz</author><guid isPermaLink="false">https://github.com/junyanz/CycleGAN</guid><pubDate>Thu, 12 Dec 2019 00:04:00 GMT</pubDate></item><item><title>cardwing/Codes-for-Lane-Detection #5 in Lua, Today</title><link>https://github.com/cardwing/Codes-for-Lane-Detection</link><description>&lt;p&gt;&lt;i&gt;Learning Lightweight Lane Detection CNNs by Self Attention Distillation (ICCV 2019)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;Codes for &lt;a href="https://arxiv.org/abs/1908.00821" rel="nofollow"&gt;"Learning Lightweight Lane Detection CNNs by Self Attention Distillation"&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repo also contains Tensorflow implementation of &lt;a href="https://arxiv.org/abs/1712.06080" rel="nofollow"&gt;"Spatial As Deep: Spatial CNN for Traffic Scene Understanding"&lt;/a&gt;. (SCNN-Tensorflow)&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-news" class="anchor" aria-hidden="true" href="#news"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;News&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./ERFNet-CULane-PyTorch"&gt;ERFNet-CULane-PyTorch&lt;/a&gt; has been released. (It can achieve &lt;strong&gt;73.1&lt;/strong&gt; F1-measure in CULane testing set)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./ENet-Label-Torch"&gt;ENet-Label-Torch&lt;/a&gt;, &lt;a href="./ENet-TuSimple-Torch"&gt;ENet-TuSimple-Torch&lt;/a&gt; and &lt;a href="./ENet-BDD100K-Torch"&gt;ENet-BDD100K-Torch&lt;/a&gt; have been released.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Key features:&lt;/p&gt;
&lt;p&gt;(1) ENet-label is a &lt;strong&gt;light-weight&lt;/strong&gt; lane detection model based on &lt;a href="https://arxiv.org/abs/1606.02147" rel="nofollow"&gt;ENet&lt;/a&gt; and adopts &lt;strong&gt;self attention distillation&lt;/strong&gt; (more details can be found in our paper).&lt;/p&gt;
&lt;p&gt;(2) It has &lt;strong&gt;20&lt;/strong&gt; √ó fewer parameters and runs &lt;strong&gt;10&lt;/strong&gt; √ó faster compared to the state-of-the-art SCNN, and achieves &lt;strong&gt;72.0&lt;/strong&gt; (F1-measure) on CULane testing set (better than SCNN which achieves 71.6). It also achieves &lt;strong&gt;96.64%&lt;/strong&gt; accuracy in TuSimple testing set (better than SCNN which achieves 96.53%) and &lt;strong&gt;36.56%&lt;/strong&gt; accuracy in BDD100K testing set (better than SCNN which achieves 35.79%).&lt;/p&gt;
&lt;p&gt;(3) Applying ENet-SAD to &lt;a href="https://unsupervised-llamas.com/llamas/" rel="nofollow"&gt;LLAMAS&lt;/a&gt; dataset yields &lt;strong&gt;0.635&lt;/strong&gt; mAP in the &lt;a href="https://unsupervised-llamas.com/llamas/benchmark_multi" rel="nofollow"&gt;multi-class lane marker segmentation task&lt;/a&gt;, which is much better than the baseline algorithm which achieves 0.500 mAP. Details can be found in &lt;a href="https://github.com/cardwing/unsupervised_llamas/tree/master/ENet-SAD-Simple"&gt;this repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;(Do not hesitate to try our model!!!)&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Multi-GPU training has been supported. Just change BATCH_SIZE and GPU_NUM in global_config.py, and then use &lt;code&gt;CUDA_VISIBLE_DEVICES="0,1,2,3" python file_name.py&lt;/code&gt;. Thanks @ yujincheng08.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-content" class="anchor" aria-hidden="true" href="#content"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Content&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#Installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Datasets"&gt;Datasets&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#TuSimple"&gt;TuSimple&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#CULane"&gt;CULane&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#BDD100K"&gt;BDD100K&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#SCNN-Tensorflow"&gt;SCNN-Tensorflow&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#Test"&gt;Test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Train"&gt;Train&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#Performance"&gt;Performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Others"&gt;Others&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#Citation"&gt;Citation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Acknowledgement"&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Contact"&gt;Contact&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Install necessary packages:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;    conda create -n tensorflow_gpu pip python=3.5
    source activate tensorflow_gpu
    pip install --upgrade tensorflow-gpu==1.3.0
    pip3 install -r SCNN-Tensorflow/lane-detection-model/requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Download VGG-16:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Download the vgg.npy &lt;a href="https://github.com/machrisaa/tensorflow-vgg"&gt;here&lt;/a&gt; and put it in SCNN-Tensorflow/lane-detection-model/data.&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Pre-trained model for testing:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Download the pre-trained model &lt;a href="https://drive.google.com/open?id=1-E0Bws7-v35vOVfqEXDTJdfovUTQ2sf5" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-tusimple" class="anchor" aria-hidden="true" href="#tusimple"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TuSimple&lt;/h2&gt;
&lt;p&gt;The ground-truth labels of TuSimple testing set is now available at &lt;a href="https://github.com/TuSimple/tusimple-benchmark/issues/3"&gt;TuSimple&lt;/a&gt;. The annotated training (#frame = 3268) and validation labels (#frame = 358) can be found &lt;a href="https://github.com/cardwing/Codes-for-Lane-Detection/issues/11"&gt;here&lt;/a&gt;, please use them (list-name.txt) to replace the train_gt.txt and val_gt.txt in &lt;a href="./SCNN-Tensorflow/lane-detection-model/tools/train_lanenet.py"&gt;train_lanenet.py&lt;/a&gt;. Moreover, you need to resize the image to 256 x 512 instead of 288 x 800 in TuSimple. Remember to change the maximum index of rows and columns, and detailed explanations can be seen &lt;a href="https://github.com/cardwing/Codes-for-Lane-Detection/issues/18"&gt;here&lt;/a&gt;. Please evaluate your pred.json using the labels and &lt;a href="https://github.com/TuSimple/tusimple-benchmark/blob/master/evaluate/lane.py"&gt;this script&lt;/a&gt;. Besides, to generate pred.json, you can refer to &lt;a href="https://github.com/cardwing/Codes-for-Lane-Detection/issues/4"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-culane" class="anchor" aria-hidden="true" href="#culane"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CULane&lt;/h2&gt;
&lt;p&gt;The whole dataset is available at &lt;a href="https://xingangpan.github.io/projects/CULane.html" rel="nofollow"&gt;CULane&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-bdd100k" class="anchor" aria-hidden="true" href="#bdd100k"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BDD100K&lt;/h2&gt;
&lt;p&gt;The whole dataset is available at &lt;a href="http://bdd-data.berkeley.edu/" rel="nofollow"&gt;BDD100K&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-scnn-tensorflow" class="anchor" aria-hidden="true" href="#scnn-tensorflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SCNN-Tensorflow&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-test" class="anchor" aria-hidden="true" href="#test"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Test&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;cd SCNN-Tensorflow/lane-detection-model
CUDA_VISIBLE_DEVICES="0" python tools/test_lanenet.py --weights_path path/to/model_weights_file --image_path path/to/image_name_list --save_dir to_be_saved_dir
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that path/to/image_name_list should be like &lt;a href="./SCNN-Tensorflow/lane-detection-model/demo_file/test_img.txt"&gt;test_img.txt&lt;/a&gt;. Now, you get the probability maps from our model. To get the final performance, you need to follow &lt;a href="https://github.com/XingangPan/SCNN"&gt;SCNN&lt;/a&gt; to get curve lines from probability maps as well as calculate precision, recall and F1-measure.&lt;/p&gt;
&lt;p&gt;Reminder: you should check &lt;a href="./SCNN-Tensorflow/lane-detection-model/data_provider/lanenet_data_processor.py"&gt;lanenet_data_processor.py&lt;/a&gt; and &lt;a href="./SCNN-Tensorflow/lane-detection-model/data_provider/lanenet_data_processor.py"&gt;lanenet_data_processor_test.py&lt;/a&gt; to ensure that the processing of image path is right. You are recommended to use the absolute path in your image path list. Besides, this code needs batch size used in training and testing to be consistent. To enable arbitrary batch size in the testing phase, please refer to &lt;a href="https://github.com/cardwing/Codes-for-Lane-Detection/issues/10"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-train" class="anchor" aria-hidden="true" href="#train"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Train&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES="0" python tools/train_lanenet.py --net vgg --dataset_dir path/to/CULane-dataset/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that path/to/CULane-dataset/ should contain files like &lt;a href="./SCNN-Tensorflow/lane-detection-model/demo_file/train_gt.txt"&gt;train_gt.txt&lt;/a&gt; and &lt;a href="./SCNN-Tensorflow/lane-detection-model/demo_file/train_gt.txt"&gt;val_gt.txt&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-performance" class="anchor" aria-hidden="true" href="#performance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Performance&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;TuSimple testing set:&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Accuracy&lt;/th&gt;
&lt;th align="center"&gt;FP&lt;/th&gt;
&lt;th align="center"&gt;FN&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/XingangPan/SCNN"&gt;SCNN-Torch&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;96.53%&lt;/td&gt;
&lt;td align="center"&gt;0.0617&lt;/td&gt;
&lt;td align="center"&gt;0.0180&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SCNN-Tensorflow&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;ENet-Label-Torch&lt;/td&gt;
&lt;td align="center"&gt;96.64%&lt;/td&gt;
&lt;td align="center"&gt;0.0602&lt;/td&gt;
&lt;td align="center"&gt;0.0205&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The pre-trained model for testing is here. (coming soon!) Note that in TuSimple, SCNN-Torch is based on ResNet-101 while SCNN-Tensorflow is based on VGG-16. In CULane and BDD100K, both SCNN-Torch and SCNN-Tensorflow are based on VGG-16.&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;CULane testing set (F1-measure):&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Category&lt;/th&gt;
&lt;th align="center"&gt;&lt;a href="https://github.com/XingangPan/SCNN"&gt;SCNN-Torch&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;SCNN-Tensorflow&lt;/th&gt;
&lt;th align="center"&gt;ENet-Label-Torch&lt;/th&gt;
&lt;th align="center"&gt;ERFNet-CULane-PyTorch&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Normal&lt;/td&gt;
&lt;td align="center"&gt;90.6&lt;/td&gt;
&lt;td align="center"&gt;90.2&lt;/td&gt;
&lt;td align="center"&gt;90.7&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.5&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Crowded&lt;/td&gt;
&lt;td align="center"&gt;69.7&lt;/td&gt;
&lt;td align="center"&gt;71.9&lt;/td&gt;
&lt;td align="center"&gt;70.8&lt;/td&gt;
&lt;td align="center"&gt;71.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Night&lt;/td&gt;
&lt;td align="center"&gt;66.1&lt;/td&gt;
&lt;td align="center"&gt;64.6&lt;/td&gt;
&lt;td align="center"&gt;65.9&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;67.1&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;No line&lt;/td&gt;
&lt;td align="center"&gt;43.4&lt;/td&gt;
&lt;td align="center"&gt;45.8&lt;/td&gt;
&lt;td align="center"&gt;44.7&lt;/td&gt;
&lt;td align="center"&gt;45.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Shadow&lt;/td&gt;
&lt;td align="center"&gt;66.9&lt;/td&gt;
&lt;td align="center"&gt;73.8&lt;/td&gt;
&lt;td align="center"&gt;70.6&lt;/td&gt;
&lt;td align="center"&gt;71.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Arrow&lt;/td&gt;
&lt;td align="center"&gt;84.1&lt;/td&gt;
&lt;td align="center"&gt;83.8&lt;/td&gt;
&lt;td align="center"&gt;85.8&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;87.2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Dazzle light&lt;/td&gt;
&lt;td align="center"&gt;58.5&lt;/td&gt;
&lt;td align="center"&gt;59.5&lt;/td&gt;
&lt;td align="center"&gt;64.4&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;66.0&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Curve&lt;/td&gt;
&lt;td align="center"&gt;64.4&lt;/td&gt;
&lt;td align="center"&gt;63.4&lt;/td&gt;
&lt;td align="center"&gt;65.4&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;66.3&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Crossroad&lt;/td&gt;
&lt;td align="center"&gt;1990&lt;/td&gt;
&lt;td align="center"&gt;4137&lt;/td&gt;
&lt;td align="center"&gt;2729&lt;/td&gt;
&lt;td align="center"&gt;2199&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Total&lt;/td&gt;
&lt;td align="center"&gt;71.6&lt;/td&gt;
&lt;td align="center"&gt;71.3&lt;/td&gt;
&lt;td align="center"&gt;72.0&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;73.1&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Runtime(ms)&lt;/td&gt;
&lt;td align="center"&gt;133.5&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;13.4&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;10.2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Parameter(M)&lt;/td&gt;
&lt;td align="center"&gt;20.72&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;0.98&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;2.49&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The pre-trained model for testing is &lt;a href="https://drive.google.com/open?id=1-E0Bws7-v35vOVfqEXDTJdfovUTQ2sf5" rel="nofollow"&gt;here&lt;/a&gt;. Note that you need to exchange the order of VGG-MEAN in test_lanenet.py and change the order of input images from RGB to BGR since the pre-trained model uses opencv to read images. You can further boost the performance by referring to &lt;a href="https://github.com/cardwing/Codes-for-Lane-Detection/issues/5"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;BDD100K testing set:&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Accuracy&lt;/th&gt;
&lt;th align="center"&gt;IoU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/XingangPan/SCNN"&gt;SCNN-Torch&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;35.79%&lt;/td&gt;
&lt;td align="center"&gt;15.84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SCNN-Tensorflow&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;ENet-Label-Torch&lt;/td&gt;
&lt;td align="center"&gt;36.56%&lt;/td&gt;
&lt;td align="center"&gt;16.02&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The accuracy and IoU of lane pixels are computed. The pre-trained model for testing is here. (coming soon!)&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-others" class="anchor" aria-hidden="true" href="#others"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Others&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use the codes, please cite the following publications:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{hou2019learning,
  title={Learning Lightweight Lane Detection CNNs by Self Attention Distillation},
  author={Hou, Yuenan and Ma, Zheng and Liu, Chunxiao and Loy, Chen Change},
  journal={arXiv preprint arXiv:1908.00821},
  year={2019}
}

@inproceedings{pan2018SCNN,  
  author = {Xingang Pan, Jianping Shi, Ping Luo, Xiaogang Wang, and Xiaoou Tang},  
  title = {Spatial As Deep: Spatial CNN for Traffic Scene Understanding},  
  booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},  
  month = {February},  
  year = {2018}  
}

@misc{hou2019agnostic,
    title={Agnostic Lane Detection},
    author={Yuenan Hou},
    year={2019},
    eprint={1905.03704},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This repo is built upon &lt;a href="https://github.com/XingangPan/SCNN"&gt;SCNN&lt;/a&gt; and &lt;a href="https://github.com/MaybeShewill-CV/lanenet-lane-detection"&gt;LaneNet&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;If you have any problems in reproducing the results, just raise an issue in this repo.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-to-do-list" class="anchor" aria-hidden="true" href="#to-do-list"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;To-Do List&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Test SCNN-Tensorflow in TuSimple and BDD100K&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Provide detailed instructions to run SCNN-Tensorflow in TuSimple and BDD100K&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Upload our light-weight model (ENet-SAD) and its training &amp;amp; testing scripts&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>cardwing</author><guid isPermaLink="false">https://github.com/cardwing/Codes-for-Lane-Detection</guid><pubDate>Thu, 12 Dec 2019 00:05:00 GMT</pubDate></item><item><title>loveshell/ngx_lua_waf #6 in Lua, Today</title><link>https://github.com/loveshell/ngx_lua_waf</link><description>&lt;p&gt;&lt;i&gt;ngx_lua_wafÊòØ‰∏Ä‰∏™Âü∫‰∫élua-nginx-module(openresty)ÁöÑwebÂ∫îÁî®Èò≤ÁÅ´Â¢ô&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;##ngx_lua_waf&lt;/p&gt;
&lt;p&gt;ngx_lua_wafÊòØÊàëÂàöÂÖ•ËÅåË∂£Ê∏∏Êó∂ÂÄôÂºÄÂèëÁöÑ‰∏Ä‰∏™Âü∫‰∫éngx_luaÁöÑwebÂ∫îÁî®Èò≤ÁÅ´Â¢ô„ÄÇ&lt;/p&gt;
&lt;p&gt;‰ª£Á†ÅÂæàÁÆÄÂçïÔºåÂºÄÂèëÂàùË°∑‰∏ªË¶ÅÊòØ‰ΩøÁî®ÁÆÄÂçïÔºåÈ´òÊÄßËÉΩÂíåËΩªÈáèÁ∫ß„ÄÇ&lt;/p&gt;
&lt;p&gt;Áé∞Âú®ÂºÄÊ∫êÂá∫Êù•ÔºåÈÅµ‰ªéMITËÆ∏ÂèØÂçèËÆÆ„ÄÇÂÖ∂‰∏≠ÂåÖÂê´Êàë‰ª¨ÁöÑËøáÊª§ËßÑÂàô„ÄÇÂ¶ÇÊûúÂ§ßÂÆ∂Êúâ‰ªÄ‰πàÂª∫ËÆÆÂíåÊÉ≥faÔºåÊ¨¢ËøéÂíåÊàë‰∏ÄËµ∑ÂÆåÂñÑ„ÄÇ&lt;/p&gt;
&lt;p&gt;###Áî®ÈÄîÔºö&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Èò≤Ê≠¢sqlÊ≥®ÂÖ•ÔºåÊú¨Âú∞ÂåÖÂê´ÔºåÈÉ®ÂàÜÊ∫¢Âá∫ÔºåfuzzingÊµãËØïÔºåxss,SSRFÁ≠âwebÊîªÂáª
Èò≤Ê≠¢svn/Â§á‰ªΩ‰πãÁ±ªÊñá‰ª∂Ê≥ÑÊºè
Èò≤Ê≠¢ApacheBench‰πãÁ±ªÂéãÂäõÊµãËØïÂ∑•ÂÖ∑ÁöÑÊîªÂáª
Â±èËîΩÂ∏∏ËßÅÁöÑÊâ´ÊèèÈªëÂÆ¢Â∑•ÂÖ∑ÔºåÊâ´ÊèèÂô®
Â±èËîΩÂºÇÂ∏∏ÁöÑÁΩëÁªúËØ∑Ê±Ç
Â±èËîΩÂõæÁâáÈôÑ‰ª∂Á±ªÁõÆÂΩïphpÊâßË°åÊùÉÈôê
Èò≤Ê≠¢webshell‰∏ä‰º†
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;###Êé®ËçêÂÆâË£Ö:&lt;/p&gt;
&lt;p&gt;Êé®Ëçê‰ΩøÁî®lujit2.1ÂÅöluaÊîØÊåÅ&lt;/p&gt;
&lt;p&gt;ngx_luaÂ¶ÇÊûúÊòØ0.9.2‰ª•‰∏äÁâàÊú¨ÔºåÂª∫ËÆÆÊ≠£ÂàôËøáÊª§ÂáΩÊï∞Êîπ‰∏∫ngx.re.findÔºåÂåπÈÖçÊïàÁéá‰ºöÊèêÈ´ò‰∏âÂÄçÂ∑¶Âè≥„ÄÇ&lt;/p&gt;
&lt;p&gt;###‰ΩøÁî®ËØ¥ÊòéÔºö&lt;/p&gt;
&lt;p&gt;nginxÂÆâË£ÖË∑ØÂæÑÂÅáËÆæ‰∏∫:/usr/local/nginx/conf/&lt;/p&gt;
&lt;p&gt;Êäängx_lua_waf‰∏ãËΩΩÂà∞confÁõÆÂΩï‰∏ã,Ëß£ÂéãÂëΩÂêç‰∏∫waf&lt;/p&gt;
&lt;p&gt;Âú®nginx.confÁöÑhttpÊÆµÊ∑ªÂä†&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;	lua_package_path "/usr/local/nginx/conf/waf/?.lua";
    lua_shared_dict limit 10m;
    init_by_lua_file  /usr/local/nginx/conf/waf/init.lua; 
	access_by_lua_file /usr/local/nginx/conf/waf/waf.lua;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ÈÖçÁΩÆconfig.luaÈáåÁöÑwafËßÑÂàôÁõÆÂΩï(‰∏ÄËà¨Âú®waf/conf/ÁõÆÂΩï‰∏ã)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    RulePath = "/usr/local/nginx/conf/waf/wafconf/"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ÁªùÂØπË∑ØÂæÑÂ¶ÇÊúâÂèòÂä®ÔºåÈúÄÂØπÂ∫î‰øÆÊîπ&lt;/p&gt;
&lt;p&gt;ÁÑ∂ÂêéÈáçÂêØnginxÂç≥ÂèØ&lt;/p&gt;
&lt;p&gt;###ÈÖçÁΩÆÊñá‰ª∂ËØ¶ÁªÜËØ¥ÊòéÔºö&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;	RulePath = "/usr/local/nginx/conf/waf/wafconf/"
    --ËßÑÂàôÂ≠òÊîæÁõÆÂΩï
    attacklog = "off"
    --ÊòØÂê¶ÂºÄÂêØÊîªÂáª‰ø°ÊÅØËÆ∞ÂΩïÔºåÈúÄË¶ÅÈÖçÁΩÆlogdir
    logdir = "/usr/local/nginx/logs/hack/"
    --logÂ≠òÂÇ®ÁõÆÂΩïÔºåËØ•ÁõÆÂΩïÈúÄË¶ÅÁî®Êà∑Ëá™Â∑±Êñ∞Âª∫ÔºåÂàáÈúÄË¶ÅnginxÁî®Êà∑ÁöÑÂèØÂÜôÊùÉÈôê
    UrlDeny="on"
    --ÊòØÂê¶Êã¶Êà™urlËÆøÈóÆ
    Redirect="on"
    --ÊòØÂê¶Êã¶Êà™ÂêéÈáçÂÆöÂêë
    CookieMatch = "on"
    --ÊòØÂê¶Êã¶Êà™cookieÊîªÂáª
    postMatch = "on" 
    --ÊòØÂê¶Êã¶Êà™postÊîªÂáª
    whiteModule = "on" 
    --ÊòØÂê¶ÂºÄÂêØURLÁôΩÂêçÂçï
    black_fileExt={"php","jsp"}
    --Â°´ÂÜô‰∏çÂÖÅËÆ∏‰∏ä‰º†Êñá‰ª∂ÂêéÁºÄÁ±ªÂûã
    ipWhitelist={"127.0.0.1"}
    --ipÁôΩÂêçÂçïÔºåÂ§ö‰∏™ipÁî®ÈÄóÂè∑ÂàÜÈöî
    ipBlocklist={"1.0.0.1"}
    --ipÈªëÂêçÂçïÔºåÂ§ö‰∏™ipÁî®ÈÄóÂè∑ÂàÜÈöî
    CCDeny="on"
    --ÊòØÂê¶ÂºÄÂêØÊã¶Êà™ccÊîªÂáª(ÈúÄË¶Ånginx.confÁöÑhttpÊÆµÂ¢ûÂä†lua_shared_dict limit 10m;)
    CCrate = "100/60"
    --ËÆæÁΩÆccÊîªÂáªÈ¢ëÁéáÔºåÂçï‰Ωç‰∏∫Áßí.
    --ÈªòËÆ§1ÂàÜÈíüÂêå‰∏Ä‰∏™IPÂè™ËÉΩËØ∑Ê±ÇÂêå‰∏Ä‰∏™Âú∞ÂùÄ100Ê¨°
    html=[[Please go away~~]]
    --Ë≠¶ÂëäÂÜÖÂÆπ,ÂèØÂú®‰∏≠Êã¨Âè∑ÂÜÖËá™ÂÆö‰πâ
    Â§áÊ≥®:‰∏çË¶Å‰π±Âä®ÂèåÂºïÂè∑ÔºåÂå∫ÂàÜÂ§ßÂ∞èÂÜô
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;###Ê£ÄÊü•ËßÑÂàôÊòØÂê¶ÁîüÊïà&lt;/p&gt;
&lt;p&gt;ÈÉ®ÁΩ≤ÂÆåÊØïÂèØ‰ª•Â∞ùËØïÂ¶Ç‰∏ãÂëΩ‰ª§Ôºö&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    curl http://xxxx/test.php?id=../etc/passwd
    ËøîÂõû"Please go away~~"Â≠óÊ†∑ÔºåËØ¥ÊòéËßÑÂàôÁîüÊïà„ÄÇ
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ê≥®ÊÑè:ÈªòËÆ§ÔºåÊú¨Êú∫Âú®ÁôΩÂêçÂçï‰∏çËøáÊª§ÔºåÂèØËá™Ë°åË∞ÉÊï¥config.luaÈÖçÁΩÆ&lt;/p&gt;
&lt;p&gt;###ÊïàÊûúÂõæÂ¶Ç‰∏ãÔºö&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e79807b95573c06a0c35a6c50608fa0e32500876/687474703a2f2f692e696d6775722e636f6d2f7754674f636d322e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/e79807b95573c06a0c35a6c50608fa0e32500876/687474703a2f2f692e696d6775722e636f6d2f7754674f636d322e706e67" alt="sec" data-canonical-src="http://i.imgur.com/wTgOcm2.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/80f6dda070cdea82c50de503ac4036ecdbb5ebc9/687474703a2f2f692e696d6775722e636f6d2f447155333061752e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/80f6dda070cdea82c50de503ac4036ecdbb5ebc9/687474703a2f2f692e696d6775722e636f6d2f447155333061752e706e67" alt="sec" data-canonical-src="http://i.imgur.com/DqU30au.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;###ËßÑÂàôÊõ¥Êñ∞Ôºö&lt;/p&gt;
&lt;p&gt;ËÄÉËôëÂà∞Ê≠£ÂàôÁöÑÁºìÂ≠òÈóÆÈ¢òÔºåÂä®ÊÄÅËßÑÂàô‰ºöÂΩ±ÂìçÊÄßËÉΩÔºåÊâÄ‰ª•ÊöÇÊ≤°Áî®ÂÖ±‰∫´ÂÜÖÂ≠òÂ≠óÂÖ∏Âíåredis‰πãÁ±ª‰∏úË•øÂÅöÂä®ÊÄÅÁÆ°ÁêÜ„ÄÇ&lt;/p&gt;
&lt;p&gt;ËßÑÂàôÊõ¥Êñ∞ÂèØ‰ª•ÊääËßÑÂàôÊñá‰ª∂ÊîæÁΩÆÂà∞ÂÖ∂‰ªñÊúçÂä°Âô®ÔºåÈÄöËøácrontab‰ªªÂä°ÂÆöÊó∂‰∏ãËΩΩÊù•Êõ¥Êñ∞ËßÑÂàôÔºånginx reloadÂç≥ÂèØÁîüÊïà„ÄÇ‰ª•‰øùÈöúngx lua wafÁöÑÈ´òÊÄßËÉΩ„ÄÇ&lt;/p&gt;
&lt;p&gt;Âè™ËÆ∞ÂΩïËøáÊª§Êó•ÂøóÔºå‰∏çÂºÄÂêØËøáÊª§ÔºåÂú®‰ª£Á†ÅÈáåÂú®checkÂâçÈù¢Âä†‰∏ä--Ê≥®ÈáäÂç≥ÂèØÔºåÂ¶ÇÊûúÈúÄË¶ÅËøáÊª§ÔºåÂèç‰πã&lt;/p&gt;
&lt;p&gt;###‰∏Ä‰∫õËØ¥ÊòéÔºö&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ËøáÊª§ËßÑÂàôÂú®wafconf‰∏ãÔºåÂèØÊ†πÊçÆÈúÄÊ±ÇËá™Ë°åË∞ÉÊï¥ÔºåÊØèÊù°ËßÑÂàôÈúÄÊç¢Ë°å,ÊàñËÄÖÁî®|ÂàÜÂâ≤

	argsÈáåÈù¢ÁöÑËßÑÂàôgetÂèÇÊï∞ËøõË°åËøáÊª§ÁöÑ
	urlÊòØÂè™Âú®getËØ∑Ê±ÇurlËøáÊª§ÁöÑËßÑÂàô		
	postÊòØÂè™Âú®postËØ∑Ê±ÇËøáÊª§ÁöÑËßÑÂàô		
	whitelistÊòØÁôΩÂêçÂçïÔºåÈáåÈù¢ÁöÑurlÂåπÈÖçÂà∞‰∏çÂÅöËøáÊª§		
	user-agentÊòØÂØπuser-agentÁöÑËøáÊª§ËßÑÂàô


ÈªòËÆ§ÂºÄÂêØ‰∫ÜgetÂíåpostËøáÊª§ÔºåÈúÄË¶ÅÂºÄÂêØcookieËøáÊª§ÁöÑÔºåÁºñËæëwaf.luaÂèñÊ∂àÈÉ®ÂàÜ--Ê≥®ÈáäÂç≥ÂèØ

Êó•ÂøóÊñá‰ª∂ÂêçÁß∞Ê†ºÂºèÂ¶Ç‰∏ã:ËôöÊãü‰∏ªÊú∫Âêç_sec.log
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-copyright" class="anchor" aria-hidden="true" href="#copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Copyright&lt;/h2&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;Weibo&lt;/td&gt;&lt;td&gt;Á•ûÂ•áÁöÑÈ≠îÊ≥ïÂ∏à&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Forum&lt;/td&gt;&lt;td&gt;&lt;a href="http://bbs.linuxtone.org/" rel="nofollow"&gt;http://bbs.linuxtone.org/&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Copyright&lt;/td&gt;&lt;td&gt;Copyright (c) 2013- loveshell&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;License&lt;/td&gt;&lt;td&gt;MIT License&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;ÊÑüË∞¢ngx_luaÊ®°ÂùóÁöÑÂºÄÂèëËÄÖ&lt;a href="https://github.com/agentzh/"&gt;@agentzh&lt;/a&gt;,Êò•Âì•ÊòØÊàëÊâÄÊé•Ëß¶ËøáÂºÄÊ∫êÁ≤æÁ•ûÊúÄÂ•ΩÁöÑ‰∫∫&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>loveshell</author><guid isPermaLink="false">https://github.com/loveshell/ngx_lua_waf</guid><pubDate>Thu, 12 Dec 2019 00:06:00 GMT</pubDate></item><item><title>kakysha/HonorSpy #7 in Lua, Today</title><link>https://github.com/kakysha/HonorSpy</link><description>&lt;p&gt;&lt;i&gt;World of Warcraft: Classic 1.13.2 HonorSpy addon&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-honorspy-addon-for-wow-classic" class="anchor" aria-hidden="true" href="#honorspy-addon-for-wow-classic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HonorSpy addon for WoW: Classic&lt;/h2&gt;
&lt;p&gt;Addon helps players estimate their PvP next week rank and overall progress.&lt;/p&gt;
&lt;p&gt;It uses the exact formulaes as game server does, the only difference is that it operates on the database collected by players themselves. The final result is pretty close to what you get in reality, as the database is collected by all addon users and is synced instantly across other players.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-it-works" class="anchor" aria-hidden="true" href="#how-it-works"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How it works&lt;/h3&gt;
&lt;p&gt;Addon does all the magic in background.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;addon inspects every player you meet (you should mouseover the player or target him, in inspect range), stores his PvP data in your local database, send this info to other addon users&lt;/li&gt;
&lt;li&gt;occasionally, when you die, you broadcast your whole database to other users. It works other way around, so you get the most recent database from every other player when they die, and merge it into your database.&lt;/li&gt;
&lt;li&gt;data is synced across "RAID", "BATTLEGROUND" and "GUILD" channels, so when you play on BGs you transmit and receive data from your teammates. And all the time you exchange your data with your guildmates.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Right click on minimap icon to estimate your progress without opening the addon window.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-install" class="anchor" aria-hidden="true" href="#install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install&lt;/h3&gt;
&lt;p&gt;You have three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use Twitch app to install this addon. Just search for 'honorspy' in Mods section of the app.&lt;/li&gt;
&lt;li&gt;Download directly from Curseforge &lt;a href="https://www.curseforge.com/wow/addons/honorspy" rel="nofollow"&gt;https://www.curseforge.com/wow/addons/honorspy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Download latest release from Github (&lt;a href="https://github.com/kakysha/HonorSpy/releases/latest"&gt;https://github.com/kakysha/HonorSpy/releases/latest&lt;/a&gt;), unzip and put it in Interface/Addons folder, relaunch WoW.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Estimates your honor during the day&lt;/li&gt;
&lt;li&gt;Calculates diminishing returns after each kill, prints into chat real honor gained and number of kills for every victim&lt;/li&gt;
&lt;li&gt;It inspects every player in 'inspect range' which you target or mouseover&lt;/li&gt;
&lt;li&gt;It syncs your db with other party/raid/bg members and your guildmates on your death&lt;/li&gt;
&lt;li&gt;It can estimate your (or specific player) onward RP, Rank and Progress, taking into account your (player's) standing and pool size&lt;/li&gt;
&lt;li&gt;It can export your internal DB in CSV format to copy-paste it into Google Spreadsheets for future calculations. &lt;a href="https://docs.google.com/spreadsheets/d/1OvZ7PRhrFjRn8IoH8HIPwHfRDEq50uO64YLCsSsjBQc/edit#gid=2113352865" rel="nofollow"&gt;Spreadsheet done specially for HonorSpy&lt;/a&gt;, it will estimate RP for all players&lt;/li&gt;
&lt;li&gt;It supports automatic weekly pvp reset. Reset day can be configured&lt;/li&gt;
&lt;li&gt;Supports sorting by Rank and Honor&lt;/li&gt;
&lt;li&gt;Groups players in table by brackets&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Esc ‚Üí Interface Options ‚Üí Addons ‚Üí HonorSpy&lt;/em&gt; for addon settings&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It only stores players with &amp;gt;15HKs.
Reset day can be configured, default is Wednesday. Reset time is fixed at 10AM UTC.&lt;/p&gt;
&lt;p&gt;P.S. Do not be afraid of losing all your data, very likely that other players with HonorSpy will push you their database very soon. The more players use and collect data -&amp;gt; the more up-to-date data you will have. Magic of sync.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-commands" class="anchor" aria-hidden="true" href="#commands"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Commands&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;/hs show&lt;/code&gt; -&amp;gt; show/hide standings table&lt;/p&gt;
&lt;p&gt;&lt;code&gt;/hs search player_name&lt;/code&gt; -&amp;gt; report specific player's standing&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-screenshot" class="anchor" aria-hidden="true" href="#screenshot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Screenshot&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/b066a8c627a539a0abc0a0140279dcef21a2c03e/68747470733a2f2f686162726173746f726167652e6f72672f776562742f316a2f63612f2d7a2f316a63612d7a67616272356532727667306f756a616b646d6e73612e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/b066a8c627a539a0abc0a0140279dcef21a2c03e/68747470733a2f2f686162726173746f726167652e6f72672f776562742f316a2f63612f2d7a2f316a63612d7a67616272356532727667306f756a616b646d6e73612e706e67" alt="HonorSpy Screenshot" data-canonical-src="https://habrastorage.org/webt/1j/ca/-z/1jca-zgabr5e2rvg0oujakdmnsa.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>kakysha</author><guid isPermaLink="false">https://github.com/kakysha/HonorSpy</guid><pubDate>Thu, 12 Dec 2019 00:07:00 GMT</pubDate></item><item><title>Stephan-S/FS19_AutoDrive #8 in Lua, Today</title><link>https://github.com/Stephan-S/FS19_AutoDrive</link><description>&lt;p&gt;&lt;i&gt;FS19 version of AutoDrive - Developer Version&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-fs19_autodrive" class="anchor" aria-hidden="true" href="#fs19_autodrive"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FS19_AutoDrive&lt;/h1&gt;
&lt;p&gt;FS19 version of AutoDrive&lt;/p&gt;
&lt;p&gt;If you want to support my development effort, the best way is to open issues on any bugs you encounter or for features you would like to be added to the mod.&lt;/p&gt;
&lt;p&gt;Wer die Weiterentwicklung des Mods unterst√ºtzen m√∂chte, kann dies am Besten durch flei√üiges Erstellen von Issues zu gefundenen Bugs und/oder gew√ºnschten Erweiterungen zum Mod tun.&lt;/p&gt;
&lt;p&gt;If you like my work, feel free to buy me a coffee (of which I drink quite a lot :D )
&lt;a href="https://www.buymeacoffee.com/9Di7EUSI2" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/031fc5a134cdca5ae3460822aba371e63f794233/68747470733a2f2f7777772e6275796d6561636f666665652e636f6d2f6173736574732f696d672f637573746f6d5f696d616765732f6f72616e67655f696d672e706e67" alt="Buy Me A Coffee" data-canonical-src="https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.paypal.me/StephanSchlosser" rel="nofollow"&gt;https://www.paypal.me/StephanSchlosser&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Stephan-S</author><guid isPermaLink="false">https://github.com/Stephan-S/FS19_AutoDrive</guid><pubDate>Thu, 12 Dec 2019 00:08:00 GMT</pubDate></item><item><title>jcjohnson/neural-style #9 in Lua, Today</title><link>https://github.com/jcjohnson/neural-style</link><description>&lt;p&gt;&lt;i&gt;Torch implementation of neural style algorithm&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-neural-style" class="anchor" aria-hidden="true" href="#neural-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;neural-style&lt;/h1&gt;
&lt;p&gt;This is a torch implementation of the paper &lt;a href="http://arxiv.org/abs/1508.06576" rel="nofollow"&gt;A Neural Algorithm of Artistic Style&lt;/a&gt;
by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.&lt;/p&gt;
&lt;p&gt;The paper presents an algorithm for combining the content of one image with the style of another image using
convolutional neural networks. Here's an example that maps the artistic style of
&lt;a href="https://en.wikipedia.org/wiki/The_Starry_Night" rel="nofollow"&gt;The Starry Night&lt;/a&gt;
onto a night-time photograph of the Stanford campus:&lt;/p&gt;
&lt;div align="center"&gt;
 &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/starry_night_google.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/starry_night_google.jpg" height="223px" style="max-width:100%;"&gt;&lt;/a&gt;
 &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/hoovertowernight.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/hoovertowernight.jpg" height="223px" style="max-width:100%;"&gt;&lt;/a&gt;
 &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/starry_stanford_bigger.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/starry_stanford_bigger.png" width="710px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Applying the style of different images to the same content image gives interesting results.
Here we reproduce Figure 2 from the paper, which renders a photograph of the Tubingen in Germany in a
variety of styles:&lt;/p&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/tubingen.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/tubingen.jpg" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_shipwreck.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_shipwreck.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_scream.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_scream.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_seated_nude.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_seated_nude.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_composition_vii.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_composition_vii.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Here are the results of applying the style of various pieces of artwork to this photograph of the
golden gate bridge:&lt;/p&gt;
&lt;div align="center" height="200px"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/frida_kahlo.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/frida_kahlo.jpg" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_kahlo.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_kahlo.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/escher_sphere.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/escher_sphere.jpg" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_escher.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_escher.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/woman-with-hat-matisse.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/woman-with-hat-matisse.jpg" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_matisse.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_matisse.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/the_scream.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/the_scream.jpg" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_scream.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_scream.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/starry_night_crop.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/starry_night_crop.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/seated-nude.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/seated-nude.jpg" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_seated.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_seated.png" height="160px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-content--style-tradeoff" class="anchor" aria-hidden="true" href="#content--style-tradeoff"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Content / Style Tradeoff&lt;/h3&gt;
&lt;p&gt;The algorithm allows the user to trade-off the relative weight of the style and content reconstruction terms,
as shown in this example where we port the style of &lt;a href="http://www.wikiart.org/en/pablo-picasso/self-portrait-1907" rel="nofollow"&gt;Picasso's 1907 self-portrait&lt;/a&gt; onto Brad Pitt:&lt;/p&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/picasso_selfport1907.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/picasso_selfport1907.jpg" height="220px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/brad_pitt.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/inputs/brad_pitt.jpg" height="220px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_5_style_10.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_5_style_10.png" height="220px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_1_style_10.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_1_style_10.png" height="220px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_01_style_10.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_01_style_10.png" height="220px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_0025_style_10.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/pitt_picasso_content_0025_style_10.png" height="220px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-style-scale" class="anchor" aria-hidden="true" href="#style-scale"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Style Scale&lt;/h3&gt;
&lt;p&gt;By resizing the style image before extracting style features, we can control the types of artistic
features that are transfered from the style image; you can control this behavior with the &lt;code&gt;-style_scale&lt;/code&gt; flag.
Below we see three examples of rendering the Golden Gate Bridge in the style of The Starry Night.
From left to right, &lt;code&gt;-style_scale&lt;/code&gt; is 2.0, 1.0, and 0.5.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scale2.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scale2.png" height="175px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scale1.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scale1.png" height="175px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scale05.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scale05.png" height="175px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-multiple-style-images" class="anchor" aria-hidden="true" href="#multiple-style-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multiple Style Images&lt;/h3&gt;
&lt;p&gt;You can use more than one style image to blend multiple artistic styles.&lt;/p&gt;
&lt;p&gt;Clockwise from upper left: "The Starry Night" + "The Scream", "The Scream" + "Composition VII",
"Seated Nude" + "Composition VII", and "Seated Nude" + "The Starry Night"&lt;/p&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry_scream.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry_scream.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_scream_composition_vii.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_scream_composition_vii.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry_seated.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry_seated.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_seated_nude_composition_vii.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_seated_nude_composition_vii.png" height="250px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-style-interpolation" class="anchor" aria-hidden="true" href="#style-interpolation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Style Interpolation&lt;/h3&gt;
&lt;p&gt;When using multiple style images, you can control the degree to which they are blended:&lt;/p&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scream_3_7.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scream_3_7.png" height="175px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scream_5_5.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scream_5_5.png" height="175px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scream_7_3.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/golden_gate_starry_scream_7_3.png" height="175px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-transfer-style-but-not-color" class="anchor" aria-hidden="true" href="#transfer-style-but-not-color"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transfer style but not color&lt;/h3&gt;
&lt;p&gt;If you add the flag &lt;code&gt;-original_colors 1&lt;/code&gt; then the output image will retain the colors of the original image;
this is similar to &lt;a href="http://blog.deepart.io/2016/06/04/color-independent-style-transfer/" rel="nofollow"&gt;the recent blog post by deepart.io&lt;/a&gt;.&lt;/p&gt;
&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_starry.png" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_scream.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_scream.png" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_composition_vii.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/tubingen_composition_vii.png" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/original_color/tubingen_starry.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/original_color/tubingen_starry.png" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/original_color/tubingen_scream.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/original_color/tubingen_scream.png" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/original_color/tubingen_composition_vii.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/original_color/tubingen_composition_vii.png" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup:&lt;/h2&gt;
&lt;p&gt;Dependencies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/torch/torch7"&gt;torch7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/szagoruyko/loadcaffe"&gt;loadcaffe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Optional dependencies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For CUDA backend:
&lt;ul&gt;
&lt;li&gt;CUDA 6.5+&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/torch/cunn"&gt;cunn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For cuDNN backend:
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/soumith/cudnn.torch"&gt;cudnn.torch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For OpenCL backend:
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/hughperkins/cltorch"&gt;cltorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hughperkins/clnn"&gt;clnn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After installing dependencies, you'll need to run the following script to download the VGG model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sh models/download_models.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will download the original &lt;a href="https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md"&gt;VGG-19 model&lt;/a&gt;.
Leon Gatys has graciously provided the modified version of the VGG-19 model that was used in their paper;
this will also be downloaded. By default the original VGG-19 model is used.&lt;/p&gt;
&lt;p&gt;If you have a smaller memory GPU then using NIN Imagenet model will be better and gives slightly worse yet comparable results. You can get the details on the model from &lt;a href="https://github.com/BVLC/caffe/wiki/Model-Zoo"&gt;BVLC Caffe ModelZoo&lt;/a&gt; and can download the files from &lt;a href="https://drive.google.com/folderview?id=0B0IedYUunOQINEFtUi1QNWVhVVU&amp;amp;usp=drive_web" rel="nofollow"&gt;NIN-Imagenet Download Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can find detailed installation instructions for Ubuntu in the &lt;a href="INSTALL.md"&gt;installation guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;Basic usage:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;th neural_style.lua -style_image &amp;lt;image.jpg&amp;gt; -content_image &amp;lt;image.jpg&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OpenCL usage with NIN Model (This requires you download the NIN Imagenet model files as described above):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;th neural_style.lua -style_image examples/inputs/picasso_selfport1907.jpg -content_image examples/inputs/brad_pitt.jpg -output_image profile.png -model_file models/nin_imagenet_conv.caffemodel -proto_file models/train_val.prototxt -gpu 0 -backend clnn -num_iterations 1000 -seed 123 -content_layers relu0,relu3,relu7,relu12 -style_layers relu0,relu3,relu7,relu12 -content_weight 10 -style_weight 1000 -image_size 512 -optimizer adam
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/examples/outputs/pitt_picasso_nin_opencl.png"&gt;&lt;img src="/examples/outputs/pitt_picasso_nin_opencl.png" alt="OpenCL NIN Model Picasso Brad Pitt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To use multiple style images, pass a comma-separated list like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-style_image starry_night.jpg,the_scream.jpg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note that paths to images should not contain the &lt;code&gt;~&lt;/code&gt; character to represent your home directory; you should instead use a relative
path or a full absolute path.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Options&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-image_size&lt;/code&gt;: Maximum side length (in pixels) of of the generated image. Default is 512.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-style_blend_weights&lt;/code&gt;: The weight for blending the style of multiple style images, as a
comma-separated list, such as &lt;code&gt;-style_blend_weights 3,7&lt;/code&gt;. By default all style images
are equally weighted.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-gpu&lt;/code&gt;: Zero-indexed ID of the GPU to use; for CPU mode set &lt;code&gt;-gpu&lt;/code&gt; to -1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Optimization options&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-content_weight&lt;/code&gt;: How much to weight the content reconstruction term. Default is 5e0.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-style_weight&lt;/code&gt;: How much to weight the style reconstruction term. Default is 1e2.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-tv_weight&lt;/code&gt;: Weight of total-variation (TV) regularization; this helps to smooth the image.
Default is 1e-3. Set to 0 to disable TV regularization.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-num_iterations&lt;/code&gt;: Default is 1000.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-init&lt;/code&gt;: Method for generating the generated image; one of &lt;code&gt;random&lt;/code&gt; or &lt;code&gt;image&lt;/code&gt;.
Default is &lt;code&gt;random&lt;/code&gt; which uses a noise initialization as in the paper; &lt;code&gt;image&lt;/code&gt;
initializes with the content image.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-optimizer&lt;/code&gt;: The optimization algorithm to use; either &lt;code&gt;lbfgs&lt;/code&gt; or &lt;code&gt;adam&lt;/code&gt;; default is &lt;code&gt;lbfgs&lt;/code&gt;.
L-BFGS tends to give better results, but uses more memory. Switching to ADAM will reduce memory usage;
when using ADAM you will probably need to play with other parameters to get good results, especially
the style weight, content weight, and learning rate; you may also want to normalize gradients when
using ADAM.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-learning_rate&lt;/code&gt;: Learning rate to use with the ADAM optimizer. Default is 1e1.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-normalize_gradients&lt;/code&gt;: If this flag is present, style and content gradients from each layer will be
L1 normalized. Idea from &lt;a href="https://github.com/andersbll/neural_artistic_style"&gt;andersbll/neural_artistic_style&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Output options&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-output_image&lt;/code&gt;: Name of the output image. Default is &lt;code&gt;out.png&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-print_iter&lt;/code&gt;: Print progress every &lt;code&gt;print_iter&lt;/code&gt; iterations. Set to 0 to disable printing.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-save_iter&lt;/code&gt;: Save the image every &lt;code&gt;save_iter&lt;/code&gt; iterations. Set to 0 to disable saving intermediate results.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Layer options&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-content_layers&lt;/code&gt;: Comma-separated list of layer names to use for content reconstruction.
Default is &lt;code&gt;relu4_2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-style_layers&lt;/code&gt;: Comma-separated list of layer names to use for style reconstruction.
Default is &lt;code&gt;relu1_1,relu2_1,relu3_1,relu4_1,relu5_1&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Other options&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-style_scale&lt;/code&gt;: Scale at which to extract features from the style image. Default is 1.0.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-original_colors&lt;/code&gt;: If you set this to 1, then the output image will keep the colors of the content image.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-proto_file&lt;/code&gt;: Path to the &lt;code&gt;deploy.txt&lt;/code&gt; file for the VGG Caffe model.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-model_file&lt;/code&gt;: Path to the &lt;code&gt;.caffemodel&lt;/code&gt; file for the VGG Caffe model.
Default is the original VGG-19 model; you can also try the normalized VGG-19 model used in the paper.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-pooling&lt;/code&gt;: The type of pooling layers to use; one of &lt;code&gt;max&lt;/code&gt; or &lt;code&gt;avg&lt;/code&gt;. Default is &lt;code&gt;max&lt;/code&gt;.
The VGG-19 models uses max pooling layers, but the paper mentions that replacing these layers with average
pooling layers can improve the results. I haven't been able to get good results using average pooling, but
the option is here.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend&lt;/code&gt;: &lt;code&gt;nn&lt;/code&gt;, &lt;code&gt;cudnn&lt;/code&gt;, or &lt;code&gt;clnn&lt;/code&gt;. Default is &lt;code&gt;nn&lt;/code&gt;. &lt;code&gt;cudnn&lt;/code&gt; requires
&lt;a href="https://github.com/soumith/cudnn.torch"&gt;cudnn.torch&lt;/a&gt; and may reduce memory usage.
&lt;code&gt;clnn&lt;/code&gt; requires &lt;a href="https://github.com/hughperkins/cltorch"&gt;cltorch&lt;/a&gt; and &lt;a href="https://github.com/hughperkins/clnn"&gt;clnn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-cudnn_autotune&lt;/code&gt;: When using the cuDNN backend, pass this flag to use the built-in cuDNN autotuner to select
the best convolution algorithms for your architecture. This will make the first iteration a bit slower and can
take a bit more memory, but may significantly speed up the cuDNN backend.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-frequently-asked-questions" class="anchor" aria-hidden="true" href="#frequently-asked-questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Frequently Asked Questions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Generated image has saturation artifacts:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://cloud.githubusercontent.com/assets/1310570/9694690/fa8e8782-5328-11e5-9c91-11f7b215ad19.png"&gt;&lt;img src="https://cloud.githubusercontent.com/assets/1310570/9694690/fa8e8782-5328-11e5-9c91-11f7b215ad19.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Update the &lt;code&gt;image&lt;/code&gt; packge to the latest version: &lt;code&gt;luarocks install image&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Running without a GPU gives an error message complaining about &lt;code&gt;cutorch&lt;/code&gt; not found&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;
Pass the flag &lt;code&gt;-gpu -1&lt;/code&gt; when running in CPU-only mode&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; The program runs out of memory and dies&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Try reducing the image size: &lt;code&gt;-image_size 256&lt;/code&gt; (or lower). Note that different image sizes will likely
require non-default values for &lt;code&gt;-style_weight&lt;/code&gt; and &lt;code&gt;-content_weight&lt;/code&gt; for optimal results.
If you are running on a GPU, you can also try running with &lt;code&gt;-backend cudnn&lt;/code&gt; to reduce memory usage.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Get the following error message:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;models/VGG_ILSVRC_19_layers_deploy.prototxt.cpu.lua:7: attempt to call method 'ceil' (a nil value)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Update &lt;code&gt;nn&lt;/code&gt; package to the latest version: &lt;code&gt;luarocks install nn&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Get an error message complaining about &lt;code&gt;paths.extname&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Update &lt;code&gt;torch.paths&lt;/code&gt; package to the latest version: &lt;code&gt;luarocks install paths&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; NIN Imagenet model is not giving good results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Make sure the correct &lt;code&gt;-proto_file&lt;/code&gt; is selected. Also make sure the correct parameters for &lt;code&gt;-content_layers&lt;/code&gt; and &lt;code&gt;-style_layers&lt;/code&gt; are set. (See OpenCL usage example above.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; &lt;code&gt;-backend cudnn&lt;/code&gt; is slower than default NN backend&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Add the flag &lt;code&gt;-cudnn_autotune&lt;/code&gt;; this will use the built-in cuDNN autotuner to select the best convolution algorithms.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-memory-usage" class="anchor" aria-hidden="true" href="#memory-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Memory Usage&lt;/h2&gt;
&lt;p&gt;By default, &lt;code&gt;neural-style&lt;/code&gt; uses the &lt;code&gt;nn&lt;/code&gt; backend for convolutions and L-BFGS for optimization.
These give good results, but can both use a lot of memory. You can reduce memory usage with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Use cuDNN&lt;/strong&gt;: Add the flag &lt;code&gt;-backend cudnn&lt;/code&gt; to use the cuDNN backend. This will only work in GPU mode.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use ADAM&lt;/strong&gt;: Add the flag &lt;code&gt;-optimizer adam&lt;/code&gt; to use ADAM instead of L-BFGS. This should significantly
reduce memory usage, but may require tuning of other parameters for good results; in particular you should
play with the learning rate, content weight, style weight, and also consider using gradient normalization.
This should work in both CPU and GPU modes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduce image size&lt;/strong&gt;: If the above tricks are not enough, you can reduce the size of the generated image;
pass the flag &lt;code&gt;-image_size 256&lt;/code&gt; to generate an image at half the default size.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the default settings, &lt;code&gt;neural-style&lt;/code&gt; uses about 3.5GB of GPU memory on my system;
switching to ADAM and cuDNN reduces the GPU memory footprint to about 1GB.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-speed" class="anchor" aria-hidden="true" href="#speed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed&lt;/h2&gt;
&lt;p&gt;Speed can vary a lot depending on the backend and the optimizer.
Here are some times for running 500 iterations with &lt;code&gt;-image_size=512&lt;/code&gt; on a Maxwell Titan X with different settings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-backend nn -optimizer lbfgs&lt;/code&gt;: 62 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend nn -optimizer adam&lt;/code&gt;: 49 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend cudnn -optimizer lbfgs&lt;/code&gt;: 79 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend cudnn -cudnn_autotune -optimizer lbfgs&lt;/code&gt;: 58 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend cudnn -cudnn_autotune -optimizer adam&lt;/code&gt;: 44 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend clnn -optimizer lbfgs&lt;/code&gt;: 169 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend clnn -optimizer adam&lt;/code&gt;: 106 seconds&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are the same benchmarks on a Pascal Titan X with cuDNN 5.0 on CUDA 8.0 RC:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-backend nn -optimizer lbfgs&lt;/code&gt;: 43 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend nn -optimizer adam&lt;/code&gt;: 36 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend cudnn -optimizer lbfgs&lt;/code&gt;: 45 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend cudnn -cudnn_autotune -optimizer lbfgs&lt;/code&gt;: 30 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-backend cudnn -cudnn_autotune -optimizer adam&lt;/code&gt;: 22 seconds&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-multi-gpu-scaling" class="anchor" aria-hidden="true" href="#multi-gpu-scaling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-GPU scaling&lt;/h2&gt;
&lt;p&gt;You can use multiple GPUs to process images at higher resolutions; different layers of the network will be
computed on different GPUs. You can control which GPUs are used with the &lt;code&gt;-gpu&lt;/code&gt; flag, and you can control
how to split layers across GPUs using the &lt;code&gt;-multigpu_strategy&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;For example in a server with four GPUs, you can give the flag &lt;code&gt;-gpu 0,1,2,3&lt;/code&gt; to process on GPUs 0, 1, 2, and
3 in that order; by also giving the flag &lt;code&gt;-multigpu_strategy 3,6,12&lt;/code&gt; you indicate that the first two layers
should be computed on GPU 0, layers 3 to 5 should be computed on GPU 1, layers 6 to 11 should be computed on
GPU 2, and the remaining layers should be computed on GPU 3. You will need to tune the &lt;code&gt;-multigpu_strategy&lt;/code&gt;
for your setup in order to achieve maximal resolution.&lt;/p&gt;
&lt;p&gt;We can achieve very high quality results at high resolution by combining multi-GPU processing with multiscale
generation as described in the paper
&lt;a href="https://arxiv.org/abs/1611.07865" rel="nofollow"&gt;&lt;strong&gt;Controlling Perceptual Factors in Neural Style Transfer&lt;/strong&gt;&lt;/a&gt; by Leon A. Gatys,
Alexander S. Ecker, Matthias Bethge, Aaron Hertzmann and Eli Shechtman.&lt;/p&gt;
&lt;p&gt;Here is a 3620 x 1905 image generated on a server with four Pascal Titan X GPUs:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/starry_stanford_bigger.png"&gt;&lt;img src="https://raw.githubusercontent.com/jcjohnson/neural-style/master/examples/outputs/starry_stanford_bigger.png" height="400px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The script used to generate this image &lt;a href="examples/multigpu_scripts/starry_stanford.sh"&gt;can be found here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-implementation-details" class="anchor" aria-hidden="true" href="#implementation-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementation details&lt;/h2&gt;
&lt;p&gt;Images are initialized with white noise and optimized using L-BFGS.&lt;/p&gt;
&lt;p&gt;We perform style reconstructions using the &lt;code&gt;conv1_1&lt;/code&gt;, &lt;code&gt;conv2_1&lt;/code&gt;, &lt;code&gt;conv3_1&lt;/code&gt;, &lt;code&gt;conv4_1&lt;/code&gt;, and &lt;code&gt;conv5_1&lt;/code&gt; layers
and content reconstructions using the &lt;code&gt;conv4_2&lt;/code&gt; layer. As in the paper, the five style reconstruction losses have
equal weights.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you find this code useful for your research, please cite:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@misc{Johnson2015,
  author = {Johnson, Justin},
  title = {neural-style},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jcjohnson/neural-style}},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jcjohnson</author><guid isPermaLink="false">https://github.com/jcjohnson/neural-style</guid><pubDate>Thu, 12 Dec 2019 00:09:00 GMT</pubDate></item></channel></rss>