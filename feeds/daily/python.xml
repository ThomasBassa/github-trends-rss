<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Python, Today</title><link>https://github.com/trending/python?since=daily</link><description>The top repositories on GitHub for python, measured daily</description><pubDate>Tue, 11 Feb 2020 01:06:38 GMT</pubDate><lastBuildDate>Tue, 11 Feb 2020 01:06:38 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>cycz/jdBuyMask #1 in Python, Today</title><link>https://github.com/cycz/jdBuyMask</link><description>&lt;p&gt;&lt;i&gt;京东监控口罩有货爬虫，自动下单爬虫，口罩爬虫&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;hr&gt;
&lt;p&gt;如果帮助到你，star一下，谢谢你&lt;/p&gt;
&lt;p&gt;武汉加油，中国加油&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;使用上有问题的加一下这个群958535145（个人时间有限，回答不过来了）&lt;/p&gt;
&lt;p&gt;本代码使用方式 &lt;a href="https://blog.csdn.net/cyz52/article/details/104239558" rel="nofollow"&gt;https://blog.csdn.net/cyz52/article/details/104239558&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;下单的部分代码参考了tychxn大佬的代码&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;建议使用V2极速版&lt;/strong&gt;
每10分钟自动读取一次配置修改商品不需要退出重开&lt;/p&gt;
&lt;p&gt;避免抢购，程序自动一次只买一件&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-v2版本" class="anchor" aria-hidden="true" href="#v2版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;V2版本&lt;/h2&gt;
&lt;p&gt;请在configDemo.ini 加入商品id、地区id、cookie等参数
区分下单模式（默认2正常模式）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意--极速模式默认清空购物车&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;正常模式下单流程（1.7秒左右）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 检测有货&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 检测下柜&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 加入购物车&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 查看购物车&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 下单&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;极速模式下单流程（1.4秒左右）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 检测有货&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 加入购物车&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 下单&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-v3版本下单更快" class="anchor" aria-hidden="true" href="#v3版本下单更快"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;V3版本（下单更快）&lt;/h2&gt;
&lt;p&gt;下单更快，但只能扫描单独一件商品&lt;/p&gt;
&lt;p&gt;在配置文件configDemo.ini中，填写[V3]下面的skuid&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意--V3版本默认清空购物车&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;V3版本下单流程（1秒左右）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 提前加入购物车&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 检测有货&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 下单&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-exe版本" class="anchor" aria-hidden="true" href="#exe版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;exe版本&lt;/h2&gt;
&lt;p&gt;进群要吧，地址随时变，就不放了。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-温馨提示" class="anchor" aria-hidden="true" href="#温馨提示"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;温馨提示&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;在京东购物车结算页面设置发票为电子普通发票-个人设置支付方式为在线支付&lt;/li&gt;
&lt;li&gt;地区id不知道如何获取的，请使用AreaTool.py获取&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-版本" class="anchor" aria-hidden="true" href="#版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;版本&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; python3&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-功能" class="anchor" aria-hidden="true" href="#功能"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;功能&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 检查登录&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 确认是否有货&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 有货自动下单&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 邮件、微信通知&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-更新记录" class="anchor" aria-hidden="true" href="#更新记录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新记录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;【2020.02.10】每10分钟自动读取一次配置修改商品不需要退出重开，优化有货，不支持省份出售情况，优化日志，加快查询频率。&lt;/li&gt;
&lt;li&gt;【2020.02.09】部分下单需要验证码识别问题，部分bug优化。&lt;/li&gt;
&lt;li&gt;【2020.02.08】V2版本，区分下单模式，config中错别字，bug修复。&lt;/li&gt;
&lt;li&gt;【2020.02.07】V3版本，减少提交订单的请求量，总而言之就是更快（只能监控一件商品）。&lt;/li&gt;
&lt;li&gt;【2020.02.07】无货等情况下单失败不重试。&lt;/li&gt;
&lt;li&gt;【2020.02.07】新增微信通知（&lt;a href="http://sc.ftqq.com/3.version" rel="nofollow"&gt;http://sc.ftqq.com/3.version&lt;/a&gt; 查看sc_key），bug修复。&lt;/li&gt;
&lt;li&gt;【2020.02.06】V2版本，刷新更快更频繁，通过配置文件添加商品和地区id。&lt;/li&gt;
&lt;li&gt;【2020.02.06】提交失败之后会继续不会暂停。&lt;/li&gt;
&lt;li&gt;【2020.02.06】购物车有套装商品导致解析skuid错误。&lt;/li&gt;
&lt;li&gt;【2020.02.05】商品有货，但是该商品已下柜，提交会报错，对部分代码进行了优化。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-反馈问题" class="anchor" aria-hidden="true" href="#反馈问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;反馈问题&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果有红包先花掉再开脚本，不然可能需要支付密码&lt;/li&gt;
&lt;li&gt;出现下单地址不是默认地址的，在线下一单，取getOrderInfo.action链接的cookie&lt;/li&gt;
&lt;li&gt;CMD界面卡住、关闭CMD的快速编辑模式就行了&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>cycz</author><guid isPermaLink="false">https://github.com/cycz/jdBuyMask</guid><pubDate>Tue, 11 Feb 2020 00:01:00 GMT</pubDate></item><item><title>domlysz/BlenderGIS #2 in Python, Today</title><link>https://github.com/domlysz/BlenderGIS</link><description>&lt;p&gt;&lt;i&gt;Blender addons to make the bridge between Blender and geographic data&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-blender-gis" class="anchor" aria-hidden="true" href="#blender-gis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Blender GIS&lt;/h1&gt;
&lt;p&gt;Blender minimal version : 2.8&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mac users warning :&lt;/strong&gt; currently the addon does not work anymore on Mac because of an issue relative to Blender Mac build itself. Please do not report the issue here. It should be fixed by the Blender team soon. Check &lt;a href="https://developer.blender.org/T68243" rel="nofollow"&gt;the bug report&lt;/a&gt; to follow the progress on it.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wiki---faq---quick-start-guide" class="anchor" aria-hidden="true" href="#wiki---faq---quick-start-guide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Home"&gt;Wiki&lt;/a&gt; - &lt;a href="https://github.com/domlysz/BlenderGIS/wiki/FAQ"&gt;FAQ&lt;/a&gt; - &lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Quick-start"&gt;Quick start guide&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-basemaps" class="anchor" aria-hidden="true" href="#basemaps"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Basemaps"&gt;Basemaps&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Display web map service like OpenStreetMap directly in Blender&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/Blender27x/images/basemaps_demo.gif"&gt;&lt;img src="https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/Blender27x/images/basemaps_demo.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-srtm-download" class="anchor" aria-hidden="true" href="#srtm-download"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/SRTM"&gt;SRTM download&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Get SRTM topographic data and apply it as height texture&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/Blender27x/images/srtm_demo.gif"&gt;&lt;img src="https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/Blender27x/images/srtm_demo.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-esri-shapefile-import--export" class="anchor" aria-hidden="true" href="#esri-shapefile-import--export"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Shapefile-import"&gt;ESRI Shapefile import / export&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A &lt;a href="http://en.wikipedia.org/wiki/Shapefile" rel="nofollow"&gt;Shapefile&lt;/a&gt; is a popular geospatial vector data format for geographic information system software.&lt;/p&gt;
&lt;p&gt;This tool can import into Blender most of shapefile feature type. It can also uses attributes data to define Z elevation values or Z extrusion values.&lt;/p&gt;
&lt;p&gt;Exporter script can export a mesh to pointZ, polylineZ or polygonZ shapefile. Note that currently this tool does not re-export attribute data include in the dbase file linked to the shapefile. So if you want to import a shapefile for edit it into Blender and then re-export it, you will lose attribute data.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-georeferenced-raster-importer" class="anchor" aria-hidden="true" href="#georeferenced-raster-importer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Import-georef-raster"&gt;Georeferenced raster importer&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Import geotiff or common image format georeferenced with a &lt;a href="http://en.wikipedia.org/wiki/World_file" rel="nofollow"&gt;world file&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can import the raster as a plane mesh, as backgound image for orthographic view, as UV texture mapping on a mesh or &lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Import-DEM-grid"&gt;as DEM&lt;/a&gt; for warp a mesh with the displace modifier.&lt;/p&gt;
&lt;p&gt;ESRI ASCII GRID format is also supported through a dedicated import tool.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-openstreetmap-import" class="anchor" aria-hidden="true" href="#openstreetmap-import"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/OSM-import"&gt;OpenStreetMap import&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/Blender27x/images/osm_demo.gif"&gt;&lt;img src="https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/Blender27x/images/osm_demo.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-georeferenced-render-output" class="anchor" aria-hidden="true" href="#georeferenced-render-output"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Make-a-georef-render"&gt;Georeferenced render output&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is a tool to create a new camera correctly setup for produce a map render. Georeferencing data (worldfile) are writing in text file accessible from the Blender text editor.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-delaunay-triangulation--voronoi-diagram" class="anchor" aria-hidden="true" href="#delaunay-triangulation--voronoi-diagram"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Make-terrain-mesh-with-Delaunay-triangulation"&gt;Delaunay triangulation &amp;amp; Voronoi diagram&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This script computes &lt;a href="http://en.wikipedia.org/wiki/Delaunay_triangulation" rel="nofollow"&gt;Delaunay triangulation&lt;/a&gt; in 2.5D. This triangulation is suitable for create a 3D terrain mesh from &lt;a href="http://en.wikipedia.org/wiki/Point_cloud" rel="nofollow"&gt;points cloud&lt;/a&gt; or &lt;a href="http://en.wikipedia.org/wiki/Contour_line" rel="nofollow"&gt;contour lines&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The script can also compute &lt;a href="http://en.wikipedia.org/wiki/Voronoi" rel="nofollow"&gt;Voronoi tessellation&lt;/a&gt; in 2D which is the dual of delaunay triangulation. Voronoi diagram is suitable to make neighborhood analysis map.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-terrain-analysis" class="anchor" aria-hidden="true" href="#terrain-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Terrain-analysis"&gt;Terrain analysis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This part of Blender GIS is designed to assist in the analysis of the topography : height, slope and azimuth (aspect).&lt;/p&gt;
&lt;p&gt;There are 2 tools, one to build materials nodes setup for Cycles engine, and a second to configure the color ramp as usual in common GIS software (reclassify values and apply color ramp presets).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-georeferencing-management" class="anchor" aria-hidden="true" href="#georeferencing-management"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Gereferencing-management"&gt;Georeferencing management&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Handle various projection systems with reprojection capabilities and compatibility with some others addons&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>domlysz</author><guid isPermaLink="false">https://github.com/domlysz/BlenderGIS</guid><pubDate>Tue, 11 Feb 2020 00:02:00 GMT</pubDate></item><item><title>baowenbo/DAIN #3 in Python, Today</title><link>https://github.com/baowenbo/DAIN</link><description>&lt;p&gt;&lt;i&gt;Depth-Aware Video Frame Interpolation (CVPR 2019)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-dain-depth-aware-video-frame-interpolation" class="anchor" aria-hidden="true" href="#dain-depth-aware-video-frame-interpolation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DAIN (Depth-Aware Video Frame Interpolation)&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://sites.google.com/view/wenbobao/dain" rel="nofollow"&gt;Project&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; &lt;a href="http://arxiv.org/abs/1904.00830" rel="nofollow"&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://sites.google.com/view/wenbobao/home" rel="nofollow"&gt;Wenbo Bao&lt;/a&gt;,
&lt;a href="http://graduatestudents.ucmerced.edu/wlai24/" rel="nofollow"&gt;Wei-Sheng Lai&lt;/a&gt;,
&lt;a href="https://sites.google.com/site/chaoma99/" rel="nofollow"&gt;Chao Ma&lt;/a&gt;,
Xiaoyun Zhang,
Zhiyong Gao,
and &lt;a href="http://faculty.ucmerced.edu/mhyang/" rel="nofollow"&gt;Ming-Hsuan Yang&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CVPR 2019&lt;/p&gt;
&lt;p&gt;This work is developed based on our TPAMI work &lt;a href="https://github.com/baowenbo/MEMC-Net"&gt;MEMC-Net&lt;/a&gt;, where we propose the adaptive warping layer. Please also consider referring to it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#requirements-and-dependencies"&gt;Requirements and Dependencies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#testing-pre-trained-models"&gt;Testing Pre-trained Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#downloading-results"&gt;Downloading Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#slow-motion-generation"&gt;Slow-motion Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#training-new-models"&gt;Training New Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h3&gt;
&lt;p&gt;We propose the &lt;strong&gt;D&lt;/strong&gt;epth-&lt;strong&gt;A&lt;/strong&gt;ware video frame &lt;strong&gt;IN&lt;/strong&gt;terpolation (&lt;strong&gt;DAIN&lt;/strong&gt;) model to explicitly detect the occlusion by exploring the depth cue.
We develop a depth-aware flow projection layer to synthesize intermediate flows that preferably sample closer objects than farther ones.
Our method achieves state-of-the-art performance on the Middlebury dataset.
We provide videos &lt;a href="https://www.youtube.com/watch?v=-f8f0igQi5I&amp;amp;t=5s" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/6250fdda565ca8b72bb40fd5abdfeb62e2da394e/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d3159574179414a33543438664d4676324b386a38774956636d516d333963526f66"&gt;&lt;img src="https://camo.githubusercontent.com/6250fdda565ca8b72bb40fd5abdfeb62e2da394e/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d3159574179414a33543438664d4676324b386a38774956636d516d333963526f66" width="200" data-canonical-src="https://drive.google.com/uc?export=view&amp;amp;id=1YWAyAJ3T48fMFv2K8j8wIVcmQm39cRof" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0c626cd20e01cb53df7f8aaf28b6853ba2d58da0/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d314367434c6d56435f575456544163415f496457624c7152384d5331387a486f61"&gt;&lt;img src="https://camo.githubusercontent.com/0c626cd20e01cb53df7f8aaf28b6853ba2d58da0/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d314367434c6d56435f575456544163415f496457624c7152384d5331387a486f61" width="200" data-canonical-src="https://drive.google.com/uc?export=view&amp;amp;id=1CgCLmVC_WTVTAcA_IdWbLqR8MS18zHoa" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/28a8d756ef122a38b057b9fcb2b3f64f8af59ae5/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d31705748747942535a734f5443374e5456644854727631572d6478613935424c6f"&gt;&lt;img src="https://camo.githubusercontent.com/28a8d756ef122a38b057b9fcb2b3f64f8af59ae5/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d31705748747942535a734f5443374e5456644854727631572d6478613935424c6f" width="200" data-canonical-src="https://drive.google.com/uc?export=view&amp;amp;id=1pWHtyBSZsOTC7NTVdHTrv1W-dxa95BLo" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/bd4633d487b2a4f9a596990f651641f7df9ff12c/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d313730766478414e476f4e4b4f355f384d594f756944766f49587a756376374857"&gt;&lt;img src="https://camo.githubusercontent.com/bd4633d487b2a4f9a596990f651641f7df9ff12c/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d313730766478414e476f4e4b4f355f384d594f756944766f49587a756376374857" width="200" data-canonical-src="https://drive.google.com/uc?export=view&amp;amp;id=170vdxANGoNKO5_8MYOuiDvoIXzucv7HW" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0e7e1066b97887c91d4d7ebcb070743af55fdfb0/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d31734a4c776451644c364a595853516f5f4265763061514d6c655761637843734e"&gt;&lt;img src="https://camo.githubusercontent.com/0e7e1066b97887c91d4d7ebcb070743af55fdfb0/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d31734a4c776451644c364a595853516f5f4265763061514d6c655761637843734e" width="200" data-canonical-src="https://drive.google.com/uc?export=view&amp;amp;id=1sJLwdQdL6JYXSQo_Bev0aQMleWacxCsN" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/764633e1b37eeac66cc4c18460f1e64f6c6f40f2/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d316a476a3355644770706f4a4f30324f66385a614e5871444834666e587551384f"&gt;&lt;img src="https://camo.githubusercontent.com/764633e1b37eeac66cc4c18460f1e64f6c6f40f2/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d316a476a3355644770706f4a4f30324f66385a614e5871444834666e587551384f" width="200" data-canonical-src="https://drive.google.com/uc?export=view&amp;amp;id=1jGj3UdGppoJO02Of8ZaNXqDH4fnXuQ8O" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/05155533ceb32decb97dc3e4f48b417b79fb14ed/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d31763537524d6d397835764d33366d43675079356872657358445a577477335673"&gt;&lt;img src="https://camo.githubusercontent.com/05155533ceb32decb97dc3e4f48b417b79fb14ed/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d31763537524d6d397835764d33366d43675079356872657358445a577477335673" width="200" data-canonical-src="https://drive.google.com/uc?export=view&amp;amp;id=1v57RMm9x5vM36mCgPy5hresXDZWtw3Vs" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/50c764e2a7d9376881d0237e1db20e3c3569fba7/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d314c4d77535530507247345f4761446a5752493276396876577059777a524b6361"&gt;&lt;img src="https://camo.githubusercontent.com/50c764e2a7d9376881d0237e1db20e3c3569fba7/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d314c4d77535530507247345f4761446a5752493276396876577059777a524b6361" width="200" data-canonical-src="https://drive.google.com/uc?export=view&amp;amp;id=1LMwSU0PrG4_GaDjWRI2v9hvWpYwzRKca" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/abfa335f54af8f2e5df0d2d5f2fc8f37bf3b2967/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d317069506e456578754861694172345a7a575341784769317531586f5f36765070"&gt;&lt;img src="https://camo.githubusercontent.com/abfa335f54af8f2e5df0d2d5f2fc8f37bf3b2967/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d317069506e456578754861694172345a7a575341784769317531586f5f36765070" width="200" data-canonical-src="https://drive.google.com/uc?export=view&amp;amp;id=1piPnEexuHaiAr4ZzWSAxGi1u1Xo_6vPp" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e5706dbb382ffae897f35558f3a50f798149da36/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d316b6f72625873477053674a6e37544842486b4c5256724a4d74437435595a5042"&gt;&lt;img src="https://camo.githubusercontent.com/e5706dbb382ffae897f35558f3a50f798149da36/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d316b6f72625873477053674a6e37544842486b4c5256724a4d74437435595a5042" width="200" data-canonical-src="https://drive.google.com/uc?export=view&amp;amp;id=1korbXsGpSgJn7THBHkLRVrJMtCt5YZPB" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/56ba4b81bd2b0883cd7e279322f60839ac428a64/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d315f346b566c6876726d4376353461586937765a4d6b332d467452514637733073"&gt;&lt;img src="https://camo.githubusercontent.com/56ba4b81bd2b0883cd7e279322f60839ac428a64/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d315f346b566c6876726d4376353461586937765a4d6b332d467452514637733073" width="200" data-canonical-src="https://drive.google.com/uc?export=view&amp;amp;id=1_4kVlhvrmCv54aXi7vZMk3-FtRQF7s0s" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/73436fed8985b61b2ea7af3607b786c35463b061/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d31346e3778766239686a544b71666372375a70454679664d76783645384e68445f"&gt;&lt;img src="https://camo.githubusercontent.com/73436fed8985b61b2ea7af3607b786c35463b061/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d31346e3778766239686a544b71666372375a70454679664d76783645384e68445f" width="200" data-canonical-src="https://drive.google.com/uc?export=view&amp;amp;id=14n7xvb9hjTKqfcr7ZpEFyfMvx6E8NhD_" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you find the code and datasets useful in your research, please cite:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{DAIN,
    author    = {Bao, Wenbo and Lai, Wei-Sheng and Ma, Chao and Zhang, Xiaoyun and Gao, Zhiyong and Yang, Ming-Hsuan}, 
    title     = {Depth-Aware Video Frame Interpolation}, 
    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
    year      = {2019}
}
@article{MEMC-Net,
     title={MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement},
     author={Bao, Wenbo and Lai, Wei-Sheng, and Zhang, Xiaoyun and Gao, Zhiyong and Yang, Ming-Hsuan},
     journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
     doi={10.1109/TPAMI.2019.2941941},
     year={2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-requirements-and-dependencies" class="anchor" aria-hidden="true" href="#requirements-and-dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements and Dependencies&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ubuntu (We test with Ubuntu = 16.04.5 LTS)&lt;/li&gt;
&lt;li&gt;Python (We test with Python = 3.6.8 in Anaconda3 = 4.1.1)&lt;/li&gt;
&lt;li&gt;Cuda &amp;amp; Cudnn (We test with Cuda = 9.0 and Cudnn = 7.0)&lt;/li&gt;
&lt;li&gt;PyTorch (The customized depth-aware flow projection and other layers require ATen API in PyTorch = 1.0.0)&lt;/li&gt;
&lt;li&gt;GCC (Compiling PyTorch 1.0.0 extension files (.c/.cu) requires gcc = 4.9.1 and nvcc = 9.0 compilers)&lt;/li&gt;
&lt;li&gt;NVIDIA GPU (We use Titan X (Pascal) with compute = 6.1, but we support compute_50/52/60/61 devices, should you have devices with higher compute capability, please revise &lt;a href="https://github.com/baowenbo/DAIN/blob/master/my_package/DepthFlowProjection/setup.py"&gt;this&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;p&gt;Download repository:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/baowenbo/DAIN.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before building Pytorch extensions, be sure you have &lt;code&gt;pytorch &amp;gt;= 1.0.0&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python -c "import torch; print(torch.__version__)"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Generate our PyTorch extensions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd DAIN
$ cd my_package 
$ ./build.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Generate the Correlation package required by &lt;a href="https://github.com/NVlabs/PWC-Net/tree/master/PyTorch/external_packages/correlation-pytorch-master"&gt;PWCNet&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd ../PWCNet/correlation_package_pytorch1_0
$ ./build.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-testing-pre-trained-models" class="anchor" aria-hidden="true" href="#testing-pre-trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing Pre-trained Models&lt;/h3&gt;
&lt;p&gt;Make model weights dir and Middlebury dataset dir:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd DAIN
$ mkdir model_weights
$ mkdir MiddleBurySet
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Download pretrained models,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd model_weights
$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/best.pth
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and Middlebury dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd ../MiddleBurySet
$ wget http://vision.middlebury.edu/flow/data/comp/zip/other-color-allframes.zip
$ unzip other-color-allframes.zip
$ wget http://vision.middlebury.edu/flow/data/comp/zip/other-gt-interp.zip
$ unzip other-gt-interp.zip
$ cd ..
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are good to go by:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The interpolated results are under &lt;code&gt;MiddleBurySet/other-result-author/[random number]/&lt;/code&gt;, where the &lt;code&gt;random number&lt;/code&gt; is used to distinguish different runnings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-downloading-results" class="anchor" aria-hidden="true" href="#downloading-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading Results&lt;/h3&gt;
&lt;p&gt;Our DAIN model achieves the state-of-the-art performance on the UCF101, Vimeo90K, and Middlebury (&lt;a href="http://vision.middlebury.edu/flow/eval/results/results-n1.php" rel="nofollow"&gt;&lt;em&gt;eval&lt;/em&gt;&lt;/a&gt; and &lt;em&gt;other&lt;/em&gt;).
Dowload our interpolated results with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/UCF101_DAIN.zip
$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/Vimeo90K_interp_DAIN.zip
$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/Middlebury_eval_DAIN.zip
$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/Middlebury_other_DAIN.zip
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-slow-motion-generation" class="anchor" aria-hidden="true" href="#slow-motion-generation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Slow-motion Generation&lt;/h3&gt;
&lt;p&gt;Our model is fully capable of generating slow-motion effect with minor modification on the network architecture.
Run the following code by specifying &lt;code&gt;time_step = 0.25&lt;/code&gt; to generate x4 slow-motion effect:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.25
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or set &lt;code&gt;time_step&lt;/code&gt; to &lt;code&gt;0.125&lt;/code&gt; or &lt;code&gt;0.1&lt;/code&gt; as follows&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.125
$ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to generate x8 and x10 slow-motion respectively. Or if you would like to have x100 slow-motion for a little fun.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.01
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may also want to create gif animations by:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd MiddleBurySet/other-result-author/[random number]/Beanbags
$ convert -delay 1 *.png -loop 0 Beanbags.gif //1*10ms delay 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Have fun and enjoy yourself!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-training-new-models" class="anchor" aria-hidden="true" href="#training-new-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training New Models&lt;/h3&gt;
&lt;p&gt;Download the Vimeo90K triplet dataset for video frame interpolation task, also see &lt;a href="https://github.com/anchen1011/toflow/blob/master/download_dataset.sh"&gt;here&lt;/a&gt; by &lt;a href="https://arxiv.org/abs/1711.09078" rel="nofollow"&gt;Xue et al., IJCV19&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd DAIN
$ mkdir /path/to/your/dataset &amp;amp; cd /path/to/your/dataset 
$ wget http://data.csail.mit.edu/tofu/dataset/vimeo_triplet.zip
$ unzip vimeo_triplet.zip
$ rm vimeo_triplet.zip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Download the pretrained MegaDepth and PWCNet models&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd MegaDepth/checkpoints/test_local
$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/best_generalization_net_G.pth
$ cd ../../../PWCNet
$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/pwc_net.pth.tar
$ cd  ..
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the training script:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ CUDA_VISIBLE_DEVICES=0 python train.py --datasetPath /path/to/your/dataset --batch_size 1 --save_which 1 --lr 0.0001 --rectify_lr 0.0001 --flow_lr_coe 0.01 --occ_lr_coe 0.0 --filter_lr_coe 1.0 --ctx_lr_coe 1.0 --alpha 0.0 1.0 --patience 4 --factor 0.2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The optimized models will be saved to the &lt;code&gt;model_weights/[random number]&lt;/code&gt; directory, where [random number] is generated for different runs.&lt;/p&gt;
&lt;p&gt;Replace the pre-trained &lt;code&gt;model_weights/best.pth&lt;/code&gt; model with the newly trained &lt;code&gt;model_weights/[random number]/best.pth&lt;/code&gt; model.
Then test the new model by executing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h3&gt;
&lt;p&gt;&lt;a href="mailto:bwb0813@gmail.com"&gt;Wenbo Bao&lt;/a&gt;; &lt;a href="mailto:phoenix104104@gmail.com"&gt;Wei-Sheng (Jason) Lai&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;See &lt;a href="https://github.com/baowenbo/DAIN/blob/master/LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>baowenbo</author><guid isPermaLink="false">https://github.com/baowenbo/DAIN</guid><pubDate>Tue, 11 Feb 2020 00:03:00 GMT</pubDate></item><item><title>Rlacat/jd-automask #4 in Python, Today</title><link>https://github.com/Rlacat/jd-automask</link><description>&lt;p&gt;&lt;i&gt;防护-京东口罩自动抢购并下单&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;hr&gt;
&lt;p&gt;如果帮助到你，star一下，谢谢你
武汉加油，中国加油
使用上有问题的加一下这个群958535145（个人时间有限，回答不过来了）
目前v1出现cookie经常失效 被拒绝访问问题 正在修复&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;本代码使用方式 &lt;a href="https://blog.csdn.net/cyz52/article/details/104177981" rel="nofollow"&gt;https://blog.csdn.net/cyz52/article/details/104177981&lt;/a&gt; 或者下载后阅读README.md
100M电信网络实测1-2s刷新100个商品&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-版本" class="anchor" aria-hidden="true" href="#版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;版本&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; python3 V1&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 基于&lt;a href="https://github.com/cycz/jdBuyMask%5Dgithub"&gt;https://github.com/cycz/jdBuyMask]github&lt;/a&gt; V3版本制作&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 已编译版本链接：链接: &lt;a href="https://pan.baidu.com/s/1FqZP39ow_CrsAn0DfeRJ2w" rel="nofollow"&gt;https://pan.baidu.com/s/1FqZP39ow_CrsAn0DfeRJ2w&lt;/a&gt; 提取码: 97wb&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-使用方法" class="anchor" aria-hidden="true" href="#使用方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;使用方法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1.修改（config.ini）：地区id、推送方式（微信、邮箱二选一）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2.打开exe运行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;商品skuid修改方法（config.ini）：
1.谷歌（内核）浏览器要监控的商品url
2.按F12 ，点开Nework
3.点击需要的商品 和所在的地区
4.ctrl+f搜索 stock并点击
5.复制skuid
6.修改或者添加在config.ini内的skuids&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;地区id修改方法（config.ini）：
1.用谷歌（内核）浏览器随意打开一个京东的有货商品网页（例子：&lt;a href="https://item.jd.com/100004770235.html%EF%BC%89" rel="nofollow"&gt;https://item.jd.com/100004770235.html）&lt;/a&gt;
2.右键你的收货地址并点击审查元素
3.双击并复制那串数字（xx-xx-xxxxx）
4.修改在config.ini内的area(area = xx-xx-xxxxx)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-运行图片" class="anchor" aria-hidden="true" href="#运行图片"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;运行图片&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-功能" class="anchor" aria-hidden="true" href="#功能"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;功能&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 确认是否有货&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 有货自动下单&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 邮件、微信（需要申请方糖api）通知&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 扫码登陆&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 无限个商品支持&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 多线程极速刷新网页&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-更新记录" class="anchor" aria-hidden="true" href="#更新记录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新记录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;【2020.02.08】修复了一些bug&lt;/li&gt;
&lt;li&gt;【2020.02.08】大幅优化刷新速度，增加多线程技术（可在配置调节线程数）&lt;/li&gt;
&lt;li&gt;【2020.02.08】新增微信通知（&lt;a href="http://sc.ftqq.com/3.version" rel="nofollow"&gt;http://sc.ftqq.com/3.version&lt;/a&gt; 查看sc_key）&lt;/li&gt;
&lt;li&gt;【2020.02.08】jd-automask_V1版本上线&lt;/li&gt;
&lt;li&gt;【2020.02.07】增加扫码登陆，自动保存cookie&lt;/li&gt;
&lt;li&gt;【2020.02.07】V4版本，解决商品个数限制&lt;/li&gt;
&lt;li&gt;Code By Rlacat&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;【2020.02.06】V2版本，刷新更快更频繁，通过配置文件添加商品和地区id&lt;/li&gt;
&lt;li&gt;【2020.02.06】提交失败之后会继续不会暂停&lt;/li&gt;
&lt;li&gt;【2020.02.06】购物车有套装商品导致解析skuid错误&lt;/li&gt;
&lt;li&gt;【2020.02.05】商品有货，但是该商品已下柜，提交会报错，对部分代码进行了优化&lt;/li&gt;
&lt;li&gt;Code By cycz&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-反馈问题" class="anchor" aria-hidden="true" href="#反馈问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;反馈问题&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果有红包先花掉再开脚本，不然可能需要支付密码&lt;/li&gt;
&lt;li&gt;其他问题Github提issues，或者私信我！！&lt;/li&gt;
&lt;li&gt;如果闪退，请打开目录jdBuyMask.txt文件查看帮助说明&lt;/li&gt;
&lt;li&gt;CMD界面卡住、关闭CMD的快速编辑模式就行了&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Rlacat</author><guid isPermaLink="false">https://github.com/Rlacat/jd-automask</guid><pubDate>Tue, 11 Feb 2020 00:04:00 GMT</pubDate></item><item><title>programthink/zhao #5 in Python, Today</title><link>https://github.com/programthink/zhao</link><description>&lt;p&gt;&lt;i&gt;【编程随想】整理的《太子党关系网络》，专门揭露赵国的权贵&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="wiki" data-path="README.wiki"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;
&lt;p&gt;&lt;/p&gt;&lt;table id="user-content-toc" summary="Contents"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;div id="user-content-toctitle"&gt;&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="#"&gt;俺整理的《太子党关系网络》&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="#-2"&gt;简介&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-3"&gt;下载说明&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-4"&gt;多人协作说明&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-5"&gt;数据格式说明&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-6"&gt;目录说明&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="#data_"&gt;data 目录&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#bin_"&gt;bin 目录&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#download_"&gt;download 目录&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-7"&gt;编译脚本使用说明&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="#-8"&gt;脚本的命令行参数&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-9"&gt;依赖的软件&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-10"&gt;致“反对此项目的墙内程序员”&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;/p&gt;&lt;h1&gt;&lt;a id="user-content-俺整理的太子党关系网络" class="anchor" aria-hidden="true" href="#俺整理的太子党关系网络"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name=""&gt;&lt;/a&gt;&lt;span id=""&gt;俺整理的《太子党关系网络》&lt;/span&gt;&lt;/h1&gt;




&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--2"&gt;&lt;/a&gt;&lt;span id="user-content--2"&gt;简介&lt;/span&gt;&lt;/h2&gt;



&lt;p&gt;此项目创建于2016年2月，专门用来揭露天朝的权贵（也就是传说中的“赵家人”）。
&lt;/p&gt;
&lt;p&gt;俺把这几年收集整理的数据开源到 GitHub，便于多人协作——大伙儿群策群力，一起来曝光权贵家族。
&lt;/p&gt;
&lt;p&gt;初次上传的数据包括：700多个数据文件（ &lt;b&gt;对应700多人，130多个家族&lt;/b&gt; ），另有200多张图片（人物头像）。随着俺不断完善，数据会越来越多。
&lt;/p&gt;
&lt;p&gt;对这个项目，俺会【持续更新】。比如朝廷每次换届的时候，俺都会补充新的素材。
&lt;/p&gt;
&lt;p&gt;为了确保数据的可信度，俺主要参考“维基百科”以及一些国际权威媒体的报道（比如《纽约时报》、《华尔街日版》、《金融时报》等等）。
&lt;/p&gt;
&lt;p&gt;另外，对于某些客观事实（比如：生卒年月、简历、亲戚关系），俺也参考了天朝政府的官方网站，以及墙内的“百度百科”。
&lt;/p&gt;


&lt;h2&gt;&lt;a id="user-content-下载说明" class="anchor" aria-hidden="true" href="#下载说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--3"&gt;&lt;/a&gt;&lt;span id="user-content--3"&gt;下载说明&lt;/span&gt;&lt;/h2&gt;



&lt;p&gt;GitHub 提供了“下载整个项目”的功能，但是会比较大。
&lt;/p&gt;
&lt;p&gt;如果你仅仅想看《太子党关系网络》这份文档，只需在首页上方点击进入 &lt;b&gt;download&lt;/b&gt; 这个目录。
&lt;/p&gt;
&lt;p&gt;该目录下有 &lt;b&gt;pdf&lt;/b&gt; 和 &lt;b&gt;jpg&lt;/b&gt; 两个子目录，分别存放对应的 &lt;b&gt;【文件类型】&lt;/b&gt; 。你想要看哪一种文件格式，就进入哪个子目录里面。
&lt;/p&gt;
&lt;p&gt;进入【文件类型】的子目录之后，会看到一个文件列表（目前有13个文件）。先点击你想要的某个文件，会进入该文件的页面。
&lt;/p&gt;
&lt;p&gt;然后在【右上方】你会看到一个 &lt;b&gt;Raw 按钮&lt;/b&gt; ，在这个按钮上点【右键】，在【右键菜单】里面选“保存”或“另存为”，就可以把这个文件下载到你本机。
&lt;/p&gt;


&lt;h2&gt;&lt;a id="user-content-多人协作说明" class="anchor" aria-hidden="true" href="#多人协作说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--4"&gt;&lt;/a&gt;&lt;span id="user-content--4"&gt;多人协作说明&lt;/span&gt;&lt;/h2&gt;



&lt;p&gt;俺非常希望有更多的网友参与该项目，大伙儿一起来完善天朝权贵家族的资料。
&lt;/p&gt;
&lt;p&gt;想要参与的同学，可以通过如下方式：
&lt;/p&gt;






&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;到&lt;a href="https://program-think.blogspot.com/" rel="nofollow"&gt;俺博客&lt;/a&gt;留言进行反馈，补充信息或反馈错误。&lt;/li&gt;&lt;li&gt;在&lt;a href="https://github.com/programthink/zhao/issues"&gt;本项目发一个 issue&lt;/a&gt;，补充信息或反馈错误。&lt;/li&gt;&lt;li&gt;Fork 该项目，进行修改，然后向俺发一个 Pull Request&lt;/li&gt;&lt;/ul&gt;
（后面两种方式，你需要有 GitHub 的帐号）
&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;&lt;a id="user-content-数据格式说明" class="anchor" aria-hidden="true" href="#数据格式说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--5"&gt;&lt;/a&gt;&lt;span id="user-content--5"&gt;数据格式说明&lt;/span&gt;&lt;/h2&gt;



&lt;p&gt;本项目的数据文件，全部采用&lt;a href="https://zh.wikipedia.org/wiki/YAML" rel="nofollow"&gt;YAML 格式&lt;/a&gt;。这种格式非常简洁明了，有利于完全不懂技术的网友参与编辑。
&lt;/p&gt;
&lt;p&gt;而且俺在每一个 YAML 格式的文件中都写了详细的注释，便于其他网友修改。
&lt;/p&gt;


&lt;h2&gt;&lt;a id="user-content-目录说明" class="anchor" aria-hidden="true" href="#目录说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--6"&gt;&lt;/a&gt;&lt;span id="user-content--6"&gt;目录说明&lt;/span&gt;&lt;/h2&gt;




&lt;h3&gt;&lt;a id="user-content-data-目录" class="anchor" aria-hidden="true" href="#data-目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-data_"&gt;&lt;/a&gt;&lt;span id="user-content-data_"&gt;data 目录&lt;/span&gt;&lt;/h3&gt;



&lt;p&gt;data 目录用来保存数据文件，该目录下另有如下三个子目录：
&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;person&lt;/li&gt;&lt;/ul&gt;
这个目录存放个人的资料，每个人一个目录，目录名就是人名。对于偶尔有同名的情况，在目录名末尾追加数字序号来区分。
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;每个目录下都有一个 brief.yaml 文件，包含此人的简介。
&lt;/p&gt;
&lt;p&gt;有些目录下还有一个 portrait.png 文件，对应此人的头像。
&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;company&lt;/li&gt;&lt;/ul&gt;
这个目录存放与太子党有关的公司或组织机构。目录结构与 person 类似。
&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;family&lt;/li&gt;&lt;/ul&gt;
这个目录存放家族关系的信息。每个家族是一个 yaml 格式的文件。
&lt;p&gt;&lt;/p&gt;

&lt;h3&gt;&lt;a id="user-content-bin-目录" class="anchor" aria-hidden="true" href="#bin-目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-bin_"&gt;&lt;/a&gt;&lt;span id="user-content-bin_"&gt;bin 目录&lt;/span&gt;&lt;/h3&gt;



&lt;p&gt;该目录存放编译脚本。该脚本的使用参见下面的章节。
&lt;/p&gt;

&lt;h3&gt;&lt;a id="user-content-download-目录" class="anchor" aria-hidden="true" href="#download-目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-download_"&gt;&lt;/a&gt;&lt;span id="user-content-download_"&gt;download 目录&lt;/span&gt;&lt;/h3&gt;



&lt;p&gt;该目录存放制作好的文件，目前先提供 jpg 和 pdf 两种格式。
&lt;/p&gt;
&lt;p&gt;如果你需要其它格式，可以用 bin 目录下的编译脚本自行搞定（编译脚本的使用，参见下面的章节）。
&lt;/p&gt;


&lt;h2&gt;&lt;a id="user-content-编译脚本使用说明" class="anchor" aria-hidden="true" href="#编译脚本使用说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--7"&gt;&lt;/a&gt;&lt;span id="user-content--7"&gt;编译脚本使用说明&lt;/span&gt;&lt;/h2&gt;



&lt;p&gt;（俺是在 Linux 上编写该脚本，尚未在 Windows 上进行测试）
&lt;/p&gt;
&lt;p&gt;如果你在 Windows 上使用碰到问题，可以到&lt;a href="https://program-think.blogspot.com/" rel="nofollow"&gt;俺博客&lt;/a&gt;留言进行反馈。也可以在&lt;a href="https://github.com/programthink/zhao/issues"&gt;本项目发一个 issue&lt;/a&gt;。
&lt;/p&gt;

&lt;h3&gt;&lt;a id="user-content-脚本的命令行参数" class="anchor" aria-hidden="true" href="#脚本的命令行参数"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--8"&gt;&lt;/a&gt;&lt;span id="user-content--8"&gt;脚本的命令行参数&lt;/span&gt;&lt;/h3&gt;



&lt;p&gt;俺使用 python 作为编译脚本，该脚本位于 bin 目录下。
&lt;/p&gt;
&lt;p&gt;通过该脚本可以把原始数据生成为 dot 语言的脚本。然后再调用 Graphviz 把 dot 脚本生成各种格式（比如：pdf、jpeg）。
&lt;/p&gt;
&lt;p&gt;要使用该脚本，先在命令行模式下进入 bin 目录，然后运行如下命令：
&lt;/p&gt;
&lt;p&gt;（生成 pdf 格式的示例）
&lt;/p&gt;
&lt;p&gt;&lt;b&gt;python make.py pdf&lt;/b&gt;
&lt;/p&gt;
&lt;p&gt;（生成 jpg 格式的示例）
&lt;/p&gt;
&lt;p&gt;&lt;b&gt;python make.py jpg&lt;/b&gt;
&lt;/p&gt;

&lt;h3&gt;&lt;a id="user-content-依赖的软件" class="anchor" aria-hidden="true" href="#依赖的软件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--9"&gt;&lt;/a&gt;&lt;span id="user-content--9"&gt;依赖的软件&lt;/span&gt;&lt;/h3&gt;



&lt;p&gt;要使用上述脚本，你需要事先安装相关的软件（如下）
&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Python（2 或 3）&lt;/li&gt;&lt;/ul&gt;
因为俺用的是 Python 脚本，所以你需要先安装 python 软件。
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;目前 Python 有两种大版本——python2 和 python3——俺的编译脚本 &lt;b&gt;【同时兼容】&lt;/b&gt; 这两种 Python 的大版本。
&lt;/p&gt;
&lt;p&gt;对于 Python 的小版本，俺本人在 &lt;b&gt;2.7&lt;/b&gt; 和 &lt;b&gt;3.5&lt;/b&gt; 上测试通过。2.6 和 3.4 估计也可以。
&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;PyYAML&lt;/li&gt;&lt;/ul&gt;
这是一个基于 python 开发的软件包，专门用来处理 YAML 格式的文件。
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;你需要在你的 python 环境中安装该软件包。其官方链接如下：
&lt;/p&gt;
&lt;p&gt;&lt;a href="http://pyyaml.org/wiki/PyYAML" rel="nofollow"&gt;PyYAML 的官网的 wiki&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pypi.python.org/pypi/PyYAML" rel="nofollow"&gt;Python 官网的 PYPI&lt;/a&gt;
&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Graphviz&lt;/li&gt;&lt;/ul&gt;
这个软件是用来生成【关系图】的。关于该这个软件，俺已经写了一篇扫盲教程：
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;《&lt;a href="https://program-think.blogspot.com/2016/02/opensource-review-graphviz.html" rel="nofollow"&gt;开源项目：【自动】绘图工具Graphviz——《太子党关系网络》就是用它制作&lt;/a&gt;》
&lt;/p&gt;


&lt;h2&gt;&lt;a id="user-content-致反对此项目的墙内程序员" class="anchor" aria-hidden="true" href="#致反对此项目的墙内程序员"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--10"&gt;&lt;/a&gt;&lt;span id="user-content--10"&gt;致“反对此项目的墙内程序员”&lt;/span&gt;&lt;/h2&gt;



&lt;p&gt;本项目上线第二天，就收获 363 个 star 兼 88 个 fork，甚至还挤进 GitHub 的“当日 Trending”——俺很荣幸，也很高兴有这么多人给俺捧场。
&lt;/p&gt;
&lt;p&gt;但是在&lt;a href="https://github.com/programthink/zhao/issues"&gt;本项目的 issue 列表&lt;/a&gt;中也看到好几个反对此项目的程序员（应该都来自墙内），他们担心这个项目导致 GitHub 被 GFW 封杀。
&lt;/p&gt;
&lt;p&gt;这几年来，类似的言论俺已经看了不少。就好比强盗拿刀杀人，围观者不但没有谴责强盗，反而去谴责卖刀的店家——这就是传说中的“斯德哥尔摩综合症”。
&lt;/p&gt;
&lt;p&gt;有兴趣的同学，可以看俺之前的博文——《&lt;a href="https://program-think.blogspot.com/2012/06/stockholm-syndrome.html" rel="nofollow"&gt;天朝民众的心理分析：斯德哥尔摩综合症&lt;/a&gt;》&lt;/p&gt;&lt;/article&gt;&lt;/div&gt;</description><author>programthink</author><guid isPermaLink="false">https://github.com/programthink/zhao</guid><pubDate>Tue, 11 Feb 2020 00:05:00 GMT</pubDate></item><item><title>xinntao/ESRGAN #6 in Python, Today</title><link>https://github.com/xinntao/ESRGAN</link><description>&lt;p&gt;&lt;i&gt;ECCV18 Workshops - Enhanced SRGAN. Champion PIRM Challenge on Perceptual Super-Resolution (Third Region) &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-esrgan-enhanced-srgan-basicsr-edvr-dni" class="anchor" aria-hidden="true" href="#esrgan-enhanced-srgan-basicsr-edvr-dni"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ESRGAN (Enhanced SRGAN) &lt;a href="https://github.com/xinntao/BasicSR"&gt;[BasicSR]&lt;/a&gt; &lt;a href="https://github.com/xinntao/EDVR"&gt;[EDVR]&lt;/a&gt; &lt;a href="https://xinntao.github.io/projects/DNI" rel="nofollow"&gt;[DNI]&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-we-have-merged-the-training-codes-of-esrgan-into-mmsr-smile" class="anchor" aria-hidden="true" href="#we-have-merged-the-training-codes-of-esrgan-into-mmsr-smile"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;We have merged the training codes of ESRGAN into &lt;a href="https://github.com/open-mmlab/mmsr"&gt;MMSR&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="smile" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png"&gt;😄&lt;/g-emoji&gt;&lt;/h2&gt;
&lt;p&gt;MMSR is an open source image and video super-resolution toolbox based on PyTorch. It is a part of the &lt;a href="https://github.com/open-mmlab"&gt;open-mmlab&lt;/a&gt; project developed by &lt;a href="http://mmlab.ie.cuhk.edu.hk/" rel="nofollow"&gt;Multimedia Laboratory, CUHK&lt;/a&gt;. MMSR is based on our previous projects: &lt;a href="https://github.com/xinntao/BasicSR"&gt;BasicSR&lt;/a&gt;, &lt;a href="https://github.com/xinntao/ESRGAN"&gt;ESRGAN&lt;/a&gt;, and &lt;a href="https://github.com/xinntao/EDVR"&gt;EDVR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We have simplified the network structure file.&lt;br&gt;
You can convert the previously save models (&lt;code&gt;*.pth&lt;/code&gt;) with the script &lt;code&gt;transer_RRDB_models.py&lt;/code&gt;;&lt;br&gt;
If you want to use the old arch, you can find it &lt;a href="https://github.com/xinntao/ESRGAN/releases/tag/old-arch"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Check out our new work on:&lt;br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Video Super-Resolution&lt;/strong&gt;: &lt;a href="https://xinntao.github.io/projects/EDVR" rel="nofollow"&gt;&lt;code&gt;EDVR: Video Restoration with Enhanced Deformable Convolutional Networks&lt;/code&gt;&lt;/a&gt;, which has won all four tracks in NTIRE 2019 Challenges on Video Restoration and Enhancement (CVPR19 Workshops).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DNI (CVPR19)&lt;/strong&gt;: &lt;a href="https://xinntao.github.io/projects/DNI" rel="nofollow"&gt;&lt;code&gt;Deep Network Interpolation for Continuous Imagery Effect Transition&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-enhanced-super-resolution-generative-adversarial-networks" class="anchor" aria-hidden="true" href="#enhanced-super-resolution-generative-adversarial-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Enhanced Super-Resolution Generative Adversarial Networks&lt;/h3&gt;
&lt;p&gt;By Xintao Wang, &lt;a href="https://yuke93.github.io/" rel="nofollow"&gt;Ke Yu&lt;/a&gt;, Shixiang Wu, &lt;a href="http://www.jasongt.com/" rel="nofollow"&gt;Jinjin Gu&lt;/a&gt;, Yihao Liu, &lt;a href="https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ&amp;amp;hl=en" rel="nofollow"&gt;Chao Dong&lt;/a&gt;, &lt;a href="http://mmlab.siat.ac.cn/yuqiao/" rel="nofollow"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://personal.ie.cuhk.edu.hk/~ccloy/" rel="nofollow"&gt;Chen Change Loy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repo only provides simple testing codes, pretrained models and the network strategy demo.  For full training and testing codes, please refer to  &lt;a href="https://github.com/xinntao/BasicSR"&gt;BasicSR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We won the first place in &lt;a href="https://www.pirm2018.org/PIRM-SR.html" rel="nofollow"&gt;PIRM2018-SR competition&lt;/a&gt; (region 3) and got the best perceptual index.
The paper is accepted to &lt;a href="https://pirm2018.org/" rel="nofollow"&gt;ECCV2018 PIRM Workshop&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="triangular_flag_on_post" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a9.png"&gt;🚩&lt;/g-emoji&gt; Add &lt;a href="https://github.com/xinntao/ESRGAN/blob/master/QA.md"&gt;Frequently Asked Questions&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For instance,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How to reproduce your results in the PIRM18-SR Challenge (with low perceptual index)?&lt;/li&gt;
&lt;li&gt;How do you get the perceptual index in your ESRGAN paper?&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-bibtex" class="anchor" aria-hidden="true" href="#bibtex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BibTeX&lt;/h4&gt;
    
&lt;pre&gt;&lt;code&gt;@InProceedings{wang2018esrgan,
    author = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Loy, Chen Change},
    title = {ESRGAN: Enhanced super-resolution generative adversarial networks},
    booktitle = {The European Conference on Computer Vision Workshops (ECCVW)},
    month = {September},
    year = {2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/baboon.jpg"&gt;&lt;img src="figures/baboon.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;RRDB_PSNR&lt;/strong&gt; PSNR_oriented model trained with DF2K dataset (a merged dataset with &lt;a href="https://data.vision.ee.ethz.ch/cvl/DIV2K/" rel="nofollow"&gt;DIV2K&lt;/a&gt; and &lt;a href="http://cv.snu.ac.kr/research/EDSR/Flickr2K.tar" rel="nofollow"&gt;Flickr2K&lt;/a&gt; (proposed in &lt;a href="https://github.com/LimBee/NTIRE2017"&gt;EDSR&lt;/a&gt;)) is also able to achive high PSNR performance.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;sub&gt;Method&lt;/sub&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;sub&gt;Training dataset&lt;/sub&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;sub&gt;Set5&lt;/sub&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;sub&gt;Set14&lt;/sub&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;sub&gt;BSD100&lt;/sub&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;sub&gt;Urban100&lt;/sub&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;sub&gt;Manga109&lt;/sub&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;sub&gt;&lt;a href="http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html" rel="nofollow"&gt;SRCNN&lt;/a&gt;&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;291&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;30.48/0.8628&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;27.50/0.7513&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;26.90/0.7101&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;24.52/0.7221&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;27.58/0.8555&lt;/sub&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;sub&gt;&lt;a href="https://github.com/thstkdgus35/EDSR-PyTorch"&gt;EDSR&lt;/a&gt;&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;DIV2K&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;32.46/0.8968&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;28.80/0.7876&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;27.71/0.7420&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;26.64/0.8033&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;31.02/0.9148&lt;/sub&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;sub&gt;&lt;a href="https://github.com/yulunzhang/RCAN"&gt;RCAN&lt;/a&gt;&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;DIV2K&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;32.63/0.9002&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;28.87/0.7889&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;27.77/0.7436&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;26.82/ 0.8087&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;31.22/ 0.9173&lt;/sub&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;sub&gt;RRDB(ours)&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;DF2K&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;&lt;strong&gt;32.73/0.9011&lt;/strong&gt;&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;&lt;strong&gt;28.99/0.7917&lt;/strong&gt;&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;&lt;strong&gt;27.85/0.7455&lt;/strong&gt;&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;&lt;strong&gt;27.03/0.8153&lt;/strong&gt;&lt;/sub&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;sub&gt;&lt;strong&gt;31.66/0.9196&lt;/strong&gt;&lt;/sub&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-quick-test" class="anchor" aria-hidden="true" href="#quick-test"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Test&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Python 3&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/" rel="nofollow"&gt;PyTorch &amp;gt;= 1.0&lt;/a&gt; (CUDA version &amp;gt;= 7.5 if installing with CUDA. &lt;a href="https://pytorch.org/get-started/previous-versions/" rel="nofollow"&gt;More details&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Python packages:  &lt;code&gt;pip install numpy opencv-python&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-test-models" class="anchor" aria-hidden="true" href="#test-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Test models&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Clone this github repo.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/xinntao/ESRGAN
cd ESRGAN
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Place your own &lt;strong&gt;low-resolution images&lt;/strong&gt; in &lt;code&gt;./LR&lt;/code&gt; folder. (There are two sample images - baboon and comic).&lt;/li&gt;
&lt;li&gt;Download pretrained models from &lt;a href="https://drive.google.com/drive/u/0/folders/17VYV_SoZZesU6mbxz2dMAIccSSlqLecY" rel="nofollow"&gt;Google Drive&lt;/a&gt; or &lt;a href="https://pan.baidu.com/s/1-Lh6ma-wXzfH8NqeBtPaFQ" rel="nofollow"&gt;Baidu Drive&lt;/a&gt;. Place the models in &lt;code&gt;./models&lt;/code&gt;. We provide two models with high perceptual quality and high PSNR performance (see &lt;a href="https://github.com/xinntao/ESRGAN/tree/master/models"&gt;model list&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Run test. We provide ESRGAN model and RRDB_PSNR model and you can config in the &lt;code&gt;test.py&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;python test.py
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="5"&gt;
&lt;li&gt;The results are in &lt;code&gt;./results&lt;/code&gt; folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-network-interpolation-demo" class="anchor" aria-hidden="true" href="#network-interpolation-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network interpolation demo&lt;/h3&gt;
&lt;p&gt;You can interpolate the RRDB_ESRGAN and RRDB_PSNR models with alpha in [0, 1].&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run &lt;code&gt;python net_interp.py 0.8&lt;/code&gt;, where &lt;em&gt;0.8&lt;/em&gt; is the interpolation parameter and you can change it to any value in [0,1].&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;python test.py models/interp_08.pth&lt;/code&gt;, where &lt;em&gt;models/interp_08.pth&lt;/em&gt; is the model path.&lt;/li&gt;
&lt;/ol&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/43074.gif"&gt;&lt;img height="400" src="figures/43074.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-perceptual-driven-sr-results" class="anchor" aria-hidden="true" href="#perceptual-driven-sr-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Perceptual-driven SR Results&lt;/h2&gt;
&lt;p&gt;You can download all the resutls from &lt;a href="https://drive.google.com/drive/folders/1iaM-c6EgT1FNoJAOKmDrK7YhEhtlKcLx?usp=sharing" rel="nofollow"&gt;Google Drive&lt;/a&gt;. (&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt; included;  &lt;g-emoji class="g-emoji" alias="heavy_minus_sign" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2796.png"&gt;➖&lt;/g-emoji&gt; not included; &lt;g-emoji class="g-emoji" alias="o" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b55.png"&gt;⭕️&lt;/g-emoji&gt; TODO)&lt;/p&gt;
&lt;p&gt;HR images can be downloaed from &lt;a href="https://github.com/xinntao/BasicSR#datasets"&gt;BasicSR-Datasets&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Datasets&lt;/th&gt;
&lt;th align="center"&gt;LR&lt;/th&gt;
&lt;th align="center"&gt;&lt;a href="https://arxiv.org/abs/1809.00219" rel="nofollow"&gt;&lt;em&gt;ESRGAN&lt;/em&gt;&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a href="https://arxiv.org/abs/1609.04802" rel="nofollow"&gt;SRGAN&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper.pdf" rel="nofollow"&gt;EnhanceNet&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a href="https://arxiv.org/abs/1803.04626" rel="nofollow"&gt;CX&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Set5&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="o" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b55.png"&gt;⭕️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Set14&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="o" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b55.png"&gt;⭕️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;BSDS100&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="o" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b55.png"&gt;⭕️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://pirm.github.io/" rel="nofollow"&gt;PIRM&lt;/a&gt; &lt;br&gt;&lt;sup&gt;(val, test)&lt;/sup&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_minus_sign" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2796.png"&gt;➖&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://arxiv.org/pdf/1804.02815.pdf" rel="nofollow"&gt;OST300&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_minus_sign" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2796.png"&gt;➖&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="o" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b55.png"&gt;⭕️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;urban100&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_minus_sign" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2796.png"&gt;➖&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="o" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b55.png"&gt;⭕️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://data.vision.ee.ethz.ch/cvl/DIV2K/" rel="nofollow"&gt;DIV2K&lt;/a&gt; &lt;br&gt;&lt;sup&gt;(val, test)&lt;/sup&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_minus_sign" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2796.png"&gt;➖&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="heavy_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png"&gt;✔️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="o" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b55.png"&gt;⭕️&lt;/g-emoji&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-esrgan" class="anchor" aria-hidden="true" href="#esrgan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ESRGAN&lt;/h2&gt;
&lt;p&gt;We improve the &lt;a href="https://arxiv.org/abs/1609.04802" rel="nofollow"&gt;SRGAN&lt;/a&gt; from three aspects:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;adopt a deeper model using Residual-in-Residual Dense Block (RRDB) without batch normalization layers.&lt;/li&gt;
&lt;li&gt;employ &lt;a href="https://ajolicoeur.wordpress.com/relativisticgan/" rel="nofollow"&gt;Relativistic average GAN&lt;/a&gt; instead of the vanilla GAN.&lt;/li&gt;
&lt;li&gt;improve the perceptual loss by using the features before activation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In contrast to SRGAN, which claimed that &lt;strong&gt;deeper models are increasingly difficult to train&lt;/strong&gt;, our deeper ESRGAN model shows its superior performance with easy training.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/architecture.jpg"&gt;&lt;img height="120" src="figures/architecture.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/RRDB.png"&gt;&lt;img height="180" src="figures/RRDB.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-network-interpolation" class="anchor" aria-hidden="true" href="#network-interpolation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network Interpolation&lt;/h2&gt;
&lt;p&gt;We propose the &lt;strong&gt;network interpolation strategy&lt;/strong&gt; to balance the visual quality and PSNR.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/net_interp.jpg"&gt;&lt;img height="500" src="figures/net_interp.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;We show the smooth animation with the interpolation parameters changing from 0 to 1.
Interestingly, it is observed that the network interpolation strategy provides a smooth control of the RRDB_PSNR model and the fine-tuned ESRGAN model.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/81.gif"&gt;&lt;img height="480" src="figures/81.gif" style="max-width:100%;"&gt;&lt;/a&gt;
     
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/102061.gif"&gt;&lt;img height="480" src="figures/102061.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-qualitative-results" class="anchor" aria-hidden="true" href="#qualitative-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Qualitative Results&lt;/h2&gt;
&lt;p&gt;PSNR (evaluated on the Y channel) and the perceptual index used in the PIRM-SR challenge are also provided for reference.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/qualitative_cmp_01.jpg"&gt;&lt;img src="figures/qualitative_cmp_01.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/qualitative_cmp_02.jpg"&gt;&lt;img src="figures/qualitative_cmp_02.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/qualitative_cmp_03.jpg"&gt;&lt;img src="figures/qualitative_cmp_03.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/qualitative_cmp_04.jpg"&gt;&lt;img src="figures/qualitative_cmp_04.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ablation-study" class="anchor" aria-hidden="true" href="#ablation-study"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ablation Study&lt;/h2&gt;
&lt;p&gt;Overall visual comparisons for showing the effects of each component in
ESRGAN. Each column represents a model with its configurations in the top.
The red sign indicates the main improvement compared with the previous model.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/abalation_study.png"&gt;&lt;img src="figures/abalation_study.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-bn-artifacts" class="anchor" aria-hidden="true" href="#bn-artifacts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BN artifacts&lt;/h2&gt;
&lt;p&gt;We empirically observe that BN layers tend to bring artifacts. These artifacts,
namely BN artifacts, occasionally appear among iterations and different settings,
violating the needs for a stable performance over training. We find that
the network depth, BN position, training dataset and training loss
have impact on the occurrence of BN artifacts.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/BN_artifacts.jpg"&gt;&lt;img src="figures/BN_artifacts.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-useful-techniques-to-train-a-very-deep-network" class="anchor" aria-hidden="true" href="#useful-techniques-to-train-a-very-deep-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Useful techniques to train a very deep network&lt;/h2&gt;
&lt;p&gt;We find that residual scaling and smaller initialization can help to train a very deep network. More details are in the Supplementary File attached in our &lt;a href="https://arxiv.org/abs/1809.00219" rel="nofollow"&gt;paper&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/train_deeper_neta.png"&gt;&lt;img height="250" src="figures/train_deeper_neta.png" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/train_deeper_netb.png"&gt;&lt;img height="250" src="figures/train_deeper_netb.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-influence-of-training-patch-size" class="anchor" aria-hidden="true" href="#the-influence-of-training-patch-size"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The influence of training patch size&lt;/h2&gt;
&lt;p&gt;We observe that training a deeper network benefits from a larger patch size. Moreover, the deeper model achieves more improvement (∼0.12dB) than the shallower one (∼0.04dB) since larger model capacity is capable of taking full advantage of
larger training patch size. (Evaluated on Set5 dataset with RGB channels.)&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/patch_a.png"&gt;&lt;img height="250" src="figures/patch_a.png" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="figures/patch_b.png"&gt;&lt;img height="250" src="figures/patch_b.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>xinntao</author><guid isPermaLink="false">https://github.com/xinntao/ESRGAN</guid><pubDate>Tue, 11 Feb 2020 00:06:00 GMT</pubDate></item><item><title>ProtonVPN/protonvpn-cli-ng #7 in Python, Today</title><link>https://github.com/ProtonVPN/protonvpn-cli-ng</link><description>&lt;p&gt;&lt;i&gt;Linux command-line client for ProtonVPN. Written in Python.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1 align="center"&gt;&lt;a id="user-content-protonvpn-cli" class="anchor" aria-hidden="true" href="#protonvpn-cli"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ProtonVPN-CLI&lt;/h1&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d5549a02d34f3c2f28d0d57605ed0ad07a5a7a2c/68747470733a2f2f692e696d6775722e636f6d2f744472776b58356c2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/d5549a02d34f3c2f28d0d57605ed0ad07a5a7a2c/68747470733a2f2f692e696d6775722e636f6d2f744472776b58356c2e706e67" alt="Logo" data-canonical-src="https://i.imgur.com/tDrwkX5l.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a href="https://pepy.tech/project/protonvpn-cli" rel="nofollow"&gt;&lt;img alt="Downloads" src="https://camo.githubusercontent.com/3e757d1ea7247cecca9fcd5b506ea7384a84a83e/68747470733a2f2f706570792e746563682f62616467652f70726f746f6e76706e2d636c69" data-canonical-src="https://pepy.tech/badge/protonvpn-cli" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/755ca291891065ba0f3efde9575ede9ba5408e7f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f50726f746f6e56504e2f70726f746f6e76706e2d636c692d6e67"&gt;&lt;img alt="GitHub" src="https://camo.githubusercontent.com/755ca291891065ba0f3efde9575ede9ba5408e7f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f50726f746f6e56504e2f70726f746f6e76706e2d636c692d6e67" data-canonical-src="https://img.shields.io/github/license/ProtonVPN/protonvpn-cli-ng" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://pepy.tech/project/protonvpn-cli/week" rel="nofollow"&gt;&lt;img alt="Downloads per Week" src="https://camo.githubusercontent.com/e9e8263b18966b942f91fd58a287467e9262943f/68747470733a2f2f706570792e746563682f62616467652f70726f746f6e76706e2d636c692f7765656b" data-canonical-src="https://pepy.tech/badge/protonvpn-cli/week" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href="https://twitter.com/ProtonVPN" rel="nofollow"&gt;&lt;img alt="Twitter Follow" src="https://camo.githubusercontent.com/f99169a97156f621eb4bf95e02a487250ea92598/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f50726f746f6e56504e3f7374796c653d736f6369616c" data-canonical-src="https://img.shields.io/twitter/follow/ProtonVPN?style=social" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://www.reddit.com/r/ProtonVPN" rel="nofollow"&gt;&lt;img alt="Subreddit subscribers" src="https://camo.githubusercontent.com/523feee84eb87e56eba2b96def32907bc1431589/68747470733a2f2f696d672e736869656c64732e696f2f7265646469742f7375627265646469742d73756273637269626572732f50726f746f6e56504e3f6c6162656c3d4a6f696e2532307225324650726f746f6e56504e267374796c653d736f6369616c" data-canonical-src="https://img.shields.io/reddit/subreddit-subscribers/ProtonVPN?label=Join%20r%2FProtonVPN&amp;amp;style=social" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3 align="center"&gt;&lt;a id="user-content-a-linux-cli-for-protonvpn-written-in-python" class="anchor" aria-hidden="true" href="#a-linux-cli-for-protonvpn-written-in-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A Linux CLI for ProtonVPN. Written in Python.&lt;/h3&gt;
&lt;p&gt;ProtonVPN-CLI is a full rewrite of the &lt;a href="https://github.com/ProtonVPN/protonvpn-cli/blob/master/protonvpn-cli.sh"&gt;bash protonvpn-cli&lt;/a&gt; in Python, which adds more features and functionality with the purpose of improving readability, speed and reliability.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation--updating" class="anchor" aria-hidden="true" href="#installation--updating"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation &amp;amp; Updating&lt;/h2&gt;
&lt;p&gt;For more detailed information on installing, updating and uninstalling, please view the extensive &lt;a href="https://github.com/ProtonVPN/protonvpn-cli-ng/blob/master/USAGE.md#installation--updating"&gt;usage guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-installing-dependencies" class="anchor" aria-hidden="true" href="#installing-dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing Dependencies&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Dependencies:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;openvpn&lt;/li&gt;
&lt;li&gt;dialog (optional, needed for interactive selection)&lt;/li&gt;
&lt;li&gt;pip for python3 (pip3)&lt;/li&gt;
&lt;li&gt;python3.5+&lt;/li&gt;
&lt;li&gt;setuptools for python3 (python3-setuptools)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Depending on your distribution, run the appropriate following command to install the necessary dependencies&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;strong&gt;Distro&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Command&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Fedora/CentOS/RHEL&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;sudo dnf install -y openvpn dialog python3-pip python3-setuptools&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Ubuntu/Linux Mint/Debian and derivatives&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;sudo apt install -y openvpn dialog python3-pip python3-setuptools&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;OpenSUSE/SLES&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;sudo zypper in -y openvpn dialog python3-pip python3-setuptools&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Arch Linux/Manjaro&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;sudo pacman -S openvpn dialog python-pip python-setuptools&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-installing-protonvpn-cli" class="anchor" aria-hidden="true" href="#installing-protonvpn-cli"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing ProtonVPN-CLI&lt;/h3&gt;
&lt;p&gt;Installation happens via Python's package manager PIP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Make sure to run pip with sudo, so it installs globally and recognizes the command with sudo&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo pip3 install protonvpn-cli&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-updating-protonvpn-cli" class="anchor" aria-hidden="true" href="#updating-protonvpn-cli"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updating ProtonVPN-CLI&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;sudo pip3 install protonvpn-cli --upgrade&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-manual-installation-from-source" class="anchor" aria-hidden="true" href="#manual-installation-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Manual Installation from source&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: If you are unsure about what you're doing, please follow the &lt;a href="https://github.com/ProtonVPN/protonvpn-cli-ng/blob/master/USAGE.md#installation--updating"&gt;normal installation guide&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It is recommended to do the manual installation in a virtual environment. Especially if it serves the purpose of developing.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Clone this repository&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git clone https://github.com/protonvpn/protonvpn-cli-ng&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step into the directory&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cd protonvpn-cli-ng&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pip3 install -e .&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For updating, you just need to pull the latest version of the repository with git.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-for-more-detailed-information-see-the-extensive-usage-guide" class="anchor" aria-hidden="true" href="#for-more-detailed-information-see-the-extensive-usage-guide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;For more detailed information, see the extensive &lt;a href="https://github.com/ProtonVPN/protonvpn-cli-ng/blob/master/USAGE.md"&gt;usage guide&lt;/a&gt;.&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;strong&gt;Command&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn init&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Initialize ProtonVPN profile.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn connect, c&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Select a ProtonVPN server and connect to it.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn c [servername]&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Connect to a specified server.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn c -r&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Connect to a random server.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn c -f&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Connect to the fastest server.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn c --p2p&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Connect to the fastest P2P server.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn c --cc [countrycode]&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Connect to the fastest server in a specified country.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn c --sc&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Connect to the fastest Secure Core server.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn reconnect, r&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Reconnect or connect to the last server used.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn disconnect, d&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Disconnect the current session.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn status, s&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Print connection status.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn configure&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Change CLI configuration.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn refresh&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Refresh OpenVPN configuration and server data.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn examples&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Print example commands.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn --version&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Display version.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;protonvpn --help&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Show help message.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;All connect options can be used with the &lt;code&gt;-p&lt;/code&gt; flag to explicitly specify which transmission protocol is used for that connection (either &lt;code&gt;udp&lt;/code&gt; or &lt;code&gt;tcp&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;If you want to contribute to this project, please read the &lt;a href="https://github.com/ProtonVPN/protonvpn-cli-ng/blob/master/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ProtonVPN</author><guid isPermaLink="false">https://github.com/ProtonVPN/protonvpn-cli-ng</guid><pubDate>Tue, 11 Feb 2020 00:07:00 GMT</pubDate></item><item><title>injetlee/Python #8 in Python, Today</title><link>https://github.com/injetlee/Python</link><description>&lt;p&gt;&lt;i&gt;Python脚本。模拟登录知乎， 爬虫，操作excel，微信公众号，远程开机&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-欢迎关注我的微信公众号智能制造社区" class="anchor" aria-hidden="true" href="#欢迎关注我的微信公众号智能制造社区"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;欢迎关注我的微信公众号【智能制造社区】&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-左手代码右手制造分享智能制造相关技术和业务包括-python-c-数据库工业大数据物联网技术及meserpsap等系统" class="anchor" aria-hidden="true" href="#左手代码右手制造分享智能制造相关技术和业务包括-python-c-数据库工业大数据物联网技术及meserpsap等系统"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;左手代码，右手制造，分享智能制造相关技术和业务，包括 Python, C#, 数据库，工业大数据、物联网技术及MES/ERP/SAP等系统。&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-可以通过微信公众号加我好友" class="anchor" aria-hidden="true" href="#可以通过微信公众号加我好友"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;可以通过微信公众号加我好友&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="qrcode.jpg"&gt;&lt;img src="qrcode.jpg" alt="二维码" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-内容列表" class="anchor" aria-hidden="true" href="#内容列表"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;内容列表&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-python微信公众号开发" class="anchor" aria-hidden="true" href="#python微信公众号开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/injetlee/Python/tree/master/wechat"&gt;Python微信公众号开发&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-python-微信公众号开发小白篇一" class="anchor" aria-hidden="true" href="#python-微信公众号开发小白篇一"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python 微信公众号开发—小白篇(一)&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-python-公众号开发颜值检测" class="anchor" aria-hidden="true" href="#python-公众号开发颜值检测"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python 公众号开发—颜值检测&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-python-爬虫入门合集" class="anchor" aria-hidden="true" href="#python-爬虫入门合集"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/injetlee/Python/tree/master/%E7%88%AC%E8%99%AB%E9%9B%86%E5%90%88"&gt;Python 爬虫入门合集&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-python-爬虫入门一爬取糗事百科" class="anchor" aria-hidden="true" href="#python-爬虫入门一爬取糗事百科"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python 爬虫入门(一)——爬取糗事百科&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-python-爬虫入门二爬取妹子图" class="anchor" aria-hidden="true" href="#python-爬虫入门二爬取妹子图"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python 爬虫入门(二)——爬取妹子图&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-python-爬虫python-岗位分析报告" class="anchor" aria-hidden="true" href="#python-爬虫python-岗位分析报告"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python 爬虫——Python 岗位分析报告&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-python-爬虫利器selenium介绍" class="anchor" aria-hidden="true" href="#python-爬虫利器selenium介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python 爬虫利器——Selenium介绍&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-python-爬虫-抖音-app-视频抓包爬取" class="anchor" aria-hidden="true" href="#python-爬虫-抖音-app-视频抓包爬取"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python 爬虫—— 抖音 App 视频抓包爬取&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-python-黑魔法" class="anchor" aria-hidden="true" href="#python-黑魔法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/injetlee/Python/tree/master/Python%20%E9%BB%91%E9%AD%94%E6%B3%95"&gt;Python 黑魔法&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-python-远程关机" class="anchor" aria-hidden="true" href="#python-远程关机"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python 远程关机&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-sql-数据库" class="anchor" aria-hidden="true" href="#sql-数据库"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQL 数据库&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/Lx4B349OlD49ihJPnB6YiA" rel="nofollow"&gt;1 小时 SQL 极速入门（一）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/D-CEtGYomne5kV_Ji4lodA" rel="nofollow"&gt;1 小时 SQL 极速入门（二）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/7aJqrhCNcvnt2gO3p5P50Q" rel="nofollow"&gt;1 小时 SQL 极速入门（三）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/R9Yldd-5AK4ObRA9Lfbz-Q" rel="nofollow"&gt;SQL 高级查询——（层次化查询，递归）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/_OK6dtHGhp7ukC2pe1ginQ" rel="nofollow"&gt;GROUP BY高级查询,ROLLUP，CUBE，GROUPPING详解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/xOFIg42FQhNpyg94ajhtqQ" rel="nofollow"&gt;SQL 行转列，列转行&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-其他" class="anchor" aria-hidden="true" href="#其他"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;其他&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1.&lt;a href="https://github.com/injetlee/demo/blob/master/CpuToInfluxdb.py"&gt;获取当前CPU状态，存储到Influxdb&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2.&lt;a href="https://github.com/injetlee/demo/blob/master/login_zhihu.py"&gt;模拟登录知乎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3.&lt;a href="https://github.com/injetlee/demo/blob/master/countFile.py"&gt;对目录下所有文件计数&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;4.&lt;a href="https://github.com/injetlee/demo/blob/master/douban_movie.py"&gt;爬取豆瓣电影top250&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;5.&lt;a href="https://github.com/injetlee/demo/blob/master/excelToDatabase.py"&gt;Excel文件读入数据库&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;6.&lt;a href="https://github.com/injetlee/demo/blob/master/lagouSpider.py"&gt;爬取拉勾网职位信息&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;7.&lt;a href="https://github.com/injetlee/demo/blob/master/ModifyFilename.py"&gt;批量修改文件名&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;8.&lt;a href="https://github.com/injetlee/demo/blob/master/readExcel.py"&gt;读写excel&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;9.&lt;a href="https://github.com/injetlee/Python/blob/master/biyingSpider.py"&gt;下载必应首页图片,只下载当天的，一张。&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>injetlee</author><guid isPermaLink="false">https://github.com/injetlee/Python</guid><pubDate>Tue, 11 Feb 2020 00:08:00 GMT</pubDate></item><item><title>facebookresearch/pytorch3d #9 in Python, Today</title><link>https://github.com/facebookresearch/pytorch3d</link><description>&lt;p&gt;&lt;i&gt;PyTorch3D is FAIR's library of reusable components for deep learning with 3D data&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/.github/pytorch3dlogo.png"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/.github/pytorch3dlogo.png" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://circleci.com/gh/facebookresearch/pytorch3d" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/13c43da53efe5f2c4656ff495c9b5c4eb9f75ebe/68747470733a2f2f636972636c6563692e636f6d2f67682f66616365626f6f6b72657365617263682f7079746f72636833642e7376673f7374796c653d737667" alt="CircleCI" data-canonical-src="https://circleci.com/gh/facebookresearch/pytorch3d.svg?style=svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://anaconda.org/pytorch3d/pytorch3d" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/47eb16cb1c002cb33475be4abb126a6eb9f222c6/68747470733a2f2f616e61636f6e64612e6f72672f7079746f72636833642f7079746f72636833642f6261646765732f76657273696f6e2e737667" alt="Anaconda-Server Badge" data-canonical-src="https://anaconda.org/pytorch3d/pytorch3d/badges/version.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;PyTorch3d provides efficient, reusable components for 3D Computer Vision research with &lt;a href="https://pytorch.org" rel="nofollow"&gt;PyTorch&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Key features include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data structure for storing and manipulating triangle meshes&lt;/li&gt;
&lt;li&gt;Efficient operations on triangle meshes (projective transformations, graph convolution, sampling, loss functions)&lt;/li&gt;
&lt;li&gt;A differentiable mesh renderer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PyTorch3d is designed to integrate smoothly with deep learning methods for predicting and manipulating 3D data.
For this reason, all operators in PyTorch3d:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are implemented using PyTorch tensors&lt;/li&gt;
&lt;li&gt;Can handle minibatches of hetereogenous data&lt;/li&gt;
&lt;li&gt;Can be differentiated&lt;/li&gt;
&lt;li&gt;Can utilize GPUs for acceleration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Within FAIR, PyTorch3d has been used to power research projects such as &lt;a href="https://arxiv.org/abs/1906.02739" rel="nofollow"&gt;Mesh R-CNN&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;For detailed instructions refer to &lt;a href="INSTALL.md"&gt;INSTALL.md&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;PyTorch3d is released under the &lt;a href="LICENSE"&gt;BSD-3-Clause License&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;Get started with PyTorch3d by trying one of the tutorial notebooks.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/.github/dolphin_deform.gif"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/.github/dolphin_deform.gif" width="310" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/.github/bundle_adjust.gif"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/.github/bundle_adjust.gif" width="310" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/facebookresearch/pytorch3d/blob/master/docs/tutorials/deform_source_mesh_to_target_mesh.ipynb"&gt;Deform a sphere mesh to dolphin&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/facebookresearch/pytorch3d/blob/master/docs/tutorials/bundle_adjustment.ipynb"&gt;Bundle adjustment&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/.github/render_textured_mesh.gif"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/.github/render_textured_mesh.gif" width="310" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/.github/camera_position_teapot.gif"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/.github/camera_position_teapot.gif" width="310" height="310" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/facebookresearch/pytorch3d/blob/master/docs/tutorials/render_textured_meshes.ipynb"&gt;Render textured meshes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/facebookresearch/pytorch3d/blob/master/docs/tutorials/camera_position_optimization_with_differentiable_rendering.ipynb"&gt;Camera position optimization&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;Learn more about the API by reading the PyTorch3d &lt;a href="https://pytorch3d.readthedocs.org/" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We also have deep dive notes on several API components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/pytorch3d/tree/master/docs/notes/batching.md"&gt;Heterogeneous Batching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/pytorch3d/tree/master/docs/notes/meshes_io.md"&gt;Mesh IO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/pytorch3d/tree/master/docs/notes/renderer_getting_started.md"&gt;Differentiable Rendering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h2&gt;
&lt;p&gt;We welcome new contributions to Pytorch3d and we will be actively maintaining this library! Please refer to &lt;a href="./.github/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for full instructions on how to run the code, tests and linter, and submit your pull requests.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;PyTorch3d is written and maintained by the Facebook AI Research Computer Vision Team.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you find PyTorch3d useful in your research, please cite:&lt;/p&gt;
&lt;div class="highlight highlight-text-bibtex"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;@misc&lt;/span&gt;{&lt;span class="pl-en"&gt;ravi2020pytorch3d&lt;/span&gt;,
  &lt;span class="pl-s"&gt;author&lt;/span&gt; =       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;Nikhila Ravi and Jeremy Reizenstein and David Novotny and Taylor Gordon&lt;/span&gt;
&lt;span class="pl-s"&gt;                  and Wan-Yen Lo and Justin Johnson and Georgia Gkioxari&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;title&lt;/span&gt; =        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;PyTorch3D&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;howpublished&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;\url{https://github.com/facebookresearch/pytorch3d}&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;year&lt;/span&gt; =         &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;2020&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>facebookresearch</author><guid isPermaLink="false">https://github.com/facebookresearch/pytorch3d</guid><pubDate>Tue, 11 Feb 2020 00:09:00 GMT</pubDate></item><item><title>d2l-ai/d2l-zh #10 in Python, Today</title><link>https://github.com/d2l-ai/d2l-zh</link><description>&lt;p&gt;&lt;i&gt;《动手学深度学习》：面向中文读者、能运行、可讨论。英文版即伯克利“深度学习导论”教材。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-动手学深度学习" class="anchor" aria-hidden="true" href="#动手学深度学习"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;动手学深度学习&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://ci.d2l.ai/job/d2l-zh/job/master/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0ad6b1453131a3debc7ccf1171d454334c1d5ff5/687474703a2f2f63692e64326c2e61692f6a6f622f64326c2d7a682f6a6f622f6d61737465722f62616467652f69636f6e" alt="Build Status" data-canonical-src="http://ci.d2l.ai/job/d2l-zh/job/master/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;本书网址：zh.d2l.ai&lt;/a&gt; |  &lt;a href="https://zh.d2l.ai/chapter_prerequisite/install.html" rel="nofollow"&gt;如何安装和使用书中源代码&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-更新" class="anchor" aria-hidden="true" href="#更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新&lt;/h2&gt;
&lt;p&gt;英文版全面改进了&lt;a href="https://d2l.ai/chapter_preliminaries/index.html" rel="nofollow"&gt;预备知识&lt;/a&gt;一章，
新增了&lt;a href="https://d2l.ai/chapter_recommender-systems/index.html" rel="nofollow"&gt;推荐系统&lt;/a&gt;一章和&lt;a href="https://d2l.ai/chapter_appendix_math/index.html" rel="nofollow"&gt;深度学习的数学&lt;/a&gt;一章。
欢迎关注英文版开源项目：&lt;a href="https://github.com/d2l-ai/d2l-en"&gt;https://github.com/d2l-ai/d2l-en&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-英文版-dive-into-deep-learning" class="anchor" aria-hidden="true" href="#英文版-dive-into-deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;英文版 &lt;em&gt;Dive into Deep Learning&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;加州大学伯克利分校 2019 年春学期 &lt;a href="http://courses.d2l.ai/berkeley-stat-157/index.html" rel="nofollow"&gt;&lt;em&gt;Introduction to Deep Learning&lt;/em&gt; 课程&lt;/a&gt;教材（&lt;a href="https://github.com/d2l-ai/berkeley-stat-157/tree/master/slides-zh"&gt;含教学视频地址的中文版课件&lt;/a&gt;）。&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-英文版引用" class="anchor" aria-hidden="true" href="#英文版引用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;英文版引用&lt;/h3&gt;
&lt;p&gt;BibTeX entry:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@book{zhang2019dive,
    title={Dive into Deep Learning},
    author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
    note={\url{http://www.d2l.ai}},
    year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-贡献" class="anchor" aria-hidden="true" href="#贡献"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;贡献&lt;/h2&gt;
&lt;p&gt;感谢&lt;a href="https://github.com/d2l-ai/d2l-zh/graphs/contributors"&gt;社区贡献者们&lt;/a&gt;为每一位读者改进这本开源书。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zh.d2l.ai/chapter_appendix/how-to-contribute.html" rel="nofollow"&gt;如何贡献&lt;/a&gt; | &lt;a href="https://zh.d2l.ai/chapter_preface/preface.html#%E8%87%B4%E8%B0%A2" rel="nofollow"&gt;致谢&lt;/a&gt; | &lt;a href="https://discuss.gluon.ai" rel="nofollow"&gt;讨论或报告问题&lt;/a&gt; | &lt;a href="INFO.md"&gt;其他&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>d2l-ai</author><guid isPermaLink="false">https://github.com/d2l-ai/d2l-zh</guid><pubDate>Tue, 11 Feb 2020 00:10:00 GMT</pubDate></item><item><title>quantumblacklabs/causalnex #11 in Python, Today</title><link>https://github.com/quantumblacklabs/causalnex</link><description>&lt;p&gt;&lt;i&gt;A Python library that helps data scientists to infer causation rather than observing correlation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/quantumblacklabs/causalnex/master/docs/source/causalnex_banner.png"&gt;&lt;img src="https://raw.githubusercontent.com/quantumblacklabs/causalnex/master/docs/source/causalnex_banner.png" alt="CausalNex" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Theme&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Latest Release&lt;/td&gt;
&lt;td&gt;&lt;a href="https://pypi.org/project/causalnex/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2fe0fed64233ea771a151e4370c51a7f6599ce7a/68747470733a2f2f62616467652e667572792e696f2f70792f63617573616c6e65782e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/causalnex.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Python Version&lt;/td&gt;
&lt;td&gt;&lt;a href="https://pypi.org/project/causalnex/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/afb1cec38b66cde0f87c123e96a4bf19ff698f60/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e35253230253743253230332e36253230253743253230332e372d626c75652e737667" alt="Python Version" data-canonical-src="https://img.shields.io/badge/python-3.5%20%7C%203.6%20%7C%203.7-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master&lt;/code&gt; Branch Build&lt;/td&gt;
&lt;td&gt;&lt;a href="https://circleci.com/gh/quantumblacklabs/causalnex/tree/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/126527e2e944781084e12a1039ded3e3c5bafcf2/68747470733a2f2f636972636c6563692e636f6d2f67682f7175616e74756d626c61636b6c6162732f63617573616c6e65782f747265652f6d61737465722e7376673f7374796c653d736869656c6426636972636c652d746f6b656e3d39326162373066303366333138333635353437336461643136626536343139353963643331623833" alt="CircleCI" data-canonical-src="https://circleci.com/gh/quantumblacklabs/causalnex/tree/master.svg?style=shield&amp;amp;circle-token=92ab70f03f3183655473dad16be641959cd31b83" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;develop&lt;/code&gt; Branch Build&lt;/td&gt;
&lt;td&gt;&lt;a href="https://circleci.com/gh/quantumblacklabs/causalnex/tree/develop" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/81dfb83e7aa12c7f75be5dd0add989b6991c3797/68747470733a2f2f636972636c6563692e636f6d2f67682f7175616e74756d626c61636b6c6162732f63617573616c6e65782f747265652f646576656c6f702e7376673f7374796c653d736869656c6426636972636c652d746f6b656e3d39326162373066303366333138333635353437336461643136626536343139353963643331623833" alt="CircleCI" data-canonical-src="https://circleci.com/gh/quantumblacklabs/causalnex/tree/develop.svg?style=shield&amp;amp;circle-token=92ab70f03f3183655473dad16be641959cd31b83" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Documentation Build&lt;/td&gt;
&lt;td&gt;&lt;a href="https://causalnex.readthedocs.io/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/26063ee32dc66a1448a97a9d8d8070308bc91092/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f63617573616c6e65782f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation" data-canonical-src="https://readthedocs.org/projects/causalnex/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;License&lt;/td&gt;
&lt;td&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/208c24da54eea1ae12f8abed5dcc6b84b6ce8440/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/license-Apache%202.0-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Code Style&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ambv/black"&gt;&lt;img src="https://camo.githubusercontent.com/af101b6e2ffeb62c0019ef67b84d5ce4dbe850d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d626c61636b2e737667" alt="Code Style: Black" data-canonical-src="https://img.shields.io/badge/code%20style-black-black.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-causalnex" class="anchor" aria-hidden="true" href="#what-is-causalnex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is CausalNex?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;"A toolkit for causal reasoning with Bayesian Networks."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;CausalNex aims to become one of the leading libraries for causal reasoning and "what-if" analysis using Bayesian Networks. It helps to simplify the steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To learn causal structures,&lt;/li&gt;
&lt;li&gt;To allow domain experts to augment the relationships,&lt;/li&gt;
&lt;li&gt;To estimate the effects of potential interventions using data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-why-causalnex" class="anchor" aria-hidden="true" href="#why-causalnex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why CausalNex?&lt;/h2&gt;
&lt;p&gt;CausalNex is built on our collective experience to leverage Bayesian Networks to identify causal relationships in data so that we can develop the right interventions from analytics. We developed CausalNex because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We believe &lt;strong&gt;leveraging Bayesian Networks&lt;/strong&gt; is more intuitive to describe causality compared to traditional machine learning methodology that are built on pattern recognition and correlation analysis.&lt;/li&gt;
&lt;li&gt;Causal relationships are more accurate if we can easily &lt;strong&gt;encode or augment domain expertise&lt;/strong&gt; in the graph model.&lt;/li&gt;
&lt;li&gt;We can then use the graph model to &lt;strong&gt;assess the impact&lt;/strong&gt; from changes to underlying features, i.e. counterfactual analysis, and &lt;strong&gt;identify the right intervention&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our experience, a data scientist generally has to use at least 3-4 different open-source libraries before arriving at the final step of finding the right intervention.  CausalNex aims to simplify this end-to-end process for causality and counterfactual analysis.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-are-the-main-features-of-causalnex" class="anchor" aria-hidden="true" href="#what-are-the-main-features-of-causalnex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What are the main features of CausalNex?&lt;/h2&gt;
&lt;p&gt;The main features of this library are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use state-of-the-art structure learning methods to understand conditional dependencies between variables&lt;/li&gt;
&lt;li&gt;Allow domain knowledge to augment model relationship&lt;/li&gt;
&lt;li&gt;Build predictive models based on structural relationships&lt;/li&gt;
&lt;li&gt;Fit probability distribution of the Bayesian Networks&lt;/li&gt;
&lt;li&gt;Evaluate model quality with standard statistical checks&lt;/li&gt;
&lt;li&gt;Simplify how causality is understood in Bayesian Networks through visualisation&lt;/li&gt;
&lt;li&gt;Analyse the impact of interventions using Do-calculus&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-how-do-i-install-causalnex" class="anchor" aria-hidden="true" href="#how-do-i-install-causalnex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I install CausalNex?&lt;/h2&gt;
&lt;p&gt;CausalNex is a Python package. To install it, simply run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install causalnex&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See more detailed installation instructions, including how to setup Python virtual environments, in our &lt;a href="https://causalnex.readthedocs.io/en/latest/02_getting_started/02_install.html" rel="nofollow"&gt;installation guide&lt;/a&gt; and get started with our &lt;a href="https://causalnex.readthedocs.io/en/latest/03_tutorial/03_tutorial.html" rel="nofollow"&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-do-i-use-causalnex" class="anchor" aria-hidden="true" href="#how-do-i-use-causalnex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I use CausalNex?&lt;/h2&gt;
&lt;p&gt;You can find the documentation for the latest stable release &lt;a href="https://causalnex.readthedocs.io/en/latest/" rel="nofollow"&gt;here&lt;/a&gt;. It explains:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An end-to-end &lt;a href="https://causalnex.readthedocs.io/en/latest/03_tutorial/03_tutorial.html" rel="nofollow"&gt;tutorial on how to use CausalNex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &lt;a href="https://causalnex.readthedocs.io/en/latest/04_user_guide/04_user_guide.html" rel="nofollow"&gt;main concepts and methods&lt;/a&gt; in using Bayesian Networks for Causal Inference&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: You can find the notebook and markdown files used to build the docs in &lt;a href="docs/source"&gt;&lt;code&gt;docs/source&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-can-i-contribute" class="anchor" aria-hidden="true" href="#can-i-contribute"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Can I contribute?&lt;/h2&gt;
&lt;p&gt;Yes! We'd love you to join us and help us build CausalNex. Check out our &lt;a href="CONTRIBUTING.md"&gt;contributing&lt;/a&gt; documentation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-do-i-upgrade-causalnex" class="anchor" aria-hidden="true" href="#how-do-i-upgrade-causalnex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I upgrade CausalNex?&lt;/h2&gt;
&lt;p&gt;We use &lt;a href="http://semver.org/" rel="nofollow"&gt;SemVer&lt;/a&gt; for versioning. The best way to upgrade safely is to check our &lt;a href="RELEASE.md"&gt;release notes&lt;/a&gt; for any notable breaking changes.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-licence-do-you-use" class="anchor" aria-hidden="true" href="#what-licence-do-you-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What licence do you use?&lt;/h2&gt;
&lt;p&gt;See our &lt;a href="LICENSE.md"&gt;LICENSE&lt;/a&gt; for more detail.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-were-hiring" class="anchor" aria-hidden="true" href="#were-hiring"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;We're hiring!&lt;/h2&gt;
&lt;p&gt;Do you want to be part of the team that builds CausalNex and &lt;a href="https://quantumblack.com/labs" rel="nofollow"&gt;other great products&lt;/a&gt; at QuantumBlack? If so, you're in luck! QuantumBlack is currently hiring Machine Learning Engineers who love using data to drive their decisions. Take a look at &lt;a href="https://www.quantumblack.com/careers/current-openings#content" rel="nofollow"&gt;our open positions&lt;/a&gt; and see if you're a fit.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>quantumblacklabs</author><guid isPermaLink="false">https://github.com/quantumblacklabs/causalnex</guid><pubDate>Tue, 11 Feb 2020 00:11:00 GMT</pubDate></item><item><title>wonderfulsuccess/weixin_crawler #12 in Python, Today</title><link>https://github.com/wonderfulsuccess/weixin_crawler</link><description>&lt;p&gt;&lt;i&gt;高效微信公众号全部历史文章和阅读数据爬虫powered by scrapy 微信公众号爬虫 微信采集 公众号采集 微信爬虫&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-hi-这是个非常用心的微信公众号爬虫自带丰富数据分析功能" class="anchor" aria-hidden="true" href="#hi-这是个非常用心的微信公众号爬虫自带丰富数据分析功能"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hi 这是个非常用心的微信公众号爬虫，自带丰富数据分析功能&lt;/h2&gt;
&lt;p&gt;希望它能在技术学习、公众号爬虫、开发者变现等多方面给你带来启发&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;源代码数十个模块，结构清晰，付费社区还有详细的配置文档&lt;/li&gt;
&lt;li&gt;项目介绍详尽，概括了主要的特性和技术栈&lt;/li&gt;
&lt;li&gt;配有精心剪辑的介绍视频和动图，如果你正在为公司的公众号采集开发犯愁，分享给技术负责人，相信会帮助你们节约时间&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-文末有彩蛋送给有孩子的工程师爸爸" class="anchor" aria-hidden="true" href="#文末有彩蛋送给有孩子的工程师爸爸"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;文末有彩蛋，送给有孩子的工程师爸爸&lt;/h2&gt;
&lt;p&gt;如果你和我一样是一个有孩子的工程师，相信也一定面临着多重挑战和压力，文末我给你准备了一个小彩蛋，希望在家庭教育上，能给你多一份自信。带孩子本可以很轻松...&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-weixin_crawler" class="anchor" aria-hidden="true" href="#what-is-weixin_crawler"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is weixin_crawler?&lt;/h2&gt;
&lt;p&gt;weixin_crawler是一款使用Scrapy、Flask、Echarts、Elasticsearch等实现的微信公众号文章爬虫,可采集任意公众号的全部历史文章,包括阅读数据.自带分析报告(&lt;a href="readme_img/report_example.png"&gt;报告样例&lt;/a&gt;)和全文检索功能，几百万的文档都能瞬间搜索。weixin_crawler设计的初衷是尽可能多、尽可能快地爬取微信公众的历史发文&lt;/p&gt;
&lt;p&gt;weixin_crawler尚处于维护之中, 方案有效, 请放心尝试. weixin_crawler is under maintaining, the code works greatly free to explore please&lt;/p&gt;
&lt;p&gt;如果你想先看看这个项目是否有趣，这段不足3分钟的介绍视频一定是你需要的
If you want to check if weixin_crawler is interesting or powerful enougth, this video will help to save time
&lt;a href="https://www.youtube.com/watch?v=CbfLRCV7oeU&amp;amp;t" rel="nofollow"&gt;https://www.youtube.com/watch?v=CbfLRCV7oeU&amp;amp;t&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-主要特点" class="anchor" aria-hidden="true" href="#主要特点"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;主要特点&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;使用Python3编写&lt;/p&gt;
&lt;p&gt;Python3 is used&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;爬虫框架为Scrapy并且实际用到了Scrapy的诸多特性，是深入学习Scrapy的不错开源项目&lt;/p&gt;
&lt;p&gt;Made full use of scrapy, if you are struggling with scrapy this repo helps to spark&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用Flask、Flask-socketio、Vue实现了高可用性的UI界面。功能强大实用，是新媒体运营等岗位不错的数据助手&lt;/p&gt;
&lt;p&gt;Flask、Flask-socketio、Vue are used to build a full stack project crawler&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;得益于Scrapy、MongoDB、Elasticsearch的使用，数据爬取、存储、索引均简单高效&lt;/p&gt;
&lt;p&gt;Thanks to scrapy mongodb elasticsearch weixin_crawler is not only a crawler but also a search engine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持微信公众号的全部历史发文爬取&lt;/p&gt;
&lt;p&gt;Able to crawl all the history articles of any weixin official account&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持微信公众号文章的阅读量、点赞量、赞赏量、评论量等数据的爬取&lt;/p&gt;
&lt;p&gt;Able to crawl the reading data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自带面向单个公众号的数据分析报告&lt;/p&gt;
&lt;p&gt;Released with report module based on sigle official account&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用Elasticsearch实现了全文检索，支持多种搜索和模式和排序模式，针对搜索结果提供了趋势分析图表&lt;/p&gt;
&lt;p&gt;It is also a search engine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持对公众号进行分组，可利用分组数据限定搜索范围&lt;/p&gt;
&lt;p&gt;Able to group official account which can be used to define searching range&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;原创手机自动化操作方法，可实现爬虫无人监管&lt;/p&gt;
&lt;p&gt;Whith the help of adb, weixin_crawler is able to opereate Android phone automatically, which means it can work without any human monitoring&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持多微信APP同时采集, 理论上采集速度可线性增加&lt;/p&gt;
&lt;p&gt;Mutiple weixin app is supported to imporove crawling speed linearly&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-使用到的主要工具" class="anchor" aria-hidden="true" href="#使用到的主要工具"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;使用到的主要工具&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;语言&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Python3.6&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;前端&lt;/td&gt;
&lt;td&gt;web框架&lt;/td&gt;
&lt;td&gt;Flask / Flask-socketio / gevent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;js/css库&lt;/td&gt;
&lt;td&gt;Vue / Jquery / W3css / Echarts / Front-awsome&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;后端&lt;/td&gt;
&lt;td&gt;爬虫&lt;/td&gt;
&lt;td&gt;Scrapy&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;存储&lt;/td&gt;
&lt;td&gt;Mongodb / Redis&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;索引&lt;/td&gt;
&lt;td&gt;Elasticsearch&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-运行方法" class="anchor" aria-hidden="true" href="#运行方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;运行方法&lt;/h2&gt;
&lt;p&gt;weixin_crawler已经在Win/Mac/Linux系统下运行成功, 建议优先使用win系统尝试
weixin_crawler could work on win/mac/linux, although it is suggested to try on win os firstly&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-insatall--mongodb--redis--elasticsearch-and-run-them-in-the-background" class="anchor" aria-hidden="true" href="#insatall--mongodb--redis--elasticsearch-and-run-them-in-the-background"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Insatall  mongodb / redis / elasticsearch and run them in the background&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;downlaod mongodb / redis / elasticsearch from their official sites and install them&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;run them at the same time under the default configuration. In this case mongodb is localhost:27017 redis is localhost:6379(or you have to config in weixin_crawler/project/configs/auth.py)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inorder to tokenize Chinese, &lt;em&gt;elasticsearch-analysis-ik&lt;/em&gt; have to be installed for Elasticsearch&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-install-proxy-server-and-run-proxyjs" class="anchor" aria-hidden="true" href="#install-proxy-server-and-run-proxyjs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install proxy server and run proxy.js&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;install nodejs and then npm install anyproxy and redis in weixin_crawler/proxy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cd to weixin_crawler/proxy and run node proxy.js&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;install anyproxy https CA in both computer and phone side&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if you are not sure how to use anyproxy, &lt;a href="https://github.com/alibaba/anyproxy"&gt;here &lt;/a&gt;is the doc&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-install-the-needed-python-packages" class="anchor" aria-hidden="true" href="#install-the-needed-python-packages"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install the needed python packages&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;NOTE: you may can not simply type pip install -r requirements.txt to install every package, twisted is one of them which is needed by scrapy. When you get some problems about installing python package(twisted for instance), &lt;a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/" rel="nofollow"&gt;here&lt;/a&gt; always have a solution——downlod the right version package to your drive and run $ pip install package_name&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I am not sure if your python enviroment will throw other package not found error, just install any package that is needed&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-some-source-code-have-to-be-modifiedmaybe-it-is-not-reasonable" class="anchor" aria-hidden="true" href="#some-source-code-have-to-be-modifiedmaybe-it-is-not-reasonable"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Some source code have to be modified(maybe it is not reasonable)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;scrapy Python36\Lib\site-packages\scrapy\http\request\ _&lt;em&gt;init_&lt;/em&gt;.py  --&amp;gt; weixin_crawler\source_code\request\__init__.py&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;scrapy Python36\Lib\site-packages\scrapy\http\response\ _&lt;em&gt;init_&lt;/em&gt;.py --&amp;gt; weixin_crawler\source_code\response\_&lt;em&gt;init_&lt;/em&gt;.py&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;pyecharts Python36\Lib\site-packages\pyecharts\base.py --&amp;gt; weixin_crawler\source_code\base.py. In this case function get_echarts_options is added in line 106&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-if-you-want-weixin_crawler-work-automatically-those-steps-are-necessary-or-you-shoud-operate-the-phone-to-get-the-request-data-that-will-be-detected-by-anyproxy-manual" class="anchor" aria-hidden="true" href="#if-you-want-weixin_crawler-work-automatically-those-steps-are-necessary-or-you-shoud-operate-the-phone-to-get-the-request-data-that-will-be-detected-by-anyproxy-manual"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;If you want weixin_crawler work automatically those steps are necessary or you shoud operate the phone to get the request data that will be detected by Anyproxy manual&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install adb and add it to your path(windows for example)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;install android emulator(NOX suggested) or plugin your phone and make sure you can operate them with abd from command line tools&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If mutiple phone are connected to your computer you have to find out their adb ports which will be used to add crawler&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;adb does not support Chinese input, this is a bad news for weixin official account searching. In order to input Chinese, adb keyboard has to be installed in your android phone and set it as the default input method, more is &lt;a href="https://github.com/senzhk/ADBKeyBoard"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Why could weixin_crawler work automatically? Here is the reason:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wechat 7.03 or lower version is required&lt;/li&gt;
&lt;li&gt;If you want to crawl a wechat official account, you have to search the account in you phone and click its "全部消息" then you will get a message list , if you roll down more lists will be loaded.  Anyone of the messages in the list could be taped if you want to crawl this account's reading data&lt;/li&gt;
&lt;li&gt;If a nickname of a wechat official account is given, then wexin_crawler operate the wechat app installed in a phone, at the same time anyproxy is 'listening background'...Anyway weixin_crawler get all the request data requested by wechat app, then it is the show time for scrapy&lt;/li&gt;
&lt;li&gt;As you supposed, in order to let weixin_crawler operate wechat app we have to tell adb where to click swap and input,  most of them are defined in weixin_crawler/project/phone_operate/config.py. BTW phone_operate is responsible for wechat operate just like human beings, its eyes are baidu OCR API and predefined location tap area, its fingers are adb&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-run-the-mainpy" class="anchor" aria-hidden="true" href="#run-the-mainpy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run the main.py&lt;/h4&gt;
&lt;p&gt;$ cd weixin_crawler/project/&lt;/p&gt;
&lt;p&gt;$ python(3) ./main.py&lt;/p&gt;
&lt;p&gt;Now open the browser and everything you want would be in localhost:5000.&lt;/p&gt;
&lt;p&gt;In this long step list you may get stucked, join our community for help, tell us what you have done and what kind of error you have found.&lt;/p&gt;
&lt;p&gt;Let's go to explore the world in localhost:5000 together&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-功能展示" class="anchor" aria-hidden="true" href="#功能展示"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;功能展示&lt;/h2&gt;
&lt;p&gt;UI主界面&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="readme_img/%E7%88%AC%E8%99%AB%E4%B8%BB%E7%95%8C%E9%9D%A2.gif"&gt;&lt;img src="readme_img/%E7%88%AC%E8%99%AB%E4%B8%BB%E7%95%8C%E9%9D%A2.gif" alt="1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;添加公众号爬取任务和已经爬取的公众号列表&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="readme_img/%E5%85%AC%E4%BC%97%E5%8F%B7.png"&gt;&lt;img src="readme_img/%E5%85%AC%E4%BC%97%E5%8F%B7.png" alt="1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;爬虫界面&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="readme_img/caiji.png"&gt;&lt;img src="readme_img/caiji.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;设置界面&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="readme_img/%E8%AE%BE%E7%BD%AE.png"&gt;&lt;img src="readme_img/%E8%AE%BE%E7%BD%AE.png" alt=" " style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;公众号历史文章列表&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="readme_img/%E5%8E%86%E5%8F%B2%E6%96%87%E7%AB%A0%E5%88%97%E8%A1%A8.gif"&gt;&lt;img src="readme_img/%E5%8E%86%E5%8F%B2%E6%96%87%E7%AB%A0%E5%88%97%E8%A1%A8.gif" alt=" " style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;报告&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="readme_img/%E6%8A%A5%E5%91%8A.gif"&gt;&lt;img src="readme_img/%E6%8A%A5%E5%91%8A.gif" alt=" " style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;搜索&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="readme_img/%E6%90%9C%E7%B4%A2.gif"&gt;&lt;img src="readme_img/%E6%90%9C%E7%B4%A2.gif" alt=" " style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-配置过程繁琐-搞不定" class="anchor" aria-hidden="true" href="#配置过程繁琐-搞不定"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;配置过程繁琐 搞不定？&lt;/h2&gt;
&lt;p&gt;我为 weixin_crawler 配套了付费社群，内有详细的配置文档和其它开发者的问题讨论历史，一定会帮你节约大量时间。&lt;/p&gt;
&lt;p&gt;如果你对微信公众号爬虫有跟多想法，欢迎到社群和我深入讨论。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;添加作者微信&lt;/th&gt;
&lt;th&gt;加入付费社群&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="readme_img/%E7%9F%A5%E8%AF%86%E6%98%9F%E7%90%83.jpeg"&gt;&lt;img src="readme_img/%E7%9F%A5%E8%AF%86%E6%98%9F%E7%90%83.jpeg" alt=" " style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="readme_img/%E6%94%AF%E4%BB%98%E6%98%9F%E7%90%83.png"&gt;&lt;img src="readme_img/%E6%94%AF%E4%BB%98%E6%98%9F%E7%90%83.png" alt=" " style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-彩蛋来了" class="anchor" aria-hidden="true" href="#彩蛋来了"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;彩蛋来了&lt;/h2&gt;
&lt;p&gt;做生意赚钱得靠信息差，做教育带孩子这事儿，它也有信息不对称&lt;/p&gt;
&lt;p&gt;内容很多，你不用专门抽时间学习。做饭、走路、公交、地铁、开车、家务时随便听听，没准儿哪句话就改变你家孩子的一生&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="readme_img/ct.jpeg"&gt;&lt;img src="readme_img/ct.jpeg" alt=" " style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>wonderfulsuccess</author><guid isPermaLink="false">https://github.com/wonderfulsuccess/weixin_crawler</guid><pubDate>Tue, 11 Feb 2020 00:12:00 GMT</pubDate></item><item><title>kevinzakka/nca #13 in Python, Today</title><link>https://github.com/kevinzakka/nca</link><description>&lt;p&gt;&lt;i&gt;A PyTorch implementation of Neighbourhood Components Analysis.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-neighbourhood-components-analysis" class="anchor" aria-hidden="true" href="#neighbourhood-components-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Neighbourhood Components Analysis&lt;/h1&gt;
&lt;p&gt;A PyTorch implementation of &lt;a href="https://www.cs.toronto.edu/~hinton/absps/nca.pdf" rel="nofollow"&gt;Neighbourhood Components Analysis&lt;/a&gt; by &lt;em&gt;J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;NCA learns a linear transformation of the dataset such that the expected leave-one-out performance of kNN in the transformed space is maximized.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-api" class="anchor" aria-hidden="true" href="#api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;API&lt;/h2&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; instantiate nca object and initialize with&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; an identity matrix&lt;/span&gt;
nca &lt;span class="pl-k"&gt;=&lt;/span&gt; NCA(&lt;span class="pl-v"&gt;dim&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-v"&gt;init&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;identity&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; fit an nca model to a dataset&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; normalize the input data before&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; running the optimization&lt;/span&gt;
nca.train(X, y, &lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;normalize&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; apply the learned linear map to the data&lt;/span&gt;
X_nca &lt;span class="pl-k"&gt;=&lt;/span&gt; nca(X)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-dimensionality-reduction" class="anchor" aria-hidden="true" href="#dimensionality-reduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dimensionality Reduction&lt;/h2&gt;
&lt;p&gt;We generate a 3-D dataset where the first 2 dimensions are concentric rings and the third dimension is Gaussian noise. We plot the result of PCA and NCA with 2 components.&lt;/p&gt;
&lt;p align="center"&gt;
 &lt;a target="_blank" rel="noopener noreferrer" href="./assets/res.png"&gt;&lt;img src="./assets/res.png" width="80%" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Notice how PCA has failed to project out the noise, a result of a high noise variance in the third dimension. You can try lowering it (e.g. &lt;code&gt;0.1&lt;/code&gt;) using the &lt;code&gt;--sigma&lt;/code&gt; command line argument to see its effect on PCA.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>kevinzakka</author><guid isPermaLink="false">https://github.com/kevinzakka/nca</guid><pubDate>Tue, 11 Feb 2020 00:13:00 GMT</pubDate></item><item><title>eriklindernoren/ML-From-Scratch #14 in Python, Today</title><link>https://github.com/eriklindernoren/ML-From-Scratch</link><description>&lt;p&gt;&lt;i&gt;Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-from-scratch" class="anchor" aria-hidden="true" href="#machine-learning-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning From Scratch&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.&lt;/p&gt;
&lt;p&gt;The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible
but rather to present the inner workings of them in a transparent and accessible way.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#machine-learning-from-scratch"&gt;Machine Learning From Scratch&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#about"&gt;About&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#polynomial-regression"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#classification-with-cnn"&gt;Classification With CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#density-based-clustering"&gt;Density-Based Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#generating-handwritten-digits"&gt;Generating Handwritten Digits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-reinforcement-learning"&gt;Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image-reconstruction-with-rbm"&gt;Image Reconstruction With RBM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#evolutionary-evolved-neural-network"&gt;Evolutionary Evolved Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#genetic-algorithm"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#association-analysis"&gt;Association Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#implementations"&gt;Implementations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#supervised-learning"&gt;Supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unsupervised-learning"&gt;Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement-learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-learning"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#contact"&gt;Contact&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/eriklindernoren/ML-From-Scratch
$ cd ML-From-Scratch
$ python setup.py install
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-polynomial-regression" class="anchor" aria-hidden="true" href="#polynomial-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Polynomial Regression&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/polynomial_regression.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d82416364e7916546886f94027e2652d3247e8ab/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f705f7265672e676966"&gt;&lt;img src="https://camo.githubusercontent.com/d82416364e7916546886f94027e2652d3247e8ab/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f705f7265672e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/p_reg.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Training progress of a regularized polynomial regression model fitting &lt;br&gt;
    temperature data measured in Linköping, Sweden 2016.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-classification-with-cnn" class="anchor" aria-hidden="true" href="#classification-with-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Classification With CNN&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/convolutional_neural_network.py

+---------+
| ConvNet |
+---------+
Input Shape: (1, 8, 8)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Conv2D               | 160        | (16, 8, 8)   |
| Activation (ReLU)    | 0          | (16, 8, 8)   |
| Dropout              | 0          | (16, 8, 8)   |
| BatchNormalization   | 2048       | (16, 8, 8)   |
| Conv2D               | 4640       | (32, 8, 8)   |
| Activation (ReLU)    | 0          | (32, 8, 8)   |
| Dropout              | 0          | (32, 8, 8)   |
| BatchNormalization   | 4096       | (32, 8, 8)   |
| Flatten              | 0          | (2048,)      |
| Dense                | 524544     | (256,)       |
| Activation (ReLU)    | 0          | (256,)       |
| Dropout              | 0          | (256,)       |
| BatchNormalization   | 512        | (256,)       |
| Dense                | 2570       | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 538570

Training: 100% [------------------------------------------------------------------------] Time: 0:01:55
Accuracy: 0.987465181058
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c2bca09f5d1ce2b72f33fe61464408607797caa3/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f636e6e312e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/c2bca09f5d1ce2b72f33fe61464408607797caa3/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f636e6e312e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_cnn1.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Classification of the digit dataset using CNN.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-density-based-clustering" class="anchor" aria-hidden="true" href="#density-based-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Density-Based Clustering&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/dbscan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/eaf413b6e8cbf3f8fd048f3a63984482ffd7350e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64627363616e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/eaf413b6e8cbf3f8fd048f3a63984482ffd7350e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64627363616e2e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_dbscan.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Clustering of the moons dataset using DBSCAN.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-generating-handwritten-digits" class="anchor" aria-hidden="true" href="#generating-handwritten-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating Handwritten Digits&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py

+-----------+
| Generator |
+-----------+
Input Shape: (100,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 25856      | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| BatchNormalization     | 512        | (256,)       |
| Dense                  | 131584     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| BatchNormalization     | 1024       | (512,)       |
| Dense                  | 525312     | (1024,)      |
| Activation (LeakyReLU) | 0          | (1024,)      |
| BatchNormalization     | 2048       | (1024,)      |
| Dense                  | 803600     | (784,)       |
| Activation (TanH)      | 0          | (784,)       |
+------------------------+------------+--------------+
Total Parameters: 1489936

+---------------+
| Discriminator |
+---------------+
Input Shape: (784,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 401920     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| Dropout                | 0          | (512,)       |
| Dense                  | 131328     | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| Dropout                | 0          | (256,)       |
| Dense                  | 514        | (2,)         |
| Activation (Softmax)   | 0          | (2,)         |
+------------------------+------------+--------------+
Total Parameters: 533762
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/15ad5010011227a7ab8c6c77d19b7cc625cced30/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f67616e5f6d6e697374352e676966"&gt;&lt;img src="https://camo.githubusercontent.com/15ad5010011227a7ab8c6c77d19b7cc625cced30/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f67616e5f6d6e697374352e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/gan_mnist5.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Training progress of a Generative Adversarial Network generating &lt;br&gt;
    handwritten digits.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-deep-reinforcement-learning" class="anchor" aria-hidden="true" href="#deep-reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Reinforcement Learning&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/deep_q_network.py

+----------------+
| Deep Q-Network |
+----------------+
Input Shape: (4,)
+-------------------+------------+--------------+
| Layer Type        | Parameters | Output Shape |
+-------------------+------------+--------------+
| Dense             | 320        | (64,)        |
| Activation (ReLU) | 0          | (64,)        |
| Dense             | 130        | (2,)         |
+-------------------+------------+--------------+
Total Parameters: 450
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c605134f41b739121c4710f3d5c6e8370a592e0c/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64716c312e676966"&gt;&lt;img src="https://camo.githubusercontent.com/c605134f41b739121c4710f3d5c6e8370a592e0c/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64716c312e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_dql1.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-image-reconstruction-with-rbm" class="anchor" aria-hidden="true" href="#image-reconstruction-with-rbm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Reconstruction With RBM&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/restricted_boltzmann_machine.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d209d42aed9e8e32a10eaec9b76f141319a2b0d7/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f72626d5f646967697473312e676966"&gt;&lt;img src="https://camo.githubusercontent.com/d209d42aed9e8e32a10eaec9b76f141319a2b0d7/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f72626d5f646967697473312e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/rbm_digits1.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Shows how the network gets better during training at reconstructing &lt;br&gt;
    the digit 2 in the MNIST dataset.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-evolutionary-evolved-neural-network" class="anchor" aria-hidden="true" href="#evolutionary-evolved-neural-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evolutionary Evolved Neural Network&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/neuroevolution.py

+---------------+
| Model Summary |
+---------------+
Input Shape: (64,)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Dense                | 1040       | (16,)        |
| Activation (ReLU)    | 0          | (16,)        |
| Dense                | 170        | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 1210

Population Size: 100
Generations: 3000
Mutation Rate: 0.01

[0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]
[1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]
...
[2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]
Test set accuracy: 96.7%
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1a8abe4882d0195b8f8bd4c6f24caab639291e6e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f65766f5f6e6e342e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/1a8abe4882d0195b8f8bd4c6f24caab639291e6e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f65766f5f6e6e342e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/evo_nn4.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Classification of the digit dataset by a neural network which has&lt;br&gt;
    been evolutionary evolved.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-genetic-algorithm" class="anchor" aria-hidden="true" href="#genetic-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Genetic Algorithm&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/genetic_algorithm.py

+--------+
|   GA   |
+--------+
Description: Implementation of a Genetic Algorithm which aims to produce
the user specified target string. This implementation calculates each
candidate's fitness based on the alphabetical distance between the candidate
and the target. A candidate is selected as a parent with probabilities proportional
to the candidate's fitness. Reproduction is implemented as a single-point
crossover between pairs of parents. Mutation is done by randomly assigning
new characters with uniform probability.

Parameters
----------
Target String: 'Genetic Algorithm'
Population Size: 100
Mutation Rate: 0.05

[0 Closest Candidate: 'CJqlJguPlqzvpoJmb', Fitness: 0.00]
[1 Closest Candidate: 'MCxZxdr nlfiwwGEk', Fitness: 0.01]
[2 Closest Candidate: 'MCxZxdm nlfiwwGcx', Fitness: 0.01]
[3 Closest Candidate: 'SmdsAklMHn kBIwKn', Fitness: 0.01]
[4 Closest Candidate: '  lotneaJOasWfu Z', Fitness: 0.01]
...
[292 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[293 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[294 Answer: 'Genetic Algorithm']
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-association-analysis" class="anchor" aria-hidden="true" href="#association-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Association Analysis&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/apriori.py
+-------------+
|   Apriori   |
+-------------+
Minimum Support: 0.25
Minimum Confidence: 0.8
Transactions:
    [1, 2, 3, 4]
    [1, 2, 4]
    [1, 2]
    [2, 3, 4]
    [2, 3]
    [3, 4]
    [2, 4]
Frequent Itemsets:
    [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]
Rules:
    1 -&amp;gt; 2 (support: 0.43, confidence: 1.0)
    4 -&amp;gt; 2 (support: 0.57, confidence: 0.8)
    [1, 4] -&amp;gt; 2 (support: 0.29, confidence: 1.0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-implementations" class="anchor" aria-hidden="true" href="#implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementations&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-supervised-learning" class="anchor" aria-hidden="true" href="#supervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supervised Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/adaboost.py"&gt;Adaboost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/bayesian_regression.py"&gt;Bayesian Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/decision_tree.py"&gt;Decision Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Elastic Net&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/gradient_boosting.py"&gt;Gradient Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/k_nearest_neighbors.py"&gt;K Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Lasso Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/linear_discriminant_analysis.py"&gt;Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/logistic_regression.py"&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/multi_class_lda.py"&gt;Multi-class Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/naive_bayes.py"&gt;Naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/neuroevolution.py"&gt;Neuroevolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/particle_swarm_optimization.py"&gt;Particle Swarm Optimization of Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/perceptron.py"&gt;Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/random_forest.py"&gt;Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Ridge Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/support_vector_machine.py"&gt;Support Vector Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/xgboost.py"&gt;XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-unsupervised-learning" class="anchor" aria-hidden="true" href="#unsupervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/apriori.py"&gt;Apriori&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/autoencoder.py"&gt;Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/dbscan.py"&gt;DBSCAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/fp_growth.py"&gt;FP-Growth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/gaussian_mixture_model.py"&gt;Gaussian Mixture Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/generative_adversarial_network.py"&gt;Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/genetic_algorithm.py"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/k_means.py"&gt;K-Means&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/partitioning_around_medoids.py"&gt;Partitioning Around Medoids&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/principal_component_analysis.py"&gt;Principal Component Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/restricted_boltzmann_machine.py"&gt;Restricted Boltzmann Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-reinforcement-learning" class="anchor" aria-hidden="true" href="#reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reinforcement Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/reinforcement_learning/deep_q_network.py"&gt;Deep Q-Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deep-learning" class="anchor" aria-hidden="true" href="#deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/deep_learning/neural_network.py"&gt;Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/deep_learning/layers.py"&gt;Layers&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Activation Layer&lt;/li&gt;
&lt;li&gt;Average Pooling Layer&lt;/li&gt;
&lt;li&gt;Batch Normalization Layer&lt;/li&gt;
&lt;li&gt;Constant Padding Layer&lt;/li&gt;
&lt;li&gt;Convolutional Layer&lt;/li&gt;
&lt;li&gt;Dropout Layer&lt;/li&gt;
&lt;li&gt;Flatten Layer&lt;/li&gt;
&lt;li&gt;Fully-Connected (Dense) Layer&lt;/li&gt;
&lt;li&gt;Fully-Connected RNN Layer&lt;/li&gt;
&lt;li&gt;Max Pooling Layer&lt;/li&gt;
&lt;li&gt;Reshape Layer&lt;/li&gt;
&lt;li&gt;Up Sampling Layer&lt;/li&gt;
&lt;li&gt;Zero Padding Layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model Types
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/convolutional_neural_network.py"&gt;Convolutional Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/recurrent_neural_network.py"&gt;Recurrent Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;If there's some implementation you would like to see here or if you're just feeling social,
feel free to &lt;a href="mailto:eriklindernoren@gmail.com"&gt;email&lt;/a&gt; me or connect with me on &lt;a href="https://www.linkedin.com/in/eriklindernoren/" rel="nofollow"&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>eriklindernoren</author><guid isPermaLink="false">https://github.com/eriklindernoren/ML-From-Scratch</guid><pubDate>Tue, 11 Feb 2020 00:14:00 GMT</pubDate></item><item><title>google/diff-match-patch #15 in Python, Today</title><link>https://github.com/google/diff-match-patch</link><description>&lt;p&gt;&lt;i&gt;Diff Match Patch is a high-performance library in multiple languages that manipulates plain text.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;The Diff Match and Patch libraries offer robust algorithms to perform the
operations required for synchronizing plain text.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Diff:
&lt;ul&gt;
&lt;li&gt;Compare two blocks of plain text and efficiently return a list of differences.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neil.fraser.name/software/diff_match_patch/demos/diff.html" rel="nofollow"&gt;Diff Demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Match:
&lt;ul&gt;
&lt;li&gt;Given a search string, find its best fuzzy match in a block of plain text. Weighted for both accuracy and location.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neil.fraser.name/software/diff_match_patch/demos/match.html" rel="nofollow"&gt;Match Demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Patch:
&lt;ul&gt;
&lt;li&gt;Apply a list of patches onto plain text. Use best-effort to apply patch even when the underlying text doesn't match.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neil.fraser.name/software/diff_match_patch/demos/patch.html" rel="nofollow"&gt;Patch Demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Originally built in 2006 to power Google Docs, this library is now available in C++, C#, Dart, Java, JavaScript, Lua, Objective C, and Python.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-reference" class="anchor" aria-hidden="true" href="#reference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/google/diff-match-patch/wiki/API"&gt;API&lt;/a&gt; - Common API across all languages.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/diff-match-patch/wiki/Line-or-Word-Diffs"&gt;Line or Word Diffs&lt;/a&gt; - Less detailed diffs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/diff-match-patch/wiki/Plain-Text-vs.-Structured-Content"&gt;Plain Text vs. Structured Content&lt;/a&gt; - How to deal with data like XML.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/diff-match-patch/wiki/Unidiff"&gt;Unidiff&lt;/a&gt; - The patch serialization format.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://groups.google.com/forum/#!forum/diff-match-patch" rel="nofollow"&gt;Support&lt;/a&gt; - Newsgroup for developers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-languages" class="anchor" aria-hidden="true" href="#languages"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Languages&lt;/h3&gt;
&lt;p&gt;Although each language port of Diff Match Patch uses the same API, there are some language-specific notes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/google/diff-match-patch/wiki/Language:-Cpp"&gt;C++&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/diff-match-patch/wiki/Language:-C%23"&gt;C#&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/diff-match-patch/wiki/Language:-Dart"&gt;Dart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/diff-match-patch/wiki/Language:-Java"&gt;Java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/diff-match-patch/wiki/Language:-JavaScript"&gt;JavaScript&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/diff-match-patch/wiki/Language:-Lua"&gt;Lua&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/diff-match-patch/wiki/Language:-Objective-C"&gt;Objective-C&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/diff-match-patch/wiki/Language:-Python"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A standardized speed test tracks the &lt;a href="https://docs.google.com/spreadsheets/d/1zpZccuBpjMZTvL1nGDMKJc7rWL_m_drF4XKOJvB27Kc/edit#gid=0" rel="nofollow"&gt;relative performance of diffs&lt;/a&gt; in each language.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-algorithms" class="anchor" aria-hidden="true" href="#algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Algorithms&lt;/h3&gt;
&lt;p&gt;This library implements &lt;a href="https://neil.fraser.name/writing/diff/myers.pdf" rel="nofollow"&gt;Myer's diff algorithm&lt;/a&gt; which is generally considered to be the best general-purpose diff. A layer of &lt;a href="https://neil.fraser.name/writing/diff/" rel="nofollow"&gt;pre-diff speedups and post-diff cleanups&lt;/a&gt; surround the diff algorithm, improving both performance and output quality.&lt;/p&gt;
&lt;p&gt;This library also implements a &lt;a href="https://neil.fraser.name/writing/patch/bitap.ps" rel="nofollow"&gt;Bitap matching algorithm&lt;/a&gt; at the heart of a &lt;a href="https://neil.fraser.name/writing/patch/" rel="nofollow"&gt;flexible matching and patching strategy&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google</author><guid isPermaLink="false">https://github.com/google/diff-match-patch</guid><pubDate>Tue, 11 Feb 2020 00:15:00 GMT</pubDate></item><item><title>Spazzlo/folderclone #16 in Python, Today</title><link>https://github.com/Spazzlo/folderclone</link><description>&lt;p&gt;&lt;i&gt;A project that allows you copy large folders to Shared Drives.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-folderclone---a-project-that-allows-you-copy-large-folders-to-shared-drives" class="anchor" aria-hidden="true" href="#folderclone---a-project-that-allows-you-copy-large-folders-to-shared-drives"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;folderclone - A project that allows you copy large folders to Shared Drives.&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;folderclone is available on PyPI, so you can install it using pip.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install folderclone
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-multimanager" class="anchor" aria-hidden="true" href="#multimanager"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;multimanager&lt;/h2&gt;
&lt;p&gt;Multi Manager is the tool that will help you setup everything you need to make folderclone work.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-multi-manager-setup" class="anchor" aria-hidden="true" href="#multi-manager-setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi Manager Setup&lt;/h3&gt;
&lt;p&gt;To set it up, head over to the &lt;a href="https://developers.google.com/drive/api/v3/quickstart/python" rel="nofollow"&gt;Python Quickstart&lt;/a&gt; page and click the Enable the Drive API. Go through the setup and once its done, download the credentials to a new folder on your computer.&lt;/p&gt;
&lt;p&gt;On your terminal, change your directory to that folder you just made and run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;multimanager interactive
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will start multimanager in interactive mode. It'll start by taking you to a login page to authenticate yourself. You'll then be met with a prompt to enable the Service Usage API. Visit the link it provides, enable the API, then go back and press Enter to retry. Don't worry about having to do this every time, this is a one time setup.&lt;/p&gt;
&lt;p&gt;Once it's done, you'll be met with the Multi Manager prompt.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Multi Manager
mm&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You have successfully setup Multi Manager!&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-quick-setup" class="anchor" aria-hidden="true" href="#quick-setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Setup&lt;/h4&gt;
&lt;p&gt;For folderclone, you'll need a few Service Accounts (SAs) ready. To do this in, run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mm&amp;gt; quick-setup N SHARED_DRIVE_ID
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;N&lt;/code&gt; is the amount of projects you'd like to use and &lt;code&gt;SHARED_DRIVE_ID&lt;/code&gt; the ID of the Shared Drive you'd like to copy to.&lt;/p&gt;
&lt;p&gt;For example, say I wanted to copy 100 TB worth of content. I'd need 134 SAs (750 GB each) to do the copy, so 2 projects. I'll be copying to a fresh new Shared Drive who's ID is 0ABCdeyz_ZaMsxxxLGA. I'll be running:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mm&amp;gt; quick-setup 2 0ABCdeyz_ZaMsxxxLGA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will automatically;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create 2 projects&lt;/li&gt;
&lt;li&gt;enable the required services&lt;/li&gt;
&lt;li&gt;create Service Accounts&lt;/li&gt;
&lt;li&gt;add them to the Shared Drive&lt;/li&gt;
&lt;li&gt;and download their credentials into a new folder &lt;code&gt;accounts&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You are now ready to go to the next step.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-multifolderclone" class="anchor" aria-hidden="true" href="#multifolderclone"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;multifolderclone&lt;/h2&gt;
&lt;p&gt;multifoldeclone is the tool that will do all the cloning for you. It is the simplest thing to use.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;multifolderclone -s SOURCE_FOLDER_ID -d DESTINATION_FOLDER_ID
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;SOURCE_FOLDER_ID&lt;/code&gt; is the ID of the folder you'll want to copy (Make sure the source folder is accessible to the service accounts by either making the folder public or sharing the folder with the service accounts you are using to copy with), and &lt;code&gt;DESTINATION_FOLDER_ID&lt;/code&gt; is the ID of the folder you are copying to. This could be the ID of the Shared Drive, or a folder inside the Shared Drive.&lt;/p&gt;
&lt;p&gt;This will automatically start cloning the folder!
And that's it! You did it!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Spazzlo</author><guid isPermaLink="false">https://github.com/Spazzlo/folderclone</guid><pubDate>Tue, 11 Feb 2020 00:16:00 GMT</pubDate></item><item><title>cguZZman/plugin.googledrive #17 in Python, Today</title><link>https://github.com/cguZZman/plugin.googledrive</link><description>&lt;p&gt;&lt;i&gt;The Google Drive addon for Kodi&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-google-drive-kodi-addon" class="anchor" aria-hidden="true" href="#google-drive-kodi-addon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google Drive KODI Addon&lt;/h1&gt;
&lt;p&gt;Play all your media from Google Drive including Videos, Music and Pictures (including Google Photos).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlimited accounts&lt;/li&gt;
&lt;li&gt;Team Drives support&lt;/li&gt;
&lt;li&gt;Google Photos support&lt;/li&gt;
&lt;li&gt;Playback your music and videos. Listing of videos with thumbnails.&lt;/li&gt;
&lt;li&gt;Use Google Drive as a source.&lt;/li&gt;
&lt;li&gt;Subtitles can be assigned automatically if a .str file exists with the same name as the video.&lt;/li&gt;
&lt;li&gt;Export your videos to your library (.strm files). You can export your music too, but kodi won't support it yet. It's a Kodi issue for now.&lt;/li&gt;
&lt;li&gt;Show your photos individually or run a slideshow of them. Listing of pictures with thumbnails.&lt;/li&gt;
&lt;li&gt;Auto-Refreshed slideshow.&lt;/li&gt;
&lt;li&gt;Use of OAuth 2 login. You don't have to write your user/password within the add-on. Use the login process in your browser.&lt;/li&gt;
&lt;li&gt;Extremely fast. Using the Google Drive API&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This program is not affiliated with or sponsored by Google.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;From the &lt;strong&gt;Kodi Add-on repository&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;From &lt;strong&gt;&lt;a href="https://github.com/cguZZman/repository.plugins"&gt;my repository&lt;/a&gt;&lt;/strong&gt; if for any reason the latest version is still not in the Kodi Add-on repository.&lt;/li&gt;
&lt;li&gt;Manual, by downloading the source code and creating your zip.
If your installation is manual, you &lt;strong&gt;must install first&lt;/strong&gt; the latest version of the &lt;strong&gt;&lt;a href="https://github.com/cguZZman/script.module.clouddrive.common"&gt;common module&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>cguZZman</author><guid isPermaLink="false">https://github.com/cguZZman/plugin.googledrive</guid><pubDate>Tue, 11 Feb 2020 00:17:00 GMT</pubDate></item><item><title>LikeLy-Journey/SegmenTron #18 in Python, Today</title><link>https://github.com/LikeLy-Journey/SegmenTron</link><description>&lt;p&gt;&lt;i&gt;Support Fast_SCNN, HRNet, Deeplabv3_plus(xception, resnet, mobilenet), ContextNet, FPENet, DABNet, EdaNet, ENet, Espnetv2, RefineNet, UNet, DANet, HRNet, DFANet, HardNet, LedNet, OCNet, EncNet, DuNet, CGNet, CCNet, BiSeNet, PSPNet, ICNet, FCN, deeplab) &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytorch-for-semantic-segmentation" class="anchor" aria-hidden="true" href="#pytorch-for-semantic-segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch for Semantic Segmentation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-introduce" class="anchor" aria-hidden="true" href="#introduce"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduce&lt;/h2&gt;
&lt;p&gt;This repository contains some models for semantic segmentation and the pipeline of training and testing models,
implemented in PyTorch.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/images/demo.png"&gt;&lt;img src="docs/images/demo.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-zoo" class="anchor" aria-hidden="true" href="#model-zoo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model zoo&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Backbone&lt;/th&gt;
&lt;th align="center"&gt;Datasets&lt;/th&gt;
&lt;th align="center"&gt;eval size&lt;/th&gt;
&lt;th align="center"&gt;Mean IoU(paper)&lt;/th&gt;
&lt;th align="center"&gt;Mean IoU(this repo)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DeepLabv3_plus&lt;/td&gt;
&lt;td align="center"&gt;xception65&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1025,2049)&lt;/td&gt;
&lt;td align="center"&gt;78.8&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/deeplabv3_plus_xception_segmentron.pth"&gt;78.93&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DeepLabv3_plus&lt;/td&gt;
&lt;td align="center"&gt;xception65&lt;/td&gt;
&lt;td align="center"&gt;coco(val)&lt;/td&gt;
&lt;td align="center"&gt;480/520&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/deeplabv3_plus_xception_coco_segmentron.pth"&gt;70.50&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DeepLabv3_plus&lt;/td&gt;
&lt;td align="center"&gt;xception65&lt;/td&gt;
&lt;td align="center"&gt;pascal_aug(val)&lt;/td&gt;
&lt;td align="center"&gt;480/520&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/deeplabv3_plus_xception_pascal_aug_segmentron.pth"&gt;89.56&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DeepLabv3_plus&lt;/td&gt;
&lt;td align="center"&gt;xception65&lt;/td&gt;
&lt;td align="center"&gt;pascal_voc(val)&lt;/td&gt;
&lt;td align="center"&gt;480/520&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/deeplabv3_plus_xception_pascal_voc_segmentron.pth"&gt;88.39&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DeepLabv3_plus&lt;/td&gt;
&lt;td align="center"&gt;resnet101&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1025,2049)&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/deeplabv3_plus_resnet101_segmentron.pth"&gt;78.27&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Danet&lt;/td&gt;
&lt;td align="center"&gt;resnet101&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1024,2048)&lt;/td&gt;
&lt;td align="center"&gt;79.9&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/danet101_segmentron.pth"&gt;79.34&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Pspnet&lt;/td&gt;
&lt;td align="center"&gt;resnet101&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1025,2049)&lt;/td&gt;
&lt;td align="center"&gt;78.63&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/pspnet_resnet101_segmentron.pth"&gt;77.00&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-real-time-models" class="anchor" aria-hidden="true" href="#real-time-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;real-time models&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Backbone&lt;/th&gt;
&lt;th align="center"&gt;Datasets&lt;/th&gt;
&lt;th align="center"&gt;eval size&lt;/th&gt;
&lt;th align="center"&gt;Mean IoU(paper)&lt;/th&gt;
&lt;th align="center"&gt;Mean IoU(this repo)&lt;/th&gt;
&lt;th align="center"&gt;FPS&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;ICnet&lt;/td&gt;
&lt;td align="center"&gt;resnet50(0.5)&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1024,2048)&lt;/td&gt;
&lt;td align="center"&gt;67.8&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;41.39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DeepLabv3_plus&lt;/td&gt;
&lt;td align="center"&gt;mobilenetV2&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1024,2048)&lt;/td&gt;
&lt;td align="center"&gt;70.7&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/deeplabv3_plus_mobilenetv2_segmentron.pth"&gt;70.3&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;46.64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;BiSeNet&lt;/td&gt;
&lt;td align="center"&gt;resnet18&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1024,2048)&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;39.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;LEDNet&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1024,2048)&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;31.78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;CGNet&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1024,2048)&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;46.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;HardNet&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1024,2048)&lt;/td&gt;
&lt;td align="center"&gt;75.9&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;69.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DFANet&lt;/td&gt;
&lt;td align="center"&gt;xceptionA&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1024,2048)&lt;/td&gt;
&lt;td align="center"&gt;70.3&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;21.46&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;HRNet&lt;/td&gt;
&lt;td align="center"&gt;w18_small_v1&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1024,2048)&lt;/td&gt;
&lt;td align="center"&gt;70.3&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/hrnet_w18_small_v1_segmentron.pth"&gt;70.5&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;66.01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Fast_SCNN&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;cityscape(val)&lt;/td&gt;
&lt;td align="center"&gt;(1024,2048)&lt;/td&gt;
&lt;td align="center"&gt;68.3&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/fast_scnn_segmentron.pth"&gt;68.9&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;145.77&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;FPS was tested on V100.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-environments" class="anchor" aria-hidden="true" href="#environments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Environments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;python 3&lt;/li&gt;
&lt;li&gt;torch &amp;gt;= 1.1.0&lt;/li&gt;
&lt;li&gt;torchvision&lt;/li&gt;
&lt;li&gt;pyyaml&lt;/li&gt;
&lt;li&gt;Pillow&lt;/li&gt;
&lt;li&gt;numpy&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-install" class="anchor" aria-hidden="true" href="#install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;INSTALL&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;python setup.py develop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;if you do not want to run CCNet, you do not need to install, just comment following line in &lt;code&gt;segmentron/models/__init__.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from .ccnet import CCNet
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-dataset-prepare" class="anchor" aria-hidden="true" href="#dataset-prepare"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dataset prepare&lt;/h2&gt;
&lt;p&gt;Support cityscape, coco, voc, ade20k now.&lt;/p&gt;
&lt;p&gt;Please refer to &lt;a href="docs/DATA_PREPARE.md"&gt;DATA_PREPARE.md&lt;/a&gt; for dataset preparation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pretrained-backbone-models" class="anchor" aria-hidden="true" href="#pretrained-backbone-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained backbone models&lt;/h2&gt;
&lt;p&gt;pretrained backbone models will be download automatically in pytorch default directory(&lt;code&gt;~/.cache/torch/checkpoints/&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-code-structure" class="anchor" aria-hidden="true" href="#code-structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code structure&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;├── configs    # yaml config file
├── segmentron # core code
├── tools      # train eval code
└── datasets   # put datasets here 
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-train" class="anchor" aria-hidden="true" href="#train"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Train&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-train-with-a-single-gpu" class="anchor" aria-hidden="true" href="#train-with-a-single-gpu"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Train with a single GPU&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 python -u tools/train.py --config-file configs/cityscapes_deeplabv3_plus.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-train-with-multiple-gpus" class="anchor" aria-hidden="true" href="#train-with-multiple-gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Train with multiple GPUs&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 ./tools/dist_train.sh ${CONFIG_FILE} ${GPU_NUM} [optional arguments]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-eval" class="anchor" aria-hidden="true" href="#eval"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eval&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-eval-with-a-single-gpu" class="anchor" aria-hidden="true" href="#eval-with-a-single-gpu"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eval with a single GPU&lt;/h3&gt;
&lt;p&gt;You can download trained model from model zoo table above, or train by yourself.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 python -u ./tools/eval.py --config-file configs/cityscapes_deeplabv3_plus.yaml \
TEST.TEST_MODEL_PATH your_test_model_path

&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-eval-with-a-multiple-gpus" class="anchor" aria-hidden="true" href="#eval-with-a-multiple-gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eval with a multiple GPUs&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 ./tools/dist_test.sh ${CONFIG_FILE} ${GPU_NUM} \
TEST.TEST_MODEL_PATH your_test_model_path
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanghang1989/PyTorch-Encoding"&gt;PyTorch-Encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/detectron2"&gt;detectron2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dmlc/gluon-cv"&gt;gloun-cv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>LikeLy-Journey</author><guid isPermaLink="false">https://github.com/LikeLy-Journey/SegmenTron</guid><pubDate>Tue, 11 Feb 2020 00:18:00 GMT</pubDate></item><item><title>EvilCult/iptv-m3u-maker #19 in Python, Today</title><link>https://github.com/EvilCult/iptv-m3u-maker</link><description>&lt;p&gt;&lt;i&gt;IPTV 国内+国外 电视台直播源m3u文件, 收集&amp;汇总脚本,目前状况: 收录频道总数:4523, 优质频道数:已放出优质频道总是: 687&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-iptv-m3u-直播源-收集--汇总" class="anchor" aria-hidden="true" href="#iptv-m3u-直播源-收集--汇总"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;IPTV m3u 直播源 收集 &amp;amp; 汇总&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://evilcult.dev/07/19/2019/IPTV-Projects/" rel="nofollow"&gt;项目详细说明&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://evilcult.dev/tags/iptv-m3u-maker/" rel="nofollow"&gt;项目更新记录&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简介&lt;/h2&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-主要功能" class="anchor" aria-hidden="true" href="#主要功能"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;主要功能&lt;/h3&gt;
&lt;p&gt;收集网络上现有的一些网友共享的直播源, 将其汇总后.&lt;/p&gt;
&lt;p&gt;对每个连接进行测试, 同时记录当前网络对该连接的延迟, 同时对其标题进行一定的格式化.&lt;/p&gt;
&lt;p&gt;最终, 针对当前网络生成一份可用的, 同类速度最优的 “播放列表”.&lt;/p&gt;
&lt;p&gt;将其输出为 &lt;strong&gt;m3u&lt;/strong&gt; 文件&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-目前进度" class="anchor" aria-hidden="true" href="#目前进度"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目前进度&lt;/h3&gt;
&lt;p&gt;目前库存频道总数: 4523&lt;/p&gt;
&lt;p&gt;已放出优质频道总是: 687&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;!!! 项目持续更新中 !!!&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-数据来源" class="anchor" aria-hidden="true" href="#数据来源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数据来源&lt;/h2&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.jianshu.com/p/2499255c7e79" rel="nofollow"&gt;https://www.jianshu.com/p/2499255c7e79&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;V2ex - &lt;a href="https://www.v2ex.com/member/Dotpy" rel="nofollow"&gt;Dotpy&lt;/a&gt; 提供1600+可用播放源&lt;/li&gt;
&lt;li&gt;&lt;a href="http://iptv807.com/" rel="nofollow"&gt;http://iptv807.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更多来源还在考虑要不要加入....&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-项目使用方法" class="anchor" aria-hidden="true" href="#项目使用方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目使用方法&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;本项目基于 &lt;strong&gt;python3.7&lt;/strong&gt; 进行开发&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/EvilCult/iptv-m3u-maker.git
cd iptv-m3u-maker
python main.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;人生苦短, 我用Docker&lt;/p&gt;
&lt;p&gt;建议以&lt;a href="https://www.docker.com/" rel="nofollow"&gt;Docker&lt;/a&gt; 的方式,直接在路由器上运行,本地检测地址访问,更为精准.&lt;/p&gt;
&lt;p&gt;两行命令构建运行环境&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker pull python:3.7
docker run -it --name python3 -v {脚本所在滤镜}:{容器里随便你想要的路径} python:3.7
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-其他" class="anchor" aria-hidden="true" href="#其他"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;其他&lt;/h2&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-相关项目" class="anchor" aria-hidden="true" href="#相关项目"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;相关项目&lt;/h3&gt;
&lt;p&gt;「&lt;a href="https://github.com/EvilCult/iptv-m3u-player"&gt;iptv-m3u-player&lt;/a&gt;」 - 基于本项目的衍生项目, 基于Electron+React编写的一个轻量级桌面客户端.频道数据会随本项目更新.
Mac上不知道用什么客户端的,可以试试.&lt;/p&gt;
&lt;p&gt;Android TV 请使用 &lt;a href="https://kodi.tv/" rel="nofollow"&gt;Kodi&lt;/a&gt; + ‘PVR IPTV Simple Client’&lt;/p&gt;
&lt;p&gt;iOS 请使用 Cloud Stream&lt;/p&gt;
&lt;p&gt;PC 我就不太了解了....&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-已知问题" class="anchor" aria-hidden="true" href="#已知问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;已知问题&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;访问速度慢,视频卡顿
&lt;ul&gt;
&lt;li&gt;解决方案: 不要直接引用项目中的tv.m3u8文件,clone项目到本地,在本地网络环境下执行项目,生成新的文件&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;电视,广播未分开
&lt;ul&gt;
&lt;li&gt;暂时未处理,会v在后续版本进行分类&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;程序运行报错
&lt;ul&gt;
&lt;li&gt;可能是部分 分享源 网站服务当机... 可自行注释部分代码 or 提交 &lt;a href="https://github.com/EvilCult/iptv-m3u-maker/issues"&gt;issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>EvilCult</author><guid isPermaLink="false">https://github.com/EvilCult/iptv-m3u-maker</guid><pubDate>Tue, 11 Feb 2020 00:19:00 GMT</pubDate></item><item><title>SpiderClub/haipproxy #20 in Python, Today</title><link>https://github.com/SpiderClub/haipproxy</link><description>&lt;p&gt;&lt;i&gt;:sparkling_heart: High available distributed ip proxy pool, powerd by Scrapy and Redis&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-高可用ip代理池" class="anchor" aria-hidden="true" href="#高可用ip代理池"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;高可用IP代理池&lt;/h1&gt;
&lt;p&gt;&lt;a href="README_EN.md"&gt;README&lt;/a&gt;　｜　&lt;a href="README.md"&gt;中文文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本项目所采集的IP资源都来自互联网，愿景是为大型爬虫项目提供一个&lt;strong&gt;高可用低延迟的高匿IP代理池&lt;/strong&gt;。&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-项目亮点" class="anchor" aria-hidden="true" href="#项目亮点"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目亮点&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;代理来源丰富&lt;/li&gt;
&lt;li&gt;代理抓取提取精准&lt;/li&gt;
&lt;li&gt;代理校验严格合理&lt;/li&gt;
&lt;li&gt;监控完备，鲁棒性强&lt;/li&gt;
&lt;li&gt;架构灵活，便于扩展&lt;/li&gt;
&lt;li&gt;各个组件分布式部署&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-快速开始" class="anchor" aria-hidden="true" href="#快速开始"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;快速开始&lt;/h1&gt;
&lt;p&gt;注意，代码请在&lt;a href="https://github.com/SpiderClub/haipproxy/releases"&gt;release&lt;/a&gt;列表中下载，&lt;strong&gt;master&lt;/strong&gt;分支的代码不保证能稳定运行&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-单机部署" class="anchor" aria-hidden="true" href="#单机部署"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;单机部署&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-服务端" class="anchor" aria-hidden="true" href="#服务端"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;服务端&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;安装Python3和Redis。有问题可以阅读&lt;a href="https://github.com/SpiderClub/weibospider/wiki/%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"&gt;这篇文章&lt;/a&gt;的相关部分。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;根据Redis的实际配置修改项目配置文件&lt;a href="config/settings.py"&gt;config/settings.py&lt;/a&gt;中的&lt;code&gt;REDIS_HOST&lt;/code&gt;、&lt;code&gt;REDIS_PASSWORD&lt;/code&gt;等参数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装&lt;a href="https://github.com/scrapy-plugins/scrapy-splash"&gt;scrapy-splash&lt;/a&gt;，并修改配置文件&lt;a href="config/settings.py"&gt;config/settings.py&lt;/a&gt;中的&lt;code&gt;SPLASH_URL&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装项目相关依赖&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install -r requirements.txt&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动&lt;em&gt;scrapy worker&lt;/em&gt;，包括代理IP采集器和校验器&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;python crawler_booter.py --usage crawler&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;python crawler_booter.py --usage validator&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动&lt;em&gt;调度器&lt;/em&gt;，包括代理IP定时调度和校验&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;python scheduler_booter.py --usage crawler&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;python scheduler_booter.py --usage validator&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-客户端" class="anchor" aria-hidden="true" href="#客户端"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;客户端&lt;/h3&gt;
&lt;p&gt;近日不断有同学问，如何获取该项目中可用的代理IP列表。&lt;code&gt;haipproxy&lt;/code&gt;提供代理的方式并不是通过&lt;code&gt;api api&lt;/code&gt;来提供，而是通过具体的客户端来提供。
目前支持的是&lt;a href="client/py_cli.py"&gt;Python客户端&lt;/a&gt;和语言无关的&lt;a href="client/squid.py"&gt;squid二级代理&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-python客户端调用示例" class="anchor" aria-hidden="true" href="#python客户端调用示例"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python客户端调用示例&lt;/h4&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; client.py_cli &lt;span class="pl-k"&gt;import&lt;/span&gt; ProxyFetcher
args &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;dict&lt;/span&gt;(&lt;span class="pl-v"&gt;host&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;127.0.0.1&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;port&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;6379&lt;/span&gt;, &lt;span class="pl-v"&gt;password&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;123456&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;db&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
＃　这里&lt;span class="pl-bu"&gt;`zhihu`&lt;/span&gt;的意思是，去和&lt;span class="pl-bu"&gt;`zhihu`&lt;/span&gt;相关的代理ip校验队列中获取ip
＃　这么做的原因是同一个代理&lt;span class="pl-c1"&gt;IP&lt;/span&gt;对不同网站代理效果不同
fetcher &lt;span class="pl-k"&gt;=&lt;/span&gt; ProxyFetcher(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;zhihu&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;strategy&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;greedy&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;redis_args&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;args)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 获取一个可用代理&lt;/span&gt;
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(fetcher.get_proxy())
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 获取可用代理列表&lt;/span&gt;
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(fetcher.get_proxies()) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; or print(fetcher.pool)&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;更具体的示例见&lt;a href="examples/zhihu/zhihu_spider.py"&gt;examples/zhihu&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-squid作为二级代理" class="anchor" aria-hidden="true" href="#squid作为二级代理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;squid作为二级代理&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;安装squid，备份squid的配置文件并且启动squid，以ubuntu为例&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;sudo apt-get install squid&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;sudo sed -i 's/http_access deny all/http_access allow all/g' /etc/squid/squid.conf&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;sudo cp /etc/squid/squid.conf /etc/squid/squid.conf.backup&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;sudo service squid start&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;根据操作系统修改项目配置文件&lt;a href="config/settings.py"&gt;config/settings.py&lt;/a&gt;中的&lt;code&gt;SQUID_BIN_PATH&lt;/code&gt;、&lt;code&gt;SQUID_CONF_PATH&lt;/code&gt;、&lt;code&gt;SQUID_TEMPLATE_PATH&lt;/code&gt;等参数&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动&lt;code&gt;squid conf&lt;/code&gt;的定时更新程序&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;sudo python squid_update.py&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用squid作为代理中间层请求目标网站,默认代理URL为'http://squid_host:3128',用Python请求示例如下&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; requests
proxies &lt;span class="pl-k"&gt;=&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;http://127.0.0.1:3128&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;}
resp &lt;span class="pl-k"&gt;=&lt;/span&gt; requests.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https://httpbin.org/ip&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;proxies&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;proxies)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(resp.text)&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-docker部署" class="anchor" aria-hidden="true" href="#docker部署"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;安装Docker&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装&lt;em&gt;docker-compose&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install -U docker-compose&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改&lt;a href="config/settings.py"&gt;settings.py&lt;/a&gt;中的&lt;code&gt;SPLASH_URL&lt;/code&gt;和&lt;code&gt;REDIS_HOST&lt;/code&gt;参数&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 注意，如果您使用master分支下的代码，这步可被省略&lt;/span&gt;
&lt;span class="pl-c1"&gt;SPLASH_URL&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;http://splash:8050&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c1"&gt;REDIS_HOST&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;redis&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用&lt;em&gt;docker-compose&lt;/em&gt;启动各个应用组件&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;docker-compose up&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这种方式会一同部署&lt;code&gt;squid&lt;/code&gt;，您可以通过&lt;code&gt;squid&lt;/code&gt;调用代理IP池，也可以使用客户端调用，和单机部署调用方式一样&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-注意事项" class="anchor" aria-hidden="true" href="#注意事项"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;注意事项&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;本项目高度依赖Redis，除了消息通信和数据存储之外，IP校验和任务定时工具也使用了Redis中的多种数据结构。
如果需要替换Redis，请自行度量&lt;/li&gt;
&lt;li&gt;由于&lt;em&gt;GFW&lt;/em&gt;的原因，某些网站需要通过科学上网才能进行访问和采集，如果用户无法访问墙外的网站，请将&lt;a href="config/rules.py"&gt;rules.py&lt;/a&gt;
&lt;code&gt;task_queue&lt;/code&gt;为&lt;code&gt; SPIDER_GFW_TASK&lt;/code&gt;和&lt;code&gt;SPIDER_AJAX_GFW_TASK&lt;/code&gt;的任务&lt;code&gt;enable&lt;/code&gt;属性设置为0或者启动爬虫的时候指定爬虫类型为&lt;code&gt;common&lt;/code&gt;和
&lt;code&gt;ajax&lt;/code&gt;
&lt;blockquote&gt;
&lt;p&gt;python crawler_booter.py --usage crawler common ajax&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;相同代理IP，对于不同网站的代理效果可能大不相同。如果通用代理无法满足您的需求，您可以&lt;a href="https://github.com/SpiderClub/haipproxy/blob/master/docs/%E9%92%88%E5%AF%B9%E7%89%B9%E5%AE%9A%E7%AB%99%E7%82%B9%E6%B7%BB%E5%8A%A0%E6%A0%A1%E9%AA%8C%E5%99%A8.md"&gt;为特定网站编写代理IP校验器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-工作流程" class="anchor" aria-hidden="true" href="#工作流程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;工作流程&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="static/workflow.png"&gt;&lt;img src="static/workflow.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-效果测试" class="anchor" aria-hidden="true" href="#效果测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;效果测试&lt;/h1&gt;
&lt;p&gt;以单机模式部署&lt;code&gt;haipproxy&lt;/code&gt;和&lt;a href="examples/zhihu/zhihu_spider.py"&gt;测试代码&lt;/a&gt;，以知乎为目标请求站点，实测抓取效果如下&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./static/zhihu.png"&gt;&lt;img src="./static/zhihu.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;测试代码见&lt;a href="examples/zhihu/zhihu_spider.py"&gt;examples/zhihu&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-项目监控可选" class="anchor" aria-hidden="true" href="#项目监控可选"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目监控(可选)&lt;/h1&gt;
&lt;p&gt;项目监控主要通过&lt;a href="https://sentry.io/welcome/" rel="nofollow"&gt;sentry&lt;/a&gt;和&lt;a href="https://prometheus.io/" rel="nofollow"&gt;prometheus&lt;/a&gt;,通过在关键地方
进行业务埋点对项目各个维度进行监测，以提高项目的鲁棒性&lt;/p&gt;
&lt;p&gt;项目使用&lt;a href="https://sentry.io/welcome/" rel="nofollow"&gt;Sentry&lt;/a&gt;作&lt;code&gt;Bug Trace&lt;/code&gt;工具，通过Sentry可以很容易跟踪项目健康情况&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./static/bug_trace.jpg"&gt;&lt;img src="./static/bug_trace.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用&lt;a href="https://prometheus.io/" rel="nofollow"&gt;Prometheus&lt;/a&gt;+&lt;a href="https://grafana.com/" rel="nofollow"&gt;Grafana&lt;/a&gt;做业务监控，了解项目当前状态&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./static/monitor.png"&gt;&lt;img src="./static/monitor.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-捐赠作者" class="anchor" aria-hidden="true" href="#捐赠作者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;捐赠作者&lt;/h1&gt;
&lt;p&gt;开源不易，如果本项目对您有用，不妨进行小额捐赠，以支持项目的持续维护&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./static/donate.jpg"&gt;&lt;img src="./static/donate.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-同类项目" class="anchor" aria-hidden="true" href="#同类项目"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;同类项目&lt;/h1&gt;
&lt;p&gt;本项目参考了Github上开源的各个爬虫代理的实现，感谢他们的付出，下面是笔者参考的所有项目，排名不分先后。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/virjar/dungproxy"&gt;dungproxy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/zhangchenchen/proxyspider"&gt;proxyspider&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/henson/ProxyPool"&gt;ProxyPool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/jhao104/proxy_pool"&gt;proxy_pool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/WiseDoge/ProxyPool"&gt;ProxyPool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/awolfly9/IPProxyTool"&gt;IPProxyTool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/qiyeboy/IPProxyPool"&gt;IPProxyPool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/gavin66/proxy_list"&gt;proxy_list&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/lujqme/proxy_pool"&gt;proxy_pool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/fengzhizi715/ProxyPool"&gt;ProxyPool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/imWildCat/scylla"&gt;scylla&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>SpiderClub</author><guid isPermaLink="false">https://github.com/SpiderClub/haipproxy</guid><pubDate>Tue, 11 Feb 2020 00:20:00 GMT</pubDate></item><item><title>CorentinJ/Real-Time-Voice-Cloning #21 in Python, Today</title><link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link><description>&lt;p&gt;&lt;i&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-real-time-voice-cloning" class="anchor" aria-hidden="true" href="#real-time-voice-cloning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Real-Time Voice Cloning&lt;/h1&gt;
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;Transfer Learning from Speaker Verification to
Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. Feel free to check &lt;a href="https://matheo.uliege.be/handle/2268.2/6801" rel="nofollow"&gt;my thesis&lt;/a&gt; if you're curious or if you're looking for info I haven't documented yet (don't hesitate to make an issue for that too). Mostly I would recommend giving a quick look to the figures beyond the introduction.&lt;/p&gt;
&lt;p&gt;SV2TTS is a three-stage deep learning framework that allows to create a numerical representation of a voice from a few seconds of audio, and to use it to condition a text-to-speech model trained to generalize to new voices.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9c33f78be8afe656503da974c478ea2ba2647db7/68747470733a2f2f692e696d6775722e636f6d2f386c46556c677a2e706e67" alt="Toolbox demo" data-canonical-src="https://i.imgur.com/8lFUlgz.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-papers-implemented" class="anchor" aria-hidden="true" href="#papers-implemented"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Papers implemented&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Designation&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Implementation source&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf" rel="nofollow"&gt;1802.08435&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;WaveRNN (vocoder)&lt;/td&gt;
&lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1712.05884.pdf" rel="nofollow"&gt;1712.05884&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tacotron 2 (synthesizer)&lt;/td&gt;
&lt;td&gt;Natural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Rayhane-mamah/Tacotron-2"&gt;Rayhane-mamah/Tacotron-2&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf" rel="nofollow"&gt;1710.10467&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GE2E (encoder)&lt;/td&gt;
&lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-news" class="anchor" aria-hidden="true" href="#news"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;News&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;13/11/19&lt;/strong&gt;: I'm sorry that I can't maintain this repo as much as I wish I could. I'm working full time on improving voice cloning techniques and I don't have the time to share my improvements here. Plus this repo relies on a lot of old tensorflow code and it's hard to work with. If you're a researcher, then this repo might be of use to you. &lt;strong&gt;If you just want to clone your voice&lt;/strong&gt;, do check our demo on &lt;a href="https://www.resemble.ai/" rel="nofollow"&gt;Resemble.AI&lt;/a&gt; - it can run for free but it will be a bit slower, and it will give much better results than this repo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;20/08/19:&lt;/strong&gt; I'm working on &lt;a href="https://github.com/resemble-ai/Resemblyzer"&gt;resemblyzer&lt;/a&gt;, an independent package for the voice encoder. You can use your trained encoder models from this repo with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;06/07/19:&lt;/strong&gt; Need to run within a docker container on a remote server? See &lt;a href="https://sean.lane.sh/posts/2019/07/Running-the-Real-Time-Voice-Cloning-project-in-Docker/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;25/06/19:&lt;/strong&gt; Experimental support for low-memory GPUs (~2gb) added for the synthesizer. Pass &lt;code&gt;--low_mem&lt;/code&gt; to &lt;code&gt;demo_cli.py&lt;/code&gt; or &lt;code&gt;demo_toolbox.py&lt;/code&gt; to enable it. It adds a big overhead, so it's not recommended if you have enough VRAM.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h3&gt;
&lt;p&gt;You will need the following whether you plan to use the toolbox only or to retrain the models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Python 3.7&lt;/strong&gt;. Python 3.6 might work too, but I wouldn't go lower because I make extensive use of pathlib.&lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to install the necessary packages. Additionally you will need &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;PyTorch&lt;/a&gt; (&amp;gt;=1.0.1).&lt;/p&gt;
&lt;p&gt;A GPU is mandatory, but you don't necessarily need a high tier GPU if you only want to use the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pretrained-models" class="anchor" aria-hidden="true" href="#pretrained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained models&lt;/h3&gt;
&lt;p&gt;Download the latest &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-preliminary" class="anchor" aria-hidden="true" href="#preliminary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preliminary&lt;/h3&gt;
&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If all tests pass, you're good to go.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h3&gt;
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="http://www.openslr.org/resources/12/train-clean-100.tar.gz" rel="nofollow"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-toolbox" class="anchor" aria-hidden="true" href="#toolbox"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Toolbox&lt;/h3&gt;
&lt;p&gt;You can then try the toolbox:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br&gt;
or&lt;br&gt;
&lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wiki" class="anchor" aria-hidden="true" href="#wiki"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wiki&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How it all works&lt;/strong&gt; (WIP - &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/How-it-all-works"&gt;stub&lt;/a&gt;, you might be better off reading my thesis until it's done)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training"&gt;&lt;strong&gt;Training models yourself&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training with other data/languages&lt;/strong&gt; (WIP - see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-507864097"&gt;here&lt;/a&gt; for now)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/TODO-&amp;amp;-planned-features"&gt;&lt;strong&gt;TODO and planned features&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributions--issues" class="anchor" aria-hidden="true" href="#contributions--issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions &amp;amp; Issues&lt;/h2&gt;
&lt;p&gt;I'm working full-time as of June 2019. I don't have time to maintain this repo nor reply to issues. Sorry.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CorentinJ</author><guid isPermaLink="false">https://github.com/CorentinJ/Real-Time-Voice-Cloning</guid><pubDate>Tue, 11 Feb 2020 00:21:00 GMT</pubDate></item><item><title>sherlock-project/sherlock #22 in Python, Today</title><link>https://github.com/sherlock-project/sherlock</link><description>&lt;p&gt;&lt;i&gt;🔎 Hunt down social media accounts by username across social networks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/27065646/53551960-ae4dff80-3b3a-11e9-9075-cef786c69364.png"&gt;&lt;img src="https://user-images.githubusercontent.com/27065646/53551960-ae4dff80-3b3a-11e9-9075-cef786c69364.png" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;span&gt;Hunt down social media accounts by username across &lt;a href="https://github.com/theyahya/sherlock/blob/master/sites.md"&gt;social networks&lt;/a&gt;&lt;/span&gt;
  &lt;br&gt;
  &lt;a href="https://www.python.org/downloads/" title="Python version" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/392b343efc8b7ac39cdb7fd56d19a8ca6792c12b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d2533453d5f332e362d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/python-%3E=_3.6-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="LICENSE" title="License: MIT"&gt;&lt;img src="https://camo.githubusercontent.com/311762166ef25238116d3cadd22fcb6091edab98/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667" data-canonical-src="https://img.shields.io/badge/License-MIT-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://travis-ci.com/TheYahya/sherlock/" title="Build Status" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a93b7b21f175cd07941d299809dbda32764d2cff/68747470733a2f2f7472617669732d63692e636f6d2f54686559616879612f736865726c6f636b2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.com/TheYahya/sherlock.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://twitter.com/intent/tweet?text=%F0%9F%94%8E%20Find%20usernames%20across%20social%20networks%20&amp;amp;url=https://github.com/sherlock-project/sherlock&amp;amp;hashtags=hacking,%20osint,%20bugbounty,%20reconnaissance" title="Share on Tweeter" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/83d4084f7b71558e33b08844da5c773a8657e271/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c" data-canonical-src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="http://sherlock-project.github.io/" rel="nofollow"&gt;&lt;img alt="Website" src="https://camo.githubusercontent.com/adc37cc0993bd76eb6e4a6e661146251f8b663af/68747470733a2f2f696d672e736869656c64732e696f2f776562736974652d75702d646f776e2d677265656e2d7265642f687474702f736865726c6f636b2d70726f6a6563742e6769746875622e696f2f2e2e737667" data-canonical-src="https://img.shields.io/website-up-down-green-red/http/sherlock-project.github.io/..svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://microbadger.com/images/theyahya/sherlock" rel="nofollow"&gt;&lt;img alt="docker image" src="https://camo.githubusercontent.com/7369ca4d589232865f5ff69b001ba2794474a285/68747470733a2f2f696d616765732e6d6963726f6261646765722e636f6d2f6261646765732f76657273696f6e2f74686579616879612f736865726c6f636b2e737667" data-canonical-src="https://images.microbadger.com/badges/version/theyahya/sherlock.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="#demo"&gt;Demo&lt;/a&gt;
     |   
  &lt;a href="#installation"&gt;Installation&lt;/a&gt;
     |   
  &lt;a href="#usage"&gt;Usage&lt;/a&gt;
     |   
  &lt;a href="#docker-notes"&gt;Docker Notes&lt;/a&gt;
     |   
  &lt;a href="#adding-new-sites"&gt;Adding New Sites&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;a href="https://asciinema.org/a/223115" rel="nofollow"&gt;
&lt;img src="./images/sherlock_preview.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-demo" class="anchor" aria-hidden="true" href="#demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Demo&lt;/h2&gt;
&lt;p&gt;Use this link to test Sherlock directly in your browser:
&lt;a href="https://elody.com/scenario/plan/16/" rel="nofollow"&gt;https://elody.com/scenario/plan/16/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Python 3.6 or higher is required.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; clone the repo&lt;/span&gt;
$ git clone https://github.com/sherlock-project/sherlock.git

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; change the working directory to sherlock&lt;/span&gt;
$ &lt;span class="pl-c1"&gt;cd&lt;/span&gt; sherlock

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install python3 and python3-pip if they are not installed&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install the requirements&lt;/span&gt;
$ python3 -m pip install -r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/sherlock-project/sherlock&amp;amp;tutorial=README.md" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f4397884ebe7c8fac1cfd4433c6c78455b24a4f0/68747470733a2f2f677374617469632e636f6d2f636c6f75647373682f696d616765732f6f70656e2d62746e2e706e67" alt="Open in Cloud Shell" data-canonical-src="https://gstatic.com/cloudssh/images/open-btn.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python3 sherlock.py --help
usage: sherlock.py [-h] [--version] [--verbose] [--rank]
                   [--folderoutput FOLDEROUTPUT] [--output OUTPUT] [--tor]
                   [--unique-tor] [--csv] [--site SITE_NAME]
                   [--proxy PROXY_URL] [--json JSON_FILE]
                   [--proxy_list PROXY_LIST] [--check_proxies CHECK_PROXY]
                   [--timeout TIMEOUT] [--print-found]
                   USERNAMES [USERNAMES ...]

Sherlock: Find Usernames Across Social Networks (Version 0.10.3)

positional arguments:
  USERNAMES             One or more usernames to check with social networks.

optional arguments:
  -h, --help            show this &lt;span class="pl-c1"&gt;help&lt;/span&gt; message and &lt;span class="pl-c1"&gt;exit&lt;/span&gt;
  --version             Display version information and dependencies.
  --verbose, -v, -d, --debug
                        Display extra debugging information and metrics.
  --rank, -r            Present websites ordered by their Alexa.com global
                        rank &lt;span class="pl-k"&gt;in&lt;/span&gt; popularity.
  --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT
                        If using multiple usernames, the output of the results
                        will be saved to this folder.
  --output OUTPUT, -o OUTPUT
                        If using single username, the output of the result
                        will be saved to this file.
  --tor, -t             Make requests over Tor&lt;span class="pl-k"&gt;;&lt;/span&gt; increases runtime&lt;span class="pl-k"&gt;;&lt;/span&gt; requires
                        Tor to be installed and &lt;span class="pl-k"&gt;in&lt;/span&gt; system path.
  --unique-tor, -u      Make requests over Tor with new Tor circuit after each
                        request&lt;span class="pl-k"&gt;;&lt;/span&gt; increases runtime&lt;span class="pl-k"&gt;;&lt;/span&gt; requires Tor to be
                        installed and &lt;span class="pl-k"&gt;in&lt;/span&gt; system path.
  --csv                 Create Comma-Separated Values (CSV) File.
  --site SITE_NAME      Limit analysis to just the listed sites. Add multiple
                        options to specify more than one site.
  --proxy PROXY_URL, -p PROXY_URL
                        Make requests over a proxy. e.g.
                        socks5://127.0.0.1:1080
  --json JSON_FILE, -j JSON_FILE
                        Load data from a JSON file or an online, valid, JSON
                        file.
  --proxy_list PROXY_LIST, -pl PROXY_LIST
                        Make requests over a proxy randomly chosen from a list
                        generated from a .csv file.
  --check_proxies CHECK_PROXY, -cp CHECK_PROXY
                        To be used with the &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;--proxy_list&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; parameter. The
                        script will check &lt;span class="pl-k"&gt;if&lt;/span&gt; the proxies supplied &lt;span class="pl-k"&gt;in&lt;/span&gt; the .csv
                        file are working and anonymous.Put 0 &lt;span class="pl-k"&gt;for&lt;/span&gt; no limit on
                        successfully checked proxies, or another number to
                        institute a limit.
  --timeout TIMEOUT     Time (in seconds) to &lt;span class="pl-c1"&gt;wait&lt;/span&gt; &lt;span class="pl-k"&gt;for&lt;/span&gt; response to requests.
                        Default timeout of 60.0s.A longer timeout will be more
                        likely to get results from slow sites.On the other
                        hand, this may cause a long delay to gather all
                        results.
  --print-found         Do not output sites where the username was not found.&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To search for only one user:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python3 sherlock.py user123
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To search for more than one user:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python3 sherlock.py user1 user2 user3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Accounts found will be stored in an individual text file with the corresponding username (e.g &lt;code&gt;user123.txt&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-anaconda-windows-notes" class="anchor" aria-hidden="true" href="#anaconda-windows-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Anaconda (Windows) Notes&lt;/h2&gt;
&lt;p&gt;If you are using Anaconda in Windows, using 'python3' might not work. Use 'python' instead.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-docker-notes" class="anchor" aria-hidden="true" href="#docker-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker Notes&lt;/h2&gt;
&lt;p&gt;If docker is installed you can build an image and run this as a container.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build -t mysherlock-image .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the image is built, sherlock can be invoked by running the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -t mysherlock-image user123
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The optional &lt;code&gt;--rm&lt;/code&gt; flag removes the container filesystem on completion to prevent cruft build-up. See: &lt;a href="https://docs.docker.com/engine/reference/run/#clean-up---rm" rel="nofollow"&gt;https://docs.docker.com/engine/reference/run/#clean-up---rm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The optional &lt;code&gt;-t&lt;/code&gt; flag allocates a pseudo-TTY which allows colored output. See: &lt;a href="https://docs.docker.com/engine/reference/run/#foreground" rel="nofollow"&gt;https://docs.docker.com/engine/reference/run/#foreground&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Use the following command to access the saved results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -t -v "$PWD/results:/opt/sherlock/results" mysherlock-image -o /opt/sherlock/results/text.txt user123
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;-v "$PWD/results:/opt/sherlock/results"&lt;/code&gt; option tells docker to create (or use) the folder &lt;code&gt;results&lt;/code&gt; in the
present working directory and to mount it at &lt;code&gt;/opt/sherlock/results&lt;/code&gt; on the docker container.
The &lt;code&gt;-o /opt/sherlock/results/text.txt&lt;/code&gt; option tells &lt;code&gt;sherlock&lt;/code&gt; to output the result.&lt;/p&gt;
&lt;p&gt;Or you can use "Docker Hub" to run &lt;code&gt;sherlock&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run theyahya/sherlock user123
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-using-docker-compose" class="anchor" aria-hidden="true" href="#using-docker-compose"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using &lt;code&gt;docker-compose&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;You can use the &lt;code&gt;docker-compose.yml&lt;/code&gt; file from the repository and use this command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker-compose run sherlock -o /opt/sherlock/results/text.txt user123
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-adding-new-sites" class="anchor" aria-hidden="true" href="#adding-new-sites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding New Sites&lt;/h2&gt;
&lt;p&gt;Please look at the Wiki entry on
&lt;a href="https://github.com/TheYahya/sherlock/wiki/Adding-Sites-To-Sherlock"&gt;adding new sites&lt;/a&gt;
to understand the issues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Sherlock is not accepting adult sites in the standard list.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h2&gt;
&lt;p&gt;Thank you for contributing to Sherlock!&lt;/p&gt;
&lt;p&gt;Before creating a pull request with new development, please run the tests
to ensure that everything is working great.  It would also be a good idea to run the tests
before starting development to distinguish problems between your
environment and the Sherlock software.&lt;/p&gt;
&lt;p&gt;The following is an example of the command line to run all the tests for
Sherlock.  This invocation hides the progress text that Sherlock normally
outputs, and instead shows the verbose output of the tests.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m unittest tests.all --buffer --verbose
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we do currently have 100% test coverage.  Unfortunately, some of
the sites that Sherlock checks are not always reliable, so it is common
to get response errors.&lt;/p&gt;
&lt;p&gt;If some sites are failing due to conection problems (site is down, in maintainence, etc)
you can exclude them from tests by creating a &lt;code&gt;tests/.excluded_sites&lt;/code&gt; file with a
list of sites to ignore (one site name per line).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-stargazers-over-time" class="anchor" aria-hidden="true" href="#stargazers-over-time"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stargazers over time&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://starcharts.herokuapp.com/TheYahya/sherlock" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9a700fddd7ae13c41a23e7ebd1b5eaa40a1bb549/68747470733a2f2f737461726368617274732e6865726f6b756170702e636f6d2f54686559616879612f736865726c6f636b2e737667" alt="Stargazers over time" data-canonical-src="https://starcharts.herokuapp.com/TheYahya/sherlock.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;MIT © Sherlock Project&lt;br&gt;
Original Creator - &lt;a href="https://github.com/sdushantha"&gt;Siddharth Dushantha&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sherlock-project</author><guid isPermaLink="false">https://github.com/sherlock-project/sherlock</guid><pubDate>Tue, 11 Feb 2020 00:22:00 GMT</pubDate></item><item><title>cython/cython #23 in Python, Today</title><link>https://github.com/cython/cython</link><description>&lt;p&gt;&lt;i&gt;The most widely used Python to C compiler&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-welcome-to-cython" class="anchor" aria-hidden="true" href="#welcome-to-cython"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Welcome to Cython!&lt;/h1&gt;
&lt;p&gt;Cython is a language that makes writing C extensions for
Python as easy as Python itself.  Cython is based on
Pyrex, but supports more cutting edge functionality and
optimizations.&lt;/p&gt;
&lt;p&gt;The Cython language is very close to the Python language, but Cython
additionally supports calling C functions and declaring C types on variables
and class attributes.  This allows the compiler to generate very efficient C
code from Cython code.&lt;/p&gt;
&lt;p&gt;This makes Cython the ideal language for wrapping external C libraries, and
for fast C modules that speed up the execution of Python code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Official website: &lt;a href="https://cython.org/" rel="nofollow"&gt;https://cython.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Documentation: &lt;a href="http://docs.cython.org/" rel="nofollow"&gt;http://docs.cython.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Github repository: &lt;a href="https://github.com/cython/cython"&gt;https://github.com/cython/cython&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wiki: &lt;a href="https://github.com/cython/cython/wiki"&gt;https://github.com/cython/cython/wiki&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can &lt;strong&gt;support the Cython project&lt;/strong&gt; via
&lt;a href="https://github.com/users/scoder/sponsorship"&gt;Github Sponsors&lt;/a&gt; or
&lt;a href="https://tidelift.com/subscription/pkg/pypi-cython" rel="nofollow"&gt;Tidelift&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-installation"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation:&lt;/h2&gt;
&lt;p&gt;If you already have a C compiler, just do:&lt;/p&gt;
&lt;pre&gt;pip install Cython
&lt;/pre&gt;
&lt;p&gt;otherwise, see &lt;a href="http://docs.cython.org/en/latest/src/quickstart/install.html" rel="nofollow"&gt;the installation page&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-license"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License:&lt;/h2&gt;
&lt;p&gt;The original Pyrex program was licensed "free of restrictions" (see below).
Cython itself is licensed under the permissive &lt;strong&gt;Apache License&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://github.com/cython/cython/blob/master/LICENSE.txt"&gt;LICENSE.txt&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing:&lt;/h2&gt;
&lt;p&gt;Want to contribute to the Cython project?
Here is some &lt;a href="https://github.com/cython/cython/blob/master/docs/CONTRIBUTING.rst"&gt;help to get you started&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are currently building the next great Cython edition:
&lt;a href="https://github.com/cython/cython/milestone/58"&gt;Cython 3.0&lt;/a&gt;.
You can help us make the life of Python 3.x users easier.&lt;/p&gt;
&lt;a name="user-content-get-the-full-source-history"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-get-the-full-source-history" class="anchor" aria-hidden="true" href="#get-the-full-source-history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Get the full source history:&lt;/h2&gt;
&lt;p&gt;Note that Cython used to ship the full version control repository in its source
distribution, but no longer does so due to space constraints.  To get the
full source history from a downloaded source archive, make sure you have git
installed, then step into the base directory of the Cython source distribution
and type:&lt;/p&gt;
&lt;pre&gt;make repo
&lt;/pre&gt;
&lt;a name="user-content-the-following-is-from-pyrex"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-the-following-is-from-pyrex" class="anchor" aria-hidden="true" href="#the-following-is-from-pyrex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The following is from Pyrex:&lt;/h2&gt;
&lt;p&gt;This is a development version of Pyrex, a language
for writing Python extension modules.&lt;/p&gt;
&lt;p&gt;For more info, see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Doc/About.html for a description of the language&lt;/li&gt;
&lt;li&gt;INSTALL.txt    for installation instructions&lt;/li&gt;
&lt;li&gt;USAGE.txt      for usage instructions&lt;/li&gt;
&lt;li&gt;Demos          for usage examples&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Comments, suggestions, bug reports, etc. are
welcome!&lt;/p&gt;
&lt;p&gt;Copyright stuff: Pyrex is free of restrictions. You
may use, redistribute, modify and distribute modified
versions.&lt;/p&gt;
&lt;p&gt;The latest version of Pyrex can be found &lt;a href="http://www.cosc.canterbury.ac.nz/~greg/python/Pyrex/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div&gt;
&lt;div&gt;Greg Ewing, Computer Science Dept&lt;/div&gt;
&lt;div&gt;University of Canterbury&lt;/div&gt;
&lt;div&gt;Christchurch, New Zealand&lt;/div&gt;
&lt;/div&gt;
&lt;blockquote&gt;
A citizen of NewZealandCorp, a wholly-owned subsidiary of USA Inc.&lt;/blockquote&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>cython</author><guid isPermaLink="false">https://github.com/cython/cython</guid><pubDate>Tue, 11 Feb 2020 00:23:00 GMT</pubDate></item><item><title>uhlik/bpy #24 in Python, Today</title><link>https://github.com/uhlik/bpy</link><description>&lt;p&gt;&lt;i&gt;blender python scripts&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-addons-for-blender-280" class="anchor" aria-hidden="true" href="#addons-for-blender-280"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Addons for blender 2.80&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#point-cloud-visualizer-for-blender-280"&gt;Point Cloud Visualizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#color-management-presets-for-blender-280"&gt;Color Management Presets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tube-uv-unwrap-for-blender-280"&gt;Tube UV Unwrap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#fast-wavefront2-for-blender-280"&gt;Fast Wavefront^2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#import-agisoft-photoscan-cameras"&gt;Import Agisoft PhotoScan Cameras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#carbon-tools"&gt;Carbon Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#time-tracker-for-blender-280"&gt;Time Tracker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt;Addons for &lt;strong&gt;blender 2.7x&lt;/strong&gt; are here: &lt;a href="https://github.com/uhlik/bpy/tree/2.7x"&gt;2.7x branch&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-point-cloud-visualizer-for-blender-280" class="anchor" aria-hidden="true" href="#point-cloud-visualizer-for-blender-280"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://raw.githubusercontent.com/uhlik/bpy/master/space_view3d_point_cloud_visualizer.py" rel="nofollow"&gt;Point Cloud Visualizer&lt;/a&gt; (for blender 2.80)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Display, edit, filter, render, convert, generate and export colored point cloud PLY files.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Works with any PLY file with 'x, y, z, nx, ny, nz, red, green, blue' vertex values. Vertex normals and colors are optional.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.8.9.gif"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.8.9.gif" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-general-info" class="anchor" aria-hidden="true" href="#general-info"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;General info&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.28.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.28.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic Usage:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Install and activate addon in a usual way.&lt;/li&gt;
&lt;li&gt;Add any object type to scene.&lt;/li&gt;
&lt;li&gt;Go to 3d View Sidebar (N) &amp;gt; &lt;code&gt;Point Cloud Visualizer&lt;/code&gt; tab, click file browser icon, select ply file, click &lt;code&gt;Load PLY&lt;/code&gt;. &lt;code&gt;Reload&lt;/code&gt; button next to it reloads ply from disk.&lt;/li&gt;
&lt;li&gt;Click &lt;code&gt;Draw&lt;/code&gt; button to display point cloud, &lt;code&gt;Erase&lt;/code&gt; to hide point cloud. Adjust percentage of displayed points with &lt;code&gt;Display&lt;/code&gt;, point size with &lt;code&gt;Size&lt;/code&gt; and point transparency with &lt;code&gt;Alpha&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Display point normals as lines - click &lt;code&gt;Normal&lt;/code&gt; icon, adjust line length with &lt;code&gt;Length&lt;/code&gt; next to it.&lt;/li&gt;
&lt;li&gt;Transforming parent object transforms point cloud as well.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Illumination&lt;/code&gt; 'adds' single artificial light on points, you can edit its direction and strength. Works only when vertex normals are present.&lt;/li&gt;
&lt;li&gt;When vertex colors are missing, cloud will be displayed in uniform gray, in that case you can enable &lt;code&gt;Illumination&lt;/code&gt; to have better cloud view&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-display-options" class="anchor" aria-hidden="true" href="#display-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Display Options:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Display&lt;/code&gt; - percentage of displayed points&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Size&lt;/code&gt; - point size in pixels&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Alpha&lt;/code&gt; - global points alpha&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Normals&lt;/code&gt; - display point normals as lines, adjust line length with &lt;code&gt;Length&lt;/code&gt; next to it&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Illumination&lt;/code&gt; - enable extra illumination, works only when vertex normals can be loaded&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Light Direction&lt;/code&gt; - illumination light direction&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Light Intensity&lt;/code&gt; - illumination light intensity&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Shadow Intensity&lt;/code&gt; - illumination shadow intensity&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Depth&lt;/code&gt; - enable depth debug shader&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Brightness&lt;/code&gt; - depth shader color brightness&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Contrast&lt;/code&gt; - depth shader color contrast&lt;/li&gt;
&lt;li&gt;&lt;code&gt;False Colors&lt;/code&gt; - display depth shader in false colors&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Color A&lt;/code&gt; - depth shader false colors front color&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Color B&lt;/code&gt; - depth shader false colors back color&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Normal&lt;/code&gt; - enable normal debug shader&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Position&lt;/code&gt; - enable position debug shader&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-shaders" class="anchor" aria-hidden="true" href="#shaders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Shaders&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.23-shaders.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.23-shaders.jpg" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-clip" class="anchor" aria-hidden="true" href="#clip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Clip&lt;/h3&gt;
&lt;p&gt;Shader with 6 clipping planes. Planes can be enabled/disabled independently with eye icon, first 3 values are plane normal (x, y, z), last value is plane distance from origin. These can be set from different object bounding box, choose object from scene in &lt;code&gt;Object&lt;/code&gt; and hit &lt;code&gt;Set Clip Planes From Object Bounding Box&lt;/code&gt;, object can then be hidden/deleted, set values will stay until operator is executed again. &lt;code&gt;X&lt;/code&gt; reset all the settings to their defaults.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.28-clip-viewport.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.28-clip-viewport.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.28-clip.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.28-clip.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Enable Clipping Planes Shader&lt;/code&gt; - enable shader&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Plane 0&lt;/code&gt; - plane 0 normal (x, y, z), plane 0 distance from origin&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Plane 1&lt;/code&gt; - plane 1 normal (x, y, z), plane 1 distance from origin&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Plane 2&lt;/code&gt; - plane 2 normal (x, y, z), plane 2 distance from origin&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Plane 3&lt;/code&gt; - plane 3 normal (x, y, z), plane 3 distance from origin&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Plane 4&lt;/code&gt; - plane 4 normal (x, y, z), plane 4 distance from origin&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Plane 5&lt;/code&gt; - plane 5 normal (x, y, z), plane 5 distance from origin&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Object&lt;/code&gt; - object to use as bounding box source&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Set Clip Planes From Object Bounding Box&lt;/code&gt; - set planes from selected object bounding box&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X&lt;/code&gt; - reset all the settings to their defaults&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-edit" class="anchor" aria-hidden="true" href="#edit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Edit&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.10-editing.gif"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.10-editing.gif" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.10-edit.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.10-edit.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.20-edit.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.20-edit.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Quasi point cloud Edit Mode. Hit &lt;code&gt;Start&lt;/code&gt; and all points are converted to helper mesh with vertices and entered to mesh edit mode. You can transform, delete and duplicate vertices using regular Blender's tools. If you want update displayed points, hit &lt;code&gt;Update&lt;/code&gt;, when you are finished editing hit &lt;code&gt;End&lt;/code&gt; to update points for a last time and delete helper mesh. If something went wrong, select main object with cloud and hit &lt;code&gt;Cancel&lt;/code&gt; to reload original points, return interface to regular mode and attempt to clean helper mesh if it is still available.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To save edited point cloud, you have to use &lt;code&gt;Export&lt;/code&gt; feature and check &lt;code&gt;Use Viewport Points&lt;/code&gt; because edits are only in memory, if you close Blender, edits will be lost.&lt;/li&gt;
&lt;li&gt;Point normals are not changed (at this time), if you rotate points, normals will be still oriented as before.&lt;/li&gt;
&lt;li&gt;New points can be reliably (for now) created by duplicating existing points. If you create new points, they will all have the same random normal and random color.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;Start&lt;/code&gt; - Start edit mode, create helper object and switch to it
&lt;code&gt;Overlay Alpha&lt;/code&gt; - Overlay point alpha
&lt;code&gt;Overlay Size&lt;/code&gt; - Overlay point size
&lt;code&gt;Update&lt;/code&gt; - Update displayed cloud from edited mesh
&lt;code&gt;End&lt;/code&gt; - Update displayed cloud from edited mesh, stop edit mode and remove helper object
&lt;code&gt;Cancel&lt;/code&gt; - Stop edit mode, try to remove helper object and reload original point cloud&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-filter" class="anchor" aria-hidden="true" href="#filter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Filter&lt;/h3&gt;
&lt;p&gt;Filter current point cloud, all changes are only temporary, original data are still intact. To keep changes, you have to export cloud as ply file.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.16-filter.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.16-filter.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-simplify" class="anchor" aria-hidden="true" href="#simplify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Simplify&lt;/h5&gt;
&lt;p&gt;Simplify point cloud to exact number of evenly distributed samples. All loaded points are processed. Higher samples counts may take a long time to finish. Surely there are better (and faster) tools for the job, but.. Basically it takes some random point and set as accepted sample, then another set of random candidates, measure distance from already accepted samples and stores the one that is most distant as another accepted, repeat until number of samples is reached.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Samples&lt;/code&gt; - Number of points in simplified point cloud, best result when set to less than 20% of points, when samples has value close to total expect less points in result&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Candidates&lt;/code&gt; - Number of candidates used during resampling, the higher value, the slower calculation, but more even&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Simplify&lt;/code&gt; - run operator&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-project" class="anchor" aria-hidden="true" href="#project"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Project&lt;/h5&gt;
&lt;p&gt;Project points on mesh (or object convertible to mesh) surface. Projects point along their normals until it hit surface or &lt;code&gt;Search Distance&lt;/code&gt; is reached. You can choose between &lt;code&gt;Positive&lt;/code&gt; (along normal direction), &lt;code&gt;Negative&lt;/code&gt; (vice versa) or both. Optionally you can &lt;code&gt;Discard Unprojectable&lt;/code&gt; points that was not possible to project and after projection &lt;code&gt;Shift&lt;/code&gt; points a fixed distance along normal (positive value) or the other way around (negative value). Projected points can be optionally colorized by vertex colors, uv texture and vertex group from target mesh.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Object&lt;/code&gt; - Mesh or object convertible to mesh&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Search Distance&lt;/code&gt; - Maximum search distance in which to search for surface&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Positive&lt;/code&gt; - Search along point normal forwards&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Negative&lt;/code&gt; - Search along point normal backwards&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Discard Unprojectable&lt;/code&gt; - Discard points which didn't hit anything&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Colorize&lt;/code&gt; - Colorize projected points&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Source&lt;/code&gt; - Color source for projected point cloud
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Vertex Colors&lt;/code&gt; - Use active vertex colors from target&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UV Texture&lt;/code&gt; - Use colors from active image texture node in active material using active UV layout from target&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Vertex Group Monochromatic&lt;/code&gt; - Use active vertex group from target, result will be shades of grey&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Vertex Group Colorized&lt;/code&gt; - Use active vertex group from target, result will be colored from red (1.0) to blue (0.0) like in weight paint viewport&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Shift&lt;/code&gt; - Shift points after projection above (positive value) or below (negative value) surface&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Project&lt;/code&gt; - execute operator&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-boolean" class="anchor" aria-hidden="true" href="#boolean"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Boolean&lt;/h5&gt;
&lt;p&gt;Intersect or Exclude points with mesh object. Mesh have to be non-manifold.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Object&lt;/code&gt; - Mesh or object convertible to mesh&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Intersect&lt;/code&gt; - Keep points inside mesh, remove points outside&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Exclude&lt;/code&gt; - Keep points outside mesh, remove points inside&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-remove-color" class="anchor" aria-hidden="true" href="#remove-color"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Remove Color&lt;/h5&gt;
&lt;p&gt;Remove points with exact/similar color as chosen in color picker (Eyedropper works too). Currently i can't get to match sampled color from viewport with color in loaded cloud. Floating point error, incorrectly handled Gamma (at my side for sure), color management in Blender's viewport or any combination of all, or something else.. Anyway, if you leave at least one delta at hue/saturation/value (whatever works best for given cloud) it should remove the color you picked.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Color&lt;/code&gt; - Color to remove from point cloud&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Δ Hue&lt;/code&gt; - Delta hue&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Δ Saturation&lt;/code&gt; - Delta saturation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Δ Value&lt;/code&gt; - Delta value&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Select Color&lt;/code&gt; - Run operator and display selected points in viewport for review&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X&lt;/code&gt; - Deselect&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Delete Selected&lt;/code&gt; - Delete selected points&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-merge" class="anchor" aria-hidden="true" href="#merge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Merge&lt;/h5&gt;
&lt;p&gt;Load another ply and merge with currently displayed. Hit &lt;code&gt;Merge With Other PLY&lt;/code&gt;, select ply file and load. New point will be appended to old, shuffled if shuffle is enabled in preferences.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Merge With Other PLY&lt;/code&gt; - run operator&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-join" class="anchor" aria-hidden="true" href="#join"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Join&lt;/h5&gt;
&lt;p&gt;Join current active point cloud with another on different object. Object transformations are applied to clouds to look the same as before joining.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Object&lt;/code&gt; - select object with another point cloud to join&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Join&lt;/code&gt; - run operator&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-color-adjustment" class="anchor" aria-hidden="true" href="#color-adjustment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Color Adjustment&lt;/h5&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.25-color-adjustment.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.25-color-adjustment.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Adjust exposure, gamma, brightness, contrast, hue, saturation, value or invert colors. Click &lt;code&gt;Enable&lt;/code&gt; to enable color adjustment shader, adjust values as needed, click &lt;code&gt;Apply&lt;/code&gt; to apply colors to points. Click &lt;code&gt;Reset&lt;/code&gt; to set all to default value. Shader can be disabled without changes anytime unchecking &lt;code&gt;Enable&lt;/code&gt;. When color adjustment shader is enabled, all other shaders are disabled.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Enabled&lt;/code&gt; - Enable color adjustment shader, other shaders will be overrided until disabled&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Exposure&lt;/code&gt; - formula: color = color * (2 ** value)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Gamma&lt;/code&gt; - formula: color = color ** (1 / value)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Brightness&lt;/code&gt; - formula: color = (color - 0.5) * contrast + 0.5 + brightness&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Contrast&lt;/code&gt; - formula: color = (color - 0.5) * contrast + 0.5 + brightness&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Hue&lt;/code&gt; - formula: color.h = (color.h + (value % 1.0)) % 1.0&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Saturation&lt;/code&gt; - formula: color.s += value&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Value&lt;/code&gt; - formula: color.v += value&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Invert&lt;/code&gt; - formula: color = 1.0 - color&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Reset&lt;/code&gt; - Reset color adjustment values&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Apply&lt;/code&gt; - Apply color adjustments to points, reset and exit&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-render" class="anchor" aria-hidden="true" href="#render"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Render&lt;/h3&gt;
&lt;p&gt;Currently only sigle point cloud per render/frame is supported. Output image is RGBA 8bit PNG - transparent background with colored point cloud, which can be composed over something else later.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Blend file has to be saved&lt;/li&gt;
&lt;li&gt;Load and display ply first.&lt;/li&gt;
&lt;li&gt;Make a camera and adjust as needed.&lt;/li&gt;
&lt;li&gt;Select cloud parent object, set point size with &lt;code&gt;Size&lt;/code&gt; or percentage of rendered points with &lt;code&gt;Count&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Set render path in &lt;code&gt;Output&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Set render image size with &lt;code&gt;Resolution X&lt;/code&gt;, &lt;code&gt;Resolution Y&lt;/code&gt; and &lt;code&gt;Resolution %&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;At default resolution settings are taken from scene, to make them independent, click chain icon next to properties, correct aspect ratio is not calculated, if you link properties again, values are copied from scene.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;Illumination&lt;/code&gt; is enabled it will be rendered as well&lt;/li&gt;
&lt;li&gt;If one of debug shaders is enabled, it will be rendered instead of regular shader&lt;/li&gt;
&lt;li&gt;Hit &lt;code&gt;Render&lt;/code&gt; or &lt;code&gt;Animation&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-render-options" class="anchor" aria-hidden="true" href="#render-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Render options:&lt;/h5&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.10-render.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.10-render.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Count&lt;/code&gt; - percentage of rendered points&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Size&lt;/code&gt; - point render size in pixels&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Supersampling&lt;/code&gt; - Render larger image and then resize back to anti-alias, 1 - disabled, 2 - render 200%, 3 - render 300%, etc.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Smooth Circles&lt;/code&gt; - Currently works only for basic shader with/without illumination and generally is much slower than Supersampling, use only when Supersampling fails&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Output&lt;/code&gt; - path where to save rendered images, &lt;code&gt;#&lt;/code&gt; characters defines the position and length of frame numbers, image is always saved, filetype is always png, accepts relative paths, upon hitting &lt;code&gt;Render&lt;/code&gt; path is validated, changed to absolute and written back&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Resolution X&lt;/code&gt; - image width in pixels&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Resolution Y&lt;/code&gt; - image height in pixels&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Resolution %&lt;/code&gt; - percentage scale for resolution&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Resolution Linked&lt;/code&gt; - when enabled, settings are taken from scene, if not they are independent on scene, but aspect ratio is not calculated&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-render-smoothinganti-aliasing-notes" class="anchor" aria-hidden="true" href="#render-smoothinganti-aliasing-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Render smoothing/anti-aliasing notes:&lt;/h5&gt;
&lt;p&gt;Use &lt;code&gt;Supersampling&lt;/code&gt; for most of the time with lowest value (to be exact, &lt;code&gt;2&lt;/code&gt; works great) that look still good. If you run into problems due to very large supersampled image sizes Blender or hardware cannot handle, you can try to disable &lt;code&gt;Supersampling&lt;/code&gt; and use &lt;code&gt;Smooth Circles&lt;/code&gt; to draw smooth points directly in target resolution, but you are limited to basic shader and points before drawing have to be depth sorted manually which can be very slow with large point counts.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-convert" class="anchor" aria-hidden="true" href="#convert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convert&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-convert.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-convert.jpg" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-psys.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-psys.jpg" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Convert point cloud to mesh. May result in very large meshes, e.g. 1m point cloud to cubes = 8m poly mesh. Depending on what point cloud data is available and desired mesh type, some options may not be enabled.&lt;/p&gt;
&lt;p&gt;Conversion to instancer specifics: points are converted to triangle mesh object, vertex colors are baked to texture, extra instanced sphere object is added as child object of main mesh, material using baked colors is added to sphere and each instance inherits color of corresponding face it is instanced from.&lt;/p&gt;
&lt;p&gt;Conversion to particles specifics: points are converted to triangle mesh object, vertex colors are baked to texture, particle system is added to mesh with one particle on each face, extra instanced sphere added as child object of main mesh and particle system is set to render that sphere, material using baked colors is added to sphere and each instance inherits color of corresponding face it emit from. Result is regular particle system which can be further edited, e.g. instance mesh changed, physics added etc.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.10-convert.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.10-convert.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Type&lt;/code&gt; - Instance mesh type, Vertex, Equilateral Triangle, Tetrahedron, Cube or Ico Sphere&lt;/li&gt;
&lt;li&gt;&lt;code&gt;All&lt;/code&gt;, &lt;code&gt;Subset&lt;/code&gt; - Use all points or random subset of by given percentage&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Size&lt;/code&gt; - Mesh instance size, internal instanced mesh has size 1.0 so if you set size to 0.01, resulting instances will have actual size of 0.01 event when cloud is scaled&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Align To Normal&lt;/code&gt; - Align instance to point normal, e.g. tetrahedron point will align to normal, triangle plane will align to normal etc.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Colors&lt;/code&gt; - Assign point color to instance vertex colors, each instance will be colored by point color (except vertices)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Sphere Subdivisions&lt;/code&gt; - Conversion to instancer / particles only, number of subdivisions of particle system instanced ico sphere&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-generate" class="anchor" aria-hidden="true" href="#generate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generate&lt;/h3&gt;
&lt;p&gt;Generate point cloud from mesh (or object convertible to mesh). To store point cloud, use &lt;code&gt;Export&lt;/code&gt; to save as ply file.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.18-generate-color-types.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.18-generate-color-types.jpg" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.19-verts-psys.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.19-verts-psys.jpg" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.19-gen-surface.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.19-gen-surface.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.20-gen-poisson.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.20-gen-poisson.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.19-gen-verts.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.19-gen-verts.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.19-gen-psys.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.19-gen-psys.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Source&lt;/code&gt; - Points generation source
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Vertices&lt;/code&gt; - Use mesh vertices&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Surface&lt;/code&gt; - Use triangulated mesh surface&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Particles&lt;/code&gt; - Use active particle system&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Particles&lt;/code&gt; &lt;code&gt;(Source: Particles)&lt;/code&gt; - Particles source selection
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;All&lt;/code&gt; &lt;code&gt;(Source: Particles)&lt;/code&gt; - Use all particles&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Alive&lt;/code&gt; &lt;code&gt;(Source: Particles)&lt;/code&gt; - Use only alive particles&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Algorithm&lt;/code&gt; &lt;code&gt;(Source: Surface)&lt;/code&gt; - Point generating algorithm
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Weighted Random In Triangle&lt;/code&gt; - Average triangle areas to approximate number of random points in each to get even distribution of points. If some very small polygons are left without points, increase number of samples. Mesh is triangulated before processing, on non-planar polygons, points will not be exactly on original polygon surface.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Poisson Disk Sampling&lt;/code&gt; - Warning: slow, very slow indeed.. Uses Weighted Random In Triangle algorithm to pregenerate samples with all its inconveniences.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Approximate Number Of Points&lt;/code&gt; &lt;code&gt;(Source: Surface)&lt;/code&gt; - Number of points to generate, some algorithms may not generate exact number of points.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Seed&lt;/code&gt; &lt;code&gt;(Source: Surface)&lt;/code&gt; - Random number generator seed&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Colors&lt;/code&gt; &lt;code&gt;(Source: Vertices, Surface, Particles)&lt;/code&gt; - Color source for generated point cloud
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Constant Color&lt;/code&gt; - Use constant color value&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Vertex Colors&lt;/code&gt; - Use active vertex colors&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UV Texture&lt;/code&gt; - Generate colors from active image texture node in active material using active UV layout&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Vertex Group Monochromatic&lt;/code&gt; - Use active vertex group, result will be shades of grey&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Vertex Group Colorized&lt;/code&gt; - Use active vertex group, result will be colored from red (1.0) to blue (0.0) like in weight paint viewport&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Color&lt;/code&gt; &lt;code&gt;(Source: Vertices, Surface, Particles)&lt;/code&gt; - Constant color&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Exact Number of Samples&lt;/code&gt; &lt;code&gt;(Source: Surface, Algorithm: Triangle)&lt;/code&gt; - Generate exact number of points, if selected algorithm result is less points, more points will be calculated on random polygons at the end, if result is more points, points will be shuffled and sliced to match exact value&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Minimal Distance&lt;/code&gt; &lt;code&gt;(Source: Surface, Algorithm: Poisson)&lt;/code&gt; - Poisson Disk minimal distance between points, the smaller value, the slower calculation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Sampling Exponent&lt;/code&gt; &lt;code&gt;(Source: Surface, Algorithm: Poisson)&lt;/code&gt; - Poisson Disk presampling exponent, lower values are faster but less even, higher values are slower exponentially&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When &lt;code&gt;Source&lt;/code&gt; is &lt;code&gt;Particles&lt;/code&gt;, for generating colors (apart from &lt;code&gt;Constant&lt;/code&gt; color), &lt;strong&gt;non-overlapping UV layout&lt;/strong&gt; is required (can be really bad, useless for real production).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-export" class="anchor" aria-hidden="true" href="#export"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Export&lt;/h3&gt;
&lt;p&gt;Export current point cloud as binary ply file with several options. If exporting modified (filtered) points, check &lt;code&gt;Use Viewport Points&lt;/code&gt;, otherwise you will not get modified points. If exporting viewport points colors may slightly differ. Transformation and axis conversion can be applied on both loaded and viewport points.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.10-export.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.10-export.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Use Viewport Points&lt;/code&gt; - When checked, export points currently displayed in viewport or when unchecked, export data loaded from original ply file&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Visible Points Only&lt;/code&gt; - Export currently visible points only (controlled by 'Display' on main panel)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Apply Transformation&lt;/code&gt; - Apply parent object transformation to points&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Convert Axes&lt;/code&gt; - Convert from blender (y forward, z up) to forward -z, up y axes&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-sequence" class="anchor" aria-hidden="true" href="#sequence"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sequence&lt;/h3&gt;
&lt;p&gt;Load sequence of ply files to play in viewport. Load first frame as regular file and when &lt;code&gt;Preload Sequence&lt;/code&gt; is clicked it tries to load all ply files matching selected ply filename, e.g. you select &lt;code&gt;sequence-001.ply&lt;/code&gt; and all &lt;code&gt;sequence-###.ply&lt;/code&gt; will be loaded from directory. Only last number in filename is considered. Numbers should start at 1. All other features works when animation is not playing, but all changes are lost when you change frame to another.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.14-sequence.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.14-sequence.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Preload Sequence&lt;/code&gt; - Load all matching ply files&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Cycle Forever&lt;/code&gt; - Cycle frames if timeline is longer than number of loaded frames&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Clear Sequence&lt;/code&gt; - Clear all loaded and return object to regular state i.e. you can load another ply, changes are kept etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-external-api" class="anchor" aria-hidden="true" href="#external-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;External API&lt;/h3&gt;
&lt;p&gt;To display point cloud data from other addons/custom scripts.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; bpy
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;from&lt;/span&gt; space_view3d_point_cloud_visualizer &lt;span class="pl-k"&gt;import&lt;/span&gt; PCVControl
o &lt;span class="pl-k"&gt;=&lt;/span&gt; bpy.context.active_object
c &lt;span class="pl-k"&gt;=&lt;/span&gt; PCVControl(o)
n &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt;
vs &lt;span class="pl-k"&gt;=&lt;/span&gt; np.random.normal(&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, (n, &lt;span class="pl-c1"&gt;3&lt;/span&gt;))
ns &lt;span class="pl-k"&gt;=&lt;/span&gt; np.array([[&lt;span class="pl-c1"&gt;0.0&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.0&lt;/span&gt;, &lt;span class="pl-c1"&gt;1.0&lt;/span&gt;]] &lt;span class="pl-k"&gt;*&lt;/span&gt; n)
cs &lt;span class="pl-k"&gt;=&lt;/span&gt; np.random.random((n, &lt;span class="pl-c1"&gt;3&lt;/span&gt;))
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; draw points&lt;/span&gt;
c.draw(vs, ns, cs)&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; if some data like normals/colors are not available&lt;/span&gt;
c.draw(vs, &lt;span class="pl-c1"&gt;None&lt;/span&gt;, &lt;span class="pl-c1"&gt;None&lt;/span&gt;)
c.draw(vs, [], [])
c.draw(vs)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; it is also possible to pass nothing in which case nothing is drawn&lt;/span&gt;
c.draw()&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; to stop any drawing&lt;/span&gt;
c.erase()
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; to return object control to user&lt;/span&gt;
c.reset()&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-addon-preferences" class="anchor" aria-hidden="true" href="#addon-preferences"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Addon Preferences:&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.21-prefs.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pcv-0.9.21-prefs.png" alt="Point Cloud Visualizer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Default&lt;/code&gt; - Default color to be used upon loading PLY to cache when vertex colors are missing&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Normal&lt;/code&gt; - Display color for vertex normals lines&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Selection&lt;/code&gt; - Display color for selection&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Shuffle Points&lt;/code&gt; - Shuffle points upon loading, display percentage is more useable if points are shuffled, disabled if you plan to export ply and you need to keep point order&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Convert 16bit Colors&lt;/code&gt; - Convert 16bit colors to 8bit, applied when Red channel has 'uint16' dtype&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Gamma Correct 16bit Colors&lt;/code&gt; - When 16bit colors are encountered apply gamma as 'c ** (1 / 2.2)'&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Tab Name&lt;/code&gt; - To have PCV in its own separate tab, choose one&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Custom Tab Name&lt;/code&gt; - Check if you want to have PCV in custom named tab or in existing tab&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Name&lt;/code&gt; - Custom PCV tab name, if you choose one from already existing tabs it will append to that tab&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;0.9.30 join filter, ui tweaks, fixes&lt;/li&gt;
&lt;li&gt;0.9.29 fixes, render logging&lt;/li&gt;
&lt;li&gt;0.9.28 clipping planes from object bounding box, faster all numpy export, many minor fixes&lt;/li&gt;
&lt;li&gt;0.9.27 render supersampling and draw smooth circles&lt;/li&gt;
&lt;li&gt;0.9.26 color adjustment fixes&lt;/li&gt;
&lt;li&gt;0.9.25 color adjustment, faster boolean&lt;/li&gt;
&lt;li&gt;0.9.24 project colors&lt;/li&gt;
&lt;li&gt;0.9.23 fixes&lt;/li&gt;
&lt;li&gt;0.9.22 extra debug shaders&lt;/li&gt;
&lt;li&gt;0.9.21 preferences&lt;/li&gt;
&lt;li&gt;0.9.20 ui changes, poisson disk sampling generation, size and alpha available in edit mode, various fixes, removed manual depth sorting during render&lt;/li&gt;
&lt;li&gt;0.9.19 point cloud generation from vertices and particles&lt;/li&gt;
&lt;li&gt;0.9.18 point cloud generation from mesh surface&lt;/li&gt;
&lt;li&gt;0.9.17 fixes&lt;/li&gt;
&lt;li&gt;0.9.16 boolean filters&lt;/li&gt;
&lt;li&gt;0.9.15 selection preview for remove color filter&lt;/li&gt;
&lt;li&gt;0.9.14 external api improvements&lt;/li&gt;
&lt;li&gt;0.9.13 faster normals drawing&lt;/li&gt;
&lt;li&gt;0.9.12 ply sequence, external api&lt;/li&gt;
&lt;li&gt;0.9.11 merge filter&lt;/li&gt;
&lt;li&gt;0.9.10 ui&lt;/li&gt;
&lt;li&gt;0.9.9 point cloud global alpha&lt;/li&gt;
&lt;li&gt;0.9.8 basic editing&lt;/li&gt;
&lt;li&gt;0.9.7 project point cloud on mesh surface&lt;/li&gt;
&lt;li&gt;0.9.6 ply exporting now uses original or viewport data&lt;/li&gt;
&lt;li&gt;0.9.5 simplify and remove color filters&lt;/li&gt;
&lt;li&gt;0.9.4 export ply&lt;/li&gt;
&lt;li&gt;0.9.3 conversion to instancer&lt;/li&gt;
&lt;li&gt;0.9.2 load ply with 16bit colors&lt;/li&gt;
&lt;li&gt;0.9.1 all new render settings&lt;/li&gt;
&lt;li&gt;0.9.0 conversion to particles&lt;/li&gt;
&lt;li&gt;0.8.14 fixes&lt;/li&gt;
&lt;li&gt;0.8.13 fixes&lt;/li&gt;
&lt;li&gt;0.8.12 fixes&lt;/li&gt;
&lt;li&gt;0.8.11 ui tweaks&lt;/li&gt;
&lt;li&gt;0.8.10 fixes&lt;/li&gt;
&lt;li&gt;0.8.9 ui tweaks, code cleanup&lt;/li&gt;
&lt;li&gt;0.8.8 refactored convert to mesh&lt;/li&gt;
&lt;li&gt;0.8.7 fixed vcols bug in convert&lt;/li&gt;
&lt;li&gt;0.8.6 ui tweaks, a few minor optimizations&lt;/li&gt;
&lt;li&gt;0.8.5 convert to mesh all or subset&lt;/li&gt;
&lt;li&gt;0.8.4 preferences, ui tweaks&lt;/li&gt;
&lt;li&gt;0.8.3 display normals&lt;/li&gt;
&lt;li&gt;0.8.2 fixed shader unknown attribute name&lt;/li&gt;
&lt;li&gt;0.8.1 fixed ply with alpha, fixed convert to mesh when normals or colors are missing&lt;/li&gt;
&lt;li&gt;0.8.0 convert to mesh&lt;/li&gt;
&lt;li&gt;0.7.2 ui tweaks&lt;/li&gt;
&lt;li&gt;0.7.1 viewport performance fixes&lt;/li&gt;
&lt;li&gt;0.7.0 ascii ply support&lt;/li&gt;
&lt;li&gt;0.6.6 fixed drawing after undo/redo&lt;/li&gt;
&lt;li&gt;0.6.5 point cloud illumination&lt;/li&gt;
&lt;li&gt;0.6.4 refactored draw handlers, fixed occasional crash on erase&lt;/li&gt;
&lt;li&gt;0.6.3 added percentage of rendered points, fixed render colors to look the same as in viewport&lt;/li&gt;
&lt;li&gt;0.6.2 fixed point size display in viewport, separated view and render point size&lt;/li&gt;
&lt;li&gt;0.6.1 single cloud rendering almost completely rewritten to be better and faster&lt;/li&gt;
&lt;li&gt;0.6.0 single cloud rendering&lt;/li&gt;
&lt;li&gt;0.5.2 refactored some logic, removed icons from buttons&lt;/li&gt;
&lt;li&gt;0.5.1 load ply without vertex colors, uniform grey will be used&lt;/li&gt;
&lt;li&gt;0.5.0 performance improvements using numpy for loading and processing data&lt;/li&gt;
&lt;li&gt;0.4.6 fixed crash when parent object is deleted while drawing, fixed removal of loaded data when parent is deleted&lt;/li&gt;
&lt;li&gt;0.4.5 added 'Display' percentage, better error handling during .ply loading&lt;/li&gt;
&lt;li&gt;0.4.0 almost complete rewrite for blender 2.80, performance improvements using shaders, simplified ui&lt;/li&gt;
&lt;li&gt;0.3.0 new ply loader, can be used with any binary ply file with vertex coordinates and colors&lt;/li&gt;
&lt;li&gt;0.2.0 display percentage&lt;/li&gt;
&lt;li&gt;0.1.0 first release&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-known-bugs" class="anchor" aria-hidden="true" href="#known-bugs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Known bugs:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;If you duplicate object with cloud, duplicate will still control the original one until you load a different one. Currently there is no reliable way (as far as i know) to get unique id of an object and therefore no way to tell to which object stored properties (e.g. path to ply) belong.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://blenderartists.org/forum/showthread.php?416158-Addon-Point-Cloud-Visualizer" rel="nofollow"&gt;BlenderArtist.org thread&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-color-management-presets-for-blender-280" class="anchor" aria-hidden="true" href="#color-management-presets-for-blender-280"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://raw.githubusercontent.com/uhlik/bpy/master/color_management_presets.py" rel="nofollow"&gt;Color Management Presets&lt;/a&gt; (for blender 2.80)&lt;/h2&gt;
&lt;p&gt;Presets support for Render &amp;gt; Color Management panel, nothing more, nothing less.. Comes with a few presets i use which are created upon activation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/cmp280.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/cmp280.png" alt="Color Management Presets" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-tube-uv-unwrap-for-blender-280" class="anchor" aria-hidden="true" href="#tube-uv-unwrap-for-blender-280"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://raw.githubusercontent.com/uhlik/bpy/master/uv_tube_unwrap.py" rel="nofollow"&gt;Tube UV Unwrap&lt;/a&gt; (for blender 2.80)&lt;/h2&gt;
&lt;p&gt;UV unwrap tube-like meshes (all quads, no caps, fixed number of vertices in each ring)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/tuv280.gif"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/tuv280.gif" alt="Tube UV Unwrap" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Works only on tube-like parts of mesh defined by selection and active vertex (therefore you must be in vertex selection mode) and the selection must have a start and an end ring. Tube-like mesh is: all quads, no caps, fixed number of vertices in each ring. (Best example of such mesh is mesh circle extruded several times or beveled curve (not cyclic) converted to mesh.) There must be an active vertex on one of the boundary loops in selection. This active vertex define place where mesh will be 'cut' - where seam will be placed.&lt;/li&gt;
&lt;li&gt;Result is rectangular UV for easy texturing, scaled to fit square, horizontal and vertical distances between vertices are averaged and proportional to each other.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;usage:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;tab to Edit mode&lt;/li&gt;
&lt;li&gt;select part of mesh you want to unwrap, tube type explained above&lt;/li&gt;
&lt;li&gt;make sure your selection has boundaries and there is an active vertex on one border of selection&lt;/li&gt;
&lt;li&gt;hit "U" and select "Tube UV Unwrap"&lt;/li&gt;
&lt;li&gt;optionally check/uncheck 'Mark Seams' or 'Flip' in operator properties&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;changelog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0.3.0 blender 2.8 update&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://blenderartists.org/forum/showthread.php?339782-UV-Equalize-and-Tube-Unwrap-addons" rel="nofollow"&gt;BlenderArtist.org thread&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-fast-wavefront2-for-blender-280" class="anchor" aria-hidden="true" href="#fast-wavefront2-for-blender-280"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/uhlik/bpy/tree/master/io_mesh_fast_obj"&gt;Fast Wavefront^2&lt;/a&gt; (for blender 2.80)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Import/Export single mesh as Wavefront OBJ. Fast. Now with Cython. Binaries not included.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Only active mesh is exported. Only single mesh is expected on import. Supported obj features: UVs, normals, vertex colors using MRGB format (ZBrush) or 'Extended' format (import only) where vertex is defined as (x,y,z,r,g,b).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/obj2.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/obj2.png" alt="Fast Wavefront^2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;changelog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0.3.6 fixed export with applied modifiers&lt;/li&gt;
&lt;li&gt;0.3.5 api changes fixes&lt;/li&gt;
&lt;li&gt;0.3.4 ui&lt;/li&gt;
&lt;li&gt;0.3.3 fallback python export implementation in case cython module is not available&lt;/li&gt;
&lt;li&gt;0.3.2 import 'extended' vertex colors (x,y,z,r,g,b), optionally apply gamma correction&lt;/li&gt;
&lt;li&gt;0.3.1 import obj (python only)&lt;/li&gt;
&lt;li&gt;0.3.0 export implemented in cython&lt;/li&gt;
&lt;li&gt;0.2.0 ported to blender 2.80&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;requirements:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;python 3.7.0 (the same as shipped with blender 2.8)&lt;/li&gt;
&lt;li&gt;Cython (install with pip)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;installation on mac (win/linux should be very similar):&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;download repository and copy whole directory &lt;code&gt;io_mesh_fast_obj&lt;/code&gt; to &lt;code&gt;/Users/*USERNAME*/Library/Application Support/Blender/2.80/scripts/addons/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;in terminal cd to &lt;code&gt;/Users/*USERNAME*/Library/Application Support/Blender/2.80/scripts/addons/io_mesh_fast_obj/&lt;/code&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;$ git clone http://git.blender.org/blender.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$ python3 setup.py build_ext --inplace&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;now delete &lt;code&gt;blender&lt;/code&gt; directory, it is no longer needed until blender is updated, then you (might) need to repeat the process&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-import-agisoft-photoscan-cameras" class="anchor" aria-hidden="true" href="#import-agisoft-photoscan-cameras"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/uhlik/bpy/tree/master/io_import_photoscan_cameras.py"&gt;Import Agisoft PhotoScan Cameras&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Import cameras from Agisoft PhotoScan xml. Works with xml version 1.4.0 which is exported from PhotoScan 1.4.x versions and xml versions 1.5.0 from Agisoft Metashape 1.5.x versions. If you want to have images actually aligned with model, undistort images first. This is done in PhotoScan by &lt;code&gt;Export &amp;gt; Undistort Photos..&lt;/code&gt;. Because you can't in Blender set resolution for cameras independently, xml with different cameras or image resolutions might not work well.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/pscamerasui.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/pscamerasui.png" alt="Import Agisoft PhotoScan Cameras" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;usage:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;go to &lt;code&gt;Properties &amp;gt; Scene &amp;gt; Import Agisoft PhotoScan Cameras&lt;/code&gt; panel&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cameras XML&lt;/strong&gt;: set path to xml&lt;/li&gt;
&lt;li&gt;set import options:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Camera Display Size&lt;/strong&gt;: size of imported cameras in viewport&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load Camera Images&lt;/strong&gt;: load images or not&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Images Directory&lt;/strong&gt;: path to directory with undistorted images&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Image Extension&lt;/strong&gt;: images extension, they all should be the same (currently)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Alpha&lt;/strong&gt;: camera image alpha, 0.0 - 1.0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Depth&lt;/strong&gt;: camera display depth, front / back&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;there are some more optional properties:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create Chunk Region Borders&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Align to Active Object&lt;/strong&gt;: if you import mesh from PhotoScan first, the transform it to correct size and orientation, this option will copy transformation from that mesh if it is active&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hit &lt;strong&gt;Import&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;import done..&lt;/li&gt;
&lt;li&gt;now you can quickly swap cameras in alphabetical order in &lt;code&gt;PhotoScan Cameras Utilities&lt;/code&gt; panel&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;changelog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0.1.3 ui&lt;/li&gt;
&lt;li&gt;0.1.2 compatibility with Agisoft Metashape XML (1.5.x)&lt;/li&gt;
&lt;li&gt;0.1.1 first release&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://blenderartists.org/t/addon-import-agisoft-photoscan-cameras/1140610" rel="nofollow"&gt;BlenderArtist.org thread&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-carbon-tools" class="anchor" aria-hidden="true" href="#carbon-tools"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/uhlik/bpy/tree/master/carbon_tools.py"&gt;Carbon Tools&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Ever-evolving set of small tools, workflows and shortcuts focused mainly on processing photogrammetry scans.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/carbon_tools.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/carbon_tools.png" alt="Carbon Tools" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-subtools" class="anchor" aria-hidden="true" href="#subtools"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Subtools&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Extract&lt;/strong&gt; selected part of mesh to a new object. If edges of extracted mesh are changed, it won't be able to merge back seamlessly - use option to hide edges (lock button) to protect them.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Insert&lt;/strong&gt; it back when finished editing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extract Non-Manifold&lt;/strong&gt; elements with part of mesh around them (10x expanded selection) as subtool&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-dyntopo" class="anchor" aria-hidden="true" href="#dyntopo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dyntopo&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dyntopo Setup&lt;/strong&gt; - Quick setup for optimizing mesh resolution. Set desired &lt;strong&gt;Constant Resolution&lt;/strong&gt; and &lt;strong&gt;Method&lt;/strong&gt; and hit &lt;strong&gt;Dyntopo Setup&lt;/strong&gt;
Mode is switched to Sculpt with Dyntopo, brush is set to strength 0 - affecting only mesh resolution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dyntopo Live Settings&lt;/strong&gt; - current settings which can be changed during sculpting&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-texture-paint" class="anchor" aria-hidden="true" href="#texture-paint"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Texture Paint&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Texture Paint Setup&lt;/strong&gt; - Quick setup for retouching texture in Photoshop, set &lt;strong&gt;Resolution&lt;/strong&gt; of exported images and hit &lt;strong&gt;TP Setup&lt;/strong&gt; (Photoshop must be set in preferences as Image Editor)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;External TP Live Commands&lt;/strong&gt;: &lt;strong&gt;Quick Edit&lt;/strong&gt; - export image and open in PS, &lt;strong&gt;Apply&lt;/strong&gt; - project image back to model, &lt;strong&gt;Save All Images&lt;/strong&gt; - save all edited textures&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-io" class="anchor" aria-hidden="true" href="#io"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;IO&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Import from ZBrush&lt;/strong&gt; (depends on other addon Fast Wavefront^2)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Export to ZBrush&lt;/strong&gt; (depends on other addon Fast Wavefront^2)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformation: Selected &amp;gt; Active&lt;/strong&gt; - Copy transformation from selected to active. Useful for setting correct scale and orientation after initial import from PhotoScan.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Matrix: Selected &amp;gt; Active&lt;/strong&gt; - Select matrix source object first and then target object. Matrix will be copied while keeping visual transformation intact.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Export to PhotoScan&lt;/strong&gt; (depends on other addon Fast Wavefront^2)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-end" class="anchor" aria-hidden="true" href="#end"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;End&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End&lt;/strong&gt; ends current procedure: Dyntopo and Texture Paint back to Object mode and reset all settings&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-utilities" class="anchor" aria-hidden="true" href="#utilities"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Utilities&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Smooth&lt;/strong&gt; / &lt;strong&gt;Flat&lt;/strong&gt; shading, just shortcuts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;UV Coverage&lt;/strong&gt; calculate and print how much percent covers active uv layout&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seams From Islands&lt;/strong&gt; mark seams from UV islands&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Select Seams&lt;/strong&gt; select seam edges&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seams &amp;gt; Wireframe&lt;/strong&gt; copy seams edges to a new mesh object&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Export UV Layout&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wireframe&lt;/strong&gt; set display to shaded + wire + all edges and deselect object&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Select Non-Manifold&lt;/strong&gt; select non-manifold elements and optionally focus camera on them (eye icon)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-conversions" class="anchor" aria-hidden="true" href="#conversions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conversions&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;UVTex &amp;gt; VCols&lt;/strong&gt; - Copy image colors from active image texture node in active material using active UV layout to new vertex colors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group &amp;gt; VCols&lt;/strong&gt; - Active vertex group to new vertex colors, vertex weight to rgba(weight, weight, weight, 1.0)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VCols &amp;gt; Group&lt;/strong&gt; - Active vertex colors to new vertex group, vertex weight by color perceived luminance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Difference &amp;gt; Group&lt;/strong&gt; - Calculate difference between two selected meshes and write as vertex group to active mesh. Selected is considered to be original, active to be modified. Objects should have the same transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;changelog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2.2.2 ui changes, mesh difference to group, calculate uv coverage&lt;/li&gt;
&lt;li&gt;0.2.1 uvtex / vcols / group conversions&lt;/li&gt;
&lt;li&gt;0.2.0 first release&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-time-tracker-for-blender-280" class="anchor" aria-hidden="true" href="#time-tracker-for-blender-280"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://raw.githubusercontent.com/uhlik/bpy/master/system_time_tracker.py" rel="nofollow"&gt;Time Tracker&lt;/a&gt; (for blender 2.80)&lt;/h2&gt;
&lt;p&gt;Simple time tracker inside blender. After you install and enable it, it will log loaded and saved files and time spent of file until it is saved. All ui is inside addon preferences.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/uhlik/bpy/media/tt2.png"&gt;&lt;img src="https://raw.githubusercontent.com/uhlik/bpy/media/tt2.png" alt="Time Tracker" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here you can enable / disable logging, clear data collected so far, set custom data path (.csv) and see short summary of collected data and open individual project directories in file browser. The project name is determined by directory name where the blend is. For example if you have project in directory named "MyNewOutstandingProject" and all blends are inside subdirectory "models", set level number to 1 and you will see project name in results. 0 is directory directly above blend, 1 is one directory above blend, and so on. If you are like me and all your projects have the same subdirectory structure, sent directory level and you are good to go.&lt;/p&gt;
&lt;p&gt;changelog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0.2.0 updated for 2.80&lt;/li&gt;
&lt;li&gt;0.1.0 added simple ui&lt;/li&gt;
&lt;li&gt;0.0.8 ui tweaks, more advanced options, minor bugfixes&lt;/li&gt;
&lt;li&gt;0.0.7 fixed performance and sorting, added tracking of files which were closed without saving once a minute (can be enabled in preferences: check Track Scene Update)&lt;/li&gt;
&lt;li&gt;0.0.6 first release&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://blenderartists.org/forum/showthread.php?345129-Time-Tracker-addon" rel="nofollow"&gt;BlenderArtist.org thread&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>uhlik</author><guid isPermaLink="false">https://github.com/uhlik/bpy</guid><pubDate>Tue, 11 Feb 2020 00:24:00 GMT</pubDate></item><item><title>tensorflow/magenta #25 in Python, Today</title><link>https://github.com/tensorflow/magenta</link><description>&lt;p&gt;&lt;i&gt;Magenta: Music and Art Generation with Machine Intelligence&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="magenta-logo-bg.png"&gt;&lt;img src="magenta-logo-bg.png" height="75" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/tensorflow/magenta" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cd985f50f1ab190472221a75434e75e5f2c6c189/68747470733a2f2f7472617669732d63692e6f72672f74656e736f72666c6f772f6d6167656e74612e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/tensorflow/magenta.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://badge.fury.io/py/magenta" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/15b971d06001b08221a3074697e3b1c81aba1969/68747470733a2f2f62616467652e667572792e696f2f70792f6d6167656e74612e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/magenta.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Magenta&lt;/strong&gt; is a research project exploring the role of machine learning
in the process of creating art and music.  Primarily this
involves developing new deep learning and reinforcement learning
algorithms for generating songs, images, drawings, and other materials. But it's also
an exploration in building smart tools and interfaces that allow
artists and musicians to extend (not replace!) their processes using
these models.  Magenta was started by some researchers and engineers
from the &lt;a href="https://research.google.com/teams/brain/" rel="nofollow"&gt;Google Brain team&lt;/a&gt;,
but many others have contributed significantly to the project. We use
&lt;a href="https://www.tensorflow.org" rel="nofollow"&gt;TensorFlow&lt;/a&gt; and release our models and
tools in open source on this GitHub.  If you’d like to learn more
about Magenta, check out our &lt;a href="https://magenta.tensorflow.org" rel="nofollow"&gt;blog&lt;/a&gt;,
where we post technical details.  You can also join our &lt;a href="https://groups.google.com/a/tensorflow.org/forum/#!forum/magenta-discuss" rel="nofollow"&gt;discussion
group&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is the home for our Python TensorFlow library. To use our models in the browser with &lt;a href="https://js.tensorflow.org/" rel="nofollow"&gt;TensorFlow.js&lt;/a&gt;, head to the &lt;a href="https://github.com/tensorflow/magenta-js"&gt;Magenta.js&lt;/a&gt; repository.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;Take a look at our &lt;a href="https://magenta.tensorflow.org/demos/colab/" rel="nofollow"&gt;colab notebooks&lt;/a&gt; for various models, including one on &lt;a href="https://colab.research.google.com/notebooks/magenta/hello_magenta/hello_magenta.ipynb" rel="nofollow"&gt;getting started&lt;/a&gt;.
&lt;a href="https://github.com/tensorflow/magenta-js"&gt;Magenta.js&lt;/a&gt; is a also a good resource for models and &lt;a href="https://magenta.tensorflow.org/demos/web/" rel="nofollow"&gt;demos&lt;/a&gt; that run in the browser.
This and more, including &lt;a href="https://magenta.tensorflow.org/blog" rel="nofollow"&gt;blog posts&lt;/a&gt; and &lt;a href="https://magenta.tensorflow.org/demos/native/" rel="nofollow"&gt;Ableton Live plugins&lt;/a&gt;, can be found at &lt;a href="https://magenta.tensorflow.org" rel="nofollow"&gt;https://magenta.tensorflow.org&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-magenta-repo" class="anchor" aria-hidden="true" href="#magenta-repo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Magenta Repo&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-magenta"&gt;Using Magenta&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#development-environment"&gt;Development Environment (Advanced)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;Magenta maintains a &lt;a href="https://pypi.python.org/pypi/magenta" rel="nofollow"&gt;pip package&lt;/a&gt; for easy
installation. We recommend using Anaconda to install it, but it can work in any
standard Python environment. We support both Python 2 (&amp;gt;= 2.7) and Python 3 (&amp;gt;= 3.5).
These instructions will assume you are using Anaconda.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-automated-install-w-anaconda" class="anchor" aria-hidden="true" href="#automated-install-w-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Automated Install (w/ Anaconda)&lt;/h3&gt;
&lt;p&gt;If you are running Mac OS X or Ubuntu, you can try using our automated
installation script. Just paste the following command into your terminal.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;curl https://raw.githubusercontent.com/tensorflow/magenta/master/magenta/tools/magenta-install.sh &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; /tmp/magenta-install.sh
bash /tmp/magenta-install.sh&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After the script completes, open a new terminal window so the environment
variable changes take effect.&lt;/p&gt;
&lt;p&gt;The Magenta libraries are now available for use within Python programs and
Jupyter notebooks, and the Magenta scripts are installed in your path!&lt;/p&gt;
&lt;p&gt;Note that you will need to run &lt;code&gt;source activate magenta&lt;/code&gt; to use Magenta every
time you open a new terminal window.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-manual-install-wo-anaconda" class="anchor" aria-hidden="true" href="#manual-install-wo-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Manual Install (w/o Anaconda)&lt;/h3&gt;
&lt;p&gt;If the automated script fails for any reason, or you'd prefer to install by
hand, do the following steps.&lt;/p&gt;
&lt;p&gt;Install the Magenta pip package:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install magenta&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: In order to install the &lt;code&gt;rtmidi&lt;/code&gt; package that we depend on, you may need to install headers for some sound libraries. On Linux, this command should install the necessary packages:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo apt-get install build-essential libasound2-dev libjack-dev portaudio19-dev&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The Magenta libraries are now available for use within Python programs and
Jupyter notebooks, and the Magenta scripts are installed in your path!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-magenta" class="anchor" aria-hidden="true" href="#using-magenta"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Magenta&lt;/h2&gt;
&lt;p&gt;You can now train our various models and use them to generate music, audio, and images. You can
find instructions for each of the models by exploring the &lt;a href="magenta/models"&gt;models directory&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-development-environment" class="anchor" aria-hidden="true" href="#development-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development Environment&lt;/h2&gt;
&lt;p&gt;If you want to develop on Magenta, you'll need to set up the full Development Environment.&lt;/p&gt;
&lt;p&gt;First, clone this repository:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/tensorflow/magenta.git&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, install the dependencies by changing to the base directory and executing the setup command:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -e &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can now edit the files and run scripts by calling Python as usual. For example, this is how you would run the &lt;code&gt;melody_rnn_generate&lt;/code&gt; script from the base directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python magenta/models/melody_rnn/melody_rnn_generate --config=...&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can also install the (potentially modified) package with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Before creating a pull request, please also test your changes with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install pytest-pylint
pytest&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-pip-release" class="anchor" aria-hidden="true" href="#pip-release"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PIP Release&lt;/h2&gt;
&lt;p&gt;To build a new version for pip, bump the version and then run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python setup.py &lt;span class="pl-c1"&gt;test&lt;/span&gt;
python setup.py bdist_wheel --universal
twine upload dist/magenta-N.N.N-py2.py3-none-any.whl&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensorflow</author><guid isPermaLink="false">https://github.com/tensorflow/magenta</guid><pubDate>Tue, 11 Feb 2020 00:25:00 GMT</pubDate></item></channel></rss>