<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Python, Today</title><link>https://github.com/trending/python?since=daily</link><description>The top repositories on GitHub for python, measured daily</description><pubDate>Wed, 30 Oct 2019 00:04:44 GMT</pubDate><lastBuildDate>Wed, 30 Oct 2019 00:04:44 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>1400</ttl><item><title>google-research/google-research #1 in Python, Today</title><link>https://github.com/google-research/google-research</link><description>&lt;p&gt;&lt;i&gt;Google AI Research&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-google-ai-research" class="anchor" aria-hidden="true" href="#google-ai-research"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google AI Research&lt;/h1&gt;
&lt;p&gt;This repository contains code released by
&lt;a href="https://ai.google/research" rel="nofollow"&gt;Google AI Research&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Because the repo is large, we recommend you clone the repo without its history.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/google-research</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>geekcomputers/Python #2 in Python, Today</title><link>https://github.com/geekcomputers/Python</link><description>&lt;p&gt;&lt;i&gt;My Python Examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-my-python-examples" class="anchor" aria-hidden="true" href="#my-python-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;My Python Examples&lt;/h1&gt;
&lt;p&gt;I do not consider myself a programmer. I create these little programs as experiments to play with the language, or to solve problems for myself. I would gladly accept pointers from others to improve, simplify, or make the code more efficient. If you would like to make any comments then please feel free to email me at &lt;a href="mailto:craig@geekcomputers.co.uk"&gt;craig@geekcomputers.co.uk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These scripts contain important functions which help reduce human workload.
Code documentation is aligned correctly when the files are viewed in &lt;a href="https://notepad-plus-plus.org/" rel="nofollow"&gt;Notepad++&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/batch_file_rename.py"&gt;batch_file_rename.py&lt;/a&gt; - This batch renames a group of files in a given directory, once you pass the current and the new extensions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/create_dir_if_not_there.py"&gt;create_dir_if_not_there.py&lt;/a&gt; - Checks to see if a directory exists in the users home directory, if not then create it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/youtube-downloader%20fast.py"&gt;Fast Youtube Downloader&lt;/a&gt; - Downloads YouTube videos quickly with parallel threads using aria2c&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/tree/master/Google_Image_Downloader"&gt;Google Image Downloader&lt;/a&gt; - Query a given term and retrieve images from the Google Image database.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/dir_test.py"&gt;dir_test.py&lt;/a&gt; - Tests to see if the directory &lt;code&gt;testdir&lt;/code&gt; exists, if not it will create the directory for you.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/env_check.py"&gt;env_check.py&lt;/a&gt; - This script will check to see if all of the environment variables required are set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/Ratna04priya/Python/blob/master/BlackJack_game/blackjack.py"&gt;blackjack.py&lt;/a&gt; - This script contains the Casino BlackJack-21 Game in Python.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/fileinfo.py"&gt;fileinfo.py&lt;/a&gt; - Shows file information for a given file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/folder_size.py"&gt;folder_size.py&lt;/a&gt; - Scans the current directory and all subdirectories and displays the size.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/logs.py"&gt;logs.py&lt;/a&gt; - This script will search for all &lt;code&gt;*.log&lt;/code&gt; files in the given directory, zip them using the program you specify, and then date stamp them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/move_files_over_x_days.py"&gt;move_files_over_x_days.py&lt;/a&gt; - Moves all files over a specified age (in days) from the source directory to the destination directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/nslookup_check.py"&gt;nslookup_check.py&lt;/a&gt; - This simple script opens the file &lt;code&gt;server_list.txt&lt;/code&gt; and then does an nslookup for each one to check the DNS entry.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/osinfo.py"&gt;osinfo.py&lt;/a&gt; - Displays some information about the OS on which you are running this script.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/ping_servers.py"&gt;ping_servers.py&lt;/a&gt; - This script, depending on the arguments supplied, will ping the servers associated with that application group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/ping_subnet.py"&gt;ping_subnet.py&lt;/a&gt; - After supplying the first 3 octets this file scans the final range for available addresses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/powerdown_startup.py"&gt;powerdown_startup.py&lt;/a&gt; - This file goes through the server list and pings the machine, if it is up it will load the putty session, if it is not it will notify you.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/puttylogs.py"&gt;puttylogs.py&lt;/a&gt; -  This file zips up all the logs in the given directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/script_count.py"&gt;script_count.py&lt;/a&gt; - This file scans the scripts directory and gives a count of the different types of scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[get_youtube_view.py] - This is very simple python script to get more views for your youtube videos.Some times I use for repeating my favorite songs by this scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/script_listing.py"&gt;script_listing.py&lt;/a&gt; - This file will list all the files in the given directory, and go through all the subdirectories as well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/testlines.py"&gt;testlines.py&lt;/a&gt; - This simple script opens a file and prints out 100 lines of whatever is the set for the line variable.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/tweeter.py"&gt;tweeter.py&lt;/a&gt; - Allows you to tweet text or a picture from the terminal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/serial_scanner.py"&gt;serial_scanner.py&lt;/a&gt; contains a method called ListAvailablePorts which returns a list with the names of the serial ports that are in use in the computer. This method works only on Linux and Windows (can be extended for mac osx). If no port is found, an empty list is returned.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/get_youtube_view.py"&gt;get_youtube_view.py&lt;/a&gt; - A simple python script to get more views for your YouTube videos. Useful for repeating songs on YouTube.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/CountMillionCharacter.py"&gt;CountMillionCharacter.py&lt;/a&gt; And &lt;a href="https://github.com/geekcomputers/Python/blob/master/CountMillionCharacters-2.0.py"&gt;CountMillionCharacter2.0&lt;/a&gt;.py - Gets character count of a text file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/xkcd_downloader.py"&gt;xkcd_downloader.py&lt;/a&gt; - Downloads the latest XKCD comic and places them in a new folder called "comics".&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/timymodule.py"&gt;timymodule.py&lt;/a&gt; - A great alternative to Pythons 'timeit' module and easier to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/calculator.py"&gt;calculator.py&lt;/a&gt; - Uses Python's eval() function to implement a calculator.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/Google_News.py"&gt;Google_News.py&lt;/a&gt; - Uses BeautifulSoup to provide Latest news headline along with news link.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/Cricket_score.py"&gt;cricket_live_score&lt;/a&gt; - Uses BeautifulSoup to provide live cricket score.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/youtube.py"&gt;youtube.py&lt;/a&gt; - Takes a song name as input and fetches the YouTube URL of the best matching song and plays it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/site_health.py"&gt;site_health.py&lt;/a&gt; - Checks the health of a remote server&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/SimpleStopWatch.py"&gt;SimpleStopWatch.py&lt;/a&gt; - Simple Stop Watch implementation using Python's time module.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/changemac.py"&gt;Changemac.py&lt;/a&gt; - This script change your MAC address , generate random MAC address or enter input as new MAC address in your linux(Successfully Tested in Ubuntu 18.04).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/whatsapp-monitor.py"&gt;whatsapp-monitor.py&lt;/a&gt; - Uses Selenium to give online status about your contacts when your contacts become online in whatsapp you will get an update about it on terminal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/subahanii/whatsapp-Chat-Analyzer"&gt;whatsapp-chat-analyzer.py&lt;/a&gt; - This is whatsapp group/individual chat analyzer .
This script is able to analyse all activity happened in whatsapp group and visualize all thing through matplotlib library(In Graph form).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>geekcomputers</author><guid isPermaLink="false">https://github.com/geekcomputers/Python</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>bitcoinbook/bitcoinbook #3 in Python, Today</title><link>https://github.com/bitcoinbook/bitcoinbook</link><description>&lt;p&gt;&lt;i&gt;Mastering Bitcoin 2nd Edition - Programming the Open Blockchain&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;Code Examples: &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/57c2d755f3c0c1f750bb5dcd687aa2b5d640aa84/68747470733a2f2f7472617669732d63692e6f72672f626974636f696e626f6f6b2f626974636f696e626f6f6b2e7376673f6272616e63683d646576656c6f70"&gt;&lt;img src="https://camo.githubusercontent.com/57c2d755f3c0c1f750bb5dcd687aa2b5d640aa84/68747470733a2f2f7472617669732d63692e6f72672f626974636f696e626f6f6b2f626974636f696e626f6f6b2e7376673f6272616e63683d646576656c6f70" alt="travis_ci" data-canonical-src="https://travis-ci.org/bitcoinbook/bitcoinbook.svg?branch=develop" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-mastering-bitcoin" class="anchor" aria-hidden="true" href="#mastering-bitcoin"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mastering Bitcoin&lt;/h1&gt;
&lt;p&gt;Mastering Bitcoin is a book for developers, although the first two chapters cover bitcoin at a level that is also approachable to non-programmers. Anyone with a basic understanding of technology can read the first two chapters to get a great understanding of bitcoin.&lt;/p&gt;
&lt;p&gt;This repository contains the complete &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/Edition1Print2"&gt;first edition, second print&lt;/a&gt;, published in December 2014, and the complete &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/second_edition_print2"&gt;second edition, second print&lt;/a&gt;, published in July 2017, as published by O'Reilly Media in paperback and ebook formats.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-issues-errors-comments-contributions" class="anchor" aria-hidden="true" href="#issues-errors-comments-contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Issues, Errors, Comments, Contributions&lt;/h1&gt;
&lt;p&gt;If you know how to make a pull request to contribute a fix, please write the correction and use a pull request to submit it for consideration against the &lt;a href="https://github.com/bitcoinbook/bitcoinbook/tree/develop"&gt;develop branch&lt;/a&gt;. If you are making several changes, please use a separate commit for each to make it easier to cherry-pick or resolve conflicts. Otherwise, please submit an issue, explaining the error or comment. If you would like to contribute extensive changes or new material, please coordinate with the author first; contact information can be found on his website: &lt;a href="https://antonopoulos.com/" rel="nofollow"&gt;https://antonopoulos.com/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-reading-this-book" class="anchor" aria-hidden="true" href="#reading-this-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reading this book&lt;/h1&gt;
&lt;p&gt;To read this book, see &lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/book.asciidoc"&gt;book.asciidoc&lt;/a&gt;. Click on each of the chapters to read in your browser. Other parties may choose to release PDFs of the book online.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-chapters" class="anchor" aria-hidden="true" href="#chapters"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Chapters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 1: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch01.asciidoc"&gt;Introduction&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 2: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch02.asciidoc"&gt;How Bitcoin Works&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 3: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch03.asciidoc"&gt;Bitcoin Core: The Reference Implementation&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 4: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch04.asciidoc"&gt;Keys, Addresses&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 5: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch05.asciidoc"&gt;Wallets&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 6: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch06.asciidoc"&gt;Transactions&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 7: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch07.asciidoc"&gt;Advanced Transactions and Scripting&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 8: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch08.asciidoc"&gt;The Bitcoin Network&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 9: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch09.asciidoc"&gt;The Blockchain&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 10: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch10.asciidoc"&gt;Mining and Consensus&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 11: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch11.asciidoc"&gt;Bitcoin Security&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 12: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch12.asciidoc"&gt;Blockchain Applications&lt;/a&gt;'&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-published" class="anchor" aria-hidden="true" href="#published"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Published&lt;/h1&gt;
&lt;p&gt;"Mastering Bitcoin (Second Edition, Second Print): Programming the Open Blockchain" is now available in paperback and ebook formats by many booksellers worldwide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Mastering-Bitcoin-Programming-Open-Blockchain/dp/1491954388" rel="nofollow"&gt;Amazon&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mastering Bitcoin (First Edition Second Print) is also published in Japanese, Korean, and Chinese (Simplified) by publishers in the respective countries.&lt;/p&gt;
&lt;p&gt;Mastering Bitcoin (Open Edition), based on the First Edition, has been translated by volunteers into more than a dozen languages. Translations are available for free under CC-BY-SA license at: &lt;a href="https://bitcoinbook.info" rel="nofollow"&gt;https://bitcoinbook.info&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-source" class="anchor" aria-hidden="true" href="#source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source&lt;/h1&gt;
&lt;p&gt;The book's source code, found in this repository, is kept synchronized with the print and ebook editions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-mastering-bitcoin---first-edition" class="anchor" aria-hidden="true" href="#mastering-bitcoin---first-edition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mastering Bitcoin - First Edition&lt;/h2&gt;
&lt;p&gt;The tags &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/Edition1Print1"&gt;Edition1Print1&lt;/a&gt;, &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/Edition1Print2"&gt;Edition1Print2&lt;/a&gt; correspond to the two existing prints of Mastering Bitcoin (First Edition) as published by O'Reilly Media.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/e170e276291254896665fa8f612b99fe5b7dd005/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d73612f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;span&gt;Mastering Bitcoin - First Edition&lt;/span&gt; by &lt;a href="https://antonopoulos.com/" rel="nofollow"&gt;Andreas M. Antonopoulos LLC&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This "Free Culture" compliant license was approved by my publisher O'Reilly Media (&lt;a href="http://oreilly.com" rel="nofollow"&gt;http://oreilly.com&lt;/a&gt;), who understands the value of open source. O'Reilly Media is not just the world's best publisher of technical books, but is also a strong supporter of this open culture and the sharing of knowledge.&lt;/p&gt;
&lt;p&gt;Thank you O'Reilly Media!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-mastering-bitcoin---second-edition" class="anchor" aria-hidden="true" href="#mastering-bitcoin---second-edition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mastering Bitcoin - Second Edition&lt;/h2&gt;
&lt;p&gt;The tags, &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/second_edition_print_1"&gt;second_edition_print_1&lt;/a&gt; and  &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/second_edition_print2"&gt;second_edition_print2&lt;/a&gt;, correspond to the first (June 8th, 2017) and second (July 20th, 2017) print of Mastering Bitcoin (Second Edition), as published by O'Reilly Media.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/e170e276291254896665fa8f612b99fe5b7dd005/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d73612f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;span&gt;Mastering Bitcoin - Second Edition&lt;/span&gt; by &lt;a href="https://antonopoulos.com/" rel="nofollow"&gt;Andreas M. Antonopoulos LLC&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h1&gt;
&lt;p&gt;If you are interested in translating this book, please join our team of volunteers at: &lt;a href="https://www.transifex.com/aantonop/mastering-bitcoin" rel="nofollow"&gt;https://www.transifex.com/aantonop/mastering-bitcoin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Free copies of "Mastering Bitcoin Open Edition," translated in many languages, can be downloaded from: &lt;a href="https://bitcoinbook.info" rel="nofollow"&gt;https://bitcoinbook.info&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bitcoinbook</author><guid isPermaLink="false">https://github.com/bitcoinbook/bitcoinbook</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>521xueweihan/HelloGitHub #4 in Python, Today</title><link>https://github.com/521xueweihan/HelloGitHub</link><description>&lt;p&gt;&lt;i&gt;:octocat: Find pearls on open-source seashore 分享 GitHub 上有趣、入门级的开源项目&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/readme.gif"&gt;&lt;img src="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/readme.gif" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;br&gt;中文 | &lt;a href="README_en.md"&gt;English&lt;/a&gt;
  &lt;br&gt;&lt;strong&gt;HelloGitHub&lt;/strong&gt; 一个分享 GitHub 上有趣、入门级的开源项目。&lt;br&gt;兴趣是最好的老师，这里能够帮你找到编程的兴趣！
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://hellogithub.com/weixin.png" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/61343b85520a4714ddb37eb300f8268cc881ae7e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f54616c6b2d2545352542452541452545342542462541312545372542452541342d627269676874677265656e2e7376673f7374796c653d706f706f75742d737175617265" alt="WeiXin" data-canonical-src="https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://github.com/521xueweihan/HelloGitHub/stargazers"&gt;&lt;img src="https://camo.githubusercontent.com/0aec7fa1a5647255bbe8af37a82a007be69d8739/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f35323178756577656968616e2f48656c6c6f4769744875622e7376673f7374796c653d706f706f75742d737175617265" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://github.com/521xueweihan/HelloGitHub/issues"&gt;&lt;img src="https://camo.githubusercontent.com/a8367e38e94eccf7e469023edfec05db15132454/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f35323178756577656968616e2f48656c6c6f4769744875622e7376673f7374796c653d706f706f75742d737175617265" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://weibo.com/hellogithub" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4627590b5d81a690c6c83abaf47f678d70d26e6b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2545362539362542302545362542352541412d576569626f2d7265642e7376673f7374796c653d706f706f75742d737175617265" alt="Sina Weibo" data-canonical-src="https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简介&lt;/h2&gt;
&lt;p&gt;这是一个面向编程新手、热爱编程、对开源社区感兴趣人群的项目，内容&lt;strong&gt;每月 28 号&lt;/strong&gt;以月刊的形式更新发布。内容包括：&lt;strong&gt;流行项目&lt;/strong&gt;、&lt;strong&gt;入门级项目&lt;/strong&gt;、&lt;strong&gt;让生活变得更美好的工具&lt;/strong&gt;、&lt;strong&gt;书籍&lt;/strong&gt;、&lt;strong&gt;学习心得笔记&lt;/strong&gt;、&lt;strong&gt;企业级项目&lt;/strong&gt;等，这些开源项目大多都是非常容易上手、很 Cool，能够让你用很短时间感受到编程的魅力和便捷。从而让大家感受到编程的乐趣，动手开始编程。&lt;/p&gt;
&lt;p&gt;希望通过本项目能够有更多人加入到开源社区、回馈社区。&lt;strong&gt;让有趣、有价值的项目被更多人发现和加入&lt;/strong&gt;。在参与这些项目的过程中，你将得到：&lt;strong&gt;热爱编程的小伙伴&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="man_dancing" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f57a.png"&gt;🕺&lt;/g-emoji&gt; 、&lt;strong&gt;更多编程知识&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png"&gt;📚&lt;/g-emoji&gt; 、&lt;strong&gt;优秀的编程技巧&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;💻&lt;/g-emoji&gt; 、&lt;strong&gt;找到编程的乐趣&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="video_game" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ae.png"&gt;🎮&lt;/g-emoji&gt; 。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;『每日精选』&lt;/strong&gt; 关注我们的&lt;a href="https://weibo.com/hellogithub" rel="nofollow"&gt;最惨官微&lt;/a&gt;获取最新项目推荐。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;『讲解开源项目』&lt;/strong&gt; 欢迎开源爱好者给我们投稿&lt;a href="https://www.yuque.com/docs/share/ac165a31-927e-4c9a-8044-5a8e0de372e7" rel="nofollow"&gt;查看创作须知&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-内容" class="anchor" aria-hidden="true" href="#内容"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;内容&lt;/h2&gt;
&lt;p&gt;每月 28 号发布&lt;a href="/content/last.md"&gt;最新一期&lt;/a&gt; | &lt;a href="https://hellogithub.com" rel="nofollow"&gt;官网&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img class="emoji" title=":shipit:" alt=":shipit:" src="https://github.githubassets.com/images/icons/emoji/shipit.png" height="20" width="20" align="absmiddle"&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="jack_o_lantern" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f383.png"&gt;🎃&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="beer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f37a.png"&gt;🍺&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="fish_cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f365.png"&gt;🍥&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;img class="emoji" title=":octocat:" alt=":octocat:" src="https://github.githubassets.com/images/icons/emoji/octocat.png" height="20" width="20" align="absmiddle"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/43/HelloGitHub43.md"&gt;第 43 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/42/HelloGitHub42.md"&gt;第 42 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/41/HelloGitHub41.md"&gt;第 41 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/40/HelloGitHub40.md"&gt;第 40 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/39/HelloGitHub39.md"&gt;第 39 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/38/HelloGitHub38.md"&gt;第 38 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/37/HelloGitHub37.md"&gt;第 37 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/36/HelloGitHub36.md"&gt;第 36 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/35/HelloGitHub35.md"&gt;第 35 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/34/HelloGitHub34.md"&gt;第 34 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/33/HelloGitHub33.md"&gt;第 33 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/32/HelloGitHub32.md"&gt;第 32 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/31/HelloGitHub31.md"&gt;第 31 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/30/HelloGitHub30.md"&gt;第 30 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/29/HelloGitHub29.md"&gt;第 29 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/28/HelloGitHub28.md"&gt;第 28 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/27/HelloGitHub27.md"&gt;第 27 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/26/HelloGitHub26.md"&gt;第 26 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/25/HelloGitHub25.md"&gt;第 25 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/24/HelloGitHub24.md"&gt;第 24 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/23/HelloGitHub23.md"&gt;第 23 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/22/HelloGitHub22.md"&gt;第 22 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/21/HelloGitHub21.md"&gt;第 21 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/20/HelloGitHub20.md"&gt;第 20 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/19/HelloGitHub19.md"&gt;第 19 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/18/HelloGitHub18.md"&gt;第 18 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/17/HelloGitHub17.md"&gt;第 17 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/16/HelloGitHub16.md"&gt;第 16 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/15/HelloGitHub15.md"&gt;第 15 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/14/HelloGitHub14.md"&gt;第 14 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/13/HelloGitHub13.md"&gt;第 13 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/12/HelloGitHub12.md"&gt;第 12 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/11/HelloGitHub11.md"&gt;第 11 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/10/HelloGitHub10.md"&gt;第 10 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/09/HelloGitHub09.md"&gt;第 09 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/08/HelloGitHub08.md"&gt;第 08 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/07/HelloGitHub07.md"&gt;第 07 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/06/HelloGitHub06.md"&gt;第 06 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/05/HelloGitHub05.md"&gt;第 05 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/04/HelloGitHub04.md"&gt;第 04 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/03/HelloGitHub03.md"&gt;第 03 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/02/HelloGitHub02.md"&gt;第 02 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/01/HelloGitHub01.md"&gt;第 01 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;欢迎&lt;a href="https://github.com/521xueweihan/HelloGitHub/issues/new"&gt;推荐或自荐项目&lt;/a&gt;成为 &lt;strong&gt;HelloGitHub&lt;/strong&gt; 的&lt;a href="https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md"&gt;贡献者&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-贡献者" class="anchor" aria-hidden="true" href="#贡献者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;贡献者&lt;/h2&gt;
&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/521xueweihan"&gt;
          &lt;img src="https://avatars2.githubusercontent.com/u/8255800?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;削微寒&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/ming995"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/46031112?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;糖醋里脊&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/FrontMage"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/17007026?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;FrontMage&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/xibinyue"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/14122146?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;xibinyue&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/Eurus-Holmes"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/34226570?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Feiyang Chen&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/ChungZH"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/42088872?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;ChungZH&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/daixiang0"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/26538619?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;daixiang0&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/nivance"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/3291404?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;nivance&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/hellowHuaairen"&gt;
          &lt;img src="https://avatars2.githubusercontent.com/u/19610305?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;hellowHuaairen&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/17665302?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;更多贡献者&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-合作组织" class="anchor" aria-hidden="true" href="#合作组织"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;合作组织&lt;/h2&gt;
&lt;p&gt;欢迎各种&lt;img class="emoji" title=":octocat:" alt=":octocat:" src="https://github.githubassets.com/images/icons/emoji/octocat.png" height="20" width="20" align="absmiddle"&gt;开源组织合作&lt;a href="Mailto:595666367@qq.com"&gt;点击联系我&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/FGDBTKD"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/40509403?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;FGDBTKD&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;AI/ML/DL/NLP&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/d2-projects"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/40857578?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;D2 Projects&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Vue/JavaScript&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/doocs"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/43716716?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Doocs&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Technical Knowledge&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-声明" class="anchor" aria-hidden="true" href="#声明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;声明&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="nofollow"&gt;&lt;img alt="知识共享许可协议" src="https://camo.githubusercontent.com/1ae74a56e22c4897b6fbfb9f301bd829c77429a7/68747470733a2f2f6c6963656e7365627574746f6e732e6e65742f6c2f62792d6e632d6e642f342e302f38387833312e706e67" data-canonical-src="https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;本作品采用 &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="nofollow"&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 进行许可。&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>521xueweihan</author><guid isPermaLink="false">https://github.com/521xueweihan/HelloGitHub</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>google-research/bert #5 in Python, Today</title><link>https://github.com/google-research/bert</link><description>&lt;p&gt;&lt;i&gt;TensorFlow code and pre-trained models for BERT&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-bert" class="anchor" aria-hidden="true" href="#bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BERT&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;***** New May 31st, 2019: Whole Word Masking Models *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a release of several new models which were the result of an improvement
the pre-processing code.&lt;/p&gt;
&lt;p&gt;In the original pre-processing code, we randomly select WordPiece tokens to
mask. For example:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Input Text: the man jumped up , put his basket on phil ##am ##mon ' s head&lt;/code&gt;
&lt;code&gt;Original Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The new technique is called Whole Word Masking. In this case, we always mask
&lt;em&gt;all&lt;/em&gt; of the the tokens corresponding to a word at once. The overall masking
rate remains the same.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The training is identical -- we still predict each masked WordPiece token
independently. The improvement comes from the fact that the original prediction
task was too 'easy' for words that had been split into multiple WordPieces.&lt;/p&gt;
&lt;p&gt;This can be enabled during data generation by passing the flag
&lt;code&gt;--do_whole_word_mask=True&lt;/code&gt; to &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Pre-trained models with Whole Word Masking are linked below. The data and
training were otherwise identical, and the models have identical structure and
vocab to the original models. We only include BERT-Large models. When using
these models, please make it clear in the paper that you are using the Whole
Word Masking variant of BERT-Large.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;SQUAD 1.1 F1/EM&lt;/th&gt;
&lt;th align="center"&gt;Multi NLI Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.0/84.3&lt;/td&gt;
&lt;td align="center"&gt;86.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.8/86.7&lt;/td&gt;
&lt;td align="center"&gt;87.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.5/84.8&lt;/td&gt;
&lt;td align="center"&gt;86.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.9/86.7&lt;/td&gt;
&lt;td align="center"&gt;86.46&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;***** New February 7th, 2019: TfHub Module *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;BERT has been uploaded to &lt;a href="https://tfhub.dev" rel="nofollow"&gt;TensorFlow Hub&lt;/a&gt;. See
&lt;code&gt;run_classifier_with_tfhub.py&lt;/code&gt; for an example of how to use the TF Hub module,
or run an example in the browser on
&lt;a href="https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb" rel="nofollow"&gt;Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 23rd, 2018: Un-normalized multilingual model + Thai +
Mongolian *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We uploaded a new multilingual model which does &lt;em&gt;not&lt;/em&gt; perform any normalization
on the input (no lower casing, accent stripping, or Unicode normalization), and
additionally inclues Thai and Mongolian.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It is recommended to use this version for developing multilingual models,
especially on languages with non-Latin alphabets.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This does not require any code changes, and can be downloaded here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;***** New November 15th, 2018: SOTA SQuAD 2.0 System *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which is
currently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of the
README for details.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 5th, 2018: Third-party PyTorch and Chainer versions of
BERT available *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NLP researchers from HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. Sosuke Kobayashi also made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
(Thanks!) We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 3rd, 2018: Multilingual and Chinese models available
*****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have made two new BERT models available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use character-based tokenization for Chinese, and WordPiece tokenization for
all other languages. Both models should work out-of-the-box without any code
changes. We did update the implementation of &lt;code&gt;BasicTokenizer&lt;/code&gt; in
&lt;code&gt;tokenization.py&lt;/code&gt; to support Chinese character tokenization, so please update if
you forked it. However, we did not change the tokenization API.&lt;/p&gt;
&lt;p&gt;For more, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** End new information *****&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BERT&lt;/strong&gt;, or &lt;strong&gt;B&lt;/strong&gt;idirectional &lt;strong&gt;E&lt;/strong&gt;ncoder &lt;strong&gt;R&lt;/strong&gt;epresentations from
&lt;strong&gt;T&lt;/strong&gt;ransformers, is a new method of pre-training language representations which
obtains state-of-the-art results on a wide array of Natural Language Processing
(NLP) tasks.&lt;/p&gt;
&lt;p&gt;Our academic paper which describes BERT in detail and provides full results on a
number of tasks can be found here:
&lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To give a few numbers, here are the results on the
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD v1.1&lt;/a&gt; question answering
task:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SQuAD v1.1 Leaderboard (Oct 8th 2018)&lt;/th&gt;
&lt;th align="center"&gt;Test EM&lt;/th&gt;
&lt;th align="center"&gt;Test F1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Ensemble - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;87.4&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;93.2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Ensemble - nlnet&lt;/td&gt;
&lt;td align="center"&gt;86.0&lt;/td&gt;
&lt;td align="center"&gt;91.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Single Model - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;85.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.8&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Single Model - nlnet&lt;/td&gt;
&lt;td align="center"&gt;83.5&lt;/td&gt;
&lt;td align="center"&gt;90.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And several natural language inference tasks:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th align="center"&gt;MultiNLI&lt;/th&gt;
&lt;th align="center"&gt;Question NLI&lt;/th&gt;
&lt;th align="center"&gt;SWAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.7&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.3&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenAI GPT (Prev. SOTA)&lt;/td&gt;
&lt;td align="center"&gt;82.2&lt;/td&gt;
&lt;td align="center"&gt;88.1&lt;/td&gt;
&lt;td align="center"&gt;75.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Plus many other tasks.&lt;/p&gt;
&lt;p&gt;Moreover, these results were all obtained with almost no task-specific neural
network architecture design.&lt;/p&gt;
&lt;p&gt;If you already know what BERT is and you just want to get started, you can
&lt;a href="#pre-trained-models"&gt;download the pre-trained models&lt;/a&gt; and
&lt;a href="#fine-tuning-with-bert"&gt;run a state-of-the-art fine-tuning&lt;/a&gt; in only a few
minutes.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-bert" class="anchor" aria-hidden="true" href="#what-is-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is BERT?&lt;/h2&gt;
&lt;p&gt;BERT is a method of pre-training language representations, meaning that we train
a general-purpose "language understanding" model on a large text corpus (like
Wikipedia), and then use that model for downstream NLP tasks that we care about
(like question answering). BERT outperforms previous methods because it is the
first &lt;em&gt;unsupervised&lt;/em&gt;, &lt;em&gt;deeply bidirectional&lt;/em&gt; system for pre-training NLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unsupervised&lt;/em&gt; means that BERT was trained using only a plain text corpus, which
is important because an enormous amount of plain text data is publicly available
on the web in many languages.&lt;/p&gt;
&lt;p&gt;Pre-trained representations can also either be &lt;em&gt;context-free&lt;/em&gt; or &lt;em&gt;contextual&lt;/em&gt;,
and contextual representations can further be &lt;em&gt;unidirectional&lt;/em&gt; or
&lt;em&gt;bidirectional&lt;/em&gt;. Context-free models such as
&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec" rel="nofollow"&gt;word2vec&lt;/a&gt; or
&lt;a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow"&gt;GloVe&lt;/a&gt; generate a single "word
embedding" representation for each word in the vocabulary, so &lt;code&gt;bank&lt;/code&gt; would have
the same representation in &lt;code&gt;bank deposit&lt;/code&gt; and &lt;code&gt;river bank&lt;/code&gt;. Contextual models
instead generate a representation of each word that is based on the other words
in the sentence.&lt;/p&gt;
&lt;p&gt;BERT was built upon recent work in pre-training contextual representations —
including &lt;a href="https://arxiv.org/abs/1511.01432" rel="nofollow"&gt;Semi-supervised Sequence Learning&lt;/a&gt;,
&lt;a href="https://blog.openai.com/language-unsupervised/" rel="nofollow"&gt;Generative Pre-Training&lt;/a&gt;,
&lt;a href="https://allennlp.org/elmo" rel="nofollow"&gt;ELMo&lt;/a&gt;, and
&lt;a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html" rel="nofollow"&gt;ULMFit&lt;/a&gt;
— but crucially these models are all &lt;em&gt;unidirectional&lt;/em&gt; or &lt;em&gt;shallowly
bidirectional&lt;/em&gt;. This means that each word is only contextualized using the words
to its left (or right). For example, in the sentence &lt;code&gt;I made a bank deposit&lt;/code&gt; the
unidirectional representation of &lt;code&gt;bank&lt;/code&gt; is only based on &lt;code&gt;I made a&lt;/code&gt; but not
&lt;code&gt;deposit&lt;/code&gt;. Some previous work does combine the representations from separate
left-context and right-context models, but only in a "shallow" manner. BERT
represents "bank" using both its left and right context — &lt;code&gt;I made a ... deposit&lt;/code&gt;
— starting from the very bottom of a deep neural network, so it is &lt;em&gt;deeply
bidirectional&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;BERT uses a simple approach for this: We mask out 15% of the words in the input,
run the entire sequence through a deep bidirectional
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; encoder, and then predict only
the masked words. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: the man went to the [MASK1] . he bought a [MASK2] of milk.
Labels: [MASK1] = store; [MASK2] = gallon
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to learn relationships between sentences, we also train on a simple
task which can be generated from any monolingual corpus: Given two sentences &lt;code&gt;A&lt;/code&gt;
and &lt;code&gt;B&lt;/code&gt;, is &lt;code&gt;B&lt;/code&gt; the actual next sentence that comes after &lt;code&gt;A&lt;/code&gt;, or just a random
sentence from the corpus?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: he bought a gallon of milk .
Label: IsNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: penguins are flightless .
Label: NotNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then train a large model (12-layer to 24-layer Transformer) on a large corpus
(Wikipedia + &lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt;) for a long time (1M
update steps), and that's BERT.&lt;/p&gt;
&lt;p&gt;Using BERT has two stages: &lt;em&gt;Pre-training&lt;/em&gt; and &lt;em&gt;fine-tuning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pre-training&lt;/strong&gt; is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a
one-time procedure for each language (current models are English-only, but
multilingual models will be released in the near future). We are releasing a
number of pre-trained models from the paper which were pre-trained at Google.
Most NLP researchers will never need to pre-train their own model from scratch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt; is inexpensive. All of the results in the paper can be
replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU,
starting from the exact same pre-trained model. SQuAD, for example, can be
trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of
91.0%, which is the single system state-of-the-art.&lt;/p&gt;
&lt;p&gt;The other important aspect of BERT is that it can be adapted to many types of
NLP tasks very easily. In the paper, we demonstrate state-of-the-art results on
sentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level
(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific
modifications.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-has-been-released-in-this-repository" class="anchor" aria-hidden="true" href="#what-has-been-released-in-this-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What has been released in this repository?&lt;/h2&gt;
&lt;p&gt;We are releasing the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow code for the BERT model architecture (which is mostly a standard
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; architecture).&lt;/li&gt;
&lt;li&gt;Pre-trained checkpoints for both the lowercase and cased version of
&lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; from the paper.&lt;/li&gt;
&lt;li&gt;TensorFlow code for push-button replication of the most important
fine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of the code in this repository works out-of-the-box with CPU, GPU, and Cloud
TPU.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-trained-models" class="anchor" aria-hidden="true" href="#pre-trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-trained models&lt;/h2&gt;
&lt;p&gt;We are releasing the &lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; models from the paper.
&lt;code&gt;Uncased&lt;/code&gt; means that the text has been lowercased before WordPiece tokenization,
e.g., &lt;code&gt;John Smith&lt;/code&gt; becomes &lt;code&gt;john smith&lt;/code&gt;. The &lt;code&gt;Uncased&lt;/code&gt; model also strips out any
accent markers. &lt;code&gt;Cased&lt;/code&gt; means that the true case and accent markers are
preserved. Typically, the &lt;code&gt;Uncased&lt;/code&gt; model is better unless you know that case
information is important for your task (e.g., Named Entity Recognition or
Part-of-Speech tagging).&lt;/p&gt;
&lt;p&gt;These models are all released under the same license as the source code (Apache
2.0).&lt;/p&gt;
&lt;p&gt;For information about the Multilingual and Chinese model, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When using a cased model, make sure to pass &lt;code&gt;--do_lower=False&lt;/code&gt; to the training
scripts. (Or pass &lt;code&gt;do_lower_case=False&lt;/code&gt; directly to &lt;code&gt;FullTokenizer&lt;/code&gt; if you're
using your own script.)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The links to the models are here (right-click, 'Save link as...' on the name):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads , 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased (New, recommended)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Uncased (Orig, not recommended)&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each .zip file contains three items:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A TensorFlow checkpoint (&lt;code&gt;bert_model.ckpt&lt;/code&gt;) containing the pre-trained
weights (which is actually 3 files).&lt;/li&gt;
&lt;li&gt;A vocab file (&lt;code&gt;vocab.txt&lt;/code&gt;) to map WordPiece to word id.&lt;/li&gt;
&lt;li&gt;A config file (&lt;code&gt;bert_config.json&lt;/code&gt;) which specifies the hyperparameters of
the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-fine-tuning-with-bert" class="anchor" aria-hidden="true" href="#fine-tuning-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with BERT&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: All results on the paper were fine-tuned on a single Cloud TPU,
which has 64GB of RAM. It is currently not possible to re-produce most of the
&lt;code&gt;BERT-Large&lt;/code&gt; results on the paper using a GPU with 12GB - 16GB of RAM, because
the maximum batch size that can fit in memory is too small. We are working on
adding code to this repository which allows for much larger effective batch size
on the GPU. See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for
more details.&lt;/p&gt;
&lt;p&gt;This code was tested with TensorFlow 1.11.0. It was tested with Python2 and
Python3 (but more thoroughly with Python2, since this is what's used internally
in Google).&lt;/p&gt;
&lt;p&gt;The fine-tuning examples which use &lt;code&gt;BERT-Base&lt;/code&gt; should be able to run on a GPU
that has at least 12GB of RAM using the hyperparameters given.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-fine-tuning-with-cloud-tpus" class="anchor" aria-hidden="true" href="#fine-tuning-with-cloud-tpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with Cloud TPUs&lt;/h3&gt;
&lt;p&gt;Most of the examples below assumes that you will be running training/evaluation
on your local machine, using a GPU like a Titan X or GTX 1080.&lt;/p&gt;
&lt;p&gt;However, if you have access to a Cloud TPU that you want to train on, just add
the following flags to &lt;code&gt;run_classifier.py&lt;/code&gt; or &lt;code&gt;run_squad.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --use_tpu=True \
  --tpu_name=$TPU_NAME
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please see the
&lt;a href="https://cloud.google.com/tpu/docs/tutorials/mnist" rel="nofollow"&gt;Google Cloud TPU tutorial&lt;/a&gt;
for how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;On Cloud TPUs, the pretrained model and the output directory will need to be on
Google Cloud Storage. For example, if you have a bucket named &lt;code&gt;some_bucket&lt;/code&gt;, you
might use the following flags instead:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --output_dir=gs://some_bucket/my_output_dir/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The unzipped pre-trained model files can also be found in the Google Cloud
Storage folder &lt;code&gt;gs://bert_models/2018_10_18&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export BERT_BASE_DIR=gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-sentence-and-sentence-pair-classification-tasks" class="anchor" aria-hidden="true" href="#sentence-and-sentence-pair-classification-tasks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sentence (and sentence-pair) classification tasks&lt;/h3&gt;
&lt;p&gt;Before running this example you must download the
&lt;a href="https://gluebenchmark.com/tasks" rel="nofollow"&gt;GLUE data&lt;/a&gt; by running
&lt;a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"&gt;this script&lt;/a&gt;
and unpack it to some directory &lt;code&gt;$GLUE_DIR&lt;/code&gt;. Next, download the &lt;code&gt;BERT-Base&lt;/code&gt;
checkpoint and unzip it to some directory &lt;code&gt;$BERT_BASE_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This example code fine-tunes &lt;code&gt;BERT-Base&lt;/code&gt; on the Microsoft Research Paraphrase
Corpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in a
few minutes on most GPUs.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue

python run_classifier.py \
  --task_name=MRPC \
  --do_train=true \
  --do_eval=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  eval_accuracy = 0.845588
  eval_loss = 0.505248
  global_step = 343
  loss = 0.505248
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that the Dev set accuracy was 84.55%. Small sets like MRPC have a
high variance in the Dev set accuracy, even when starting from the same
pre-training checkpoint. If you re-run multiple times (making sure to point to
different &lt;code&gt;output_dir&lt;/code&gt;), you should see results between 84% and 88%.&lt;/p&gt;
&lt;p&gt;A few other pre-trained models are implemented off-the-shelf in
&lt;code&gt;run_classifier.py&lt;/code&gt;, so it should be straightforward to follow those examples to
use BERT for any single-sentence or sentence-pair classification task.&lt;/p&gt;
&lt;p&gt;Note: You might see a message &lt;code&gt;Running train on CPU&lt;/code&gt;. This really just means
that it's running on something other than a Cloud TPU, which includes a GPU.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-prediction-from-classifier" class="anchor" aria-hidden="true" href="#prediction-from-classifier"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prediction from classifier&lt;/h4&gt;
&lt;p&gt;Once you have trained your classifier you can use it in inference mode by using
the --do_predict=true command. You need to have a file named test.tsv in the
input folder. Output will be created in file called test_results.tsv in the
output folder. Each line will contain output for each sample, columns are the
class probabilities.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue
&lt;span class="pl-k"&gt;export&lt;/span&gt; TRAINED_CLASSIFIER=/path/to/fine/tuned/classifier

python run_classifier.py \
  --task_name=MRPC \
  --do_predict=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$TRAINED_CLASSIFIER&lt;/span&gt; \
  --max_seq_length=128 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-squad-11" class="anchor" aria-hidden="true" href="#squad-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 1.1&lt;/h3&gt;
&lt;p&gt;The Stanford Question Answering Dataset (SQuAD) is a popular question answering
benchmark dataset. BERT (at the time of the release) obtains state-of-the-art
results on SQuAD with almost no task-specific network architecture modifications
or data augmentation. However, it does require semi-complex data pre-processing
and post-processing to deal with (a) the variable-length nature of SQuAD context
paragraphs, and (b) the character-level answer annotations which are used for
SQuAD training. This processing is implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD, you will first need to download the dataset. The
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD website&lt;/a&gt; does not seem to
link to the v1.1 datasets any longer, but the necessary files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json" rel="nofollow"&gt;train-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json" rel="nofollow"&gt;dev-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py"&gt;evaluate-v1.1.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The state-of-the-art SQuAD results from the paper currently cannot be reproduced
on a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 does
not seem to fit on a 12GB GPU using &lt;code&gt;BERT-Large&lt;/code&gt;). However, a reasonably strong
&lt;code&gt;BERT-Base&lt;/code&gt; model can be trained on the GPU with these hyperparameters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=12 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=/tmp/squad_base/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The dev set predictions will be saved into a file called &lt;code&gt;predictions.json&lt;/code&gt; in
the &lt;code&gt;output_dir&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/evaluate-v1.1.py &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json ./squad/predictions.json&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Which should produce an output like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 88.41249612335034, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 81.2488174077578}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see a result similar to the 88.5% reported in the paper for
&lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you have access to a Cloud TPU, you can train with &lt;code&gt;BERT-Large&lt;/code&gt;. Here is a
set of hyperparameters (slightly different than the paper) which consistently
obtain around 90.5%-91.0% F1 single-system trained only on SQuAD:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For example, one random run with these parameters produces the following Dev
scores:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 90.87081895814865, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 84.38978240302744}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you fine-tune for one epoch on
&lt;a href="http://nlp.cs.washington.edu/triviaqa/" rel="nofollow"&gt;TriviaQA&lt;/a&gt; before this the results will
be even better, but you will need to convert TriviaQA into the SQuAD json
format.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-squad-20" class="anchor" aria-hidden="true" href="#squad-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 2.0&lt;/h3&gt;
&lt;p&gt;This model is also implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD 2.0, you will first need to download the dataset. The necessary
files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json" rel="nofollow"&gt;train-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json" rel="nofollow"&gt;dev-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/" rel="nofollow"&gt;evaluate-v2.0.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On Cloud TPU you can run with BERT-Large as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We assume you have copied everything from the output directory to a local
directory called ./squad/. The initial dev set predictions will be at
./squad/predictions.json and the differences between the score of no answer ("")
and the best non-null answer for each question will be in the file
./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Run this script to tune a threshold for predicting null versus non-null answers:&lt;/p&gt;
&lt;p&gt;python $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json
./squad/predictions.json --na-prob-file ./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Assume the script outputs "best_f1_thresh" THRESH. (Typical values are between
-1.0 and -5.0). You can now re-run the model to generate predictions with the
derived threshold or alternatively you can extract the appropriate answers from
./squad/nbest_predictions.json.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=False \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True \
  --null_score_diff_threshold=&lt;span class="pl-smi"&gt;$THRESH&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-out-of-memory-issues" class="anchor" aria-hidden="true" href="#out-of-memory-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Out-of-memory issues&lt;/h3&gt;
&lt;p&gt;All experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB of
device RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely
to encounter out-of-memory issues if you use the same hyperparameters described
in the paper.&lt;/p&gt;
&lt;p&gt;The factors that affect memory usage are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;max_seq_length&lt;/code&gt;&lt;/strong&gt;: The released models were trained with sequence lengths
up to 512, but you can fine-tune with a shorter max sequence length to save
substantial memory. This is controlled by the &lt;code&gt;max_seq_length&lt;/code&gt; flag in our
example code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;train_batch_size&lt;/code&gt;&lt;/strong&gt;: The memory usage is also directly proportional to
the batch size.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model type, &lt;code&gt;BERT-Base&lt;/code&gt; vs. &lt;code&gt;BERT-Large&lt;/code&gt;&lt;/strong&gt;: The &lt;code&gt;BERT-Large&lt;/code&gt; model
requires significantly more memory than &lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimizer&lt;/strong&gt;: The default optimizer for BERT is Adam, which requires a lot
of extra memory to store the &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;v&lt;/code&gt; vectors. Switching to a more memory
efficient optimizer can reduce memory usage, but can also affect the
results. We have not experimented with other optimizers for fine-tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the default training scripts (&lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;run_squad.py&lt;/code&gt;), we
benchmarked the maximum batch size on single Titan X GPU (12GB RAM) with
TensorFlow 1.11.0:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Seq Length&lt;/th&gt;
&lt;th&gt;Max Batch Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Base&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Large&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unfortunately, these max batch sizes for &lt;code&gt;BERT-Large&lt;/code&gt; are so small that they
will actually harm the model accuracy, regardless of the learning rate used. We
are working on adding code to this repository which will allow much larger
effective batch sizes to be used on the GPU. The code will be based on one (or
both) of the following techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gradient accumulation&lt;/strong&gt;: The samples in a minibatch are typically
independent with respect to gradient computation (excluding batch
normalization, which is not used here). This means that the gradients of
multiple smaller minibatches can be accumulated before performing the weight
update, and this will be exactly equivalent to a single larger update.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/openai/gradient-checkpointing"&gt;&lt;strong&gt;Gradient checkpointing&lt;/strong&gt;&lt;/a&gt;:
The major use of GPU/TPU memory during DNN training is caching the
intermediate activations in the forward pass that are necessary for
efficient computation in the backward pass. "Gradient checkpointing" trades
memory for compute time by re-computing the activations in an intelligent
way.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;However, this is not implemented in the current release.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-to-extract-fixed-feature-vectors-like-elmo" class="anchor" aria-hidden="true" href="#using-bert-to-extract-fixed-feature-vectors-like-elmo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT to extract fixed feature vectors (like ELMo)&lt;/h2&gt;
&lt;p&gt;In certain cases, rather than fine-tuning the entire pre-trained model
end-to-end, it can be beneficial to obtained &lt;em&gt;pre-trained contextual
embeddings&lt;/em&gt;, which are fixed contextual representations of each input token
generated from the hidden layers of the pre-trained model. This should also
mitigate most of the out-of-memory issues.&lt;/p&gt;
&lt;p&gt;As an example, we include the script &lt;code&gt;extract_features.py&lt;/code&gt; which can be used
like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Sentence A and Sentence B are separated by the ||| delimiter for sentence&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pair tasks like question answering and entailment.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; For single sentence inputs, put one sentence per line and DON'T use the&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; delimiter.&lt;/span&gt;
&lt;span class="pl-c1"&gt;echo&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Who was Jim Henson ? ||| Jim Henson was a puppeteer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; /tmp/input.txt

python extract_features.py \
  --input_file=/tmp/input.txt \
  --output_file=/tmp/output.jsonl \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --layers=-1,-2,-3,-4 \
  --max_seq_length=128 \
  --batch_size=8&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will create a JSON file (one line per line of input) containing the BERT
activations from each Transformer layer specified by &lt;code&gt;layers&lt;/code&gt; (-1 is the final
hidden layer of the Transformer, etc.)&lt;/p&gt;
&lt;p&gt;Note that this script will produce very large output files (by default, around
15kb for every input token).&lt;/p&gt;
&lt;p&gt;If you need to maintain alignment between the original and tokenized words (for
projecting training labels), see the &lt;a href="#tokenization"&gt;Tokenization&lt;/a&gt; section
below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You may see a message like &lt;code&gt;Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict.&lt;/code&gt; This message is expected, it
just means that we are using the &lt;code&gt;init_from_checkpoint()&lt;/code&gt; API rather than the
saved model API. If you don't specify a checkpoint or specify an invalid
checkpoint, this script will complain.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tokenization" class="anchor" aria-hidden="true" href="#tokenization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;For sentence-level tasks (or sentence-pair) tasks, tokenization is very simple.
Just follow the example code in &lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;extract_features.py&lt;/code&gt;.
The basic procedure for sentence-level tasks is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Instantiate an instance of &lt;code&gt;tokenizer = tokenization.FullTokenizer&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tokenize the raw text with &lt;code&gt;tokens = tokenizer.tokenize(raw_text)&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Truncate to the maximum sequence length. (You can use up to 512, but you
probably want to use shorter if possible for memory and speed reasons.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[SEP]&lt;/code&gt; tokens in the right place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Word-level and span-level tasks (e.g., SQuAD and NER) are more complex, since
you need to maintain alignment between your input text and output text so that
you can project your training labels. SQuAD is a particularly complex example
because the input labels are &lt;em&gt;character&lt;/em&gt;-based, and SQuAD paragraphs are often
longer than our maximum sequence length. See the code in &lt;code&gt;run_squad.py&lt;/code&gt; to show
how we handle this.&lt;/p&gt;
&lt;p&gt;Before we describe the general recipe for handling word-level tasks, it's
important to understand what exactly our tokenizer is doing. It has three main
steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Text normalization&lt;/strong&gt;: Convert all whitespace characters to spaces, and
(for the &lt;code&gt;Uncased&lt;/code&gt; model) lowercase the input and strip out accent markers.
E.g., &lt;code&gt;John Johanson's, → john johanson's,&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Punctuation splitting&lt;/strong&gt;: Split &lt;em&gt;all&lt;/em&gt; punctuation characters on both sides
(i.e., add whitespace around all punctuation characters). Punctuation
characters are defined as (a) Anything with a &lt;code&gt;P*&lt;/code&gt; Unicode class, (b) any
non-letter/number/space ASCII character (e.g., characters like &lt;code&gt;$&lt;/code&gt; which are
technically not punctuation). E.g., &lt;code&gt;john johanson's, → john johanson ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;WordPiece tokenization&lt;/strong&gt;: Apply whitespace tokenization to the output of
the above procedure, and apply
&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py"&gt;WordPiece&lt;/a&gt;
tokenization to each token separately. (Our implementation is directly based
on the one from &lt;code&gt;tensor2tensor&lt;/code&gt;, which is linked). E.g., &lt;code&gt;john johanson ' s , → john johan ##son ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The advantage of this scheme is that it is "compatible" with most existing
English tokenizers. For example, imagine that you have a part-of-speech tagging
task which looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input:  John Johanson 's   house
Labels: NNP  NNP      POS NN
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tokenized output will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Tokens: john johan ##son ' s house
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Crucially, this would be the same output as if the raw text were &lt;code&gt;John Johanson's house&lt;/code&gt; (with no space before the &lt;code&gt;'s&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you have a pre-tokenized representation with word-level annotations, you can
simply tokenize each input word independently, and deterministically maintain an
original-to-tokenized alignment:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Input&lt;/span&gt;
orig_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;John&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Johanson&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;'s&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;house&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
labels      &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;POS&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Output&lt;/span&gt;
bert_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; []

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Token map will be an int -&amp;gt; int mapping between the `orig_tokens` index and&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; the `bert_tokens` index.&lt;/span&gt;
orig_to_tok_map &lt;span class="pl-k"&gt;=&lt;/span&gt; []

tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenization.FullTokenizer(
    &lt;span class="pl-v"&gt;vocab_file&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;vocab_file, &lt;span class="pl-v"&gt;do_lower_case&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[CLS]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;for&lt;/span&gt; orig_token &lt;span class="pl-k"&gt;in&lt;/span&gt; orig_tokens:
  orig_to_tok_map.append(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(bert_tokens))
  bert_tokens.extend(tokenizer.tokenize(orig_token))
bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[SEP]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; bert_tokens == ["[CLS]", "john", "johan", "##son", "'", "s", "house", "[SEP]"]&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; orig_to_tok_map == [1, 2, 4, 6]&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now &lt;code&gt;orig_to_tok_map&lt;/code&gt; can be used to project &lt;code&gt;labels&lt;/code&gt; to the tokenized
representation.&lt;/p&gt;
&lt;p&gt;There are common English tokenization schemes which will cause a slight mismatch
between how BERT was pre-trained. For example, if your input tokenization splits
off contractions like &lt;code&gt;do n't&lt;/code&gt;, this will cause a mismatch. If it is possible to
do so, you should pre-process your data to convert these back to raw-looking
text, but if it's not possible, this mismatch is likely not a big deal.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-training-with-bert" class="anchor" aria-hidden="true" href="#pre-training-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training with BERT&lt;/h2&gt;
&lt;p&gt;We are releasing code to do "masked LM" and "next sentence prediction" on an
arbitrary text corpus. Note that this is &lt;em&gt;not&lt;/em&gt; the exact code that was used for
the paper (the original code was written in C++, and had some additional
complexity), but this code does generate pre-training data as described in the
paper.&lt;/p&gt;
&lt;p&gt;Here's how to run the data generation. The input is a plain text file, with one
sentence per line. (It is important that these be actual sentences for the "next
sentence prediction" task). Documents are delimited by empty lines. The output
is a set of &lt;code&gt;tf.train.Example&lt;/code&gt;s serialized into &lt;code&gt;TFRecord&lt;/code&gt; file format.&lt;/p&gt;
&lt;p&gt;You can perform sentence segmentation with an off-the-shelf NLP toolkit such as
&lt;a href="https://spacy.io/" rel="nofollow"&gt;spaCy&lt;/a&gt;. The &lt;code&gt;create_pretraining_data.py&lt;/code&gt; script will
concatenate segments until they reach the maximum sequence length to minimize
computational waste from padding (see the script for more details). However, you
may want to intentionally add a slight amount of noise to your input data (e.g.,
randomly truncate 2% of input segments) to make it more robust to non-sentential
input during fine-tuning.&lt;/p&gt;
&lt;p&gt;This script stores all of the examples for the entire input file in memory, so
for large data files you should shard the input file and call the script
multiple times. (You can pass in a file glob to &lt;code&gt;run_pretraining.py&lt;/code&gt;, e.g.,
&lt;code&gt;tf_examples.tf_record*&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;max_predictions_per_seq&lt;/code&gt; is the maximum number of masked LM predictions per
sequence. You should set this to around &lt;code&gt;max_seq_length&lt;/code&gt; * &lt;code&gt;masked_lm_prob&lt;/code&gt; (the
script doesn't do that automatically because the exact value needs to be passed
to both scripts).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python create_pretraining_data.py \
  --input_file=./sample_text.txt \
  --output_file=/tmp/tf_examples.tfrecord \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's how to run the pre-training. Do not include &lt;code&gt;init_checkpoint&lt;/code&gt; if you are
pre-training from scratch. The model configuration (including vocab size) is
specified in &lt;code&gt;bert_config_file&lt;/code&gt;. This demo code only pre-trains for a small
number of steps (20), but in practice you will probably want to set
&lt;code&gt;num_train_steps&lt;/code&gt; to 10000 steps or more. The &lt;code&gt;max_seq_length&lt;/code&gt; and
&lt;code&gt;max_predictions_per_seq&lt;/code&gt; parameters passed to &lt;code&gt;run_pretraining.py&lt;/code&gt; must be the
same as &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_pretraining.py \
  --input_file=/tmp/tf_examples.tfrecord \
  --output_dir=/tmp/pretraining_output \
  --do_train=True \
  --do_eval=True \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --train_batch_size=32 \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --num_train_steps=20 \
  --num_warmup_steps=10 \
  --learning_rate=2e-5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will produce an output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  global_step = 20
  loss = 0.0979674
  masked_lm_accuracy = 0.985479
  masked_lm_loss = 0.0979328
  next_sentence_accuracy = 1.0
  next_sentence_loss = 3.45724e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that since our &lt;code&gt;sample_text.txt&lt;/code&gt; file is very small, this example training
will overfit that data in only a few steps and produce unrealistically high
accuracy numbers.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-tips-and-caveats" class="anchor" aria-hidden="true" href="#pre-training-tips-and-caveats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training tips and caveats&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;If using your own vocabulary, make sure to change &lt;code&gt;vocab_size&lt;/code&gt; in
&lt;code&gt;bert_config.json&lt;/code&gt;. If you use a larger vocabulary without changing this,
you will likely get NaNs when training on GPU or TPU due to unchecked
out-of-bounds access.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;If your task has a large domain-specific corpus available (e.g., "movie
reviews" or "scientific papers"), it will likely be beneficial to run
additional steps of pre-training on your corpus, starting from the BERT
checkpoint.&lt;/li&gt;
&lt;li&gt;The learning rate we used in the paper was 1e-4. However, if you are doing
additional steps of pre-training starting from an existing BERT checkpoint,
you should use a smaller learning rate (e.g., 2e-5).&lt;/li&gt;
&lt;li&gt;Current BERT models are English-only, but we do plan to release a
multilingual model which has been pre-trained on a lot of languages in the
near future (hopefully by the end of November 2018).&lt;/li&gt;
&lt;li&gt;Longer sequences are disproportionately expensive because attention is
quadratic to the sequence length. In other words, a batch of 64 sequences of
length 512 is much more expensive than a batch of 256 sequences of
length 128. The fully-connected/convolutional cost is the same, but the
attention cost is far greater for the 512-length sequences. Therefore, one
good recipe is to pre-train for, say, 90,000 steps with a sequence length of
128 and then for 10,000 additional steps with a sequence length of 512. The
very long sequences are mostly needed to learn positional embeddings, which
can be learned fairly quickly. Note that this does require generating the
data twice with different values of &lt;code&gt;max_seq_length&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you are pre-training from scratch, be prepared that pre-training is
computationally expensive, especially on GPUs. If you are pre-training from
scratch, our recommended recipe is to pre-train a &lt;code&gt;BERT-Base&lt;/code&gt; on a single
&lt;a href="https://cloud.google.com/tpu/docs/pricing" rel="nofollow"&gt;preemptible Cloud TPU v2&lt;/a&gt;, which
takes about 2 weeks at a cost of about $500 USD (based on the pricing in
October 2018). You will have to scale down the batch size when only training
on a single Cloud TPU, compared to what was used in the paper. It is
recommended to use the largest batch size that fits into TPU memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-data" class="anchor" aria-hidden="true" href="#pre-training-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training data&lt;/h3&gt;
&lt;p&gt;We will &lt;strong&gt;not&lt;/strong&gt; be able to release the pre-processed datasets used in the paper.
For Wikipedia, the recommended pre-processing is to download
&lt;a href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2" rel="nofollow"&gt;the latest dump&lt;/a&gt;,
extract the text with
&lt;a href="https://github.com/attardi/wikiextractor"&gt;&lt;code&gt;WikiExtractor.py&lt;/code&gt;&lt;/a&gt;, and then apply
any necessary cleanup to convert it into plain text.&lt;/p&gt;
&lt;p&gt;Unfortunately the researchers who collected the
&lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt; no longer have it available for
public download. The
&lt;a href="https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html" rel="nofollow"&gt;Project Guttenberg Dataset&lt;/a&gt;
is a somewhat smaller (200M word) collection of older books that are public
domain.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://commoncrawl.org/" rel="nofollow"&gt;Common Crawl&lt;/a&gt; is another very large collection of
text, but you will likely have to do substantial pre-processing and cleanup to
extract a usable corpus for pre-training BERT.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-learning-a-new-wordpiece-vocabulary" class="anchor" aria-hidden="true" href="#learning-a-new-wordpiece-vocabulary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learning a new WordPiece vocabulary&lt;/h3&gt;
&lt;p&gt;This repository does not include code for &lt;em&gt;learning&lt;/em&gt; a new WordPiece vocabulary.
The reason is that the code used in the paper was implemented in C++ with
dependencies on Google's internal libraries. For English, it is almost always
better to just start with our vocabulary and pre-trained models. For learning
vocabularies of other languages, there are a number of open source options
available. However, keep in mind that these are not compatible with our
&lt;code&gt;tokenization.py&lt;/code&gt; library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;Google's SentencePiece library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py"&gt;tensor2tensor's WordPiece generation script&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rsennrich/subword-nmt"&gt;Rico Sennrich's Byte Pair Encoding library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-in-colab" class="anchor" aria-hidden="true" href="#using-bert-in-colab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT in Colab&lt;/h2&gt;
&lt;p&gt;If you want to use BERT with &lt;a href="https://colab.research.google.com" rel="nofollow"&gt;Colab&lt;/a&gt;, you can
get started with the notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".
&lt;strong&gt;At the time of this writing (October 31st, 2018), Colab users can access a
Cloud TPU completely for free.&lt;/strong&gt; Note: One per user, availability limited,
requires a Google Cloud Platform account with storage (although storage may be
purchased with free credit for signing up with GCP), and this capability may not
longer be available in the future. Click on the BERT Colab that was just linked
for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-is-this-code-compatible-with-cloud-tpus-what-about-gpus" class="anchor" aria-hidden="true" href="#is-this-code-compatible-with-cloud-tpus-what-about-gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is this code compatible with Cloud TPUs? What about GPUs?&lt;/h4&gt;
&lt;p&gt;Yes, all of the code in this repository works out-of-the-box with CPU, GPU, and
Cloud TPU. However, GPU training is single-GPU only.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-i-am-getting-out-of-memory-errors-what-is-wrong" class="anchor" aria-hidden="true" href="#i-am-getting-out-of-memory-errors-what-is-wrong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I am getting out-of-memory errors, what is wrong?&lt;/h4&gt;
&lt;p&gt;See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for more
information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-pytorch-version-available" class="anchor" aria-hidden="true" href="#is-there-a-pytorch-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a PyTorch version available?&lt;/h4&gt;
&lt;p&gt;There is no official PyTorch implementation. However, NLP researchers from
HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-chainer-version-available" class="anchor" aria-hidden="true" href="#is-there-a-chainer-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a Chainer version available?&lt;/h4&gt;
&lt;p&gt;There is no official Chainer implementation. However, Sosuke Kobayashi made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the Chainer
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-in-other-languages-be-released" class="anchor" aria-hidden="true" href="#will-models-in-other-languages-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models in other languages be released?&lt;/h4&gt;
&lt;p&gt;Yes, we plan to release a multi-lingual BERT model in the near future. We cannot
make promises about exactly which languages will be included, but it will likely
be a single model which includes &lt;em&gt;most&lt;/em&gt; of the languages which have a
significantly-sized Wikipedia.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-larger-than-bert-large-be-released" class="anchor" aria-hidden="true" href="#will-models-larger-than-bert-large-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models larger than &lt;code&gt;BERT-Large&lt;/code&gt; be released?&lt;/h4&gt;
&lt;p&gt;So far we have not attempted to train anything larger than &lt;code&gt;BERT-Large&lt;/code&gt;. It is
possible that we will release larger models if we are able to obtain significant
improvements.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-what-license-is-this-library-released-under" class="anchor" aria-hidden="true" href="#what-license-is-this-library-released-under"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What license is this library released under?&lt;/h4&gt;
&lt;p&gt;All code &lt;em&gt;and&lt;/em&gt; models are released under the Apache 2.0 license. See the
&lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-how-do-i-cite-bert" class="anchor" aria-hidden="true" href="#how-do-i-cite-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I cite BERT?&lt;/h4&gt;
&lt;p&gt;For now, cite &lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;the Arxiv paper&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we submit the paper to a conference or journal, we will update the BibTeX.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;This is not an official Google product.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact-information" class="anchor" aria-hidden="true" href="#contact-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact information&lt;/h2&gt;
&lt;p&gt;For help or issues using BERT, please submit a GitHub issue.&lt;/p&gt;
&lt;p&gt;For personal communication related to BERT, please contact Jacob Devlin
(&lt;code&gt;jacobdevlin@google.com&lt;/code&gt;), Ming-Wei Chang (&lt;code&gt;mingweichang@google.com&lt;/code&gt;), or
Kenton Lee (&lt;code&gt;kentonl@google.com&lt;/code&gt;).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/bert</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>dbolya/yolact #6 in Python, Today</title><link>https://github.com/dbolya/yolact</link><description>&lt;p&gt;&lt;i&gt;A simple, fully convolutional model for real-time instance segmentation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-you-only-look-at-coefficients" class="anchor" aria-hidden="true" href="#you-only-look-at-coefficients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Y&lt;/strong&gt;ou &lt;strong&gt;O&lt;/strong&gt;nly &lt;strong&gt;L&lt;/strong&gt;ook &lt;strong&gt;A&lt;/strong&gt;t &lt;strong&gt;C&lt;/strong&gt;oefficien&lt;strong&gt;T&lt;/strong&gt;s&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;    ██╗   ██╗ ██████╗ ██╗      █████╗  ██████╗████████╗
    ╚██╗ ██╔╝██╔═══██╗██║     ██╔══██╗██╔════╝╚══██╔══╝
     ╚████╔╝ ██║   ██║██║     ███████║██║        ██║   
      ╚██╔╝  ██║   ██║██║     ██╔══██║██║        ██║   
       ██║   ╚██████╔╝███████╗██║  ██║╚██████╗   ██║   
       ╚═╝    ╚═════╝ ╚══════╝╚═╝  ╚═╝ ╚═════╝   ╚═╝ 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A simple, fully convolutional model for real-time instance segmentation. This is the code for &lt;a href="https://arxiv.org/abs/1904.02689" rel="nofollow"&gt;our paper&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-iccv-update-v11-released-check-out-the-iccv-trailer-here" class="anchor" aria-hidden="true" href="#iccv-update-v11-released-check-out-the-iccv-trailer-here"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ICCV update (v1.1) released! Check out the ICCV trailer here:&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=0pMfmo8qfpQ" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c9f0f1403e25276c0beea78732b5cec6c9b610ab/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f30704d666d6f38716670512f302e6a7067" alt="IMAGE ALT TEXT HERE" data-canonical-src="https://img.youtube.com/vi/0pMfmo8qfpQ/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Read &lt;a href="CHANGELOG.md"&gt;the changelog&lt;/a&gt; for details on, well, what changed. Oh, and the paper got updated too with pascal results and an appendix with box mAP.&lt;/p&gt;
&lt;p&gt;Some examples from our base model (33.5 fps on a Titan Xp and 29.8 mAP on COCO's &lt;code&gt;test-dev&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_0.png"&gt;&lt;img src="data/yolact_example_0.png" alt="Example 0" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_1.png"&gt;&lt;img src="data/yolact_example_1.png" alt="Example 1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_2.png"&gt;&lt;img src="data/yolact_example_2.png" alt="Example 2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Set up a Python3 environment.&lt;/li&gt;
&lt;li&gt;Install &lt;a href="http://pytorch.org/" rel="nofollow"&gt;Pytorch&lt;/a&gt; 1.0.1 (or higher) and TorchVision.&lt;/li&gt;
&lt;li&gt;Install some other packages:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Cython needs to be installed before pycocotools&lt;/span&gt;
pip install cython
pip install opencv-python pillow pycocotools matplotlib &lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Clone this repository and enter it:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/dbolya/yolact.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; yolact&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you'd like to train YOLACT, download the COCO dataset and the 2014/2017 annotations. Note that this script will take a while and dump 21gb of files into &lt;code&gt;./data/coco&lt;/code&gt;.
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh data/scripts/COCO.sh&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you'd like to evaluate YOLACT on &lt;code&gt;test-dev&lt;/code&gt;, download &lt;code&gt;test-dev&lt;/code&gt; with this script.
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh data/scripts/COCO_test.sh&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-evaluation" class="anchor" aria-hidden="true" href="#evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evaluation&lt;/h1&gt;
&lt;p&gt;As of April 5th, 2019 here are our latest models along with their FPS on a Titan Xp and mAP on &lt;code&gt;test-dev&lt;/code&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Image Size&lt;/th&gt;
&lt;th align="center"&gt;Backbone&lt;/th&gt;
&lt;th align="center"&gt;FPS&lt;/th&gt;
&lt;th align="center"&gt;mAP&lt;/th&gt;
&lt;th&gt;Weights&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Resnet50-FPN&lt;/td&gt;
&lt;td align="center"&gt;42.5&lt;/td&gt;
&lt;td align="center"&gt;28.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1yp7ZbbDwvMiFJEq4ptVKTYTI2VeRDXl0/view?usp=sharing" rel="nofollow"&gt;yolact_resnet50_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EUVpxoSXaqNIlssoLKOEoCcB1m0RpzGq_Khp5n1VX3zcUw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Darknet53-FPN&lt;/td&gt;
&lt;td align="center"&gt;40.0&lt;/td&gt;
&lt;td align="center"&gt;28.7&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1dukLrTzZQEuhzitGkHaGjphlmRJOjVnP/view?usp=sharing" rel="nofollow"&gt;yolact_darknet53_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/ERrao26c8llJn25dIyZPhwMBxUp2GdZTKIMUQA3t0djHLw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Resnet101-FPN&lt;/td&gt;
&lt;td align="center"&gt;33.0&lt;/td&gt;
&lt;td align="center"&gt;29.8&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1UYy3dMapbH1BnmtZU4WH1zbYgOzzHHf_/view?usp=sharing" rel="nofollow"&gt;yolact_base_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EYRWxBEoKU9DiblrWx2M89MBGFkVVB_drlRd_v5sdT3Hgg" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;700&lt;/td&gt;
&lt;td align="center"&gt;Resnet101-FPN&lt;/td&gt;
&lt;td align="center"&gt;23.6&lt;/td&gt;
&lt;td align="center"&gt;31.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1lE4Lz5p25teiXV-6HdTiOJSnS7u7GBzg/view?usp=sharing" rel="nofollow"&gt;yolact_im700_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/Eagg5RSc5hFEhp7sPtvLNyoBjhlf2feog7t8OQzHKKphjw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To evalute the model, put the corresponding weights file in the &lt;code&gt;./weights&lt;/code&gt; directory and run one of the following commands.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quantitative-results-on-coco" class="anchor" aria-hidden="true" href="#quantitative-results-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quantitative Results on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Quantitatively evaluate a trained model on the entire validation set. Make sure you have COCO downloaded as above.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This should get 29.92 validation mask mAP last time I checked.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Output a COCOEval json to submit to the website or to use the run_coco_eval.py script.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This command will create './results/bbox_detections.json' and './results/mask_detections.json' for detection and instance segmentation respectively.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; You can run COCOEval on the files created in the previous command. The performance should match my implementation in eval.py.&lt;/span&gt;
python run_coco_eval.py

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To output a coco json file for test-dev, make sure you have test-dev downloaded from above and go&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json --dataset=coco2017_testdev_dataset&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-qualitative-results-on-coco" class="anchor" aria-hidden="true" href="#qualitative-results-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Qualitative Results on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display qualitative results on COCO. From here on I'll use a confidence threshold of 0.15.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --display&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-benchmarking-on-coco" class="anchor" aria-hidden="true" href="#benchmarking-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmarking on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run just the raw model on the first 1k images of the validation set&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --benchmark --max_images=1000&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-images" class="anchor" aria-hidden="true" href="#images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Images&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display qualitative results on the specified image.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=my_image.png

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process an image and save it to another file.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=input_image.png:output_image.png

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process a whole folder of images.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --images=path/to/input/folder:path/to/output/folder&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-video" class="anchor" aria-hidden="true" href="#video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display a video in real-time. "--video_multiframe" will process that many frames at once for improved performance.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; If you want, use "--display_fps" to draw the FPS directly on the frame.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=my_video.mp4

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display a webcam feed in real-time. If you have multiple webcams pass the index of the webcam you want instead of 0.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=0

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process a video and save it to another file. This uses the same pipeline as the ones above now, so it's fast!&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=input_video.mp4:output_video.mp4&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can tell, &lt;code&gt;eval.py&lt;/code&gt; can do a ton of stuff. Run the &lt;code&gt;--help&lt;/code&gt; command to see everything it can do.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python eval.py --help&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h1&gt;
&lt;p&gt;By default, we train on COCO. Make sure to download the entire dataset using the commands above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To train, grab an imagenet-pretrained model and put it in &lt;code&gt;./weights&lt;/code&gt;.
&lt;ul&gt;
&lt;li&gt;For Resnet101, download &lt;code&gt;resnet101_reducedfc.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/1tvqFPd4bJtakOlmn-uIA492g2qurRChj/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Resnet50, download &lt;code&gt;resnet50-19c8e357.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/1Jy3yCdbatgXa5YYIdTCRrSV0S9V5g1rn/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Darknet53, download &lt;code&gt;darknet53.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/17Y431j4sagFpSReuPNoFcj9h7azDTZFf/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run one of the training commands below.
&lt;ul&gt;
&lt;li&gt;Note that you can press ctrl+c while training and it will save an &lt;code&gt;*_interrupt.pth&lt;/code&gt; file at the current iteration.&lt;/li&gt;
&lt;li&gt;All weights are saved in the &lt;code&gt;./weights&lt;/code&gt; directory by default with the file name &lt;code&gt;&amp;lt;config&amp;gt;_&amp;lt;epoch&amp;gt;_&amp;lt;iter&amp;gt;.pth&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trains using the base config with a batch size of 8 (the default).&lt;/span&gt;
python train.py --config=yolact_base_config

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trains yolact_base_config with a batch_size of 5. For the 550px models, 1 batch takes up around 1.5 gigs of VRAM, so specify accordingly.&lt;/span&gt;
python train.py --config=yolact_base_config --batch_size=5

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Resume training yolact_base with a specific weight file and start from the iteration specified in the weight file's name.&lt;/span&gt;
python train.py --config=yolact_base_config --resume=weights/yolact_base_10_32100.pth --start_iter=-1

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Use the help option to see a description of all available command line arguments&lt;/span&gt;
python train.py --help&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-multi-gpu-support" class="anchor" aria-hidden="true" href="#multi-gpu-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-GPU Support&lt;/h2&gt;
&lt;p&gt;YOLACT now supports multiple GPUs seamlessly during training:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before running any of the scripts, run: &lt;code&gt;export CUDA_VISIBLE_DEVICES=[gpus]&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Where you should replace [gpus] with a comma separated list of the index of each GPU you want to use (e.g., 0,1,2,3).&lt;/li&gt;
&lt;li&gt;You should still do this if only using 1 GPU.&lt;/li&gt;
&lt;li&gt;You can check the indices of your GPUs with &lt;code&gt;nvidia-smi&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Then, simply set the batch size to &lt;code&gt;8*num_gpus&lt;/code&gt; with the training commands above. The training script will automatically scale the hyperparameters to the right values.
&lt;ul&gt;
&lt;li&gt;If you have memory to spare you can increase the batch size further, but keep it a multiple of the number of GPUs you're using.&lt;/li&gt;
&lt;li&gt;If you want to allocate the images per GPU specific for different GPUs, you can use &lt;code&gt;--batch_alloc=[alloc]&lt;/code&gt; where [alloc] is a comma seprated list containing the number of images on each GPU. This must sum to &lt;code&gt;batch_size&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-logging" class="anchor" aria-hidden="true" href="#logging"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Logging&lt;/h2&gt;
&lt;p&gt;YOLACT now logs training and validation information by default. You can disable this with &lt;code&gt;--no_log&lt;/code&gt;. A guide on how to visualize these logs is coming soon, but now you can look at &lt;code&gt;LogVizualizer&lt;/code&gt; in &lt;code&gt;utils/logger.py&lt;/code&gt; for help.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pascal-sbd" class="anchor" aria-hidden="true" href="#pascal-sbd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pascal SBD&lt;/h2&gt;
&lt;p&gt;We also include a config for training on Pascal SBD annotations (for rapid experimentation or comparing with other methods). To train on Pascal SBD, proceed with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download the dataset from &lt;a href="http://home.bharathh.info/pubs/codes/SBD/download.html" rel="nofollow"&gt;here&lt;/a&gt;. It's the first link in the top "Overview" section (and the file is called &lt;code&gt;benchmark.tgz&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Extract the dataset somewhere. In the dataset there should be a folder called &lt;code&gt;dataset/img&lt;/code&gt;. Create the directory &lt;code&gt;./data/sbd&lt;/code&gt; (where &lt;code&gt;.&lt;/code&gt; is YOLACT's root) and copy &lt;code&gt;dataset/img&lt;/code&gt; to &lt;code&gt;./data/sbd/img&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Download the COCO-style annotations from &lt;a href="https://drive.google.com/open?id=1yLVwtkRtNxyl0kxeMCtPXJsXFFyc_FHe" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Extract the annotations into &lt;code&gt;./data/sbd/&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Now you can train using &lt;code&gt;--config=yolact_resnet50_pascal_config&lt;/code&gt;. Check that config to see how to extend it to other models.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will automate this all with a script soon, don't worry. Also, if you want the script I used to convert the annotations, I put it in &lt;code&gt;./scripts/convert_sbd.py&lt;/code&gt;, but you'll have to check how it works to be able to use it because I don't actually remember at this point.&lt;/p&gt;
&lt;p&gt;If you want to verify our results, you can download our &lt;code&gt;yolact_resnet50_pascal_config&lt;/code&gt; weights from &lt;a href="https://drive.google.com/open?id=1ExrRSPVctHW8Nxrn0SofU1lVhK5Wn0_S" rel="nofollow"&gt;here&lt;/a&gt;. This model should get 72.3 mask AP_50 and 56.2 mask AP_70. Note that the "all" AP isn't the same as the "vol" AP reported in others papers for pascal (they use an averages of the thresholds from &lt;code&gt;0.1 - 0.9&lt;/code&gt; in increments of &lt;code&gt;0.1&lt;/code&gt; instead of what COCO uses).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-custom-datasets" class="anchor" aria-hidden="true" href="#custom-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h2&gt;
&lt;p&gt;You can also train on your own dataset by following these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a COCO-style Object Detection JSON annotation file for your dataset. The specification for this can be found &lt;a href="http://cocodataset.org/#format-data" rel="nofollow"&gt;here&lt;/a&gt;. Note that we don't use some fields, so the following may be omitted:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;info&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;liscense&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Under &lt;code&gt;image&lt;/code&gt;: &lt;code&gt;license, flickr_url, coco_url, date_captured&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;categories&lt;/code&gt; (we use our own format for categories, see below)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create a definition for your dataset under &lt;code&gt;dataset_base&lt;/code&gt; in &lt;code&gt;data/config.py&lt;/code&gt; (see the comments in &lt;code&gt;dataset_base&lt;/code&gt; for an explanation of each field):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;my_custom_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; dataset_base.copy({
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;name&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;My Dataset&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_training_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_info&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:   &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_training_annotation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_validation_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid_info&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:   &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_validation_annotation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;has_gt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;True&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;class_names&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_1&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_2&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_3&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;...&lt;/span&gt;)
})&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;A couple things to note:
&lt;ul&gt;
&lt;li&gt;Class IDs in the annotation file should start at 1 and increase sequentially on the order of &lt;code&gt;class_names&lt;/code&gt;. If this isn't the case for your annotation file (like in COCO), see the field &lt;code&gt;label_map&lt;/code&gt; in &lt;code&gt;dataset_base&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you do not want to create a validation split, use the same image path and annotations file for validation. By default (see &lt;code&gt;python train.py --help&lt;/code&gt;), &lt;code&gt;train.py&lt;/code&gt; will output validation mAP for the first 5000 images in the dataset every 2 epochs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Finally, in &lt;code&gt;yolact_base_config&lt;/code&gt; in the same file, change the value for &lt;code&gt;'dataset'&lt;/code&gt; to &lt;code&gt;'my_custom_dataset'&lt;/code&gt; or whatever you named the config object above. Then you can use any of the training commands in the previous section.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-creating-a-custom-dataset-from-scratch" class="anchor" aria-hidden="true" href="#creating-a-custom-dataset-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Creating a Custom Dataset from Scratch&lt;/h4&gt;
&lt;p&gt;See &lt;a href="https://github.com/dbolya/yolact/issues/70#issuecomment-504283008"&gt;this nice post by @Amit12690&lt;/a&gt; for tips on how to annotate a custom dataset and prepare it for use with YOLACT.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;p&gt;If you use YOLACT or this code base in your work, please cite&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{bolya-iccv2019,
  author    = {Daniel Bolya and Chong Zhou and Fanyi Xiao and Yong Jae Lee},
  title     = {YOLACT: {Real-time} Instance Segmentation},
  booktitle = {ICCV},
  year      = {2019},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h1&gt;
&lt;p&gt;For questions about our paper or code, please contact &lt;a href="mailto:dbolya@ucdavis.edu"&gt;Daniel Bolya&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dbolya</author><guid isPermaLink="false">https://github.com/dbolya/yolact</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>KubeOperator/KubeOperator #7 in Python, Today</title><link>https://github.com/KubeOperator/KubeOperator</link><description>&lt;p&gt;&lt;i&gt;KubeOperator 是一个开源项目，通过 Web UI 在 VMware、OpenStack、物理机上一键部署和管理生产级别的 Kubernetes 集群。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-kubeoperator---从这里开启您的-kubernetes-之旅" class="anchor" aria-hidden="true" href="#kubeoperator---从这里开启您的-kubernetes-之旅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;KubeOperator - 从这里开启您的 Kubernetes 之旅&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/KubeOperatpr/KubeOperatpr/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/7197a397ba1baf73679f3cf0edf68d821c35ae52/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d61706163686525323076322d626c75652e737667" alt="License" data-canonical-src="http://img.shields.io/badge/license-apache%20v2-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.python.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d4c11ac2b538cba463dfd1e43d05fe4f30f2d33d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d677265656e2e7376673f7374796c653d706c6173746963" alt="Python3" data-canonical-src="https://img.shields.io/badge/python-3.6-green.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.djangoproject.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/35798c7a6bb116ad2e8d420db49766bce91239b1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646a616e676f2d322e312d627269676874677265656e2e7376673f7374796c653d706c6173746963" alt="Django" data-canonical-src="https://img.shields.io/badge/django-2.1-brightgreen.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.ansible.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/dbfb9037d993ab109b0dd41252b2aabcd703e4a5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f616e7369626c652d322e362e352d626c75652e7376673f7374796c653d706c6173746963" alt="Ansible" data-canonical-src="https://img.shields.io/badge/ansible-2.6.5-blue.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.angular.cn/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9829fdfaae3736e19d738b29efaeec4aaf21c61c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f616e67756c61722d372e302e342d7265642e7376673f7374796c653d706c6173746963" alt="Angular" data-canonical-src="https://img.shields.io/badge/angular-7.0.4-red.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;KubeOperator 是一个开源项目，在离线网络环境下，通过可视化 Web UI 在 VMware、Openstack 或者物理机上规划、部署和管理生产级别的 Kubernetes 集群。KubeOperator 是 &lt;a href="https://github.com/jumpserver/jumpserver"&gt;Jumpserver&lt;/a&gt; 明星开源团队在 Kubernetes 领域的的又一全新力作。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/KubeOperator/docs/blob/master/website/static/img/overview.png?raw=true"&gt;&lt;img src="https://github.com/KubeOperator/docs/raw/master/website/static/img/overview.png?raw=true" alt="overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-web-ui-展示" class="anchor" aria-hidden="true" href="#web-ui-展示"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web UI 展示&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/KubeOperator/website/master/images/kubeoperator-ui.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/KubeOperator/website/master/images/kubeoperator-ui.jpg" alt="overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;更多功能截屏请查看：&lt;a href="https://docs.kubeoperator.io/kubeoperator-v2.1/screenshot" rel="nofollow"&gt;https://docs.kubeoperator.io/kubeoperator-v2.1/screenshot&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-整体架构" class="anchor" aria-hidden="true" href="#整体架构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;整体架构&lt;/h2&gt;
&lt;p&gt;KubeOperator 使用 Terraform 在 IaaS 平台上自动创建主机（用户也可以自行准备主机，比如物理机或者虚机），通过 Ansible 完成自动化部署和变更操作，支持 Kubernetes 集群 从 Day 0 规划，到 Day 1 部署，到 Day 2 运维及变更的全生命周期管理。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/KubeOperator/docs/blob/master/website/static/img/KubeOperator.jpeg?raw=true"&gt;&lt;img src="https://github.com/KubeOperator/docs/raw/master/website/static/img/KubeOperator.jpeg?raw=true" alt="overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-技术优势" class="anchor" aria-hidden="true" href="#技术优势"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;技术优势&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;按需创建：调用云平台 API，一键快速创建和部署 Kubernetes 集群 (即 Kubernetes as a Service)；&lt;/li&gt;
&lt;li&gt;按需伸缩：快速伸缩 Kubernetes 集群，优化资源使用效率；&lt;/li&gt;
&lt;li&gt;按需修补：快速升级和修补 Kubernetes 集群，并与社区最新版本同步，保证安全性；&lt;/li&gt;
&lt;li&gt;自我修复：通过重建故障节点确保集群可用性；&lt;/li&gt;
&lt;li&gt;离线部署：持续更新包括 Kubernetes 及常用组件的离线包；&lt;/li&gt;
&lt;li&gt;Multi-AZ 支持：通过把 Kubernetes 集群 Master 节点分布在不同的故障域上确保的高可用；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-demo-视频使用文档" class="anchor" aria-hidden="true" href="#demo-视频使用文档"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Demo 视频、使用文档&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubeoperator-1256577600.file.myqcloud.com/video/KubeOperator2.1.mp4" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="tv" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4fa.png"&gt;📺&lt;/g-emoji&gt;8 分钟演示视频&lt;/a&gt;：详细演示 KubeOperator 的功能。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.kubeoperator.io/" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png"&gt;📚&lt;/g-emoji&gt;安装及使用文档&lt;/a&gt;：包括 KubeOperator 安装文档、使用文档、功能截屏、常见问题等。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-kubernetes-离线安装包" class="anchor" aria-hidden="true" href="#kubernetes-离线安装包"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kubernetes 离线安装包&lt;/h2&gt;
&lt;p&gt;KubeOperator 提供完整的离线 Kubernetes 安装包（包括 Kubernetes、Docker、etcd、Dashboard、Promethus、OS 补丁等），每个安装包会被构建成一个独立容器镜像供 KubeOperator 使用，具体信息请参考：&lt;a href="https://github.com/KubeOperator/k8s-package"&gt;k8s-package&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-版本规划" class="anchor" aria-hidden="true" href="#版本规划"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;版本规划&lt;/h2&gt;
&lt;p&gt;v1.0 （已发布）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 提供原生 Kubernetes 的离线包仓库；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持一主多节点部署模式；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持离线环境下的一键自动化部署，可视化展示集群部署进展和结果；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 内置 Kubernetes 常用系统应用的安装，包括 Registry、Promethus、Dashboard、Traefik Ingress、Helm 等；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 提供简易明了的 Kubernetes 集群运行状况面板；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 NFS 作为持久化存储；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Flannel 网络插件；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群手动部署模式（自行准备主机和 NFS）；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.0 （已发布）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持调用 VMware vCenter API 自动创建集群主机；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 VMware vSAN 、VMFS/NFS 作为持久化存储；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Multi AZ，支持多主多节点部署模式；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Calico 网络插件；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 内置 Weave Scope；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持通过 F5 BIG-IP Controller 对外暴露服务（Nodeport mode, 七层和四层服务都支持）；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.1 （已发布）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Openstack 云平台；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Openstack Cinder 作为持久化存储；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群升级 （Day 2）；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群扩缩容（Day 2）；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群备份与恢复（Day 2）；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群健康检查与诊断（Day 2）；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 &lt;a href="https://github.com/webkubectl/webkubectl"&gt;webkubectl&lt;/a&gt; ；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.2 （进行中，2019.11.30 发布）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 日志收集及管理方案；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 离线环境下使用 Sonobuoy 进行 Kubernetes 集群合规检查并可视化展示结果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.3 （计划中）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; KubeApps 应用商店；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 国际化支持；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 支持 VMware NSX-T；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-沟通交流" class="anchor" aria-hidden="true" href="#沟通交流"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;沟通交流&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;技术交流 QQ 群：825046920；&lt;/li&gt;
&lt;li&gt;技术支持邮箱：&lt;a href="mailto:support@fit2cloud.com"&gt;support@fit2cloud.com&lt;/a&gt;；&lt;/li&gt;
&lt;li&gt;微信群： 搜索微信号 wh_it0224，添加好友，备注（城市-github用户名）, 验证通过会加入群聊；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-致谢" class="anchor" aria-hidden="true" href="#致谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;致谢&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/hashicorp/terraform"&gt;Terraform&lt;/a&gt;: KubeOperator 采用 Terraform 来自动创建虚机；&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vmware/clarity/"&gt;Clarity&lt;/a&gt;: KubeOperator 采用 Clarity 作为前端 Web 框架；&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ansible/ansible"&gt;Ansible&lt;/a&gt;: KubeOperator 采用 Ansible 作为自动化部署工具；&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/easzlab/kubeasz"&gt;kubeasz&lt;/a&gt;: 提供各种 Kubernetes Ansible 脚本；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright (c) 2014-2019 FIT2CLOUD 飞致云&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.fit2cloud.com" rel="nofollow"&gt;https://www.fit2cloud.com&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;KubeOperator is licensed under the Apache License, Version 2.0.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>KubeOperator</author><guid isPermaLink="false">https://github.com/KubeOperator/KubeOperator</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>home-assistant/home-assistant #8 in Python, Today</title><link>https://github.com/home-assistant/home-assistant</link><description>&lt;p&gt;&lt;i&gt;:house_with_garden: Open source home automation that puts local control and privacy first&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-home-assistant-" class="anchor" aria-hidden="true" href="#home-assistant-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Home Assistant &lt;a href="https://discord.gg/c5DvZ4e" rel="nofollow"&gt;&lt;img alt="Chat Status" src="https://camo.githubusercontent.com/599662b08725231a9f847723b9bc4f6dc4757d0c/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3333303934343233383931303936333731342e737667" data-canonical-src="https://img.shields.io/discord/330944238910963714.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Home Assistant is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control.&lt;/p&gt;
&lt;p&gt;To get started:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 -m pip install homeassistant
hass --open-ui&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Check out &lt;a href="https://home-assistant.io" rel="nofollow"&gt;home-assistant.io&lt;/a&gt; for &lt;a href="https://home-assistant.io/demo/" rel="nofollow"&gt;a
demo&lt;/a&gt;, &lt;a href="https://home-assistant.io/getting-started/" rel="nofollow"&gt;installation instructions&lt;/a&gt;,
&lt;a href="https://home-assistant.io/getting-started/automation-2/" rel="nofollow"&gt;tutorials&lt;/a&gt; and &lt;a href="https://home-assistant.io/docs/" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://home-assistant.io/demo/" rel="nofollow"&gt;&lt;img alt="screenshot-states" src="https://camo.githubusercontent.com/99578d7bca06d9c2973c2564e06f1ca444a4cce1/68747470733a2f2f7261772e6769746875622e636f6d2f686f6d652d617373697374616e742f686f6d652d617373697374616e742f6d61737465722f646f63732f73637265656e73686f74732e706e67" data-canonical-src="https://raw.github.com/home-assistant/home-assistant/master/docs/screenshots.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-featured-integrations"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-featured-integrations" class="anchor" aria-hidden="true" href="#featured-integrations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Featured integrations&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://home-assistant.io/integrations/" rel="nofollow"&gt;&lt;img alt="screenshot-components" src="https://camo.githubusercontent.com/ba21a6029ccb81d4a26b1ad9c198e61d01a07e7a/68747470733a2f2f7261772e6769746875622e636f6d2f686f6d652d617373697374616e742f686f6d652d617373697374616e742f6465762f646f63732f73637265656e73686f742d636f6d706f6e656e74732e706e67" data-canonical-src="https://raw.github.com/home-assistant/home-assistant/dev/docs/screenshot-components.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The system is built using a modular approach so support for other devices or actions can be implemented easily. See also the &lt;a href="https://developers.home-assistant.io/docs/en/architecture_index.html" rel="nofollow"&gt;section on architecture&lt;/a&gt; and the &lt;a href="https://developers.home-assistant.io/docs/en/creating_component_index.html" rel="nofollow"&gt;section on creating your own
components&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you run into issues while using Home Assistant or during development
of a component, check the &lt;a href="https://home-assistant.io/help/" rel="nofollow"&gt;Home Assistant help section&lt;/a&gt; of our website for further help and information.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>home-assistant</author><guid isPermaLink="false">https://github.com/home-assistant/home-assistant</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>eriklindernoren/ML-From-Scratch #9 in Python, Today</title><link>https://github.com/eriklindernoren/ML-From-Scratch</link><description>&lt;p&gt;&lt;i&gt;Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-from-scratch" class="anchor" aria-hidden="true" href="#machine-learning-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning From Scratch&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.&lt;/p&gt;
&lt;p&gt;The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible
but rather to present the inner workings of them in a transparent and accessible way.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#machine-learning-from-scratch"&gt;Machine Learning From Scratch&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#about"&gt;About&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#polynomial-regression"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#classification-with-cnn"&gt;Classification With CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#density-based-clustering"&gt;Density-Based Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#generating-handwritten-digits"&gt;Generating Handwritten Digits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-reinforcement-learning"&gt;Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image-reconstruction-with-rbm"&gt;Image Reconstruction With RBM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#evolutionary-evolved-neural-network"&gt;Evolutionary Evolved Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#genetic-algorithm"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#association-analysis"&gt;Association Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#implementations"&gt;Implementations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#supervised-learning"&gt;Supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unsupervised-learning"&gt;Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement-learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-learning"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#contact"&gt;Contact&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/eriklindernoren/ML-From-Scratch
$ cd ML-From-Scratch
$ python setup.py install
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-polynomial-regression" class="anchor" aria-hidden="true" href="#polynomial-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Polynomial Regression&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/polynomial_regression.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d82416364e7916546886f94027e2652d3247e8ab/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f705f7265672e676966"&gt;&lt;img src="https://camo.githubusercontent.com/d82416364e7916546886f94027e2652d3247e8ab/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f705f7265672e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/p_reg.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Training progress of a regularized polynomial regression model fitting &lt;br&gt;
    temperature data measured in Linköping, Sweden 2016.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-classification-with-cnn" class="anchor" aria-hidden="true" href="#classification-with-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Classification With CNN&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/convolutional_neural_network.py

+---------+
| ConvNet |
+---------+
Input Shape: (1, 8, 8)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Conv2D               | 160        | (16, 8, 8)   |
| Activation (ReLU)    | 0          | (16, 8, 8)   |
| Dropout              | 0          | (16, 8, 8)   |
| BatchNormalization   | 2048       | (16, 8, 8)   |
| Conv2D               | 4640       | (32, 8, 8)   |
| Activation (ReLU)    | 0          | (32, 8, 8)   |
| Dropout              | 0          | (32, 8, 8)   |
| BatchNormalization   | 4096       | (32, 8, 8)   |
| Flatten              | 0          | (2048,)      |
| Dense                | 524544     | (256,)       |
| Activation (ReLU)    | 0          | (256,)       |
| Dropout              | 0          | (256,)       |
| BatchNormalization   | 512        | (256,)       |
| Dense                | 2570       | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 538570

Training: 100% [------------------------------------------------------------------------] Time: 0:01:55
Accuracy: 0.987465181058
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c2bca09f5d1ce2b72f33fe61464408607797caa3/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f636e6e312e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/c2bca09f5d1ce2b72f33fe61464408607797caa3/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f636e6e312e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_cnn1.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Classification of the digit dataset using CNN.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-density-based-clustering" class="anchor" aria-hidden="true" href="#density-based-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Density-Based Clustering&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/dbscan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/eaf413b6e8cbf3f8fd048f3a63984482ffd7350e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64627363616e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/eaf413b6e8cbf3f8fd048f3a63984482ffd7350e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64627363616e2e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_dbscan.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Clustering of the moons dataset using DBSCAN.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-generating-handwritten-digits" class="anchor" aria-hidden="true" href="#generating-handwritten-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating Handwritten Digits&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py

+-----------+
| Generator |
+-----------+
Input Shape: (100,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 25856      | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| BatchNormalization     | 512        | (256,)       |
| Dense                  | 131584     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| BatchNormalization     | 1024       | (512,)       |
| Dense                  | 525312     | (1024,)      |
| Activation (LeakyReLU) | 0          | (1024,)      |
| BatchNormalization     | 2048       | (1024,)      |
| Dense                  | 803600     | (784,)       |
| Activation (TanH)      | 0          | (784,)       |
+------------------------+------------+--------------+
Total Parameters: 1489936

+---------------+
| Discriminator |
+---------------+
Input Shape: (784,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 401920     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| Dropout                | 0          | (512,)       |
| Dense                  | 131328     | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| Dropout                | 0          | (256,)       |
| Dense                  | 514        | (2,)         |
| Activation (Softmax)   | 0          | (2,)         |
+------------------------+------------+--------------+
Total Parameters: 533762
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/15ad5010011227a7ab8c6c77d19b7cc625cced30/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f67616e5f6d6e697374352e676966"&gt;&lt;img src="https://camo.githubusercontent.com/15ad5010011227a7ab8c6c77d19b7cc625cced30/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f67616e5f6d6e697374352e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/gan_mnist5.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Training progress of a Generative Adversarial Network generating &lt;br&gt;
    handwritten digits.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-deep-reinforcement-learning" class="anchor" aria-hidden="true" href="#deep-reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Reinforcement Learning&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/deep_q_network.py

+----------------+
| Deep Q-Network |
+----------------+
Input Shape: (4,)
+-------------------+------------+--------------+
| Layer Type        | Parameters | Output Shape |
+-------------------+------------+--------------+
| Dense             | 320        | (64,)        |
| Activation (ReLU) | 0          | (64,)        |
| Dense             | 130        | (2,)         |
+-------------------+------------+--------------+
Total Parameters: 450
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c605134f41b739121c4710f3d5c6e8370a592e0c/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64716c312e676966"&gt;&lt;img src="https://camo.githubusercontent.com/c605134f41b739121c4710f3d5c6e8370a592e0c/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64716c312e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_dql1.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-image-reconstruction-with-rbm" class="anchor" aria-hidden="true" href="#image-reconstruction-with-rbm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Reconstruction With RBM&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/restricted_boltzmann_machine.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d209d42aed9e8e32a10eaec9b76f141319a2b0d7/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f72626d5f646967697473312e676966"&gt;&lt;img src="https://camo.githubusercontent.com/d209d42aed9e8e32a10eaec9b76f141319a2b0d7/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f72626d5f646967697473312e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/rbm_digits1.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Shows how the network gets better during training at reconstructing &lt;br&gt;
    the digit 2 in the MNIST dataset.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-evolutionary-evolved-neural-network" class="anchor" aria-hidden="true" href="#evolutionary-evolved-neural-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evolutionary Evolved Neural Network&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/neuroevolution.py

+---------------+
| Model Summary |
+---------------+
Input Shape: (64,)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Dense                | 1040       | (16,)        |
| Activation (ReLU)    | 0          | (16,)        |
| Dense                | 170        | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 1210

Population Size: 100
Generations: 3000
Mutation Rate: 0.01

[0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]
[1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]
...
[2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]
Test set accuracy: 96.7%
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1a8abe4882d0195b8f8bd4c6f24caab639291e6e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f65766f5f6e6e342e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/1a8abe4882d0195b8f8bd4c6f24caab639291e6e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f65766f5f6e6e342e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/evo_nn4.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Classification of the digit dataset by a neural network which has&lt;br&gt;
    been evolutionary evolved.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-genetic-algorithm" class="anchor" aria-hidden="true" href="#genetic-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Genetic Algorithm&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/genetic_algorithm.py

+--------+
|   GA   |
+--------+
Description: Implementation of a Genetic Algorithm which aims to produce
the user specified target string. This implementation calculates each
candidate's fitness based on the alphabetical distance between the candidate
and the target. A candidate is selected as a parent with probabilities proportional
to the candidate's fitness. Reproduction is implemented as a single-point
crossover between pairs of parents. Mutation is done by randomly assigning
new characters with uniform probability.

Parameters
----------
Target String: 'Genetic Algorithm'
Population Size: 100
Mutation Rate: 0.05

[0 Closest Candidate: 'CJqlJguPlqzvpoJmb', Fitness: 0.00]
[1 Closest Candidate: 'MCxZxdr nlfiwwGEk', Fitness: 0.01]
[2 Closest Candidate: 'MCxZxdm nlfiwwGcx', Fitness: 0.01]
[3 Closest Candidate: 'SmdsAklMHn kBIwKn', Fitness: 0.01]
[4 Closest Candidate: '  lotneaJOasWfu Z', Fitness: 0.01]
...
[292 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[293 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[294 Answer: 'Genetic Algorithm']
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-association-analysis" class="anchor" aria-hidden="true" href="#association-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Association Analysis&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/apriori.py
+-------------+
|   Apriori   |
+-------------+
Minimum Support: 0.25
Minimum Confidence: 0.8
Transactions:
    [1, 2, 3, 4]
    [1, 2, 4]
    [1, 2]
    [2, 3, 4]
    [2, 3]
    [3, 4]
    [2, 4]
Frequent Itemsets:
    [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]
Rules:
    1 -&amp;gt; 2 (support: 0.43, confidence: 1.0)
    4 -&amp;gt; 2 (support: 0.57, confidence: 0.8)
    [1, 4] -&amp;gt; 2 (support: 0.29, confidence: 1.0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-implementations" class="anchor" aria-hidden="true" href="#implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementations&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-supervised-learning" class="anchor" aria-hidden="true" href="#supervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supervised Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/adaboost.py"&gt;Adaboost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/bayesian_regression.py"&gt;Bayesian Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/decision_tree.py"&gt;Decision Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Elastic Net&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/gradient_boosting.py"&gt;Gradient Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/k_nearest_neighbors.py"&gt;K Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Lasso Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/linear_discriminant_analysis.py"&gt;Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/logistic_regression.py"&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/multi_class_lda.py"&gt;Multi-class Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/naive_bayes.py"&gt;Naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/neuroevolution.py"&gt;Neuroevolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/particle_swarm_optimization.py"&gt;Particle Swarm Optimization of Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/perceptron.py"&gt;Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/random_forest.py"&gt;Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Ridge Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/support_vector_machine.py"&gt;Support Vector Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/xgboost.py"&gt;XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-unsupervised-learning" class="anchor" aria-hidden="true" href="#unsupervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/apriori.py"&gt;Apriori&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/autoencoder.py"&gt;Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/dbscan.py"&gt;DBSCAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/fp_growth.py"&gt;FP-Growth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/gaussian_mixture_model.py"&gt;Gaussian Mixture Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/generative_adversarial_network.py"&gt;Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/genetic_algorithm.py"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/k_means.py"&gt;K-Means&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/partitioning_around_medoids.py"&gt;Partitioning Around Medoids&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/principal_component_analysis.py"&gt;Principal Component Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/restricted_boltzmann_machine.py"&gt;Restricted Boltzmann Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-reinforcement-learning" class="anchor" aria-hidden="true" href="#reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reinforcement Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/reinforcement_learning/deep_q_network.py"&gt;Deep Q-Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deep-learning" class="anchor" aria-hidden="true" href="#deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/deep_learning/neural_network.py"&gt;Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/deep_learning/layers.py"&gt;Layers&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Activation Layer&lt;/li&gt;
&lt;li&gt;Average Pooling Layer&lt;/li&gt;
&lt;li&gt;Batch Normalization Layer&lt;/li&gt;
&lt;li&gt;Constant Padding Layer&lt;/li&gt;
&lt;li&gt;Convolutional Layer&lt;/li&gt;
&lt;li&gt;Dropout Layer&lt;/li&gt;
&lt;li&gt;Flatten Layer&lt;/li&gt;
&lt;li&gt;Fully-Connected (Dense) Layer&lt;/li&gt;
&lt;li&gt;Fully-Connected RNN Layer&lt;/li&gt;
&lt;li&gt;Max Pooling Layer&lt;/li&gt;
&lt;li&gt;Reshape Layer&lt;/li&gt;
&lt;li&gt;Up Sampling Layer&lt;/li&gt;
&lt;li&gt;Zero Padding Layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model Types
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/convolutional_neural_network.py"&gt;Convolutional Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/recurrent_neural_network.py"&gt;Recurrent Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;If there's some implementation you would like to see here or if you're just feeling social,
feel free to &lt;a href="mailto:eriklindernoren@gmail.com"&gt;email&lt;/a&gt; me or connect with me on &lt;a href="https://www.linkedin.com/in/eriklindernoren/" rel="nofollow"&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>eriklindernoren</author><guid isPermaLink="false">https://github.com/eriklindernoren/ML-From-Scratch</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>kovidgoyal/kitty #10 in Python, Today</title><link>https://github.com/kovidgoyal/kitty</link><description>&lt;p&gt;&lt;i&gt;A cross-platform, fast, feature full, GPU based terminal emulator&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body asciidoc" data-path="README.asciidoc"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-kitty---the-fast-featureful-gpu-based-terminal-emulator" class="anchor" aria-hidden="true" href="#kitty---the-fast-featureful-gpu-based-terminal-emulator"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;kitty - the fast, featureful, GPU based, terminal emulator&lt;/h1&gt;
&lt;div&gt;
&lt;p&gt;See &lt;a href="https://sw.kovidgoyal.net/kitty" rel="nofollow"&gt;https://sw.kovidgoyal.net/kitty&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;&lt;span&gt;&lt;a href="https://circleci.com/gh/kovidgoyal/kitty" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/17d4f18b8b8149d6fed9b62b2635ef595db49f23/68747470733a2f2f636972636c6563692e636f6d2f67682f6b6f766964676f79616c2f6b697474792e7376673f7374796c653d737667" alt="Build status" data-canonical-src="https://circleci.com/gh/kovidgoyal/kitty.svg?style=svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/article&gt;&lt;/div&gt;</description><author>kovidgoyal</author><guid isPermaLink="false">https://github.com/kovidgoyal/kitty</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>scikit-learn/scikit-learn #11 in Python, Today</title><link>https://github.com/scikit-learn/scikit-learn</link><description>&lt;p&gt;&lt;i&gt;scikit-learn: machine learning in Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img alt="Azure" src="https://camo.githubusercontent.com/bfe67a3604768c16e941f3331709bf55507a4b57/68747470733a2f2f6465762e617a7572652e636f6d2f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f5f617069732f6275696c642f7374617475732f7363696b69742d6c6561726e2e7363696b69742d6c6561726e3f6272616e63684e616d653d6d6173746572" data-canonical-src="https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://travis-ci.org/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="Travis" src="https://camo.githubusercontent.com/590475799489c962f111c9fc5c1432ecbc577578/68747470733a2f2f6170692e7472617669732d63692e6f72672f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://api.travis-ci.org/scikit-learn/scikit-learn.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/github/scikit-learn/scikit-learn?branch=master" rel="nofollow"&gt;&lt;img alt="Codecov" src="https://camo.githubusercontent.com/58a0b06906ca5d106ec090fe8a1ac85a092b81c2/68747470733a2f2f636f6465636f762e696f2f6769746875622f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" data-canonical-src="https://codecov.io/github/scikit-learn/scikit-learn/badge.svg?branch=master&amp;amp;service=github" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="CircleCI" src="https://camo.githubusercontent.com/d2913194913f85128f908483a265e64dcd6d31e4/68747470733a2f2f636972636c6563692e636f6d2f67682f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f747265652f6d61737465722e7376673f7374796c653d736869656c6426636972636c652d746f6b656e3d3a636972636c652d746f6b656e" data-canonical-src="https://circleci.com/gh/scikit-learn/scikit-learn/tree/master.svg?style=shield&amp;amp;circle-token=:circle-token" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/scikit-learn" rel="nofollow"&gt;&lt;img alt="Python35" src="https://camo.githubusercontent.com/53aa0b9151bc545b404852175644228ee0efffe2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e352d626c75652e737667" data-canonical-src="https://img.shields.io/badge/python-3.5-blue.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://badge.fury.io/py/scikit-learn" rel="nofollow"&gt;&lt;img alt="PyPi" src="https://camo.githubusercontent.com/9f0ed32d05350afa18a801573e4da7f4a240e181/68747470733a2f2f62616467652e667572792e696f2f70792f7363696b69742d6c6561726e2e737667" data-canonical-src="https://badge.fury.io/py/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="DOI" src="https://camo.githubusercontent.com/73c63e44b8bee62df142664048c58f83ec8ad95c/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f32313336392f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e737667" data-canonical-src="https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-scikit-learn"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-scikit-learn" class="anchor" aria-hidden="true" href="#scikit-learn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;scikit-learn&lt;/h2&gt;
&lt;p&gt;scikit-learn is a Python module for machine learning built on top of
SciPy and is distributed under the 3-Clause BSD license.&lt;/p&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the &lt;a href="http://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;It is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-installation"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;a name="user-content-dependencies"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h4&gt;
&lt;p&gt;scikit-learn requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python (&amp;gt;= 3.5)&lt;/li&gt;
&lt;li&gt;NumPy (&amp;gt;= 1.11.0)&lt;/li&gt;
&lt;li&gt;SciPy (&amp;gt;= 0.17.0)&lt;/li&gt;
&lt;li&gt;joblib (&amp;gt;= 0.11)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.&lt;/strong&gt;
scikit-learn 0.21 and later require Python 3.5 or newer.&lt;/p&gt;
&lt;p&gt;Scikit-learn plotting capabilities (i.e., functions start with "&lt;a href="#id2"&gt;&lt;span id="user-content-id3"&gt;plot_&lt;/span&gt;&lt;/a&gt;"
and classes end with "Display") require Matplotlib (&amp;gt;= 1.5.1). For running the
examples Matplotlib &amp;gt;= 1.5.1 is required. A few examples require
scikit-image &amp;gt;= 0.12.3, a few examples require pandas &amp;gt;= 0.18.0.&lt;/p&gt;
&lt;a name="user-content-user-installation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-user-installation" class="anchor" aria-hidden="true" href="#user-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;User installation&lt;/h4&gt;
&lt;p&gt;If you already have a working installation of numpy and scipy,
the easiest way to install scikit-learn is using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;pip install -U scikit-learn
&lt;/pre&gt;
&lt;p&gt;or &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;conda install scikit-learn
&lt;/pre&gt;
&lt;p&gt;The documentation includes more detailed &lt;a href="http://scikit-learn.org/stable/install.html" rel="nofollow"&gt;installation instructions&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-changelog"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h3&gt;
&lt;p&gt;See the &lt;a href="http://scikit-learn.org/dev/whats_new.html" rel="nofollow"&gt;changelog&lt;/a&gt;
for a history of notable changes to scikit-learn.&lt;/p&gt;
&lt;a name="user-content-development"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h3&gt;
&lt;p&gt;We welcome new contributors of all experience levels. The scikit-learn
community goals are to be helpful, welcoming, and effective. The
&lt;a href="http://scikit-learn.org/stable/developers/index.html" rel="nofollow"&gt;Development Guide&lt;/a&gt;
has detailed information about contributing code, documentation, tests, and
more. We've included some basic information in this README.&lt;/p&gt;
&lt;a name="user-content-important-links"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-important-links" class="anchor" aria-hidden="true" href="#important-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Important links&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Official source code repo: &lt;a href="https://github.com/scikit-learn/scikit-learn"&gt;https://github.com/scikit-learn/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Download releases: &lt;a href="https://pypi.org/project/scikit-learn/" rel="nofollow"&gt;https://pypi.org/project/scikit-learn/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Issue tracker: &lt;a href="https://github.com/scikit-learn/scikit-learn/issues"&gt;https://github.com/scikit-learn/scikit-learn/issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-source-code"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-source-code" class="anchor" aria-hidden="true" href="#source-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source code&lt;/h4&gt;
&lt;p&gt;You can check the latest sources with the command:&lt;/p&gt;
&lt;pre&gt;git clone https://github.com/scikit-learn/scikit-learn.git
&lt;/pre&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h4&gt;
&lt;p&gt;To learn more about making a contribution to scikit-learn, please see our
&lt;a href="https://scikit-learn.org/dev/developers/contributing.html" rel="nofollow"&gt;Contributing guide&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-testing"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h4&gt;
&lt;p&gt;After installation, you can launch the test suite from outside the
source directory (you will need to have &lt;code&gt;pytest&lt;/code&gt; &amp;gt;= 3.3.0 installed):&lt;/p&gt;
&lt;pre&gt;pytest sklearn
&lt;/pre&gt;
&lt;p&gt;See the web page &lt;a href="http://scikit-learn.org/dev/developers/advanced_installation.html#testing" rel="nofollow"&gt;http://scikit-learn.org/dev/developers/advanced_installation.html#testing&lt;/a&gt;
for more information.&lt;/p&gt;
&lt;blockquote&gt;
Random number generation can be controlled during testing by setting
the &lt;code&gt;SKLEARN_SEED&lt;/code&gt; environment variable.&lt;/blockquote&gt;
&lt;a name="user-content-submitting-a-pull-request"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-submitting-a-pull-request" class="anchor" aria-hidden="true" href="#submitting-a-pull-request"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Submitting a Pull Request&lt;/h4&gt;
&lt;p&gt;Before opening a Pull Request, have a look at the
full Contributing page to make sure your code complies
with our guidelines: &lt;a href="http://scikit-learn.org/stable/developers/index.html" rel="nofollow"&gt;http://scikit-learn.org/stable/developers/index.html&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-project-history"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-project-history" class="anchor" aria-hidden="true" href="#project-history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Project History&lt;/h3&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the  &lt;a href="http://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;The project is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: scikit-learn was previously referred to as scikits.learn.&lt;/p&gt;
&lt;a name="user-content-help-and-support"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-help-and-support" class="anchor" aria-hidden="true" href="#help-and-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Help and Support&lt;/h3&gt;
&lt;a name="user-content-documentation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;HTML documentation (stable release): &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HTML documentation (development version): &lt;a href="http://scikit-learn.org/dev/" rel="nofollow"&gt;http://scikit-learn.org/dev/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FAQ: &lt;a href="http://scikit-learn.org/stable/faq.html" rel="nofollow"&gt;http://scikit-learn.org/stable/faq.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-communication"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-communication" class="anchor" aria-hidden="true" href="#communication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Communication&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mailing list: &lt;a href="https://mail.python.org/mailman/listinfo/scikit-learn" rel="nofollow"&gt;https://mail.python.org/mailman/listinfo/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IRC channel: &lt;code&gt;#scikit-learn&lt;/code&gt; at &lt;code&gt;webchat.freenode.net&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Stack Overflow: &lt;a href="https://stackoverflow.com/questions/tagged/scikit-learn" rel="nofollow"&gt;https://stackoverflow.com/questions/tagged/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Website: &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-citation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h4&gt;
&lt;p&gt;If you use scikit-learn in a scientific publication, we would appreciate citations: &lt;a href="http://scikit-learn.org/stable/about.html#citing-scikit-learn" rel="nofollow"&gt;http://scikit-learn.org/stable/about.html#citing-scikit-learn&lt;/a&gt;&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>scikit-learn</author><guid isPermaLink="false">https://github.com/scikit-learn/scikit-learn</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>0Kee-Team/WatchAD #12 in Python, Today</title><link>https://github.com/0Kee-Team/WatchAD</link><description>&lt;p&gt;&lt;i&gt;AD Security Intrusion Detection System&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-watchad" class="anchor" aria-hidden="true" href="#watchad"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WatchAD&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0cf7b2ac10e10066587703fd1ab226f43738ccc4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e362b2d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/0cf7b2ac10e10066587703fd1ab226f43738ccc4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e362b2d626c75652e737667" alt="PyPI version" data-canonical-src="https://img.shields.io/badge/Python-3.6+-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.2/index.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a621aef6f67fc6d82cda928e20a147ed451ac26b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f456c61737469635365617263682d352e582d737563636573732e737667" alt="ElasticSearch version" data-canonical-src="https://img.shields.io/badge/ElasticSearch-5.X-success.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://www.elastic.co/guide/en/logstash/6.2/index.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/fdc4cc36d119bd27d61b72dfe98a2dfa6c029fef/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6f6773746173682d362e582d79656c6c6f77677265656e2e737667" alt="Logstash version" data-canonical-src="https://img.shields.io/badge/Logstash-6.X-yellowgreen.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://www.rabbitmq.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/66a1d53a5ee3d3700f45cc4330f7d036403331ec/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5261626269744d512d332e372d6f72616e67652e737667" alt="RabbitMQ version" data-canonical-src="https://img.shields.io/badge/RabbitMQ-3.7-orange.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://www.blueteamvillage.org/home/dc27/talks#h.p_5uroKErLDdmP" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5c492ee97f022b5253ffc9847a2f46bc3c5a18e5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444546253230434f4e25323032372d426c75652532305465616d25323056696c6c6167652d626c75652e737667" alt="DEF CON 27 Blue Team Village" data-canonical-src="https://img.shields.io/badge/DEF%20CON%2027-Blue%20Team%20Village-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;域安全入侵感知系统&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;WatchAD收集所有域控上的事件日志和kerberos流量，通过特征匹配、Kerberos协议分析、历史行为、敏感操作和蜜罐账户等方式来检测各种已知与未知威胁，功能覆盖了大部分目前的常见内网域渗透手法。该项目在360内部上线运行半年有余，发现多起威胁活动，取得了较好的效果。现决定开源系统中基于事件日志的检测部分。&lt;/p&gt;
&lt;p&gt;目前支持的具体检测功能如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;信息探测&lt;/strong&gt;：使用SAMR查询敏感用户组、使用SAMR查询敏感用户、蜜罐账户的活动、PsLoggedOn信息收集&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;凭证盗取&lt;/strong&gt;：Kerberoasting （流量）、AS-REP Roasting、远程Dump域控密码&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;横向移动&lt;/strong&gt;：账户爆破、显式凭据远程登录、目标域控的远程代码执行、未知文件共享名、Kerberos票据加密方式降级（流量）、异常的Kerberos票据请求（流量）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;权限提升&lt;/strong&gt;：ACL修改、MS17-010攻击检测、新增组策略监控、NTLM 中继检测、基于资源的约束委派权限授予检测、攻击打印机服务 SpoolSample、未知权限提升、MS14-068攻击检测（流量）、Kerberos约束委派滥用（流量）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;权限维持&lt;/strong&gt;：AdminSDHolder对象修改、DCShadow攻击检测、DSRM密码重置、组策略委派权限授予检测、Kerberos约束委派权限授予检测、敏感用户组修改、域控新增系统服务、域控新增计划任务、SIDHistory属性修改、万能钥匙-主动检测、万能钥匙-被动检测（流量）、黄金票据（流量）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;防御绕过&lt;/strong&gt;：事件日志清空、事件日志服务被关闭&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;其中标注了&lt;strong&gt;流量&lt;/strong&gt;的检测方法暂未在本次开源计划中，后续会根据大家的反馈继续开源。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本项目部分技术点在 &lt;a href="https://www.blueteamvillage.org/home/dc27/talks#h.p_5uroKErLDdmP" rel="nofollow"&gt;DEF CON 27 @ Blue Team Village&lt;/a&gt; 上发表演讲。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-安装部署" class="anchor" aria-hidden="true" href="#安装部署"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;安装部署&lt;/h2&gt;
&lt;p&gt;WatchAD是一个完整的检测系统，涉及的内容较多，请参考 &lt;a href="https://github.com/0Kee-Team/WatchAD/wiki/Install"&gt;安装教程&lt;/a&gt; 进行安装。如果你需要设置蜜罐账户，也可以参考我们关于蜜罐账户的&lt;a href="https://github.com/0Kee-Team/WatchAD/wiki/Honeypot-Account"&gt;说明&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;项目架构简图：&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/Architecture.png"&gt;&lt;img src="./images/Architecture.png" alt="Architecture" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本项目 WatchAD 只包含了检测引擎相关的代码，你可以选择直接将告警数据导入运营中心统一管理，或者使用我们开发的Web平台 &lt;a href="https://github.com/0Kee-Team/WatchAD-Web"&gt;WatchAD-Web&lt;/a&gt; ，它是一个为WatchAD定制的简易平台，可进行简单的运营工作，如果你对界面设计或者操作体验有更高的要求，请根据WatchAD的告警数据格式自行定制化开发。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-自定义检测模块" class="anchor" aria-hidden="true" href="#自定义检测模块"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;自定义检测模块&lt;/h2&gt;
&lt;p&gt;WatchAD支持开发自定义的检测模块，详情请参考我们的&lt;a href="https://github.com/0Kee-Team/WatchAD/wiki/Development"&gt;教程&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;如果你不需要某个&lt;strong&gt;检测模块&lt;/strong&gt;，直接将&lt;strong&gt;该py文件删除&lt;/strong&gt;并重启检测引擎即可，无需其它配置。&lt;code&gt;record&lt;/code&gt;目录中的文件不参与告警检测，只负责记录域内实体的关键活动，请勿删除。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--todo" class="anchor" aria-hidden="true" href="#-todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;// TODO&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;English Document&lt;/li&gt;
&lt;li&gt;English code comment&lt;/li&gt;
&lt;li&gt;ElasticSearch兼容6.X&lt;/li&gt;
&lt;li&gt;各个检测模块的误报持续优化&lt;/li&gt;
&lt;li&gt;Kerberoasting：基于事件日志检测的代码被流量替代，后续可添加&lt;/li&gt;
&lt;li&gt;Pass-the-Hash(PtH)：内部存在误报，暂缓开源&lt;/li&gt;
&lt;li&gt;Pass-the-Ticket(PtT)：内部存在误报，暂缓开源&lt;/li&gt;
&lt;li&gt;Silver-Ticket：内部存在误报，暂缓开源&lt;/li&gt;
&lt;li&gt;伪造用户信息：内部存在误报，暂缓开源&lt;/li&gt;
&lt;li&gt;基于用户行为的失陷账户检测&lt;/li&gt;
&lt;li&gt;基于Kerberos流量的分析开源&lt;/li&gt;
&lt;li&gt;NTLM流量分析&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果你有认为需要加入到WatchAD检测的攻击方法，请提issue告诉我们相关复现方式，或者提交PR成为本项目的贡献者。&lt;/p&gt;
&lt;p&gt;如果你发现某个检测模块有较多的误报（日均超过&lt;strong&gt;10&lt;/strong&gt;条），请提issue告诉我们或由你优化之后提交PR。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-follow-me" class="anchor" aria-hidden="true" href="#follow-me"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Follow me&lt;/h2&gt;
&lt;p&gt;微博： &lt;a href="https://weibo.com/u/5242748339" rel="nofollow"&gt;@9ian1i&lt;/a&gt;     Github： &lt;a href="https://github.com/Qianlitp"&gt;@9ian1i&lt;/a&gt;    Twitter：&lt;a href="https://twitter.com/9ian1i" rel="nofollow"&gt;@9ian1i&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-联系我们" class="anchor" aria-hidden="true" href="#联系我们"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;联系我们&lt;/h2&gt;
&lt;p&gt;我们来自360信息安全部&lt;a href="https://0kee.360.cn/" rel="nofollow"&gt;0KEE Team&lt;/a&gt;，如果你有安全工具或者安全系统开发经验，热衷于甲方安全建设，请投递简历到：zhanglu-it#360.cn、renyan-it#360.cn、zhusiyu1#360.cn。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-参考文档" class="anchor" aria-hidden="true" href="#参考文档"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;参考文档&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/infosecn1nja/AD-Attack-Defense"&gt;Active Directory Kill Chain Attack &amp;amp; Defense&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://adsecurity.org/" rel="nofollow"&gt;Active Directory Security&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/default.aspx?i=j" rel="nofollow"&gt;Windows Security Log Events&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://blog.harmj0y.net/" rel="nofollow"&gt;harmj0y's blog&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.microsoft.com/en-us/windows/security/threat-protection/auditing/event-4624" rel="nofollow"&gt;event log doc&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://itconnect.uw.edu/wares/msinf/other-help/understanding-sddl-syntax/" rel="nofollow"&gt;Understanding SDDL Syntax&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://blog.fox-it.com/2018/04/26/escalating-privileges-with-acls-in-active-directory/" rel="nofollow"&gt;Escalating privileges with ACLs in Active Directory&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://dirkjanm.io/abusing-exchange-one-api-call-away-from-domain-admin/" rel="nofollow"&gt;Abusing Exchange: One API call away from Domain Admin&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://3gstudent.github.io/3gstudent.github.io/" rel="nofollow"&gt;3gstudent's blog&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Penetration Testing Lab Blog&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://adsecurity.org/?page_id=4031" rel="nofollow"&gt;Attack Defense &amp;amp; Detection&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://blog.stealthbits.com/" rel="nofollow"&gt;INSIDER THREAT SECURITY BLOG&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://support.microsoft.com/en-us/help/305144/how-to-use-useraccountcontrol-to-manipulate-user-account-properties" rel="nofollow"&gt;How to use the UserAccountControl flags to manipulate user account properties&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.microsoft.com/en-us/advanced-threat-analytics/" rel="nofollow"&gt;Advanced Threat Analytics documentation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>0Kee-Team</author><guid isPermaLink="false">https://github.com/0Kee-Team/WatchAD</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>GuidoPaul/CAIL2019 #13 in Python, Today</title><link>https://github.com/GuidoPaul/CAIL2019</link><description>&lt;p&gt;&lt;i&gt;中国法研杯司法人工智能挑战赛之相似案例匹配第一名解决方案&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-中国法研杯相似案例匹配top1团队解决方案" class="anchor" aria-hidden="true" href="#中国法研杯相似案例匹配top1团队解决方案"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;中国法研杯相似案例匹配Top1团队解决方案&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-赛题介绍" class="anchor" aria-hidden="true" href="#赛题介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;赛题介绍&lt;/h2&gt;
&lt;p&gt;大赛共有三个赛道，我们参加了其中的法律文书“相似案例匹配”赛道。相似案例匹配赛题是针对多篇法律文书进行相似度的计算和判断。数据集是“中国裁判文书网”公开的民间借贷相关法律文书，每组数据由三篇法律文书组成。文书主要为案件的事实描述部分，选手需要从两篇候选集文书中找到与询问文书案件性质更为相似的一篇文书。&lt;/p&gt;
&lt;p&gt;比赛分三个阶段。第一阶段训练数据有500组三元文书，我们用(A, B, C)表示一条数据中的三篇文书。每条数据中A与B的相似度大于A与C的相似度。第二阶段有5102组训练数据，第三阶段为封闭式评测阶段。每个阶段如果预测正确，那么该测试数据可以得到1分，否则是0分。每个阶段成绩为在该阶段的平均准确率。最终成绩计算方式： 第二阶段的成绩 * 0.3 + 第三阶段的成绩 * 0.7。&lt;/p&gt;
&lt;p&gt;详情可查看&lt;a href="http://cail.cipsc.org.cn/" rel="nofollow"&gt;官网网站&lt;/a&gt;和&lt;a href="https://github.com/china-ai-law-challenge/CAIL2019"&gt;官方Github&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-项目构建" class="anchor" aria-hidden="true" href="#项目构建"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目构建&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-软件依赖" class="anchor" aria-hidden="true" href="#软件依赖"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;软件依赖&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Python 3.6+&lt;/li&gt;
&lt;li&gt;PyTorch 1.1.0+&lt;/li&gt;
&lt;li&gt;requirements.txt&lt;/li&gt;
&lt;li&gt;Windows 和 Linux 均可&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-模型文件" class="anchor" aria-hidden="true" href="#模型文件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模型文件&lt;/h3&gt;
&lt;p&gt;比赛中使用的BERT模型文件来自刘知远老师提供的&lt;a href="https://github.com/thunlp/OpenCLaP"&gt;预训练模型文件&lt;/a&gt;。&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-切分数据集" class="anchor" aria-hidden="true" href="#切分数据集"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;切分数据集&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd datasets
$ python ./split_folds.py
$ cd ..
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-模型训练" class="anchor" aria-hidden="true" href="#模型训练"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模型训练&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python train_bert.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-模型预测" class="anchor" aria-hidden="true" href="#模型预测"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模型预测&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python main.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>GuidoPaul</author><guid isPermaLink="false">https://github.com/GuidoPaul/CAIL2019</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>joelbarmettlerUZH/auto-tinder #14 in Python, Today</title><link>https://github.com/joelbarmettlerUZH/auto-tinder</link><description>&lt;p&gt;&lt;i&gt;🖖 Train an artificial intelligence to play tinder for you&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-auto-tinder---train-an-ai-to-swipe-tinder-for-you" class="anchor" aria-hidden="true" href="#auto-tinder---train-an-ai-to-swipe-tinder-for-you"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Auto-Tinder - Train an AI to swipe tinder for you&lt;/h1&gt;
&lt;p&gt;Auto-tinder was created to train an API using Tensorflow and Python3 that learns your
interests in the other sex and automatically plays the tinder swiping-game for you.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/joelbarmettlerUZH/auto-tinder/blob/master/resources//banner.png"&gt;&lt;img src="https://github.com/joelbarmettlerUZH/auto-tinder/raw/master/resources//banner.png" alt="alt text" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this document, I am going to explain the following steps that were needed to
create auto-tinder:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;analyze the tinder webpage to find out what internal API calls tinder makes, reconstruct the API calls in &lt;a href="https://www.getpostman.com/" rel="nofollow"&gt;Postman&lt;/a&gt; and analyze its content&lt;/li&gt;
&lt;li&gt;Build a api wrapper class in python that uses the tinder api to like/dislike/match etc.&lt;/li&gt;
&lt;li&gt;Download a bunch of images of people nearby&lt;/li&gt;
&lt;li&gt;Write a simple mouse-click classifier to label our images&lt;/li&gt;
&lt;li&gt;Develop a preprocessor that uses the tensorflow object detection API to only cut out the person
in our image&lt;/li&gt;
&lt;li&gt;Retrain inceptionv3, a deep convolutional neural network, to learn on our classified data&lt;/li&gt;
&lt;li&gt;Use the classifier in combination with the tinder API wrapper to play tinder for us&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-step-0-motivation-and-disclaimer" class="anchor" aria-hidden="true" href="#step-0-motivation-and-disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 0: Motivation and disclaimer&lt;/h2&gt;
&lt;p&gt;Auto tinder is a concept project purely created for fun and educational purposes.
It shall never be abused to harm anybody or spam the platform. The auto-tinder scripts
should not be used with your tinder profile since they surely violate tinders terms of service.&lt;/p&gt;
&lt;p&gt;I've written this piece of software mainly out of two reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Because I can and it was fun to create :)&lt;/li&gt;
&lt;li&gt;I wanted to find out whether an AI would actually be able to learn my
preferences in the other sex and be a reliable left-right-swipe partner for me.&lt;/li&gt;
&lt;li&gt;(Purely fictional reason: I am a lazy person, so why not invest
15 hours to code auto-tinder + 5 hours to label all images to save me a few hours of
actually swiping tinder myself? Sounds like a good deal to me!)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-step-1-analyze-the-tinder-api" class="anchor" aria-hidden="true" href="#step-1-analyze-the-tinder-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 1: Analyze the tinder API&lt;/h2&gt;
&lt;p&gt;The first step is to find out how the tinder app communicates to tinders backend server.
Since tinder offers a web version of its portal, this is as easy as going to
tinder.com, opening up chrome devtools and have a quick look at the network protocol.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/joelbarmettlerUZH/auto-tinder/master/resources/network_protocol.png"&gt;&lt;img src="https://raw.githubusercontent.com/joelbarmettlerUZH/auto-tinder/master/resources/network_protocol.png" alt="alt text" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The content shown in the picture above was from a request to &lt;a href="https://api.gotinder.com/v2/recs/core" rel="nofollow"&gt;https://api.gotinder.com/v2/recs/core&lt;/a&gt; that
is made when the tinder.com landing page is loading. Clearly, tinder has some sort
of internal API that they are using to communicate between the front- and backend.&lt;/p&gt;
&lt;p&gt;With analyzing the content of &lt;em&gt;/recs/core&lt;/em&gt;, it becomes clear that this API endpoint returns a list of
user profiles of people nearby.&lt;/p&gt;
&lt;p&gt;The data includes (among many other fields), the following data:&lt;/p&gt;
&lt;div class="highlight highlight-source-json"&gt;&lt;pre&gt;{
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;meta&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: {
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;status&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;200&lt;/span&gt;
    },
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;data&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: {
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;results&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [
            {
                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;type&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;user&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;user&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: {
                 &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;_id&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;4adfwe547s8df64df&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;bio&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;19y.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;birth_date&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;1997-17-06T18:21:44.654Z&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;name&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Anna&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;photos&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [
                        {
                            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;id&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;879sdfert-lskdföj-8asdf879-987sdflkj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;crop_info&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: {
                                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;user&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: {
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;width_pct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;1&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;x_offset_pct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;height_pct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.8&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;y_offset_pct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.08975463&lt;/span&gt;
                                },
                                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;algo&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: {
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;width_pct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.45674357&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;x_offset_pct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.984341657&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;height_pct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.234165403&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;y_offset_pct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.78902343&lt;/span&gt;
                                },
                                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;processed_by_bullseye&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;true&lt;/span&gt;,
                                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;user_customized&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;false&lt;/span&gt;
                            },
                            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;url&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://images-ssl.gotinder.com/4adfwe547s8df64df/original_879sdfert-lskdföj-8asdf879-987sdflkj.jpeg&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;processedFiles&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [
                                {
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;url&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://images-ssl.gotinder.com/4adfwe547s8df64df/640x800_879sdfert-lskdföj-8asdf879-987sdflkj.jpg&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;height&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;800&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;width&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;640&lt;/span&gt;
                                },
                                {
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;url&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://images-ssl.gotinder.com/4adfwe547s8df64df/320x400_879sdfert-lskdföj-8asdf879-987sdflkj.jpg&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;height&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;400&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;width&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;320&lt;/span&gt;
                                },
                                {
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;url&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://images-ssl.gotinder.com/4adfwe547s8df64df/172x216_879sdfert-lskdföj-8asdf879-987sdflkj.jpg&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;height&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;216&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;width&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;172&lt;/span&gt;
                                },
                                {
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;url&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://images-ssl.gotinder.com/4adfwe547s8df64df/84x106_879sdfert-lskdföj-8asdf879-987sdflkj.jpg&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;height&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;106&lt;/span&gt;,
                                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;width&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;84&lt;/span&gt;
                                }
                            ],
                            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;last_update_time&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;2019-10-03T16:18:30.532Z&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;fileName&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;879sdfert-lskdföj-8asdf879-987sdflkj.webp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;extension&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;jpg,webp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;webp_qf&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [
                                &lt;span class="pl-c1"&gt;75&lt;/span&gt;
                            ]
                        }
                    ],
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;gender&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;1&lt;/span&gt;,
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;jobs&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [],
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;schools&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [],
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;show_gender_on_profile&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;false&lt;/span&gt;
                },
                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;facebook&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: {
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;common_connections&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [],
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;connection_count&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0&lt;/span&gt;,
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;common_interests&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: []
                },
                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spotify&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: {
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spotify_connected&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;false&lt;/span&gt;
                },
                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;distance_mi&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;1&lt;/span&gt;,
                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;content_hash&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;slkadjfiuwejsdfuzkejhrsdbfskdzufiuerwer&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;s_number&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;9876540657341&lt;/span&gt;,
                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;teaser&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: {
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;string&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
                },
                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;teasers&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [],
                &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;snap&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: {
                    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;snaps&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: []
                }
            }
        ]
    }
}
           &lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A few things are very interesting here &lt;em&gt;(note that I changed all the data to not violate this persons privacy)&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All images are publicly accessible. If you copy the image URL and open it in a private window, it still loads instantly - meaning that tinder
uploads all user images publicly to the internet, free to be seen by anybody.&lt;/li&gt;
&lt;li&gt;The original photos accessible via the API are extremely high resolution. If you upload a photo to tinder, they will scale it down for the in-app
usage, but they store the original version publicly on their servers, accessible by anybody.&lt;/li&gt;
&lt;li&gt;Even if you choose to "show_gender_on_profile", everybody can still see your gender via the API &lt;em&gt;("gender": 1, where 1=Woman, 0=Man)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;If you send multiple requests to the tinder API consecutively, you always get different results (e.g. different profiles). We can therefore
just call this endpoint repeatedly to "farm" a bunch of pictures that we can later use to train our neural network.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With analyzing the content headers, we quickly find our private API Keys: &lt;strong&gt;X-Auth-Token&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/joelbarmettlerUZH/auto-tinder/master/resources/request.png"&gt;&lt;img src="https://raw.githubusercontent.com/joelbarmettlerUZH/auto-tinder/master/resources/request.png" alt="alt text" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With copying this token and going over to Postman, we can validate that we can
indeed freely communicate with the tinder API with just the right URL and our auth token.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/joelbarmettlerUZH/auto-tinder/master/resources/postman.png"&gt;&lt;img src="https://raw.githubusercontent.com/joelbarmettlerUZH/auto-tinder/master/resources/postman.png" alt="alt text" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With clicking a bit through tinders webapp, I quickly discover all relevant API endpoints:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th align="center"&gt;URL&lt;/th&gt;
&lt;th align="right"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;GET&lt;/td&gt;
&lt;td align="center"&gt;/v2/recs/core&lt;/td&gt;
&lt;td align="right"&gt;Returns a list of people nearby&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GET&lt;/td&gt;
&lt;td align="center"&gt;/v2/profile?include=account%2Cuser&lt;/td&gt;
&lt;td align="right"&gt;Returns all information about your own profile&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GET&lt;/td&gt;
&lt;td align="center"&gt;/v2/matches&lt;/td&gt;
&lt;td align="right"&gt;Returns  a list of all people that have matched with you&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GET&lt;/td&gt;
&lt;td align="center"&gt;/like/{user_id}&lt;/td&gt;
&lt;td align="right"&gt;Likes the person with the given user_id&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GET&lt;/td&gt;
&lt;td align="center"&gt;/pass/{user_id}&lt;/td&gt;
&lt;td align="right"&gt;Passes the person with the given user_id&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-step-2-building-an-api-wrapper-in-python" class="anchor" aria-hidden="true" href="#step-2-building-an-api-wrapper-in-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 2: Building an API Wrapper in Python&lt;/h2&gt;
&lt;p&gt;So let's get into the code. We will use the python &lt;a href="https://requests.kennethreitz.org/en/master/" rel="nofollow"&gt;Requests&lt;/a&gt; library to communicate with
the API and write an API wrapper class around it for convenience.&lt;/p&gt;
&lt;p&gt;Similarly, we write a small Person class that takes the API response from Tinder representing a Person and
offers a few basic interfaces to the tinder API.&lt;/p&gt;
&lt;p&gt;Let's start with the Person Class. It shall receive API data, a tinder-api object and save all relevant data
into instance variables. It shall further offer some basic features like "like" or "dislike" that make
a request to the tinder-api, which allows us to conveniently use "some_person.like()" in order to like
a profile we find interesting.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; datetime
&lt;span class="pl-k"&gt;from&lt;/span&gt; geopy.geocoders &lt;span class="pl-k"&gt;import&lt;/span&gt; Nominatim

&lt;span class="pl-c1"&gt;TINDER_URL&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://api.gotinder.com&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
geolocator &lt;span class="pl-k"&gt;=&lt;/span&gt; Nominatim(&lt;span class="pl-v"&gt;user_agent&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;auto-tinder&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-c1"&gt;PROF_FILE&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;./images/unclassified/profiles.txt&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Person&lt;/span&gt;(&lt;span class="pl-c1"&gt;object&lt;/span&gt;):

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;data&lt;/span&gt;, &lt;span class="pl-smi"&gt;api&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;._api &lt;span class="pl-k"&gt;=&lt;/span&gt; api

        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.id &lt;span class="pl-k"&gt;=&lt;/span&gt; data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;_id&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.name &lt;span class="pl-k"&gt;=&lt;/span&gt; data.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;name&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Unknown&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.bio &lt;span class="pl-k"&gt;=&lt;/span&gt; data.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;bio&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.distance &lt;span class="pl-k"&gt;=&lt;/span&gt; data.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;distance_mi&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;) &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;1.60934&lt;/span&gt;

        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.birth_date &lt;span class="pl-k"&gt;=&lt;/span&gt; datetime.datetime.strptime(data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;birth_date&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;%Y-%m-&lt;span class="pl-c1"&gt;%d&lt;/span&gt;T%H:%M:%S.&lt;span class="pl-c1"&gt;%f&lt;/span&gt;Z&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;if&lt;/span&gt; data.get(
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;birth_date&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;False&lt;/span&gt;) &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-c1"&gt;None&lt;/span&gt;
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.gender &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Male&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Female&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Unknown&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;][data.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;gender&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;)]

        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.images &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-c1"&gt;map&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;photo&lt;/span&gt;: photo[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;url&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], data.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;photos&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, [])))

        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.jobs &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(
            &lt;span class="pl-c1"&gt;map&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;job&lt;/span&gt;: {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;title&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: job.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;title&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, {}).get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;name&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;), &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;company&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: job.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;company&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, {}).get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;name&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)}, data.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;jobs&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, [])))
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.schools &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-c1"&gt;map&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;school&lt;/span&gt;: school[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;name&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], data.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;schools&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, [])))

        &lt;span class="pl-k"&gt;if&lt;/span&gt; data.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pos&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;False&lt;/span&gt;):
            &lt;span class="pl-c1"&gt;self&lt;/span&gt;.location &lt;span class="pl-k"&gt;=&lt;/span&gt; geolocator.reverse(&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pos&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;lat&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-s"&gt;, &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pos&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;lon&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;)


    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__repr__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;&lt;span class="pl-c1"&gt;self&lt;/span&gt;.id&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-s"&gt;  -  &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;&lt;span class="pl-c1"&gt;self&lt;/span&gt;.name&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-s"&gt; (&lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;&lt;span class="pl-c1"&gt;self&lt;/span&gt;.birth_date.strftime(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-c1"&gt;%d&lt;/span&gt;.%m.%Y&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-s"&gt;)&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;


    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;like&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._api.like(&lt;span class="pl-c1"&gt;self&lt;/span&gt;.id)

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;dislike&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._api.dislike(&lt;span class="pl-c1"&gt;self&lt;/span&gt;.id)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Our API wrapper is not much more than a fancy way of calling the tinder API using a class:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; requests

&lt;span class="pl-c1"&gt;TINDER_URL&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://api.gotinder.com&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;tinderAPI&lt;/span&gt;():

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;token&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;._token &lt;span class="pl-k"&gt;=&lt;/span&gt; token

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;profile&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        data &lt;span class="pl-k"&gt;=&lt;/span&gt; requests.get(&lt;span class="pl-c1"&gt;TINDER_URL&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/v2/profile?include=account%2Cuser&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;headers&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;X-Auth-Token&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;self&lt;/span&gt;._token}).json()
        &lt;span class="pl-k"&gt;return&lt;/span&gt; Profile(data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;data&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-c1"&gt;self&lt;/span&gt;)

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;matches&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;limit&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;10&lt;/span&gt;):
        data &lt;span class="pl-k"&gt;=&lt;/span&gt; requests.get(&lt;span class="pl-c1"&gt;TINDER_URL&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;/v2/matches?count=&lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;limit&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;, &lt;span class="pl-v"&gt;headers&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;X-Auth-Token&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;self&lt;/span&gt;._token}).json()
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-c1"&gt;map&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;match&lt;/span&gt;: Person(match[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;person&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-c1"&gt;self&lt;/span&gt;), data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;data&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;matches&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]))

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;like&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;user_id&lt;/span&gt;):
        data &lt;span class="pl-k"&gt;=&lt;/span&gt; requests.get(&lt;span class="pl-c1"&gt;TINDER_URL&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;/like/&lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;user_id&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;, &lt;span class="pl-v"&gt;headers&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;X-Auth-Token&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;self&lt;/span&gt;._token}).json()
        &lt;span class="pl-k"&gt;return&lt;/span&gt; {
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;is_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;],
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;liked_remaining&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;likes_remaining&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
        }

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;dislike&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;user_id&lt;/span&gt;):
        requests.get(&lt;span class="pl-c1"&gt;TINDER_URL&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;/pass/&lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;user_id&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;, &lt;span class="pl-v"&gt;headers&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;X-Auth-Token&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;self&lt;/span&gt;._token}).json()
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;nearby_persons&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        data &lt;span class="pl-k"&gt;=&lt;/span&gt; requests.get(&lt;span class="pl-c1"&gt;TINDER_URL&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/v2/recs/core&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;headers&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;X-Auth-Token&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;self&lt;/span&gt;._token}).json()
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-c1"&gt;map&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;user&lt;/span&gt;: Person(user[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;user&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-c1"&gt;self&lt;/span&gt;), data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;data&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;results&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can now use the API to find people nearby and have a look at their profile, or even like all of them.
Replace YOUR-API-TOKEN with the X-Auth-Token you found in the chrome dev console earlier.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;__name__&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;__main__&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;:
    token &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;YOUR-API-TOKEN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
    api &lt;span class="pl-k"&gt;=&lt;/span&gt; tinderAPI(token)

    &lt;span class="pl-k"&gt;while&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;:
        persons &lt;span class="pl-k"&gt;=&lt;/span&gt; api.nearby_persons()
        &lt;span class="pl-k"&gt;for&lt;/span&gt; person &lt;span class="pl-k"&gt;in&lt;/span&gt; persons:
            &lt;span class="pl-c1"&gt;print&lt;/span&gt;(person)
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; person.like()&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-step-3-download-images-of-people-nearby" class="anchor" aria-hidden="true" href="#step-3-download-images-of-people-nearby"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 3: Download images of people nearby&lt;/h2&gt;
&lt;p&gt;Next, we want to automatically download some images of people nearby that we can use for training our AI.
With 'some', I mean like 1500-2500 images.&lt;/p&gt;
&lt;p&gt;First, let's extend our Person class with a function that allows us to download images.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; At the top of auto_tinder.py&lt;/span&gt;
&lt;span class="pl-c1"&gt;PROF_FILE&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;./images/unclassified/profiles.txt&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; inside the Person-class&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;download_images&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;folder&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;sleep_max_for&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;):
        &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-c1"&gt;PROF_FILE&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;r&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
            lines &lt;span class="pl-k"&gt;=&lt;/span&gt; f.readlines()
            &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.id &lt;span class="pl-k"&gt;in&lt;/span&gt; lines:
                &lt;span class="pl-k"&gt;return&lt;/span&gt;
        &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-c1"&gt;PROF_FILE&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;a&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
            f.write(&lt;span class="pl-c1"&gt;self&lt;/span&gt;.id&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-cce"&gt;\r\n&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
        index &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;
        &lt;span class="pl-k"&gt;for&lt;/span&gt; image_url &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.images:
            index &lt;span class="pl-k"&gt;+=&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;
            req &lt;span class="pl-k"&gt;=&lt;/span&gt; requests.get(image_url, &lt;span class="pl-v"&gt;stream&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
            &lt;span class="pl-k"&gt;if&lt;/span&gt; req.status_code &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-c1"&gt;200&lt;/span&gt;:
                &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;folder&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-s"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;&lt;span class="pl-c1"&gt;self&lt;/span&gt;.id&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-s"&gt;_&lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;&lt;span class="pl-c1"&gt;self&lt;/span&gt;.name&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-s"&gt;_&lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;index&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-s"&gt;.jpeg&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;wb&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
                    f.write(req.content)
            sleep(random()&lt;span class="pl-k"&gt;*&lt;/span&gt;sleep_max_for)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that I added some random sleeps here and there, just because we will likely be blocked if we
spam the tinder CDN and download many pictures in just a few seconds.&lt;/p&gt;
&lt;p&gt;We write all the peoples profile IDs into a file called "profiles.txt". By first scanning the document
whether a particular person is already in there, we can skip people we already encountered, and
we ensure that we don't classify people several times (you will see later why this is a risk).&lt;/p&gt;
&lt;p&gt;We can now just loop over nearby persons and download their images into an "unclassified" folder.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;__name__&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;__main__&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;:
    token &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;YOUR-API-TOKEN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
    api &lt;span class="pl-k"&gt;=&lt;/span&gt; tinderAPI(token)

    &lt;span class="pl-k"&gt;while&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;:
        persons &lt;span class="pl-k"&gt;=&lt;/span&gt; api.nearby_persons()
        &lt;span class="pl-k"&gt;for&lt;/span&gt; person &lt;span class="pl-k"&gt;in&lt;/span&gt; persons:
            person.download_images(&lt;span class="pl-v"&gt;folder&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;./images/unclassified&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;sleep_max_for&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;random()&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-c1"&gt;3&lt;/span&gt;)
            sleep(random()&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-c1"&gt;10&lt;/span&gt;)
        sleep(random()&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-c1"&gt;10&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can now simply start this script and let it run for a few hours to get a few hundret profile images of people
nearby. If you are a tinder PRO user, update your location now and then to get new people.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-step-4-classify-the-images-manually" class="anchor" aria-hidden="true" href="#step-4-classify-the-images-manually"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 4: Classify the images manually&lt;/h2&gt;
&lt;p&gt;Now that we have a bunch of images to work with, let's build a really simple and ugly classifier.&lt;/p&gt;
&lt;p&gt;It shall just loop over all the images in our "unclassified" folder and open the image in a GUI window.
By right-clicking a person, we can mark the person as "dislike", while a left-click marks the person
as "like". This will be represented in the filename later on: &lt;em&gt;4tz3kjldfj3482.jpg&lt;/em&gt; will be renamed
to &lt;em&gt;1_4tz3kjldfj3482.jpg&lt;/em&gt; if we mark the image as "like", or &lt;em&gt;0_4tz3kjldfj3482.jpg&lt;/em&gt; otherwise.
The label like/dislike is encoded as 1/0 in the beginning of the filenmae.&lt;/p&gt;
&lt;p&gt;Let's use tkinter to write this GUI quickly:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; os &lt;span class="pl-k"&gt;import&lt;/span&gt; listdir, rename
&lt;span class="pl-k"&gt;from&lt;/span&gt; os.path &lt;span class="pl-k"&gt;import&lt;/span&gt; isfile, join
&lt;span class="pl-k"&gt;import&lt;/span&gt; tkinter &lt;span class="pl-k"&gt;as&lt;/span&gt; tk
&lt;span class="pl-k"&gt;from&lt;/span&gt; &lt;span class="pl-c1"&gt;PIL&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; ImageTk, Image

&lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;./images/unclassified&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;

images &lt;span class="pl-k"&gt;=&lt;/span&gt; [f &lt;span class="pl-k"&gt;for&lt;/span&gt; f &lt;span class="pl-k"&gt;in&lt;/span&gt; listdir(&lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt;) &lt;span class="pl-k"&gt;if&lt;/span&gt; isfile(join(&lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt;, f))]
unclassified_images &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;filter&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;image&lt;/span&gt;: &lt;span class="pl-k"&gt;not&lt;/span&gt; (image.startswith(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;0_&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;or&lt;/span&gt; image.startswith(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;1_&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)), images)
current &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;None&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;next_img&lt;/span&gt;():
    &lt;span class="pl-k"&gt;global&lt;/span&gt; current, unclassified_images
    &lt;span class="pl-k"&gt;try&lt;/span&gt;:
        current &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;next&lt;/span&gt;(unclassified_images)
    &lt;span class="pl-k"&gt;except&lt;/span&gt; &lt;span class="pl-c1"&gt;StopIteration&lt;/span&gt;:
        root.quit()
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(current)
    pil_img &lt;span class="pl-k"&gt;=&lt;/span&gt; Image.open(&lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt;&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-k"&gt;+&lt;/span&gt;current)
    width, height &lt;span class="pl-k"&gt;=&lt;/span&gt; pil_img.size
    max_height &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1000&lt;/span&gt;
    &lt;span class="pl-k"&gt;if&lt;/span&gt; height &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; max_height:
        resize_factor &lt;span class="pl-k"&gt;=&lt;/span&gt; max_height &lt;span class="pl-k"&gt;/&lt;/span&gt; height
        pil_img &lt;span class="pl-k"&gt;=&lt;/span&gt; pil_img.resize((&lt;span class="pl-c1"&gt;int&lt;/span&gt;(width&lt;span class="pl-k"&gt;*&lt;/span&gt;resize_factor), &lt;span class="pl-c1"&gt;int&lt;/span&gt;(height&lt;span class="pl-k"&gt;*&lt;/span&gt;resize_factor)), &lt;span class="pl-v"&gt;resample&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;Image.&lt;span class="pl-c1"&gt;LANCZOS&lt;/span&gt;)
    img_tk &lt;span class="pl-k"&gt;=&lt;/span&gt; ImageTk.PhotoImage(pil_img)
    img_label.img &lt;span class="pl-k"&gt;=&lt;/span&gt; img_tk
    img_label.config(&lt;span class="pl-v"&gt;image&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;img_label.img)

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;positive&lt;/span&gt;(&lt;span class="pl-smi"&gt;arg&lt;/span&gt;):
    &lt;span class="pl-k"&gt;global&lt;/span&gt; current
    rename(&lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt;&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-k"&gt;+&lt;/span&gt;current, &lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt;&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/1_&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-k"&gt;+&lt;/span&gt;current)
    next_img()

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;negative&lt;/span&gt;(&lt;span class="pl-smi"&gt;arg&lt;/span&gt;):
    &lt;span class="pl-k"&gt;global&lt;/span&gt; current
    rename(&lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; current, &lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/0_&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; current)
    next_img()


&lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;__name__&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;__main__&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;:

    root &lt;span class="pl-k"&gt;=&lt;/span&gt; tk.Tk()

    img_label &lt;span class="pl-k"&gt;=&lt;/span&gt; tk.Label(root)
    img_label.pack()
    img_label.bind(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&amp;lt;Button-1&amp;gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, positive)
    img_label.bind(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&amp;lt;Button-3&amp;gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, negative)

    btn &lt;span class="pl-k"&gt;=&lt;/span&gt; tk.Button(root, &lt;span class="pl-v"&gt;text&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Next image&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;command&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;next_img)

    next_img() &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load first image&lt;/span&gt;

    root.mainloop()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We load all unclassified images into the "unclassified_images" list, open up a tkinter window, pack the first image into it
by calling next_img() and resize the image to fit onto the screen. Then, we register two clicks, left-and right mouse buttons,
and call the functions positive/negative that renames the images according to their label and show the next image.&lt;/p&gt;
&lt;p&gt;Ugly but effective.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-step-5-develop-a-preprocessor-to-cut-out-only-the-person-in-our-images" class="anchor" aria-hidden="true" href="#step-5-develop-a-preprocessor-to-cut-out-only-the-person-in-our-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 5: Develop a preprocessor to cut out only the person in our images&lt;/h2&gt;
&lt;p&gt;For the next step, we need to bring our image data into a format that allows us to
do a classification. There are a few difficulties we have to consider given our dataset.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Dataset Size:&lt;/strong&gt; Our Dataset is relatively small. We deal with +-2000 Images, which is considered
a very low amount of data, given the complexity of them (RGB Images with high resolution)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Variance:&lt;/strong&gt; The pictures sometimes contain people from behind, sometimes only faces, sometimes
no people at all.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Noise:&lt;/strong&gt; Most pictures not only contain the person itself, but often the surrounding which can
be distracting four our AI.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We combat these challenges by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Converting our images to greyscale, to reduce the amount of information that our AI has to learn
by a factor of 3 (RGB to G)&lt;/li&gt;
&lt;li&gt;Cutting out only the part of the image that actually contains the person, nothing else&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/joelbarmettlerUZH/auto-tinder/blob/master/resources//preprocessing.png"&gt;&lt;img src="https://github.com/joelbarmettlerUZH/auto-tinder/raw/master/resources//preprocessing.png" alt="alt text" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first part is as easy as using Pillow to open up our image and convert it to greyscale.
For the second part, we use the
&lt;a href="https://github.com/tensorflow/models/tree/master/research/object_detection"&gt;Tensorflow Object Detection API&lt;/a&gt;
with the mobilenet network architecture, pretrained on the coco dataset that also contains a label
for "Person".&lt;/p&gt;
&lt;p&gt;Our script for person detection has four parts:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-part-1-opening-the-pretrained-mobilenet-coco-dataset-as-a-tensorflow-graph" class="anchor" aria-hidden="true" href="#part-1-opening-the-pretrained-mobilenet-coco-dataset-as-a-tensorflow-graph"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 1: Opening the pretrained mobilenet coco dataset as a Tensorflow graph&lt;/h3&gt;
&lt;p&gt;You find the .bp file for the tensorflow mobilenet coco graph in my Github repository.
Let's open it as a Tensorflow graph:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow &lt;span class="pl-k"&gt;as&lt;/span&gt; tf

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;open_graph&lt;/span&gt;():
    detection_graph &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.Graph()
    &lt;span class="pl-k"&gt;with&lt;/span&gt; detection_graph.as_default():
        od_graph_def &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.GraphDef()
        &lt;span class="pl-k"&gt;with&lt;/span&gt; tf.gfile.GFile(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;rb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; fid:
            serialized_graph &lt;span class="pl-k"&gt;=&lt;/span&gt; fid.read()
            od_graph_def.ParseFromString(serialized_graph)
            tf.import_graph_def(od_graph_def, &lt;span class="pl-v"&gt;name&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    &lt;span class="pl-k"&gt;return&lt;/span&gt; detection_graph&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-part-2-load-in-images-as-numpy-arrays" class="anchor" aria-hidden="true" href="#part-2-load-in-images-as-numpy-arrays"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 2: Load in images as numpy arrays&lt;/h3&gt;
&lt;p&gt;We use Pillow for image manipulation. Since tensorflow needs raw numpy arrays to work with the data,
let's write a small function that converts Pillow images to numpy arrays:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;load_image_into_numpy_array&lt;/span&gt;(&lt;span class="pl-smi"&gt;image&lt;/span&gt;):
    (im_width, im_height) &lt;span class="pl-k"&gt;=&lt;/span&gt; image.size
    &lt;span class="pl-k"&gt;return&lt;/span&gt; np.array(image.getdata()).reshape(
        (im_height, im_width, &lt;span class="pl-c1"&gt;3&lt;/span&gt;)).astype(np.uint8)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-part-3-call-object-detection-api" class="anchor" aria-hidden="true" href="#part-3-call-object-detection-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 3: Call object detection API&lt;/h3&gt;
&lt;p&gt;The next function takes an image and a tensorflow graph, runs a tensorflow session using it
and return all informations about the detected classes (object types), bounding boxes
and scores (certainty that the object was detected correctly).&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;from&lt;/span&gt; object_detection.utils &lt;span class="pl-k"&gt;import&lt;/span&gt; ops &lt;span class="pl-k"&gt;as&lt;/span&gt; utils_ops
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow &lt;span class="pl-k"&gt;as&lt;/span&gt; tf

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;run_inference_for_single_image&lt;/span&gt;(&lt;span class="pl-smi"&gt;image&lt;/span&gt;, &lt;span class="pl-smi"&gt;sess&lt;/span&gt;):
    ops &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.get_default_graph().get_operations()
    all_tensor_names &lt;span class="pl-k"&gt;=&lt;/span&gt; {output.name &lt;span class="pl-k"&gt;for&lt;/span&gt; op &lt;span class="pl-k"&gt;in&lt;/span&gt; ops &lt;span class="pl-k"&gt;for&lt;/span&gt; output &lt;span class="pl-k"&gt;in&lt;/span&gt; op.outputs}
    tensor_dict &lt;span class="pl-k"&gt;=&lt;/span&gt; {}
    &lt;span class="pl-k"&gt;for&lt;/span&gt; key &lt;span class="pl-k"&gt;in&lt;/span&gt; [
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;num_detections&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_boxes&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_scores&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_classes&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_masks&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
    ]:
        tensor_name &lt;span class="pl-k"&gt;=&lt;/span&gt; key &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;:0&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
        &lt;span class="pl-k"&gt;if&lt;/span&gt; tensor_name &lt;span class="pl-k"&gt;in&lt;/span&gt; all_tensor_names:
            tensor_dict[key] &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.get_default_graph().get_tensor_by_name(
                tensor_name)
    &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_masks&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;in&lt;/span&gt; tensor_dict:
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; The following processing is only for single image&lt;/span&gt;
        detection_boxes &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.squeeze(tensor_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_boxes&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;0&lt;/span&gt;])
        detection_masks &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.squeeze(tensor_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_masks&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;0&lt;/span&gt;])
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.&lt;/span&gt;
        real_num_detection &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.cast(tensor_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;num_detections&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;], tf.int32)
        detection_boxes &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.slice(detection_boxes, [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;], [real_num_detection, &lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;])
        detection_masks &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.slice(detection_masks, [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;], [real_num_detection, &lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;])
        detection_masks_reframed &lt;span class="pl-k"&gt;=&lt;/span&gt; utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[&lt;span class="pl-c1"&gt;1&lt;/span&gt;], image.shape[&lt;span class="pl-c1"&gt;2&lt;/span&gt;])
        detection_masks_reframed &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.cast(
            tf.greater(detection_masks_reframed, &lt;span class="pl-c1"&gt;0.5&lt;/span&gt;), tf.uint8)
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Follow the convention by adding back the batch dimension&lt;/span&gt;
        tensor_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_masks&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.expand_dims(
            detection_masks_reframed, &lt;span class="pl-c1"&gt;0&lt;/span&gt;)
    image_tensor &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.get_default_graph().get_tensor_by_name(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;image_tensor:0&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run inference&lt;/span&gt;
    output_dict &lt;span class="pl-k"&gt;=&lt;/span&gt; sess.run(tensor_dict,
                           &lt;span class="pl-v"&gt;feed_dict&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;{image_tensor: image})

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; all outputs are float32 numpy arrays, so convert types as appropriate&lt;/span&gt;
    output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;num_detections&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;int&lt;/span&gt;(output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;num_detections&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;])
    output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_classes&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; output_dict[
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_classes&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;].astype(np.int64)
    output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_boxes&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_boxes&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;]
    output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_scores&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_scores&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;]
    &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_masks&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;in&lt;/span&gt; output_dict:
        output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_masks&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;detection_masks&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;]
    &lt;span class="pl-k"&gt;return&lt;/span&gt; output_dict&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-part-4-bringing-it-all-together-to-find-the-person" class="anchor" aria-hidden="true" href="#part-4-bringing-it-all-together-to-find-the-person"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 4: Bringing it all together to find the person&lt;/h3&gt;
&lt;p&gt;The last step is to write a function that takes an image path, opens it using Pillow,
calls the object detection api interface and crops the image according to the
detected persons bounding box.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;from&lt;/span&gt; &lt;span class="pl-c1"&gt;PIL&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; Image

&lt;span class="pl-c1"&gt;PERSON_CLASS&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;
&lt;span class="pl-c1"&gt;SCORE_THRESHOLD&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0.5&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;get_person&lt;/span&gt;(&lt;span class="pl-smi"&gt;image_path&lt;/span&gt;, &lt;span class="pl-smi"&gt;sess&lt;/span&gt;):
    img &lt;span class="pl-k"&gt;=&lt;/span&gt; Image.open(image_path)
    image_np &lt;span class="pl-k"&gt;=&lt;/span&gt; load_image_into_numpy_array(img)
    image_np_expanded &lt;span class="pl-k"&gt;=&lt;/span&gt; np.expand_dims(image_np, &lt;span class="pl-v"&gt;axis&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
    output_dict &lt;span class="pl-k"&gt;=&lt;/span&gt; run_inference_for_single_image(image_np_expanded, sess)

    persons_coordinates &lt;span class="pl-k"&gt;=&lt;/span&gt; []
    &lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;detection_boxes&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;])):
        score &lt;span class="pl-k"&gt;=&lt;/span&gt; output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;detection_scores&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;][i]
        classtype &lt;span class="pl-k"&gt;=&lt;/span&gt; output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;detection_classes&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;][i]
        &lt;span class="pl-k"&gt;if&lt;/span&gt; score &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;SCORE_THRESHOLD&lt;/span&gt; &lt;span class="pl-k"&gt;and&lt;/span&gt; classtype &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-c1"&gt;PERSON_CLASS&lt;/span&gt;:
            persons_coordinates.append(output_dict[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;detection_boxes&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;][i])

    w, h &lt;span class="pl-k"&gt;=&lt;/span&gt; img.size
    &lt;span class="pl-k"&gt;for&lt;/span&gt; person_coordinate &lt;span class="pl-k"&gt;in&lt;/span&gt; persons_coordinates:
        cropped_img &lt;span class="pl-k"&gt;=&lt;/span&gt; img.crop((
            &lt;span class="pl-c1"&gt;int&lt;/span&gt;(w &lt;span class="pl-k"&gt;*&lt;/span&gt; person_coordinate[&lt;span class="pl-c1"&gt;1&lt;/span&gt;]),
            &lt;span class="pl-c1"&gt;int&lt;/span&gt;(h &lt;span class="pl-k"&gt;*&lt;/span&gt; person_coordinate[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]),
            &lt;span class="pl-c1"&gt;int&lt;/span&gt;(w &lt;span class="pl-k"&gt;*&lt;/span&gt; person_coordinate[&lt;span class="pl-c1"&gt;3&lt;/span&gt;]),
            &lt;span class="pl-c1"&gt;int&lt;/span&gt;(h &lt;span class="pl-k"&gt;*&lt;/span&gt; person_coordinate[&lt;span class="pl-c1"&gt;2&lt;/span&gt;]),
        ))
        &lt;span class="pl-k"&gt;return&lt;/span&gt; cropped_img
    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;None&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-part-5-move-all-images-into-according-classified-folder" class="anchor" aria-hidden="true" href="#part-5-move-all-images-into-according-classified-folder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 5: Move all images into according classified folder&lt;/h3&gt;
&lt;p&gt;As a last step, we write a script that loops over all images in the "unclassified" folder,
checks whether they have an encoded label in the name copies the image in the according
"classified" folder with applying the previously developed preprocessing steps:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; os
&lt;span class="pl-k"&gt;import&lt;/span&gt; person_detector
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow &lt;span class="pl-k"&gt;as&lt;/span&gt; tf

&lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;./images/unclassified&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c1"&gt;POS_FOLDER&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;./images/classified/positive&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c1"&gt;NEG_FOLDER&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;./images/classified/negative&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;


&lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;__name__&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;__main__&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;:
    detection_graph &lt;span class="pl-k"&gt;=&lt;/span&gt; person_detector.open_graph()

    images &lt;span class="pl-k"&gt;=&lt;/span&gt; [f &lt;span class="pl-k"&gt;for&lt;/span&gt; f &lt;span class="pl-k"&gt;in&lt;/span&gt; os.listdir(&lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt;) &lt;span class="pl-k"&gt;if&lt;/span&gt; os.path.isfile(os.path.join(&lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt;, f))]
    positive_images &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;filter&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;image&lt;/span&gt;: (image.startswith(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;1_&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)), images)
    negative_images &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;filter&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;image&lt;/span&gt;: (image.startswith(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;0_&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)), images)

    &lt;span class="pl-k"&gt;with&lt;/span&gt; detection_graph.as_default():
        &lt;span class="pl-k"&gt;with&lt;/span&gt; tf.Session() &lt;span class="pl-k"&gt;as&lt;/span&gt; sess:

            &lt;span class="pl-k"&gt;for&lt;/span&gt; pos &lt;span class="pl-k"&gt;in&lt;/span&gt; positive_images:
                old_filename &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; pos
                new_filename &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;POS_FOLDER&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; pos[:&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;5&lt;/span&gt;] &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;.jpg&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
                &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; os.path.isfile(new_filename):
                    img &lt;span class="pl-k"&gt;=&lt;/span&gt; person_detector.get_person(old_filename, sess)
                    &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; img:
                        &lt;span class="pl-k"&gt;continue&lt;/span&gt;
                    img &lt;span class="pl-k"&gt;=&lt;/span&gt; img.convert(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
                    img.save(new_filename, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;jpeg&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

            &lt;span class="pl-k"&gt;for&lt;/span&gt; neg &lt;span class="pl-k"&gt;in&lt;/span&gt; negative_images:
                old_filename &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;IMAGE_FOLDER&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; neg
                new_filename &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;NEG_FOLDER&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; neg[:&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;5&lt;/span&gt;] &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;.jpg&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
                &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; os.path.isfile(new_filename):
                    img &lt;span class="pl-k"&gt;=&lt;/span&gt; person_detector.get_person(old_filename, sess)
                    &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; img:
                        &lt;span class="pl-k"&gt;continue&lt;/span&gt;
                    img &lt;span class="pl-k"&gt;=&lt;/span&gt; img.convert(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
                    img.save(new_filename, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;jpeg&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Whenver we run this script, all labeled images are being processed and moved into corresponding
subfolders in the "classified" directory.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-step-6-retrain-inceptionv3-and-write-a-classifier" class="anchor" aria-hidden="true" href="#step-6-retrain-inceptionv3-and-write-a-classifier"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 6: Retrain inceptionv3 and write a classifier&lt;/h2&gt;
&lt;p&gt;For the retraining part, we'll just use tensorflows &lt;a href="https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py"&gt;retrain.py&lt;/a&gt;
script with the inceptionv3 model.&lt;/p&gt;
&lt;p&gt;Call the script in your project root directory with the following parameters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python retrain.py --bottleneck_dir=tf/training_data/bottlenecks --model_dir=tf/training_data/inception --summaries_dir=tf/training_data/summaries/basic --output_graph=tf/training_output/retrained_graph.pb --output_labels=tf/training_output/retrained_labels.txt --image_dir=./images/classified --how_many_training_steps=50000 --testing_percentage=20 --learning_rate=0.001&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The learning takes roughly 15 minutes on a GTX 1080 ti, with a final accuracy of about 80% for my
labeled dataset, but this heavily depends on the quality of your input data and your labeling.&lt;/p&gt;
&lt;p&gt;The result of the training process is a retrained inceptionV3 model in the "tf/training_output/retrained_graph.pb"
file. We must now write a Classifier class that efficiently uses the new weights in the tensorflow
graph to make a classification prediction.&lt;/p&gt;
&lt;p&gt;Let's write a Classifier-Class that opens the graph as a session and offers a "classify" method
with an image file that returns a dict with certainty values matching our labels "positive" and "negative".&lt;/p&gt;
&lt;p&gt;The class takes as input both the path to the graph as well as the path to the label file, both
sitting in our "tf/training_output/" folder. We develop helper functions for converting
an image file to a tensor that we can feed into our graph, a helper function for loading the graph and
labels and an important little function to close our graph after we are done using it.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow &lt;span class="pl-k"&gt;as&lt;/span&gt; tf

&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Classifier&lt;/span&gt;():
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;graph&lt;/span&gt;, &lt;span class="pl-smi"&gt;labels&lt;/span&gt;):

        &lt;span class="pl-c1"&gt;self&lt;/span&gt;._graph &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.load_graph(graph)
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;._labels &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.load_labels(labels)

        &lt;span class="pl-c1"&gt;self&lt;/span&gt;._input_operation &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._graph.get_operation_by_name(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;import/Placeholder&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;._output_operation &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._graph.get_operation_by_name(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;import/final_result&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

        &lt;span class="pl-c1"&gt;self&lt;/span&gt;._session &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.Session(&lt;span class="pl-v"&gt;graph&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;self&lt;/span&gt;._graph)

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;classify&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;file_name&lt;/span&gt;):
        t &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.read_tensor_from_image_file(file_name)

        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Open up a new tensorflow session and run it on the input&lt;/span&gt;
        results &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._session.run(&lt;span class="pl-c1"&gt;self&lt;/span&gt;._output_operation.outputs[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], {&lt;span class="pl-c1"&gt;self&lt;/span&gt;._input_operation.outputs[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]: t})
        results &lt;span class="pl-k"&gt;=&lt;/span&gt; np.squeeze(results)

        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Sort the output predictions by prediction accuracy&lt;/span&gt;
        top_k &lt;span class="pl-k"&gt;=&lt;/span&gt; results.argsort()[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;5&lt;/span&gt;:][::&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;]

        result &lt;span class="pl-k"&gt;=&lt;/span&gt; {}
        &lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; top_k:
            result[&lt;span class="pl-c1"&gt;self&lt;/span&gt;._labels[i]] &lt;span class="pl-k"&gt;=&lt;/span&gt; results[i]

        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Return sorted result tuples&lt;/span&gt;
        &lt;span class="pl-k"&gt;return&lt;/span&gt; result

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;close&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;._session.close()


    &lt;span class="pl-en"&gt;@&lt;/span&gt;&lt;span class="pl-c1"&gt;staticmethod&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;load_graph&lt;/span&gt;(&lt;span class="pl-smi"&gt;model_file&lt;/span&gt;):
        graph &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.Graph()
        graph_def &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.GraphDef()
        &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(model_file, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;rb&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
            graph_def.ParseFromString(f.read())
        &lt;span class="pl-k"&gt;with&lt;/span&gt; graph.as_default():
            tf.import_graph_def(graph_def)
        &lt;span class="pl-k"&gt;return&lt;/span&gt; graph

    &lt;span class="pl-en"&gt;@&lt;/span&gt;&lt;span class="pl-c1"&gt;staticmethod&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;load_labels&lt;/span&gt;(&lt;span class="pl-smi"&gt;label_file&lt;/span&gt;):
        label &lt;span class="pl-k"&gt;=&lt;/span&gt; []
        proto_as_ascii_lines &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.gfile.GFile(label_file).readlines()
        &lt;span class="pl-k"&gt;for&lt;/span&gt; l &lt;span class="pl-k"&gt;in&lt;/span&gt; proto_as_ascii_lines:
            label.append(l.rstrip())
        &lt;span class="pl-k"&gt;return&lt;/span&gt; label

    &lt;span class="pl-en"&gt;@&lt;/span&gt;&lt;span class="pl-c1"&gt;staticmethod&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;read_tensor_from_image_file&lt;/span&gt;(&lt;span class="pl-smi"&gt;file_name&lt;/span&gt;,
                                    &lt;span class="pl-smi"&gt;input_height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;299&lt;/span&gt;,
                                    &lt;span class="pl-smi"&gt;input_width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;299&lt;/span&gt;,
                                    &lt;span class="pl-smi"&gt;input_mean&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;,
                                    &lt;span class="pl-smi"&gt;input_std&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;255&lt;/span&gt;):
        input_name &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;file_reader&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
        file_reader &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.read_file(file_name, input_name)
        image_reader &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.image.decode_jpeg(
            file_reader, &lt;span class="pl-v"&gt;channels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-v"&gt;name&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;jpeg_reader&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
        float_caster &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.cast(image_reader, tf.float32)
        dims_expander &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.expand_dims(float_caster, &lt;span class="pl-c1"&gt;0&lt;/span&gt;)
        resized &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.image.resize_bilinear(dims_expander, [input_height, input_width])
        normalized &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.divide(tf.subtract(resized, [input_mean]), [input_std])
        sess &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.Session()
        result &lt;span class="pl-k"&gt;=&lt;/span&gt; sess.run(normalized)
        &lt;span class="pl-k"&gt;return&lt;/span&gt; result&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-step-7-use-all-this-to-actually-auto-play-tinder" class="anchor" aria-hidden="true" href="#step-7-use-all-this-to-actually-auto-play-tinder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 7: Use all this to actually auto-play tinder&lt;/h2&gt;
&lt;p&gt;Now that we have our classifier in place, let's extend the "Person" class from
earlier and extend it with a "predict_likeliness" function that uses a classifier
instance to verify whether a given person should be liked or not.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; In the Person class&lt;/span&gt;

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;predict_likeliness&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;classifier&lt;/span&gt;, &lt;span class="pl-smi"&gt;sess&lt;/span&gt;):
        ratings &lt;span class="pl-k"&gt;=&lt;/span&gt; []
        &lt;span class="pl-k"&gt;for&lt;/span&gt; image &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.images:
            req &lt;span class="pl-k"&gt;=&lt;/span&gt; requests.get(image, &lt;span class="pl-v"&gt;stream&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
            tmp_filename &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;./images/tmp/run.jpg&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;
            &lt;span class="pl-k"&gt;if&lt;/span&gt; req.status_code &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-c1"&gt;200&lt;/span&gt;:
                &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(tmp_filename, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;wb&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
                    f.write(req.content)
            img &lt;span class="pl-k"&gt;=&lt;/span&gt; person_detector.get_person(tmp_filename, sess)
            &lt;span class="pl-k"&gt;if&lt;/span&gt; img:
                img &lt;span class="pl-k"&gt;=&lt;/span&gt; img.convert(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
                img.save(tmp_filename, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;jpeg&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
                certainty &lt;span class="pl-k"&gt;=&lt;/span&gt; classifier.classify(tmp_filename)
                pos &lt;span class="pl-k"&gt;=&lt;/span&gt; certainty[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;positive&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
                ratings.append(pos)
        ratings.sort(&lt;span class="pl-v"&gt;reverse&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
        ratings &lt;span class="pl-k"&gt;=&lt;/span&gt; ratings[:&lt;span class="pl-c1"&gt;5&lt;/span&gt;]
        &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(ratings) &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;:
            &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;0.001&lt;/span&gt;
        &lt;span class="pl-k"&gt;return&lt;/span&gt; ratings[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-c1"&gt;0.6&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-c1"&gt;sum&lt;/span&gt;(ratings[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:])&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;len&lt;/span&gt;(ratings[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:])&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-c1"&gt;0.4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we have to bring all the puzzle pieces together.&lt;/p&gt;
&lt;p&gt;First, let's initialize the tinder API with our api token. Then, we open up
our classification tensorflow graph as a tensorflow session using our
retrained graph and labels. Then, we fetch persons nearby and make a
likeliness prediction.&lt;/p&gt;
&lt;p&gt;As a little bonus, I added a likeliness-multiplier of 1.2 if the person
on Tinder goes to the same university as I do, so that I am more likely
to match with local students.&lt;/p&gt;
&lt;p&gt;For all people that have a predicted likeliness score of 0.8, I call a like,
for all the other a dislike.&lt;/p&gt;
&lt;p&gt;I developed the script to auto-play for the next 2 hours after it is started.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; likeliness_classifier &lt;span class="pl-k"&gt;import&lt;/span&gt; Classifier
&lt;span class="pl-k"&gt;import&lt;/span&gt; person_detector
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow &lt;span class="pl-k"&gt;as&lt;/span&gt; tf
&lt;span class="pl-k"&gt;from&lt;/span&gt; time &lt;span class="pl-k"&gt;import&lt;/span&gt; time

&lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;__name__&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;__main__&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;:
    token &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;YOUR-API-TOKEN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
    api &lt;span class="pl-k"&gt;=&lt;/span&gt; tinderAPI(token)

    detection_graph &lt;span class="pl-k"&gt;=&lt;/span&gt; person_detector.open_graph()
    &lt;span class="pl-k"&gt;with&lt;/span&gt; detection_graph.as_default():
        &lt;span class="pl-k"&gt;with&lt;/span&gt; tf.Session() &lt;span class="pl-k"&gt;as&lt;/span&gt; sess:

            classifier &lt;span class="pl-k"&gt;=&lt;/span&gt; Classifier(&lt;span class="pl-v"&gt;graph&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;./tf/training_output/retrained_graph.pb&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
                                    &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;./tf/training_output/retrained_labels.txt&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

            end_time &lt;span class="pl-k"&gt;=&lt;/span&gt; time() &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-c1"&gt;60&lt;/span&gt;&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-c1"&gt;60&lt;/span&gt;&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;
            &lt;span class="pl-k"&gt;while&lt;/span&gt; time() &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; end_time:
                &lt;span class="pl-k"&gt;try&lt;/span&gt;:
                    persons &lt;span class="pl-k"&gt;=&lt;/span&gt; api.nearby_persons()
                    pos_schools &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Universität Zürich&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;University of Zurich&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;UZH&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]

                    &lt;span class="pl-k"&gt;for&lt;/span&gt; person &lt;span class="pl-k"&gt;in&lt;/span&gt; persons:
                        score &lt;span class="pl-k"&gt;=&lt;/span&gt; person.predict_likeliness(classifier, sess)

                        &lt;span class="pl-k"&gt;for&lt;/span&gt; school &lt;span class="pl-k"&gt;in&lt;/span&gt; pos_schools:
                            &lt;span class="pl-k"&gt;if&lt;/span&gt; school &lt;span class="pl-k"&gt;in&lt;/span&gt; person.schools:
                                &lt;span class="pl-c1"&gt;print&lt;/span&gt;()
                                score &lt;span class="pl-k"&gt;*=&lt;/span&gt; &lt;span class="pl-c1"&gt;1.2&lt;/span&gt;

                        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;-------------------------&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
                        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;ID: &lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, person.id)
                        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Name: &lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, person.name)
                        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Schools: &lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, person.schools)
                        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Images: &lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, person.images)
                        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(score)

                        &lt;span class="pl-k"&gt;if&lt;/span&gt; score &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.8&lt;/span&gt;:
                            res &lt;span class="pl-k"&gt;=&lt;/span&gt; person.like()
                            &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;LIKE&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
                        &lt;span class="pl-k"&gt;else&lt;/span&gt;:
                            res &lt;span class="pl-k"&gt;=&lt;/span&gt; person.dislike()
                            &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;DISLIKE&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
                &lt;span class="pl-k"&gt;except&lt;/span&gt; &lt;span class="pl-c1"&gt;Exception&lt;/span&gt;:
                    &lt;span class="pl-k"&gt;pass&lt;/span&gt;

    classifier.close()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's it! We can now let our script run for as long as we like
and play tinder without abusing our thumb!&lt;/p&gt;
&lt;p&gt;If you have questions or found bugs, feel free to contribute
to my &lt;a href="https://github.com/joelbarmettlerUZH/auto-tinder"&gt;Github Repository&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;MIT License&lt;/p&gt;
&lt;p&gt;Copyright (c) 2018 Joel Barmettler&lt;/p&gt;
&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:&lt;/p&gt;
&lt;p&gt;The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.&lt;/p&gt;
&lt;p&gt;THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>joelbarmettlerUZH</author><guid isPermaLink="false">https://github.com/joelbarmettlerUZH/auto-tinder</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>NVIDIA/vid2vid #15 in Python, Today</title><link>https://github.com/NVIDIA/vid2vid</link><description>&lt;p&gt;&lt;i&gt;Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic video-to-video translation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="imgs/teaser.gif"&gt;&lt;img src="imgs/teaser.gif" align="right" width="360" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-vid2vid" class="anchor" aria-hidden="true" href="#vid2vid"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;vid2vid&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-project--youtubeshort--youtubefull--arxiv--paperfull" class="anchor" aria-hidden="true" href="#project--youtubeshort--youtubefull--arxiv--paperfull"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://tcwang0509.github.io/vid2vid/" rel="nofollow"&gt;Project&lt;/a&gt; | &lt;a href="https://youtu.be/5zlcXTCpQqM" rel="nofollow"&gt;YouTube(short)&lt;/a&gt; | &lt;a href="https://youtu.be/GrP_aOSXt5U" rel="nofollow"&gt;YouTube(full)&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/1808.06601" rel="nofollow"&gt;arXiv&lt;/a&gt; | &lt;a href="https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf" rel="nofollow"&gt;Paper(full)&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Pytorch implementation for high-resolution (e.g., 2048x1024) photorealistic video-to-video translation. It can be used for turning semantic label maps into photo-realistic videos, synthesizing people talking from edge maps, or generating human motions from poses. The core of video-to-video translation is image-to-image translation. Some of our work in that space can be found in &lt;a href="https://github.com/NVIDIA/pix2pixHD"&gt;pix2pixHD&lt;/a&gt; and &lt;a href="https://github.com/NVlabs/SPADE"&gt;SPADE&lt;/a&gt;. &lt;br&gt;&lt;br&gt;
&lt;a href="https://tcwang0509.github.io/vid2vid/" rel="nofollow"&gt;Video-to-Video Synthesis&lt;/a&gt;&lt;br&gt;
&lt;a href="https://tcwang0509.github.io/" rel="nofollow"&gt;Ting-Chun Wang&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href="http://mingyuliu.net/" rel="nofollow"&gt;Ming-Yu Liu&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href="http://people.csail.mit.edu/junyanz/" rel="nofollow"&gt;Jun-Yan Zhu&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href="https://liuguilin1225.github.io/" rel="nofollow"&gt;Guilin Liu&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, Andrew Tao&lt;sup&gt;1&lt;/sup&gt;, &lt;a href="http://jankautz.com/" rel="nofollow"&gt;Jan Kautz&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href="http://catanzaro.name/" rel="nofollow"&gt;Bryan Catanzaro&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;br&gt;
&lt;sup&gt;1&lt;/sup&gt;NVIDIA Corporation, &lt;sup&gt;2&lt;/sup&gt;MIT CSAIL&lt;br&gt;
In Neural Information Processing Systems (&lt;strong&gt;NeurIPS&lt;/strong&gt;) 2018&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-video-to-video-translation" class="anchor" aria-hidden="true" href="#video-to-video-translation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video-to-Video Translation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Label-to-Streetview Results&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;  
  &lt;a target="_blank" rel="noopener noreferrer" href="imgs/city_change_styles.gif"&gt;&lt;img src="imgs/city_change_styles.gif" width="440" style="max-width:100%;"&gt;&lt;/a&gt;  
  &lt;a target="_blank" rel="noopener noreferrer" href="imgs/city_change_labels.gif"&gt;&lt;img src="imgs/city_change_labels.gif" width="440" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Edge-to-Face Results&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="imgs/face.gif"&gt;&lt;img src="imgs/face.gif" width="440" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="imgs/face_multiple.gif"&gt;&lt;img src="imgs/face_multiple.gif" width="440" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pose-to-Body Results&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="imgs/pose.gif"&gt;&lt;img src="imgs/pose.gif" width="550" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Frame Prediction Results&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="imgs/framePredict.gif"&gt;&lt;img src="imgs/framePredict.gif" width="550" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Linux or macOS&lt;/li&gt;
&lt;li&gt;Python 3&lt;/li&gt;
&lt;li&gt;NVIDIA GPU + CUDA cuDNN&lt;/li&gt;
&lt;li&gt;PyTorch 0.4&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Install python libraries &lt;a href="https://github.com/Knio/dominate"&gt;dominate&lt;/a&gt; and requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install dominate requests&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;If you plan to train with face datasets, please install dlib.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install dlib&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;If you plan to train with pose datasets, please install &lt;a href="https://github.com/facebookresearch/DensePose"&gt;DensePose&lt;/a&gt; and/or &lt;a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose"&gt;OpenPose&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Clone this repo:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/NVIDIA/vid2vid
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; vid2vid&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Docker Image
If you have difficulty building the repo, a docker image can be found in the &lt;code&gt;docker&lt;/code&gt; folder.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Please first download example dataset by running &lt;code&gt;python scripts/download_datasets.py&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, compile a snapshot of &lt;a href="https://github.com/NVIDIA/flownet2-pytorch"&gt;FlowNet2&lt;/a&gt; by running &lt;code&gt;python scripts/download_flownet2.py&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cityscapes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Please download the pre-trained Cityscapes model by:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python scripts/street/download_models.py&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To test the model (&lt;code&gt;bash ./scripts/street/test_2048.sh&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#!&lt;/span&gt;./scripts/street/test_2048.sh&lt;/span&gt;
python test.py --name label2city_2048 --label_nc 35 --loadSize 2048 --n_scales_spatial 3 --use_instance --fg --use_single_G&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The test results will be saved in: &lt;code&gt;./results/label2city_2048/test_latest/&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We also provide a smaller model trained with single GPU, which produces slightly worse performance at 1024 x 512 resolution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Please download the model by&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python scripts/street/download_models_g1.py&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;To test the model (&lt;code&gt;bash ./scripts/street/test_g1_1024.sh&lt;/code&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#!&lt;/span&gt;./scripts/street/test_g1_1024.sh&lt;/span&gt;
python test.py --name label2city_1024_g1 --label_nc 35 --loadSize 1024 --n_scales_spatial 3 --use_instance --fg --n_downsample_G 2 --use_single_G&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can find more example scripts in the &lt;code&gt;scripts/street/&lt;/code&gt; directory.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Faces&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Please download the pre-trained model by:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python scripts/face/download_models.py&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;To test the model (&lt;code&gt;bash ./scripts/face/test_512.sh&lt;/code&gt;):
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#!&lt;/span&gt;./scripts/face/test_512.sh&lt;/span&gt;
python test.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --use_single_G&lt;/pre&gt;&lt;/div&gt;
The test results will be saved in: &lt;code&gt;./results/edge2face_512/test_latest/&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-dataset" class="anchor" aria-hidden="true" href="#dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cityscapes
&lt;ul&gt;
&lt;li&gt;We use the Cityscapes dataset as an example. To train a model on the full dataset, please download it from the &lt;a href="https://www.cityscapes-dataset.com/" rel="nofollow"&gt;official website&lt;/a&gt; (registration required).&lt;/li&gt;
&lt;li&gt;We apply a pre-trained segmentation algorithm to get the corresponding semantic maps (train_A) and instance maps (train_inst).&lt;/li&gt;
&lt;li&gt;Please add the obtained images to the &lt;code&gt;datasets&lt;/code&gt; folder in the same way the example images are provided.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Face
&lt;ul&gt;
&lt;li&gt;We use the &lt;a href="http://niessnerlab.org/projects/roessler2018faceforensics.html" rel="nofollow"&gt;FaceForensics&lt;/a&gt; dataset. We then use landmark detection to estimate the face keypoints, and interpolate them to get face edges.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pose
&lt;ul&gt;
&lt;li&gt;We use random dancing videos found on YouTube. We then apply DensePose / OpenPose to estimate the poses for each frame.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-training-with-cityscapes-dataset" class="anchor" aria-hidden="true" href="#training-with-cityscapes-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training with Cityscapes dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;First, download the FlowNet2 checkpoint file by running &lt;code&gt;python scripts/download_models_flownet2.py&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Training with 8 GPUs:
&lt;ul&gt;
&lt;li&gt;We adopt a coarse-to-fine approach, sequentially increasing the resolution from 512 x 256, 1024 x 512, to 2048 x 1024.&lt;/li&gt;
&lt;li&gt;Train a model at 512 x 256 resolution (&lt;code&gt;bash ./scripts/street/train_512.sh&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#!&lt;/span&gt;./scripts/street/train_512.sh&lt;/span&gt;
python train.py --name label2city_512 --label_nc 35 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 6 --use_instance --fg&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Train a model at 1024 x 512 resolution (must train 512 x 256 first) (&lt;code&gt;bash ./scripts/street/train_1024.sh&lt;/code&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#!&lt;/span&gt;./scripts/street/train_1024.sh&lt;/span&gt;
python train.py --name label2city_1024 --label_nc 35 --loadSize 1024 --n_scales_spatial 2 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 4 --use_instance --fg --niter_step 2 --niter_fix_global 10 --load_pretrain checkpoints/label2city_512&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you have TensorFlow installed, you can see TensorBoard logs in &lt;code&gt;./checkpoints/label2city_1024/logs&lt;/code&gt; by adding &lt;code&gt;--tf_log&lt;/code&gt; to the training scripts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Training with a single GPU:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We trained our models using multiple GPUs. For convenience, we provide some sample training scripts (train_g1_XXX.sh) for single GPU users, up to 1024 x 512 resolution. Again a coarse-to-fine approach is adopted (256 x 128, 512 x 256, 1024 x 512). Performance is not guaranteed using these scripts.&lt;/li&gt;
&lt;li&gt;For example, to train a 256 x 128 video with a single GPU (&lt;code&gt;bash ./scripts/street/train_g1_256.sh&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#!&lt;/span&gt;./scripts/street/train_g1_256.sh&lt;/span&gt;
python train.py --name label2city_256_g1 --label_nc 35 --loadSize 256 --use_instance --fg --n_downsample_G 2 --num_D 1 --max_frames_per_gpu 6 --n_frames_total 6&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training at full (2k x 1k) resolution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To train the images at full resolution (2048 x 1024) requires 8 GPUs with at least 24G memory (&lt;code&gt;bash ./scripts/street/train_2048.sh&lt;/code&gt;). If only GPUs with 12G/16G memory are available, please use the script &lt;code&gt;./scripts/street/train_2048_crop.sh&lt;/code&gt;, which will crop the images during training. Performance is not guaranteed with this script.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-training-with-face-datasets" class="anchor" aria-hidden="true" href="#training-with-face-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training with face datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If you haven't, please first download example dataset by running &lt;code&gt;python scripts/download_datasets.py&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Run the following command to compute face landmarks for training dataset:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python data/face_landmark_detection.py train&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Run the example script (&lt;code&gt;bash ./scripts/face/train_512.sh&lt;/code&gt;)
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12  &lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;For single GPU users, example scripts are in train_g1_XXX.sh. These scripts are not fully tested and please use at your own discretion. If you still hit out of memory errors, try reducing &lt;code&gt;max_frames_per_gpu&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;More examples scripts can be found in &lt;code&gt;scripts/face/&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Please refer to &lt;a href="https://github.com/NVIDIA/vid2vid#more-trainingtest-details"&gt;More Training/Test Details&lt;/a&gt; for more explanations about training flags.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-training-with-pose-datasets" class="anchor" aria-hidden="true" href="#training-with-pose-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training with pose datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If you haven't, please first download example dataset by running &lt;code&gt;python scripts/download_datasets.py&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Example DensePose and OpenPose results are included. If you plan to use your own dataset, please generate these results and put them in the same way the example dataset is provided.&lt;/li&gt;
&lt;li&gt;Run the example script (&lt;code&gt;bash ./scripts/pose/train_256p.sh&lt;/code&gt;)
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --name pose2body_256p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 384 --fineSize 256 --gpu_ids 0,1,2,3,4,5,6,7 --batchSize 8 --max_frames_per_gpu 3 --no_first_img --n_frames_total 12 --max_t_step 4&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Again, for single GPU users, example scripts are in train_g1_XXX.sh. These scripts are not fully tested and please use at your own discretion. If you still hit out of memory errors, try reducing &lt;code&gt;max_frames_per_gpu&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;More examples scripts can be found in &lt;code&gt;scripts/pose/&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Please refer to &lt;a href="https://github.com/NVIDIA/vid2vid#more-trainingtest-details"&gt;More Training/Test Details&lt;/a&gt; for more explanations about training flags.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-training-with-your-own-dataset" class="anchor" aria-hidden="true" href="#training-with-your-own-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training with your own dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If your input is a label map, please generate label maps which are one-channel whose pixel values correspond to the object labels (i.e. 0,1,...,N-1, where N is the number of labels). This is because we need to generate one-hot vectors from the label maps. Please use &lt;code&gt;--label_nc N&lt;/code&gt; during both training and testing.&lt;/li&gt;
&lt;li&gt;If your input is not a label map, please specify &lt;code&gt;--input_nc N&lt;/code&gt; where N is the number of input channels (The default is 3 for RGB images).&lt;/li&gt;
&lt;li&gt;The default setting for preprocessing is &lt;code&gt;scaleWidth&lt;/code&gt;, which will scale the width of all training images to &lt;code&gt;opt.loadSize&lt;/code&gt; (1024) while keeping the aspect ratio. If you want a different setting, please change it by using the &lt;code&gt;--resize_or_crop&lt;/code&gt; option. For example, &lt;code&gt;scaleWidth_and_crop&lt;/code&gt; first resizes the image to have width &lt;code&gt;opt.loadSize&lt;/code&gt; and then does random cropping of size &lt;code&gt;(opt.fineSize, opt.fineSize)&lt;/code&gt;. &lt;code&gt;crop&lt;/code&gt; skips the resizing step and only performs random cropping. &lt;code&gt;scaledCrop&lt;/code&gt; crops the image while retraining the original aspect ratio. &lt;code&gt;randomScaleHeight&lt;/code&gt; will randomly scale the image height to be between &lt;code&gt;opt.loadSize&lt;/code&gt; and &lt;code&gt;opt.fineSize&lt;/code&gt;. If you don't want any preprocessing, please specify &lt;code&gt;none&lt;/code&gt;, which will do nothing other than making sure the image is divisible by 32.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-more-trainingtest-details" class="anchor" aria-hidden="true" href="#more-trainingtest-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More Training/Test Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We generate frames in the video sequentially, where the generation of the current frame depends on previous frames. To generate the first frame for the model, there are 3 different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Using another generator which was trained on generating single images (e.g., pix2pixHD) by specifying &lt;code&gt;--use_single_G&lt;/code&gt;. This is the option we use in the test scripts.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Using the first frame in the real sequence by specifying &lt;code&gt;--use_real_img&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Forcing the model to also synthesize the first frame by specifying &lt;code&gt;--no_first_img&lt;/code&gt;. This must be trained separately before inference.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The way we train the model is as follows: suppose we have 8 GPUs, 4 for generators and 4 for discriminators, and we want to train 28 frames. Also, assume each GPU can generate only one frame. The first GPU generates the first frame, and pass it to the next GPU, and so on. After the 4 frames are generated, they are passed to the 4 discriminator GPUs to compute the losses. Then the last generated frame becomes input to the next batch, and the next 4 frames in the training sequence are loaded into GPUs. This is repeated 7 times (4 x 7 = 28), to train all the 28 frames.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some important flags:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;n_gpus_gen&lt;/code&gt;: the number of GPUs to use for generators (while the others are used for discriminators). We separate generators and discriminators into different GPUs since when dealing with high resolutions, even one frame cannot fit in a GPU. If the number is set to &lt;code&gt;-1&lt;/code&gt;, there is no separation and all GPUs are used for both generators and discriminators (only works for low-res images).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n_frames_G&lt;/code&gt;: the number of input frames to feed into the generator network; i.e., &lt;code&gt;n_frames_G - 1&lt;/code&gt; is the number of frames we look into the past. the default is 3 (conditioned on previous two frames).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n_frames_D&lt;/code&gt;: the number of frames to feed into the temporal discriminator. The default is 3.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n_scales_spatial&lt;/code&gt;: the number of scales in the spatial domain. We train from the coarsest scale and all the way to the finest scale. The default is 3.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n_scales_temporal&lt;/code&gt;: the number of scales for the temporal discriminator. The finest scale takes in the sequence in the original frame rate. The coarser scales subsample the frames by a factor of &lt;code&gt;n_frames_D&lt;/code&gt; before feeding the frames into the discriminator. For example, if &lt;code&gt;n_frames_D = 3&lt;/code&gt; and &lt;code&gt;n_scales_temporal = 3&lt;/code&gt;, the discriminator effectively sees 27 frames. The default is 3.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_frames_per_gpu&lt;/code&gt;: the number of frames in one GPU during training. If you run into out of memory error, please first try to reduce this number. If your GPU memory can fit more frames, try to make this number bigger to make training faster. The default is 1.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_frames_backpropagate&lt;/code&gt;: the number of frames that loss backpropagates to previous frames. For example, if this number is 4, the loss on frame n will backpropagate to frame n-3. Increasing this number will slightly improve the performance, but also cause training to be less stable. The default is 1.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n_frames_total&lt;/code&gt;: the total number of frames in a sequence we want to train with. We gradually increase this number during training.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;niter_step&lt;/code&gt;: for how many epochs do we double &lt;code&gt;n_frames_total&lt;/code&gt;. The default is 5.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;niter_fix_global&lt;/code&gt;: if this number if not 0, only train the finest spatial scale for this number of epochs before starting to fine-tune all scales.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;batchSize&lt;/code&gt;: the number of sequences to train at a time. We normally set batchSize to 1 since often, one sequence is enough to occupy all GPUs. If you want to do batchSize &amp;gt; 1, currently only &lt;code&gt;batchSize == n_gpus_gen&lt;/code&gt; is supported.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;no_first_img&lt;/code&gt;: if not specified, the model will assume the first frame is given and synthesize the successive frames. If specified, the model will also try to synthesize the first frame instead.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fg&lt;/code&gt;: if specified, use the foreground-background separation model as stated in the paper. The foreground labels must be specified by &lt;code&gt;--fg_labels&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;no_flow&lt;/code&gt;: if specified, do not use flow warping and directly synthesize frames. We found this usually still works reasonably well when the background is static, while saving memory and training time.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sparse_D&lt;/code&gt;: if specified, only apply temporal discriminator on sparse frames in the sequence. This helps save memory while having little effect on performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For other flags, please see &lt;code&gt;options/train_options.py&lt;/code&gt; and &lt;code&gt;options/base_options.py&lt;/code&gt; for all the training flags; see &lt;code&gt;options/test_options.py&lt;/code&gt; and &lt;code&gt;options/base_options.py&lt;/code&gt; for all the test flags.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Additional flags for edge2face examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;no_canny_edge&lt;/code&gt;: do not use canny edges for background as input.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;no_dist_map&lt;/code&gt;: by default, we use distrance transform on the face edge map as input. This flag will make it directly use edge maps.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Additional flags for pose2body examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;densepose_only&lt;/code&gt;: use only densepose results as input. Please also remember to change &lt;code&gt;input_nc&lt;/code&gt; to be 3.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;openpose_only&lt;/code&gt;: use only openpose results as input. Please also remember to change &lt;code&gt;input_nc&lt;/code&gt; to be 3.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;add_face_disc&lt;/code&gt;: add an additional discriminator that only works on the face region.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;remove_face_labels&lt;/code&gt;: remove densepose results for face, and add noise to openpose face results, so the network can get more robust to different face shapes. This is important if you plan to do inference on half-body videos (if not, usually this flag is unnecessary).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;random_drop_prob&lt;/code&gt;: the probability to randomly drop each pose segment during training, so the network can get more robust to missing poses at inference time. Default is 0.05.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;basic_point_only&lt;/code&gt;: if specified, only use basic joint keypoints for OpenPose output, without using any hand or face keypoints.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you find this useful for your research, please cite the following paper.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{wang2018vid2vid,
   author    = {Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Guilin Liu
                and Andrew Tao and Jan Kautz and Bryan Catanzaro},
   title     = {Video-to-Video Synthesis},
   booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},   
   year      = {2018},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgments" class="anchor" aria-hidden="true" href="#acknowledgments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;We thank Karan Sapra, Fitsum Reda, and Matthieu Le for generating the segmentation maps for us. We also thank Lisa Rhee for allowing us to use her dance videos for training. We thank William S. Peebles for proofreading the paper.&lt;br&gt;
This code borrows heavily from &lt;a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"&gt;pytorch-CycleGAN-and-pix2pix&lt;/a&gt; and &lt;a href="https://github.com/NVIDIA/pix2pixHD"&gt;pix2pixHD&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA</author><guid isPermaLink="false">https://github.com/NVIDIA/vid2vid</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>ytdl-org/youtube-dl #16 in Python, Today</title><link>https://github.com/ytdl-org/youtube-dl</link><description>&lt;p&gt;&lt;i&gt;Command-line program to download videos from YouTube.com and other video sites&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://travis-ci.org/ytdl-org/youtube-dl" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/410a0334f73fc86f47605f8d7c7c581f0202e1be/68747470733a2f2f7472617669732d63692e6f72672f7974646c2d6f72672f796f75747562652d646c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/ytdl-org/youtube-dl.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;youtube-dl - download videos from youtube.com or other video platforms&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#installation"&gt;INSTALLATION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#description"&gt;DESCRIPTION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#options"&gt;OPTIONS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#configuration"&gt;CONFIGURATION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#output-template"&gt;OUTPUT TEMPLATE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#format-selection"&gt;FORMAT SELECTION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#video-selection"&gt;VIDEO SELECTION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#developer-instructions"&gt;DEVELOPER INSTRUCTIONS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#embedding-youtube-dl"&gt;EMBEDDING YOUTUBE-DL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bugs"&gt;BUGS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#copyright"&gt;COPYRIGHT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;INSTALLATION&lt;/h1&gt;
&lt;p&gt;To install it right away for all UNIX users (Linux, macOS, etc.), type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do not have curl, you can alternatively use a recent wget:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Windows users can &lt;a href="https://yt-dl.org/latest/youtube-dl.exe" rel="nofollow"&gt;download an .exe file&lt;/a&gt; and place it in any location on their &lt;a href="https://en.wikipedia.org/wiki/PATH_%28variable%29" rel="nofollow"&gt;PATH&lt;/a&gt; except for &lt;code&gt;%SYSTEMROOT%\System32&lt;/code&gt; (e.g. &lt;strong&gt;do not&lt;/strong&gt; put in &lt;code&gt;C:\Windows\System32&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;You can also use pip:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo -H pip install --upgrade youtube-dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command will update youtube-dl if you have already installed it. See the &lt;a href="https://pypi.python.org/pypi/youtube_dl" rel="nofollow"&gt;pypi page&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;macOS users can install youtube-dl with &lt;a href="https://brew.sh/" rel="nofollow"&gt;Homebrew&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew install youtube-dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or with &lt;a href="https://www.macports.org/" rel="nofollow"&gt;MacPorts&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo port install youtube-dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, refer to the &lt;a href="#developer-instructions"&gt;developer instructions&lt;/a&gt; for how to check out and work with the git repository. For further options, including PGP signatures, see the &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html" rel="nofollow"&gt;youtube-dl Download Page&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DESCRIPTION&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;youtube-dl&lt;/strong&gt; is a command-line program to download videos from YouTube.com and a few more sites. It requires the Python interpreter, version 2.6, 2.7, or 3.2+, and it is not platform specific. It should work on your Unix box, on Windows or on macOS. It is released to the public domain, which means you can modify it, redistribute it or use it however you like.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;youtube-dl [OPTIONS] URL [URL...]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-options" class="anchor" aria-hidden="true" href="#options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OPTIONS&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;-h, --help                       Print this help text and exit
--version                        Print program version and exit
-U, --update                     Update this program to latest version. Make
                                 sure that you have sufficient permissions
                                 (run with sudo if needed)
-i, --ignore-errors              Continue on download errors, for example to
                                 skip unavailable videos in a playlist
--abort-on-error                 Abort downloading of further videos (in the
                                 playlist or the command line) if an error
                                 occurs
--dump-user-agent                Display the current browser identification
--list-extractors                List all supported extractors
--extractor-descriptions         Output descriptions of all supported
                                 extractors
--force-generic-extractor        Force extraction to use the generic
                                 extractor
--default-search PREFIX          Use this prefix for unqualified URLs. For
                                 example "gvsearch2:" downloads two videos
                                 from google videos for youtube-dl "large
                                 apple". Use the value "auto" to let
                                 youtube-dl guess ("auto_warning" to emit a
                                 warning when guessing). "error" just throws
                                 an error. The default value "fixup_error"
                                 repairs broken URLs, but emits an error if
                                 this is not possible instead of searching.
--ignore-config                  Do not read configuration files. When given
                                 in the global configuration file
                                 /etc/youtube-dl.conf: Do not read the user
                                 configuration in ~/.config/youtube-
                                 dl/config (%APPDATA%/youtube-dl/config.txt
                                 on Windows)
--config-location PATH           Location of the configuration file; either
                                 the path to the config or its containing
                                 directory.
--flat-playlist                  Do not extract the videos of a playlist,
                                 only list them.
--mark-watched                   Mark videos watched (YouTube only)
--no-mark-watched                Do not mark videos watched (YouTube only)
--no-color                       Do not emit color codes in output
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-network-options" class="anchor" aria-hidden="true" href="#network-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--proxy URL                      Use the specified HTTP/HTTPS/SOCKS proxy.
                                 To enable SOCKS proxy, specify a proper
                                 scheme. For example
                                 socks5://127.0.0.1:1080/. Pass in an empty
                                 string (--proxy "") for direct connection
--socket-timeout SECONDS         Time to wait before giving up, in seconds
--source-address IP              Client-side IP address to bind to
-4, --force-ipv4                 Make all connections via IPv4
-6, --force-ipv6                 Make all connections via IPv6
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-geo-restriction" class="anchor" aria-hidden="true" href="#geo-restriction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Geo Restriction:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--geo-verification-proxy URL     Use this proxy to verify the IP address for
                                 some geo-restricted sites. The default
                                 proxy specified by --proxy (or none, if the
                                 option is not present) is used for the
                                 actual downloading.
--geo-bypass                     Bypass geographic restriction via faking
                                 X-Forwarded-For HTTP header
--no-geo-bypass                  Do not bypass geographic restriction via
                                 faking X-Forwarded-For HTTP header
--geo-bypass-country CODE        Force bypass geographic restriction with
                                 explicitly provided two-letter ISO 3166-2
                                 country code
--geo-bypass-ip-block IP_BLOCK   Force bypass geographic restriction with
                                 explicitly provided IP block in CIDR
                                 notation
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-video-selection" class="anchor" aria-hidden="true" href="#video-selection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video Selection:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--playlist-start NUMBER          Playlist video to start at (default is 1)
--playlist-end NUMBER            Playlist video to end at (default is last)
--playlist-items ITEM_SPEC       Playlist video items to download. Specify
                                 indices of the videos in the playlist
                                 separated by commas like: "--playlist-items
                                 1,2,5,8" if you want to download videos
                                 indexed 1, 2, 5, 8 in the playlist. You can
                                 specify range: "--playlist-items
                                 1-3,7,10-13", it will download the videos
                                 at index 1, 2, 3, 7, 10, 11, 12 and 13.
--match-title REGEX              Download only matching titles (regex or
                                 caseless sub-string)
--reject-title REGEX             Skip download for matching titles (regex or
                                 caseless sub-string)
--max-downloads NUMBER           Abort after downloading NUMBER files
--min-filesize SIZE              Do not download any videos smaller than
                                 SIZE (e.g. 50k or 44.6m)
--max-filesize SIZE              Do not download any videos larger than SIZE
                                 (e.g. 50k or 44.6m)
--date DATE                      Download only videos uploaded in this date
--datebefore DATE                Download only videos uploaded on or before
                                 this date (i.e. inclusive)
--dateafter DATE                 Download only videos uploaded on or after
                                 this date (i.e. inclusive)
--min-views COUNT                Do not download any videos with less than
                                 COUNT views
--max-views COUNT                Do not download any videos with more than
                                 COUNT views
--match-filter FILTER            Generic video filter. Specify any key (see
                                 the "OUTPUT TEMPLATE" for a list of
                                 available keys) to match if the key is
                                 present, !key to check if the key is not
                                 present, key &amp;gt; NUMBER (like "comment_count
                                 &amp;gt; 12", also works with &amp;gt;=, &amp;lt;, &amp;lt;=, !=, =) to
                                 compare against a number, key = 'LITERAL'
                                 (like "uploader = 'Mike Smith'", also works
                                 with !=) to match against a string literal
                                 and &amp;amp; to require multiple matches. Values
                                 which are not known are excluded unless you
                                 put a question mark (?) after the operator.
                                 For example, to only match videos that have
                                 been liked more than 100 times and disliked
                                 less than 50 times (or the dislike
                                 functionality is not available at the given
                                 service), but who also have a description,
                                 use --match-filter "like_count &amp;gt; 100 &amp;amp;
                                 dislike_count &amp;lt;? 50 &amp;amp; description" .
--no-playlist                    Download only the video, if the URL refers
                                 to a video and a playlist.
--yes-playlist                   Download the playlist, if the URL refers to
                                 a video and a playlist.
--age-limit YEARS                Download only videos suitable for the given
                                 age
--download-archive FILE          Download only videos not listed in the
                                 archive file. Record the IDs of all
                                 downloaded videos in it.
--include-ads                    Download advertisements as well
                                 (experimental)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-download-options" class="anchor" aria-hidden="true" href="#download-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;-r, --limit-rate RATE            Maximum download rate in bytes per second
                                 (e.g. 50K or 4.2M)
-R, --retries RETRIES            Number of retries (default is 10), or
                                 "infinite".
--fragment-retries RETRIES       Number of retries for a fragment (default
                                 is 10), or "infinite" (DASH, hlsnative and
                                 ISM)
--skip-unavailable-fragments     Skip unavailable fragments (DASH, hlsnative
                                 and ISM)
--abort-on-unavailable-fragment  Abort downloading when some fragment is not
                                 available
--keep-fragments                 Keep downloaded fragments on disk after
                                 downloading is finished; fragments are
                                 erased by default
--buffer-size SIZE               Size of download buffer (e.g. 1024 or 16K)
                                 (default is 1024)
--no-resize-buffer               Do not automatically adjust the buffer
                                 size. By default, the buffer size is
                                 automatically resized from an initial value
                                 of SIZE.
--http-chunk-size SIZE           Size of a chunk for chunk-based HTTP
                                 downloading (e.g. 10485760 or 10M) (default
                                 is disabled). May be useful for bypassing
                                 bandwidth throttling imposed by a webserver
                                 (experimental)
--playlist-reverse               Download playlist videos in reverse order
--playlist-random                Download playlist videos in random order
--xattr-set-filesize             Set file xattribute ytdl.filesize with
                                 expected file size
--hls-prefer-native              Use the native HLS downloader instead of
                                 ffmpeg
--hls-prefer-ffmpeg              Use ffmpeg instead of the native HLS
                                 downloader
--hls-use-mpegts                 Use the mpegts container for HLS videos,
                                 allowing to play the video while
                                 downloading (some players may not be able
                                 to play it)
--external-downloader COMMAND    Use the specified external downloader.
                                 Currently supports
                                 aria2c,avconv,axel,curl,ffmpeg,httpie,wget
--external-downloader-args ARGS  Give these arguments to the external
                                 downloader
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-filesystem-options" class="anchor" aria-hidden="true" href="#filesystem-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Filesystem Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;-a, --batch-file FILE            File containing URLs to download ('-' for
                                 stdin), one URL per line. Lines starting
                                 with '#', ';' or ']' are considered as
                                 comments and ignored.
--id                             Use only video ID in file name
-o, --output TEMPLATE            Output filename template, see the "OUTPUT
                                 TEMPLATE" for all the info
--autonumber-start NUMBER        Specify the start value for %(autonumber)s
                                 (default is 1)
--restrict-filenames             Restrict filenames to only ASCII
                                 characters, and avoid "&amp;amp;" and spaces in
                                 filenames
-w, --no-overwrites              Do not overwrite files
-c, --continue                   Force resume of partially downloaded files.
                                 By default, youtube-dl will resume
                                 downloads if possible.
--no-continue                    Do not resume partially downloaded files
                                 (restart from beginning)
--no-part                        Do not use .part files - write directly
                                 into output file
--no-mtime                       Do not use the Last-modified header to set
                                 the file modification time
--write-description              Write video description to a .description
                                 file
--write-info-json                Write video metadata to a .info.json file
--write-annotations              Write video annotations to a
                                 .annotations.xml file
--load-info-json FILE            JSON file containing the video information
                                 (created with the "--write-info-json"
                                 option)
--cookies FILE                   File to read cookies from and dump cookie
                                 jar in
--cache-dir DIR                  Location in the filesystem where youtube-dl
                                 can store some downloaded information
                                 permanently. By default
                                 $XDG_CACHE_HOME/youtube-dl or
                                 ~/.cache/youtube-dl . At the moment, only
                                 YouTube player files (for videos with
                                 obfuscated signatures) are cached, but that
                                 may change.
--no-cache-dir                   Disable filesystem caching
--rm-cache-dir                   Delete all filesystem cache files
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-thumbnail-images" class="anchor" aria-hidden="true" href="#thumbnail-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Thumbnail images:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--write-thumbnail                Write thumbnail image to disk
--write-all-thumbnails           Write all thumbnail image formats to disk
--list-thumbnails                Simulate and list all available thumbnail
                                 formats
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-verbosity--simulation-options" class="anchor" aria-hidden="true" href="#verbosity--simulation-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Verbosity / Simulation Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;-q, --quiet                      Activate quiet mode
--no-warnings                    Ignore warnings
-s, --simulate                   Do not download the video and do not write
                                 anything to disk
--skip-download                  Do not download the video
-g, --get-url                    Simulate, quiet but print URL
-e, --get-title                  Simulate, quiet but print title
--get-id                         Simulate, quiet but print id
--get-thumbnail                  Simulate, quiet but print thumbnail URL
--get-description                Simulate, quiet but print video description
--get-duration                   Simulate, quiet but print video length
--get-filename                   Simulate, quiet but print output filename
--get-format                     Simulate, quiet but print output format
-j, --dump-json                  Simulate, quiet but print JSON information.
                                 See the "OUTPUT TEMPLATE" for a description
                                 of available keys.
-J, --dump-single-json           Simulate, quiet but print JSON information
                                 for each command-line argument. If the URL
                                 refers to a playlist, dump the whole
                                 playlist information in a single line.
--print-json                     Be quiet and print the video information as
                                 JSON (video is still being downloaded).
--newline                        Output progress bar as new lines
--no-progress                    Do not print progress bar
--console-title                  Display progress in console titlebar
-v, --verbose                    Print various debugging information
--dump-pages                     Print downloaded pages encoded using base64
                                 to debug problems (very verbose)
--write-pages                    Write downloaded intermediary pages to
                                 files in the current directory to debug
                                 problems
--print-traffic                  Display sent and read HTTP traffic
-C, --call-home                  Contact the youtube-dl server for debugging
--no-call-home                   Do NOT contact the youtube-dl server for
                                 debugging
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-workarounds" class="anchor" aria-hidden="true" href="#workarounds"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Workarounds:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--encoding ENCODING              Force the specified encoding (experimental)
--no-check-certificate           Suppress HTTPS certificate validation
--prefer-insecure                Use an unencrypted connection to retrieve
                                 information about the video. (Currently
                                 supported only for YouTube)
--user-agent UA                  Specify a custom user agent
--referer URL                    Specify a custom referer, use if the video
                                 access is restricted to one domain
--add-header FIELD:VALUE         Specify a custom HTTP header and its value,
                                 separated by a colon ':'. You can use this
                                 option multiple times
--bidi-workaround                Work around terminals that lack
                                 bidirectional text support. Requires bidiv
                                 or fribidi executable in PATH
--sleep-interval SECONDS         Number of seconds to sleep before each
                                 download when used alone or a lower bound
                                 of a range for randomized sleep before each
                                 download (minimum possible number of
                                 seconds to sleep) when used along with
                                 --max-sleep-interval.
--max-sleep-interval SECONDS     Upper bound of a range for randomized sleep
                                 before each download (maximum possible
                                 number of seconds to sleep). Must only be
                                 used along with --min-sleep-interval.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-video-format-options" class="anchor" aria-hidden="true" href="#video-format-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video Format Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;-f, --format FORMAT              Video format code, see the "FORMAT
                                 SELECTION" for all the info
--all-formats                    Download all available video formats
--prefer-free-formats            Prefer free video formats unless a specific
                                 one is requested
-F, --list-formats               List all available formats of requested
                                 videos
--youtube-skip-dash-manifest     Do not download the DASH manifests and
                                 related data on YouTube videos
--merge-output-format FORMAT     If a merge is required (e.g.
                                 bestvideo+bestaudio), output to given
                                 container format. One of mkv, mp4, ogg,
                                 webm, flv. Ignored if no merge is required
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-subtitle-options" class="anchor" aria-hidden="true" href="#subtitle-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Subtitle Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--write-sub                      Write subtitle file
--write-auto-sub                 Write automatically generated subtitle file
                                 (YouTube only)
--all-subs                       Download all the available subtitles of the
                                 video
--list-subs                      List all available subtitles for the video
--sub-format FORMAT              Subtitle format, accepts formats
                                 preference, for example: "srt" or
                                 "ass/srt/best"
--sub-lang LANGS                 Languages of the subtitles to download
                                 (optional) separated by commas, use --list-
                                 subs for available language tags
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-authentication-options" class="anchor" aria-hidden="true" href="#authentication-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authentication Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;-u, --username USERNAME          Login with this account ID
-p, --password PASSWORD          Account password. If this option is left
                                 out, youtube-dl will ask interactively.
-2, --twofactor TWOFACTOR        Two-factor authentication code
-n, --netrc                      Use .netrc authentication data
--video-password PASSWORD        Video password (vimeo, smotri, youku)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-adobe-pass-options" class="anchor" aria-hidden="true" href="#adobe-pass-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adobe Pass Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--ap-mso MSO                     Adobe Pass multiple-system operator (TV
                                 provider) identifier, use --ap-list-mso for
                                 a list of available MSOs
--ap-username USERNAME           Multiple-system operator account login
--ap-password PASSWORD           Multiple-system operator account password.
                                 If this option is left out, youtube-dl will
                                 ask interactively.
--ap-list-mso                    List all supported multiple-system
                                 operators
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-post-processing-options" class="anchor" aria-hidden="true" href="#post-processing-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Post-processing Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;-x, --extract-audio              Convert video files to audio-only files
                                 (requires ffmpeg or avconv and ffprobe or
                                 avprobe)
--audio-format FORMAT            Specify audio format: "best", "aac",
                                 "flac", "mp3", "m4a", "opus", "vorbis", or
                                 "wav"; "best" by default; No effect without
                                 -x
--audio-quality QUALITY          Specify ffmpeg/avconv audio quality, insert
                                 a value between 0 (better) and 9 (worse)
                                 for VBR or a specific bitrate like 128K
                                 (default 5)
--recode-video FORMAT            Encode the video to another format if
                                 necessary (currently supported:
                                 mp4|flv|ogg|webm|mkv|avi)
--postprocessor-args ARGS        Give these arguments to the postprocessor
-k, --keep-video                 Keep the video file on disk after the post-
                                 processing; the video is erased by default
--no-post-overwrites             Do not overwrite post-processed files; the
                                 post-processed files are overwritten by
                                 default
--embed-subs                     Embed subtitles in the video (only for mp4,
                                 webm and mkv videos)
--embed-thumbnail                Embed thumbnail in the audio as cover art
--add-metadata                   Write metadata to the video file
--metadata-from-title FORMAT     Parse additional metadata like song title /
                                 artist from the video title. The format
                                 syntax is the same as --output. Regular
                                 expression with named capture groups may
                                 also be used. The parsed parameters replace
                                 existing values. Example: --metadata-from-
                                 title "%(artist)s - %(title)s" matches a
                                 title like "Coldplay - Paradise". Example
                                 (regex): --metadata-from-title
                                 "(?P&amp;lt;artist&amp;gt;.+?) - (?P&amp;lt;title&amp;gt;.+)"
--xattrs                         Write metadata to the video file's xattrs
                                 (using dublin core and xdg standards)
--fixup POLICY                   Automatically correct known faults of the
                                 file. One of never (do nothing), warn (only
                                 emit a warning), detect_or_warn (the
                                 default; fix file if we can, warn
                                 otherwise)
--prefer-avconv                  Prefer avconv over ffmpeg for running the
                                 postprocessors
--prefer-ffmpeg                  Prefer ffmpeg over avconv for running the
                                 postprocessors (default)
--ffmpeg-location PATH           Location of the ffmpeg/avconv binary;
                                 either the path to the binary or its
                                 containing directory.
--exec CMD                       Execute a command on the file after
                                 downloading, similar to find's -exec
                                 syntax. Example: --exec 'adb push {}
                                 /sdcard/Music/ &amp;amp;&amp;amp; rm {}'
--convert-subs FORMAT            Convert the subtitles to other format
                                 (currently supported: srt|ass|vtt|lrc)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-configuration" class="anchor" aria-hidden="true" href="#configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CONFIGURATION&lt;/h1&gt;
&lt;p&gt;You can configure youtube-dl by placing any supported command line option to a configuration file. On Linux and macOS, the system wide configuration file is located at &lt;code&gt;/etc/youtube-dl.conf&lt;/code&gt; and the user wide configuration file at &lt;code&gt;~/.config/youtube-dl/config&lt;/code&gt;. On Windows, the user wide configuration file locations are &lt;code&gt;%APPDATA%\youtube-dl\config.txt&lt;/code&gt; or &lt;code&gt;C:\Users\&amp;lt;user name&amp;gt;\youtube-dl.conf&lt;/code&gt;. Note that by default configuration file may not exist so you may need to create it yourself.&lt;/p&gt;
&lt;p&gt;For example, with the following configuration file youtube-dl will always extract the audio, not copy the mtime, use a proxy and save all videos under &lt;code&gt;Movies&lt;/code&gt; directory in your home directory:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Lines starting with # are comments

# Always extract audio
-x

# Do not copy the mtime
--no-mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under Movies directory in your home directory
-o ~/Movies/%(title)s.%(ext)s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that options in configuration file are just the same options aka switches used in regular command line calls thus there &lt;strong&gt;must be no whitespace&lt;/strong&gt; after &lt;code&gt;-&lt;/code&gt; or &lt;code&gt;--&lt;/code&gt;, e.g. &lt;code&gt;-o&lt;/code&gt; or &lt;code&gt;--proxy&lt;/code&gt; but not &lt;code&gt;- o&lt;/code&gt; or &lt;code&gt;-- proxy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can use &lt;code&gt;--ignore-config&lt;/code&gt; if you want to disable the configuration file for a particular youtube-dl run.&lt;/p&gt;
&lt;p&gt;You can also use &lt;code&gt;--config-location&lt;/code&gt; if you want to use custom configuration file for a particular youtube-dl run.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-authentication-with-netrc-file" class="anchor" aria-hidden="true" href="#authentication-with-netrc-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authentication with &lt;code&gt;.netrc&lt;/code&gt; file&lt;/h3&gt;
&lt;p&gt;You may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with &lt;code&gt;--username&lt;/code&gt; and &lt;code&gt;--password&lt;/code&gt;) in order not to pass credentials as command line arguments on every youtube-dl execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a &lt;a href="https://stackoverflow.com/tags/.netrc/info" rel="nofollow"&gt;&lt;code&gt;.netrc&lt;/code&gt; file&lt;/a&gt; on a per extractor basis. For that you will need to create a &lt;code&gt;.netrc&lt;/code&gt; file in your &lt;code&gt;$HOME&lt;/code&gt; and restrict permissions to read/write by only you:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;touch $HOME/.netrc
chmod a-rwx,u+rw $HOME/.netrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After that you can add credentials for an extractor in the following format, where &lt;em&gt;extractor&lt;/em&gt; is the name of the extractor in lowercase:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;machine &amp;lt;extractor&amp;gt; login &amp;lt;login&amp;gt; password &amp;lt;password&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To activate authentication with the &lt;code&gt;.netrc&lt;/code&gt; file you should pass &lt;code&gt;--netrc&lt;/code&gt; to youtube-dl or place it in the &lt;a href="#configuration"&gt;configuration file&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On Windows you may also need to setup the &lt;code&gt;%HOME%&lt;/code&gt; environment variable manually. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;set HOME=%USERPROFILE%
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-output-template" class="anchor" aria-hidden="true" href="#output-template"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OUTPUT TEMPLATE&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;-o&lt;/code&gt; option allows users to indicate a template for the output file names.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; &lt;a href="#output-template-examples"&gt;navigate me to examples&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The basic usage is not to set any template arguments when downloading a single file, like in &lt;code&gt;youtube-dl -o funny_video.flv "https://some/video"&lt;/code&gt;. However, it may contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to &lt;a href="https://docs.python.org/2/library/stdtypes.html#string-formatting" rel="nofollow"&gt;python string formatting operations&lt;/a&gt;. For example, &lt;code&gt;%(NAME)s&lt;/code&gt; or &lt;code&gt;%(NAME)05d&lt;/code&gt;. To clarify, that is a percent symbol followed by a name in parentheses, followed by formatting operations. Allowed names along with sequence type are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;id&lt;/code&gt; (string): Video identifier&lt;/li&gt;
&lt;li&gt;&lt;code&gt;title&lt;/code&gt; (string): Video title&lt;/li&gt;
&lt;li&gt;&lt;code&gt;url&lt;/code&gt; (string): Video URL&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ext&lt;/code&gt; (string): Video filename extension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alt_title&lt;/code&gt; (string): A secondary title of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;display_id&lt;/code&gt; (string): An alternative identifier for the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;uploader&lt;/code&gt; (string): Full name of the video uploader&lt;/li&gt;
&lt;li&gt;&lt;code&gt;license&lt;/code&gt; (string): License name the video is licensed under&lt;/li&gt;
&lt;li&gt;&lt;code&gt;creator&lt;/code&gt; (string): The creator of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;release_date&lt;/code&gt; (string): The date (YYYYMMDD) when the video was released&lt;/li&gt;
&lt;li&gt;&lt;code&gt;timestamp&lt;/code&gt; (numeric): UNIX timestamp of the moment the video became available&lt;/li&gt;
&lt;li&gt;&lt;code&gt;upload_date&lt;/code&gt; (string): Video upload date (YYYYMMDD)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;uploader_id&lt;/code&gt; (string): Nickname or id of the video uploader&lt;/li&gt;
&lt;li&gt;&lt;code&gt;channel&lt;/code&gt; (string): Full name of the channel the video is uploaded on&lt;/li&gt;
&lt;li&gt;&lt;code&gt;channel_id&lt;/code&gt; (string): Id of the channel&lt;/li&gt;
&lt;li&gt;&lt;code&gt;location&lt;/code&gt; (string): Physical location where the video was filmed&lt;/li&gt;
&lt;li&gt;&lt;code&gt;duration&lt;/code&gt; (numeric): Length of the video in seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;view_count&lt;/code&gt; (numeric): How many users have watched the video on the platform&lt;/li&gt;
&lt;li&gt;&lt;code&gt;like_count&lt;/code&gt; (numeric): Number of positive ratings of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dislike_count&lt;/code&gt; (numeric): Number of negative ratings of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;repost_count&lt;/code&gt; (numeric): Number of reposts of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;average_rating&lt;/code&gt; (numeric): Average rating give by users, the scale used depends on the webpage&lt;/li&gt;
&lt;li&gt;&lt;code&gt;comment_count&lt;/code&gt; (numeric): Number of comments on the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;age_limit&lt;/code&gt; (numeric): Age restriction for the video (years)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;is_live&lt;/code&gt; (boolean): Whether this video is a live stream or a fixed-length video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;start_time&lt;/code&gt; (numeric): Time in seconds where the reproduction should start, as specified in the URL&lt;/li&gt;
&lt;li&gt;&lt;code&gt;end_time&lt;/code&gt; (numeric): Time in seconds where the reproduction should end, as specified in the URL&lt;/li&gt;
&lt;li&gt;&lt;code&gt;format&lt;/code&gt; (string): A human-readable description of the format&lt;/li&gt;
&lt;li&gt;&lt;code&gt;format_id&lt;/code&gt; (string): Format code specified by &lt;code&gt;--format&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;format_note&lt;/code&gt; (string): Additional info about the format&lt;/li&gt;
&lt;li&gt;&lt;code&gt;width&lt;/code&gt; (numeric): Width of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;height&lt;/code&gt; (numeric): Height of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;resolution&lt;/code&gt; (string): Textual description of width and height&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tbr&lt;/code&gt; (numeric): Average bitrate of audio and video in KBit/s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;abr&lt;/code&gt; (numeric): Average audio bitrate in KBit/s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;acodec&lt;/code&gt; (string): Name of the audio codec in use&lt;/li&gt;
&lt;li&gt;&lt;code&gt;asr&lt;/code&gt; (numeric): Audio sampling rate in Hertz&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vbr&lt;/code&gt; (numeric): Average video bitrate in KBit/s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fps&lt;/code&gt; (numeric): Frame rate&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt; (string): Name of the video codec in use&lt;/li&gt;
&lt;li&gt;&lt;code&gt;container&lt;/code&gt; (string): Name of the container format&lt;/li&gt;
&lt;li&gt;&lt;code&gt;filesize&lt;/code&gt; (numeric): The number of bytes, if known in advance&lt;/li&gt;
&lt;li&gt;&lt;code&gt;filesize_approx&lt;/code&gt; (numeric): An estimate for the number of bytes&lt;/li&gt;
&lt;li&gt;&lt;code&gt;protocol&lt;/code&gt; (string): The protocol that will be used for the actual download&lt;/li&gt;
&lt;li&gt;&lt;code&gt;extractor&lt;/code&gt; (string): Name of the extractor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;extractor_key&lt;/code&gt; (string): Key name of the extractor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;epoch&lt;/code&gt; (numeric): Unix epoch when creating the file&lt;/li&gt;
&lt;li&gt;&lt;code&gt;autonumber&lt;/code&gt; (numeric): Five-digit number that will be increased with each download, starting at zero&lt;/li&gt;
&lt;li&gt;&lt;code&gt;playlist&lt;/code&gt; (string): Name or id of the playlist that contains the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;playlist_index&lt;/code&gt; (numeric): Index of the video in the playlist padded with leading zeros according to the total length of the playlist&lt;/li&gt;
&lt;li&gt;&lt;code&gt;playlist_id&lt;/code&gt; (string): Playlist identifier&lt;/li&gt;
&lt;li&gt;&lt;code&gt;playlist_title&lt;/code&gt; (string): Playlist title&lt;/li&gt;
&lt;li&gt;&lt;code&gt;playlist_uploader&lt;/code&gt; (string): Full name of the playlist uploader&lt;/li&gt;
&lt;li&gt;&lt;code&gt;playlist_uploader_id&lt;/code&gt; (string): Nickname or id of the playlist uploader&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Available for the video that belongs to some logical chapter or section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;chapter&lt;/code&gt; (string): Name or title of the chapter the video belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;chapter_number&lt;/code&gt; (numeric): Number of the chapter the video belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;chapter_id&lt;/code&gt; (string): Id of the chapter the video belongs to&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Available for the video that is an episode of some series or programme:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;series&lt;/code&gt; (string): Title of the series or programme the video episode belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;season&lt;/code&gt; (string): Title of the season the video episode belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;season_number&lt;/code&gt; (numeric): Number of the season the video episode belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;season_id&lt;/code&gt; (string): Id of the season the video episode belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;episode&lt;/code&gt; (string): Title of the video episode&lt;/li&gt;
&lt;li&gt;&lt;code&gt;episode_number&lt;/code&gt; (numeric): Number of the video episode within a season&lt;/li&gt;
&lt;li&gt;&lt;code&gt;episode_id&lt;/code&gt; (string): Id of the video episode&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Available for the media that is a track or a part of a music album:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;track&lt;/code&gt; (string): Title of the track&lt;/li&gt;
&lt;li&gt;&lt;code&gt;track_number&lt;/code&gt; (numeric): Number of the track within an album or a disc&lt;/li&gt;
&lt;li&gt;&lt;code&gt;track_id&lt;/code&gt; (string): Id of the track&lt;/li&gt;
&lt;li&gt;&lt;code&gt;artist&lt;/code&gt; (string): Artist(s) of the track&lt;/li&gt;
&lt;li&gt;&lt;code&gt;genre&lt;/code&gt; (string): Genre(s) of the track&lt;/li&gt;
&lt;li&gt;&lt;code&gt;album&lt;/code&gt; (string): Title of the album the track belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;album_type&lt;/code&gt; (string): Type of the album&lt;/li&gt;
&lt;li&gt;&lt;code&gt;album_artist&lt;/code&gt; (string): List of all artists appeared on the album&lt;/li&gt;
&lt;li&gt;&lt;code&gt;disc_number&lt;/code&gt; (numeric): Number of the disc or other physical medium the track belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;release_year&lt;/code&gt; (numeric): Year (YYYY) when the album was released&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. Note that some of the sequences are not guaranteed to be present since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example for &lt;code&gt;-o %(title)s-%(id)s.%(ext)s&lt;/code&gt; and an mp4 video with title &lt;code&gt;youtube-dl test video&lt;/code&gt; and id &lt;code&gt;BaW_jenozKcj&lt;/code&gt;, this will result in a &lt;code&gt;youtube-dl test video-BaW_jenozKcj.mp4&lt;/code&gt; file created in the current directory.&lt;/p&gt;
&lt;p&gt;For numeric sequences you can use numeric related formatting, for example, &lt;code&gt;%(view_count)05d&lt;/code&gt; will result in a string with view count padded with zeros up to 5 characters, like in &lt;code&gt;00042&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Output templates can also contain arbitrary hierarchical path, e.g. &lt;code&gt;-o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s'&lt;/code&gt; which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.&lt;/p&gt;
&lt;p&gt;To use percent literals in an output template use &lt;code&gt;%%&lt;/code&gt;. To output to stdout use &lt;code&gt;-o -&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The current default template is &lt;code&gt;%(title)s-%(id)s.%(ext)s&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In some cases, you don't want special characters such as 中, spaces, or &amp;amp;, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the &lt;code&gt;--restrict-filenames&lt;/code&gt; flag to get a shorter title:&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-output-template-and-windows-batch-files" class="anchor" aria-hidden="true" href="#output-template-and-windows-batch-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Output template and Windows batch files&lt;/h4&gt;
&lt;p&gt;If you are using an output template inside a Windows batch file then you must escape plain percent characters (&lt;code&gt;%&lt;/code&gt;) by doubling, so that &lt;code&gt;-o "%(title)s-%(id)s.%(ext)s"&lt;/code&gt; should become &lt;code&gt;-o "%%(title)s-%%(id)s.%%(ext)s"&lt;/code&gt;. However you should not touch &lt;code&gt;%&lt;/code&gt;'s that are not plain characters, e.g. environment variables for expansion should stay intact: &lt;code&gt;-o "C:\%HOMEPATH%\Desktop\%%(title)s.%%(ext)s"&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-output-template-examples" class="anchor" aria-hidden="true" href="#output-template-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Output template examples&lt;/h4&gt;
&lt;p&gt;Note that on Windows you may need to use double quotes instead of single.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ youtube-dl --get-filename -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;%(title)s.%(ext)s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; BaW_jenozKc
youtube-dl &lt;span class="pl-c1"&gt;test&lt;/span&gt; video &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;_ä↭𝕐.mp4    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; All kinds of weird characters&lt;/span&gt;

$ youtube-dl --get-filename -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;%(title)s.%(ext)s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.mp4          &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; A simple file name&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download YouTube playlist videos in separate directory indexed by video order in a playlist&lt;/span&gt;
$ youtube-dl -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; https://www.youtube.com/playlist&lt;span class="pl-k"&gt;?&lt;/span&gt;list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download all playlists of YouTube channel/user keeping each playlist in separate directory:&lt;/span&gt;
$ youtube-dl -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; https://www.youtube.com/user/TheLinuxFoundation/playlists

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home&lt;/span&gt;
$ youtube-dl -u user -p password -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;~/MyVideos/%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; https://www.udemy.com/java-tutorial/

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download entire series season keeping each series and each season in separate directory under C:/MyVideos&lt;/span&gt;
$ youtube-dl -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;C:/MyVideos/%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; https://videomore.ru/kino_v_detalayah/5_sezon/367617

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Stream the video being downloaded to stdout&lt;/span&gt;
$ youtube-dl -o - BaW_jenozKc&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-format-selection" class="anchor" aria-hidden="true" href="#format-selection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FORMAT SELECTION&lt;/h1&gt;
&lt;p&gt;By default youtube-dl tries to download the best available quality, i.e. if you want the best quality you &lt;strong&gt;don't need&lt;/strong&gt; to pass any special options, youtube-dl will guess it for you by &lt;strong&gt;default&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;But sometimes you may want to download in a different format, for example when you are on a slow or intermittent connection. The key mechanism for achieving this is so-called &lt;em&gt;format selection&lt;/em&gt; based on which you can explicitly specify desired format, select formats based on some criterion or criteria, setup precedence and much more.&lt;/p&gt;
&lt;p&gt;The general syntax for format selection is &lt;code&gt;--format FORMAT&lt;/code&gt; or shorter &lt;code&gt;-f FORMAT&lt;/code&gt; where &lt;code&gt;FORMAT&lt;/code&gt; is a &lt;em&gt;selector expression&lt;/em&gt;, i.e. an expression that describes format or formats you would like to download.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; &lt;a href="#format-selection-examples"&gt;navigate me to examples&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The simplest case is requesting a specific format, for example with &lt;code&gt;-f 22&lt;/code&gt; you can download the format with format code equal to 22. You can get the list of available format codes for particular video using &lt;code&gt;--list-formats&lt;/code&gt; or &lt;code&gt;-F&lt;/code&gt;. Note that these format codes are extractor specific.&lt;/p&gt;
&lt;p&gt;You can also use a file extension (currently &lt;code&gt;3gp&lt;/code&gt;, &lt;code&gt;aac&lt;/code&gt;, &lt;code&gt;flv&lt;/code&gt;, &lt;code&gt;m4a&lt;/code&gt;, &lt;code&gt;mp3&lt;/code&gt;, &lt;code&gt;mp4&lt;/code&gt;, &lt;code&gt;ogg&lt;/code&gt;, &lt;code&gt;wav&lt;/code&gt;, &lt;code&gt;webm&lt;/code&gt; are supported) to download the best quality format of a particular file extension served as a single file, e.g. &lt;code&gt;-f webm&lt;/code&gt; will download the best quality format with the &lt;code&gt;webm&lt;/code&gt; extension served as a single file.&lt;/p&gt;
&lt;p&gt;You can also use special names to select particular edge case formats:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;best&lt;/code&gt;: Select the best quality format represented by a single file with video and audio.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;worst&lt;/code&gt;: Select the worst quality format represented by a single file with video and audio.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bestvideo&lt;/code&gt;: Select the best quality video-only format (e.g. DASH video). May not be available.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;worstvideo&lt;/code&gt;: Select the worst quality video-only format. May not be available.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bestaudio&lt;/code&gt;: Select the best quality audio only-format. May not be available.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;worstaudio&lt;/code&gt;: Select the worst quality audio only-format. May not be available.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, to download the worst quality video-only format you can use &lt;code&gt;-f worstvideo&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you want to download multiple videos and they don't have the same formats available, you can specify the order of preference using slashes. Note that slash is left-associative, i.e. formats on the left hand side are preferred, for example &lt;code&gt;-f 22/17/18&lt;/code&gt; will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.&lt;/p&gt;
&lt;p&gt;If you want to download several formats of the same video use a comma as a separator, e.g. &lt;code&gt;-f 22,17,18&lt;/code&gt; will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: &lt;code&gt;-f 136/137/mp4/bestvideo,140/m4a/bestaudio&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can also filter the video formats by putting a condition in brackets, as in &lt;code&gt;-f "best[height=720]"&lt;/code&gt; (or &lt;code&gt;-f "[filesize&amp;gt;10M]"&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The following numeric meta fields can be used with comparisons &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;=&lt;/code&gt; (equals), &lt;code&gt;!=&lt;/code&gt; (not equals):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;filesize&lt;/code&gt;: The number of bytes, if known in advance&lt;/li&gt;
&lt;li&gt;&lt;code&gt;width&lt;/code&gt;: Width of the video, if known&lt;/li&gt;
&lt;li&gt;&lt;code&gt;height&lt;/code&gt;: Height of the video, if known&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tbr&lt;/code&gt;: Average bitrate of audio and video in KBit/s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;abr&lt;/code&gt;: Average audio bitrate in KBit/s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vbr&lt;/code&gt;: Average video bitrate in KBit/s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;asr&lt;/code&gt;: Audio sampling rate in Hertz&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fps&lt;/code&gt;: Frame rate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also filtering work for comparisons &lt;code&gt;=&lt;/code&gt; (equals), &lt;code&gt;^=&lt;/code&gt; (starts with), &lt;code&gt;$=&lt;/code&gt; (ends with), &lt;code&gt;*=&lt;/code&gt; (contains) and following string meta fields:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ext&lt;/code&gt;: File extension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;acodec&lt;/code&gt;: Name of the audio codec in use&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt;: Name of the video codec in use&lt;/li&gt;
&lt;li&gt;&lt;code&gt;container&lt;/code&gt;: Name of the container format&lt;/li&gt;
&lt;li&gt;&lt;code&gt;protocol&lt;/code&gt;: The protocol that will be used for the actual download, lower-case (&lt;code&gt;http&lt;/code&gt;, &lt;code&gt;https&lt;/code&gt;, &lt;code&gt;rtsp&lt;/code&gt;, &lt;code&gt;rtmp&lt;/code&gt;, &lt;code&gt;rtmpe&lt;/code&gt;, &lt;code&gt;mms&lt;/code&gt;, &lt;code&gt;f4m&lt;/code&gt;, &lt;code&gt;ism&lt;/code&gt;, &lt;code&gt;http_dash_segments&lt;/code&gt;, &lt;code&gt;m3u8&lt;/code&gt;, or &lt;code&gt;m3u8_native&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;format_id&lt;/code&gt;: A short description of the format&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Any string comparison may be prefixed with negation &lt;code&gt;!&lt;/code&gt; in order to produce an opposite comparison, e.g. &lt;code&gt;!*=&lt;/code&gt; (does not contain).&lt;/p&gt;
&lt;p&gt;Note that none of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by particular extractor, i.e. the metadata offered by the video hoster.&lt;/p&gt;
&lt;p&gt;Formats for which the value is not known are excluded unless you put a question mark (&lt;code&gt;?&lt;/code&gt;) after the operator. You can combine format filters, so &lt;code&gt;-f "[height &amp;lt;=? 720][tbr&amp;gt;500]"&lt;/code&gt; selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 KBit/s.&lt;/p&gt;
&lt;p&gt;You can merge the video and audio of two formats into a single file using &lt;code&gt;-f &amp;lt;video-format&amp;gt;+&amp;lt;audio-format&amp;gt;&lt;/code&gt; (requires ffmpeg or avconv installed), for example &lt;code&gt;-f bestvideo+bestaudio&lt;/code&gt; will download the best video-only format, the best audio-only format and mux them together with ffmpeg/avconv.&lt;/p&gt;
&lt;p&gt;Format selectors can also be grouped using parentheses, for example if you want to download the best mp4 and webm formats with a height lower than 480 you can use &lt;code&gt;-f '(mp4,webm)[height&amp;lt;480]'&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Since the end of April 2015 and version 2015.04.26, youtube-dl uses &lt;code&gt;-f bestvideo+bestaudio/best&lt;/code&gt; as the default format selection (see &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/5447"&gt;#5447&lt;/a&gt;, &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/5456"&gt;#5456&lt;/a&gt;). If ffmpeg or avconv are installed this results in downloading &lt;code&gt;bestvideo&lt;/code&gt; and &lt;code&gt;bestaudio&lt;/code&gt; separately and muxing them together into a single file giving the best overall quality available. Otherwise it falls back to &lt;code&gt;best&lt;/code&gt; and results in downloading the best available quality served as a single file. &lt;code&gt;best&lt;/code&gt; is also needed for videos that don't come from YouTube because they don't provide the audio and video in two different files. If you want to only download some DASH formats (for example if you are not interested in getting videos with a resolution higher than 1080p), you can add &lt;code&gt;-f bestvideo[height&amp;lt;=?1080]+bestaudio/best&lt;/code&gt; to your configuration file. Note that if you use youtube-dl to stream to &lt;code&gt;stdout&lt;/code&gt; (and most likely to pipe it to your media player then), i.e. you explicitly specify output template as &lt;code&gt;-o -&lt;/code&gt;, youtube-dl still uses &lt;code&gt;-f best&lt;/code&gt; format selection in order to start content delivery immediately to your player and not to wait until &lt;code&gt;bestvideo&lt;/code&gt; and &lt;code&gt;bestaudio&lt;/code&gt; are downloaded and muxed.&lt;/p&gt;
&lt;p&gt;If you want to preserve the old format selection behavior (prior to youtube-dl 2015.04.26), i.e. you want to download the best available quality media served as a single file, you should explicitly specify your choice with &lt;code&gt;-f best&lt;/code&gt;. You may want to add it to the &lt;a href="#configuration"&gt;configuration file&lt;/a&gt; in order not to type it every time you run youtube-dl.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-format-selection-examples" class="anchor" aria-hidden="true" href="#format-selection-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Format selection examples&lt;/h4&gt;
&lt;p&gt;Note that on Windows you may need to use double quotes instead of single.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download best mp4 format available or any other best if no mp4 available&lt;/span&gt;
$ youtube-dl -f &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download best format available but no better than 480p&lt;/span&gt;
$ youtube-dl -f &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bestvideo[height&amp;lt;=480]+bestaudio/best[height&amp;lt;=480]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download best video only format but no bigger than 50 MB&lt;/span&gt;
$ youtube-dl -f &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;best[filesize&amp;lt;50M]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download best format available via direct link over HTTP/HTTPS protocol&lt;/span&gt;
$ youtube-dl -f &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;(bestvideo+bestaudio/best)[protocol^=http]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download the best video format and the best audio format without merging them&lt;/span&gt;
$ youtube-dl -f &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bestvideo,bestaudio&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;%(title)s.f%(format_id)s.%(ext)s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that in the last example, an output template is recommended as bestvideo and bestaudio may have the same file name.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-video-selection-1" class="anchor" aria-hidden="true" href="#video-selection-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VIDEO SELECTION&lt;/h1&gt;
&lt;p&gt;Videos can be filtered by their upload date using the options &lt;code&gt;--date&lt;/code&gt;, &lt;code&gt;--datebefore&lt;/code&gt; or &lt;code&gt;--dateafter&lt;/code&gt;. They accept dates in two formats:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Absolute dates: Dates in the format &lt;code&gt;YYYYMMDD&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Relative dates: Dates in the format &lt;code&gt;(now|today)[+-][0-9](day|week|month|year)(s)?&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download only the videos uploaded in the last 6 months&lt;/span&gt;
$ youtube-dl --dateafter now-6months

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download only the videos uploaded on January 1, 1970&lt;/span&gt;
$ youtube-dl --date 19700101

$ &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download only the videos uploaded in the 200x decade&lt;/span&gt;
$ youtube-dl --dateafter 20000101 --datebefore 20091231&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-how-do-i-update-youtube-dl" class="anchor" aria-hidden="true" href="#how-do-i-update-youtube-dl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I update youtube-dl?&lt;/h3&gt;
&lt;p&gt;If you've followed &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html" rel="nofollow"&gt;our manual installation instructions&lt;/a&gt;, you can simply run &lt;code&gt;youtube-dl -U&lt;/code&gt; (or, on Linux, &lt;code&gt;sudo youtube-dl -U&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you have used pip, a simple &lt;code&gt;sudo pip install -U youtube-dl&lt;/code&gt; is sufficient to update.&lt;/p&gt;
&lt;p&gt;If you have installed youtube-dl using a package manager like &lt;em&gt;apt-get&lt;/em&gt; or &lt;em&gt;yum&lt;/em&gt;, use the standard system update mechanism to update. Note that distribution packages are often outdated. As a rule of thumb, youtube-dl releases at least once a month, and often weekly or even daily. Simply go to &lt;a href="https://yt-dl.org" rel="nofollow"&gt;https://yt-dl.org&lt;/a&gt; to find out the current version. Unfortunately, there is nothing we youtube-dl developers can do if your distribution serves a really outdated version. You can (and should) complain to your distribution in their bugtracker or support forum.&lt;/p&gt;
&lt;p&gt;As a last resort, you can also uninstall the version installed by your package manager and follow our manual installation instructions. For that, remove the distribution's package, with a line like&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get remove -y youtube-dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Afterwards, simply follow &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html" rel="nofollow"&gt;our manual installation instructions&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo wget https://yt-dl.org/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+x /usr/local/bin/youtube-dl
hash -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, from then on you'll be able to update with &lt;code&gt;sudo youtube-dl -U&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-youtube-dl-is-extremely-slow-to-start-on-windows" class="anchor" aria-hidden="true" href="#youtube-dl-is-extremely-slow-to-start-on-windows"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;youtube-dl is extremely slow to start on Windows&lt;/h3&gt;
&lt;p&gt;Add a file exclusion for &lt;code&gt;youtube-dl.exe&lt;/code&gt; in Windows Defender settings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-im-getting-an-error-unable-to-extract-opengraph-title-on-youtube-playlists" class="anchor" aria-hidden="true" href="#im-getting-an-error-unable-to-extract-opengraph-title-on-youtube-playlists"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I'm getting an error &lt;code&gt;Unable to extract OpenGraph title&lt;/code&gt; on YouTube playlists&lt;/h3&gt;
&lt;p&gt;YouTube changed their playlist format in March 2014 and later on, so you'll need at least youtube-dl 2014.07.25 to download all YouTube videos.&lt;/p&gt;
&lt;p&gt;If you have installed youtube-dl with a package manager, pip, setup.py or a tarball, please use that to update. Note that Ubuntu packages do not seem to get updated anymore. Since we are not affiliated with Ubuntu, there is little we can do. Feel free to &lt;a href="https://bugs.launchpad.net/ubuntu/+source/youtube-dl/+filebug" rel="nofollow"&gt;report bugs&lt;/a&gt; to the &lt;a href="mailto:ubuntu-motu@lists.ubuntu.com?subject=outdated%20version%20of%20youtube-dl"&gt;Ubuntu packaging people&lt;/a&gt; - all they have to do is update the package to a somewhat recent version. See above for a way to update.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-im-getting-an-error-when-trying-to-use-output-template-error-using-output-template-conflicts-with-using-title-video-id-or-auto-number" class="anchor" aria-hidden="true" href="#im-getting-an-error-when-trying-to-use-output-template-error-using-output-template-conflicts-with-using-title-video-id-or-auto-number"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I'm getting an error when trying to use output template: &lt;code&gt;error: using output template conflicts with using title, video ID or auto number&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Make sure you are not using &lt;code&gt;-o&lt;/code&gt; with any of these options &lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--title&lt;/code&gt;, &lt;code&gt;--id&lt;/code&gt;, &lt;code&gt;-A&lt;/code&gt; or &lt;code&gt;--auto-number&lt;/code&gt; set in command line or in a configuration file. Remove the latter if any.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-do-i-always-have-to-pass--citw" class="anchor" aria-hidden="true" href="#do-i-always-have-to-pass--citw"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Do I always have to pass &lt;code&gt;-citw&lt;/code&gt;?&lt;/h3&gt;
&lt;p&gt;By default, youtube-dl intends to have the best options (incidentally, if you have a convincing case that these should be different, &lt;a href="https://yt-dl.org/bug" rel="nofollow"&gt;please file an issue where you explain that&lt;/a&gt;). Therefore, it is unnecessary and sometimes harmful to copy long option strings from webpages. In particular, the only option out of &lt;code&gt;-citw&lt;/code&gt; that is regularly useful is &lt;code&gt;-i&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-can-you-please-put-the--b-option-back" class="anchor" aria-hidden="true" href="#can-you-please-put-the--b-option-back"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Can you please put the &lt;code&gt;-b&lt;/code&gt; option back?&lt;/h3&gt;
&lt;p&gt;Most people asking this question are not aware that youtube-dl now defaults to downloading the highest available quality as reported by YouTube, which will be 1080p or 720p in some cases, so you no longer need the &lt;code&gt;-b&lt;/code&gt; option. For some specific videos, maybe YouTube does not report them to be available in a specific high quality format you're interested in. In that case, simply request it with the &lt;code&gt;-f&lt;/code&gt; option and youtube-dl will try to download it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-i-get-http-error-402-when-trying-to-download-a-video-whats-this" class="anchor" aria-hidden="true" href="#i-get-http-error-402-when-trying-to-download-a-video-whats-this"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I get HTTP error 402 when trying to download a video. What's this?&lt;/h3&gt;
&lt;p&gt;Apparently YouTube requires you to pass a CAPTCHA test if you download too much. We're &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/154"&gt;considering to provide a way to let you solve the CAPTCHA&lt;/a&gt;, but at the moment, your best course of action is pointing a web browser to the youtube URL, solving the CAPTCHA, and restart youtube-dl.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-do-i-need-any-other-programs" class="anchor" aria-hidden="true" href="#do-i-need-any-other-programs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Do I need any other programs?&lt;/h3&gt;
&lt;p&gt;youtube-dl works fine on its own on most sites. However, if you want to convert video/audio, you'll need &lt;a href="https://libav.org/" rel="nofollow"&gt;avconv&lt;/a&gt; or &lt;a href="https://www.ffmpeg.org/" rel="nofollow"&gt;ffmpeg&lt;/a&gt;. On some sites - most notably YouTube - videos can be retrieved in a higher quality format without sound. youtube-dl will detect whether avconv/ffmpeg is present and automatically pick the best option.&lt;/p&gt;
&lt;p&gt;Videos or video formats streamed via RTMP protocol can only be downloaded when &lt;a href="https://rtmpdump.mplayerhq.hu/" rel="nofollow"&gt;rtmpdump&lt;/a&gt; is installed. Downloading MMS and RTSP videos requires either &lt;a href="https://mplayerhq.hu/" rel="nofollow"&gt;mplayer&lt;/a&gt; or &lt;a href="https://mpv.io/" rel="nofollow"&gt;mpv&lt;/a&gt; to be installed.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-i-have-downloaded-a-video-but-how-can-i-play-it" class="anchor" aria-hidden="true" href="#i-have-downloaded-a-video-but-how-can-i-play-it"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I have downloaded a video but how can I play it?&lt;/h3&gt;
&lt;p&gt;Once the video is fully downloaded, use any video player, such as &lt;a href="https://mpv.io/" rel="nofollow"&gt;mpv&lt;/a&gt;, &lt;a href="https://www.videolan.org/" rel="nofollow"&gt;vlc&lt;/a&gt; or &lt;a href="https://www.mplayerhq.hu/" rel="nofollow"&gt;mplayer&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-i-extracted-a-video-url-with--g-but-it-does-not-play-on-another-machine--in-my-web-browser" class="anchor" aria-hidden="true" href="#i-extracted-a-video-url-with--g-but-it-does-not-play-on-another-machine--in-my-web-browser"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I extracted a video URL with &lt;code&gt;-g&lt;/code&gt;, but it does not play on another machine / in my web browser.&lt;/h3&gt;
&lt;p&gt;It depends a lot on the service. In many cases, requests for the video (to download/play it) must come from the same IP address and with the same cookies and/or HTTP headers. Use the &lt;code&gt;--cookies&lt;/code&gt; option to write the required cookies into a file, and advise your downloader to read cookies from that file. Some sites also require a common user agent to be used, use &lt;code&gt;--dump-user-agent&lt;/code&gt; to see the one in use by youtube-dl. You can also get necessary cookies and HTTP headers from JSON output obtained with &lt;code&gt;--dump-json&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It may be beneficial to use IPv6; in some cases, the restrictions are only applied to IPv4. Some services (sometimes only for a subset of videos) do not restrict the video URL by IP address, cookie, or user-agent, but these are the exception rather than the rule.&lt;/p&gt;
&lt;p&gt;Please bear in mind that some URL protocols are &lt;strong&gt;not&lt;/strong&gt; supported by browsers out of the box, including RTMP. If you are using &lt;code&gt;-g&lt;/code&gt;, your own downloader must support these as well.&lt;/p&gt;
&lt;p&gt;If you want to play the video on a machine that is not running youtube-dl, you can relay the video content from the machine that runs youtube-dl. You can use &lt;code&gt;-o -&lt;/code&gt; to let youtube-dl stream a video to stdout, or simply allow the player to download the files written by youtube-dl in turn.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-error-no-fmt_url_map-or-conn-information-found-in-video-info" class="anchor" aria-hidden="true" href="#error-no-fmt_url_map-or-conn-information-found-in-video-info"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ERROR: no fmt_url_map or conn information found in video info&lt;/h3&gt;
&lt;p&gt;YouTube has switched to a new video info format in July 2011 which is not supported by old versions of youtube-dl. See &lt;a href="#how-do-i-update-youtube-dl"&gt;above&lt;/a&gt; for how to update youtube-dl.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-error-unable-to-download-video" class="anchor" aria-hidden="true" href="#error-unable-to-download-video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ERROR: unable to download video&lt;/h3&gt;
&lt;p&gt;YouTube requires an additional signature since September 2012 which is not supported by old versions of youtube-dl. See &lt;a href="#how-do-i-update-youtube-dl"&gt;above&lt;/a&gt; for how to update youtube-dl.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-video-url-contains-an-ampersand-and-im-getting-some-strange-output-1-2839-or-v-is-not-recognized-as-an-internal-or-external-command" class="anchor" aria-hidden="true" href="#video-url-contains-an-ampersand-and-im-getting-some-strange-output-1-2839-or-v-is-not-recognized-as-an-internal-or-external-command"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video URL contains an ampersand and I'm getting some strange output &lt;code&gt;[1] 2839&lt;/code&gt; or &lt;code&gt;'v' is not recognized as an internal or external command&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;That's actually the output from your shell. Since ampersand is one of the special shell characters it's interpreted by the shell preventing you from passing the whole URL to youtube-dl. To disable your shell from interpreting the ampersands (or any other special characters) you have to either put the whole URL in quotes or escape them with a backslash (which approach will work depends on your shell).&lt;/p&gt;
&lt;p&gt;For example if your URL is &lt;a href="https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc" rel="nofollow"&gt;https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc&lt;/a&gt; you should end up with following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;youtube-dl 'https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc'&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;&lt;code&gt;youtube-dl https://www.youtube.com/watch?t=4\&amp;amp;v=BaW_jenozKc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For Windows you have to use the double quotes:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;youtube-dl "https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc"&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-extractorerror-could-not-find-js-function-uof" class="anchor" aria-hidden="true" href="#extractorerror-could-not-find-js-function-uof"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ExtractorError: Could not find JS function u'OF'&lt;/h3&gt;
&lt;p&gt;In February 2015, the new YouTube player contained a character sequence in a string that was misinterpreted by old versions of youtube-dl. See &lt;a href="#how-do-i-update-youtube-dl"&gt;above&lt;/a&gt; for how to update youtube-dl.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-http-error-429-too-many-requests-or-402-payment-required" class="anchor" aria-hidden="true" href="#http-error-429-too-many-requests-or-402-payment-required"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTTP Error 429: Too Many Requests or 402: Payment Required&lt;/h3&gt;
&lt;p&gt;These two error codes indicate that the service is blocking your IP address because of overuse. Contact the service and ask them to unblock your IP address, or - if you have acquired a whitelisted IP address already - use the &lt;a href="#network-options"&gt;&lt;code&gt;--proxy&lt;/code&gt; or &lt;code&gt;--source-address&lt;/code&gt; options&lt;/a&gt; to select another IP address.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-syntaxerror-non-ascii-character" class="anchor" aria-hidden="true" href="#syntaxerror-non-ascii-character"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SyntaxError: Non-ASCII character&lt;/h3&gt;
&lt;p&gt;The error&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;File "youtube-dl", line 2
SyntaxError: Non-ASCII character '\x93' ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;means you're using an outdated version of Python. Please update to Python 2.6 or 2.7.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-what-is-this-binary-file-where-has-the-code-gone" class="anchor" aria-hidden="true" href="#what-is-this-binary-file-where-has-the-code-gone"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is this binary file? Where has the code gone?&lt;/h3&gt;
&lt;p&gt;Since June 2012 (&lt;a href="https://github.com/ytdl-org/youtube-dl/issues/342"&gt;#342&lt;/a&gt;) youtube-dl is packed as an executable zipfile, simply unzip it (might need renaming to &lt;code&gt;youtube-dl.zip&lt;/code&gt; first on some systems) or clone the git repository, as laid out above. If you modify the code, you can run it by executing the &lt;code&gt;__main__.py&lt;/code&gt; file. To recompile the executable, run &lt;code&gt;make youtube-dl&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-the-exe-throws-an-error-due-to-missing-msvcr100dll" class="anchor" aria-hidden="true" href="#the-exe-throws-an-error-due-to-missing-msvcr100dll"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The exe throws an error due to missing &lt;code&gt;MSVCR100.dll&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;To run the exe you need to install first the &lt;a href="https://www.microsoft.com/en-US/download/details.aspx?id=5555" rel="nofollow"&gt;Microsoft Visual C++ 2010 Redistributable Package (x86)&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-on-windows-how-should-i-set-up-ffmpeg-and-youtube-dl-where-should-i-put-the-exe-files" class="anchor" aria-hidden="true" href="#on-windows-how-should-i-set-up-ffmpeg-and-youtube-dl-where-should-i-put-the-exe-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;On Windows, how should I set up ffmpeg and youtube-dl? Where should I put the exe files?&lt;/h3&gt;
&lt;p&gt;If you put youtube-dl and ffmpeg in the same directory that you're running the command from, it will work, but that's rather cumbersome.&lt;/p&gt;
&lt;p&gt;To make a different directory work - either for ffmpeg, or for youtube-dl, or for both - simply create the directory (say, &lt;code&gt;C:\bin&lt;/code&gt;, or &lt;code&gt;C:\Users\&amp;lt;User name&amp;gt;\bin&lt;/code&gt;), put all the executables directly in there, and then &lt;a href="https://www.java.com/en/download/help/path.xml" rel="nofollow"&gt;set your PATH environment variable&lt;/a&gt; to include that directory.&lt;/p&gt;
&lt;p&gt;From then on, after restarting your shell, you will be able to access both youtube-dl and ffmpeg (and youtube-dl will be able to find ffmpeg) by simply typing &lt;code&gt;youtube-dl&lt;/code&gt; or &lt;code&gt;ffmpeg&lt;/code&gt;, no matter what directory you're in.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-do-i-put-downloads-into-a-specific-folder" class="anchor" aria-hidden="true" href="#how-do-i-put-downloads-into-a-specific-folder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I put downloads into a specific folder?&lt;/h3&gt;
&lt;p&gt;Use the &lt;code&gt;-o&lt;/code&gt; to specify an &lt;a href="#output-template"&gt;output template&lt;/a&gt;, for example &lt;code&gt;-o "/home/user/videos/%(title)s-%(id)s.%(ext)s"&lt;/code&gt;. If you want this for all of your downloads, put the option into your &lt;a href="#configuration"&gt;configuration file&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-do-i-download-a-video-starting-with-a--" class="anchor" aria-hidden="true" href="#how-do-i-download-a-video-starting-with-a--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I download a video starting with a &lt;code&gt;-&lt;/code&gt;?&lt;/h3&gt;
&lt;p&gt;Either prepend &lt;code&gt;https://www.youtube.com/watch?v=&lt;/code&gt; or separate the ID from the options with &lt;code&gt;--&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;youtube-dl -- -wNyEUrxzFU
youtube-dl "https://www.youtube.com/watch?v=-wNyEUrxzFU"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-how-do-i-pass-cookies-to-youtube-dl" class="anchor" aria-hidden="true" href="#how-do-i-pass-cookies-to-youtube-dl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I pass cookies to youtube-dl?&lt;/h3&gt;
&lt;p&gt;Use the &lt;code&gt;--cookies&lt;/code&gt; option, for example &lt;code&gt;--cookies /path/to/cookies/file.txt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In order to extract cookies from browser use any conforming browser extension for exporting cookies. For example, &lt;a href="https://chrome.google.com/webstore/detail/cookiestxt/njabckikapfpffapmjgojcnbfjonfjfg" rel="nofollow"&gt;cookies.txt&lt;/a&gt; (for Chrome) or &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/cookies-txt/" rel="nofollow"&gt;cookies.txt&lt;/a&gt; (for Firefox).&lt;/p&gt;
&lt;p&gt;Note that the cookies file must be in Mozilla/Netscape format and the first line of the cookies file must be either &lt;code&gt;# HTTP Cookie File&lt;/code&gt; or &lt;code&gt;# Netscape HTTP Cookie File&lt;/code&gt;. Make sure you have correct &lt;a href="https://en.wikipedia.org/wiki/Newline" rel="nofollow"&gt;newline format&lt;/a&gt; in the cookies file and convert newlines if necessary to correspond with your OS, namely &lt;code&gt;CRLF&lt;/code&gt; (&lt;code&gt;\r\n&lt;/code&gt;) for Windows and &lt;code&gt;LF&lt;/code&gt; (&lt;code&gt;\n&lt;/code&gt;) for Unix and Unix-like systems (Linux, macOS, etc.). &lt;code&gt;HTTP Error 400: Bad Request&lt;/code&gt; when using &lt;code&gt;--cookies&lt;/code&gt; is a good sign of invalid newline format.&lt;/p&gt;
&lt;p&gt;Passing cookies to youtube-dl is a good way to workaround login when a particular extractor does not implement it explicitly. Another use case is working around &lt;a href="https://en.wikipedia.org/wiki/CAPTCHA" rel="nofollow"&gt;CAPTCHA&lt;/a&gt; some websites require you to solve in particular cases in order to get access (e.g. YouTube, CloudFlare).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-do-i-stream-directly-to-media-player" class="anchor" aria-hidden="true" href="#how-do-i-stream-directly-to-media-player"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I stream directly to media player?&lt;/h3&gt;
&lt;p&gt;You will first need to tell youtube-dl to stream media to stdout with &lt;code&gt;-o -&lt;/code&gt;, and also tell your media player to read from stdin (it must be capable of this for streaming) and then pipe former to latter. For example, streaming to &lt;a href="https://www.videolan.org/" rel="nofollow"&gt;vlc&lt;/a&gt; can be achieved with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;youtube-dl -o - "https://www.youtube.com/watch?v=BaW_jenozKcj" | vlc -
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-how-do-i-download-only-new-videos-from-a-playlist" class="anchor" aria-hidden="true" href="#how-do-i-download-only-new-videos-from-a-playlist"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I download only new videos from a playlist?&lt;/h3&gt;
&lt;p&gt;Use download-archive feature. With this feature you should initially download the complete playlist with &lt;code&gt;--download-archive /path/to/download/archive/file.txt&lt;/code&gt; that will record identifiers of all the videos in a special file. Each subsequent run with the same &lt;code&gt;--download-archive&lt;/code&gt; will download only new videos and skip all videos that have been downloaded before. Note that only successful downloads are recorded in the file.&lt;/p&gt;
&lt;p&gt;For example, at first,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;youtube-dl --download-archive archive.txt "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will download the complete &lt;code&gt;PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re&lt;/code&gt; playlist and create a file &lt;code&gt;archive.txt&lt;/code&gt;. Each subsequent run will only download new videos if any:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;youtube-dl --download-archive archive.txt "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-should-i-add---hls-prefer-native-into-my-config" class="anchor" aria-hidden="true" href="#should-i-add---hls-prefer-native-into-my-config"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Should I add &lt;code&gt;--hls-prefer-native&lt;/code&gt; into my config?&lt;/h3&gt;
&lt;p&gt;When youtube-dl detects an HLS video, it can download it either with the built-in downloader or ffmpeg. Since many HLS streams are slightly invalid and ffmpeg/youtube-dl each handle some invalid cases better than the other, there is an option to switch the downloader if needed.&lt;/p&gt;
&lt;p&gt;When youtube-dl knows that one particular downloader works better for a given website, that downloader will be picked. Otherwise, youtube-dl will pick the best downloader for general compatibility, which at the moment happens to be ffmpeg. This choice may change in future versions of youtube-dl, with improvements of the built-in downloader and/or ffmpeg.&lt;/p&gt;
&lt;p&gt;In particular, the generic extractor (used when your website is not in the &lt;a href="https://ytdl-org.github.io/youtube-dl/supportedsites.html" rel="nofollow"&gt;list of supported sites by youtube-dl&lt;/a&gt; cannot mandate one specific downloader.&lt;/p&gt;
&lt;p&gt;If you put either &lt;code&gt;--hls-prefer-native&lt;/code&gt; or &lt;code&gt;--hls-prefer-ffmpeg&lt;/code&gt; into your configuration, a different subset of videos will fail to download correctly. Instead, it is much better to &lt;a href="https://yt-dl.org/bug" rel="nofollow"&gt;file an issue&lt;/a&gt; or a pull request which details why the native or the ffmpeg HLS downloader is a better choice for your use case.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-can-you-add-support-for-this-anime-video-site-or-site-which-shows-current-movies-for-free" class="anchor" aria-hidden="true" href="#can-you-add-support-for-this-anime-video-site-or-site-which-shows-current-movies-for-free"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Can you add support for this anime video site, or site which shows current movies for free?&lt;/h3&gt;
&lt;p&gt;As a matter of policy (as well as legality), youtube-dl does not include support for services that specialize in infringing copyright. As a rule of thumb, if you cannot easily find a video that the service is quite obviously allowed to distribute (i.e. that has been uploaded by the creator, the creator's distributor, or is published under a free license), the service is probably unfit for inclusion to youtube-dl.&lt;/p&gt;
&lt;p&gt;A note on the service that they don't host the infringing content, but just link to those who do, is evidence that the service should &lt;strong&gt;not&lt;/strong&gt; be included into youtube-dl. The same goes for any DMCA note when the whole front page of the service is filled with videos they are not allowed to distribute. A "fair use" note is equally unconvincing if the service shows copyright-protected videos in full without authorization.&lt;/p&gt;
&lt;p&gt;Support requests for services that &lt;strong&gt;do&lt;/strong&gt; purchase the rights to distribute their content are perfectly fine though. If in doubt, you can simply include a source that mentions the legitimate purchase of content.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-can-i-speed-up-work-on-my-issue" class="anchor" aria-hidden="true" href="#how-can-i-speed-up-work-on-my-issue"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How can I speed up work on my issue?&lt;/h3&gt;
&lt;p&gt;(Also known as: Help, my important issue not being solved!) The youtube-dl core developer team is quite small. While we do our best to solve as many issues as possible, sometimes that can take quite a while. To speed up your issue, here's what you can do:&lt;/p&gt;
&lt;p&gt;First of all, please do report the issue &lt;a href="https://yt-dl.org/bugs" rel="nofollow"&gt;at our issue tracker&lt;/a&gt;. That allows us to coordinate all efforts by users and developers, and serves as a unified point. Unfortunately, the youtube-dl project has grown too large to use personal email as an effective communication channel.&lt;/p&gt;
&lt;p&gt;Please read the &lt;a href="#bugs"&gt;bug reporting instructions&lt;/a&gt; below. A lot of bugs lack all the necessary information. If you can, offer proxy, VPN, or shell access to the youtube-dl developers. If you are able to, test the issue from multiple computers in multiple countries to exclude local censorship or misconfiguration issues.&lt;/p&gt;
&lt;p&gt;If nobody is interested in solving your issue, you are welcome to take matters into your own hands and submit a pull request (or coerce/pay somebody else to do so).&lt;/p&gt;
&lt;p&gt;Feel free to bump the issue from time to time by writing a small comment ("Issue is still present in youtube-dl version ...from France, but fixed from Belgium"), but please not more than once a month. Please do not declare your issue as &lt;code&gt;important&lt;/code&gt; or &lt;code&gt;urgent&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-can-i-detect-whether-a-given-url-is-supported-by-youtube-dl" class="anchor" aria-hidden="true" href="#how-can-i-detect-whether-a-given-url-is-supported-by-youtube-dl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How can I detect whether a given URL is supported by youtube-dl?&lt;/h3&gt;
&lt;p&gt;For one, have a look at the &lt;a href="docs/supportedsites.md"&gt;list of supported sites&lt;/a&gt;. Note that it can sometimes happen that the site changes its URL scheme (say, from &lt;a href="https://example.com/video/1234567" rel="nofollow"&gt;https://example.com/video/1234567&lt;/a&gt; to &lt;a href="https://example.com/v/1234567" rel="nofollow"&gt;https://example.com/v/1234567&lt;/a&gt; ) and youtube-dl reports an URL of a service in that list as unsupported. In that case, simply report a bug.&lt;/p&gt;
&lt;p&gt;It is &lt;em&gt;not&lt;/em&gt; possible to detect whether a URL is supported or not. That's because youtube-dl contains a generic extractor which matches &lt;strong&gt;all&lt;/strong&gt; URLs. You may be tempted to disable, exclude, or remove the generic extractor, but the generic extractor not only allows users to extract videos from lots of websites that embed a video from another service, but may also be used to extract video from a service that it's hosting itself. Therefore, we neither recommend nor support disabling, excluding, or removing the generic extractor.&lt;/p&gt;
&lt;p&gt;If you want to find out whether a given URL is supported, simply call youtube-dl with it. If you get no videos back, chances are the URL is either not referring to a video or unsupported. You can find out which by examining the output (if you run youtube-dl on the console) or catching an &lt;code&gt;UnsupportedError&lt;/code&gt; exception if you run it from a Python program.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-why-do-i-need-to-go-through-that-much-red-tape-when-filing-bugs" class="anchor" aria-hidden="true" href="#why-do-i-need-to-go-through-that-much-red-tape-when-filing-bugs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why do I need to go through that much red tape when filing bugs?&lt;/h1&gt;
&lt;p&gt;Before we had the issue template, despite our extensive &lt;a href="#bugs"&gt;bug reporting instructions&lt;/a&gt;, about 80% of the issue reports we got were useless, for instance because people used ancient versions hundreds of releases old, because of simple syntactic errors (not in youtube-dl but in general shell usage), because the problem was already reported multiple times before, because people did not actually read an error message, even if it said "please install ffmpeg", because people did not mention the URL they were trying to download and many more simple, easy-to-avoid problems, many of whom were totally unrelated to youtube-dl.&lt;/p&gt;
&lt;p&gt;youtube-dl is an open-source project manned by too few volunteers, so we'd rather spend time fixing bugs where we are certain none of those simple problems apply, and where we can be reasonably confident to be able to reproduce the issue without asking the reporter repeatedly. As such, the output of &lt;code&gt;youtube-dl -v YOUR_URL_HERE&lt;/code&gt; is really all that's required to file an issue. The issue template also guides you through some basic steps you can do, such as checking that your version of youtube-dl is current.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-developer-instructions" class="anchor" aria-hidden="true" href="#developer-instructions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DEVELOPER INSTRUCTIONS&lt;/h1&gt;
&lt;p&gt;Most users do not need to build youtube-dl and can &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html" rel="nofollow"&gt;download the builds&lt;/a&gt; or get them from their distribution.&lt;/p&gt;
&lt;p&gt;To run youtube-dl as a developer, you don't need to build anything either. Simply execute&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python -m youtube_dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run the test, simply invoke your favorite test runner, or execute a test file directly; any of the following work:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python -m unittest discover
python test/test_download.py
nosetests
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See item 6 of &lt;a href="#adding-support-for-a-new-site"&gt;new extractor tutorial&lt;/a&gt; for how to run extractor specific test cases.&lt;/p&gt;
&lt;p&gt;If you want to create a build of youtube-dl yourself, you'll need&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;python&lt;/li&gt;
&lt;li&gt;make (only GNU make is supported)&lt;/li&gt;
&lt;li&gt;pandoc&lt;/li&gt;
&lt;li&gt;zip&lt;/li&gt;
&lt;li&gt;nosetests&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-adding-support-for-a-new-site" class="anchor" aria-hidden="true" href="#adding-support-for-a-new-site"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding support for a new site&lt;/h3&gt;
&lt;p&gt;If you want to add support for a new site, first of all &lt;strong&gt;make sure&lt;/strong&gt; this site is &lt;strong&gt;not dedicated to &lt;a href="README.md#can-you-add-support-for-this-anime-video-site-or-site-which-shows-current-movies-for-free"&gt;copyright infringement&lt;/a&gt;&lt;/strong&gt;. youtube-dl does &lt;strong&gt;not support&lt;/strong&gt; such sites thus pull requests adding support for them &lt;strong&gt;will be rejected&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;After you have ensured this site is distributing its content legally, you can follow this quick list (assuming your service is called &lt;code&gt;yourextractor&lt;/code&gt;):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/ytdl-org/youtube-dl/fork"&gt;Fork this repository&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check out the source code with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; git clone git@github.com:YOUR_GITHUB_USERNAME/youtube-dl.git
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start a new git branch with&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; cd youtube-dl
 git checkout -b yourextractor
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start with this simple template and save it to &lt;code&gt;youtube_dl/extractor/yourextractor.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; coding: utf-8&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; &lt;span class="pl-c1"&gt;__future__&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; unicode_literals

&lt;span class="pl-k"&gt;from&lt;/span&gt; .common &lt;span class="pl-k"&gt;import&lt;/span&gt; InfoExtractor


&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;YourExtractorIE&lt;/span&gt;(&lt;span class="pl-e"&gt;InfoExtractor&lt;/span&gt;):
    &lt;span class="pl-c1"&gt;_VALID_URL&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https&lt;span class="pl-k"&gt;?&lt;/span&gt;://&lt;span class="pl-c1"&gt;(?:&lt;/span&gt;www&lt;span class="pl-cce"&gt;\.&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&lt;span class="pl-k"&gt;?&lt;/span&gt;yourextractor&lt;span class="pl-cce"&gt;\.&lt;/span&gt;com/watch/&lt;span class="pl-c1"&gt;(&lt;/span&gt;&lt;span class="pl-ent"&gt;?P&amp;lt;id&amp;gt;&lt;/span&gt;[&lt;span class="pl-c1"&gt;0-9&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-c1"&gt;_TEST&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; {
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;url&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https://yourextractor.com/watch/42&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;md5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;TODO: md5 sum of the first 10241 bytes of the video file (use --test)&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;info_dict&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: {
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;id&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;42&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ext&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mp4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Video title goes here&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;thumbnail&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;re:&lt;span class="pl-c1"&gt;^&lt;/span&gt;https&lt;span class="pl-k"&gt;?&lt;/span&gt;://&lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-cce"&gt;\.&lt;/span&gt;jpg&lt;span class="pl-c1"&gt;$&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; &lt;span class="pl-k"&gt;TODO&lt;/span&gt; more properties, either as:&lt;/span&gt;
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; * A value&lt;/span&gt;
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; * MD5 checksum; start the string with md5:&lt;/span&gt;
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; * A regular expression; start the string with re:&lt;/span&gt;
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; * Any Python type (for example int or float)&lt;/span&gt;
        }
    }

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;_real_extract&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;url&lt;/span&gt;):
        video_id &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._match_id(url)
        webpage &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._download_webpage(url, video_id)

        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; &lt;span class="pl-k"&gt;TODO&lt;/span&gt; more code goes here, for example ...&lt;/span&gt;
        title &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._html_search_regex(&lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;h1&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;&lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;span class="pl-k"&gt;+?&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;/h1&amp;gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

        &lt;span class="pl-k"&gt;return&lt;/span&gt; {
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;id&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: video_id,
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: title,
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;self&lt;/span&gt;._og_search_description(webpage),
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;uploader&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;self&lt;/span&gt;._search_regex(&lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;div[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;id="uploader"[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;*&lt;/span&gt;&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;uploader&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;fatal&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;),
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; &lt;span class="pl-k"&gt;TODO&lt;/span&gt; more properties (see youtube_dl/extractor/common.py)&lt;/span&gt;
        }&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add an import in &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/extractors.py"&gt;&lt;code&gt;youtube_dl/extractor/extractors.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;python test/test_download.py TestDownload.test_YourExtractor&lt;/code&gt;. This &lt;em&gt;should fail&lt;/em&gt; at first, but you can continually re-run it until you're done. If you decide to add more than one test, then rename &lt;code&gt;_TEST&lt;/code&gt; to &lt;code&gt;_TESTS&lt;/code&gt; and make it into a list of dictionaries. The tests will then be named &lt;code&gt;TestDownload.test_YourExtractor&lt;/code&gt;, &lt;code&gt;TestDownload.test_YourExtractor_1&lt;/code&gt;, &lt;code&gt;TestDownload.test_YourExtractor_2&lt;/code&gt;, etc. Note that tests with &lt;code&gt;only_matching&lt;/code&gt; key in test's dict are not counted in.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Have a look at &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py"&gt;&lt;code&gt;youtube_dl/extractor/common.py&lt;/code&gt;&lt;/a&gt; for possible helper methods and a &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303"&gt;detailed description of what your extractor should and may return&lt;/a&gt;. Add tests and code for as many as you want.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make sure your code follows &lt;a href="#youtube-dl-coding-conventions"&gt;youtube-dl coding conventions&lt;/a&gt; and check the code with &lt;a href="http://flake8.pycqa.org/en/latest/index.html#quickstart" rel="nofollow"&gt;flake8&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; $ flake8 youtube_dl/extractor/yourextractor.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make sure your code works under all &lt;a href="https://www.python.org/" rel="nofollow"&gt;Python&lt;/a&gt; versions claimed supported by youtube-dl, namely 2.6, 2.7, and 3.2+.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When the tests pass, &lt;a href="https://git-scm.com/docs/git-add" rel="nofollow"&gt;add&lt;/a&gt; the new files and &lt;a href="https://git-scm.com/docs/git-commit" rel="nofollow"&gt;commit&lt;/a&gt; them and &lt;a href="https://git-scm.com/docs/git-push" rel="nofollow"&gt;push&lt;/a&gt; the result, like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git add youtube_dl/extractor/extractors.py
$ git add youtube_dl/extractor/yourextractor.py
$ git commit -m '[yourextractor] Add new extractor'
$ git push origin yourextractor
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, &lt;a href="https://help.github.com/articles/creating-a-pull-request"&gt;create a pull request&lt;/a&gt;. We'll then review and merge it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In any case, thank you very much for your contributions!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-youtube-dl-coding-conventions" class="anchor" aria-hidden="true" href="#youtube-dl-coding-conventions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;youtube-dl coding conventions&lt;/h2&gt;
&lt;p&gt;This section introduces a guide lines for writing idiomatic, robust and future-proof extractor code.&lt;/p&gt;
&lt;p&gt;Extractors are very fragile by nature since they depend on the layout of the source data provided by 3rd party media hosters out of your control and this layout tends to change. As an extractor implementer your task is not only to write code that will extract media links and metadata correctly but also to minimize dependency on the source's layout and even to make the code foresee potential future changes and be ready for that. This is important because it will allow the extractor not to break on minor layout changes thus keeping old youtube-dl versions working. Even though this breakage issue is easily fixed by emitting a new version of youtube-dl with a fix incorporated, all the previous versions become broken in all repositories and distros' packages that may not be so prompt in fetching the update from us. Needless to say, some non rolling release distros may never receive an update at all.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-mandatory-and-optional-metafields" class="anchor" aria-hidden="true" href="#mandatory-and-optional-metafields"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mandatory and optional metafields&lt;/h3&gt;
&lt;p&gt;For extraction to work youtube-dl relies on metadata your extractor extracts and provides to youtube-dl expressed by an &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303"&gt;information dictionary&lt;/a&gt; or simply &lt;em&gt;info dict&lt;/em&gt;. Only the following meta fields in the &lt;em&gt;info dict&lt;/em&gt; are considered mandatory for a successful extraction process by youtube-dl:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;id&lt;/code&gt; (media identifier)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;title&lt;/code&gt; (media title)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;url&lt;/code&gt; (media download URL) or &lt;code&gt;formats&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In fact only the last option is technically mandatory (i.e. if you can't figure out the download location of the media the extraction does not make any sense). But by convention youtube-dl also treats &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;title&lt;/code&gt; as mandatory. Thus the aforementioned metafields are the critical data that the extraction does not make any sense without and if any of them fail to be extracted then the extractor is considered completely broken.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/ytdl-org/youtube-dl/blob/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L188-L303"&gt;Any field&lt;/a&gt; apart from the aforementioned ones are considered &lt;strong&gt;optional&lt;/strong&gt;. That means that extraction should be &lt;strong&gt;tolerant&lt;/strong&gt; to situations when sources for these fields can potentially be unavailable (even if they are always available at the moment) and &lt;strong&gt;future-proof&lt;/strong&gt; in order not to break the extraction of general purpose mandatory fields.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-example" class="anchor" aria-hidden="true" href="#example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h4&gt;
&lt;p&gt;Say you have some source dictionary &lt;code&gt;meta&lt;/code&gt; that you've fetched as JSON with HTTP request and it has a key &lt;code&gt;summary&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;meta &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._download_json(url, video_id)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Assume at this point &lt;code&gt;meta&lt;/code&gt;'s layout is:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;{
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;summary&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;some fancy summary text&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Assume you want to extract &lt;code&gt;summary&lt;/code&gt; and put it into the resulting info dict as &lt;code&gt;description&lt;/code&gt;. Since &lt;code&gt;description&lt;/code&gt; is an optional meta field you should be ready that this key may be missing from the &lt;code&gt;meta&lt;/code&gt; dict, so that you should extract it like:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; meta.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;summary&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; correct&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and not like:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; meta[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;summary&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; incorrect&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The latter will break extraction process with &lt;code&gt;KeyError&lt;/code&gt; if &lt;code&gt;summary&lt;/code&gt; disappears from &lt;code&gt;meta&lt;/code&gt; at some later time but with the former approach extraction will just go ahead with &lt;code&gt;description&lt;/code&gt; set to &lt;code&gt;None&lt;/code&gt; which is perfectly fine (remember &lt;code&gt;None&lt;/code&gt; is equivalent to the absence of data).&lt;/p&gt;
&lt;p&gt;Similarly, you should pass &lt;code&gt;fatal=False&lt;/code&gt; when extracting optional data from a webpage with &lt;code&gt;_search_regex&lt;/code&gt;, &lt;code&gt;_html_search_regex&lt;/code&gt; or similar methods, for instance:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._search_regex(
    &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;span[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;id="title"[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;*&lt;/span&gt;&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;fatal&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With &lt;code&gt;fatal&lt;/code&gt; set to &lt;code&gt;False&lt;/code&gt; if &lt;code&gt;_search_regex&lt;/code&gt; fails to extract &lt;code&gt;description&lt;/code&gt; it will emit a warning and continue extraction.&lt;/p&gt;
&lt;p&gt;You can also pass &lt;code&gt;default=&amp;lt;some fallback value&amp;gt;&lt;/code&gt;, for example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._search_regex(
    &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;span[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;id="title"[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;*&lt;/span&gt;&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;default&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On failure this code will silently continue the extraction with &lt;code&gt;description&lt;/code&gt; set to &lt;code&gt;None&lt;/code&gt;. That is useful for metafields that may or may not be present.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-provide-fallbacks" class="anchor" aria-hidden="true" href="#provide-fallbacks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Provide fallbacks&lt;/h3&gt;
&lt;p&gt;When extracting metadata try to do so from multiple sources. For example if &lt;code&gt;title&lt;/code&gt; is present in several places, try extracting from at least some of them. This makes it more future-proof in case some of the sources become unavailable.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-example-1" class="anchor" aria-hidden="true" href="#example-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h4&gt;
&lt;p&gt;Say &lt;code&gt;meta&lt;/code&gt; from the previous example has a &lt;code&gt;title&lt;/code&gt; and you are about to extract it. Since &lt;code&gt;title&lt;/code&gt; is a mandatory meta field you should end up with something like:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;title &lt;span class="pl-k"&gt;=&lt;/span&gt; meta[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If &lt;code&gt;title&lt;/code&gt; disappears from &lt;code&gt;meta&lt;/code&gt; in future due to some changes on the hoster's side the extraction would fail since &lt;code&gt;title&lt;/code&gt; is mandatory. That's expected.&lt;/p&gt;
&lt;p&gt;Assume that you have some another source you can extract &lt;code&gt;title&lt;/code&gt; from, for example &lt;code&gt;og:title&lt;/code&gt; HTML meta of a &lt;code&gt;webpage&lt;/code&gt;. In this case you can provide a fallback scenario:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;title &lt;span class="pl-k"&gt;=&lt;/span&gt; meta.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;or&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._og_search_title(webpage)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This code will try to extract from &lt;code&gt;meta&lt;/code&gt; first and if it fails it will try extracting &lt;code&gt;og:title&lt;/code&gt; from a &lt;code&gt;webpage&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-regular-expressions" class="anchor" aria-hidden="true" href="#regular-expressions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Regular expressions&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-dont-capture-groups-you-dont-use" class="anchor" aria-hidden="true" href="#dont-capture-groups-you-dont-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Don't capture groups you don't use&lt;/h4&gt;
&lt;p&gt;Capturing group must be an indication that it's used somewhere in the code. Any group that is not used must be non capturing.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-example-2" class="anchor" aria-hidden="true" href="#example-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h5&gt;
&lt;p&gt;Don't capture id attribute name here since you can't use it for anything anyway.&lt;/p&gt;
&lt;p&gt;Correct:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;(?:id|ID)=(?P&amp;lt;id&amp;gt;\d+)&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Incorrect:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;(id|ID)=(?P&amp;lt;id&amp;gt;\d+)&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-make-regular-expressions-relaxed-and-flexible" class="anchor" aria-hidden="true" href="#make-regular-expressions-relaxed-and-flexible"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Make regular expressions relaxed and flexible&lt;/h4&gt;
&lt;p&gt;When using regular expressions try to write them fuzzy, relaxed and flexible, skipping insignificant parts that are more likely to change, allowing both single and double quotes for quoted values and so on.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-example-3" class="anchor" aria-hidden="true" href="#example-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h5&gt;
&lt;p&gt;Say you need to extract &lt;code&gt;title&lt;/code&gt; from the following HTML code:&lt;/p&gt;
&lt;div class="highlight highlight-text-html-basic"&gt;&lt;pre&gt;&amp;lt;&lt;span class="pl-ent"&gt;span&lt;/span&gt; &lt;span class="pl-e"&gt;style&lt;/span&gt;=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s1"&gt;&lt;span class="pl-c1"&gt;&lt;span class="pl-c1"&gt;position&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;absolute&lt;/span&gt;; &lt;span class="pl-c1"&gt;&lt;span class="pl-c1"&gt;left&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;910&lt;span class="pl-k"&gt;px&lt;/span&gt;&lt;/span&gt;; &lt;span class="pl-c1"&gt;&lt;span class="pl-c1"&gt;width&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;90&lt;span class="pl-k"&gt;px&lt;/span&gt;&lt;/span&gt;; &lt;span class="pl-c1"&gt;&lt;span class="pl-c1"&gt;float&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;right&lt;/span&gt;; &lt;span class="pl-c1"&gt;&lt;span class="pl-c1"&gt;z-index&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;9999&lt;/span&gt;;&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-e"&gt;class&lt;/span&gt;=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;title&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&amp;gt;some fancy title&amp;lt;/&lt;span class="pl-ent"&gt;span&lt;/span&gt;&amp;gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The code for that task should look similar to:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;title &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._search_regex(
    &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;span[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;class="title"[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;*&lt;/span&gt;&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or even better:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;title &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._search_regex(
    &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;span[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;class=&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-c1"&gt;"&lt;/span&gt;&lt;span class="pl-cce"&gt;\'&lt;/span&gt;]&lt;span class="pl-c1"&gt;)&lt;/span&gt;title&lt;span class="pl-ent"&gt;\1&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;*&lt;/span&gt;&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;&lt;span class="pl-ent"&gt;?P&amp;lt;title&amp;gt;&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;group&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note how you tolerate potential changes in the &lt;code&gt;style&lt;/code&gt; attribute's value or switch from using double quotes to single for &lt;code&gt;class&lt;/code&gt; attribute:&lt;/p&gt;
&lt;p&gt;The code definitely should not look like:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;title &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._search_regex(
    &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;span style="position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;" class="title"&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;&lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;span class="pl-k"&gt;*?&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;/span&amp;gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;group&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-long-lines-policy" class="anchor" aria-hidden="true" href="#long-lines-policy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Long lines policy&lt;/h3&gt;
&lt;p&gt;There is a soft limit to keep lines of code under 80 characters long. This means it should be respected if possible and if it does not make readability and code maintenance worse.&lt;/p&gt;
&lt;p&gt;For example, you should &lt;strong&gt;never&lt;/strong&gt; split long string literals like URLs or some other often copied entities over multiple lines to fit this limit:&lt;/p&gt;
&lt;p&gt;Correct:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https://www.youtube.com/watch?v=FqZTN594JQw&amp;amp;list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Incorrect:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https://www.youtube.com/watch?v=FqZTN594JQw&amp;amp;list=&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-inline-values" class="anchor" aria-hidden="true" href="#inline-values"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inline values&lt;/h3&gt;
&lt;p&gt;Extracting variables is acceptable for reducing code duplication and improving readability of complex expressions. However, you should avoid extracting variables used only once and moving them to opposite parts of the extractor file, which makes reading the linear flow difficult.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-example-4" class="anchor" aria-hidden="true" href="#example-4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h4&gt;
&lt;p&gt;Correct:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;title &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._html_search_regex(&lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;title&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;/title&amp;gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Incorrect:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;TITLE_RE&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;title&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;/title&amp;gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ...some lines of code...&lt;/span&gt;
title &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._html_search_regex(&lt;span class="pl-c1"&gt;TITLE_RE&lt;/span&gt;, webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-collapse-fallbacks" class="anchor" aria-hidden="true" href="#collapse-fallbacks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Collapse fallbacks&lt;/h3&gt;
&lt;p&gt;Multiple fallback values can quickly become unwieldy. Collapse multiple fallback values into a single expression via a list of patterns.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-example-5" class="anchor" aria-hidden="true" href="#example-5"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h4&gt;
&lt;p&gt;Good:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._html_search_meta(
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;og:description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;twitter:description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
    webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;default&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Unwieldy:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; (
    &lt;span class="pl-c1"&gt;self&lt;/span&gt;._og_search_description(webpage, &lt;span class="pl-v"&gt;default&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;)
    &lt;span class="pl-k"&gt;or&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._html_search_meta(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, webpage, &lt;span class="pl-v"&gt;default&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;)
    &lt;span class="pl-k"&gt;or&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._html_search_meta(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;twitter:description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, webpage, &lt;span class="pl-v"&gt;default&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Methods supporting list of patterns are: &lt;code&gt;_search_regex&lt;/code&gt;, &lt;code&gt;_html_search_regex&lt;/code&gt;, &lt;code&gt;_og_search_property&lt;/code&gt;, &lt;code&gt;_html_search_meta&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-trailing-parentheses" class="anchor" aria-hidden="true" href="#trailing-parentheses"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trailing parentheses&lt;/h3&gt;
&lt;p&gt;Always move trailing parentheses after the last argument.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-example-6" class="anchor" aria-hidden="true" href="#example-6"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h4&gt;
&lt;p&gt;Correct:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;    &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ResultSet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Result&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VideoUrlSet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VideoUrl&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
    &lt;span class="pl-c1"&gt;list&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Incorrect:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;    &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ResultSet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Result&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VideoUrlSet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VideoUrl&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
    &lt;span class="pl-c1"&gt;list&lt;/span&gt;,
)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-use-convenience-conversion-and-parsing-functions" class="anchor" aria-hidden="true" href="#use-convenience-conversion-and-parsing-functions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use convenience conversion and parsing functions&lt;/h3&gt;
&lt;p&gt;Wrap all extracted numeric data into safe functions from &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/utils.py"&gt;&lt;code&gt;youtube_dl/utils.py&lt;/code&gt;&lt;/a&gt;: &lt;code&gt;int_or_none&lt;/code&gt;, &lt;code&gt;float_or_none&lt;/code&gt;. Use them for string to number conversions as well.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;url_or_none&lt;/code&gt; for safe URL processing.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;try_get&lt;/code&gt; for safe metadata extraction from parsed JSON.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;unified_strdate&lt;/code&gt; for uniform &lt;code&gt;upload_date&lt;/code&gt; or any &lt;code&gt;YYYYMMDD&lt;/code&gt; meta field extraction, &lt;code&gt;unified_timestamp&lt;/code&gt; for uniform &lt;code&gt;timestamp&lt;/code&gt; extraction, &lt;code&gt;parse_filesize&lt;/code&gt; for &lt;code&gt;filesize&lt;/code&gt; extraction, &lt;code&gt;parse_count&lt;/code&gt; for count meta fields extraction, &lt;code&gt;parse_resolution&lt;/code&gt;, &lt;code&gt;parse_duration&lt;/code&gt; for &lt;code&gt;duration&lt;/code&gt; extraction, &lt;code&gt;parse_age_limit&lt;/code&gt; for &lt;code&gt;age_limit&lt;/code&gt; extraction.&lt;/p&gt;
&lt;p&gt;Explore &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/utils.py"&gt;&lt;code&gt;youtube_dl/utils.py&lt;/code&gt;&lt;/a&gt; for more useful convenience functions.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-more-examples" class="anchor" aria-hidden="true" href="#more-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More examples&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-safely-extract-optional-description-from-parsed-json" class="anchor" aria-hidden="true" href="#safely-extract-optional-description-from-parsed-json"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Safely extract optional description from parsed JSON&lt;/h5&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; try_get(response, &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;result&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;video&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;summary&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], compat_str)&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-safely-extract-more-optional-metadata" class="anchor" aria-hidden="true" href="#safely-extract-more-optional-metadata"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Safely extract more optional metadata&lt;/h5&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;video &lt;span class="pl-k"&gt;=&lt;/span&gt; try_get(response, &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;result&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;video&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;], &lt;span class="pl-c1"&gt;dict&lt;/span&gt;) &lt;span class="pl-k"&gt;or&lt;/span&gt; {}
description &lt;span class="pl-k"&gt;=&lt;/span&gt; video.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;summary&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
duration &lt;span class="pl-k"&gt;=&lt;/span&gt; float_or_none(video.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;durationMs&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;), &lt;span class="pl-v"&gt;scale&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1000&lt;/span&gt;)
view_count &lt;span class="pl-k"&gt;=&lt;/span&gt; int_or_none(video.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;views&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;))&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-embedding-youtube-dl" class="anchor" aria-hidden="true" href="#embedding-youtube-dl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;EMBEDDING YOUTUBE-DL&lt;/h1&gt;
&lt;p&gt;youtube-dl makes the best effort to be a good command-line program, and thus should be callable from any programming language. If you encounter any problems parsing its output, feel free to &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/new"&gt;create a report&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From a Python program, you can embed youtube-dl in a more powerful fashion, like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; &lt;span class="pl-c1"&gt;__future__&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; unicode_literals
&lt;span class="pl-k"&gt;import&lt;/span&gt; youtube_dl

ydl_opts &lt;span class="pl-k"&gt;=&lt;/span&gt; {}
&lt;span class="pl-k"&gt;with&lt;/span&gt; youtube_dl.YoutubeDL(ydl_opts) &lt;span class="pl-k"&gt;as&lt;/span&gt; ydl:
    ydl.download([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https://www.youtube.com/watch?v=BaW_jenozKc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Most likely, you'll want to use various options. For a list of options available, have a look at &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/3e4cedf9e8cd3157df2457df7274d0c842421945/youtube_dl/YoutubeDL.py#L137-L312"&gt;&lt;code&gt;youtube_dl/YoutubeDL.py&lt;/code&gt;&lt;/a&gt;. For a start, if you want to intercept youtube-dl's output, set a &lt;code&gt;logger&lt;/code&gt; object.&lt;/p&gt;
&lt;p&gt;Here's a more complete example of a program that outputs only errors (and a short message after the download is finished), and downloads/converts the video to an mp3 file:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; &lt;span class="pl-c1"&gt;__future__&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; unicode_literals
&lt;span class="pl-k"&gt;import&lt;/span&gt; youtube_dl


&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;MyLogger&lt;/span&gt;(&lt;span class="pl-c1"&gt;object&lt;/span&gt;):
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;debug&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;msg&lt;/span&gt;):
        &lt;span class="pl-k"&gt;pass&lt;/span&gt;

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;warning&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;msg&lt;/span&gt;):
        &lt;span class="pl-k"&gt;pass&lt;/span&gt;

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;error&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;msg&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(msg)


&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;my_hook&lt;/span&gt;(&lt;span class="pl-smi"&gt;d&lt;/span&gt;):
    &lt;span class="pl-k"&gt;if&lt;/span&gt; d[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;status&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;finished&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Done downloading, now converting ...&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)


ydl_opts &lt;span class="pl-k"&gt;=&lt;/span&gt; {
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;format&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bestaudio/best&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;postprocessors&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [{
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;key&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;FFmpegExtractAudio&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;preferredcodec&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mp3&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;preferredquality&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;192&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    }],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;logger&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: MyLogger(),
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;progress_hooks&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [my_hook],
}
&lt;span class="pl-k"&gt;with&lt;/span&gt; youtube_dl.YoutubeDL(ydl_opts) &lt;span class="pl-k"&gt;as&lt;/span&gt; ydl:
    ydl.download([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https://www.youtube.com/watch?v=BaW_jenozKc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-bugs" class="anchor" aria-hidden="true" href="#bugs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BUGS&lt;/h1&gt;
&lt;p&gt;Bugs and suggestions should be reported at: &lt;a href="https://github.com/ytdl-org/youtube-dl/issues"&gt;https://github.com/ytdl-org/youtube-dl/issues&lt;/a&gt;. Unless you were prompted to or there is another pertinent reason (e.g. GitHub fails to accept the bug report), please do not send bug reports via personal email. For discussions, join us in the IRC channel #youtube-dl on freenode (&lt;a href="https://webchat.freenode.net/?randomnick=1&amp;amp;channels=youtube-dl" rel="nofollow"&gt;webchat&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please include the full output of youtube-dl when run with &lt;code&gt;-v&lt;/code&gt;&lt;/strong&gt;, i.e. &lt;strong&gt;add&lt;/strong&gt; &lt;code&gt;-v&lt;/code&gt; flag to &lt;strong&gt;your command line&lt;/strong&gt;, copy the &lt;strong&gt;whole&lt;/strong&gt; output and post it in the issue body wrapped in ``` for better formatting. It should look similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ youtube-dl -v &amp;lt;your command line&amp;gt;
[debug] System config: []
[debug] User config: []
[debug] Command-line args: [u'-v', u'https://www.youtube.com/watch?v=BaW_jenozKcj']
[debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251
[debug] youtube-dl version 2015.12.06
[debug] Git HEAD: 135392e
[debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2
[debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4
[debug] Proxy map: {}
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Do not post screenshots of verbose logs; only plain text is acceptable.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The output (including the first lines) contains important debugging information. Issues without the full output are often not reproducible and therefore do not get solved in short order, if ever.&lt;/p&gt;
&lt;p&gt;Please re-read your issue once again to avoid a couple of common mistakes (you can and should use this as a checklist):&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-is-the-description-of-the-issue-itself-sufficient" class="anchor" aria-hidden="true" href="#is-the-description-of-the-issue-itself-sufficient"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is the description of the issue itself sufficient?&lt;/h3&gt;
&lt;p&gt;We often get issue reports that we cannot really decipher. While in most cases we eventually get the required information after asking back multiple times, this poses an unnecessary drain on our resources. Many contributors, including myself, are also not native speakers, so we may misread some parts.&lt;/p&gt;
&lt;p&gt;So please elaborate on what feature you are requesting, or what bug you want to be fixed. Make sure that it's obvious&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What the problem is&lt;/li&gt;
&lt;li&gt;How it could be fixed&lt;/li&gt;
&lt;li&gt;How your proposed solution would look like&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If your report is shorter than two lines, it is almost certainly missing some of these, which makes it hard for us to respond to it. We're often too polite to close the issue outright, but the missing info makes misinterpretation likely. As a committer myself, I often get frustrated by these issues, since the only possible way for me to move forward on them is to ask for clarification over and over.&lt;/p&gt;
&lt;p&gt;For bug reports, this means that your report should contain the &lt;em&gt;complete&lt;/em&gt; output of youtube-dl when called with the &lt;code&gt;-v&lt;/code&gt; flag. The error message you get for (most) bugs even says so, but you would not believe how many of our bug reports do not contain this information.&lt;/p&gt;
&lt;p&gt;If your server has multiple IPs or you suspect censorship, adding &lt;code&gt;--call-home&lt;/code&gt; may be a good idea to get more diagnostics. If the error is &lt;code&gt;ERROR: Unable to extract ...&lt;/code&gt; and you cannot reproduce it from multiple countries, add &lt;code&gt;--dump-pages&lt;/code&gt; (warning: this will yield a rather large output, redirect it to the file &lt;code&gt;log.txt&lt;/code&gt; by adding &lt;code&gt;&amp;gt;log.txt 2&amp;gt;&amp;amp;1&lt;/code&gt; to your command-line) or upload the &lt;code&gt;.dump&lt;/code&gt; files you get when you add &lt;code&gt;--write-pages&lt;/code&gt; &lt;a href="https://gist.github.com/"&gt;somewhere&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Site support requests must contain an example URL&lt;/strong&gt;. An example URL is a URL you might want to download, like &lt;code&gt;https://www.youtube.com/watch?v=BaW_jenozKc&lt;/code&gt;. There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. &lt;code&gt;https://www.youtube.com/&lt;/code&gt;) is &lt;em&gt;not&lt;/em&gt; an example URL.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-are-you-using-the-latest-version" class="anchor" aria-hidden="true" href="#are-you-using-the-latest-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Are you using the latest version?&lt;/h3&gt;
&lt;p&gt;Before reporting any issue, type &lt;code&gt;youtube-dl -U&lt;/code&gt;. This should report that you're up-to-date. About 20% of the reports we receive are already fixed, but people are using outdated versions. This goes for feature requests as well.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-is-the-issue-already-documented" class="anchor" aria-hidden="true" href="#is-the-issue-already-documented"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is the issue already documented?&lt;/h3&gt;
&lt;p&gt;Make sure that someone has not already opened the issue you're trying to open. Search at the top of the window or browse the &lt;a href="https://github.com/ytdl-org/youtube-dl/search?type=Issues"&gt;GitHub Issues&lt;/a&gt; of this repository. If there is an issue, feel free to write something along the lines of "This affects me as well, with version 2015.01.01. Here is some more information on the issue: ...". While some issues may be old, a new post into them often spurs rapid activity.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-why-are-existing-options-not-enough" class="anchor" aria-hidden="true" href="#why-are-existing-options-not-enough"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why are existing options not enough?&lt;/h3&gt;
&lt;p&gt;Before requesting a new feature, please have a quick peek at &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/master/README.md#options"&gt;the list of supported options&lt;/a&gt;. Many feature requests are for features that actually exist already! Please, absolutely do show off your work in the issue report and detail how the existing similar options do &lt;em&gt;not&lt;/em&gt; solve your problem.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-is-there-enough-context-in-your-bug-report" class="anchor" aria-hidden="true" href="#is-there-enough-context-in-your-bug-report"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there enough context in your bug report?&lt;/h3&gt;
&lt;p&gt;People want to solve problems, and often think they do us a favor by breaking down their larger problems (e.g. wanting to skip already downloaded files) to a specific request (e.g. requesting us to look whether the file exists before downloading the info page). However, what often happens is that they break down the problem into two steps: One simple, and one impossible (or extremely complicated one).&lt;/p&gt;
&lt;p&gt;We are then presented with a very complicated request when the original problem could be solved far easier, e.g. by recording the downloaded video IDs in a separate file. To avoid this, you must include the greater context where it is non-obvious. In particular, every feature request that does not consist of adding support for a new site should contain a use case scenario that explains in what situation the missing feature would be useful.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-does-the-issue-involve-one-problem-and-one-problem-only" class="anchor" aria-hidden="true" href="#does-the-issue-involve-one-problem-and-one-problem-only"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Does the issue involve one problem, and one problem only?&lt;/h3&gt;
&lt;p&gt;Some of our users seem to think there is a limit of issues they can or should open. There is no limit of issues they can or should open. While it may seem appealing to be able to dump all your issues into one ticket, that means that someone who solves one of your issues cannot mark the issue as closed. Typically, reporting a bunch of issues leads to the ticket lingering since nobody wants to attack that behemoth, until someone mercifully splits the issue into multiple ones.&lt;/p&gt;
&lt;p&gt;In particular, every site support request issue should only pertain to services at one site (generally under a common domain, but always using the same backend technology). Do not request support for vimeo user videos, White house podcasts, and Google Plus pages in the same issue. Also, make sure that you don't post bug reports alongside feature requests. As a rule of thumb, a feature request does not include outputs of youtube-dl that are not immediately related to the feature at hand. Do not post reports of a network error alongside the request for a new video service.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-is-anyone-going-to-need-the-feature" class="anchor" aria-hidden="true" href="#is-anyone-going-to-need-the-feature"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is anyone going to need the feature?&lt;/h3&gt;
&lt;p&gt;Only post features that you (or an incapacitated friend you can personally talk to) require. Do not post features because they seem like a good idea. If they are really useful, they will be requested by someone who requires them.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-is-your-question-about-youtube-dl" class="anchor" aria-hidden="true" href="#is-your-question-about-youtube-dl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is your question about youtube-dl?&lt;/h3&gt;
&lt;p&gt;It may sound strange, but some bug reports we receive are completely unrelated to youtube-dl and relate to a different, or even the reporter's own, application. Please make sure that you are actually using youtube-dl. If you are using a UI for youtube-dl, report the bug to the maintainer of the actual application providing the UI. On the other hand, if your UI for youtube-dl fails in some way you believe is related to youtube-dl, by all means, go ahead and report the bug.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-copyright" class="anchor" aria-hidden="true" href="#copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;COPYRIGHT&lt;/h1&gt;
&lt;p&gt;youtube-dl is released into the public domain by the copyright holders.&lt;/p&gt;
&lt;p&gt;This README file was originally written by &lt;a href="https://github.com/dbbolton"&gt;Daniel Bolton&lt;/a&gt; and is likewise released into the public domain.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ytdl-org</author><guid isPermaLink="false">https://github.com/ytdl-org/youtube-dl</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>open-mmlab/mmdetection #17 in Python, Today</title><link>https://github.com/open-mmlab/mmdetection</link><description>&lt;p&gt;&lt;i&gt;Open MMLab Detection Toolbox and Benchmark&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-mmdetection" class="anchor" aria-hidden="true" href="#mmdetection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MMDetection&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;News&lt;/strong&gt;: We released the technical report on &lt;a href="https://arxiv.org/abs/1906.07155" rel="nofollow"&gt;ArXiv&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The master branch works with &lt;strong&gt;PyTorch 1.1&lt;/strong&gt; or higher.&lt;/p&gt;
&lt;p&gt;mmdetection is an open source object detection toolbox based on PyTorch. It is
a part of the open-mmlab project developed by &lt;a href="http://mmlab.ie.cuhk.edu.hk/" rel="nofollow"&gt;Multimedia Laboratory, CUHK&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="demo/coco_test_12510.jpg"&gt;&lt;img src="demo/coco_test_12510.jpg" alt="demo image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-major-features" class="anchor" aria-hidden="true" href="#major-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Major features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Modular Design&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We decompose the detection framework into different components and one can easily construct a customized object detection framework by combining different modules.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Support of multiple frameworks out of box&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The toolbox directly supports popular and contemporary detection frameworks, &lt;em&gt;e.g.&lt;/em&gt; Faster RCNN, Mask RCNN, RetinaNet, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;High efficiency&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;All basic bbox and mask operations run on GPUs now. The training speed is faster than or comparable to other codebases, including &lt;a href="https://github.com/facebookresearch/Detectron"&gt;Detectron&lt;/a&gt;, &lt;a href="https://github.com/facebookresearch/maskrcnn-benchmark"&gt;maskrcnn-benchmark&lt;/a&gt; and &lt;a href="https://github.com/TuSimple/simpledet"&gt;SimpleDet&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State of the art&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The toolbox stems from the codebase developed by the &lt;em&gt;MMDet&lt;/em&gt; team, who won &lt;a href="http://cocodataset.org/#detection-leaderboard" rel="nofollow"&gt;COCO Detection Challenge&lt;/a&gt; in 2018, and we keep pushing it forward.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Apart from MMDetection, we also released a library &lt;a href="https://github.com/open-mmlab/mmcv"&gt;mmcv&lt;/a&gt; for computer vision research, which is heavily depended on by this toolbox.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This project is released under the &lt;a href="LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;v1.0rc0 (27/07/2019)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implement lots of new methods and components (Mixed Precision Training, HTC, Libra R-CNN, Guided Anchoring, Empirical Attention, Mask Scoring R-CNN, Grid R-CNN (Plus), GHM, GCNet, FCOS, HRNet, Weight Standardization, etc.). Thank all collaborators!&lt;/li&gt;
&lt;li&gt;Support two additional datasets: WIDER FACE and Cityscapes.&lt;/li&gt;
&lt;li&gt;Refactoring for loss APIs and make it more flexible to adopt different losses and related hyper-parameters.&lt;/li&gt;
&lt;li&gt;Speed up multi-gpu testing.&lt;/li&gt;
&lt;li&gt;Integrate all compiling and installing in a single script.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v0.6.0 (14/04/2019)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Up to 30% speedup compared to the model zoo.&lt;/li&gt;
&lt;li&gt;Support both PyTorch stable and nightly version.&lt;/li&gt;
&lt;li&gt;Replace NMS and SigmoidFocalLoss with Pytorch CUDA extensions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v0.6rc0(06/02/2019)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Migrate to PyTorch 1.0.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v0.5.7 (06/02/2019)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add support for Deformable ConvNet v2. (Many thanks to the authors and &lt;a href="https://github.com/chengdazhi"&gt;@chengdazhi&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;This is the last release based on PyTorch 0.4.1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v0.5.6 (17/01/2019)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add support for Group Normalization.&lt;/li&gt;
&lt;li&gt;Unify RPNHead and single stage heads (RetinaHead, SSDHead) with AnchorHead.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v0.5.5 (22/12/2018)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add SSD for COCO and PASCAL VOC.&lt;/li&gt;
&lt;li&gt;Add ResNeXt backbones and detection models.&lt;/li&gt;
&lt;li&gt;Refactoring for Samplers/Assigners and add OHEM.&lt;/li&gt;
&lt;li&gt;Add VOC dataset and evaluation scripts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v0.5.4 (27/11/2018)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add SingleStageDetector and RetinaNet.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v0.5.3 (26/11/2018)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add Cascade R-CNN and Cascade Mask R-CNN.&lt;/li&gt;
&lt;li&gt;Add support for Soft-NMS in config files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v0.5.2 (21/10/2018)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add support for custom datasets.&lt;/li&gt;
&lt;li&gt;Add a script to convert PASCAL VOC annotations to the expected format.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v0.5.1 (20/10/2018)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add BBoxAssigner and BBoxSampler, the &lt;code&gt;train_cfg&lt;/code&gt; field in config files are restructured.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ConvFCRoIHead&lt;/code&gt; / &lt;code&gt;SharedFCRoIHead&lt;/code&gt; are renamed to &lt;code&gt;ConvFCBBoxHead&lt;/code&gt; / &lt;code&gt;SharedFCBBoxHead&lt;/code&gt; for consistency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-benchmark-and-model-zoo" class="anchor" aria-hidden="true" href="#benchmark-and-model-zoo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmark and model zoo&lt;/h2&gt;
&lt;p&gt;Supported methods and backbones are shown in the below table.
Results and models are available in the &lt;a href="docs/MODEL_ZOO.md"&gt;Model zoo&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align="center"&gt;ResNet&lt;/th&gt;
&lt;th align="center"&gt;ResNeXt&lt;/th&gt;
&lt;th align="center"&gt;SENet&lt;/th&gt;
&lt;th align="center"&gt;VGG&lt;/th&gt;
&lt;th align="center"&gt;HRNet&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;RPN&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fast R-CNN&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Faster R-CNN&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mask R-CNN&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cascade R-CNN&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cascade Mask R-CNN&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SSD&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RetinaNet&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GHM&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mask Scoring R-CNN&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FCOS&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Double-Head R-CNN&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Grid R-CNN (Plus)&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hybrid Task Cascade&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Libra R-CNN&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Guided Anchoring&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;☐&lt;/td&gt;
&lt;td align="center"&gt;✗&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Other features&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; DCNv2&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Group Normalization&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Weight Standardization&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; OHEM&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Soft-NMS&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Generalized Attention&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; GCNet&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Mixed Precision (FP16) Training&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;Please refer to &lt;a href="docs/INSTALL.md"&gt;INSTALL.md&lt;/a&gt; for installation and dataset preparation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-get-started" class="anchor" aria-hidden="true" href="#get-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Get Started&lt;/h2&gt;
&lt;p&gt;Please see &lt;a href="docs/GETTING_STARTED.md"&gt;GETTING_STARTED.md&lt;/a&gt; for the basic usage of MMDetection.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We appreciate all contributions to improve MMDetection. Please refer to &lt;a href=".github/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for the contributing guideline.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;MMDetection is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedbacks.
We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use this toolbox or benchmark in your research, please cite this project.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{mmdetection,
  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},
  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and
             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and
             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and
             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and
             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong
             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},
  journal= {arXiv preprint arXiv:1906.07155},
  year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;This repo is currently maintained by Kai Chen (&lt;a href="http://github.com/hellock"&gt;@hellock&lt;/a&gt;), Jiangmiao Pang (&lt;a href="https://github.com/OceanPang"&gt;@OceanPang&lt;/a&gt;), Jiaqi Wang (&lt;a href="https://github.com/myownskyW7"&gt;@myownskyW7&lt;/a&gt;) and Yuhang Cao (&lt;a href="https://github.com/yhcao6"&gt;@yhcao6&lt;/a&gt;).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>open-mmlab</author><guid isPermaLink="false">https://github.com/open-mmlab/mmdetection</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>yoyoyo-yo/Gasyori100knock #18 in Python, Today</title><link>https://github.com/yoyoyo-yo/Gasyori100knock</link><description>&lt;p&gt;&lt;i&gt;画像処理100本ノックして画像処理を画像処理して画像処理するためのもの For Japanese, English and Chinese&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-画像処理100本ノック" class="anchor" aria-hidden="true" href="#画像処理100本ノック"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;画像処理100本ノック!!&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;English is here&lt;/strong&gt;&lt;/em&gt; (KuKuXia translates into English)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/KuKuXia/Image_Processing_100_Questions"&gt;https://github.com/KuKuXia/Image_Processing_100_Questions&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Chinese is here&lt;/strong&gt;&lt;/em&gt;  (gzr2017, my ex-colleague, translates into Chinese)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/gzr2017/ImageProcessing100Wen"&gt;https://github.com/gzr2017/ImageProcessing100Wen&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description&lt;/h2&gt;
&lt;p&gt;画像処理の初学者のための問題１００問ですうう(´；ω；｀)&lt;/p&gt;
&lt;p&gt;これはイモリと一緒に画像処理の基本的処理の知識を身に着け、アルゴリズムを理解するための100本ノックです。ここに載っている問題はOpenCVでAPIが用意されているものが殆どですが、&lt;strong&gt;あえてそれを自分の手で実装&lt;/strong&gt;してください。解答も載っけてますが、それはあくまで解答です。自分で考えながら実装して下さい。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;問題の難易度の順番はめちゃくちゃです。なるべくポピュラーなものを採用していますが、ネタ切れであんまり聞かないものもあります笑&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;内容はいろいろな文献を調べて載っけてるので正しくないものもあるかもしれないので注意して下さい&lt;/strong&gt; まちがいがあったらプルリクもらえるととても助かります！！&lt;/li&gt;
&lt;li&gt;【注意】このページを利用して、または関して生じた事に関しては、私は一切責任を負いません。すべて&lt;strong&gt;自己責任&lt;/strong&gt;でお願い致します。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PythonとC++の好きな方でやってみてね♡（最近Javascriptも追加してるよ）&lt;/p&gt;
&lt;p&gt;2019.5.14. これ金にならんかなぁ…&lt;/p&gt;
&lt;p&gt;意見や使用実績などあればご一報ください！&lt;/p&gt;
&lt;p&gt;もしこれがみなさんのお役に立ったら寄付や募金なども受け付けてます笑&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-related" class="anchor" aria-hidden="true" href="#related"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;ディープラーニングのノックはこっち&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/yoyoyo-yo/DeepLearningMugenKnock"&gt;ディープラーニング∞本ノック!!&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-recent" class="anchor" aria-hidden="true" href="#recent"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recent&lt;/h2&gt;
&lt;p&gt;Twitterで更新を発信してますぅ&lt;/p&gt;
&lt;p&gt;&lt;a href="https://twitter.com/curry_yoyoyo" rel="nofollow"&gt;https://twitter.com/curry_yoyoyo&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2019.10.27 [C++] Q.44~46 Hough直線検出を追加、[Python]の解答を修正&lt;/li&gt;
&lt;li&gt;2019.10.22 [C++] Q.41~43 Cannyのエッジ検出を追加, [Python] の解答を修正&lt;/li&gt;
&lt;li&gt;2019.9.3 [Python] Q.81~100のAnswerコードをメソッド化&lt;/li&gt;
&lt;li&gt;2019.9.2 [Python] Q.61~80のAnswerコードをメソッド化&lt;/li&gt;
&lt;li&gt;2019.8.28 [Python] Q.51~60のAnswerコードをメソッド化&lt;/li&gt;
&lt;li&gt;2019.8.18 [Python] Q.50までのAnswerコードをメソッド化&lt;/li&gt;
&lt;li&gt;2019.8.12 [C++]Q.36-40の解答追加&lt;/li&gt;
&lt;li&gt;2019.7.32 [C++]Q.32-35の解答追加&lt;/li&gt;
&lt;li&gt;2019.7.23 [C++]Q.30-31の解答追加&lt;/li&gt;
&lt;li&gt;2019.7.22 [C++]Q.25-29の解答追加&lt;/li&gt;
&lt;li&gt;2019.6.30 Q.21-24のC++の解答追加&lt;/li&gt;
&lt;li&gt;2019.6.8 JavaScriptのチュートリアルを追加&lt;/li&gt;
&lt;li&gt;2019    Q.11-20 C++ を追加　Q.15 Sobelを修正&lt;/li&gt;
&lt;li&gt;2019.3.25 Q.31 フーリエ系 Q.36 DCT,  Q.47,48 トップハット変換系を修正&lt;/li&gt;
&lt;li&gt;2019.3.13 Q95-100 Neural Networkを修正&lt;/li&gt;
&lt;li&gt;2019.3.8 Questions_01_10 にC++の解答を追加！&lt;/li&gt;
&lt;li&gt;2019.3.7 TutorialにC++用を追加　そろそろC++用の答えもつくろっかなーと&lt;/li&gt;
&lt;li&gt;2019.3.5 各Questionの答えをanswersディレクトリに収納&lt;/li&gt;
&lt;li&gt;2019.3.3 Q.18-22. 一部修正&lt;/li&gt;
&lt;li&gt;2019.2.26 Q.10. メディアンフィルタの解答を一部修正&lt;/li&gt;
&lt;li&gt;2019.2.25 Q.9. ガウシアンフィルタの解答を一部修正&lt;/li&gt;
&lt;li&gt;2019.2.23 Q.6. 減色処理のREADMEを修正&lt;/li&gt;
&lt;li&gt;2019.1.29 HSVを修正&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-まずは" class="anchor" aria-hidden="true" href="#まずは"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;まずは&lt;/h2&gt;
&lt;p&gt;gitをインストールします。そして、端末を開いて、このコマンドを実行します。このコマンドでこのディレクトリを丸ごと自分のパソコンにコピーできます。&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/yoyoyo-yo/Gasyori100knock.git&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;そしたら、PythonかC++のやりたい方を選んで「&lt;strong&gt;チュートリアル&lt;/strong&gt;」に進みましょう！&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-チュートリアル" class="anchor" aria-hidden="true" href="#チュートリアル"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Tutorial"&gt;チュートリアル&lt;/a&gt;&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;内容&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;th align="center"&gt;JavaScript&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;1&lt;/td&gt;
&lt;td align="center"&gt;インストール&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Tutorial"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/Tutorial/README_opencv_c_install.md"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/Tutorial/README_javascript.md"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td align="center"&gt;画像読み込み・表示&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Tutorial#%E7%94%BB%E5%83%8F%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%81%BF%E8%A1%A8%E7%A4%BA"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/Tutorial/README_opencv_c_install.md#%E7%94%BB%E5%83%8F%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%81%BF"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/Tutorial/"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td align="center"&gt;画素をいじる&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Tutorial#%E7%94%BB%E7%B4%A0%E3%82%92%E3%81%84%E3%81%98%E3%82%8B"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/Tutorial/README_opencv_c_install.md#%E7%94%BB%E7%B4%A0%E3%82%92%E3%81%84%E3%81%98%E3%82%8B"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/Tutorial/README_javascript.md#%E7%94%BB%E7%B4%A0%E3%82%92%E3%81%84%E3%81%98%E3%82%8B"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;画像のコピー&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Tutorial#%E7%94%BB%E5%83%8F%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/Tutorial/README_opencv_c_install.md#%E7%94%BB%E5%83%8F%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/Tutorial/README_javascript.md#%E7%94%BB%E5%83%8F%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;5&lt;/td&gt;
&lt;td align="center"&gt;画像の保存&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Tutorial#%E7%94%BB%E5%83%8F%E3%81%AE%E4%BF%9D%E5%AD%98"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/Tutorial/README_opencv_c_install.md#%E7%94%BB%E5%83%8F%E3%81%AE%E4%BF%9D%E5%AD%98"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;ー&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;6&lt;/td&gt;
&lt;td align="center"&gt;練習問題&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Tutorial#%E7%B7%B4%E7%BF%92"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/Tutorial/README_opencv_c_install.md#%E7%B7%B4%E7%BF%92%E5%95%8F%E9%A1%8C"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/Tutorial/README_javascript.md#%E7%B7%B4%E7%BF%92"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a href="Image_processing_tips.ipynb"&gt;MatplotlibとかOpenCVのTips&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;あとは問題を解いていってください。それぞれのフォルダに問題内容が入っています。問題では assets/imori.jpg を使用して下さい。各フォルダのREADME.mdに問題、解答プログラムがあります。&lt;code&gt;python answers/answer_@@.py&lt;/code&gt;　とすると解答が出ます。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-問題" class="anchor" aria-hidden="true" href="#問題"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;問題&lt;/h2&gt;
&lt;p&gt;詳細な問題内容は各ディレクトリのREADMEにあります。（ディレクトリで下にスクロールすればあります）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;numpy中心ですが、numpyの基本知識は自分で調べて下さい。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-問題1---10" class="anchor" aria-hidden="true" href="#問題1---10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Question_01_10"&gt;問題1 - 10&lt;/a&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;1&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10#q1-%E3%83%81%E3%83%A3%E3%83%8D%E3%83%AB%E5%85%A5%E3%82%8C%E6%9B%BF%E3%81%88"&gt;チャネル入れ替え&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_py/answer_1.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_cpp/answer_1.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;6&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10#q6-%E6%B8%9B%E8%89%B2%E5%87%A6%E7%90%86"&gt;減色処理&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_py/answer_6.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_cpp/answer_6.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10#q2-%E3%82%B0%E3%83%AC%E3%83%BC%E3%82%B9%E3%82%B1%E3%83%BC%E3%83%AB%E5%8C%96"&gt;グレースケール化&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_py/answer_2.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_cpp/answer_2.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;7&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10#q7-%E5%B9%B3%E5%9D%87%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0"&gt;平均プーリング&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_py/answer_7.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_cpp/answer_7.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10#q3-%E4%BA%8C%E5%80%A4%E5%8C%96"&gt;二値化&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_py/answer_3.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_cpp/answer_3.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;8&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10#q8-max%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0"&gt;Maxプーリング&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_py/answer_8.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_cpp/answer_8.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10#q4-%E5%A4%A7%E6%B4%A5%E3%81%AE%E4%BA%8C%E5%80%A4%E5%8C%96"&gt;大津の二値化&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_py/answer_4.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_cpp/answer_4.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;9&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10#q9-%E3%82%AC%E3%82%A6%E3%82%B7%E3%82%A2%E3%83%B3%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;ガウシアンフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_py/answer_9.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_cpp/answer_9.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;5&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10#q5-hsv%E5%A4%89%E6%8F%9B"&gt;HSV変換&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_py/answer_5.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_cpp/answer_5.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;10&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10#q10-%E3%83%A1%E3%83%87%E3%82%A3%E3%82%A2%E3%83%B3%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;メディアンフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_py/answer_10.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_01_10/answers_cpp/answer_10.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-問題11---20" class="anchor" aria-hidden="true" href="#問題11---20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Question_11_20"&gt;問題11 - 20&lt;/a&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;11&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20#q11-%E5%B9%B3%E6%BB%91%E5%8C%96%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;平滑化フィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_py/answer_11.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_cpp/answer_11.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;16&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20#q16-prewitt%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;Prewittフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_py/answer_16.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_cpp/answer_16.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;12&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20#q12-%E3%83%A2%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;モーションフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_py/answer_12.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_cpp/answer_12.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;17&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20#q17-laplacian%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;Laplacianフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_py/answer_17.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_cpp/answer_17.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;13&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20#q13-max-min%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;MAX-MINフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_py/answer_13.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_cpp/answer_13.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;18&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20#q18-emboss%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;Embossフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_py/answer_18.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_cpp/answer_18.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;14&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20#q14-%E5%BE%AE%E5%88%86%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;微分フィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_py/answer_14.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_cpp/answer_14.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;19&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20#q19-log%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;LoGフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_py/answer_19.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_cpp/answer_19.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;15&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20#q15-sobel%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;Sobelフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_py/answer_15.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_cpp/answer_15.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;20&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20#q20-%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E8%A1%A8%E7%A4%BA"&gt;ヒストグラム表示&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_11_20/answers_py/answer_20.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-問題21---30" class="anchor" aria-hidden="true" href="#問題21---30"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Question_21_30"&gt;問題21 - 30&lt;/a&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;21&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30#q21-%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E6%AD%A3%E8%A6%8F%E5%8C%96"&gt;ヒストグラム正規化&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_py/answer_21.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_cpp/answer_21.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;26&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30#q26-bi-linear%E8%A3%9C%E9%96%93"&gt;Bi-linear補間&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_py/answer_26.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_cpp/answer_26.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;22&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30#q22-%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E6%93%8D%E4%BD%9C"&gt;ヒストグラム操作&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_py/answer_22.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_cpp/answer_22.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;27&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30#q27-bi-cubic%E8%A3%9C%E9%96%93"&gt;Bi-cubic補間&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_py/answer_27.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_cpp/answer_27.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;23&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30#q23-%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E5%B9%B3%E5%9D%A6%E5%8C%96"&gt;ヒストグラム平坦化&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_py/answer_23.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_cpp/answer_23.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;28&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30#q28-%E3%82%A2%E3%83%95%E3%82%A3%E3%83%B3%E5%A4%89%E6%8F%9B%E5%B9%B3%E8%A1%8C%E7%A7%BB%E5%8B%95"&gt;アフィン変換(平行移動)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_py/answer_28.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_cpp/answer_28.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;24&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30#q24-%E3%82%AC%E3%83%B3%E3%83%9E%E8%A3%9C%E6%AD%A3"&gt;ガンマ補正&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_py/answer_24.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_cpp/answer_24.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;29&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30#q29-%E3%82%A2%E3%83%95%E3%82%A3%E3%83%B3%E5%A4%89%E6%8F%9B%E6%8B%A1%E5%A4%A7%E7%B8%AE%E5%B0%8F"&gt;アフィン変換(拡大縮小)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_py/answer_29.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_cpp/answer_29.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;25&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30#q25-%E6%9C%80%E8%BF%91%E5%82%8D%E8%A3%9C%E9%96%93"&gt;最近傍補間&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_py/answer_25.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_cpp/answer_25.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;30&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30#q30-%E3%82%A2%E3%83%95%E3%82%A3%E3%83%B3%E5%A4%89%E6%8F%9B%E5%9B%9E%E8%BB%A2"&gt;アフィン変換(回転)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_py/answer_30.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_21_30/answers_cpp/answer_30.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-問題31---40" class="anchor" aria-hidden="true" href="#問題31---40"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Question_31_40"&gt;問題31 - 40&lt;/a&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;31&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40#q31-%E3%82%A2%E3%83%95%E3%82%A3%E3%83%B3%E5%A4%89%E6%8F%9B%E3%82%B9%E3%82%AD%E3%83%A5%E3%83%BC"&gt;アフィン変換(スキュー)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_py/answer_31.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_cpp/answer_31.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;36&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40#q36-jpeg%E5%9C%A7%E7%B8%AE-step1%E9%9B%A2%E6%95%A3%E3%82%B3%E3%82%B5%E3%82%A4%E3%83%B3%E5%A4%89%E6%8F%9B"&gt;JPEG圧縮 (Step.1)離散コサイン変換&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_py/answer_36.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_cpp/answer_36.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;32&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40#q32-%E3%83%95%E3%83%BC%E3%83%AA%E3%82%A8%E5%A4%89%E6%8F%9B"&gt;フーリエ変換&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_py/answer_31.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_cpp/answer_32.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;37&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40#q37-psnr"&gt;PSNR&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_py/answer_37.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_cpp/answer_37.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;33&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40#q33-%E3%83%95%E3%83%BC%E3%83%AA%E3%82%A8%E5%A4%89%E6%8F%9B%E3%83%AD%E3%83%BC%E3%83%91%E3%82%B9%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;フーリエ変換 ローパスフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_py/answer_33.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_cpp/answer_33.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;38&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40#q38-jpeg%E5%9C%A7%E7%B8%AE-step2dct%E9%87%8F%E5%AD%90%E5%8C%96"&gt;JPEG圧縮 (Step.2)DCT+量子化&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_py/answer_38.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_cpp/answer_38.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;34&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40#q34-%E3%83%95%E3%83%BC%E3%83%AA%E3%82%A8%E5%A4%89%E6%8F%9B%E3%83%8F%E3%82%A4%E3%83%91%E3%82%B9%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;フーリエ変換 ハイパスフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_py/answer_34.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_cpp/answer_34.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;39&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40#q39-jpeg%E5%9C%A7%E7%B8%AE-step3ycbcr%E8%A1%A8%E8%89%B2%E7%B3%BB"&gt;JPEG圧縮 (Step.3)YCbCr表色系&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_py/answer_39.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_cpp/answer_39.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;35&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40#q35-%E3%83%95%E3%83%BC%E3%83%AA%E3%82%A8%E5%A4%89%E6%8F%9B%E3%83%90%E3%83%B3%E3%83%89%E3%83%91%E3%82%B9%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;フーリエ変換 バンドパスフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_py/answer_35.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_cpp/answer_35.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;40&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40#q40-jpeg%E5%9C%A7%E7%B8%AE-step4ycbcrdct%E9%87%8F%E5%AD%90%E5%8C%96"&gt;JPEG圧縮 (Step.4)YCbCr+DCT+量子化&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_py/answer_40.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_31_40/answers_cpp/answer_40.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-問題41---50" class="anchor" aria-hidden="true" href="#問題41---50"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Question_41_50"&gt;問題41 - 50&lt;/a&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;41&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50#q41-canny%E3%82%A8%E3%83%83%E3%82%B8%E6%A4%9C%E5%87%BA-step1-%E3%82%A8%E3%83%83%E3%82%B8%E5%BC%B7%E5%BA%A6"&gt;Cannyエッジ検出 (Step.1) エッジ強度&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_py/answer_41.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_cpp/answer_41.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;46&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50#q46-hough%E5%A4%89%E6%8F%9B%E7%9B%B4%E7%B7%9A%E6%A4%9C%E5%87%BA-step3-hough%E9%80%86%E5%A4%89%E6%8F%9B"&gt;Hough変換・直線検出 (Step.3) Hough逆変換&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_py/answer_46.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_cpp/answer_46.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;42&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50#q42-canny%E3%82%A8%E3%83%83%E3%82%B8%E6%A4%9C%E5%87%BA-step2-%E7%B4%B0%E7%B7%9A%E5%8C%96"&gt;Cannyエッジ検出 (Step.2) 細線化&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_py/answer_42.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_cpp/answer_42.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;47&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50#q47-%E3%83%A2%E3%83%AB%E3%83%95%E3%82%A9%E3%83%AD%E3%82%B8%E3%83%BC%E5%87%A6%E7%90%86%E8%86%A8%E5%BC%B5"&gt;モルフォロジー処理(膨張)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_py/answer_47.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;43&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50#q43-canny%E3%82%A8%E3%83%83%E3%82%B8%E6%A4%9C%E5%87%BA-step3-%E3%83%92%E3%82%B9%E3%83%86%E3%83%AA%E3%82%B7%E3%82%B9%E9%96%BE%E5%87%A6%E7%90%86"&gt;Cannyエッジ検出 (Step.3) ヒステリシス閾処理&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_py/answer_43.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_cpp/answer_43.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;48&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50#q48-%E3%83%A2%E3%83%AB%E3%83%95%E3%82%A9%E3%83%AD%E3%82%B8%E3%83%BC%E5%87%A6%E7%90%86%E5%8F%8E%E7%B8%AE"&gt;モルフォロジー処理(収縮)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_py/answer_48.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;44&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50#q44-hough%E5%A4%89%E6%8F%9B%E7%9B%B4%E7%B7%9A%E6%A4%9C%E5%87%BA-step1-hough%E5%A4%89%E6%8F%9B"&gt;Hough変換・直線検出 (Step.1) Hough変換&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_py/answer_44.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_cpp/answer_44.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;49&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50#q49-%E3%82%AA%E3%83%BC%E3%83%97%E3%83%8B%E3%83%B3%E3%82%B0%E5%87%A6%E7%90%86"&gt;オープニング処理&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_py/answer_49.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;45&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50#q45-hough%E5%A4%89%E6%8F%9B%E7%9B%B4%E7%B7%9A%E6%A4%9C%E5%87%BA-step2-nms"&gt;Hough変換・直線検出 (Step.2) NMS&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_py/answer_45.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_cpp/answer_45.cpp"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;50&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50#q50-%E3%82%AF%E3%83%AD%E3%83%BC%E3%82%B8%E3%83%B3%E3%82%B0%E5%87%A6%E7%90%86"&gt;クロージング処理&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_41_50/answers_py/answer_50.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-問題51---60" class="anchor" aria-hidden="true" href="#問題51---60"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Question_51_60"&gt;問題51 - 60&lt;/a&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;51&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60#q51-%E3%83%A2%E3%83%AB%E3%83%95%E3%82%A9%E3%83%AD%E3%82%B8%E3%83%BC%E5%8B%BE%E9%85%8D"&gt;モルフォロジー勾配&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60/answers/answer_51.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;56&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60#q56-%E3%83%86%E3%83%B3%E3%83%97%E3%83%AC%E3%83%BC%E3%83%88%E3%83%9E%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0-ncc"&gt;テンプレートマッチング NCC&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60/answers/answer_56.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;52&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60#q52-%E3%83%88%E3%83%83%E3%83%97%E3%83%8F%E3%83%83%E3%83%88%E5%A4%89%E6%8F%9B"&gt;トップハット変換&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60/answers/answer_52.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;57&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60#q57-%E3%83%86%E3%83%B3%E3%83%97%E3%83%AC%E3%83%BC%E3%83%88%E3%83%9E%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0-zncc"&gt;テンプレートマッチング ZNCC&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60/answers/answer_57.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;53&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60#q53-%E3%83%96%E3%83%A9%E3%83%83%E3%82%AF%E3%83%8F%E3%83%83%E3%83%88%E5%A4%89%E6%8F%9B"&gt;ブラックハット変換&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60/answers/answer_53.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;58&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60#q58-%E3%83%A9%E3%83%99%E3%83%AA%E3%83%B3%E3%82%B0-4%E8%BF%91%E5%82%8D"&gt;ラベリング 4近傍&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60/answers/answer_58.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;54&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60#q54-%E3%83%86%E3%83%B3%E3%83%97%E3%83%AC%E3%83%BC%E3%83%88%E3%83%9E%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0-ssd"&gt;テンプレートマッチング SSD&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60/answers/answer_54.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;59&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60#q59-%E3%83%A9%E3%83%99%E3%83%AA%E3%83%B3%E3%82%B0-8%E8%BF%91%E5%82%8D"&gt;ラベリング 8近傍&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60/answers/answer_59.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;55&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60#q55-%E3%83%86%E3%83%B3%E3%83%97%E3%83%AC%E3%83%BC%E3%83%88%E3%83%9E%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0-sad"&gt;テンプレートマッチング SAD&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60/answers/answer_55.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;60&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60#q60-%E3%82%A2%E3%83%AB%E3%83%95%E3%82%A1%E3%83%96%E3%83%AC%E3%83%B3%E3%83%89"&gt;アルファブレンド&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_51_60/answers/answer_60.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-問題61---70" class="anchor" aria-hidden="true" href="#問題61---70"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Question_61_70"&gt;問題61 - 70&lt;/a&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;61&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70#q61-4-%E9%80%A3%E7%B5%90%E6%95%B0"&gt;4-連結数&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70/answers/answer_61.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;66&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70#q66-hog-step1-%E5%8B%BE%E9%85%8D%E5%BC%B7%E5%BA%A6%E5%8B%BE%E9%85%8D%E8%A7%92%E5%BA%A6"&gt;HOG (Step.1) 勾配強度・勾配角度&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70/answers/answer_66.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;62&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70#q62-8-%E9%80%A3%E7%B5%90%E6%95%B0"&gt;8-連結数&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70/answers/answer_62.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;67&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70#q67-hog-step2-%E5%8B%BE%E9%85%8D%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0"&gt;HOG (Step.2) 勾配ヒストグラム&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70/answers/answer_67.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;63&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70#q63-%E7%B4%B0%E7%B7%9A%E5%8C%96%E5%87%A6%E7%90%86"&gt;細線化&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70/answers/answer_63.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;68&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70#q68-hog-step3-%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E6%AD%A3%E8%A6%8F%E5%8C%96"&gt;HOG (Step.3) ヒストグラム正規化&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70/answers/answer_68.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;64&lt;strong&gt;未&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70#q64-%E3%83%92%E3%83%AB%E3%83%87%E3%82%A3%E3%83%83%E3%83%81%E3%81%AE%E7%B4%B0%E7%B7%9A%E5%8C%96"&gt;ヒルディッチの細線化&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;69&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70#q69-hog-step4-%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E6%8F%8F%E7%94%BB"&gt;HOG (Step.4) 特徴量の描画&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70/answers/answer_69.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;65&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70#q65-zhang-suen%E3%81%AE%E7%B4%B0%E7%B7%9A%E5%8C%96"&gt;Zhang-Suenの細線化&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70/answers/answer_65.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;70&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70#q70-%E3%82%AB%E3%83%A9%E3%83%BC%E3%83%88%E3%83%A9%E3%83%83%E3%82%AD%E3%83%B3%E3%82%B0"&gt;カラートラッキング&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_61_70/answers/answer_70.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-問題71---80" class="anchor" aria-hidden="true" href="#問題71---80"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Question_71_80"&gt;問題71 - 80&lt;/a&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;71&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80#q71-%E3%83%9E%E3%82%B9%E3%82%AD%E3%83%B3%E3%82%B0"&gt;マスキング&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80/answers/answer_71.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;76&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80#q76-%E9%A1%95%E8%91%97%E6%80%A7%E3%83%9E%E3%83%83%E3%83%97"&gt;顕著性マップ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80/answers/answer_76.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;72&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80#q72-%E3%83%9E%E3%82%B9%E3%82%AD%E3%83%B3%E3%82%B0%E3%82%AB%E3%83%A9%E3%83%BC%E3%83%88%E3%83%A9%E3%83%83%E3%82%AD%E3%83%B3%E3%82%B0%E3%83%A2%E3%83%AB%E3%83%95%E3%82%A9%E3%83%AD%E3%82%B8%E3%83%BC"&gt;マスキング(カラートラッキングとモルフォロジー)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80/answers/answer_72.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;77&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80#q77-%E3%82%AC%E3%83%9C%E3%83%BC%E3%83%AB%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF"&gt;ガボールフィルタ&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80/answers/answer_77.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;73&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80#q73-%E7%B8%AE%E5%B0%8F%E3%81%A8%E6%8B%A1%E5%A4%A7"&gt;縮小と拡大&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80/answers/answer_73.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;78&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80#q78-%E3%82%AC%E3%83%9C%E3%83%BC%E3%83%AB%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AE%E5%9B%9E%E8%BB%A2"&gt;ガボールフィルタの回転&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80/answers/answer_78.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;74&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80#q74-%E3%83%94%E3%83%A9%E3%83%9F%E3%83%83%E3%83%89%E5%B7%AE%E5%88%86%E3%81%AB%E3%82%88%E3%82%8B%E9%AB%98%E5%91%A8%E6%B3%A2%E6%88%90%E5%88%86%E3%81%AE%E6%8A%BD%E5%87%BA"&gt;ピラミッド差分による高周波成分の抽出&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80/answers/answer_74.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;79&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80#q79-%E3%82%AC%E3%83%9C%E3%83%BC%E3%83%AB%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AB%E3%82%88%E3%82%8B%E3%82%A8%E3%83%83%E3%82%B8%E6%8A%BD%E5%87%BA"&gt;ガボールフィルタによるエッジ抽出&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80/answers/answer_79.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;75&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80#q75-%E3%82%AC%E3%82%A6%E3%82%B7%E3%82%A2%E3%83%B3%E3%83%94%E3%83%A9%E3%83%9F%E3%83%83%E3%83%89"&gt;ガウシアンピラミッド&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80/answers/answer_75.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;80&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80#q80-%E3%82%AC%E3%83%9C%E3%83%BC%E3%83%AB%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AB%E3%82%88%E3%82%8B%E7%89%B9%E5%BE%B4%E6%8A%BD%E5%87%BA"&gt;ガボールフィルタによる特徴抽出&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_71_80/answers/answer_80.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-問題81---90" class="anchor" aria-hidden="true" href="#問題81---90"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Question_81_90"&gt;問題81 - 90&lt;/a&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;81&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90#q81-hessian%E3%81%AE%E3%82%B3%E3%83%BC%E3%83%8A%E3%83%BC%E6%A4%9C%E5%87%BA"&gt;Hessianのコーナー検出&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90/answers/answer_81.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;86&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90#q86-%E7%B0%A1%E5%8D%98%E3%81%AA%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98-step3-%E8%A9%95%E4%BE%A1accuracy"&gt;簡単な画像認識 (Step.3) 評価(Accuracy)&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90/answers/answer_86.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;82&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90#q82-harris%E3%81%AE%E3%82%B3%E3%83%BC%E3%83%8A%E3%83%BC%E6%A4%9C%E5%87%BA-step1-sobel--gauusian"&gt;Harrisのコーナー検出 (Step.1) Sobel + Gaussian&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90/answers/answer_82.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;87&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90#q87-%E7%B0%A1%E5%8D%98%E3%81%AA%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98-step4-k-nn"&gt;簡単な画像認識 (Step.4) k-NN&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90/answers/answer_87.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;83&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90#q83-harris%E3%81%AE%E3%82%B3%E3%83%BC%E3%83%8A%E3%83%BC%E6%A4%9C%E5%87%BA-step2-%E3%82%B3%E3%83%BC%E3%83%8A%E3%83%BC%E6%A4%9C%E5%87%BA"&gt;Harrisのコーナー検出 (Step.2) コーナー検出&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90/answers/answer_83.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;88&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90#q88-k-means-step1-%E9%87%8D%E5%BF%83%E4%BD%9C%E6%88%90"&gt;K-means (Step.1) 重心作成&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90/answers/answer_88.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;84&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90#q84-%E7%B0%A1%E5%8D%98%E3%81%AA%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98-step1-%E6%B8%9B%E8%89%B2%E5%8C%96--%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0"&gt;簡単な画像認識 (Step.1) 減色化 + ヒストグラム&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90/answers/answer_84.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;89&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90#q89-k-means-step2-%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%AA%E3%83%B3%E3%82%B0"&gt;K-means (Step.2) クラスタリング&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90/answers/answer_89.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;85&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90#q85-%E7%B0%A1%E5%8D%98%E3%81%AA%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98-step2-%E3%82%AF%E3%83%A9%E3%82%B9%E5%88%A4%E5%88%A5"&gt;簡単な画像認識 (Step.2) クラス判別&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90/answers/answer_85.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;90&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90#q90-k-means-step3-%E5%88%9D%E6%9C%9F%E3%83%A9%E3%83%99%E3%83%AB%E3%81%AE%E5%A4%89%E6%9B%B4"&gt;K-means (Step.3) 初期ラベルの変更&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_81_90/answers/answer_90.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-問題91---100" class="anchor" aria-hidden="true" href="#問題91---100"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Question_91_100"&gt;問題91 - 100&lt;/a&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;番号&lt;/th&gt;
&lt;th align="center"&gt;問題&lt;/th&gt;
&lt;th align="center"&gt;Python&lt;/th&gt;
&lt;th align="center"&gt;C++&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;91&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100#q91-k-means%E3%81%AB%E3%82%88%E3%82%8B%E6%B8%9B%E8%89%B2%E5%87%A6%E7%90%86-step1-%E8%89%B2%E3%81%AE%E8%B7%9D%E9%9B%A2%E3%81%AB%E3%82%88%E3%82%8B%E3%82%AF%E3%83%A9%E3%82%B9%E5%88%86%E9%A1%9E"&gt;K-meansによる減色処理 (Step.1) 色の距離によるクラス分類&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100/answers/answer_91.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;96&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100#q96-%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF-step2-%E5%AD%A6%E7%BF%92"&gt;ニューラルネットワーク (Step.2) 学習&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100/answers/answer_96.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;92&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100#q92-k-means%E3%81%AB%E3%82%88%E3%82%8B%E6%B8%9B%E8%89%B2%E5%87%A6%E7%90%86-step2-%E6%B8%9B%E8%89%B2%E5%87%A6%E7%90%86"&gt;K-meansによる減色処理 (Step.2) 減色処理&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100/answers/answer_92.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;97&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100#q97-%E7%B0%A1%E5%8D%98%E3%81%AA%E7%89%A9%E4%BD%93%E6%A4%9C%E5%87%BA-step1-%E3%82%B9%E3%83%A9%E3%82%A4%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%89%E3%82%A6--hog"&gt;簡単な物体検出 (Step.1) スライディングウィンドウ + HOG&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100/answers/answer_97.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;93&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100#q93-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E5%AD%A6%E7%BF%92%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E7%94%A8%E6%84%8F-step1-iou%E3%81%AE%E8%A8%88%E7%AE%97"&gt;機械学習の学習データの用意 (Step.1) IoUの計算&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100/answers/answer_93.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;98&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100#q98-%E7%B0%A1%E5%8D%98%E3%81%AA%E7%89%A9%E4%BD%93%E6%A4%9C%E5%87%BA-step2-%E3%82%B9%E3%83%A9%E3%82%A4%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%89%E3%82%A6--nn"&gt;簡単な物体検出 (Step.2) スライディングウィンドウ + NN&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100/answers/answer_98.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;94&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100#q94-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E5%AD%A6%E7%BF%92%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E7%94%A8%E6%84%8F-step2-%E3%83%A9%E3%83%B3%E3%83%80%E3%83%A0%E3%82%AF%E3%83%A9%E3%83%83%E3%83%94%E3%83%B3%E3%82%B0"&gt;機械学習の学習データの用意 (Step.2) ランダムクラッピング&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100/answers/answer_94.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;99&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100#q99-%E7%B0%A1%E5%8D%98%E3%81%AA%E7%89%A9%E4%BD%93%E6%A4%9C%E5%87%BA-step3-non-maximum-suppression"&gt;簡単な物体検出 (Step.3) Non-Maximum Suppression&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100/answers/answer_99.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;95&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100#q95-%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF-step1-%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AB%E3%81%99%E3%82%8B"&gt;ニューラルネットワーク (Step.1) ディープラーニングにする&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100/answers/answer_95.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;100&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100#q100-%E7%B0%A1%E5%8D%98%E3%81%AA%E7%89%A9%E4%BD%93%E6%A4%9C%E5%87%BA-step4-%E8%A9%95%E4%BE%A1-precision-recall-f-score-map"&gt;簡単な物体検出 (Step.4) 評価 Precision, Recall, F-score, mAP&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="Question_91_100/answers/answer_100.py"&gt;✓&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;p&gt;adaptivebinalizatino, poison image blending&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;@article{yoyoyo-yoGasyori100knock,
    Author = {yoyoyo-yo},
    Title = {Gasyori100knock},
    Journal = {https://github.com/yoyoyo-yo/Gasyori100knock},
    Year = {2019}
}&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;© Yoshito Nagaoka All Rights Reserved.&lt;/p&gt;
&lt;p&gt;This is under MIT License.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/LICENSE"&gt;https://github.com/yoyoyo-yo/Gasyori100knock/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>yoyoyo-yo</author><guid isPermaLink="false">https://github.com/yoyoyo-yo/Gasyori100knock</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>YCG09/chinese_ocr #19 in Python, Today</title><link>https://github.com/YCG09/chinese_ocr</link><description>&lt;p&gt;&lt;i&gt;CTPN + DenseNet + CTC based end-to-end Chinese OCR implemented using tensorflow and keras&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简介&lt;/h2&gt;
&lt;p&gt;基于Tensorflow和Keras实现端到端的不定长中文字符检测和识别&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本检测：CTPN&lt;/li&gt;
&lt;li&gt;文本识别：DenseNet + CTC&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-环境部署" class="anchor" aria-hidden="true" href="#环境部署"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;环境部署&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh setup.sh&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;注：CPU环境执行前需注释掉for gpu部分，并解开for cpu部分的注释&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-demo" class="anchor" aria-hidden="true" href="#demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Demo&lt;/h2&gt;
&lt;p&gt;将测试图片放入test_images目录，检测结果会保存到test_result中&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python demo.py&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-模型训练" class="anchor" aria-hidden="true" href="#模型训练"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模型训练&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-ctpn训练" class="anchor" aria-hidden="true" href="#ctpn训练"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CTPN训练&lt;/h3&gt;
&lt;p&gt;详见ctpn/README.md&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-densenet--ctc训练" class="anchor" aria-hidden="true" href="#densenet--ctc训练"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DenseNet + CTC训练&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-1-数据准备" class="anchor" aria-hidden="true" href="#1-数据准备"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. 数据准备&lt;/h4&gt;
&lt;p&gt;数据集：&lt;a href="https://pan.baidu.com/s/1QkI7kjah8SPHwOQ40rS1Pw" rel="nofollow"&gt;https://pan.baidu.com/s/1QkI7kjah8SPHwOQ40rS1Pw&lt;/a&gt; (密码：lu7m)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;共约364万张图片，按照99:1划分成训练集和验证集&lt;/li&gt;
&lt;li&gt;数据利用中文语料库（新闻 + 文言文），通过字体、大小、灰度、模糊、透视、拉伸等变化随机生成&lt;/li&gt;
&lt;li&gt;包含汉字、英文字母、数字和标点共5990个字符&lt;/li&gt;
&lt;li&gt;每个样本固定10个字符，字符随机截取自语料库中的句子&lt;/li&gt;
&lt;li&gt;图片分辨率统一为280x32&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;图片解压后放置到train/images目录下，描述文件放到train目录下&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-2-训练" class="anchor" aria-hidden="true" href="#2-训练"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. 训练&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; train
python train.py&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-3-结果" class="anchor" aria-hidden="true" href="#3-结果"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. 结果&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;val acc&lt;/th&gt;
&lt;th&gt;predict&lt;/th&gt;
&lt;th&gt;model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.983&lt;/td&gt;
&lt;td&gt;8ms&lt;/td&gt;
&lt;td&gt;18.9MB&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;GPU: GTX TITAN X&lt;/li&gt;
&lt;li&gt;Keras Backend: Tensorflow&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-4-生成自己的样本" class="anchor" aria-hidden="true" href="#4-生成自己的样本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4. 生成自己的样本&lt;/h4&gt;
&lt;p&gt;可参考&lt;a href="https://github.com/JarveeLee/SynthText_Chinese_version"&gt;SynthText_Chinese_version&lt;/a&gt;，&lt;a href="https://github.com/Belval/TextRecognitionDataGenerator"&gt;TextRecognitionDataGenerator&lt;/a&gt;和&lt;a href="https://github.com/Sanster/text_renderer"&gt;text_renderer&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-效果展示" class="anchor" aria-hidden="true" href="#效果展示"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;效果展示&lt;/h2&gt;
&lt;div&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/YCG09/chinese_ocr/blob/master/demo/demo_detect.jpg"&gt;&lt;img width="420" height="420" src="https://github.com/YCG09/chinese_ocr/raw/master/demo/demo_detect.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/YCG09/chinese_ocr/blob/master/demo/demo_rec.jpg"&gt;&lt;img width="420" height="420" src="https://github.com/YCG09/chinese_ocr/raw/master/demo/demo_rec.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-参考" class="anchor" aria-hidden="true" href="#参考"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;参考&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href="https://github.com/eragonruan/text-detection-ctpn"&gt;https://github.com/eragonruan/text-detection-ctpn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href="https://github.com/senlinuc/caffe_ocr"&gt;https://github.com/senlinuc/caffe_ocr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a href="https://github.com/chineseocr/chinese-ocr"&gt;https://github.com/chineseocr/chinese-ocr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a href="https://github.com/xiaomaxiao/keras_ocr"&gt;https://github.com/xiaomaxiao/keras_ocr&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>YCG09</author><guid isPermaLink="false">https://github.com/YCG09/chinese_ocr</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>tensorflow/models #20 in Python, Today</title><link>https://github.com/tensorflow/models</link><description>&lt;p&gt;&lt;i&gt;Models and examples built with TensorFlow&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-models" class="anchor" aria-hidden="true" href="#tensorflow-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Models&lt;/h1&gt;
&lt;p&gt;This repository contains a number of different models implemented in &lt;a href="https://www.tensorflow.org" rel="nofollow"&gt;TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;The &lt;a href="official"&gt;official models&lt;/a&gt; are a collection of example models that use TensorFlow's high-level APIs. They are intended to be well-maintained, tested, and kept up to date with the latest stable TensorFlow API. They should also be reasonably optimized for fast performance while still being easy to read. We especially recommend newer TensorFlow users to start here.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/tensorflow/models/tree/master/research"&gt;research models&lt;/a&gt; are a large collection of models implemented in TensorFlow by researchers. They are not officially supported or available in release branches; it is up to the individual researchers to maintain the models and/or provide support on issues and pull requests.&lt;/p&gt;
&lt;p&gt;The &lt;a href="samples"&gt;samples folder&lt;/a&gt; contains code snippets and smaller models that demonstrate features of TensorFlow, including code presented in various blog posts.&lt;/p&gt;
&lt;p&gt;The &lt;a href="tutorials"&gt;tutorials folder&lt;/a&gt; is a collection of models described in the &lt;a href="https://www.tensorflow.org/tutorials/" rel="nofollow"&gt;TensorFlow tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-guidelines" class="anchor" aria-hidden="true" href="#contribution-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution guidelines&lt;/h2&gt;
&lt;p&gt;If you want to contribute to models, be sure to review the &lt;a href="CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensorflow</author><guid isPermaLink="false">https://github.com/tensorflow/models</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>ansible/ansible #21 in Python, Today</title><link>https://github.com/ansible/ansible</link><description>&lt;p&gt;&lt;i&gt;Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy. Avoid writing scripts or custom code to deploy and update your applications — automate in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com/ansible/&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/ansible" rel="nofollow"&gt;&lt;img alt="PyPI version" src="https://camo.githubusercontent.com/1700ed8e65665052f4e72ba6ae9e1f1d7fddc6c6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f616e7369626c652e737667" data-canonical-src="https://img.shields.io/pypi/v/ansible.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/" rel="nofollow"&gt;&lt;img alt="Docs badge" src="https://camo.githubusercontent.com/dc37b81ae5ef1245837ee1f1547892e8345ccd4b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/docs-latest-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html" rel="nofollow"&gt;&lt;img alt="Chat badge" src="https://camo.githubusercontent.com/a36ab54aea33fa40f9d063c8804b9bbf1b6fbd47/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d4952432d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/chat-IRC-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://app.shippable.com/projects/573f79d02a8192902e20e34b" rel="nofollow"&gt;&lt;img alt="Build Status" src="https://camo.githubusercontent.com/c4dd185960fb101604717a4c8965ac9ba2725e69/68747470733a2f2f6170692e736869707061626c652e636f6d2f70726f6a656374732f3537336637396430326138313932393032653230653334622f62616467653f6272616e63683d646576656c" data-canonical-src="https://api.shippable.com/projects/573f79d02a8192902e20e34b/badge?branch=devel" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/code_of_conduct.html" rel="nofollow"&gt;&lt;img alt="Ansible Code of Conduct" src="https://camo.githubusercontent.com/412f4c0b8d7289e25a69d8568bd02c1bf976f9fd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532306f66253230636f6e647563742d416e7369626c652d73696c7665722e737667" data-canonical-src="https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html#mailing-list-information" rel="nofollow"&gt;&lt;img alt="Ansible mailing lists" src="https://camo.githubusercontent.com/74dd4958c493abf9d3105cbcd020e55aa5df90c9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61696c696e672532306c697374732d416e7369626c652d6f72616e67652e737667" data-canonical-src="https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="COPYING"&gt;&lt;img alt="Repository License" src="https://camo.githubusercontent.com/0ac7552afd56fbe0c2ce6722d54f68857aa92b82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c25323076332e302d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-ansible"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-ansible" class="anchor" aria-hidden="true" href="#ansible"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ansible&lt;/h2&gt;
&lt;p&gt;Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on &lt;a href="https://ansible.com/" rel="nofollow"&gt;the Ansible website&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-design-principles"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-design-principles" class="anchor" aria-hidden="true" href="#design-principles"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Design Principles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Have a dead simple setup process and a minimal learning curve.&lt;/li&gt;
&lt;li&gt;Manage machines very quickly and in parallel.&lt;/li&gt;
&lt;li&gt;Avoid custom-agents and additional open ports, be agentless by
leveraging the existing SSH daemon.&lt;/li&gt;
&lt;li&gt;Describe infrastructure in a language that is both machine and human
friendly.&lt;/li&gt;
&lt;li&gt;Focus on security and easy auditability/review/rewriting of content.&lt;/li&gt;
&lt;li&gt;Manage new remote machines instantly, without bootstrapping any
software.&lt;/li&gt;
&lt;li&gt;Allow module development in any dynamic language, not just Python.&lt;/li&gt;
&lt;li&gt;Be usable as non-root.&lt;/li&gt;
&lt;li&gt;Be the easiest IT automation system to use, ever.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-use-ansible"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-use-ansible" class="anchor" aria-hidden="true" href="#use-ansible"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use Ansible&lt;/h3&gt;
&lt;p&gt;You can install a released version of Ansible via &lt;code&gt;pip&lt;/code&gt;, a package manager, or
our &lt;a href="https://releases.ansible.com/ansible/" rel="nofollow"&gt;release repository&lt;/a&gt;. See our
&lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html" rel="nofollow"&gt;installation guide&lt;/a&gt; for details on installing Ansible
on a variety of platforms.&lt;/p&gt;
&lt;p&gt;Red Hat offers supported builds of &lt;a href="https://www.ansible.com/ansible-engine" rel="nofollow"&gt;Ansible Engine&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Power users and developers can run the &lt;code&gt;devel&lt;/code&gt; branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the &lt;code&gt;devel&lt;/code&gt; branch. We recommend getting involved
in the Ansible community if you want to run the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/p&gt;
&lt;a name="user-content-get-involved"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-get-involved" class="anchor" aria-hidden="true" href="#get-involved"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Get Involved&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Read &lt;a href="https://docs.ansible.com/ansible/latest/community" rel="nofollow"&gt;Community
Information&lt;/a&gt; for all
kinds of ways to contribute to and interact with the project,
including mailing list information and how to submit bug reports and
code to Ansible.&lt;/li&gt;
&lt;li&gt;Join a &lt;a href="https://github.com/ansible/community/wiki"&gt;Working Group&lt;/a&gt;, an organized community devoted to a specific technology domain or platform.&lt;/li&gt;
&lt;li&gt;Submit a proposed code update through a pull request to the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/li&gt;
&lt;li&gt;Talk to us before making larger changes
to avoid duplicate efforts. This not only helps everyone
know what is going on, it also helps save time and effort if we decide
some changes are needed.&lt;/li&gt;
&lt;li&gt;For a list of email lists, IRC channels and Working Groups, see the
&lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html" rel="nofollow"&gt;Communication page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-branch-info"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-branch-info" class="anchor" aria-hidden="true" href="#branch-info"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Branch Info&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;devel&lt;/code&gt; branch corresponds to the release actively under development.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;stable-2.X&lt;/code&gt; branches correspond to stable releases.&lt;/li&gt;
&lt;li&gt;Create a branch based on &lt;code&gt;devel&lt;/code&gt; and set up a &lt;a href="https://docs.ansible.com/ansible/latest/dev_guide/developing_modules_general.html#common-environment-setup" rel="nofollow"&gt;dev environment&lt;/a&gt; if you want to open a PR.&lt;/li&gt;
&lt;li&gt;See the &lt;a href="https://docs.ansible.com/ansible/latest/reference_appendices/release_and_maintenance.html" rel="nofollow"&gt;Ansible release and maintenance&lt;/a&gt; page for information about active branches.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-roadmap"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-roadmap" class="anchor" aria-hidden="true" href="#roadmap"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Roadmap&lt;/h3&gt;
&lt;p&gt;Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The &lt;a href="https://docs.ansible.com/ansible/devel/roadmap/" rel="nofollow"&gt;Ansible Roadmap page&lt;/a&gt; details what is planned and how to influence the roadmap.&lt;/p&gt;
&lt;a name="user-content-authors"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h3&gt;
&lt;p&gt;Ansible was created by &lt;a href="https://github.com/mpdehaan"&gt;Michael DeHaan&lt;/a&gt;
and has contributions from over 4600 users (and growing). Thanks everyone!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ansible.com" rel="nofollow"&gt;Ansible&lt;/a&gt; is sponsored by &lt;a href="https://www.redhat.com" rel="nofollow"&gt;Red Hat, Inc.&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-license"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;GNU General Public License v3.0&lt;/p&gt;
&lt;p&gt;See &lt;a href="COPYING"&gt;COPYING&lt;/a&gt; to see the full text.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>ansible</author><guid isPermaLink="false">https://github.com/ansible/ansible</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>NVlabs/stylegan #22 in Python, Today</title><link>https://github.com/NVlabs/stylegan</link><description>&lt;p&gt;&lt;i&gt;StyleGAN - Official TensorFlow Implementation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-stylegan--official-tensorflow-implementation" class="anchor" aria-hidden="true" href="#stylegan--official-tensorflow-implementation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;StyleGAN — Official TensorFlow Implementation&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d4c11ac2b538cba463dfd1e43d05fe4f30f2d33d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d677265656e2e7376673f7374796c653d706c6173746963"&gt;&lt;img src="https://camo.githubusercontent.com/d4c11ac2b538cba463dfd1e43d05fe4f30f2d33d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d677265656e2e7376673f7374796c653d706c6173746963" alt="Python 3.6" data-canonical-src="https://img.shields.io/badge/python-3.6-green.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/11658cad8470d233bb733d0b72dc9f85738b0c60/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f74656e736f72666c6f772d312e31302d677265656e2e7376673f7374796c653d706c6173746963"&gt;&lt;img src="https://camo.githubusercontent.com/11658cad8470d233bb733d0b72dc9f85738b0c60/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f74656e736f72666c6f772d312e31302d677265656e2e7376673f7374796c653d706c6173746963" alt="TensorFlow 1.10" data-canonical-src="https://img.shields.io/badge/tensorflow-1.10-green.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/a5dab5f383e89d8397bd6a26b35ecafbca94277c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6375646e6e2d372e332e312d677265656e2e7376673f7374796c653d706c6173746963"&gt;&lt;img src="https://camo.githubusercontent.com/a5dab5f383e89d8397bd6a26b35ecafbca94277c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6375646e6e2d372e332e312d677265656e2e7376673f7374796c653d706c6173746963" alt="cuDNN 7.3.1" data-canonical-src="https://img.shields.io/badge/cudnn-7.3.1-green.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1a94f8355ec38c4cee39dec1e250552a499c37ac/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d43435f42592d2d4e432d677265656e2e7376673f7374796c653d706c6173746963"&gt;&lt;img src="https://camo.githubusercontent.com/1a94f8355ec38c4cee39dec1e250552a499c37ac/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d43435f42592d2d4e432d677265656e2e7376673f7374796c653d706c6173746963" alt="License CC BY-NC" data-canonical-src="https://img.shields.io/badge/license-CC_BY--NC-green.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./stylegan-teaser.png"&gt;&lt;img src="./stylegan-teaser.png" alt="Teaser image" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;strong&gt;Picture:&lt;/strong&gt; &lt;em&gt;These people are not real – they were produced by our generator that allows control over different aspects of the image.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This repository contains the official TensorFlow implementation of the following paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;A Style-Based Generator Architecture for Generative Adversarial Networks&lt;/strong&gt;&lt;br&gt;
Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA)&lt;br&gt;
&lt;a href="http://stylegan.xyz/paper" rel="nofollow"&gt;http://stylegan.xyz/paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; &lt;em&gt;We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For business inquiries, please contact &lt;a href="mailto:researchinquiries@nvidia.com"&gt;researchinquiries@nvidia.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For press and other inquiries, please contact Hector Marinez at &lt;a href="mailto:hmarinez@nvidia.com"&gt;hmarinez@nvidia.com&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h2&gt;
&lt;p&gt;All material related to our paper is available via the following links:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Link&lt;/th&gt;
&lt;th align="left"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://stylegan.xyz/paper" rel="nofollow"&gt;http://stylegan.xyz/paper&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Paper PDF.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://stylegan.xyz/video" rel="nofollow"&gt;http://stylegan.xyz/video&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Result video.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://stylegan.xyz/code" rel="nofollow"&gt;http://stylegan.xyz/code&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Source code.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://stylegan.xyz/ffhq" rel="nofollow"&gt;http://stylegan.xyz/ffhq&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Flickr-Faces-HQ dataset.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://stylegan.xyz/drive" rel="nofollow"&gt;http://stylegan.xyz/drive&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Google Drive folder.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Additional material can be found in Google Drive folder:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Path&lt;/th&gt;
&lt;th align="left"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://stylegan.xyz/drive" rel="nofollow"&gt;StyleGAN&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Main folder.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;├  &lt;a href="https://drive.google.com/open?id=1v-HkF3Ehrpon7wVIx4r5DLcko_U_V6Lt" rel="nofollow"&gt;stylegan-paper.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;High-quality version of the paper PDF.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;├  &lt;a href="https://drive.google.com/open?id=1uzwkZHQX_9pYg1i0d1Nbe3D9xPO8-qBf" rel="nofollow"&gt;stylegan-video.mp4&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;High-quality version of the result video.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;├  &lt;a href="https://drive.google.com/open?id=1-l46akONUWF6LCpDoeq63H53rD7MeiTd" rel="nofollow"&gt;images&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Example images produced using our generator.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;│  ├  &lt;a href="https://drive.google.com/open?id=1ToY5P4Vvf5_c3TyUizQ8fckFFoFtBvD8" rel="nofollow"&gt;representative-images&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;High-quality images to be used in articles, blog posts, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;│  └  &lt;a href="https://drive.google.com/open?id=100DJ0QXyG89HZzB4w2Cbyf4xjNK54cQ1" rel="nofollow"&gt;100k-generated-images&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;100,000 generated images for different amounts of truncation.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;│     ├  &lt;a href="https://drive.google.com/open?id=14lm8VRN1pr4g_KVe6_LvyDX1PObst6d4" rel="nofollow"&gt;ffhq-1024x1024&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Generated using Flickr-Faces-HQ dataset at 1024×1024.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;│     ├  &lt;a href="https://drive.google.com/open?id=1Vxz9fksw4kgjiHrvHkX4Hze4dyThFW6t" rel="nofollow"&gt;bedrooms-256x256&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Generated using LSUN Bedroom dataset at 256×256.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;│     ├  &lt;a href="https://drive.google.com/open?id=1MFCvOMdLE2_mpeLPTiDw5dxc2CRuKkzS" rel="nofollow"&gt;cars-512x384&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Generated using LSUN Car dataset at 512×384.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;│     └  &lt;a href="https://drive.google.com/open?id=1gq-Gj3GRFiyghTPKhp8uDMA9HV_0ZFWQ" rel="nofollow"&gt;cats-256x256&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Generated using LSUN Cat dataset at 256×256.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;├  &lt;a href="https://drive.google.com/open?id=1N8pOd_Bf8v89NGUaROdbD8-ayLPgyRRo" rel="nofollow"&gt;videos&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Example videos produced using our generator.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;│  └  &lt;a href="https://drive.google.com/open?id=1NFO7_vH0t98J13ckJYFd7kuaTkyeRJ86" rel="nofollow"&gt;high-quality-video-clips&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Individual segments of the result video as high-quality MP4.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;├  &lt;a href="https://drive.google.com/open?id=1u2xu7bSrWxrbUxk-dT-UvEJq8IjdmNTP" rel="nofollow"&gt;ffhq-dataset&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Raw data for the &lt;a href="http://stylegan.xyz/ffhq" rel="nofollow"&gt;Flickr-Faces-HQ dataset&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;└  &lt;a href="https://drive.google.com/open?id=1MASQyN5m0voPcx7-9K0r5gObhvvPups7" rel="nofollow"&gt;networks&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Pre-trained networks as pickled instances of &lt;a href="./dnnlib/tflib/network.py"&gt;dnnlib.tflib.Network&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;   ├  &lt;a href="https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ" rel="nofollow"&gt;stylegan-ffhq-1024x1024.pkl&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;StyleGAN trained with Flickr-Faces-HQ dataset at 1024×1024.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;   ├  &lt;a href="https://drive.google.com/uc?id=1MGqJl28pN4t7SAtSrPdSRJSQJqahkzUf" rel="nofollow"&gt;stylegan-celebahq-1024x1024.pkl&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;StyleGAN trained with CelebA-HQ dataset at 1024×1024.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;   ├  &lt;a href="https://drive.google.com/uc?id=1MOSKeGF0FJcivpBI7s63V9YHloUTORiF" rel="nofollow"&gt;stylegan-bedrooms-256x256.pkl&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;StyleGAN trained with LSUN Bedroom dataset at 256×256.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;   ├  &lt;a href="https://drive.google.com/uc?id=1MJ6iCfNtMIRicihwRorsM3b7mmtmK9c3" rel="nofollow"&gt;stylegan-cars-512x384.pkl&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;StyleGAN trained with LSUN Car dataset at 512×384.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;   ├  &lt;a href="https://drive.google.com/uc?id=1MQywl0FNt6lHu8E_EUqnRbviagS7fbiJ" rel="nofollow"&gt;stylegan-cats-256x256.pkl&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;StyleGAN trained with LSUN Cat dataset at 256×256.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;   └  &lt;a href="https://drive.google.com/open?id=1MvYdWCBuMfnoYGptRH-AgKLbPTsIQLhl" rel="nofollow"&gt;metrics&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Auxiliary networks for the quality and disentanglement metrics.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;      ├  &lt;a href="https://drive.google.com/uc?id=1MzTY44rLToO5APn8TZmfR7_ENSe5aZUn" rel="nofollow"&gt;inception_v3_features.pkl&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Standard &lt;a href="https://arxiv.org/abs/1512.00567" rel="nofollow"&gt;Inception-v3&lt;/a&gt; classifier that outputs a raw feature vector.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;      ├  &lt;a href="https://drive.google.com/uc?id=1N2-m9qszOeVC9Tq77WxsLnuWwOedQiD2" rel="nofollow"&gt;vgg16_zhang_perceptual.pkl&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Standard &lt;a href="https://arxiv.org/abs/1801.03924" rel="nofollow"&gt;LPIPS&lt;/a&gt; metric to estimate perceptual similarity.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;      ├  &lt;a href="https://drive.google.com/uc?id=1Q5-AI6TwWhCVM7Muu4tBM7rp5nG_gmCX" rel="nofollow"&gt;celebahq-classifier-00-male.pkl&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Binary classifier trained to detect a single attribute of CelebA-HQ.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;      └ ⋯&lt;/td&gt;
&lt;td align="left"&gt;Please see the file listing for remaining networks.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-licenses" class="anchor" aria-hidden="true" href="#licenses"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Licenses&lt;/h2&gt;
&lt;p&gt;All material, excluding the Flickr-Faces-HQ dataset, is made available under &lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" rel="nofollow"&gt;Creative Commons BY-NC 4.0&lt;/a&gt; license by NVIDIA Corporation. You can &lt;strong&gt;use, redistribute, and adapt&lt;/strong&gt; the material for &lt;strong&gt;non-commercial purposes&lt;/strong&gt;, as long as you give appropriate credit by &lt;strong&gt;citing our paper&lt;/strong&gt; and &lt;strong&gt;indicating any changes&lt;/strong&gt; that you've made.&lt;/p&gt;
&lt;p&gt;For license information regarding the FFHQ dataset, please refer to the &lt;a href="http://stylegan.xyz/ffhq" rel="nofollow"&gt;Flickr-Faces-HQ repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;inception_v3_features.pkl&lt;/code&gt; and &lt;code&gt;inception_v3_softmax.pkl&lt;/code&gt; are derived from the pre-trained &lt;a href="https://arxiv.org/abs/1512.00567" rel="nofollow"&gt;Inception-v3&lt;/a&gt; network by Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. The network was originally shared under &lt;a href="https://github.com/tensorflow/models/blob/master/LICENSE"&gt;Apache 2.0&lt;/a&gt; license on the &lt;a href="https://github.com/tensorflow/models"&gt;TensorFlow Models&lt;/a&gt; repository.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;vgg16.pkl&lt;/code&gt; and &lt;code&gt;vgg16_zhang_perceptual.pkl&lt;/code&gt; are derived from the pre-trained &lt;a href="https://arxiv.org/abs/1409.1556" rel="nofollow"&gt;VGG-16&lt;/a&gt; network by Karen Simonyan and Andrew Zisserman. The network was originally shared under &lt;a href="https://creativecommons.org/licenses/by/4.0/" rel="nofollow"&gt;Creative Commons BY 4.0&lt;/a&gt; license on the &lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" rel="nofollow"&gt;Very Deep Convolutional Networks for Large-Scale Visual Recognition&lt;/a&gt; project page.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;vgg16_zhang_perceptual.pkl&lt;/code&gt; is further derived from the pre-trained &lt;a href="https://arxiv.org/abs/1801.03924" rel="nofollow"&gt;LPIPS&lt;/a&gt; weights by Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The weights were originally shared under &lt;a href="https://github.com/richzhang/PerceptualSimilarity/blob/master/LICENSE"&gt;BSD 2-Clause "Simplified" License&lt;/a&gt; on the &lt;a href="https://github.com/richzhang/PerceptualSimilarity"&gt;PerceptualSimilarity&lt;/a&gt; repository.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-system-requirements" class="anchor" aria-hidden="true" href="#system-requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;System requirements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Both Linux and Windows are supported, but we strongly recommend Linux for performance and compatibility reasons.&lt;/li&gt;
&lt;li&gt;64-bit Python 3.6 installation. We recommend Anaconda3 with numpy 1.14.3 or newer.&lt;/li&gt;
&lt;li&gt;TensorFlow 1.10.0 or newer with GPU support.&lt;/li&gt;
&lt;li&gt;One or more high-end NVIDIA GPUs with at least 11GB of DRAM. We recommend NVIDIA DGX-1 with 8 Tesla V100 GPUs.&lt;/li&gt;
&lt;li&gt;NVIDIA driver 391.35 or newer, CUDA toolkit 9.0 or newer, cuDNN 7.3.1 or newer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-using-pre-trained-networks" class="anchor" aria-hidden="true" href="#using-pre-trained-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using pre-trained networks&lt;/h2&gt;
&lt;p&gt;A minimal example of using a pre-trained StyleGAN generator is given in &lt;a href="./pretrained_example.py"&gt;pretrained_example.py&lt;/a&gt;. When executed, the script downloads a pre-trained StyleGAN generator from Google Drive and uses it to generate an image:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; python pretrained_example.py
Downloading https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ .... done

Gs                              Params    OutputShape          WeightShape
---                             ---       ---                  ---
latents_in                      -         (?, 512)             -
...
images_out                      -         (?, 3, 1024, 1024)   -
---                             ---       ---                  ---
Total                           26219627

&amp;gt; ls results
example.png # https://drive.google.com/uc?id=1UDLT_zb-rof9kKH0GwiJW_bS9MoZi8oP
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A more advanced example is given in &lt;a href="./generate_figures.py"&gt;generate_figures.py&lt;/a&gt;. The script reproduces the figures from our paper in order to illustrate style mixing, noise inputs, and truncation:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; python generate_figures.py
results/figure02-uncurated-ffhq.png     # https://drive.google.com/uc?id=1U3r1xgcD7o-Fd0SBRpq8PXYajm7_30cu
results/figure03-style-mixing.png       # https://drive.google.com/uc?id=1U-nlMDtpnf1RcYkaFQtbh5oxnhA97hy6
results/figure04-noise-detail.png       # https://drive.google.com/uc?id=1UX3m39u_DTU6eLnEW6MqGzbwPFt2R9cG
results/figure05-noise-components.png   # https://drive.google.com/uc?id=1UQKPcvYVeWMRccGMbs2pPD9PVv1QDyp_
results/figure08-truncation-trick.png   # https://drive.google.com/uc?id=1ULea0C12zGlxdDQFNLXOWZCHi3QNfk_v
results/figure10-uncurated-bedrooms.png # https://drive.google.com/uc?id=1UEBnms1XMfj78OHj3_cx80mUf_m9DUJr
results/figure11-uncurated-cars.png     # https://drive.google.com/uc?id=1UO-4JtAs64Kun5vIj10UXqAJ1d5Ir1Ke
results/figure12-uncurated-cats.png     # https://drive.google.com/uc?id=1USnJc14prlu3QAYxstrtlfXC9sDWPA-W
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pre-trained networks are stored as standard pickle files on Google Drive:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Load pre-trained network.
url = 'https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ' # karras2019stylegan-ffhq-1024x1024.pkl
with dnnlib.util.open_url(url, cache_dir=config.cache_dir) as f:
    _G, _D, Gs = pickle.load(f)
    # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.
    # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.
    # Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code downloads the file and unpickles it to yield 3 instances of &lt;a href="./dnnlib/tflib/network.py"&gt;dnnlib.tflib.Network&lt;/a&gt;. To generate images, you will typically want to use &lt;code&gt;Gs&lt;/code&gt; – the other two networks are provided for completeness. In order for &lt;code&gt;pickle.load()&lt;/code&gt; to work, you will need to have the &lt;code&gt;dnnlib&lt;/code&gt; source directory in your PYTHONPATH and a &lt;code&gt;tf.Session&lt;/code&gt; set as default. The session can initialized by calling &lt;code&gt;dnnlib.tflib.init_tf()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are three ways to use the pre-trained generator:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Use &lt;code&gt;Gs.run()&lt;/code&gt; for immediate-mode operation where the inputs and outputs are numpy arrays:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Pick latent vector.
rnd = np.random.RandomState(5)
latents = rnd.randn(1, Gs.input_shape[1])

# Generate image.
fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)
images = Gs.run(latents, None, truncation_psi=0.7, randomize_noise=True, output_transform=fmt)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first argument is a batch of latent vectors of shape &lt;code&gt;[num, 512]&lt;/code&gt;. The second argument is reserved for class labels (not used by StyleGAN). The remaining keyword arguments are optional and can be used to further modify the operation (see below). The output is a batch of images, whose format is dictated by the &lt;code&gt;output_transform&lt;/code&gt; argument.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use &lt;code&gt;Gs.get_output_for()&lt;/code&gt; to incorporate the generator as a part of a larger TensorFlow expression:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;latents = tf.random_normal([self.minibatch_per_gpu] + Gs_clone.input_shape[1:])
images = Gs_clone.get_output_for(latents, None, is_validation=True, randomize_noise=True)
images = tflib.convert_images_to_uint8(images)
result_expr.append(inception_clone.get_output_for(images))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code is from &lt;a href="./metrics/frechet_inception_distance.py"&gt;metrics/frechet_inception_distance.py&lt;/a&gt;. It generates a batch of random images and feeds them directly to the &lt;a href="https://arxiv.org/abs/1512.00567" rel="nofollow"&gt;Inception-v3&lt;/a&gt; network without having to convert the data to numpy arrays in between.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Look up &lt;code&gt;Gs.components.mapping&lt;/code&gt; and &lt;code&gt;Gs.components.synthesis&lt;/code&gt; to access individual sub-networks of the generator. Similar to &lt;code&gt;Gs&lt;/code&gt;, the sub-networks are represented as independent instances of &lt;a href="./dnnlib/tflib/network.py"&gt;dnnlib.tflib.Network&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;src_latents = np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in src_seeds)
src_dlatents = Gs.components.mapping.run(src_latents, None) # [seed, layer, component]
src_images = Gs.components.synthesis.run(src_dlatents, randomize_noise=False, **synthesis_kwargs)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code is from &lt;a href="./generate_figures.py"&gt;generate_figures.py&lt;/a&gt;. It first transforms a batch of latent vectors into the intermediate &lt;em&gt;W&lt;/em&gt; space using the mapping network and then turns these vectors into a batch of images using the synthesis network. The &lt;code&gt;dlatents&lt;/code&gt; array stores a separate copy of the same &lt;em&gt;w&lt;/em&gt; vector for each layer of the synthesis network to facilitate style mixing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The exact details of the generator are defined in &lt;a href="./training/networks_stylegan.py"&gt;training/networks_stylegan.py&lt;/a&gt; (see &lt;code&gt;G_style&lt;/code&gt;, &lt;code&gt;G_mapping&lt;/code&gt;, and &lt;code&gt;G_synthesis&lt;/code&gt;). The following keyword arguments can be specified to modify the behavior when calling &lt;code&gt;run()&lt;/code&gt; and &lt;code&gt;get_output_for()&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;truncation_psi&lt;/code&gt; and &lt;code&gt;truncation_cutoff&lt;/code&gt; control the truncation trick that that is performed by default when using &lt;code&gt;Gs&lt;/code&gt; (ψ=0.7, cutoff=8). It can be disabled by setting &lt;code&gt;truncation_psi=1&lt;/code&gt; or &lt;code&gt;is_validation=True&lt;/code&gt;, and the image quality can be further improved at the cost of variation by setting e.g. &lt;code&gt;truncation_psi=0.5&lt;/code&gt;. Note that truncation is always disabled when using the sub-networks directly. The average &lt;em&gt;w&lt;/em&gt; needed to manually perform the truncation trick can be looked up using &lt;code&gt;Gs.get_var('dlatent_avg')&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;randomize_noise&lt;/code&gt; determines whether to use re-randomize the noise inputs for each generated image (&lt;code&gt;True&lt;/code&gt;, default) or whether to use specific noise values for the entire minibatch (&lt;code&gt;False&lt;/code&gt;). The specific values can be accessed via the &lt;code&gt;tf.Variable&lt;/code&gt; instances that are found using &lt;code&gt;[var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When using the mapping network directly, you can specify &lt;code&gt;dlatent_broadcast=None&lt;/code&gt; to disable the automatic duplication of &lt;code&gt;dlatents&lt;/code&gt; over the layers of the synthesis network.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Runtime performance can be fine-tuned via &lt;code&gt;structure='fixed'&lt;/code&gt; and &lt;code&gt;dtype='float16'&lt;/code&gt;. The former disables support for progressive growing, which is not needed for a fully-trained generator, and the latter performs all computation using half-precision floating point arithmetic.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-preparing-datasets-for-training" class="anchor" aria-hidden="true" href="#preparing-datasets-for-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preparing datasets for training&lt;/h2&gt;
&lt;p&gt;The training and evaluation scripts operate on datasets stored as multi-resolution TFRecords. Each dataset is represented by a directory containing the same image data in several resolutions to enable efficient streaming. There is a separate *.tfrecords file for each resolution, and if the dataset contains labels, they are stored in a separate file as well. By default, the scripts expect to find the datasets at &lt;code&gt;datasets/&amp;lt;NAME&amp;gt;/&amp;lt;NAME&amp;gt;-&amp;lt;RESOLUTION&amp;gt;.tfrecords&lt;/code&gt;. The directory can be changed by editing &lt;a href="./config.py"&gt;config.py&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;result_dir = 'results'
data_dir = 'datasets'
cache_dir = 'cache'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain the FFHQ dataset (&lt;code&gt;datasets/ffhq&lt;/code&gt;), please refer to the &lt;a href="http://stylegan.xyz/ffhq" rel="nofollow"&gt;Flickr-Faces-HQ repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To obtain the CelebA-HQ dataset (&lt;code&gt;datasets/celebahq&lt;/code&gt;), please refer to the &lt;a href="https://github.com/tkarras/progressive_growing_of_gans"&gt;Progressive GAN repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To obtain other datasets, including LSUN, please consult their corresponding project pages. The datasets can be converted to multi-resolution TFRecords using the provided &lt;a href="./dataset_tool.py"&gt;dataset_tool.py&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; python dataset_tool.py create_lsun datasets/lsun-bedroom-full ~/lsun/bedroom_lmdb --resolution 256
&amp;gt; python dataset_tool.py create_lsun_wide datasets/lsun-car-512x384 ~/lsun/car_lmdb --width 512 --height 384
&amp;gt; python dataset_tool.py create_lsun datasets/lsun-cat-full ~/lsun/cat_lmdb --resolution 256
&amp;gt; python dataset_tool.py create_cifar10 datasets/cifar10 ~/cifar10
&amp;gt; python dataset_tool.py create_from_images datasets/custom-dataset ~/custom-images
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-training-networks" class="anchor" aria-hidden="true" href="#training-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training networks&lt;/h2&gt;
&lt;p&gt;Once the datasets are set up, you can train your own StyleGAN networks as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Edit &lt;a href="./train.py"&gt;train.py&lt;/a&gt; to specify the dataset and training configuration by uncommenting or editing specific lines.&lt;/li&gt;
&lt;li&gt;Run the training script with &lt;code&gt;python train.py&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The results are written to a newly created directory &lt;code&gt;results/&amp;lt;ID&amp;gt;-&amp;lt;DESCRIPTION&amp;gt;&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The training may take several days (or weeks) to complete, depending on the configuration.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By default, &lt;code&gt;train.py&lt;/code&gt; is configured to train the highest-quality StyleGAN (configuration F in Table 1) for the FFHQ dataset at 1024×1024 resolution using 8 GPUs. Please note that we have used 8 GPUs in all of our experiments. Training with fewer GPUs may not produce identical results – if you wish to compare against our technique, we strongly recommend using the same number of GPUs.&lt;/p&gt;
&lt;p&gt;Expected training times for the default configuration using Tesla V100 GPUs:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;GPUs&lt;/th&gt;
&lt;th align="left"&gt;1024×1024&lt;/th&gt;
&lt;th align="left"&gt;512×512&lt;/th&gt;
&lt;th align="left"&gt;256×256&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;1&lt;/td&gt;
&lt;td align="left"&gt;41 days 4 hours&lt;/td&gt;
&lt;td align="left"&gt;24 days 21 hours&lt;/td&gt;
&lt;td align="left"&gt;14 days 22 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2&lt;/td&gt;
&lt;td align="left"&gt;21 days 22 hours&lt;/td&gt;
&lt;td align="left"&gt;13 days 7 hours&lt;/td&gt;
&lt;td align="left"&gt;9 days 5 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4&lt;/td&gt;
&lt;td align="left"&gt;11 days 8 hours&lt;/td&gt;
&lt;td align="left"&gt;7 days 0 hours&lt;/td&gt;
&lt;td align="left"&gt;4 days 21 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;8&lt;/td&gt;
&lt;td align="left"&gt;6 days 14 hours&lt;/td&gt;
&lt;td align="left"&gt;4 days 10 hours&lt;/td&gt;
&lt;td align="left"&gt;3 days 8 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-evaluating-quality-and-disentanglement" class="anchor" aria-hidden="true" href="#evaluating-quality-and-disentanglement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evaluating quality and disentanglement&lt;/h2&gt;
&lt;p&gt;The quality and disentanglement metrics used in our paper can be evaluated using &lt;a href="./run_metrics.py"&gt;run_metrics.py&lt;/a&gt;. By default, the script will evaluate the Fréchet Inception Distance (&lt;code&gt;fid50k&lt;/code&gt;) for the pre-trained FFHQ generator and write the results into a newly created directory under &lt;code&gt;results&lt;/code&gt;. The exact behavior can be changed by uncommenting or editing specific lines in &lt;a href="./run_metrics.py"&gt;run_metrics.py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Expected evaluation time and results for the pre-trained FFHQ generator using one Tesla V100 GPU:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Metric&lt;/th&gt;
&lt;th align="left"&gt;Time&lt;/th&gt;
&lt;th align="left"&gt;Result&lt;/th&gt;
&lt;th align="left"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;fid50k&lt;/td&gt;
&lt;td align="left"&gt;16 min&lt;/td&gt;
&lt;td align="left"&gt;4.4159&lt;/td&gt;
&lt;td align="left"&gt;Fréchet Inception Distance using 50,000 images.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ppl_zfull&lt;/td&gt;
&lt;td align="left"&gt;55 min&lt;/td&gt;
&lt;td align="left"&gt;664.8854&lt;/td&gt;
&lt;td align="left"&gt;Perceptual Path Length for full paths in &lt;em&gt;Z&lt;/em&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ppl_wfull&lt;/td&gt;
&lt;td align="left"&gt;55 min&lt;/td&gt;
&lt;td align="left"&gt;233.3059&lt;/td&gt;
&lt;td align="left"&gt;Perceptual Path Length for full paths in &lt;em&gt;W&lt;/em&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ppl_zend&lt;/td&gt;
&lt;td align="left"&gt;55 min&lt;/td&gt;
&lt;td align="left"&gt;666.1057&lt;/td&gt;
&lt;td align="left"&gt;Perceptual Path Length for path endpoints in &lt;em&gt;Z&lt;/em&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ppl_wend&lt;/td&gt;
&lt;td align="left"&gt;55 min&lt;/td&gt;
&lt;td align="left"&gt;197.2266&lt;/td&gt;
&lt;td align="left"&gt;Perceptual Path Length for path endpoints in &lt;em&gt;W&lt;/em&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ls&lt;/td&gt;
&lt;td align="left"&gt;10 hours&lt;/td&gt;
&lt;td align="left"&gt;z: 165.0106&lt;br&gt;w: 3.7447&lt;/td&gt;
&lt;td align="left"&gt;Linear Separability in &lt;em&gt;Z&lt;/em&gt; and &lt;em&gt;W&lt;/em&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Please note that the exact results may vary from run to run due to the non-deterministic nature of TensorFlow.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;We thank Jaakko Lehtinen, David Luebke, and Tuomas Kynkäänniemi for in-depth discussions and helpful comments; Janne Hellsten, Tero Kuosmanen, and Pekka Jänis for compute infrastructure and help with the code release.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVlabs</author><guid isPermaLink="false">https://github.com/NVlabs/stylegan</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>STVIR/pysot #23 in Python, Today</title><link>https://github.com/STVIR/pysot</link><description>&lt;p&gt;&lt;i&gt;SenseTime Research platform for single object tracking, implementing algorithms like SiamRPN and SiamMask.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pysot" class="anchor" aria-hidden="true" href="#pysot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PySOT&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;PySOT&lt;/strong&gt; is a software system designed by SenseTime Video Intelligence Research team. It implements state-of-the-art single object tracking algorithms, including &lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Li_High_Performance_Visual_CVPR_2018_paper.html" rel="nofollow"&gt;SiamRPN&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1812.05050" rel="nofollow"&gt;SiamMask&lt;/a&gt;. It is written in Python and powered by the &lt;a href="https://pytorch.org" rel="nofollow"&gt;PyTorch&lt;/a&gt; deep learning framework. This project also contains a Python port of toolkit for evaluating trackers.&lt;/p&gt;
&lt;p&gt;PySOT has enabled research projects, including: &lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Li_High_Performance_Visual_CVPR_2018_paper.html" rel="nofollow"&gt;SiamRPN&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1808.06048" rel="nofollow"&gt;DaSiamRPN&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1812.11703" rel="nofollow"&gt;SiamRPN++&lt;/a&gt;, and &lt;a href="https://arxiv.org/abs/1812.05050" rel="nofollow"&gt;SiamMask&lt;/a&gt;.&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="demo/output/bag_demo.gif"&gt;&lt;img src="demo/output/bag_demo.gif" width="800px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;p&gt;Example SiamFC, SiamRPN and SiamMask outputs.&lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The goal of PySOT is to provide a high-quality, high-performance codebase for visual tracking &lt;em&gt;research&lt;/em&gt;. It is designed to be flexible in order to support rapid implementation and evaluation of novel research. PySOT includes implementations of the following visual tracking algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1812.05050" rel="nofollow"&gt;SiamMask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1812.11703" rel="nofollow"&gt;SiamRPN++&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1808.06048" rel="nofollow"&gt;DaSiamRPN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Li_High_Performance_Visual_CVPR_2018_paper.html" rel="nofollow"&gt;SiamRPN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1606.09549" rel="nofollow"&gt;SiamFC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;using the following backbone network architectures:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1512.03385" rel="nofollow"&gt;ResNet{18, 34, 50}&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1801.04381" rel="nofollow"&gt;MobileNetV2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" rel="nofollow"&gt;AlexNet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional backbone architectures may be easily implemented. For more details about these models, please see &lt;a href="#references"&gt;References&lt;/a&gt; below.&lt;/p&gt;
&lt;p&gt;Evaluation toolkit can support the following datasets:&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="paperclip" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ce.png"&gt;📎&lt;/g-emoji&gt; &lt;a href="http://faculty.ucmerced.edu/mhyang/papers/pami15_tracking_benchmark.pdf" rel="nofollow"&gt;OTB2015&lt;/a&gt;
&lt;g-emoji class="g-emoji" alias="paperclip" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ce.png"&gt;📎&lt;/g-emoji&gt; &lt;a href="http://votchallenge.net" rel="nofollow"&gt;VOT16/18/19&lt;/a&gt;
&lt;g-emoji class="g-emoji" alias="paperclip" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ce.png"&gt;📎&lt;/g-emoji&gt; &lt;a href="http://votchallenge.net/vot2018/index.html" rel="nofollow"&gt;VOT18-LT&lt;/a&gt;
&lt;g-emoji class="g-emoji" alias="paperclip" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ce.png"&gt;📎&lt;/g-emoji&gt; &lt;a href="https://arxiv.org/pdf/1809.07845.pdf" rel="nofollow"&gt;LaSOT&lt;/a&gt;
&lt;g-emoji class="g-emoji" alias="paperclip" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ce.png"&gt;📎&lt;/g-emoji&gt; &lt;a href="https://arxiv.org/pdf/1804.00518.pdf" rel="nofollow"&gt;UAV123&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-zoo-and-baselines" class="anchor" aria-hidden="true" href="#model-zoo-and-baselines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model Zoo and Baselines&lt;/h2&gt;
&lt;p&gt;We provide a large set of baseline results and trained models available for download in the &lt;a href="MODEL_ZOO.md"&gt;PySOT Model Zoo&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;Please find installation instructions for PyTorch and PySOT in &lt;a href="INSTALL.md"&gt;&lt;code&gt;INSTALL.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start-using-pysot" class="anchor" aria-hidden="true" href="#quick-start-using-pysot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start: Using PySOT&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-add-pysot-to-your-pythonpath" class="anchor" aria-hidden="true" href="#add-pysot-to-your-pythonpath"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Add PySOT to your PYTHONPATH&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; PYTHONPATH=/path/to/pysot:&lt;span class="pl-smi"&gt;$PYTHONPATH&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-download-models" class="anchor" aria-hidden="true" href="#download-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download models&lt;/h3&gt;
&lt;p&gt;Download models in &lt;a href="MODEL_ZOO.md"&gt;PySOT Model Zoo&lt;/a&gt; and put the model.pth in the correct directory in experiments&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-webcam-demo" class="anchor" aria-hidden="true" href="#webcam-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Webcam demo&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python tools/demo.py \
    --config experiments/siamrpn_r50_l234_dwxcorr/config.yaml \
    --snapshot experiments/siamrpn_r50_l234_dwxcorr/model.pth
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; --video demo/bag.avi # (in case you don't have webcam)&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-download-testing-datasets" class="anchor" aria-hidden="true" href="#download-testing-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download testing datasets&lt;/h3&gt;
&lt;p&gt;Download datasets and put them into &lt;code&gt;testing_dataset&lt;/code&gt; directory. Jsons of commonly used datasets can be downloaded from &lt;a href="https://drive.google.com/drive/folders/10cfXjwQQBQeu48XMf2xc_W1LucpistPI" rel="nofollow"&gt;Google Drive&lt;/a&gt; or &lt;a href="https://pan.baidu.com/s/1js0Qhykqqur7_lNRtle1tA#list/path=%2F" rel="nofollow"&gt;BaiduYun&lt;/a&gt;. If you want to test tracker on new dataset, please refer to &lt;a href="https://github.com/StrangerZhang/pysot-toolkit"&gt;pysot-toolkit&lt;/a&gt; to setting &lt;code&gt;testing_dataset&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-test-tracker" class="anchor" aria-hidden="true" href="#test-tracker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Test tracker&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; experiments/siamrpn_r50_l234_dwxcorr
python -u ../../tools/test.py 	\
	--snapshot model.pth 	&lt;span class="pl-cce"&gt;\ &lt;/span&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; model path&lt;/span&gt;
	--dataset VOT2018 	&lt;span class="pl-cce"&gt;\ &lt;/span&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; dataset name&lt;/span&gt;
	--config config.yaml	  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; config file&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The testing results will in the current directory(results/dataset/model_name/)&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-eval-tracker" class="anchor" aria-hidden="true" href="#eval-tracker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eval tracker&lt;/h3&gt;
&lt;p&gt;assume still in experiments/siamrpn_r50_l234_dwxcorr_8gpu&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python ../../tools/eval.py 	 \
	--tracker_path ./results &lt;span class="pl-cce"&gt;\ &lt;/span&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; result path&lt;/span&gt;
	--dataset VOT2018        &lt;span class="pl-cce"&gt;\ &lt;/span&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; dataset name&lt;/span&gt;
	--num 1 		 &lt;span class="pl-cce"&gt;\ &lt;/span&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; number thread to eval&lt;/span&gt;
	--tracker_prefix &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;model&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; tracker_name&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-training-wrench" class="anchor" aria-hidden="true" href="#training-wrench"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training &lt;g-emoji class="g-emoji" alias="wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f527.png"&gt;🔧&lt;/g-emoji&gt;&lt;/h3&gt;
&lt;p&gt;See &lt;a href="TRAIN.md"&gt;TRAIN.md&lt;/a&gt; for detailed instruction.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-getting-help-hammer" class="anchor" aria-hidden="true" href="#getting-help-hammer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Help &lt;g-emoji class="g-emoji" alias="hammer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f528.png"&gt;🔨&lt;/g-emoji&gt;&lt;/h3&gt;
&lt;p&gt;If you meet problem, try searching our GitHub issues first. We intend the issues page to be a forum in which the community collectively troubleshoots problems. But please do &lt;strong&gt;not&lt;/strong&gt; post &lt;strong&gt;duplicate&lt;/strong&gt; issues. If you have similar issue that has been closed, you can reopen it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ModuleNotFoundError: No module named 'pysot'&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="dart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png"&gt;🎯&lt;/g-emoji&gt;Solution: Run &lt;code&gt;export PYTHONPATH=path/to/pysot&lt;/code&gt; first before you run the code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ImportError: cannot import name region&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="dart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png"&gt;🎯&lt;/g-emoji&gt;Solution: Build &lt;code&gt;region&lt;/code&gt; by &lt;code&gt;python setup.py build_ext —-inplace&lt;/code&gt; as decribled in &lt;a href="INSTALL.md"&gt;INSTALL.md&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1812.05050" rel="nofollow"&gt;Fast Online Object Tracking and Segmentation: A Unifying Approach&lt;/a&gt;.
Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, Philip H.S. Torr.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1812.11703" rel="nofollow"&gt;SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks&lt;/a&gt;.
Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1808.06048" rel="nofollow"&gt;Distractor-aware Siamese Networks for Visual Object Tracking&lt;/a&gt;.
Zheng Zhu, Qiang Wang, Bo Li, Wu Wei, Junjie Yan, Weiming Hu.
The European Conference on Computer Vision (ECCV), 2018.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Li_High_Performance_Visual_CVPR_2018_paper.html" rel="nofollow"&gt;High Performance Visual Tracking with Siamese Region Proposal Network&lt;/a&gt;.
Bo Li, Wei Wu, Zheng Zhu, Junjie Yan, Xiaolin Hu.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1606.09549" rel="nofollow"&gt;Fully-Convolutional Siamese Networks for Object Tracking&lt;/a&gt;.
Luca Bertinetto, Jack Valmadre, João F. Henriques, Andrea Vedaldi, Philip H. S. Torr.
The European Conference on Computer Vision (ECCV) Workshops, 2016.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/StrangerZhang"&gt;Fangyi Zhang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.robots.ox.ac.uk/~qwang/" rel="nofollow"&gt;Qiang Wang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://bo-li.info/" rel="nofollow"&gt;Bo Li&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;PySOT is released under the &lt;a href="https://github.com/STVIR/pysot/blob/master/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>STVIR</author><guid isPermaLink="false">https://github.com/STVIR/pysot</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>descriptinc/melgan-neurips #24 in Python, Today</title><link>https://github.com/descriptinc/melgan-neurips</link><description>&lt;p&gt;&lt;i&gt;Official repository for the paper MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-official-repository-for-the-paper-melgan-generative-adversarial-networks-for-conditional-waveform-synthesis" class="anchor" aria-hidden="true" href="#official-repository-for-the-paper-melgan-generative-adversarial-networks-for-conditional-waveform-synthesis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Official repository for the paper MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis&lt;/h1&gt;
&lt;p&gt;Previous works have found that generating coherent raw audio waveforms with GANs is challenging. In this &lt;a href="https://arxiv.org/abs/1910.06711" rel="nofollow"&gt;paper&lt;/a&gt;, we show that it is possible to train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or MOS) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on GTX 1080Ti GPU and more than 2x faster than real-time on CPU, without any hardware specific optimization tricks. Blog post with samples and accompanying code coming soon.&lt;/p&gt;
&lt;p&gt;Visit our &lt;a href="https://melgan-neurips.github.io" rel="nofollow"&gt;website&lt;/a&gt; for samples. You can try the speech correction application &lt;a href="https://www.descript.com/overdub" rel="nofollow"&gt;here&lt;/a&gt; created based on the end-to-end speech synthesis pipeline using MelGAN.&lt;/p&gt;
&lt;p&gt;Check the &lt;a href="melgan_slides.pdf"&gt;slides&lt;/a&gt; if you aren't attending the NeurIPS 2019 conference to check out our poster.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-code-organization" class="anchor" aria-hidden="true" href="#code-organization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code organization&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;├── README.md             &amp;lt;- Top-level README.
├── set_env.sh            &amp;lt;- Set PYTHONPATH and CUDA_VISIBLE_DEVICES.
│
├── mel2wav
│   ├── dataset.py           &amp;lt;- data loader scripts
│   ├── modules.py           &amp;lt;- Model, layers and losses
│   ├── utils.py             &amp;lt;- Utilities to monitor, save, log, schedule etc.
│
├── scripts
│   ├── train.py                    &amp;lt;- training / validation / etc scripts
│   ├── generate_from_folder.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-preparing-dataset" class="anchor" aria-hidden="true" href="#preparing-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preparing dataset&lt;/h2&gt;
&lt;p&gt;Create a raw folder with all the samples stored in &lt;code&gt;wavs/&lt;/code&gt; subfolder.
Run these commands:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;ls wavs/&lt;span class="pl-k"&gt;*&lt;/span&gt;.wav &lt;span class="pl-k"&gt;|&lt;/span&gt; tail -n+10 &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; train_files.txt
ls wavs/&lt;span class="pl-k"&gt;*&lt;/span&gt;.wav &lt;span class="pl-k"&gt;|&lt;/span&gt; head -n10 &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; test_files.txt&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-training-example" class="anchor" aria-hidden="true" href="#training-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training Example&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;. source set_env.sh 0
# Set PYTHONPATH and use first GPU
python scripts/train.py --save_path logs/baseline --path &amp;lt;root_data_folder&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-pytorch-hub-example" class="anchor" aria-hidden="true" href="#pytorch-hub-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Hub Example&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;import torch
vocoder = torch.hub.load('descriptinc/melgan-neurips', 'load_melgan')
vocoder.inverse(audio)  # audio (torch.tensor) -&amp;gt; (batch_size, 80, timesteps)
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>descriptinc</author><guid isPermaLink="false">https://github.com/descriptinc/melgan-neurips</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>instaloader/instaloader #25 in Python, Today</title><link>https://github.com/instaloader/instaloader</link><description>&lt;p&gt;&lt;i&gt;Download pictures (or videos) along with their captions and other metadata from Instagram.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/instaloader/instaloader/master/docs/logo_heading.png"&gt;&lt;img alt="https://raw.githubusercontent.com/instaloader/instaloader/master/docs/logo_heading.png" src="https://raw.githubusercontent.com/instaloader/instaloader/master/docs/logo_heading.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/instaloader/instaloader" rel="nofollow"&gt;&lt;img alt="Travis-CI Build Status" src="https://camo.githubusercontent.com/11b13d9f375f3b23c3ec6116bb50e462e3480129/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f696e7374616c6f616465722f696e7374616c6f616465722f6d61737465722e737667" data-canonical-src="https://img.shields.io/travis/instaloader/instaloader/master.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://pypi.org/project/instaloader/" rel="nofollow"&gt;&lt;img alt="Instaloader PyPI Project Page" src="https://camo.githubusercontent.com/4df4c4f1e480461a68b9ede09ef5629d6174a9aa/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f696e7374616c6f616465722e737667" data-canonical-src="https://img.shields.io/pypi/v/instaloader.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0c3af8ac978c7e88ca184375a322cb6bbb525740/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f696e7374616c6f616465722e737667"&gt;&lt;img alt="Supported Python Versions" src="https://camo.githubusercontent.com/0c3af8ac978c7e88ca184375a322cb6bbb525740/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f696e7374616c6f616465722e737667" data-canonical-src="https://img.shields.io/pypi/pyversions/instaloader.svg" style="max-width:100%;"&gt;&lt;/a&gt;
 &lt;a href="https://github.com/instaloader/instaloader/blob/master/LICENSE"&gt;&lt;img alt="MIT License" src="https://camo.githubusercontent.com/2e4b2d6cd24de365110d4c55f778753fae441cbb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f696e7374616c6f616465722f696e7374616c6f616465722e737667" data-canonical-src="https://img.shields.io/github/license/instaloader/instaloader.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://aur.archlinux.org/packages/instaloader/" rel="nofollow"&gt;&lt;img alt="Arch User Repository Package" src="https://camo.githubusercontent.com/1a4dbaf7cf94e8e4c4fd7645221828ebc20d2d5b/68747470733a2f2f696d672e736869656c64732e696f2f6175722f76657273696f6e2f696e7374616c6f616465722e737667" data-canonical-src="https://img.shields.io/aur/version/instaloader.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://github.com/instaloader/instaloader/graphs/contributors"&gt;&lt;img alt="Contributor Count" src="https://camo.githubusercontent.com/a8cae60eb64e8e7ce778b02270100a03f7784b63/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f696e7374616c6f616465722f696e7374616c6f616465722e737667" data-canonical-src="https://img.shields.io/github/contributors/instaloader/instaloader.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0b35b22bbdb6ff9e4231bfa6454fa8b2c16c2150/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f696e7374616c6f616465722e737667"&gt;&lt;img alt="PyPI Download Count" src="https://camo.githubusercontent.com/0b35b22bbdb6ff9e4231bfa6454fa8b2c16c2150/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f696e7374616c6f616465722e737667" data-canonical-src="https://img.shields.io/pypi/dm/instaloader.svg" style="max-width:100%;"&gt;&lt;/a&gt;
 &lt;a href="https://saythanks.io/to/aandergr" rel="nofollow"&gt;&lt;img alt="Say Thanks!" src="https://camo.githubusercontent.com/e526aef11f56579cfca4b8dee252aa41bbb01692/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d6666376566642e737667" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-ff7efd.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;$ pip3 install instaloader

$ instaloader profile [profile ...]
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Instaloader&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;downloads &lt;strong&gt;public and private profiles, hashtags, user stories,
feeds and saved media&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;downloads &lt;strong&gt;comments, geotags and captions&lt;/strong&gt; of each post,&lt;/li&gt;
&lt;li&gt;automatically &lt;strong&gt;detects profile name changes&lt;/strong&gt; and renames the target
directory accordingly,&lt;/li&gt;
&lt;li&gt;allows &lt;strong&gt;fine-grained customization&lt;/strong&gt; of filters and where to store
downloaded media.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;instaloader [--comments] [--geotags] [--stories] [--highlights] [--tagged]
            [--login YOUR-USERNAME] [--fast-update]
            profile | "#hashtag" | :stories | :feed | :saved
&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://instaloader.github.io/" rel="nofollow"&gt;Instaloader Documentation&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-how-to-automatically-download-pictures-from-instagram"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-automatically-download-pictures-from-instagram" class="anchor" aria-hidden="true" href="#how-to-automatically-download-pictures-from-instagram"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Automatically Download Pictures from Instagram&lt;/h2&gt;
&lt;p&gt;To &lt;strong&gt;download all pictures and videos of a profile&lt;/strong&gt;, as well as the
&lt;strong&gt;profile picture&lt;/strong&gt;, do&lt;/p&gt;
&lt;pre&gt;instaloader profile [profile ...]
&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;profile&lt;/code&gt; is the name of a profile you want to download. Instead
of only one profile, you may also specify a list of profiles.&lt;/p&gt;
&lt;p&gt;To later &lt;strong&gt;update your local copy&lt;/strong&gt; of that profiles, you may run&lt;/p&gt;
&lt;pre&gt;instaloader --fast-update profile [profile ...]
&lt;/pre&gt;
&lt;p&gt;If &lt;code&gt;--fast-update&lt;/code&gt; is given, Instaloader stops when arriving at the
first already-downloaded picture. When updating profiles, Instaloader
automatically &lt;strong&gt;detects profile name changes&lt;/strong&gt; and renames the target directory
accordingly.&lt;/p&gt;
&lt;p&gt;Instaloader can also be used to &lt;strong&gt;download private profiles&lt;/strong&gt;. To do so,
invoke it with&lt;/p&gt;
&lt;pre&gt;instaloader --login=your_username profile [profile ...]
&lt;/pre&gt;
&lt;p&gt;When logging in, Instaloader &lt;strong&gt;stores the session cookies&lt;/strong&gt; in a file in your
temporary directory, which will be reused later the next time &lt;code&gt;--login&lt;/code&gt;
is given.  So you can download private profiles &lt;strong&gt;non-interactively&lt;/strong&gt; when you
already have a valid session cookie file.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://instaloader.github.io/basic-usage.html" rel="nofollow"&gt;Instaloader Documentation&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-disclaimer"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;Instaloader is in no way affiliated with, authorized, maintained or endorsed by Instagram or any of its affiliates or
subsidiaries. This is an independent and unofficial project. Use at your own risk.&lt;/p&gt;
&lt;p&gt;Instaloader is licensed under an MIT license. Refer to &lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;As an open source project, Instaloader heavily depends on the contributions from
its community. See
&lt;a href="https://instaloader.github.io/contributing.html" rel="nofollow"&gt;contributing&lt;/a&gt;
for how you may help Instaloader to become an even greater tool.&lt;/p&gt;
&lt;p&gt;It is a pleasure for us to share our Instaloader to the world, and we are proud
to have attracted such an active and motivating community, with so many users
who share their suggestions and ideas with us. Buying a community-sponsored beer
or coffee from time to time is very likely to further raise our passion for the
development of Instaloader.&lt;/p&gt;
&lt;div&gt;
&lt;div&gt;For Donations, we provide a PayPal.Me link and a Bitcoin address.&lt;/div&gt;
&lt;div&gt;
&lt;div&gt;PayPal: &lt;a href="https://www.paypal.me/aandergr" rel="nofollow"&gt;PayPal.me/aandergr&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;BTC: 1Nst4LoadeYzrKjJ1DX9CpbLXBYE9RKLwY&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>instaloader</author><guid isPermaLink="false">https://github.com/instaloader/instaloader</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item></channel></rss>