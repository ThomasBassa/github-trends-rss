<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Python, Today</title><link>https://github.com/trending/python?since=daily</link><description>The top repositories on GitHub for python, measured daily</description><pubDate>Wed, 12 Feb 2020 01:06:40 GMT</pubDate><lastBuildDate>Wed, 12 Feb 2020 01:06:40 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>brettkromkamp/contextualise #1 in Python, Today</title><link>https://github.com/brettkromkamp/contextualise</link><description>&lt;p&gt;&lt;i&gt;Contextualise is a simple and flexible tool particularly suited for organising information-heavy projects and activities consisting of unstructured and widely diverse data and information resources&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-contextualise-manage-your-knowledge" class="anchor" aria-hidden="true" href="#contextualise-manage-your-knowledge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contextualise: Manage Your Knowledge&lt;/h1&gt;
&lt;p&gt;Contextualise is a simple and flexible tool particularly suited for organising information-heavy projects and
activities consisting of unstructured and widely diverse data and information resources -- think of
investigative journalism, personal and professional research projects, &lt;a href="https://en.wikipedia.org/wiki/Worldbuilding" rel="nofollow"&gt;world building&lt;/a&gt; (for books, movies or computer
games) and many kinds of hobbies.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="resources/topic-view.png"&gt;&lt;img alt="Contextualise's topic view" src="resources/topic-view.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Contextualise's topic view&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="resources/graph-view.png"&gt;&lt;img alt="Contextualise's interactive network graph view (allowing for navigation between topics)" src="resources/graph-view.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Contextualise's interactive network graph view (allowing for navigation between topics)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="resources/interactive-3d-viewer.png"&gt;&lt;img alt="Contextualise's interactive 3D viewer" src="resources/interactive-3d-viewer.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Contextualise's interactive 3D viewer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Contextualise's main dependency is &lt;a href="https://github.com/brettkromkamp/topic-db"&gt;TopicDB&lt;/a&gt;, an open source &lt;a href="https://msdn.microsoft.com/en-us/library/aa480048.aspx" rel="nofollow"&gt;topic maps&lt;/a&gt;-based graph library. Topic maps provide
a way to describe complex relationships between abstract concepts and real-world (information) resources.&lt;/p&gt;
&lt;a name="user-content-why"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-why" class="anchor" aria-hidden="true" href="#why"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why?&lt;/h2&gt;
&lt;p&gt;I built and published my first knowledge documentation tool in 2007 which I was still using until very recently, almost
unmodified, twelve years later. If I remember correctly, it was built with &lt;a href="http://php.net/ChangeLog-5.php#5.2.5" rel="nofollow"&gt;PHP version 5.2.5&lt;/a&gt;! Twelve years is an
eternity in software terms. Nowadays, my preferred choice for web development is &lt;a href="https://www.python.org/" rel="nofollow"&gt;Python&lt;/a&gt; together with the &lt;a href="http://flask.pocoo.org/docs/1.0/" rel="nofollow"&gt;Flask&lt;/a&gt;
web development framework. What's more, after twelve years of using my own and other knowledge management tools, I have
several improvements in mind for the next version (many of which are simplifications, for that matter). And perhaps one
of the most important reasons for building a new tool like this is that I want it to be open source: both
Contextualise (the web application) and TopicDB (the actual topic maps engine on top of which Contextualise
is built -- also written by me) are licensed with the permissive open source &lt;a href="https://github.com/brettkromkamp/contextualise/blob/master/LICENSE"&gt;MIT license&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-feature-support"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-feature-support" class="anchor" aria-hidden="true" href="#feature-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Feature Support&lt;/h2&gt;
&lt;p&gt;The following provides an overview of Contextualise's existing (and planned) feature set:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Existing Features&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support for multiple (self-contained) topic maps&lt;/li&gt;
&lt;li&gt;Support for both private and public topic maps&lt;/li&gt;
&lt;li&gt;Extensive support for notes including the ability to attach a note to an existing topic and convert a note into a topic&lt;/li&gt;
&lt;li&gt;&lt;a href="https://daringfireball.net/projects/markdown/syntax" rel="nofollow"&gt;Markdown&lt;/a&gt;-based text editor for topic text and notes&lt;/li&gt;
&lt;li&gt;The ability to attach files (including images, PDFs, and so forth) to topics&lt;/li&gt;
&lt;li&gt;The ability to attach (&lt;a href="https://www.khronos.org/gltf/" rel="nofollow"&gt;glTF&lt;/a&gt;-based) 3D models to topics with an accompanying interactive 3D model viewer&lt;/li&gt;
&lt;li&gt;Powerful (semantic) associations with the ability to create typed associations with role-based members&lt;/li&gt;
&lt;li&gt;Flexible filtering of topic occurrences and associations by scope (that is, context)&lt;/li&gt;
&lt;li&gt;Interactive visual network graph of related topics (allowing navigation between topics)&lt;/li&gt;
&lt;li&gt;Auto-complete on all form fields that expect a topic reference&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Missing Version 1.0 Features&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Augmented_reality" rel="nofollow"&gt;Augmented Reality&lt;/a&gt; (AR) support for 3D occurrences&lt;/li&gt;
&lt;li&gt;Full-text search&lt;/li&gt;
&lt;li&gt;Google Maps support&lt;/li&gt;
&lt;li&gt;&lt;a href="https://timeline.knightlab.com/docs/index.html" rel="nofollow"&gt;Timeline&lt;/a&gt; support allowing to navigate between topics using a visual timeline component&lt;/li&gt;
&lt;li&gt;WikiMedia API integration to automatically enhance existing topics with relevant information from &lt;a href="https://www.wikipedia.org/" rel="nofollow"&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Semantic tagging&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more exhaustive list of missing features take a look at Contextualise's &lt;a href="https://github.com/brettkromkamp/contextualise/issues"&gt;list of issues&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-install-the-development-version"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-install-the-development-version" class="anchor" aria-hidden="true" href="#install-the-development-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install the Development Version&lt;/h2&gt;
&lt;p&gt;Contextualise officially supports Python 3.6–3.8.&lt;/p&gt;
&lt;p&gt;If you have &lt;a href="https://git-scm.com/" rel="nofollow"&gt;Git&lt;/a&gt; installed on your system, it is possible to install the development version
of Contextualise.&lt;/p&gt;
&lt;p&gt;Certain build prerequisites need to be met including the presence of a C compiler, the Python
header files, the &lt;code&gt;libpq&lt;/code&gt; header files and the &lt;code&gt;pg_config&lt;/code&gt; program as outlined, here: &lt;a href="http://initd.org/psycopg/docs/install.html#build-prerequisites" rel="nofollow"&gt;Build
prerequisites&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then do:&lt;/p&gt;
&lt;pre&gt;$ git clone https://github.com/brettkromkamp/contextualise
$ cd contextualise
$ pip install -e .
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;pip install -e .&lt;/code&gt; command allows you to follow the development branch as it changes by creating links in the
right places and installing the command line scripts to the appropriate locations.&lt;/p&gt;
&lt;p&gt;Then, if you want to update Contextualise at any time, in the same directory do:&lt;/p&gt;
&lt;pre&gt;$ git pull
&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://github.com/brettkromkamp/topic-db"&gt;TopicDB&lt;/a&gt;, the topic maps engine on top of which Contextualise is built is regularly updated. However, the version
of TopicDB published on &lt;a href="https://pypi.org/project/topic-db/" rel="nofollow"&gt;PyPI&lt;/a&gt; could lag behind. For that reason, it is
recommended that you also install TopicDB directly from GitHub:&lt;/p&gt;
&lt;pre&gt;$ pip uninstall topic-db
$ git clone https://github.com/brettkromkamp/topic-db.git
$ cd topic-db
$ pip install -e .
&lt;/pre&gt;
&lt;p&gt;Then, if you want to update TopicDB at any time, in the same directory do:&lt;/p&gt;
&lt;pre&gt;$ git pull
&lt;/pre&gt;
&lt;p&gt;After having installed Contextualise, you would have to separately install and configure the PostgreSQL database. Brief
instructions on how to do so are provided, here: &lt;a href="https://gist.github.com/brettkromkamp/87aaa99b056578ff1dc23a43a49aca89"&gt;Setting up the TopicDB
database&lt;/a&gt;. You need to ensure that the
database username, password and database name match with the &lt;code&gt;settings.ini&lt;/code&gt; file in the project's root folder.&lt;/p&gt;
&lt;p&gt;Finally, to run the application in &lt;strong&gt;development&lt;/strong&gt; mode you need to change to the project's top-level directory and set
two environment variables followed by running the &lt;code&gt;flask&lt;/code&gt; command with the &lt;code&gt;run&lt;/code&gt; parameter:&lt;/p&gt;
&lt;pre&gt;$ export FLASK_APP=contextualise
$ export FLASK_ENV=development
$ flask run
&lt;/pre&gt;
&lt;p&gt;You should see something similar to the following in the terminal:&lt;/p&gt;
&lt;pre&gt;* Serving Flask app "contextualise" (lazy loading)
* Environment: development
* Debug mode: on
* Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
* Restarting with stat
* Debugger is active!
* Debugger PIN: 521-258-444
&lt;/pre&gt;
&lt;p&gt;Opening the browser and navigating to &lt;code&gt;http://127.0.0.1:5000/&lt;/code&gt; should result in showing the application's &lt;em&gt;Welcome&lt;/em&gt;
page.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="resources/welcome-page.png"&gt;&lt;img alt="The Contextualise Welcome page" src="resources/welcome-page.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The Contextualise Welcome page&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Flask's built-in server is not suitable for production purposes. However, it is quite straightforward to run
Contextualise using &lt;a href="https://gunicorn.org/" rel="nofollow"&gt;Gunicorn&lt;/a&gt;, a Python WSGI HTTP server:&lt;/p&gt;
&lt;pre&gt;$ gunicorn -w 4 -b 0.0.0.0:5000 contextualise.wsgi:app
&lt;/pre&gt;
&lt;p&gt;For further information for properly running a flask application in production, take a look at Flask's own
&lt;a href="https://flask.palletsprojects.com/en/1.1.x/deploying/#deployment" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-docker"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-docker" class="anchor" aria-hidden="true" href="#docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker&lt;/h2&gt;
&lt;p&gt;Support for running Contextualise within &lt;a href="https://www.docker.com/" rel="nofollow"&gt;Docker&lt;/a&gt; is still in development. To run it from the
root of a local clone of the source:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Copy &lt;code&gt;settings-docker-sample.ini&lt;/code&gt; to &lt;code&gt;settings.ini&lt;/code&gt; file in the root and replace your email settings. For example:&lt;/p&gt;
&lt;pre&gt;[DATABASE]
Username = postgres
Password = postgres
Database = postgres
Host = db
Port = 5432

[EMAIL]
Username = changeme
Password = changeme
Server = mail.changeme.com
Sender = Change Me
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run &lt;code&gt;docker-compose up&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;a name="user-content-first-time-use"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-first-time-use" class="anchor" aria-hidden="true" href="#first-time-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;First-Time Use&lt;/h2&gt;
&lt;p&gt;Several users (with the roles of &lt;code&gt;admin&lt;/code&gt; and &lt;code&gt;user&lt;/code&gt;, respectively) are created by the application for testing
purposes. To log in as the admin user, provide the following credentials:
&lt;code&gt;admin@contextualise.io&lt;/code&gt; (user name) and &lt;code&gt;Passw0rd1&lt;/code&gt; (password). To log in as a non-admin user, provide the
following credentials: &lt;code&gt;user@contextualise.io&lt;/code&gt; and &lt;code&gt;Passw0rd1&lt;/code&gt;.&lt;/p&gt;
&lt;a name="user-content-tutorial"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-tutorial" class="anchor" aria-hidden="true" href="#tutorial"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorial&lt;/h2&gt;
&lt;p&gt;Pending.&lt;/p&gt;
&lt;a name="user-content-id2"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;Pending.&lt;/p&gt;
&lt;a name="user-content-miscellaneous"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-miscellaneous" class="anchor" aria-hidden="true" href="#miscellaneous"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Miscellaneous&lt;/h2&gt;
&lt;p&gt;Currently, I am using Contextualise for worldbuilding purposes of the Brave Robot fictional universe including its &lt;a href="https://brettkromkamp.com/posts/codex-roboticus/" rel="nofollow"&gt;Codex
Roboticus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="resources/codex-roboticus1.png"&gt;&lt;img alt="Codex Roboticus" src="resources/codex-roboticus1.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-how-to-contribute"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-contribute" class="anchor" aria-hidden="true" href="#how-to-contribute"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Contribute&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Check for open issues or open a fresh issue to start a discussion around a feature idea or a bug.&lt;/li&gt;
&lt;li&gt;Fork &lt;a href="https://github.com/brettkromkamp/contextualise"&gt;the repository&lt;/a&gt; on GitHub to start making your changes to the &lt;strong&gt;master&lt;/strong&gt; branch (or branch off of it).&lt;/li&gt;
&lt;li&gt;Write a test which shows that the bug was fixed or that the feature works as expected.&lt;/li&gt;
&lt;li&gt;Send a pull request and bug the maintainer until it gets merged and published. :) Make sure to add yourself to &lt;a href="https://github.com/brettkromkamp/contextualise/blob/master/AUTHORS.rst"&gt;AUTHORS&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>brettkromkamp</author><guid isPermaLink="false">https://github.com/brettkromkamp/contextualise</guid><pubDate>Wed, 12 Feb 2020 00:01:00 GMT</pubDate></item><item><title>eriklindernoren/ML-From-Scratch #2 in Python, Today</title><link>https://github.com/eriklindernoren/ML-From-Scratch</link><description>&lt;p&gt;&lt;i&gt;Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-from-scratch" class="anchor" aria-hidden="true" href="#machine-learning-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning From Scratch&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.&lt;/p&gt;
&lt;p&gt;The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible
but rather to present the inner workings of them in a transparent and accessible way.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#machine-learning-from-scratch"&gt;Machine Learning From Scratch&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#about"&gt;About&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#polynomial-regression"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#classification-with-cnn"&gt;Classification With CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#density-based-clustering"&gt;Density-Based Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#generating-handwritten-digits"&gt;Generating Handwritten Digits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-reinforcement-learning"&gt;Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image-reconstruction-with-rbm"&gt;Image Reconstruction With RBM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#evolutionary-evolved-neural-network"&gt;Evolutionary Evolved Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#genetic-algorithm"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#association-analysis"&gt;Association Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#implementations"&gt;Implementations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#supervised-learning"&gt;Supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unsupervised-learning"&gt;Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement-learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-learning"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#contact"&gt;Contact&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/eriklindernoren/ML-From-Scratch
$ cd ML-From-Scratch
$ python setup.py install
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-polynomial-regression" class="anchor" aria-hidden="true" href="#polynomial-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Polynomial Regression&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/polynomial_regression.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d82416364e7916546886f94027e2652d3247e8ab/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f705f7265672e676966"&gt;&lt;img src="https://camo.githubusercontent.com/d82416364e7916546886f94027e2652d3247e8ab/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f705f7265672e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/p_reg.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Training progress of a regularized polynomial regression model fitting &lt;br&gt;
    temperature data measured in Linköping, Sweden 2016.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-classification-with-cnn" class="anchor" aria-hidden="true" href="#classification-with-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Classification With CNN&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/convolutional_neural_network.py

+---------+
| ConvNet |
+---------+
Input Shape: (1, 8, 8)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Conv2D               | 160        | (16, 8, 8)   |
| Activation (ReLU)    | 0          | (16, 8, 8)   |
| Dropout              | 0          | (16, 8, 8)   |
| BatchNormalization   | 2048       | (16, 8, 8)   |
| Conv2D               | 4640       | (32, 8, 8)   |
| Activation (ReLU)    | 0          | (32, 8, 8)   |
| Dropout              | 0          | (32, 8, 8)   |
| BatchNormalization   | 4096       | (32, 8, 8)   |
| Flatten              | 0          | (2048,)      |
| Dense                | 524544     | (256,)       |
| Activation (ReLU)    | 0          | (256,)       |
| Dropout              | 0          | (256,)       |
| BatchNormalization   | 512        | (256,)       |
| Dense                | 2570       | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 538570

Training: 100% [------------------------------------------------------------------------] Time: 0:01:55
Accuracy: 0.987465181058
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c2bca09f5d1ce2b72f33fe61464408607797caa3/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f636e6e312e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/c2bca09f5d1ce2b72f33fe61464408607797caa3/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f636e6e312e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_cnn1.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Classification of the digit dataset using CNN.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-density-based-clustering" class="anchor" aria-hidden="true" href="#density-based-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Density-Based Clustering&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/dbscan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/eaf413b6e8cbf3f8fd048f3a63984482ffd7350e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64627363616e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/eaf413b6e8cbf3f8fd048f3a63984482ffd7350e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64627363616e2e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_dbscan.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Clustering of the moons dataset using DBSCAN.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-generating-handwritten-digits" class="anchor" aria-hidden="true" href="#generating-handwritten-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating Handwritten Digits&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py

+-----------+
| Generator |
+-----------+
Input Shape: (100,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 25856      | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| BatchNormalization     | 512        | (256,)       |
| Dense                  | 131584     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| BatchNormalization     | 1024       | (512,)       |
| Dense                  | 525312     | (1024,)      |
| Activation (LeakyReLU) | 0          | (1024,)      |
| BatchNormalization     | 2048       | (1024,)      |
| Dense                  | 803600     | (784,)       |
| Activation (TanH)      | 0          | (784,)       |
+------------------------+------------+--------------+
Total Parameters: 1489936

+---------------+
| Discriminator |
+---------------+
Input Shape: (784,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 401920     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| Dropout                | 0          | (512,)       |
| Dense                  | 131328     | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| Dropout                | 0          | (256,)       |
| Dense                  | 514        | (2,)         |
| Activation (Softmax)   | 0          | (2,)         |
+------------------------+------------+--------------+
Total Parameters: 533762
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/15ad5010011227a7ab8c6c77d19b7cc625cced30/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f67616e5f6d6e697374352e676966"&gt;&lt;img src="https://camo.githubusercontent.com/15ad5010011227a7ab8c6c77d19b7cc625cced30/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f67616e5f6d6e697374352e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/gan_mnist5.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Training progress of a Generative Adversarial Network generating &lt;br&gt;
    handwritten digits.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-deep-reinforcement-learning" class="anchor" aria-hidden="true" href="#deep-reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Reinforcement Learning&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/deep_q_network.py

+----------------+
| Deep Q-Network |
+----------------+
Input Shape: (4,)
+-------------------+------------+--------------+
| Layer Type        | Parameters | Output Shape |
+-------------------+------------+--------------+
| Dense             | 320        | (64,)        |
| Activation (ReLU) | 0          | (64,)        |
| Dense             | 130        | (2,)         |
+-------------------+------------+--------------+
Total Parameters: 450
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c605134f41b739121c4710f3d5c6e8370a592e0c/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64716c312e676966"&gt;&lt;img src="https://camo.githubusercontent.com/c605134f41b739121c4710f3d5c6e8370a592e0c/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64716c312e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_dql1.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-image-reconstruction-with-rbm" class="anchor" aria-hidden="true" href="#image-reconstruction-with-rbm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Reconstruction With RBM&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/restricted_boltzmann_machine.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d209d42aed9e8e32a10eaec9b76f141319a2b0d7/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f72626d5f646967697473312e676966"&gt;&lt;img src="https://camo.githubusercontent.com/d209d42aed9e8e32a10eaec9b76f141319a2b0d7/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f72626d5f646967697473312e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/rbm_digits1.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Shows how the network gets better during training at reconstructing &lt;br&gt;
    the digit 2 in the MNIST dataset.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-evolutionary-evolved-neural-network" class="anchor" aria-hidden="true" href="#evolutionary-evolved-neural-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evolutionary Evolved Neural Network&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/neuroevolution.py

+---------------+
| Model Summary |
+---------------+
Input Shape: (64,)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Dense                | 1040       | (16,)        |
| Activation (ReLU)    | 0          | (16,)        |
| Dense                | 170        | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 1210

Population Size: 100
Generations: 3000
Mutation Rate: 0.01

[0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]
[1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]
...
[2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]
Test set accuracy: 96.7%
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1a8abe4882d0195b8f8bd4c6f24caab639291e6e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f65766f5f6e6e342e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/1a8abe4882d0195b8f8bd4c6f24caab639291e6e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f65766f5f6e6e342e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/evo_nn4.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Classification of the digit dataset by a neural network which has&lt;br&gt;
    been evolutionary evolved.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-genetic-algorithm" class="anchor" aria-hidden="true" href="#genetic-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Genetic Algorithm&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/genetic_algorithm.py

+--------+
|   GA   |
+--------+
Description: Implementation of a Genetic Algorithm which aims to produce
the user specified target string. This implementation calculates each
candidate's fitness based on the alphabetical distance between the candidate
and the target. A candidate is selected as a parent with probabilities proportional
to the candidate's fitness. Reproduction is implemented as a single-point
crossover between pairs of parents. Mutation is done by randomly assigning
new characters with uniform probability.

Parameters
----------
Target String: 'Genetic Algorithm'
Population Size: 100
Mutation Rate: 0.05

[0 Closest Candidate: 'CJqlJguPlqzvpoJmb', Fitness: 0.00]
[1 Closest Candidate: 'MCxZxdr nlfiwwGEk', Fitness: 0.01]
[2 Closest Candidate: 'MCxZxdm nlfiwwGcx', Fitness: 0.01]
[3 Closest Candidate: 'SmdsAklMHn kBIwKn', Fitness: 0.01]
[4 Closest Candidate: '  lotneaJOasWfu Z', Fitness: 0.01]
...
[292 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[293 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[294 Answer: 'Genetic Algorithm']
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-association-analysis" class="anchor" aria-hidden="true" href="#association-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Association Analysis&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/apriori.py
+-------------+
|   Apriori   |
+-------------+
Minimum Support: 0.25
Minimum Confidence: 0.8
Transactions:
    [1, 2, 3, 4]
    [1, 2, 4]
    [1, 2]
    [2, 3, 4]
    [2, 3]
    [3, 4]
    [2, 4]
Frequent Itemsets:
    [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]
Rules:
    1 -&amp;gt; 2 (support: 0.43, confidence: 1.0)
    4 -&amp;gt; 2 (support: 0.57, confidence: 0.8)
    [1, 4] -&amp;gt; 2 (support: 0.29, confidence: 1.0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-implementations" class="anchor" aria-hidden="true" href="#implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementations&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-supervised-learning" class="anchor" aria-hidden="true" href="#supervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supervised Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/adaboost.py"&gt;Adaboost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/bayesian_regression.py"&gt;Bayesian Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/decision_tree.py"&gt;Decision Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Elastic Net&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/gradient_boosting.py"&gt;Gradient Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/k_nearest_neighbors.py"&gt;K Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Lasso Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/linear_discriminant_analysis.py"&gt;Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/logistic_regression.py"&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/multi_class_lda.py"&gt;Multi-class Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/naive_bayes.py"&gt;Naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/neuroevolution.py"&gt;Neuroevolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/particle_swarm_optimization.py"&gt;Particle Swarm Optimization of Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/perceptron.py"&gt;Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/random_forest.py"&gt;Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Ridge Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/support_vector_machine.py"&gt;Support Vector Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/xgboost.py"&gt;XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-unsupervised-learning" class="anchor" aria-hidden="true" href="#unsupervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/apriori.py"&gt;Apriori&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/autoencoder.py"&gt;Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/dbscan.py"&gt;DBSCAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/fp_growth.py"&gt;FP-Growth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/gaussian_mixture_model.py"&gt;Gaussian Mixture Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/generative_adversarial_network.py"&gt;Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/genetic_algorithm.py"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/k_means.py"&gt;K-Means&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/partitioning_around_medoids.py"&gt;Partitioning Around Medoids&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/principal_component_analysis.py"&gt;Principal Component Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/restricted_boltzmann_machine.py"&gt;Restricted Boltzmann Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-reinforcement-learning" class="anchor" aria-hidden="true" href="#reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reinforcement Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/reinforcement_learning/deep_q_network.py"&gt;Deep Q-Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deep-learning" class="anchor" aria-hidden="true" href="#deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/deep_learning/neural_network.py"&gt;Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/deep_learning/layers.py"&gt;Layers&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Activation Layer&lt;/li&gt;
&lt;li&gt;Average Pooling Layer&lt;/li&gt;
&lt;li&gt;Batch Normalization Layer&lt;/li&gt;
&lt;li&gt;Constant Padding Layer&lt;/li&gt;
&lt;li&gt;Convolutional Layer&lt;/li&gt;
&lt;li&gt;Dropout Layer&lt;/li&gt;
&lt;li&gt;Flatten Layer&lt;/li&gt;
&lt;li&gt;Fully-Connected (Dense) Layer&lt;/li&gt;
&lt;li&gt;Fully-Connected RNN Layer&lt;/li&gt;
&lt;li&gt;Max Pooling Layer&lt;/li&gt;
&lt;li&gt;Reshape Layer&lt;/li&gt;
&lt;li&gt;Up Sampling Layer&lt;/li&gt;
&lt;li&gt;Zero Padding Layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model Types
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/convolutional_neural_network.py"&gt;Convolutional Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/recurrent_neural_network.py"&gt;Recurrent Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;If there's some implementation you would like to see here or if you're just feeling social,
feel free to &lt;a href="mailto:eriklindernoren@gmail.com"&gt;email&lt;/a&gt; me or connect with me on &lt;a href="https://www.linkedin.com/in/eriklindernoren/" rel="nofollow"&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>eriklindernoren</author><guid isPermaLink="false">https://github.com/eriklindernoren/ML-From-Scratch</guid><pubDate>Wed, 12 Feb 2020 00:02:00 GMT</pubDate></item><item><title>domlysz/BlenderGIS #3 in Python, Today</title><link>https://github.com/domlysz/BlenderGIS</link><description>&lt;p&gt;&lt;i&gt;Blender addons to make the bridge between Blender and geographic data&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-blender-gis" class="anchor" aria-hidden="true" href="#blender-gis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Blender GIS&lt;/h1&gt;
&lt;p&gt;Blender minimal version : 2.8&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mac users warning :&lt;/strong&gt; currently the addon does not work anymore on Mac because of an issue relative to Blender Mac build itself. Please do not report the issue here. It should be fixed by the Blender team soon. Check &lt;a href="https://developer.blender.org/T68243" rel="nofollow"&gt;the bug report&lt;/a&gt; to follow the progress on it.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wiki---faq---quick-start-guide" class="anchor" aria-hidden="true" href="#wiki---faq---quick-start-guide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Home"&gt;Wiki&lt;/a&gt; - &lt;a href="https://github.com/domlysz/BlenderGIS/wiki/FAQ"&gt;FAQ&lt;/a&gt; - &lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Quick-start"&gt;Quick start guide&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-basemaps" class="anchor" aria-hidden="true" href="#basemaps"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Basemaps"&gt;Basemaps&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Display web map service like OpenStreetMap directly in Blender&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/Blender27x/images/basemaps_demo.gif"&gt;&lt;img src="https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/Blender27x/images/basemaps_demo.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-srtm-download" class="anchor" aria-hidden="true" href="#srtm-download"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/SRTM"&gt;SRTM download&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Get SRTM topographic data and apply it as height texture&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/Blender27x/images/srtm_demo.gif"&gt;&lt;img src="https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/Blender27x/images/srtm_demo.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-esri-shapefile-import--export" class="anchor" aria-hidden="true" href="#esri-shapefile-import--export"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Shapefile-import"&gt;ESRI Shapefile import / export&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A &lt;a href="http://en.wikipedia.org/wiki/Shapefile" rel="nofollow"&gt;Shapefile&lt;/a&gt; is a popular geospatial vector data format for geographic information system software.&lt;/p&gt;
&lt;p&gt;This tool can import into Blender most of shapefile feature type. It can also uses attributes data to define Z elevation values or Z extrusion values.&lt;/p&gt;
&lt;p&gt;Exporter script can export a mesh to pointZ, polylineZ or polygonZ shapefile. Note that currently this tool does not re-export attribute data include in the dbase file linked to the shapefile. So if you want to import a shapefile for edit it into Blender and then re-export it, you will lose attribute data.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-georeferenced-raster-importer" class="anchor" aria-hidden="true" href="#georeferenced-raster-importer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Import-georef-raster"&gt;Georeferenced raster importer&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Import geotiff or common image format georeferenced with a &lt;a href="http://en.wikipedia.org/wiki/World_file" rel="nofollow"&gt;world file&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can import the raster as a plane mesh, as backgound image for orthographic view, as UV texture mapping on a mesh or &lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Import-DEM-grid"&gt;as DEM&lt;/a&gt; for warp a mesh with the displace modifier.&lt;/p&gt;
&lt;p&gt;ESRI ASCII GRID format is also supported through a dedicated import tool.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-openstreetmap-import" class="anchor" aria-hidden="true" href="#openstreetmap-import"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/OSM-import"&gt;OpenStreetMap import&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/Blender27x/images/osm_demo.gif"&gt;&lt;img src="https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/Blender27x/images/osm_demo.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-georeferenced-render-output" class="anchor" aria-hidden="true" href="#georeferenced-render-output"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Make-a-georef-render"&gt;Georeferenced render output&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is a tool to create a new camera correctly setup for produce a map render. Georeferencing data (worldfile) are writing in text file accessible from the Blender text editor.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-delaunay-triangulation--voronoi-diagram" class="anchor" aria-hidden="true" href="#delaunay-triangulation--voronoi-diagram"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Make-terrain-mesh-with-Delaunay-triangulation"&gt;Delaunay triangulation &amp;amp; Voronoi diagram&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This script computes &lt;a href="http://en.wikipedia.org/wiki/Delaunay_triangulation" rel="nofollow"&gt;Delaunay triangulation&lt;/a&gt; in 2.5D. This triangulation is suitable for create a 3D terrain mesh from &lt;a href="http://en.wikipedia.org/wiki/Point_cloud" rel="nofollow"&gt;points cloud&lt;/a&gt; or &lt;a href="http://en.wikipedia.org/wiki/Contour_line" rel="nofollow"&gt;contour lines&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The script can also compute &lt;a href="http://en.wikipedia.org/wiki/Voronoi" rel="nofollow"&gt;Voronoi tessellation&lt;/a&gt; in 2D which is the dual of delaunay triangulation. Voronoi diagram is suitable to make neighborhood analysis map.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-terrain-analysis" class="anchor" aria-hidden="true" href="#terrain-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Terrain-analysis"&gt;Terrain analysis&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This part of Blender GIS is designed to assist in the analysis of the topography : height, slope and azimuth (aspect).&lt;/p&gt;
&lt;p&gt;There are 2 tools, one to build materials nodes setup for Cycles engine, and a second to configure the color ramp as usual in common GIS software (reclassify values and apply color ramp presets).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-georeferencing-management" class="anchor" aria-hidden="true" href="#georeferencing-management"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/domlysz/BlenderGIS/wiki/Gereferencing-management"&gt;Georeferencing management&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Handle various projection systems with reprojection capabilities and compatibility with some others addons&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>domlysz</author><guid isPermaLink="false">https://github.com/domlysz/BlenderGIS</guid><pubDate>Wed, 12 Feb 2020 00:03:00 GMT</pubDate></item><item><title>cycz/jdBuyMask #4 in Python, Today</title><link>https://github.com/cycz/jdBuyMask</link><description>&lt;p&gt;&lt;i&gt;京东监控口罩有货爬虫，自动下单爬虫，口罩爬虫&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;hr&gt;
&lt;p&gt;如果帮助到你，star一下，谢谢你&lt;/p&gt;
&lt;p&gt;武汉加油，中国加油&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;本代码使用方式 &lt;a href="https://blog.csdn.net/cyz52/article/details/104239558" rel="nofollow"&gt;https://blog.csdn.net/cyz52/article/details/104239558&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;下单的部分代码参考了tychxn大佬的代码&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;建议使用V2极速版&lt;/strong&gt;
每10分钟自动读取一次配置修改商品不需要退出重开&lt;/p&gt;
&lt;p&gt;避免抢购，程序自动一次只买一件&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-v2版本" class="anchor" aria-hidden="true" href="#v2版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;V2版本&lt;/h2&gt;
&lt;p&gt;请在configDemo.ini 加入商品id、地区id、cookie等参数
区分下单模式（默认2正常模式）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意--极速模式默认清空购物车&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;正常模式下单流程（1.7秒左右）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 检测有货&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 检测下柜&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 加入购物车&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 查看购物车&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 下单&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;极速模式下单流程（1.4秒左右）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 检测有货&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 加入购物车&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 下单&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-v3版本下单更快" class="anchor" aria-hidden="true" href="#v3版本下单更快"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;V3版本（下单更快）&lt;/h2&gt;
&lt;p&gt;下单更快，但只能扫描单独一件商品&lt;/p&gt;
&lt;p&gt;在配置文件configDemo.ini中，填写[V3]下面的skuid&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意--V3版本默认清空购物车&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;V3版本下单流程（1秒左右）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 提前加入购物车&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 检测有货&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 下单&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-exe版本" class="anchor" aria-hidden="true" href="#exe版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;exe版本&lt;/h2&gt;
&lt;p&gt;进群要吧，地址随时变，就不放了。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-温馨提示" class="anchor" aria-hidden="true" href="#温馨提示"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;温馨提示&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;在京东购物车结算页面设置发票为电子普通发票-个人设置支付方式为在线支付&lt;/li&gt;
&lt;li&gt;地区id不知道如何获取的，请使用AreaTool.py获取&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-版本" class="anchor" aria-hidden="true" href="#版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;版本&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; python3&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-功能" class="anchor" aria-hidden="true" href="#功能"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;功能&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 检查登录&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 确认是否有货&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 有货自动下单&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 邮件、微信通知&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-更新记录" class="anchor" aria-hidden="true" href="#更新记录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新记录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;【2020.02.10】每10分钟自动读取一次配置修改商品不需要退出重开，优化有货，不支持省份出售情况，优化日志，加快查询频率。&lt;/li&gt;
&lt;li&gt;【2020.02.09】部分下单需要验证码识别问题，部分bug优化。&lt;/li&gt;
&lt;li&gt;【2020.02.08】V2版本，区分下单模式，config中错别字，bug修复。&lt;/li&gt;
&lt;li&gt;【2020.02.07】V3版本，减少提交订单的请求量，总而言之就是更快（只能监控一件商品）。&lt;/li&gt;
&lt;li&gt;【2020.02.07】无货等情况下单失败不重试。&lt;/li&gt;
&lt;li&gt;【2020.02.07】新增微信通知（&lt;a href="http://sc.ftqq.com/3.version" rel="nofollow"&gt;http://sc.ftqq.com/3.version&lt;/a&gt; 查看sc_key），bug修复。&lt;/li&gt;
&lt;li&gt;【2020.02.06】V2版本，刷新更快更频繁，通过配置文件添加商品和地区id。&lt;/li&gt;
&lt;li&gt;【2020.02.06】提交失败之后会继续不会暂停。&lt;/li&gt;
&lt;li&gt;【2020.02.06】购物车有套装商品导致解析skuid错误。&lt;/li&gt;
&lt;li&gt;【2020.02.05】商品有货，但是该商品已下柜，提交会报错，对部分代码进行了优化。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-反馈问题" class="anchor" aria-hidden="true" href="#反馈问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;反馈问题&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果有红包先花掉再开脚本，不然可能需要支付密码&lt;/li&gt;
&lt;li&gt;出现下单地址不是默认地址的，在线下一单，取getOrderInfo.action链接的cookie&lt;/li&gt;
&lt;li&gt;CMD界面卡住、关闭CMD的快速编辑模式就行了&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>cycz</author><guid isPermaLink="false">https://github.com/cycz/jdBuyMask</guid><pubDate>Wed, 12 Feb 2020 00:04:00 GMT</pubDate></item><item><title>xingyizhou/CenterNet #5 in Python, Today</title><link>https://github.com/xingyizhou/CenterNet</link><description>&lt;p&gt;&lt;i&gt;Object detection, 3D detection, and pose estimation using center point detection: &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-objects-as-points" class="anchor" aria-hidden="true" href="#objects-as-points"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Objects as Points&lt;/h1&gt;
&lt;p&gt;Object detection, 3D detection, and pose estimation using center point detection:
&lt;a target="_blank" rel="noopener noreferrer" href="readme/fig2.png"&gt;&lt;img src="readme/fig2.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1904.07850" rel="nofollow"&gt;&lt;strong&gt;Objects as Points&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt;
Xingyi Zhou, Dequan Wang, Philipp Krähenbühl,&lt;br&gt;
&lt;em&gt;arXiv technical report (&lt;a href="http://arxiv.org/abs/1904.07850" rel="nofollow"&gt;arXiv 1904.07850&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Contact: &lt;a href="mailto:zhouxy@cs.utexas.edu"&gt;zhouxy@cs.utexas.edu&lt;/a&gt;. Any questions or discussions are welcomed!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-abstract" class="anchor" aria-hidden="true" href="#abstract"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point -- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-highlights" class="anchor" aria-hidden="true" href="#highlights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Highlights&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Simple:&lt;/strong&gt; One-sentence method summary: use keypoint detection technic to detect the bounding box center point and regress to all other object properties like bounding box size, 3d information, and pose.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Versatile:&lt;/strong&gt; The same framework works for object detection, 3d bounding box estimation, and multi-person pose estimation with minor modification.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fast:&lt;/strong&gt; The whole process in a single network feedforward. No NMS post processing is needed. Our DLA-34 model runs at &lt;em&gt;52&lt;/em&gt; FPS with &lt;em&gt;37.4&lt;/em&gt; COCO AP.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Strong&lt;/strong&gt;: Our best single model achieves &lt;em&gt;45.1&lt;/em&gt;AP on COCO test-dev.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Easy to use:&lt;/strong&gt; We provide user friendly testing API and webcam demos.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-main-results" class="anchor" aria-hidden="true" href="#main-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main results&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-object-detection-on-coco-validation" class="anchor" aria-hidden="true" href="#object-detection-on-coco-validation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Object Detection on COCO validation&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Backbone&lt;/th&gt;
&lt;th&gt;AP / FPS&lt;/th&gt;
&lt;th&gt;Flip AP / FPS&lt;/th&gt;
&lt;th&gt;Multi-scale AP / FPS&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Hourglass-104&lt;/td&gt;
&lt;td&gt;40.3 / 14&lt;/td&gt;
&lt;td&gt;42.2 / 7.8&lt;/td&gt;
&lt;td&gt;45.1 / 1.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DLA-34&lt;/td&gt;
&lt;td&gt;37.4 / 52&lt;/td&gt;
&lt;td&gt;39.2 / 28&lt;/td&gt;
&lt;td&gt;41.7 / 4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet-101&lt;/td&gt;
&lt;td&gt;34.6 / 45&lt;/td&gt;
&lt;td&gt;36.2 / 25&lt;/td&gt;
&lt;td&gt;39.3 / 4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet-18&lt;/td&gt;
&lt;td&gt;28.1 / 142&lt;/td&gt;
&lt;td&gt;30.0 / 71&lt;/td&gt;
&lt;td&gt;33.2 / 12&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-keypoint-detection-on-coco-validation" class="anchor" aria-hidden="true" href="#keypoint-detection-on-coco-validation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Keypoint detection on COCO validation&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Backbone&lt;/th&gt;
&lt;th&gt;AP&lt;/th&gt;
&lt;th&gt;FPS&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Hourglass-104&lt;/td&gt;
&lt;td&gt;64.0&lt;/td&gt;
&lt;td&gt;6.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DLA-34&lt;/td&gt;
&lt;td&gt;58.9&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-3d-bounding-box-detection-on-kitti-validation" class="anchor" aria-hidden="true" href="#3d-bounding-box-detection-on-kitti-validation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3D bounding box detection on KITTI validation&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Backbone&lt;/th&gt;
&lt;th&gt;FPS&lt;/th&gt;
&lt;th&gt;AP-E&lt;/th&gt;
&lt;th&gt;AP-M&lt;/th&gt;
&lt;th&gt;AP-H&lt;/th&gt;
&lt;th&gt;AOS-E&lt;/th&gt;
&lt;th&gt;AOS-M&lt;/th&gt;
&lt;th&gt;AOS-H&lt;/th&gt;
&lt;th&gt;BEV-E&lt;/th&gt;
&lt;th&gt;BEV-M&lt;/th&gt;
&lt;th&gt;BEV-H&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;DLA-34&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;96.9&lt;/td&gt;
&lt;td&gt;87.8&lt;/td&gt;
&lt;td&gt;79.2&lt;/td&gt;
&lt;td&gt;93.9&lt;/td&gt;
&lt;td&gt;84.3&lt;/td&gt;
&lt;td&gt;75.7&lt;/td&gt;
&lt;td&gt;34.0&lt;/td&gt;
&lt;td&gt;30.5&lt;/td&gt;
&lt;td&gt;26.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;All models and details are available in our &lt;a href="readme/MODEL_ZOO.md"&gt;Model zoo&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;Please refer to &lt;a href="readme/INSTALL.md"&gt;INSTALL.md&lt;/a&gt; for installation instructions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-use-centernet" class="anchor" aria-hidden="true" href="#use-centernet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use CenterNet&lt;/h2&gt;
&lt;p&gt;We support demo for image/ image folder, video, and webcam.&lt;/p&gt;
&lt;p&gt;First, download the models (By default, &lt;a href="https://drive.google.com/open?id=1pl_-ael8wERdUREEnaIfqOV_VF2bEVRT" rel="nofollow"&gt;ctdet_coco_dla_2x&lt;/a&gt; for detection and
&lt;a href="https://drive.google.com/open?id=1PO1Ax_GDtjiemEmDVD7oPWwqQkUu28PI" rel="nofollow"&gt;multi_pose_dla_3x&lt;/a&gt; for human pose estimation)
from the &lt;a href="readme/MODEL_ZOO.md"&gt;Model zoo&lt;/a&gt; and put them in &lt;code&gt;CenterNet_ROOT/models/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For object detection on images/ video, run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python demo.py ctdet --demo /path/to/image/or/folder/or/video --load_model ../models/ctdet_coco_dla_2x.pth
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We provide example images in &lt;code&gt;CenterNet_ROOT/images/&lt;/code&gt; (from &lt;a href="https://github.com/facebookresearch/Detectron/tree/master/demo"&gt;Detectron&lt;/a&gt;). If set up correctly, the output should look like&lt;/p&gt;
&lt;p align="center"&gt; &lt;a target="_blank" rel="noopener noreferrer" href="readme/det1.png"&gt;&lt;img src="readme/det1.png" align="center" height="230px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="readme/det2.png"&gt;&lt;img src="readme/det2.png" align="center" height="230px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;For webcam demo, run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python demo.py ctdet --demo webcam --load_model ../models/ctdet_coco_dla_2x.pth
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, for human pose estimation, run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python demo.py multi_pose --demo /path/to/image/or/folder/or/video/or/webcam --load_model ../models/multi_pose_dla_3x.pth
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result for the example images should look like:&lt;/p&gt;
&lt;p align="center"&gt;  &lt;a target="_blank" rel="noopener noreferrer" href="readme/pose1.png"&gt;&lt;img src="readme/pose1.png" align="center" height="200px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="readme/pose2.png"&gt;&lt;img src="readme/pose2.png" align="center" height="200px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="readme/pose3.png"&gt;&lt;img src="readme/pose3.png" align="center" height="200px" style="max-width:100%;"&gt;&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;You can add &lt;code&gt;--debug 2&lt;/code&gt; to visualize the heatmap outputs.
You can add &lt;code&gt;--flip_test&lt;/code&gt; for flip test.&lt;/p&gt;
&lt;p&gt;To use this CenterNet in your own project, you can&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import sys
CENTERNET_PATH = /path/to/CenterNet/src/lib/
sys.path.insert(0, CENTERNET_PATH)

from detectors.detector_factory import detector_factory
from opts import opts

MODEL_PATH = /path/to/model
TASK = 'ctdet' # or 'multi_pose' for human pose estimation
opt = opts().init('{} --load_model {}'.format(TASK, MODEL_PATH).split(' '))
detector = detector_factory[opt.task](opt)

img = image/or/path/to/your/image/
ret = detector.run(img)['results']
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;ret&lt;/code&gt; will be a python dict: &lt;code&gt;{category_id : [[x1, y1, x2, y2, score], ...], }&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-benchmark-evaluation-and-training" class="anchor" aria-hidden="true" href="#benchmark-evaluation-and-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmark Evaluation and Training&lt;/h2&gt;
&lt;p&gt;After &lt;a href="readme/INSTALL.md"&gt;installation&lt;/a&gt;, follow the instructions in &lt;a href="readme/DATA.md"&gt;DATA.md&lt;/a&gt; to setup the datasets. Then check &lt;a href="readme/GETTING_STARTED.md"&gt;GETTING_STARTED.md&lt;/a&gt; to reproduce the results in the paper.
We provide scripts for all the experiments in the &lt;a href="experiments"&gt;experiments&lt;/a&gt; folder.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-develop" class="anchor" aria-hidden="true" href="#develop"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Develop&lt;/h2&gt;
&lt;p&gt;If you are interested in training CenterNet in a new dataset, use CenterNet in a new task, or use a new network architecture for CenterNet, please refer to &lt;a href="readme/DEVELOP.md"&gt;DEVELOP.md&lt;/a&gt;. Also feel free to send us emails for discussions or suggestions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-third-party-resources" class="anchor" aria-hidden="true" href="#third-party-resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Third-party resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Keras Implementation: &lt;a href="https://github.com/see--/keras-centernet"&gt;keras-centernet&lt;/a&gt; from &lt;a href="https://github.com/see--"&gt;see--&lt;/a&gt; and &lt;a href="https://github.com/xuannianz/keras-CenterNet"&gt;keras-CenterNet&lt;/a&gt; from &lt;a href="https://github.com/xuannianz"&gt;xuannianz&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;CenterNet + DeepSORT tracking implementation: &lt;a href="https://github.com/kimyoon-young/centerNet-deep-sort"&gt;centerNet-deep-sort&lt;/a&gt; from &lt;a href="https://github.com/kimyoon-young"&gt;kimyoon-young&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Blogs on training CenterNet on custom datasets (in Chinese): &lt;a href="https://blog.csdn.net/weixin_42634342/article/details/97756458" rel="nofollow"&gt;ships&lt;/a&gt; from &lt;a href="https://blog.csdn.net/weixin_42634342" rel="nofollow"&gt;Rhett Chen&lt;/a&gt; and &lt;a href="https://blog.csdn.net/weixin_41765699/article/details/100118353" rel="nofollow"&gt;faces&lt;/a&gt; from &lt;a href="https://me.csdn.net/weixin_41765699" rel="nofollow"&gt;linbior&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;CenterNet itself is released under the MIT License (refer to the LICENSE file for details).
Portions of the code are borrowed from &lt;a href="https://github.com/Microsoft/human-pose-estimation.pytorch"&gt;human-pose-estimation.pytorch&lt;/a&gt; (image transform, resnet), &lt;a href="https://github.com/princeton-vl/CornerNet"&gt;CornerNet&lt;/a&gt; (hourglassnet, loss functions), &lt;a href="https://github.com/ucbdrive/dla"&gt;dla&lt;/a&gt; (DLA network), &lt;a href="https://github.com/CharlesShang/DCNv2"&gt;DCNv2&lt;/a&gt;(deformable convolutions), &lt;a href="https://github.com/endernewton/tf-faster-rcnn"&gt;tf-faster-rcnn&lt;/a&gt;(Pascal VOC evaluation) and &lt;a href="https://github.com/prclibo/kitti_eval"&gt;kitti_eval&lt;/a&gt; (KITTI dataset evaluation). Please refer to the original License of these projects (See &lt;a href="NOTICE"&gt;NOTICE&lt;/a&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you find this project useful for your research, please use the following BibTeX entry.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{zhou2019objects,
  title={Objects as Points},
  author={Zhou, Xingyi and Wang, Dequan and Kr{\"a}henb{\"u}hl, Philipp},
  booktitle={arXiv preprint arXiv:1904.07850},
  year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>xingyizhou</author><guid isPermaLink="false">https://github.com/xingyizhou/CenterNet</guid><pubDate>Wed, 12 Feb 2020 00:05:00 GMT</pubDate></item><item><title>tlbootcamp/tlroadmap #6 in Python, Today</title><link>https://github.com/tlbootcamp/tlroadmap</link><description>&lt;p&gt;&lt;i&gt;👩🏼‍💻👨🏻‍💻Карта навыков и модель развития тимлидов&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="logo.png"&gt;&lt;img src="logo.png" alt="Logotype" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Тимлид – это &lt;g-emoji class="g-emoji" alias="snowflake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2744.png"&gt;❄️&lt;/g-emoji&gt;, потому что в каждой компании он уникален и неповторим.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#roadmap"&gt;Роадмап&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tlinks.run/tlbootcamp" rel="nofollow"&gt;Telegram-чат TL Bootcamp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="translations/README-en.md"&gt;English&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-что-за-роадмап" class="anchor" aria-hidden="true" href="#что-за-роадмап"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Что за роадмап?&lt;/h1&gt;
&lt;p&gt;Понятия "тимлид", "техлид", "техрук" и прочие – очень растяжимы. Компании решают разные задачи, у этих ролей разные ответственности. Помимо этого на картину накладываются разные стили руководства, различия в личностных навыках и многое другое. Но не тоже ли самое происходит с разработчиками? Разные компании, разные задачи, опять же – разный характер задач и ответственности.&lt;/p&gt;
&lt;p&gt;Мы опросили несколько десятков крупных и небольших компаний, выяснили, что они ожидают от тимлидов и как строят с ними свою работу. В результате мы смогли сформировать единую модель базовых компетенций тимлида, которая покрывает все исследованные случаи.&lt;/p&gt;
&lt;p&gt;Эту модель можно использовать как угодно – для составления собственного плана развития, для формирования должностных инструкций в компаниях, для составления вакансий или проведения собеседований. Учтите, что скорее всего вам нужны не все ветви потенциального развития – и это нормально.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="rotating_light" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a8.png"&gt;🚨&lt;/g-emoji&gt; &lt;strong&gt;Важно:&lt;/strong&gt; В зависимости от компании тимлиду нужно качать разные ветки и обязанности. Мы не считаем, что целью должно быть развитие во всех приведённых областях в роадмапе – в первую очередь нужно ориентироваться на проблемы и потребности своего места работы.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Работа ещё в процессе, поэтому не стесняйтесь публиковать Issue, закидывать свои pull request и присоединяться к обсуждению &lt;a href="https://tlinks.run/tlbootcamp" rel="nofollow"&gt;в Telegram-чате&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-как-использовать" class="anchor" aria-hidden="true" href="#как-использовать"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Как использовать?&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content--для-компании" class="anchor" aria-hidden="true" href="#-для-компании"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="school" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3eb.png"&gt;🏫&lt;/g-emoji&gt; Для компании&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Скачайте себе &lt;a href="https://cdn.jsdelivr.net/gh/tlbootcamp/tlroadmap@master/roadmap.mm" rel="nofollow"&gt;Mindmap&lt;/a&gt; с полной моделью тимлида.&lt;/li&gt;
&lt;li&gt;Изучите все ветви обязанностей тимлида. Удалите те, которые в вашей компании не требуются, либо уже выполняются кем-то ещё.&lt;/li&gt;
&lt;li&gt;Сформируйте из получившейся карты нужные вам артефакты: профиль для найма, описание ожиданий от роли, план развития.&lt;/li&gt;
&lt;li&gt;Для углубления в любую из веток используйте нашу базу знаний. Для каждой из веток мы детально описываем её смысл, мотивацию к использованию, примеры хорошего и плохого поведения, способы развития на практике и в теории.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content--для-себя" class="anchor" aria-hidden="true" href="#-для-себя"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;👩🏼‍💻 Для себя&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Скачайте себе &lt;a href="https://cdn.jsdelivr.net/gh/tlbootcamp/tlroadmap@master/roadmap.mm" rel="nofollow"&gt;Mindmap&lt;/a&gt; с полной моделью тимлида.&lt;/li&gt;
&lt;li&gt;Отметьте на нем те компетенции, которыми вы уже обладаете и те, которые вам требуются для дальнейшего роста внутри компании. Для подсказки – посмотрите на то, чем занимается ваш руководитель или коллеги. Если тут все ещё есть сложности – задайте вопрос &lt;a href="https://tlinks.run/tlbootcamp" rel="nofollow"&gt;в нашем чате&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Составьте список с теми компетенциями, которые находятся между вашим текущим профилем и целевым.&lt;/li&gt;
&lt;li&gt;Используя нашу базу знаний, сформируйте себе план развития по каждой из компетенций, который включает в себя теорию, консультации и практическое применение.&lt;/li&gt;
&lt;li&gt;Покажите свой план развития руководителю и попросите содействовать в нем.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-roadmap" class="anchor" aria-hidden="true" href="#roadmap"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Roadmap&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="roadmap.png"&gt;&lt;img src="roadmap.png" alt="Карта Тимлида" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Роадмап содержит в себе два раздела:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Роли и обязанности.&lt;/strong&gt; Перечень высокоуровневых рабочих ролей и более конкретных обязанностей и зон ответственности.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Personal Skills.&lt;/strong&gt; Личные навыки и качества, наличие которых необходимо для определённых ролей и обязанностей.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Каждый из листьев карты со временем превратится в большую запись в базе знаний, содержащую описание, примеры поведения, способы прокачки навыка.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-скачать-роадмап" class="anchor" aria-hidden="true" href="#скачать-роадмап"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Скачать роадмап&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cdn.jsdelivr.net/gh/tlbootcamp/tlroadmap@master/roadmap.png" rel="nofollow"&gt;PNG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cdn.jsdelivr.net/gh/tlbootcamp/tlroadmap@master/roadmap.mm" rel="nofollow"&gt;Mindmap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;mm&lt;/code&gt;-файл проще всего открывать в &lt;a href="https://sourceforge.net/projects/freemind/" rel="nofollow"&gt;Freemind&lt;/a&gt; (бесплатно и сердито), &lt;a href="https://mindnode.com/" rel="nofollow"&gt;MindNode&lt;/a&gt; (дорого и премиально) или &lt;a href="https://mindmeister.com" rel="nofollow"&gt;MindMeister&lt;/a&gt; (условно-бесплатно и онлайн). Скорее всего подойдут и другие редакторы майндмепов, но их работоспособность мы не проверяли.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-примеры-использования" class="anchor" aria-hidden="true" href="#примеры-использования"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Примеры использования&lt;/h2&gt;
&lt;p&gt;Мы собираем профили тимлидов из разных компаний. Если вы использовали роадмап, &lt;a href="CONTRIBUTING.md"&gt;не забудьте пошарить&lt;/a&gt;, что у вас получилось! С их помощью можно посмотреть, как работать с роадмапом, либо просто забрать их в свою компанию как примеры:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Technical Unit Lead в Авито: &lt;a href="https://cdn.jsdelivr.net/gh/tlbootcamp/tlroadmap@master/examples/avito.mm" rel="nofollow"&gt;mindmap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Technical Lead мобильных команд в Туту: &lt;a href="https://bit.ly/2YtkEwa" rel="nofollow"&gt;puml&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-work-in-progress" class="anchor" aria-hidden="true" href="#work-in-progress"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Work in progress&lt;/h1&gt;
&lt;p&gt;Текущий роадмап – не финальный, пока наполнена только половина. Мы будем очень рады вашему участию в проекте – а про варианты и сам процесс подробно написали в &lt;a href="CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-база-знаний" class="anchor" aria-hidden="true" href="#база-знаний"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;База знаний&lt;/h1&gt;
&lt;p&gt;База наполняется постепенно Стасом Цыгановым, Егором Толстым и сообществом. Наша итоговая цель – описать каждое из направлений по следующему шаблону:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Описание (о чем идёт речь)&lt;/li&gt;
&lt;li&gt;Почему ветка важна&lt;/li&gt;
&lt;li&gt;Что будет, если её не делать&lt;/li&gt;
&lt;li&gt;На кого может быть делегирована&lt;/li&gt;
&lt;li&gt;Примеры хорошего и плохого поведения&lt;/li&gt;
&lt;li&gt;Способы прокачки: навыки, практика, консультации, теория&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Если вы не согласны с каким-то из существующих описаний, или считаете, что можете его улучшить – присылайте свои pull request. Если есть вопросы по самой структуре – точно так же, добро пожаловать &lt;a href="https://tlinks.run/tlbootcamp" rel="nofollow"&gt;в наш чат&lt;/a&gt; или сразу в pull request.&lt;/p&gt;

&lt;h2&gt;&lt;a id="user-content-роли-и-обязанности" class="anchor" aria-hidden="true" href="#роли-и-обязанности"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Роли и обязанности&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-people-manager" class="anchor" aria-hidden="true" href="#people-manager"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;People Manager&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Управление людьми
&lt;ul&gt;
&lt;li&gt;Найм
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/profile.md"&gt;Профиль кандидата&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/interview.md"&gt;Собеседования&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/onboarding.md"&gt;Онбординг&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/test-period.md"&gt;Тестовый период&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/firing.md"&gt;Увольнение&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/motivation.md"&gt;Мотивация&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/development.md"&gt;Развитие&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Промо
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/assessment.md"&gt;Ассессмент&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/career.md"&gt;Карьерная линейка&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/feedback.md"&gt;Обратная связь&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/one-to-one.md"&gt;One-to-one&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/admin.md"&gt;Административная работа&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/delegation.md"&gt;Делегирование&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Управление командой
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/team-launch.md"&gt;Запуск команды&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/competency-management.md"&gt;Управление компетенциями&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/team-design.md"&gt;Дизайн команды&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/workspace.md"&gt;Организация рабочего пространства&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/team-climate.md"&gt;Климат в команде&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/team-maturity.md"&gt;Зрелость команды&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/transparency.md"&gt;Обеспечение прозрачности&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/resource-manager/techpr.md"&gt;Развитие технического бренда&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-product-owner" class="anchor" aria-hidden="true" href="#product-owner"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Product Owner&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Понимание продукта
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/product-owner/market-knowledge.md"&gt;Знание рынка&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/product-owner/user-knowledge.md"&gt;Знание пользователей&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Знание продукта&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Принятие продуктовых решений
&lt;ul&gt;
&lt;li&gt;Целеполагание&lt;/li&gt;
&lt;li&gt;Управление продуктовым бэклогом
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/product-owner/backlog-generation.md"&gt;Генерация элементов бэклога&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/product-owner/backlog-prioritization.md"&gt;Приоритизация бэклога&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-integrator" class="anchor" aria-hidden="true" href="#integrator"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Integrator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/integrator/business-knowledge.md"&gt;Знание бизнеса&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/integrator/corporate-culture.md"&gt;Корпоративная культура&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/integrator/company-structure.md"&gt;Структура компании&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-technical-lead" class="anchor" aria-hidden="true" href="#technical-lead"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Technical Lead&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Обеспечение качества продукта
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/technical-lead/code-review.md"&gt;Code Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Тестирование
&lt;ul&gt;
&lt;li&gt;Тестовая модель&lt;/li&gt;
&lt;li&gt;Тест-планы&lt;/li&gt;
&lt;li&gt;Пирамида тестирования&lt;/li&gt;
&lt;li&gt;Оптимизация количества тестирования&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Работа с багами&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/technical-lead/incident-management.md"&gt;Управление инцидентами&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Метрики и мониторинг&lt;/li&gt;
&lt;li&gt;Нефункциональные требования&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Обеспечение технического качества
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/technical-lead/unit-testing.md"&gt;Unit-тестирование&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Чистый код&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/technical-lead/refactoring.md"&gt;Рефакторинг&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Управление техническим долгом&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Знание технологий
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/technical-lead/code.md"&gt;Написание кода&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Знание технологического стека команды&lt;/li&gt;
&lt;li&gt;Выбор технологий и контроль их стека&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Автоматизация цикла разработки
&lt;ul&gt;
&lt;li&gt;Работа с VCS&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/technical-lead/ci.md"&gt;CI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Автоматизация релизов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Управление знаниями
&lt;ul&gt;
&lt;li&gt;Распространение знаний&lt;/li&gt;
&lt;li&gt;Техническая документация&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Архитектура
&lt;ul&gt;
&lt;li&gt;Сбор технических требований&lt;/li&gt;
&lt;li&gt;Проектирование&lt;/li&gt;
&lt;li&gt;Архитектурные ревью&lt;/li&gt;
&lt;li&gt;Эволюция&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Capacity Management&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-administrator" class="anchor" aria-hidden="true" href="#administrator"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Administrator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Построение цикла разработки
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/administrator/task-inbox.md"&gt;Получение задач&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Разработка
&lt;ul&gt;
&lt;li&gt;Конструирование методологии
&lt;ul&gt;
&lt;li&gt;Описание&lt;/li&gt;
&lt;li&gt;Оценка&lt;/li&gt;
&lt;li&gt;Движение&lt;/li&gt;
&lt;li&gt;Контроль выполнения&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Готовые подходы
&lt;ul&gt;
&lt;li&gt;Scrum&lt;/li&gt;
&lt;li&gt;Lean&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Выпуск задач
&lt;ul&gt;
&lt;li&gt;Приёмка&lt;/li&gt;
&lt;li&gt;Раскатка&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Проектное управление
&lt;ul&gt;
&lt;li&gt;Конструирование методологии&lt;/li&gt;
&lt;li&gt;Готовые подходы
&lt;ul&gt;
&lt;li&gt;PMBoK&lt;/li&gt;
&lt;li&gt;PDCA&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Стейкхолдинг&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-personal-skills" class="anchor" aria-hidden="true" href="#personal-skills"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Personal Skills&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-развитие-себя" class="anchor" aria-hidden="true" href="#развитие-себя"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Развитие себя&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/learning.md"&gt;Умение учиться&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/reflection.md"&gt;Рефлексия&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/habits.md"&gt;Работа с привычками&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-отношения" class="anchor" aria-hidden="true" href="#отношения"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Отношения&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Эмпатия&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/emotional-intelligence.md"&gt;Эмоциональный интеллект&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/diversity.md"&gt;Понимание ценности различий&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-мышление" class="anchor" aria-hidden="true" href="#мышление"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Мышление&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Системное мышление&lt;/li&gt;
&lt;li&gt;Стратегическое видение&lt;/li&gt;
&lt;li&gt;Принятие решений&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-стили-управления" class="anchor" aria-hidden="true" href="#стили-управления"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Стили управления&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-коммуникации" class="anchor" aria-hidden="true" href="#коммуникации"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Коммуникации&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/facilitation.md"&gt;Фасилитация&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/cooperation.md"&gt;Сотрудничество&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/conflicts.md"&gt;Управление конфликтами&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/feedback.md"&gt;Дача и получение обратной связи&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/networking.md"&gt;Нетворкинг&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/public-speaking.md"&gt;Публичные выступления&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/text.md"&gt;Работа с текстом&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-тайм-менеджмент" class="anchor" aria-hidden="true" href="#тайм-менеджмент"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Тайм-менеджмент&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Постановка личных целей&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tlbootcamp/tlroadmap/blob/master/skills/self-improvement/time-management.md"&gt;Управление временем&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Управление приоритетами&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;&lt;a id="user-content-лицензия" class="anchor" aria-hidden="true" href="#лицензия"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Лицензия&lt;/h1&gt;
&lt;p&gt;Мы используем &lt;a href="LICENSE.md"&gt;лицензию Creative Commons Attribution-ShareAlike 4.0&lt;/a&gt;. Если кратко, вы можете свободно:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Делиться (обмениваться)&lt;/strong&gt; — копировать и распространять материал на любом носителе и в любом формате.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Адаптировать (создавать производные материалы)&lt;/strong&gt; — делать ремиксы, видоизменять, и создавать новое, опираясь на этот материал в любых целях, включая коммерческие.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;При соблюдении следующих условий:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Вы должны обеспечить соответствующее &lt;strong&gt;указание авторства&lt;/strong&gt;, предоставить ссылку на лицензию, и обозначить изменения, если таковые были сделаны.&lt;/li&gt;
&lt;li&gt;Если вы перерабатываете, преобразовываете материал или берёте его за основу для производного произведения, вы должны распространять переделанные вами части материала &lt;strong&gt;на условиях той же лицензии&lt;/strong&gt;, в соответствии с которой распространяется оригинал.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-благодарности" class="anchor" aria-hidden="true" href="#благодарности"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Благодарности&lt;/h1&gt;
&lt;p&gt;Свой вклад в разработку карты сделали следующие люди: Анатолий Панов, Роман Ивлиев, Николай Крапивный, Георгий Могелашвили, Юлия Рахманова, Николай Балакирев, Александр Миненок, Павел Антонов, Виталий Шароватов, Александр Светкин, Марат Чукмаров.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tlbootcamp</author><guid isPermaLink="false">https://github.com/tlbootcamp/tlroadmap</guid><pubDate>Wed, 12 Feb 2020 00:06:00 GMT</pubDate></item><item><title>albumentations-team/albumentations #7 in Python, Today</title><link>https://github.com/albumentations-team/albumentations</link><description>&lt;p&gt;&lt;i&gt;fast image augmentation library and easy to use wrapper around other libraries&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-albumentations" class="anchor" aria-hidden="true" href="#albumentations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Albumentations&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://badge.fury.io/py/albumentations" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/848b50dcbb84f47cd402565323aae228665efc00/68747470733a2f2f62616467652e667572792e696f2f70792f616c62756d656e746174696f6e732e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/albumentations.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/albu/albumentations" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c6b7c15c5de4a7fe1912229c7764d08290dada23/68747470733a2f2f7472617669732d63692e6f72672f616c62756d656e746174696f6e732d7465616d2f616c62756d656e746174696f6e732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/albumentations-team/albumentations.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://albumentations.readthedocs.io/en/latest/?badge=latest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/416624ea6ead60bdeaa364be35afb166f4ae9ab4/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f616c62756d656e746174696f6e732f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/albumentations/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The library works with images in &lt;code&gt;HWC&lt;/code&gt; format.&lt;/li&gt;
&lt;li&gt;The library is faster than other libraries on most of the transformations.&lt;/li&gt;
&lt;li&gt;Based on numpy, OpenCV, imgaug picking the best from each of them.&lt;/li&gt;
&lt;li&gt;Simple, flexible API that allows the library to be used in any computer vision pipeline.&lt;/li&gt;
&lt;li&gt;Large, diverse set of transformations.&lt;/li&gt;
&lt;li&gt;Easy to extend the library to wrap around other libraries.&lt;/li&gt;
&lt;li&gt;Easy to extend to other tasks.&lt;/li&gt;
&lt;li&gt;Supports transformations on images, masks, key points and bounding boxes.&lt;/li&gt;
&lt;li&gt;Supports python 3.5-3.7&lt;/li&gt;
&lt;li&gt;Easy integration with PyTorch.&lt;/li&gt;
&lt;li&gt;Easy transfer from torchvision.&lt;/li&gt;
&lt;li&gt;Was used to get top results in many DL competitions at Kaggle, topcoder, CVPR, MICCAI.&lt;/li&gt;
&lt;li&gt;Written by Kaggle Masters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#how-to-use"&gt;How to use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#authors"&gt;Authors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#pypi"&gt;PyPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conda"&gt;Conda&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pixel-level-transforms"&gt;Pixel-level transforms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#spatial-level-transforms"&gt;Spatial-level transforms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#migrating-from-torchvision-to-albumentations"&gt;Migrating from torchvision to albumentations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#benchmarking-results"&gt;Benchmarking results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contributing"&gt;Contributing&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#adding-new-transforms"&gt;Adding new transforms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#building-the-documentation"&gt;Building the documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#comments"&gt;Comments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citing"&gt;Citing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#competitions-won-with-the-library"&gt;Competitions won with the library&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#used-by"&gt;Industry users&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;All in one showcase notebook&lt;/strong&gt; - &lt;a href="https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/showcase.ipynb"&gt;&lt;code&gt;showcase.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt; - &lt;a href="https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example.ipynb"&gt;&lt;code&gt;example.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Object detection&lt;/strong&gt; - &lt;a href="https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_bboxes.ipynb"&gt;&lt;code&gt;example_bboxes.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Non-8-bit images&lt;/strong&gt; - &lt;a href="https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_16_bit_tiff.ipynb"&gt;&lt;code&gt;example_16_bit_tiff.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Image segmentation&lt;/strong&gt; &lt;a href="https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_kaggle_salt.ipynb"&gt;&lt;code&gt;example_kaggle_salt.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Keypoints&lt;/strong&gt; &lt;a href="https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_keypoints.ipynb"&gt;&lt;code&gt;example_keypoints.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custom targets&lt;/strong&gt; &lt;a href="https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_multi_target.ipynb"&gt;&lt;code&gt;example_multi_target.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Weather transforms&lt;/strong&gt; &lt;a href="https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_weather_transforms.ipynb"&gt;&lt;code&gt;example_weather_transforms.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Serialization&lt;/strong&gt; &lt;a href="https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/serialization.ipynb"&gt;&lt;code&gt;serialization.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Replay/Deterministic mode&lt;/strong&gt; &lt;a href="https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/replay.ipynb"&gt;&lt;code&gt;replay.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can use this &lt;a href="https://colab.research.google.com/drive/1JuZ23u0C0gx93kV0oJ8Mq0B6CBYhPLXy#scrollTo=GwFN-In3iagp&amp;amp;forceEdit=true&amp;amp;offline=true&amp;amp;sandboxMode=true" rel="nofollow"&gt;Google Colaboratory notebook&lt;/a&gt;
to adjust image augmentation parameters and see the resulting images.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/fd2405ab170ab4739c029d7251f5f7b4fac3b41c/68747470733a2f2f686162726173746f726167652e6f72672f776562742f62642f6e652f72762f62646e6572763563746b75646d73617a6e687734637273646669772e6a706567"&gt;&lt;img src="https://camo.githubusercontent.com/fd2405ab170ab4739c029d7251f5f7b4fac3b41c/68747470733a2f2f686162726173746f726167652e6f72672f776562742f62642f6e652f72762f62646e6572763563746b75646d73617a6e687734637273646669772e6a706567" alt="parrot" data-canonical-src="https://habrastorage.org/webt/bd/ne/rv/bdnerv5ctkudmsaznhw4crsdfiw.jpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/43d652646b37ef66762212c0e0d3150ba481347c/68747470733a2f2f686162726173746f726167652e6f72672f776562742f73752f77612f6e702f737577616e70656f36777737777077746f6274727a645f636732302e6a706567"&gt;&lt;img src="https://camo.githubusercontent.com/43d652646b37ef66762212c0e0d3150ba481347c/68747470733a2f2f686162726173746f726167652e6f72672f776562742f73752f77612f6e702f737577616e70656f36777737777077746f6274727a645f636732302e6a706567" alt="inria" data-canonical-src="https://habrastorage.org/webt/su/wa/np/suwanpeo6ww7wpwtobtrzd_cg20.jpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/88e6cb57bed568473e99ee1addefc81865263390/68747470733a2f2f686162726173746f726167652e6f72672f776562742f31692f66692f777a2f31696669777a79306c78657463346e776a7673732d37316e6b77302e6a706567"&gt;&lt;img src="https://camo.githubusercontent.com/88e6cb57bed568473e99ee1addefc81865263390/68747470733a2f2f686162726173746f726167652e6f72672f776562742f31692f66692f777a2f31696669777a79306c78657463346e776a7673732d37316e6b77302e6a706567" alt="medical" data-canonical-src="https://habrastorage.org/webt/1i/fi/wz/1ifiwzy0lxetc4nwjvss-71nkw0.jpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/dfd2d57c087ff30d68958c4ff1aec17a5fdd6b77/68747470733a2f2f686162726173746f726167652e6f72672f776562742f727a2f2d682f336a2f727a2d68336a616c62786963386f5f6668756378797374733474632e6a706567"&gt;&lt;img src="https://camo.githubusercontent.com/dfd2d57c087ff30d68958c4ff1aec17a5fdd6b77/68747470733a2f2f686162726173746f726167652e6f72672f776562742f727a2f2d682f336a2f727a2d68336a616c62786963386f5f6668756378797374733474632e6a706567" alt="vistas" data-canonical-src="https://habrastorage.org/webt/rz/-h/3j/rz-h3jalbxic8o_fhucxysts4tc.jpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/041633dc5d522d6cf583a81d4a1d85be87f44155/68747470733a2f2f686162726173746f726167652e6f72672f776562742f652d2f366b2f7a2d2f652d366b7a2d66756770326865616b336a7a6e733362632d72386f2e6a706567"&gt;&lt;img src="https://camo.githubusercontent.com/041633dc5d522d6cf583a81d4a1d85be87f44155/68747470733a2f2f686162726173746f726167652e6f72672f776562742f652d2f366b2f7a2d2f652d366b7a2d66756770326865616b336a7a6e733362632d72386f2e6a706567" width="100%" data-canonical-src="https://habrastorage.org/webt/e-/6k/z-/e-6kz-fugp2heak3jzns3bc-r8o.jpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/al-buslaev/" rel="nofollow"&gt;Alexander Buslaev&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/alex-parinov/" rel="nofollow"&gt;Alex Parinov&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/iglovikov/" rel="nofollow"&gt;Vladimir I. Iglovikov&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/cvtalks/" rel="nofollow"&gt;Evegene Khvedchenya&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/mikhail-druzhinin-548229100/" rel="nofollow"&gt;Mikhail Druzhinin&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-pypi" class="anchor" aria-hidden="true" href="#pypi"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyPI&lt;/h3&gt;
&lt;p&gt;You can use pip to install albumentations:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install albumentations
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to get the latest version of the code before it is released on PyPI you can install the library from GitHub:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install -U git+https://github.com/albu/albumentations
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And it also works in Kaggle GPU kernels &lt;a href="https://www.kaggle.com/creafz/albumentations-installation/" rel="nofollow"&gt;(proof)&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;!pip install albumentations &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-conda" class="anchor" aria-hidden="true" href="#conda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conda&lt;/h3&gt;
&lt;p&gt;To install albumentations using conda we need first to install &lt;code&gt;imgaug&lt;/code&gt; via conda-forge collection&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install -c conda-forge imgaug
conda install albumentations -c albumentations
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;The full documentation is available at &lt;a href="https://albumentations.readthedocs.io/en/latest/" rel="nofollow"&gt;albumentations.readthedocs.io&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pixel-level-transforms" class="anchor" aria-hidden="true" href="#pixel-level-transforms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pixel-level transforms&lt;/h2&gt;
&lt;p&gt;Pixel-level transforms will change just an input image and will leave any additional targets such as masks, bounding boxes, and keypoints unchanged. The list of pixel-level transforms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Blur" rel="nofollow"&gt;Blur&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.CLAHE" rel="nofollow"&gt;CLAHE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ChannelDropout" rel="nofollow"&gt;ChannelDropout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ChannelShuffle" rel="nofollow"&gt;ChannelShuffle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.CoarseDropout" rel="nofollow"&gt;CoarseDropout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Cutout" rel="nofollow"&gt;Cutout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Downscale" rel="nofollow"&gt;Downscale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Equalize" rel="nofollow"&gt;Equalize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.FancyPCA" rel="nofollow"&gt;FancyPCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.FromFloat" rel="nofollow"&gt;FromFloat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.GaussNoise" rel="nofollow"&gt;GaussNoise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.GaussianBlur" rel="nofollow"&gt;GaussianBlur&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.HueSaturationValue" rel="nofollow"&gt;HueSaturationValue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAAdditiveGaussianNoise" rel="nofollow"&gt;IAAAdditiveGaussianNoise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAEmboss" rel="nofollow"&gt;IAAEmboss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAASharpen" rel="nofollow"&gt;IAASharpen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAASuperpixels" rel="nofollow"&gt;IAASuperpixels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ISONoise" rel="nofollow"&gt;ISONoise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ImageCompression" rel="nofollow"&gt;ImageCompression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.InvertImg" rel="nofollow"&gt;InvertImg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.JpegCompression" rel="nofollow"&gt;JpegCompression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.MedianBlur" rel="nofollow"&gt;MedianBlur&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.MotionBlur" rel="nofollow"&gt;MotionBlur&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.MultiplicativeNoise" rel="nofollow"&gt;MultiplicativeNoise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Normalize" rel="nofollow"&gt;Normalize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Posterize" rel="nofollow"&gt;Posterize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RGBShift" rel="nofollow"&gt;RGBShift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomBrightness" rel="nofollow"&gt;RandomBrightness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomBrightnessContrast" rel="nofollow"&gt;RandomBrightnessContrast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomContrast" rel="nofollow"&gt;RandomContrast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomFog" rel="nofollow"&gt;RandomFog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomGamma" rel="nofollow"&gt;RandomGamma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomRain" rel="nofollow"&gt;RandomRain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomShadow" rel="nofollow"&gt;RandomShadow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomSnow" rel="nofollow"&gt;RandomSnow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomSunFlare" rel="nofollow"&gt;RandomSunFlare&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.GlassBlur" rel="nofollow"&gt;GlassBlur&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Solarize" rel="nofollow"&gt;Solarize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ToFloat" rel="nofollow"&gt;ToFloat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ToGray" rel="nofollow"&gt;ToGray&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ToSepia" rel="nofollow"&gt;ToSepia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-spatial-level-transforms" class="anchor" aria-hidden="true" href="#spatial-level-transforms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Spatial-level transforms&lt;/h2&gt;
&lt;p&gt;Spatial-level transforms will simultaneously change both an input image as well as additional targets such as masks, bounding boxes, and keypoints. The following table shows which additional targets are supported by each transform.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Transform&lt;/th&gt;
&lt;th align="center"&gt;Image&lt;/th&gt;
&lt;th align="center"&gt;Masks&lt;/th&gt;
&lt;th align="center"&gt;BBoxes&lt;/th&gt;
&lt;th align="center"&gt;Keypoints&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.CenterCrop" rel="nofollow"&gt;CenterCrop&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Crop" rel="nofollow"&gt;Crop&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.CropNonEmptyMaskIfExists" rel="nofollow"&gt;CropNonEmptyMaskIfExists&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ElasticTransform" rel="nofollow"&gt;ElasticTransform&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Flip" rel="nofollow"&gt;Flip&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.GridDistortion" rel="nofollow"&gt;GridDistortion&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.GridDropout" rel="nofollow"&gt;GridDropout&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.HorizontalFlip" rel="nofollow"&gt;HorizontalFlip&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAAffine" rel="nofollow"&gt;IAAAffine&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAACropAndPad" rel="nofollow"&gt;IAACropAndPad&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAFliplr" rel="nofollow"&gt;IAAFliplr&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAFlipud" rel="nofollow"&gt;IAAFlipud&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAPerspective" rel="nofollow"&gt;IAAPerspective&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAPiecewiseAffine" rel="nofollow"&gt;IAAPiecewiseAffine&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Lambda" rel="nofollow"&gt;Lambda&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.LongestMaxSize" rel="nofollow"&gt;LongestMaxSize&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.MaskDropout" rel="nofollow"&gt;MaskDropout&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NoOp&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.OpticalDistortion" rel="nofollow"&gt;OpticalDistortion&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.PadIfNeeded" rel="nofollow"&gt;PadIfNeeded&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomCrop" rel="nofollow"&gt;RandomCrop&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomCropNearBBox" rel="nofollow"&gt;RandomCropNearBBox&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomGridShuffle" rel="nofollow"&gt;RandomGridShuffle&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomResizedCrop" rel="nofollow"&gt;RandomResizedCrop&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomRotate90" rel="nofollow"&gt;RandomRotate90&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomScale" rel="nofollow"&gt;RandomScale&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomSizedBBoxSafeCrop" rel="nofollow"&gt;RandomSizedBBoxSafeCrop&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomSizedCrop" rel="nofollow"&gt;RandomSizedCrop&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Resize" rel="nofollow"&gt;Resize&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Rotate" rel="nofollow"&gt;Rotate&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ShiftScaleRotate" rel="nofollow"&gt;ShiftScaleRotate&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.SmallestMaxSize" rel="nofollow"&gt;SmallestMaxSize&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Transpose" rel="nofollow"&gt;Transpose&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.VerticalFlip" rel="nofollow"&gt;VerticalFlip&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-migrating-from-torchvision-to-albumentations" class="anchor" aria-hidden="true" href="#migrating-from-torchvision-to-albumentations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Migrating from torchvision to albumentations&lt;/h2&gt;
&lt;p&gt;Migrating from torchvision to albumentations is simple - you just need to change a few lines of code.
Albumentations has equivalents for common torchvision transforms as well as plenty of transforms that are not presented in torchvision.
&lt;a href="https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/migrating_from_torchvision_to_albumentations.ipynb"&gt;&lt;code&gt;migrating_from_torchvision_to_albumentations.ipynb&lt;/code&gt;&lt;/a&gt; shows how one can migrate code from torchvision to albumentations.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-benchmarking-results" class="anchor" aria-hidden="true" href="#benchmarking-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmarking results&lt;/h2&gt;
&lt;p&gt;To run the benchmark yourself follow the instructions in &lt;a href="https://github.com/albu/albumentations/blob/master/benchmark/README.md"&gt;benchmark/README.md&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Results for running the benchmark on first 2000 images from the ImageNet validation set using an Intel Xeon Platinum 8168 CPU.
All outputs are converted to a contiguous NumPy array with the np.uint8 data type.
The table shows how many images per second can be processed on a single core, higher is better.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align="center"&gt;albumentations&lt;br&gt;0.4.2&lt;/th&gt;
&lt;th align="center"&gt;imgaug&lt;br&gt;0.3.0&lt;/th&gt;
&lt;th align="center"&gt;torchvision (Pillow-SIMD backend)&lt;br&gt;0.4.1&lt;/th&gt;
&lt;th align="center"&gt;keras&lt;br&gt;2.3.1&lt;/th&gt;
&lt;th align="center"&gt;augmentor&lt;br&gt;0.2.6&lt;/th&gt;
&lt;th align="center"&gt;solt&lt;br&gt;0.1.8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;HorizontalFlip&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2183&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;1403&lt;/td&gt;
&lt;td align="center"&gt;1757&lt;/td&gt;
&lt;td align="center"&gt;1068&lt;/td&gt;
&lt;td align="center"&gt;1779&lt;/td&gt;
&lt;td align="center"&gt;1031&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VerticalFlip&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;4217&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;2334&lt;/td&gt;
&lt;td align="center"&gt;1538&lt;/td&gt;
&lt;td align="center"&gt;4196&lt;/td&gt;
&lt;td align="center"&gt;1541&lt;/td&gt;
&lt;td align="center"&gt;3820&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Rotate&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;456&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;368&lt;/td&gt;
&lt;td align="center"&gt;163&lt;/td&gt;
&lt;td align="center"&gt;32&lt;/td&gt;
&lt;td align="center"&gt;60&lt;/td&gt;
&lt;td align="center"&gt;116&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ShiftScaleRotate&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;800&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;549&lt;/td&gt;
&lt;td align="center"&gt;146&lt;/td&gt;
&lt;td align="center"&gt;34&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brightness&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2209&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;1288&lt;/td&gt;
&lt;td align="center"&gt;405&lt;/td&gt;
&lt;td align="center"&gt;211&lt;/td&gt;
&lt;td align="center"&gt;403&lt;/td&gt;
&lt;td align="center"&gt;2070&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Contrast&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2215&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;1387&lt;/td&gt;
&lt;td align="center"&gt;338&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;337&lt;/td&gt;
&lt;td align="center"&gt;2073&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BrightnessContrast&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2208&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;740&lt;/td&gt;
&lt;td align="center"&gt;193&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;193&lt;/td&gt;
&lt;td align="center"&gt;1060&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ShiftRGB&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2214&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;1303&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;407&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ShiftHSV&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;468&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;443&lt;/td&gt;
&lt;td align="center"&gt;61&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;144&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gamma&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2281&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;730&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;925&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Grayscale&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;5019&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;436&lt;/td&gt;
&lt;td align="center"&gt;788&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;1451&lt;/td&gt;
&lt;td align="center"&gt;4191&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RandomCrop64&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;173877&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;3340&lt;/td&gt;
&lt;td align="center"&gt;43792&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;36869&lt;/td&gt;
&lt;td align="center"&gt;36178&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PadToSize512&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2906&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;553&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;2711&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resize512&lt;/td&gt;
&lt;td align="center"&gt;663&lt;/td&gt;
&lt;td align="center"&gt;506&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;968&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;954&lt;/td&gt;
&lt;td align="center"&gt;673&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RandomSizedCrop_64_512&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2565&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;933&lt;/td&gt;
&lt;td align="center"&gt;1395&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;1353&lt;/td&gt;
&lt;td align="center"&gt;2360&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Equalize&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;759&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;457&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;684&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Python and library versions: Python 3.7.5 (default, Oct 19 2019, 00:03:48) [GCC 8.3.0], numpy 1.17.3, pillow-simd 6.0.0.post0, opencv-python 4.1.1.26, scikit-image 0.16.2, scipy 1.3.1.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;To create a pull request to the repository follow the documentation at &lt;a href="docs/contributing.rst"&gt;docs/contributing.rst&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-adding-new-transforms" class="anchor" aria-hidden="true" href="#adding-new-transforms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding new transforms&lt;/h3&gt;
&lt;p&gt;If you are contributing a new transformation, make sure to update &lt;a href="#pixel-level-transforms"&gt;"Pixel-level transforms"&lt;/a&gt; or/and &lt;a href="#spatial-level-transforms"&gt;"Spatial-level transforms"&lt;/a&gt; sections of this file (&lt;code&gt;README.md&lt;/code&gt;). To do this, simply run (with python3 only):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python3 tools/make_transforms_docs.py make
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and copy/paste the results into the corresponding sections. To validate your modifications, you
can run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python3 tools/make_transforms_docs.py check README.md
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-building-the-documentation" class="anchor" aria-hidden="true" href="#building-the-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building the documentation&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Go to &lt;code&gt;docs/&lt;/code&gt; directory
&lt;pre&gt;&lt;code&gt;cd docs
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Install required libraries
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Build html files
&lt;pre&gt;&lt;code&gt;make html
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Open &lt;code&gt;_build/html/index.html&lt;/code&gt; in browser.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Alternatively, you can start a web server that rebuilds the documentation
automatically when a change is detected by running &lt;code&gt;make livehtml&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-competitions-won-with-the-library" class="anchor" aria-hidden="true" href="#competitions-won-with-the-library"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Competitions won with the library&lt;/h2&gt;
&lt;p&gt;Albumentations are widely used in Computer Vision Competitions at Kaggle an other platforms.&lt;/p&gt;
&lt;p&gt;You can find their names and links to the solutions &lt;a href="docs/hall_of_fame.rst"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-used-by" class="anchor" aria-hidden="true" href="#used-by"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Used by&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.lyft.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ec245c80a1442100762e1e23dff14e110beedfad/68747470733a2f2f686162726173746f726167652e6f72672f776562742f63652f62732f73612f6365627373616a665f3561737374357973686d79796b716a6863672e706e67" width="100" data-canonical-src="https://habrastorage.org/webt/ce/bs/sa/cebssajf_5asst5yshmyykqjhcg.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.x5.ru/en" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/db5e57318cc14a7aba9b56b12100a62e3547b85a/68747470733a2f2f686162726173746f726167652e6f72672f776562742f39792f64762f66312f3979647666316662786f746b6c366e79687964726e3976386371772e706e67" width="100" data-canonical-src="https://habrastorage.org/webt/9y/dv/f1/9ydvf1fbxotkl6nyhydrn9v8cqw.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://imedhub.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ece351193f2589965b06f7c9538ee04a34cbfbb3/68747470733a2f2f686162726173746f726167652e6f72672f776562742f65712f38782f6d2d2f657138786d2d666a66785f75716b6b61345f656b787364777469712e706e67" width="100" data-canonical-src="https://habrastorage.org/webt/eq/8x/m-/eq8xm-fjfx_uqkka4_ekxsdwtiq.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://recursionpharma.com" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/60a3a096009a3f2f47878e4a0618551334606816/68747470733a2f2f7062732e7477696d672e636f6d2f70726f66696c655f696d616765732f3932353839373839373136353633393638332f6a493859764266435f343030783430302e6a7067" width="100" data-canonical-src="https://pbs.twimg.com/profile_images/925897897165639683/jI8YvBfC_400x400.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-comments" class="anchor" aria-hidden="true" href="#comments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Comments&lt;/h2&gt;
&lt;p&gt;In some systems, in the multiple GPU regime PyTorch may deadlock the DataLoader if OpenCV was compiled with OpenCL optimizations. Adding the following two lines before the library import may help. For more details &lt;a href="https://github.com/pytorch/pytorch/issues/1355"&gt;https://github.com/pytorch/pytorch/issues/1355&lt;/a&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;cv2.setNumThreads(&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
cv2.ocl.setUseOpenCL(&lt;span class="pl-c1"&gt;False&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h1&gt;
&lt;p&gt;If you find this library useful for your research, please consider citing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{2018arXiv180906839B,
    author = {A. Buslaev, A. Parinov, E. Khvedchenya, V.~I. Iglovikov and A.~A. Kalinin},
     title = "{Albumentations: fast and flexible image augmentations}",
   journal = {ArXiv e-prints},
    eprint = {1809.06839},
      year = 2018
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find the full list of papers that cite Albumentations &lt;a href="docs/citations.rst"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>albumentations-team</author><guid isPermaLink="false">https://github.com/albumentations-team/albumentations</guid><pubDate>Wed, 12 Feb 2020 00:07:00 GMT</pubDate></item><item><title>mingrammer/diagrams #8 in Python, Today</title><link>https://github.com/mingrammer/diagrams</link><description>&lt;p&gt;&lt;i&gt;:art: Diagram as Code for prototyping cloud system architectures&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="assets/img/diagrams.png"&gt;&lt;img src="assets/img/diagrams.png" alt="diagrams logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-diagrams" class="anchor" aria-hidden="true" href="#diagrams"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Diagrams&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Diagram as Code&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Diagrams lets you draw the cloud system architecture &lt;strong&gt;in Python code&lt;/strong&gt;. It was born for &lt;strong&gt;prototyping&lt;/strong&gt; a new system architecture design without any design tools. You can also describe or visualize the existing system architecture as well. Diagrams currently supports four major providers: &lt;code&gt;AWS&lt;/code&gt;, &lt;code&gt;Azure&lt;/code&gt;, &lt;code&gt;GCP&lt;/code&gt; and &lt;code&gt;Kubernetes&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Diagram as Code&lt;/strong&gt; also allows you to &lt;strong&gt;tracking&lt;/strong&gt; the architecture diagram changes on any &lt;strong&gt;version control&lt;/strong&gt; system.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: It does not control any actual cloud resources nor generate cloud formation or terraform code, but just for drawing the cloud system architecture diagrams.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;It uses &lt;a href="https://www.graphviz.org/" rel="nofollow"&gt;Graphviz&lt;/a&gt; to render the diagram, so you need to &lt;a href="https://graphviz.gitlab.io/download/" rel="nofollow"&gt;install Graphviz&lt;/a&gt; to use &lt;strong&gt;diagrams&lt;/strong&gt;. After installing graphviz (or already have it), install the &lt;strong&gt;diagrams&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;macOS users can download the Graphviz via &lt;code&gt;brew install graphviz&lt;/code&gt; if you're using &lt;a href="https://brew.sh" rel="nofollow"&gt;Homebrew&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; using pip (pip3)&lt;/span&gt;
$ pip install diagrams

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; using pipenv&lt;/span&gt;
$ pipenv install diagrams

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; using poetry&lt;/span&gt;
$ poetry add diagrams&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can start with &lt;a href="https://diagrams.mingrammer.com/docs/installation#quick-start" rel="nofollow"&gt;quick start&lt;/a&gt;. And you can go &lt;a href="https://diagrams.mingrammer.com/docs/diagram" rel="nofollow"&gt;guides&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Grouped Workers on AWS&lt;/th&gt;
&lt;th&gt;Stateful Architecture on k8s&lt;/th&gt;
&lt;th&gt;Event Processing on AWS&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/8ee5462cadc13fc03db820dc5754b27a0cb737ad/68747470733a2f2f6469616772616d732e6d696e6772616d6d65722e636f6d2f696d672f67726f757065645f776f726b6572735f6469616772616d2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/8ee5462cadc13fc03db820dc5754b27a0cb737ad/68747470733a2f2f6469616772616d732e6d696e6772616d6d65722e636f6d2f696d672f67726f757065645f776f726b6572735f6469616772616d2e706e67" alt="grouped workers" data-canonical-src="https://diagrams.mingrammer.com/img/grouped_workers_diagram.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ebe2a0226af966beab2bf10a43d2d64a422b52f1/68747470733a2f2f6469616772616d732e6d696e6772616d6d65722e636f6d2f696d672f737461746566756c5f6172636869746563747572655f6469616772616d2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/ebe2a0226af966beab2bf10a43d2d64a422b52f1/68747470733a2f2f6469616772616d732e6d696e6772616d6d65722e636f6d2f696d672f737461746566756c5f6172636869746563747572655f6469616772616d2e706e67" alt="stateful architecture" data-canonical-src="https://diagrams.mingrammer.com/img/stateful_architecture_diagram.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/9d639e9f0db69fc7bc55c9df65d1b1d4a9944bce/68747470733a2f2f6469616772616d732e6d696e6772616d6d65722e636f6d2f696d672f6576656e745f70726f63657373696e675f6469616772616d2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/9d639e9f0db69fc7bc55c9df65d1b1d4a9944bce/68747470733a2f2f6469616772616d732e6d696e6772616d6d65722e636f6d2f696d672f6576656e745f70726f63657373696e675f6469616772616d2e706e67" alt="event processing" data-canonical-src="https://diagrams.mingrammer.com/img/event_processing_diagram.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can find all the examples on the &lt;a href="https://diagrams.mingrammer.com/docs/examples" rel="nofollow"&gt;examples&lt;/a&gt; page.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;To contribute to diagram, check out &lt;a href="CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Let me know if you are using diagrams! I'll add you in showcase page. (I'm working on it!) :)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;MIT&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>mingrammer</author><guid isPermaLink="false">https://github.com/mingrammer/diagrams</guid><pubDate>Wed, 12 Feb 2020 00:08:00 GMT</pubDate></item><item><title>pypa/virtualenv #9 in Python, Today</title><link>https://github.com/pypa/virtualenv</link><description>&lt;p&gt;&lt;i&gt;Virtual Python Environment builder&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/virtualenv" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a92d334e8e30a8d91c28f1fd06917e5ccb8ce65e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7669727475616c656e763f7374796c653d666c61742d737175617265" alt="PyPI" data-canonical-src="https://img.shields.io/pypi/v/virtualenv?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/virtualenv" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f2ebaaf4d9ae78622d6f201b6d4bfeab05b2c90d/68747470733a2f2f696d672e736869656c64732e696f2f707970692f696d706c656d656e746174696f6e2f7669727475616c656e763f7374796c653d666c61742d737175617265" alt="PyPI - Implementation" data-canonical-src="https://img.shields.io/pypi/implementation/virtualenv?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/virtualenv" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/09a2b7b8235d16d96f6213aee5277f353682d745/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7669727475616c656e763f7374796c653d666c61742d737175617265" alt="PyPI - Python Version" data-canonical-src="https://img.shields.io/pypi/pyversions/virtualenv?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="http://virtualenv.pypa.io" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3de6cc703a7177302dfec34b50af292f58b9a653/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7669727475616c656e762f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666c61742d737175617265" alt="Documentation" data-canonical-src="https://readthedocs.org/projects/virtualenv/badge/?version=latest&amp;amp;style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypistats.org/packages/virtualenv" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2b8ddce1d32315cddef91315b4ee83fd54ba59d8/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f7669727475616c656e763f7374796c653d666c61742d737175617265" alt="PyPI - Downloads" data-canonical-src="https://img.shields.io/pypi/dm/virtualenv?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/58dec064b7b2d5f701e49912ac978bfadc6a91b3/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f7669727475616c656e763f7374796c653d666c61742d737175617265" alt="PyPI - License" data-canonical-src="https://img.shields.io/pypi/l/virtualenv?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://dev.azure.com/pypa/virtualenv/_build/latest?definitionId=11&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0a0012007d71f8583601389650037f8c0e392922/68747470733a2f2f6465762e617a7572652e636f6d2f707970612f7669727475616c656e762f5f617069732f6275696c642f7374617475732f707970612e7669727475616c656e763f6272616e63684e616d653d6d6173746572" alt="Build Status" data-canonical-src="https://dev.azure.com/pypa/virtualenv/_apis/build/status/pypa.virtualenv?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://dev.azure.com/pypa/virtualenv/_build?definitionId=11&amp;amp;_a=summary" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/82287fc2f0f14f770021877b2f6d3f0f89ef55cf/68747470733a2f2f696d672e736869656c64732e696f2f617a7572652d6465766f70732f636f7665726167652f707970612f7669727475616c656e762f31313f7374796c653d666c61742d737175617265" alt="Azure DevOps coverage" data-canonical-src="https://img.shields.io/azure-devops/coverage/pypa/virtualenv/11?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://camo.githubusercontent.com/28a51fe3a2c05048d8ca8ecd039d6b1619037326/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-virtualenv" class="anchor" aria-hidden="true" href="#virtualenv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;virtualenv&lt;/h1&gt;
&lt;p&gt;A tool for creating isolated &lt;code&gt;virtual&lt;/code&gt; python environments.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://virtualenv.pypa.io/en/latest/installation.html" rel="nofollow"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://virtualenv.pypa.io" rel="nofollow"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://virtualenv.pypa.io/en/latest/changelog.html" rel="nofollow"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pypa/virtualenv/issues"&gt;Issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/virtualenv" rel="nofollow"&gt;PyPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pypa/virtualenv"&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-code-of-conduct" class="anchor" aria-hidden="true" href="#code-of-conduct"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code of Conduct&lt;/h2&gt;
&lt;p&gt;Everyone interacting in the virtualenv project's codebases, issue trackers, chat rooms, and mailing lists is expected to
follow the &lt;a href="https://www.pypa.io/en/latest/code-of-conduct/" rel="nofollow"&gt;PyPA Code of Conduct&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pypa</author><guid isPermaLink="false">https://github.com/pypa/virtualenv</guid><pubDate>Wed, 12 Feb 2020 00:09:00 GMT</pubDate></item><item><title>Rlacat/jd-automask #10 in Python, Today</title><link>https://github.com/Rlacat/jd-automask</link><description>&lt;p&gt;&lt;i&gt;防护-京东口罩自动抢购并下单&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;hr&gt;
&lt;p&gt;如果帮助到你，star一下，谢谢你
武汉加油，中国加油
个人时间有限，回答不过来了
目前v1出现cookie经常失效 被拒绝访问问题 正在修复&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;本代码使用方式 &lt;a href="https://blog.csdn.net/cyz52/article/details/104177981" rel="nofollow"&gt;https://blog.csdn.net/cyz52/article/details/104177981&lt;/a&gt; 或者下载后阅读README.md
100M电信网络实测1-2s刷新100个商品&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-版本" class="anchor" aria-hidden="true" href="#版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;版本&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; python3 V1&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 基于&lt;a href="https://github.com/cycz/jdBuyMask%5Dgithub"&gt;https://github.com/cycz/jdBuyMask]github&lt;/a&gt; V3版本制作&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 已编译版本链接：链接: &lt;a href="https://pan.baidu.com/s/1FqZP39ow_CrsAn0DfeRJ2w" rel="nofollow"&gt;https://pan.baidu.com/s/1FqZP39ow_CrsAn0DfeRJ2w&lt;/a&gt; 提取码: 97wb&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-使用方法" class="anchor" aria-hidden="true" href="#使用方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;使用方法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1.修改（config.ini）：地区id、推送方式（微信、邮箱二选一）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2.打开exe运行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;商品skuid修改方法（config.ini）：
1.谷歌（内核）浏览器要监控的商品url
2.按F12 ，点开Nework
3.点击需要的商品 和所在的地区
4.ctrl+f搜索 stock并点击
5.复制skuid
6.修改或者添加在config.ini内的skuids&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;地区id修改方法（config.ini）：
1.用谷歌（内核）浏览器随意打开一个京东的有货商品网页（例子：&lt;a href="https://item.jd.com/100004770235.html%EF%BC%89" rel="nofollow"&gt;https://item.jd.com/100004770235.html）&lt;/a&gt;
2.右键你的收货地址并点击审查元素
3.双击并复制那串数字（xx-xx-xxxxx）
4.修改在config.ini内的area(area = xx-xx-xxxxx)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-运行图片" class="anchor" aria-hidden="true" href="#运行图片"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;运行图片&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-功能" class="anchor" aria-hidden="true" href="#功能"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;功能&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 确认是否有货&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 有货自动下单&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 邮件、微信（需要申请方糖api）通知&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 扫码登陆&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 无限个商品支持&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 多线程极速刷新网页&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-更新记录" class="anchor" aria-hidden="true" href="#更新记录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新记录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;【2020.02.08】修复了一些bug&lt;/li&gt;
&lt;li&gt;【2020.02.08】大幅优化刷新速度，增加多线程技术（可在配置调节线程数）&lt;/li&gt;
&lt;li&gt;【2020.02.08】新增微信通知（&lt;a href="http://sc.ftqq.com/3.version" rel="nofollow"&gt;http://sc.ftqq.com/3.version&lt;/a&gt; 查看sc_key）&lt;/li&gt;
&lt;li&gt;【2020.02.08】jd-automask_V1版本上线&lt;/li&gt;
&lt;li&gt;【2020.02.07】增加扫码登陆，自动保存cookie&lt;/li&gt;
&lt;li&gt;【2020.02.07】V4版本，解决商品个数限制&lt;/li&gt;
&lt;li&gt;Code By Rlacat&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;【2020.02.06】V2版本，刷新更快更频繁，通过配置文件添加商品和地区id&lt;/li&gt;
&lt;li&gt;【2020.02.06】提交失败之后会继续不会暂停&lt;/li&gt;
&lt;li&gt;【2020.02.06】购物车有套装商品导致解析skuid错误&lt;/li&gt;
&lt;li&gt;【2020.02.05】商品有货，但是该商品已下柜，提交会报错，对部分代码进行了优化&lt;/li&gt;
&lt;li&gt;Code By cycz&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-反馈问题" class="anchor" aria-hidden="true" href="#反馈问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;反馈问题&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果有红包先花掉再开脚本，不然可能需要支付密码&lt;/li&gt;
&lt;li&gt;其他问题Github提issues，或者私信我！！&lt;/li&gt;
&lt;li&gt;如果闪退，请打开目录jdBuyMask.txt文件查看帮助说明&lt;/li&gt;
&lt;li&gt;CMD界面卡住、关闭CMD的快速编辑模式就行了&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Rlacat</author><guid isPermaLink="false">https://github.com/Rlacat/jd-automask</guid><pubDate>Wed, 12 Feb 2020 00:10:00 GMT</pubDate></item><item><title>kevinzakka/nca #11 in Python, Today</title><link>https://github.com/kevinzakka/nca</link><description>&lt;p&gt;&lt;i&gt;A PyTorch implementation of Neighbourhood Components Analysis.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-neighbourhood-components-analysis" class="anchor" aria-hidden="true" href="#neighbourhood-components-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Neighbourhood Components Analysis&lt;/h1&gt;
&lt;p&gt;A PyTorch implementation of &lt;a href="https://www.cs.toronto.edu/~hinton/absps/nca.pdf" rel="nofollow"&gt;Neighbourhood Components Analysis&lt;/a&gt; by &lt;em&gt;J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;NCA learns a linear transformation of the dataset such that the expected leave-one-out performance of kNN in the transformed space is maximized.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-api" class="anchor" aria-hidden="true" href="#api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;API&lt;/h2&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; instantiate nca object and initialize with&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; an identity matrix&lt;/span&gt;
nca &lt;span class="pl-k"&gt;=&lt;/span&gt; NCA(&lt;span class="pl-v"&gt;dim&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-v"&gt;init&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;identity&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; fit an nca model to a dataset&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; normalize the input data before&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; running the optimization&lt;/span&gt;
nca.train(X, y, &lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;normalize&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; apply the learned linear map to the data&lt;/span&gt;
X_nca &lt;span class="pl-k"&gt;=&lt;/span&gt; nca(X)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-dimensionality-reduction" class="anchor" aria-hidden="true" href="#dimensionality-reduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dimensionality Reduction&lt;/h2&gt;
&lt;p&gt;We generate a 3-D dataset where the first 2 dimensions are concentric rings and the third dimension is Gaussian noise. We plot the result of PCA, LDA and NCA with 2 components.&lt;/p&gt;
&lt;p align="center"&gt;
 &lt;a target="_blank" rel="noopener noreferrer" href="./assets/res.png"&gt;&lt;img src="./assets/res.png" width="100%" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Notice how PCA has failed to project out the noise, a result of a high noise variance in the third dimension. LDA also struggles to recover the concentric pattern since the classes themselves are not linearly separable.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-knn-on-mnist" class="anchor" aria-hidden="true" href="#knn-on-mnist"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;kNN on MNIST&lt;/h2&gt;
&lt;p&gt;We compute the classification error, computation time and storage cost of two algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kNN (k = 5) on the raw 784 dimensional MNIST dataset&lt;/li&gt;
&lt;li&gt;kNN (k = 5) on a learned 32 dimensional NCA projection of the MNIST dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;NCA + kNN&lt;/th&gt;
&lt;th&gt;Raw kNN&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Time&lt;/td&gt;
&lt;td&gt;2.37s&lt;/td&gt;
&lt;td&gt;155.25s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Storage&lt;/td&gt;
&lt;td&gt;6.40 Mb&lt;/td&gt;
&lt;td&gt;156.8 Mb&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Error&lt;/td&gt;
&lt;td&gt;3.3%&lt;/td&gt;
&lt;td&gt;2.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>kevinzakka</author><guid isPermaLink="false">https://github.com/kevinzakka/nca</guid><pubDate>Wed, 12 Feb 2020 00:11:00 GMT</pubDate></item><item><title>robotframework/robotframework #12 in Python, Today</title><link>https://github.com/robotframework/robotframework</link><description>&lt;p&gt;&lt;i&gt;Generic automation framework for acceptance testing and RPA&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-robot-framework" class="anchor" aria-hidden="true" href="#robot-framework"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Robot Framework&lt;/h1&gt;
&lt;div id="user-content-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction" id="user-content-id7"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation" id="user-content-id8"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#example" id="user-content-id9"&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#usage" id="user-content-id10"&gt;Usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#documentation" id="user-content-id11"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#support-and-contact" id="user-content-id12"&gt;Support and contact&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contributing" id="user-content-id13"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#license" id="user-content-id14"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;a name="user-content-introduction"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id7"&gt;Introduction&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://robotframework.org" rel="nofollow"&gt;Robot Framework&lt;/a&gt; is a generic open source
automation framework for acceptance testing, acceptance test driven
development (ATDD), and robotic process automation (RPA). It has simple plain
text syntax and it can be extended easily with libraries implemented using
Python or Java.&lt;/p&gt;
&lt;p&gt;Robot Framework is operating system and application independent. The core
framework is implemented using &lt;a href="http://python.org" rel="nofollow"&gt;Python&lt;/a&gt;, supports both
Python 2 and Python 3, and runs also on &lt;a href="http://jython.org" rel="nofollow"&gt;Jython&lt;/a&gt; (JVM),
&lt;a href="http://ironpython.net" rel="nofollow"&gt;IronPython&lt;/a&gt; (.NET) and &lt;a href="http://pypy.org" rel="nofollow"&gt;PyPy&lt;/a&gt;.
The framework has a rich ecosystem around it consisting of various generic
libraries and tools that are developed as separate projects. For more
information about Robot Framework and the ecosystem, see
&lt;a href="http://robotframework.org" rel="nofollow"&gt;http://robotframework.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Robot Framework project is hosted on &lt;a href="https://github.com/robotframework/robotframework"&gt;GitHub&lt;/a&gt; where you can find source code,
an issue tracker, and some further documentation. See &lt;a href="CONTRIBUTING.rst"&gt;CONTRIBUTING.rst&lt;/a&gt;
if you are interested to contribute. Downloads are hosted on &lt;a href="https://pypi.python.org/pypi/robotframework" rel="nofollow"&gt;PyPI&lt;/a&gt;, except
for the standalone JAR distribution that is on &lt;a href="http://search.maven.org/#search%7Cga%7C1%7Ca%3Arobotframework" rel="nofollow"&gt;Maven central&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Robot Framework development is sponsored by &lt;a href="http://robotframework.org/foundation" rel="nofollow"&gt;Robot Framework Foundation&lt;/a&gt;.&lt;/p&gt;
&lt;a href="https://pypi.python.org/pypi/robotframework" rel="nofollow"&gt;&lt;img alt="Latest version" src="https://camo.githubusercontent.com/ec5354e88ee7bbd5457713ca62ac507315db1ca5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f726f626f746672616d65776f726b2e7376673f6c6162656c3d76657273696f6e" data-canonical-src="https://img.shields.io/pypi/v/robotframework.svg?label=version" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="http://www.apache.org/licenses/LICENSE-2.0.html" rel="nofollow"&gt;&lt;img alt="License" src="https://camo.githubusercontent.com/4dc279c7e634593c6b0dc6928b38f3f0cedbb377/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f726f626f746672616d65776f726b2e737667" data-canonical-src="https://img.shields.io/pypi/l/robotframework.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a name="user-content-installation"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id8"&gt;Installation&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;If you already have &lt;a href="http://python.org" rel="nofollow"&gt;Python&lt;/a&gt; with &lt;a href="http://pip-installer.org" rel="nofollow"&gt;pip&lt;/a&gt; installed,
you can simply run:&lt;/p&gt;
&lt;pre&gt;pip install robotframework
&lt;/pre&gt;
&lt;p&gt;Alternatively you can get Robot Framework source code by downloading the source
distribution from &lt;a href="https://pypi.python.org/pypi/robotframework" rel="nofollow"&gt;PyPI&lt;/a&gt; and extracting it, or by cloning the project repository
from &lt;a href="https://github.com/robotframework/robotframework"&gt;GitHub&lt;/a&gt;. After that you can install the framework with:&lt;/p&gt;
&lt;pre&gt;python setup.py install
&lt;/pre&gt;
&lt;p&gt;For more detailed installation instructions, including installing Python,
Jython, IronPython and PyPy or installing from git, see &lt;a href="INSTALL.rst"&gt;INSTALL.rst&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-example"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-example" class="anchor" aria-hidden="true" href="#example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id9"&gt;Example&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Below is a simple example test case for testing login to some system.
You can find more examples with links to related demo projects from
&lt;a href="http://robotframework.org" rel="nofollow"&gt;http://robotframework.org&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-text-robot"&gt;&lt;pre&gt;&lt;span class="pl-s"&gt;*** Settings ***&lt;/span&gt;
&lt;span class="pl-c"&gt;Documentation     A test suite with a single test for valid login.&lt;/span&gt;
&lt;span class="pl-c"&gt;...&lt;/span&gt;
&lt;span class="pl-c"&gt;...               This test has a workflow that is created using keywords in&lt;/span&gt;
&lt;span class="pl-c"&gt;...               the imported resource file.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;/span&gt;&lt;span class="pl-k"&gt;Resource&lt;/span&gt;          resource.robot

&lt;span class="pl-s"&gt;*** Test Cases ***&lt;/span&gt;
&lt;span class="pl-k"&gt;Valid Login&lt;/span&gt;
    Open Browser To Login Page
    Input Username    demo
    Input Password    mode
    Submit Credentials
    Welcome Page Should Be Open
    &lt;span class="pl-k"&gt;[Teardown]&lt;/span&gt;    Close Browser&lt;/pre&gt;&lt;/div&gt;
&lt;a name="user-content-usage"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id10"&gt;Usage&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Tests (or tasks) are executed from the command line using the &lt;code&gt;robot&lt;/code&gt;
command or by executing the &lt;code&gt;robot&lt;/code&gt; module directly like &lt;code&gt;python -m robot&lt;/code&gt;
or &lt;code&gt;jython -m robot&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The basic usage is giving a path to a test (or task) file or directory as an
argument with possible command line options before the path:&lt;/p&gt;
&lt;pre&gt;robot tests.robot
robot --variable BROWSER:Firefox --outputdir results path/to/tests/
&lt;/pre&gt;
&lt;p&gt;Additionally there is the &lt;code&gt;rebot&lt;/code&gt; tool for combining results and otherwise
post-processing outputs:&lt;/p&gt;
&lt;pre&gt;rebot --name Example output1.xml output2.xml
&lt;/pre&gt;
&lt;p&gt;Run &lt;code&gt;robot --help&lt;/code&gt; and &lt;code&gt;rebot --help&lt;/code&gt; for more information about the command
line usage. For a complete reference manual see &lt;a href="http://robotframework.org/robotframework/#user-guide" rel="nofollow"&gt;Robot Framework User Guide&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-documentation"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id11"&gt;Documentation&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://robotframework.org/robotframework/#user-guide" rel="nofollow"&gt;Robot Framework User Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://robotframework.org/robotframework/#standard-libraries" rel="nofollow"&gt;Standard libraries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://robotframework.org/robotframework/#built-in-tools" rel="nofollow"&gt;Built-in tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://robot-framework.readthedocs.org" rel="nofollow"&gt;API documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://robotframework.org/#documentation" rel="nofollow"&gt;General documentation and demos&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-support-and-contact"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-support-and-contact" class="anchor" aria-hidden="true" href="#support-and-contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id12"&gt;Support and contact&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://groups.google.com/group/robotframework-users/" rel="nofollow"&gt;robotframework-users&lt;/a&gt; mailing list&lt;/li&gt;
&lt;li&gt;&lt;a href="https://robotframework-slack-invite.herokuapp.com" rel="nofollow"&gt;Slack&lt;/a&gt; community&lt;/li&gt;
&lt;li&gt;&lt;a href="http://webchat.freenode.net/?channels=robotframework&amp;amp;prompt=1" rel="nofollow"&gt;#robotframework&lt;/a&gt;
IRC channel on freenode&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/robotframework" rel="nofollow"&gt;@robotframework&lt;/a&gt; on Twitter&lt;/li&gt;
&lt;li&gt;&lt;a href="http://robotframework.org/#support" rel="nofollow"&gt;Other forums&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id13"&gt;Contributing&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Interested to contribute to Robot Framework? Great! In that case it is a good
start by looking at the &lt;a href="CONTRIBUTING.rst"&gt;Contribution guidelines&lt;/a&gt;. If you
do not already have an issue you would like to work on, you can check
issues with &lt;a href="https://github.com/robotframework/robotframework/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;good new issue&lt;/a&gt; and &lt;a href="https://github.com/robotframework/robotframework/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22"&gt;help wanted&lt;/a&gt; labels.&lt;/p&gt;
&lt;p&gt;Remember also that there are many other tools and libraries in the wider
&lt;a href="http://robotframework.org" rel="nofollow"&gt;Robot Framework ecosystem&lt;/a&gt; that you can
contribute to!&lt;/p&gt;
&lt;a name="user-content-license"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id14"&gt;License&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Robot Framework is open source software provided under the &lt;a href="http://apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;Apache License
2.0&lt;/a&gt;. Robot Framework documentation and other similar content use the
&lt;a href="http://creativecommons.org/licenses/by/3.0" rel="nofollow"&gt;Creative Commons Attribution 3.0 Unported&lt;/a&gt; license. Most libraries and tools
in the ecosystem are also open source, but they may use different licenses.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>robotframework</author><guid isPermaLink="false">https://github.com/robotframework/robotframework</guid><pubDate>Wed, 12 Feb 2020 00:12:00 GMT</pubDate></item><item><title>python/mypy #13 in Python, Today</title><link>https://github.com/python/mypy</link><description>&lt;p&gt;&lt;i&gt;Optional static typing for Python 3 and 2 (PEP 484)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/68c7827eeb796f3a664f48f5657c04e65e04ae6e/687474703a2f2f6d7970792d6c616e672e6f72672f7374617469632f6d7970795f6c696768742e737667"&gt;&lt;img src="https://camo.githubusercontent.com/68c7827eeb796f3a664f48f5657c04e65e04ae6e/687474703a2f2f6d7970792d6c616e672e6f72672f7374617469632f6d7970795f6c696768742e737667" alt="mypy logo" width="300px" data-canonical-src="http://mypy-lang.org/static/mypy_light.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-mypy-optional-static-typing-for-python" class="anchor" aria-hidden="true" href="#mypy-optional-static-typing-for-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mypy: Optional Static Typing for Python&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/python/mypy" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/005ce1dbd4467a2f49aded6b48e870e141cf39c3/68747470733a2f2f6170692e7472617669732d63692e6f72672f707974686f6e2f6d7970792e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://api.travis-ci.org/python/mypy.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/python/typing?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cf5b66e6f121337a2c67e10e1f035e9d125d9753/68747470733a2f2f6261646765732e6769747465722e696d2f707974686f6e2f747970696e672e737667" alt="Chat at https://gitter.im/python/typing" data-canonical-src="https://badges.gitter.im/python/typing.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="http://mypy-lang.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/34b3a249cd6502d0a521ab2f42c8830b7cfd03fa/687474703a2f2f7777772e6d7970792d6c616e672e6f72672f7374617469632f6d7970795f62616467652e737667" alt="Checked with mypy" data-canonical-src="http://www.mypy-lang.org/static/mypy_badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-got-a-question-join-us-on-gitter" class="anchor" aria-hidden="true" href="#got-a-question-join-us-on-gitter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Got a question? Join us on Gitter!&lt;/h2&gt;
&lt;p&gt;We don't have a mailing list; but we are always happy to answer
questions on &lt;a href="https://gitter.im/python/typing" rel="nofollow"&gt;gitter chat&lt;/a&gt;.  If you are
sure you've found a bug please search our issue trackers for a
duplicate before filing a new issue:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/python/mypy/issues"&gt;mypy tracker&lt;/a&gt;
for mypy issues&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/python/typeshed/issues"&gt;typeshed tracker&lt;/a&gt;
for issues with specific modules&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/python/typing/issues"&gt;typing tracker&lt;/a&gt;
for discussion of new type system features (PEP 484 changes) and
runtime bugs in the typing module&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-mypy" class="anchor" aria-hidden="true" href="#what-is-mypy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is mypy?&lt;/h2&gt;
&lt;p&gt;Mypy is an optional static type checker for Python.  You can add type
hints (&lt;a href="https://www.python.org/dev/peps/pep-0484/" rel="nofollow"&gt;PEP 484&lt;/a&gt;) to your
Python programs, and use mypy to type check them statically.
Find bugs in your programs without even running them!&lt;/p&gt;
&lt;p&gt;You can mix dynamic and static typing in your programs. You can always
fall back to dynamic typing when static typing is not convenient, such
as for legacy code.&lt;/p&gt;
&lt;p&gt;Here is a small example to whet your appetite (Python 3):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; typing &lt;span class="pl-k"&gt;import&lt;/span&gt; Iterator

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;fib&lt;/span&gt;(&lt;span class="pl-smi"&gt;n&lt;/span&gt;: &lt;span class="pl-c1"&gt;int&lt;/span&gt;) -&amp;gt; Iterator[&lt;span class="pl-c1"&gt;int&lt;/span&gt;]:
    a, b &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;
    &lt;span class="pl-k"&gt;while&lt;/span&gt; a &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; n:
        &lt;span class="pl-k"&gt;yield&lt;/span&gt; a
        a, b &lt;span class="pl-k"&gt;=&lt;/span&gt; b, a &lt;span class="pl-k"&gt;+&lt;/span&gt; b&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See &lt;a href="http://mypy.readthedocs.io/en/stable/introduction.html" rel="nofollow"&gt;the documentation&lt;/a&gt; for more examples.&lt;/p&gt;
&lt;p&gt;For Python 2.7, the standard annotations are written as comments:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;is_palindrome&lt;/span&gt;(&lt;span class="pl-smi"&gt;s&lt;/span&gt;):
    &lt;span class="pl-c"&gt;# &lt;span class="pl-c"&gt;type:&lt;/span&gt; &lt;span class="pl-c"&gt;(&lt;/span&gt;&lt;span class="pl-c"&gt;str&lt;/span&gt;&lt;span class="pl-c"&gt;)&lt;/span&gt; &lt;span class="pl-c"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="pl-c"&gt;bool&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;return&lt;/span&gt; s &lt;span class="pl-k"&gt;==&lt;/span&gt; s[::&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See &lt;a href="http://mypy.readthedocs.io/en/latest/python2.html" rel="nofollow"&gt;the documentation for Python 2 support&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Mypy is in development; some features are missing and there are bugs.
See 'Development status' below.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h2&gt;
&lt;p&gt;You need Python 3.5 or later to run mypy.  You can have multiple Python
versions (2.x and 3.x) installed on the same system without problems.&lt;/p&gt;
&lt;p&gt;In Ubuntu, Mint and Debian you can install Python 3 like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo apt-get install python3 python3-pip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For other Linux flavors, macOS and Windows, packages are available at&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.python.org/getit/" rel="nofollow"&gt;http://www.python.org/getit/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick start&lt;/h2&gt;
&lt;p&gt;Mypy can be installed using pip:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install -U mypy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to run the latest version of the code, you can install from git:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install -U git+git://github.com/python/mypy.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, if Python on your system is configured properly (else see
"Troubleshooting" below), you can type-check the &lt;a href="https://mypy.readthedocs.io/en/latest/getting_started.html#function-signatures-and-dynamic-vs-static-typing" rel="nofollow"&gt;statically typed parts&lt;/a&gt; of a
program like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ mypy PROGRAM
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can always use a Python interpreter to run your statically typed
programs, even if they have type errors:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 PROGRAM
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also try mypy in an &lt;a href="https://mypy-play.net/" rel="nofollow"&gt;online playground&lt;/a&gt; (developed by
Yusuke Miyazaki).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ide-linter-integrations-and-pre-commit" class="anchor" aria-hidden="true" href="#ide-linter-integrations-and-pre-commit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;IDE, Linter Integrations, and Pre-commit&lt;/h2&gt;
&lt;p&gt;Mypy can be integrated into popular IDEs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vim:
&lt;ul&gt;
&lt;li&gt;Using &lt;a href="https://github.com/vim-syntastic/syntastic"&gt;Syntastic&lt;/a&gt;: in &lt;code&gt;~/.vimrc&lt;/code&gt; add
&lt;code&gt;let g:syntastic_python_checkers=['mypy']&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Using &lt;a href="https://github.com/dense-analysis/ale"&gt;ALE&lt;/a&gt;: should be enabled by default when &lt;code&gt;mypy&lt;/code&gt; is installed,
or can be explicitly enabled by adding &lt;code&gt;let b:ale_linters = ['mypy']&lt;/code&gt; in &lt;code&gt;~/vim/ftplugin/python.vim&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Emacs: using &lt;a href="https://github.com/flycheck/"&gt;Flycheck&lt;/a&gt; and &lt;a href="https://github.com/lbolla/emacs-flycheck-mypy"&gt;Flycheck-mypy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sublime Text: &lt;a href="https://github.com/fredcallaway/SublimeLinter-contrib-mypy"&gt;SublimeLinter-contrib-mypy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Atom: &lt;a href="https://atom.io/packages/linter-mypy" rel="nofollow"&gt;linter-mypy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PyCharm: &lt;a href="https://github.com/dropbox/mypy-PyCharm-plugin"&gt;mypy plugin&lt;/a&gt; (PyCharm integrates
&lt;a href="https://www.jetbrains.com/help/pycharm/type-hinting-in-product.html" rel="nofollow"&gt;its own implementation of PEP 484&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;VS Code: provides &lt;a href="https://code.visualstudio.com/docs/python/linting#_mypy" rel="nofollow"&gt;basic integration&lt;/a&gt; with mypy.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mypy can also be integrated into &lt;a href="http://flake8.pycqa.org/" rel="nofollow"&gt;Flake8&lt;/a&gt; using &lt;a href="https://github.com/ambv/flake8-mypy"&gt;flake8-mypy&lt;/a&gt;, or
can be set up as a pre-commit hook using &lt;a href="https://github.com/pre-commit/mirrors-mypy"&gt;pre-commit mirrors-mypy&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-web-site-and-documentation" class="anchor" aria-hidden="true" href="#web-site-and-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web site and documentation&lt;/h2&gt;
&lt;p&gt;Documentation and additional information is available at the web site:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.mypy-lang.org/" rel="nofollow"&gt;http://www.mypy-lang.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Or you can jump straight to the documentation:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://mypy.readthedocs.io/" rel="nofollow"&gt;http://mypy.readthedocs.io/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-troubleshooting" class="anchor" aria-hidden="true" href="#troubleshooting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Troubleshooting&lt;/h2&gt;
&lt;p&gt;Depending on your configuration, you may have to run &lt;code&gt;pip&lt;/code&gt; like
this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install -U mypy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should automatically install the appropriate version of
mypy's parser, typed-ast.  If for some reason it does not, you
can install it manually:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install -U typed-ast
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the &lt;code&gt;mypy&lt;/code&gt; command isn't found after installation: After
&lt;code&gt;python3 -m pip install&lt;/code&gt;, the &lt;code&gt;mypy&lt;/code&gt; script and
dependencies, including the &lt;code&gt;typing&lt;/code&gt; module, will be installed to
system-dependent locations.  Sometimes the script directory will not
be in &lt;code&gt;PATH&lt;/code&gt;, and you have to add the target directory to &lt;code&gt;PATH&lt;/code&gt;
manually or create a symbolic link to the script.  In particular, on
macOS, the script may be installed under &lt;code&gt;/Library/Frameworks&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/Library/Frameworks/Python.framework/Versions/&amp;lt;version&amp;gt;/bin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In Windows, the script is generally installed in
&lt;code&gt;\PythonNN\Scripts&lt;/code&gt;. So, type check a program like this (replace
&lt;code&gt;\Python34&lt;/code&gt; with your Python installation path):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;C:\&amp;gt;\Python34\python \Python34\Scripts\mypy PROGRAM
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-working-with-virtualenv" class="anchor" aria-hidden="true" href="#working-with-virtualenv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Working with &lt;code&gt;virtualenv&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;If you are using &lt;a href="https://virtualenv.pypa.io/en/stable/" rel="nofollow"&gt;&lt;code&gt;virtualenv&lt;/code&gt;&lt;/a&gt;,
make sure you are running a python3 environment. Installing via &lt;code&gt;pip3&lt;/code&gt;
in a v2 environment will not configure the environment to run installed
modules from the command line.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install -U virtualenv
$ python3 -m virtualenv env
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start-for-contributing-to-mypy" class="anchor" aria-hidden="true" href="#quick-start-for-contributing-to-mypy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick start for contributing to mypy&lt;/h2&gt;
&lt;p&gt;If you want to contribute, first clone the mypy git repository:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone --recurse-submodules https://github.com/python/mypy.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you've already cloned the repo without &lt;code&gt;--recurse-submodules&lt;/code&gt;,
you need to pull in the typeshed repo as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git submodule init
$ git submodule update
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Either way you should now have a subdirectory &lt;code&gt;typeshed&lt;/code&gt; inside your mypy repo,
your folders tree should be like &lt;code&gt;mypy/mypy/typeshed&lt;/code&gt;, containing a
clone of the typeshed repo (&lt;code&gt;https://github.com/python/typeshed&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;From the mypy directory, use pip to install mypy:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd mypy
$ python3 -m pip install -U .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Replace &lt;code&gt;python3&lt;/code&gt; with your Python 3 interpreter.  You may have to do
the above as root. For example, in Ubuntu:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo python3 -m pip install -U .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can use the &lt;code&gt;mypy&lt;/code&gt; program just as above.  In case of trouble
see "Troubleshooting" above.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-working-with-the-git-version-of-mypy" class="anchor" aria-hidden="true" href="#working-with-the-git-version-of-mypy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Working with the git version of mypy&lt;/h2&gt;
&lt;p&gt;mypy contains a submodule, "typeshed". See &lt;a href="http://github.com/python/typeshed"&gt;http://github.com/python/typeshed&lt;/a&gt;.
This submodule contains types for the Python standard library.&lt;/p&gt;
&lt;p&gt;Due to the way git submodules work, you'll have to do&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  git submodule update mypy/typeshed
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;whenever you change branches, merge, rebase, or pull.&lt;/p&gt;
&lt;p&gt;(It's possible to automate this: Search Google for "git hook update submodule")&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h2&gt;
&lt;p&gt;The basic way to run tests:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pip3 install -r test-requirements.txt
$ python2 -m pip install -U typing
$ ./runtests.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more on the tests, such as how to write tests and how to control
which tests to run, see &lt;a href="test-data/unit/README.md"&gt;Test README.md&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-development-status" class="anchor" aria-hidden="true" href="#development-status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development status&lt;/h2&gt;
&lt;p&gt;Mypy is beta software, but it has already been used in production
for several years at Dropbox, and it has an extensive test suite.&lt;/p&gt;
&lt;p&gt;See &lt;a href="ROADMAP.md"&gt;the roadmap&lt;/a&gt; if you are interested in plans for the
future.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h2&gt;
&lt;p&gt;Follow mypy's updates on the blog: &lt;a href="http://mypy-lang.blogspot.com/" rel="nofollow"&gt;http://mypy-lang.blogspot.com/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-issue-tracker" class="anchor" aria-hidden="true" href="#issue-tracker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Issue tracker&lt;/h2&gt;
&lt;p&gt;Please report any bugs and enhancement ideas using the mypy issue
tracker: &lt;a href="https://github.com/python/mypy/issues"&gt;https://github.com/python/mypy/issues&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you have any questions about using mypy or types, please ask
in the typing gitter instead: &lt;a href="https://gitter.im/python/typing" rel="nofollow"&gt;https://gitter.im/python/typing&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-compiled-version-of-mypy" class="anchor" aria-hidden="true" href="#compiled-version-of-mypy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compiled version of mypy&lt;/h2&gt;
&lt;p&gt;We have built a compiled version of mypy using the &lt;a href="https://github.com/python/mypy/tree/master/mypyc"&gt;mypyc
compiler&lt;/a&gt; for
mypy-annotated Python code. It is approximately 4 times faster than
interpreted mypy and is available (and the default) for 64-bit
Windows, macOS, and Linux.&lt;/p&gt;
&lt;p&gt;To install an interpreted mypy instead, use:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --no-binary mypy -U mypy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you wish to test out the compiled version of a development
version of mypy, you can directly install a binary from
&lt;a href="https://github.com/mypyc/mypy_mypyc-wheels/releases/latest"&gt;https://github.com/mypyc/mypy_mypyc-wheels/releases/latest&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-help-wanted" class="anchor" aria-hidden="true" href="#help-wanted"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Help wanted&lt;/h2&gt;
&lt;p&gt;Any help in testing, development, documentation and other tasks is
highly appreciated and useful to the project. There are tasks for
contributors of all experience levels. If you're just getting started,
ask on the &lt;a href="https://gitter.im/python/typing" rel="nofollow"&gt;gitter chat&lt;/a&gt; for ideas of good
beginner issues.&lt;/p&gt;
&lt;p&gt;For more details, see the file &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Mypy is licensed under the terms of the MIT License (see the file
LICENSE).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>python</author><guid isPermaLink="false">https://github.com/python/mypy</guid><pubDate>Wed, 12 Feb 2020 00:13:00 GMT</pubDate></item><item><title>openai/gym #14 in Python, Today</title><link>https://github.com/openai/gym</link><description>&lt;p&gt;&lt;i&gt;A toolkit for developing and comparing reinforcement learning algorithms.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Maintenance (expect bug fixes and minor updates)&lt;/p&gt;
&lt;a name="user-content-openai-gym"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-openai-gym" class="anchor" aria-hidden="true" href="#openai-gym"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id3"&gt;OpenAI Gym&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.&lt;/strong&gt; This is the &lt;code&gt;gym&lt;/code&gt; open-source library, which gives you access to a standardized set of environments.&lt;/p&gt;
&lt;a href="https://travis-ci.org/openai/gym" rel="nofollow"&gt;&lt;img alt="https://travis-ci.org/openai/gym.svg?branch=master" src="https://camo.githubusercontent.com/56bfefda9bd0784021be54fa77fc4245391d3431/68747470733a2f2f7472617669732d63692e6f72672f6f70656e61692f67796d2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.org/openai/gym.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href="#what-s-new"&gt;See What's New section below&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;gym&lt;/code&gt; makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano. You can use it from Python code, and soon from other languages.&lt;/p&gt;
&lt;p&gt;If you're not sure where to start, we recommend beginning with the
&lt;a href="https://gym.openai.com/docs" rel="nofollow"&gt;docs&lt;/a&gt; on our site. See also the &lt;a href="https://github.com/openai/gym/wiki/FAQ"&gt;FAQ&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A whitepaper for OpenAI Gym is available at &lt;a href="http://arxiv.org/abs/1606.01540" rel="nofollow"&gt;http://arxiv.org/abs/1606.01540&lt;/a&gt;, and here's a BibTeX entry that you can use to cite it in a publication:&lt;/p&gt;
&lt;pre&gt;@misc{1606.01540,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}
&lt;/pre&gt;
&lt;div id="user-content-contents-of-this-document"&gt;
&lt;p&gt;&lt;strong&gt;Contents of this document&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#openai-gym" id="user-content-id3"&gt;OpenAI Gym&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#basics" id="user-content-id4"&gt;Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation" id="user-content-id5"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#environments" id="user-content-id6"&gt;Environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples" id="user-content-id7"&gt;Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#testing" id="user-content-id8"&gt;Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-s-new" id="user-content-id9"&gt;What's new&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;a name="user-content-basics"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-basics" class="anchor" aria-hidden="true" href="#basics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id4"&gt;Basics&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;There are two basic concepts in reinforcement learning: the
environment (namely, the outside world) and the agent (namely, the
algorithm you are writing). The agent sends actions to the
environment, and the environment replies with observations and
rewards (that is, a score).&lt;/p&gt;
&lt;p&gt;The core gym interface is &lt;a href="https://github.com/openai/gym/blob/master/gym/core.py"&gt;Env&lt;/a&gt;, which is
the unified environment interface. There is no interface for agents;
that part is left to you. The following are the &lt;code&gt;Env&lt;/code&gt; methods you
should know:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reset(self): Reset the environment's state. Returns observation.&lt;/li&gt;
&lt;li&gt;step(self, action): Step the environment by one timestep. Returns observation, reward, done, info.&lt;/li&gt;
&lt;li&gt;render(self, mode='human'): Render one frame of the environment. The default mode will do something human friendly, such as pop up a window.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-supported-systems"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-supported-systems" class="anchor" aria-hidden="true" href="#supported-systems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supported systems&lt;/h4&gt;
&lt;p&gt;We currently support Linux and OS X running Python 2.7 or 3.5 -- 3.7.
Windows support is experimental - algorithmic, toy_text, classic_control and atari &lt;em&gt;should&lt;/em&gt; work on Windows (see next section for installation instructions); nevertheless, proceed at your own risk.&lt;/p&gt;
&lt;a name="user-content-installation"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id5"&gt;Installation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;You can perform a minimal install of &lt;code&gt;gym&lt;/code&gt; with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/openai/gym.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; gym
pip install -e &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you prefer, you can do a minimal install of the packaged version directly from PyPI:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install gym&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You'll be able to run a few environments right away:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;algorithmic&lt;/li&gt;
&lt;li&gt;toy_text&lt;/li&gt;
&lt;li&gt;classic_control (you'll need &lt;code&gt;pyglet&lt;/code&gt; to render though)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We recommend playing with those environments at first, and then later
installing the dependencies for the remaining environments.&lt;/p&gt;
&lt;p&gt;You can also &lt;a href="https://gitpod.io/#https://github.com/openai/gym/blob/master/examples/agents/cem.py" rel="nofollow"&gt;run gym on gitpod.io&lt;/a&gt; to play with the examples online.
In the preview window you can click on the mp4 file you want to view. If you want to view another mp4 file just press the back button and click on another mp4 file.&lt;/p&gt;
&lt;a name="user-content-installing-everything"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-installing-everything" class="anchor" aria-hidden="true" href="#installing-everything"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing everything&lt;/h4&gt;
&lt;p&gt;To install the full set of environments, you'll need to have some system
packages installed. We'll build out the list here over time; please let us know
what you end up installing on your platform. Also, take a look at the docker files (py.Dockerfile) to
see the composition of our CI-tested images.&lt;/p&gt;
&lt;p&gt;On Ubuntu 16.04 and 18.04:&lt;/p&gt;
&lt;p&gt;MuJoCo has a proprietary dependency we can't set up for you. Follow
the
&lt;a href="https://github.com/openai/mujoco-py#obtaining-the-binaries-and-license-key"&gt;instructions&lt;/a&gt;
in the &lt;code&gt;mujoco-py&lt;/code&gt; package for help.  As an alternative to &lt;code&gt;mujoco-py&lt;/code&gt;, consider &lt;a href="https://github.com/openai/gym/blob/master/docs/environments.md#pybullet-robotics-environments"&gt;PyBullet&lt;/a&gt; which uses the open source Bullet physics engine and has no license requirement.&lt;/p&gt;
&lt;p&gt;Once you're ready to install everything, run &lt;code&gt;pip install -e '.[all]'&lt;/code&gt; (or &lt;code&gt;pip install 'gym[all]'&lt;/code&gt;).&lt;/p&gt;
&lt;a name="user-content-pip-version"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-pip-version" class="anchor" aria-hidden="true" href="#pip-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pip version&lt;/h4&gt;
&lt;p&gt;To run &lt;code&gt;pip install -e '.[all]'&lt;/code&gt;, you'll need a semi-recent pip.
Please make sure your pip is at least at version &lt;code&gt;1.5.0&lt;/code&gt;. You can
upgrade using the following: &lt;code&gt;pip install --ignore-installed
pip&lt;/code&gt;. Alternatively, you can open &lt;a href="https://github.com/openai/gym/blob/master/setup.py"&gt;setup.py&lt;/a&gt; and
install the dependencies by hand.&lt;/p&gt;
&lt;a name="user-content-rendering-on-a-server"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-rendering-on-a-server" class="anchor" aria-hidden="true" href="#rendering-on-a-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rendering on a server&lt;/h4&gt;
&lt;p&gt;If you're trying to render video on a server, you'll need to connect a
fake display. The easiest way to do this is by running under
&lt;code&gt;xvfb-run&lt;/code&gt; (on Ubuntu, install the &lt;code&gt;xvfb&lt;/code&gt; package):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;xvfb-run -s &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;-screen 0 1400x900x24&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; bash&lt;/pre&gt;&lt;/div&gt;
&lt;a name="user-content-installing-dependencies-for-specific-environments"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-installing-dependencies-for-specific-environments" class="anchor" aria-hidden="true" href="#installing-dependencies-for-specific-environments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing dependencies for specific environments&lt;/h4&gt;
&lt;p&gt;If you'd like to install the dependencies for only specific
environments, see &lt;a href="https://github.com/openai/gym/blob/master/setup.py"&gt;setup.py&lt;/a&gt;. We
maintain the lists of dependencies on a per-environment group basis.&lt;/p&gt;
&lt;a name="user-content-environments"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-environments" class="anchor" aria-hidden="true" href="#environments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id6"&gt;Environments&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/environments.md"&gt;List of Environments&lt;/a&gt; and the &lt;a href="http://gym.openai.com/envs/" rel="nofollow"&gt;gym site&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For information on creating your own environments, see &lt;a href="docs/creating-environments.md"&gt;Creating your own Environments&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-examples"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id7"&gt;Examples&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;See the &lt;code&gt;examples&lt;/code&gt; directory.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run &lt;a href="https://github.com/openai/gym/blob/master/examples/agents/random_agent.py"&gt;examples/agents/random_agent.py&lt;/a&gt; to run a simple random agent.&lt;/li&gt;
&lt;li&gt;Run &lt;a href="https://github.com/openai/gym/blob/master/examples/agents/cem.py"&gt;examples/agents/cem.py&lt;/a&gt; to run an actual learning agent (using the cross-entropy method).&lt;/li&gt;
&lt;li&gt;Run &lt;a href="https://github.com/openai/gym/blob/master/examples/scripts/list_envs"&gt;examples/scripts/list_envs&lt;/a&gt; to generate a list of all environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-testing"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id8"&gt;Testing&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We are using &lt;a href="http://doc.pytest.org" rel="nofollow"&gt;pytest&lt;/a&gt; for tests. You can run them via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pytest&lt;/pre&gt;&lt;/div&gt;
&lt;a name="user-content-what-s-new"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-whats-new" class="anchor" aria-hidden="true" href="#whats-new"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#id9"&gt;What's new&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2020-02-09 (v 0.16.0)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;EnvSpec API change - remove tags field (retro-active version bump, the changes are actually already in the codebase since 0.15.5 - thanks @wookayin for keeping us in check!)&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2020-02-03 (v0.15.6)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;pyglet 1.4 compatibility (this time for real :))&lt;/li&gt;
&lt;li&gt;Fixed the bug in BipedalWalker and BipedalWalkerHardcore, bumped version to 3 (thanks @chozabu!)&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2020-01-24 (v0.15.5)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;pyglet 1.4 compatibility&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;remove python-opencv from the requirements&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2019-11-08 (v0.15.4)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;Added multiple env wrappers (thanks @zuoxingdong and @hartikainen!)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Removed mujoco &amp;gt;= 2.0 support due to lack of tests&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2019-10-09 (v0.15.3)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;VectorEnv modifications - unified the VectorEnv api (added reset_async, reset_wait, step_async, step_wait methods to SyncVectorEnv); more flexibility in AsyncVectorEnv workers&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2019-08-23 (v0.15.2)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;More Wrappers - AtariPreprocessing, FrameStack, GrayScaleObservation, FilterObservation,  FlattenDictObservationsWrapper, PixelObservationWrapper, TransformReward (thanks @zuoxingdong, @hartikainen)&lt;/li&gt;
&lt;li&gt;Remove rgb_rendering_tracking logic from mujoco environments (default behavior stays the same for the -v3 environments, rgb rendering returns a view from tracking camera)&lt;/li&gt;
&lt;li&gt;Velocity goal constraint for MountainCar (thanks @abhinavsagar)&lt;/li&gt;
&lt;li&gt;Taxi-v2 -&amp;gt; Taxi-v3 (add missing wall in the map to replicate env as describe in the original paper, thanks @kobotics)&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2019-07-26 (v0.14.0)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;Wrapper cleanup&lt;/li&gt;
&lt;li&gt;Spec-related bug fixes&lt;/li&gt;
&lt;li&gt;VectorEnv fixes&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2019-06-21 (v0.13.1)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;Bug fix for ALE 0.6 difficulty modes&lt;/li&gt;
&lt;li&gt;Use narrow range for pyglet versions&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2019-06-21 (v0.13.0)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;Upgrade to ALE 0.6 (atari-py 0.2.0) (thanks @JesseFarebro!)&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2019-06-21 (v0.12.6)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;Added vectorized environments (thanks @tristandeleu!). Vectorized environment runs multiple copies of an environment in parallel. To create a vectorized version of an environment, use gym.vector.make(env_id, num_envs, **kwargs), for instance, gym.vector.make('Pong-v4',16).&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2019-05-28 (v0.12.5)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;fixed Fetch-slide environment to be solvable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2019-05-24 (v0.12.4)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;remove pyopengl dependency and use more narrow atari-py and box2d-py versions&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2019-03-25 (v0.12.1)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;rgb rendering in MuJoCo locomotion -v3 environments now comes from tracking camera (so that agent does not run away from the field of view). The old behaviour can be restored by passing rgb_rendering_tracking=False kwarg. Also, a potentially breaking change!!! Wrapper class now forwards methods and attributes to wrapped env.&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2019-02-26 (v0.12.0)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;release mujoco environments v3 with support for gym.make kwargs such as xml_file, ctrl_cost_weight, reset_noise_scale etc&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;2019-02-06 (v0.11.0)&lt;/dt&gt;
&lt;dd&gt;&lt;ul&gt;
&lt;li&gt;remove gym.spaces.np_random common PRNG; use per-instance PRNG instead.&lt;/li&gt;
&lt;li&gt;support for kwargs in gym.make&lt;/li&gt;
&lt;li&gt;lots of bugfixes&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2018-02-28: Release of a set of new robotics environments.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2018-01-25: Made some aesthetic improvements and removed unmaintained parts of gym. This may seem like a downgrade in functionality, but it is actually a long-needed cleanup in preparation for some great new things that will be released in the next month.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Now your Env and Wrapper subclasses should define step, reset, render, close, seed rather than underscored method names.&lt;/li&gt;
&lt;li&gt;Removed the board_game, debugging, safety, parameter_tuning environments since they're not being maintained by us at OpenAI. We encourage authors and users to create new repositories for these environments.&lt;/li&gt;
&lt;li&gt;Changed MultiDiscrete action space to range from [0, ..., n-1] rather than [a, ..., b-1].&lt;/li&gt;
&lt;li&gt;No more render(close=True), use env-specific methods to close the rendering.&lt;/li&gt;
&lt;li&gt;Removed scoreboard directory, since site doesn't exist anymore.&lt;/li&gt;
&lt;li&gt;Moved gym/monitoring to gym/wrappers/monitoring&lt;/li&gt;
&lt;li&gt;Add dtype to Space.&lt;/li&gt;
&lt;li&gt;Not using python's built-in module anymore, using gym.logger&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2018-01-24: All continuous control environments now use mujoco_py &amp;gt;= 1.50.
Versions have been updated accordingly to -v2, e.g. HalfCheetah-v2. Performance
should be similar (see &lt;a href="https://github.com/openai/gym/pull/834"&gt;https://github.com/openai/gym/pull/834&lt;/a&gt;) but there are likely
some differences due to changes in MuJoCo.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2017-06-16: Make env.spec into a property to fix a bug that occurs
when you try to print out an unregistered Env.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2017-05-13: BACKWARDS INCOMPATIBILITY: The Atari environments are now at
&lt;em&gt;v4&lt;/em&gt;. To keep using the old v3 environments, keep gym &amp;lt;= 0.8.2 and atari-py
&amp;lt;= 0.0.21. Note that the v4 environments will not give identical results to
existing v3 results, although differences are minor. The v4 environments
incorporate the latest Arcade Learning Environment (ALE), including several
ROM fixes, and now handle loading and saving of the emulator state. While
seeds still ensure determinism, the effect of any given seed is not preserved
across this upgrade because the random number generator in ALE has changed.
The *NoFrameSkip-v4 environments should be considered the canonical Atari
environments from now on.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2017-03-05: BACKWARDS INCOMPATIBILITY: The configure method has been removed
from Env. configure was not used by gym, but was used by some dependent
libraries including universe. These libraries will migrate away from the
configure method by using wrappers instead. This change is on master and will be released with 0.8.0.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2016-12-27: BACKWARDS INCOMPATIBILITY: The gym monitor is now a
wrapper. Rather than starting monitoring as
env.monitor.start(directory), envs are now wrapped as follows:
env = wrappers.Monitor(env, directory). This change is on master
and will be released with 0.7.0.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2016-11-1: Several experimental changes to how a running monitor interacts
with environments. The monitor will now raise an error if reset() is called
when the env has not returned done=True. The monitor will only record complete
episodes where done=True. Finally, the monitor no longer calls seed() on the
underlying env, nor does it record or upload seed information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2016-10-31: We're experimentally expanding the environment ID format
to include an optional username.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2016-09-21: Switch the Gym automated logger setup to configure the
root logger rather than just the 'gym' logger.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2016-08-17: Calling close on an env will also close the monitor
and any rendering windows.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2016-08-17: The monitor will no longer write manifest files in
real-time, unless write_upon_reset=True is passed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2016-05-28: For controlled reproducibility, envs now support seeding
(cf #91 and #135). The monitor records which seeds are used. We will
soon add seed information to the display on the scoreboard.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>openai</author><guid isPermaLink="false">https://github.com/openai/gym</guid><pubDate>Wed, 12 Feb 2020 00:14:00 GMT</pubDate></item><item><title>explosion/spaCy #15 in Python, Today</title><link>https://github.com/explosion/spaCy</link><description>&lt;p&gt;&lt;i&gt;💫 Industrial-strength Natural Language Processing (NLP) with Python and Cython&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://explosion.ai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/7ff9a44956cca00cd229ee0d31b5cab82e8d7f2c/68747470733a2f2f6578706c6f73696f6e2e61692f6173736574732f696d672f6c6f676f2e737667" width="125" height="125" align="right" data-canonical-src="https://explosion.ai/assets/img/logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-spacy-industrial-strength-nlp" class="anchor" aria-hidden="true" href="#spacy-industrial-strength-nlp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;spaCy: Industrial-strength NLP&lt;/h1&gt;
&lt;p&gt;spaCy is a library for advanced Natural Language Processing in Python and
Cython. It's built on the very latest research, and was designed from day one to
be used in real products. spaCy comes with
&lt;a href="https://spacy.io/models" rel="nofollow"&gt;pretrained statistical models&lt;/a&gt; and word vectors, and
currently supports tokenization for &lt;strong&gt;50+ languages&lt;/strong&gt;. It features
state-of-the-art speed, convolutional &lt;strong&gt;neural network models&lt;/strong&gt; for tagging,
parsing and &lt;strong&gt;named entity recognition&lt;/strong&gt; and easy &lt;strong&gt;deep learning&lt;/strong&gt; integration.
It's commercial open-source software, released under the MIT license.&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="dizzy" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ab.png"&gt;💫&lt;/g-emoji&gt; &lt;strong&gt;Version 2.2 out now!&lt;/strong&gt;
&lt;a href="https://github.com/explosion/spaCy/releases"&gt;Check out the release notes here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://dev.azure.com/explosion-ai/public/_build?definitionId=8" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/44691c85bd63739a58e319d01805bc3509d321ed/68747470733a2f2f696d672e736869656c64732e696f2f617a7572652d6465766f70732f6275696c642f6578706c6f73696f6e2d61692f7075626c69632f382f6d61737465722e7376673f6c6f676f3d617a7572652d706970656c696e6573267374796c653d666c61742d737175617265266c6162656c3d6275696c642b28332e7829" alt="Azure Pipelines" data-canonical-src="https://img.shields.io/azure-devops/build/explosion-ai/public/8/master.svg?logo=azure-pipelines&amp;amp;style=flat-square&amp;amp;label=build+(3.x)" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/explosion/spaCy" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/84d1b54e4a92829187a86d175284be75b2996e1b/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f6578706c6f73696f6e2f73706143792f6d61737465722e7376673f7374796c653d666c61742d737175617265266c6f676f3d7472617669732d6369266c6f676f436f6c6f723d7768697465266c6162656c3d6275696c642b28322e3729" alt="Travis Build Status" data-canonical-src="https://img.shields.io/travis/explosion/spaCy/master.svg?style=flat-square&amp;amp;logo=travis-ci&amp;amp;logoColor=white&amp;amp;label=build+(2.7)" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/explosion/spaCy/releases"&gt;&lt;img src="https://camo.githubusercontent.com/3a846b021a1f95f083d5bfcb0924cb37c173b9c7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6578706c6f73696f6e2f73706163792e7376673f7374796c653d666c61742d737175617265266c6f676f3d676974687562" alt="Current Release Version" data-canonical-src="https://img.shields.io/github/release/explosion/spacy.svg?style=flat-square&amp;amp;logo=github" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/spacy/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e75e3197b2e01a77c85ede31b30f3c72e0dcc764/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f73706163792e7376673f7374796c653d666c61742d737175617265266c6f676f3d70797069266c6f676f436f6c6f723d7768697465" alt="pypi Version" data-canonical-src="https://img.shields.io/pypi/v/spacy.svg?style=flat-square&amp;amp;logo=pypi&amp;amp;logoColor=white" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://anaconda.org/conda-forge/spacy" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/34a0c453d8772f2f45f6e515e58725aad5f07496/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f766e2f636f6e64612d666f7267652f73706163792e7376673f7374796c653d666c61742d737175617265266c6f676f3d636f6e64612d666f726765266c6f676f436f6c6f723d7768697465" alt="conda Version" data-canonical-src="https://img.shields.io/conda/vn/conda-forge/spacy.svg?style=flat-square&amp;amp;logo=conda-forge&amp;amp;logoColor=white" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/explosion/wheelwright/releases"&gt;&lt;img src="https://camo.githubusercontent.com/0a3340634298fdb0d4ab4ae2ea7959fe84d5642a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f776865656c732d2545322539432539332d3463312e7376673f6c6f6e6743616368653d74727565267374796c653d666c61742d737175617265266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465" alt="Python wheels" data-canonical-src="https://img.shields.io/badge/wheels-%E2%9C%93-4c1.svg?longCache=true&amp;amp;style=flat-square&amp;amp;logo=python&amp;amp;logoColor=white" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/spacy/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/fb1798cc8dde7ed8b1a0e9b120be8b8c296752c4/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f73706163793f7374796c653d666c61742d737175617265266c6f676f3d70797069266c6f676f436f6c6f723d7768697465" alt="PyPi downloads" data-canonical-src="https://img.shields.io/pypi/dm/spacy?style=flat-square&amp;amp;logo=pypi&amp;amp;logoColor=white" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://anaconda.org/conda-forge/spacy" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2581943f7d5da0cd63bdb8095da9bbd408c0c9f0/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f636f6e64612d666f7267652f73706163793f7374796c653d666c61742d737175617265266c6f676f3d636f6e64612d666f726765266c6f676f436f6c6f723d7768697465" alt="Conda downloads" data-canonical-src="https://img.shields.io/conda/dn/conda-forge/spacy?style=flat-square&amp;amp;logo=conda-forge&amp;amp;logoColor=white" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/explosion/spacy-models/releases"&gt;&lt;img src="https://camo.githubusercontent.com/3606022a9ffd5b09aac991fa82fb2c2d586c6967/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f6578706c6f73696f6e2f73706163792d6d6f64656c732f746f74616c3f7374796c653d666c61742d737175617265266c6162656c3d6d6f64656c2b646f776e6c6f616473" alt="Model downloads" data-canonical-src="https://img.shields.io/github/downloads/explosion/spacy-models/total?style=flat-square&amp;amp;label=model+downloads" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/ambv/black"&gt;&lt;img src="https://camo.githubusercontent.com/b243a4527ef0973b38d54728651624816b6d7844/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e7376673f7374796c653d666c61742d737175617265" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/spacy_io" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/115e5588e7a408e00ac76bd603551a90e7e2d9df/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f73706163795f696f2e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="spaCy on Twitter" data-canonical-src="https://img.shields.io/twitter/follow/spacy_io.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--documentation" class="anchor" aria-hidden="true" href="#-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;📖&lt;/g-emoji&gt; Documentation&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Documentation&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://spacy.io/usage/spacy-101" rel="nofollow"&gt;spaCy 101&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;New to spaCy? Here's everything you need to know!&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://spacy.io/usage/" rel="nofollow"&gt;Usage Guides&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;How to use spaCy and its features.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://spacy.io/usage/v2-2" rel="nofollow"&gt;New in v2.2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;New features, backwards incompatibilities and migration guide.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://spacy.io/api/" rel="nofollow"&gt;API Reference&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The detailed reference for spaCy's API.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://spacy.io/models" rel="nofollow"&gt;Models&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Download statistical language models for spaCy.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://spacy.io/universe" rel="nofollow"&gt;Universe&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Libraries, extensions, demos, books and courses.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://spacy.io/usage#changelog" rel="nofollow"&gt;Changelog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Changes and version history.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/explosion/spaCy/blob/master/CONTRIBUTING.md"&gt;Contribute&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;How to contribute to the spaCy project and code base.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content--where-to-ask-questions" class="anchor" aria-hidden="true" href="#-where-to-ask-questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="speech_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png"&gt;💬&lt;/g-emoji&gt; Where to ask questions&lt;/h2&gt;
&lt;p&gt;The spaCy project is maintained by &lt;a href="https://github.com/honnibal"&gt;@honnibal&lt;/a&gt; and
&lt;a href="https://github.com/ines"&gt;@ines&lt;/a&gt;, along with core contributors
&lt;a href="https://github.com/svlandeg"&gt;@svlandeg&lt;/a&gt; and
&lt;a href="https://github.com/adrianeboyd"&gt;@adrianeboyd&lt;/a&gt;. Please understand that we won't
be able to provide individual support via email. We also believe that help is
much more valuable if it's shared publicly, so that more people can benefit from
it.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Platforms&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;g-emoji class="g-emoji" alias="rotating_light" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a8.png"&gt;🚨&lt;/g-emoji&gt; &lt;strong&gt;Bug Reports&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/explosion/spaCy/issues"&gt;GitHub Issue Tracker&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;g-emoji class="g-emoji" alias="gift" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f381.png"&gt;🎁&lt;/g-emoji&gt; &lt;strong&gt;Feature Requests&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/explosion/spaCy/issues"&gt;GitHub Issue Tracker&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;g-emoji class="g-emoji" alias="woman_technologist" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f469-1f4bb.png"&gt;👩‍💻&lt;/g-emoji&gt; &lt;strong&gt;Usage Questions&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://stackoverflow.com/questions/tagged/spacy" rel="nofollow"&gt;Stack Overflow&lt;/a&gt; · &lt;a href="https://gitter.im/explosion/spaCy" rel="nofollow"&gt;Gitter Chat&lt;/a&gt; · &lt;a href="https://www.reddit.com/r/spacynlp" rel="nofollow"&gt;Reddit User Group&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;g-emoji class="g-emoji" alias="right_anger_bubble" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5ef.png"&gt;🗯&lt;/g-emoji&gt; &lt;strong&gt;General Discussion&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://gitter.im/explosion/spaCy" rel="nofollow"&gt;Gitter Chat&lt;/a&gt; · &lt;a href="https://www.reddit.com/r/spacynlp" rel="nofollow"&gt;Reddit User Group&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Non-destructive &lt;strong&gt;tokenization&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Named entity&lt;/strong&gt; recognition&lt;/li&gt;
&lt;li&gt;Support for &lt;strong&gt;50+ languages&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;pretrained &lt;a href="https://spacy.io/models" rel="nofollow"&gt;statistical models&lt;/a&gt; and word vectors&lt;/li&gt;
&lt;li&gt;State-of-the-art speed&lt;/li&gt;
&lt;li&gt;Easy &lt;strong&gt;deep learning&lt;/strong&gt; integration&lt;/li&gt;
&lt;li&gt;Part-of-speech tagging&lt;/li&gt;
&lt;li&gt;Labelled dependency parsing&lt;/li&gt;
&lt;li&gt;Syntax-driven sentence segmentation&lt;/li&gt;
&lt;li&gt;Built in &lt;strong&gt;visualizers&lt;/strong&gt; for syntax and NER&lt;/li&gt;
&lt;li&gt;Convenient string-to-hash mapping&lt;/li&gt;
&lt;li&gt;Export to numpy data arrays&lt;/li&gt;
&lt;li&gt;Efficient binary serialization&lt;/li&gt;
&lt;li&gt;Easy &lt;strong&gt;model packaging&lt;/strong&gt; and deployment&lt;/li&gt;
&lt;li&gt;Robust, rigorously evaluated accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;📖&lt;/g-emoji&gt; &lt;strong&gt;For more details, see the
&lt;a href="https://spacy.io/usage/facts-figures" rel="nofollow"&gt;facts, figures and benchmarks&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-install-spacy" class="anchor" aria-hidden="true" href="#install-spacy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install spaCy&lt;/h2&gt;
&lt;p&gt;For detailed installation instructions, see the
&lt;a href="https://spacy.io/usage" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Operating system&lt;/strong&gt;: macOS / OS X · Linux · Windows (Cygwin, MinGW, Visual
Studio)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Python version&lt;/strong&gt;: Python 2.7, 3.5+ (only 64 bit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Package managers&lt;/strong&gt;: &lt;a href="https://pypi.org/project/spacy/" rel="nofollow"&gt;pip&lt;/a&gt; · &lt;a href="https://anaconda.org/conda-forge/spacy" rel="nofollow"&gt;conda&lt;/a&gt; (via &lt;code&gt;conda-forge&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pip" class="anchor" aria-hidden="true" href="#pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pip&lt;/h3&gt;
&lt;p&gt;Using pip, spaCy releases are available as source packages and binary wheels (as
of &lt;code&gt;v2.0.13&lt;/code&gt;).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install spacy&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To install additional data tables for lemmatization in &lt;strong&gt;spaCy v2.2+&lt;/strong&gt; you can
run &lt;code&gt;pip install spacy[lookups]&lt;/code&gt; or install
&lt;a href="https://github.com/explosion/spacy-lookups-data"&gt;&lt;code&gt;spacy-lookups-data&lt;/code&gt;&lt;/a&gt;
separately. The lookups package is needed to create blank models with
lemmatization data, and to lemmatize in languages that don't yet come with
pretrained models and aren't powered by third-party libraries.&lt;/p&gt;
&lt;p&gt;When using pip it is generally recommended to install packages in a virtual
environment to avoid modifying system state:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m venv .env
&lt;span class="pl-c1"&gt;source&lt;/span&gt; .env/bin/activate
pip install spacy&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-conda" class="anchor" aria-hidden="true" href="#conda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;conda&lt;/h3&gt;
&lt;p&gt;Thanks to our great community, we've finally re-added conda support. You can now
install spaCy via &lt;code&gt;conda-forge&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c conda-forge spacy&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For the feedstock including the build recipe and configuration, check out
&lt;a href="https://github.com/conda-forge/spacy-feedstock"&gt;this repository&lt;/a&gt;. Improvements
and pull requests to the recipe and setup are always appreciated.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-updating-spacy" class="anchor" aria-hidden="true" href="#updating-spacy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updating spaCy&lt;/h3&gt;
&lt;p&gt;Some updates to spaCy may require downloading new statistical models. If you're
running spaCy v2.0 or higher, you can use the &lt;code&gt;validate&lt;/code&gt; command to check if
your installed models are compatible and if not, print details on how to update
them:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -U spacy
python -m spacy validate&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you've trained your own models, keep in mind that your training and runtime
inputs must match. After updating spaCy, we recommend &lt;strong&gt;retraining your models&lt;/strong&gt;
with the new version.&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;📖&lt;/g-emoji&gt; &lt;strong&gt;For details on upgrading from spaCy 1.x to spaCy 2.x, see the
&lt;a href="https://spacy.io/usage/v2#migrating" rel="nofollow"&gt;migration guide&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-download-models" class="anchor" aria-hidden="true" href="#download-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download models&lt;/h2&gt;
&lt;p&gt;As of v1.7.0, models for spaCy can be installed as &lt;strong&gt;Python packages&lt;/strong&gt;. This
means that they're a component of your application, just like any other module.
Models can be installed using spaCy's &lt;code&gt;download&lt;/code&gt; command, or manually by
pointing pip to a path or URL.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Documentation&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://spacy.io/models" rel="nofollow"&gt;Available Models&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Detailed model descriptions, accuracy figures and benchmarks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://spacy.io/docs/usage/models" rel="nofollow"&gt;Models Documentation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Detailed usage instructions.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; download best-matching version of specific model for your spaCy installation&lt;/span&gt;
python -m spacy download en_core_web_sm

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pip install .tar.gz archive from path or URL&lt;/span&gt;
pip install /Users/you/en_core_web_sm-2.2.0.tar.gz
pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-loading-and-using-models" class="anchor" aria-hidden="true" href="#loading-and-using-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Loading and using models&lt;/h3&gt;
&lt;p&gt;To load a model, use &lt;code&gt;spacy.load()&lt;/code&gt; with the model name, a shortcut link or a
path to the model data directory.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; spacy
nlp &lt;span class="pl-k"&gt;=&lt;/span&gt; spacy.load(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;en_core_web_sm&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
doc &lt;span class="pl-k"&gt;=&lt;/span&gt; nlp(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;This is a sentence.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can also &lt;code&gt;import&lt;/code&gt; a model directly via its full name and then call its
&lt;code&gt;load()&lt;/code&gt; method with no arguments.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; spacy
&lt;span class="pl-k"&gt;import&lt;/span&gt; en_core_web_sm

nlp &lt;span class="pl-k"&gt;=&lt;/span&gt; en_core_web_sm.load()
doc &lt;span class="pl-k"&gt;=&lt;/span&gt; nlp(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;This is a sentence.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;📖&lt;/g-emoji&gt; &lt;strong&gt;For more info and examples, check out the
&lt;a href="https://spacy.io/docs/usage/models" rel="nofollow"&gt;models documentation&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-compile-from-source" class="anchor" aria-hidden="true" href="#compile-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compile from source&lt;/h2&gt;
&lt;p&gt;The other way to install spaCy is to clone its
&lt;a href="https://github.com/explosion/spaCy"&gt;GitHub repository&lt;/a&gt; and build it from
source. That is the common way if you want to make changes to the code base.
You'll need to make sure that you have a development environment consisting of a
Python distribution including header files, a compiler,
&lt;a href="https://pip.pypa.io/en/latest/installing/" rel="nofollow"&gt;pip&lt;/a&gt;,
&lt;a href="https://virtualenv.pypa.io/en/latest/" rel="nofollow"&gt;virtualenv&lt;/a&gt; and
&lt;a href="https://git-scm.com" rel="nofollow"&gt;git&lt;/a&gt; installed. The compiler part is the trickiest. How to
do that depends on your system. See notes on Ubuntu, OS X and Windows for
details.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; make sure you are using the latest pip&lt;/span&gt;
python -m pip install -U pip
git clone https://github.com/explosion/spaCy
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; spaCy

python -m venv .env
&lt;span class="pl-c1"&gt;source&lt;/span&gt; .env/bin/activate
&lt;span class="pl-k"&gt;export&lt;/span&gt; PYTHONPATH=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;`&lt;/span&gt;pwd&lt;span class="pl-pds"&gt;`&lt;/span&gt;&lt;/span&gt;
pip install -r requirements.txt
python setup.py build_ext --inplace&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Compared to regular install via pip, &lt;a href="requirements.txt"&gt;requirements.txt&lt;/a&gt;
additionally installs developer dependencies such as Cython. For more details
and instructions, see the documentation on
&lt;a href="https://spacy.io/usage#source" rel="nofollow"&gt;compiling spaCy from source&lt;/a&gt; and the
&lt;a href="https://spacy.io/usage#section-quickstart" rel="nofollow"&gt;quickstart widget&lt;/a&gt; to get the right
commands for your platform and Python version.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-ubuntu" class="anchor" aria-hidden="true" href="#ubuntu"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ubuntu&lt;/h3&gt;
&lt;p&gt;Install system-level dependencies via &lt;code&gt;apt-get&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo apt-get install build-essential python-dev git&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-macos--os-x" class="anchor" aria-hidden="true" href="#macos--os-x"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;macOS / OS X&lt;/h3&gt;
&lt;p&gt;Install a recent version of &lt;a href="https://developer.apple.com/xcode/" rel="nofollow"&gt;XCode&lt;/a&gt;,
including the so-called "Command Line Tools". macOS and OS X ship with Python
and git preinstalled.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-windows" class="anchor" aria-hidden="true" href="#windows"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Windows&lt;/h3&gt;
&lt;p&gt;Install a version of the
&lt;a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/" rel="nofollow"&gt;Visual C++ Build Tools&lt;/a&gt;
or &lt;a href="https://visualstudio.microsoft.com/vs/express/" rel="nofollow"&gt;Visual Studio Express&lt;/a&gt; that
matches the version that was used to compile your Python interpreter. For
official distributions these are VS 2008 (Python 2.7), VS 2010 (Python 3.4) and
VS 2015 (Python 3.5).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-run-tests" class="anchor" aria-hidden="true" href="#run-tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run tests&lt;/h2&gt;
&lt;p&gt;spaCy comes with an &lt;a href="spacy/tests"&gt;extensive test suite&lt;/a&gt;. In order to run the
tests, you'll usually want to clone the repository and build spaCy from source.
This will also install the required development dependencies and test utilities
defined in the &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Alternatively, you can find out where spaCy is installed and run &lt;code&gt;pytest&lt;/code&gt; on
that directory. Don't forget to also install the test utilities via spaCy's
&lt;code&gt;requirements.txt&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;import os; import spacy; print(os.path.dirname(spacy.__file__))&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
pip install -r path/to/requirements.txt
python -m pytest &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;spacy-directory&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See &lt;a href="https://spacy.io/usage#tests" rel="nofollow"&gt;the documentation&lt;/a&gt; for more details and
examples.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>explosion</author><guid isPermaLink="false">https://github.com/explosion/spaCy</guid><pubDate>Wed, 12 Feb 2020 00:15:00 GMT</pubDate></item><item><title>tensortrade-org/tensortrade #16 in Python, Today</title><link>https://github.com/tensortrade-org/tensortrade</link><description>&lt;p&gt;&lt;i&gt;An open source reinforcement learning framework for training, evaluating, and deploying robust trading agents.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensortrade-trade-efficiently-with-reinforcement-learning" class="anchor" aria-hidden="true" href="#tensortrade-trade-efficiently-with-reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://towardsdatascience.com/trade-smarter-w-reinforcement-learning-a5e91163f315?source=friends_link&amp;amp;sk=ea3afd0a305141eb9147be4718826dfb" rel="nofollow"&gt;TensorTrade: Trade Efficiently with Reinforcement Learning&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/tensortrade-org/tensortrade" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2bcdab8df0b80aa293f5759c971c3a3668cad096/68747470733a2f2f7472617669732d63692e636f6d2f74656e736f7274726164652d6f72672f74656e736f7274726164652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/tensortrade-org/tensortrade.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://tensortrade.org" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/1507bfd72bf32b8fc46754503d5d8dc464c023fe/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f74656e736f7274726164652f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/tensortrade/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/7614d67c407e113e33724953d614b3fc6a488fb2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f74656e736f7274726164652d6f72672f74656e736f7274726164652e7376673f636f6c6f723d627269676874677265656e" alt="Apache License" data-canonical-src="https://img.shields.io/github/license/tensortrade-org/tensortrade.svg?color=brightgreen" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://discord.gg/ZZ7BGWh" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/11bef0bdfed0a6d99ad8c933cfa835cef46a755b/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3539323434363632343838323439313430322e7376673f636f6c6f723d627269676874677265656e" alt="Discord" data-canonical-src="https://img.shields.io/discord/592446624882491402.svg?color=brightgreen" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.python.org/downloads/release/python-360/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c2ed0c1d8ac1a5ebbe7281923d42b50b7962912c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d626c75652e737667" alt="Python 3.6" data-canonical-src="https://img.shields.io/badge/python-3.6-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/notadamking/tensortrade/blob/master/docs/source/_static/logo.jpg"&gt;&lt;img src="https://github.com/notadamking/tensortrade/raw/master/docs/source/_static/logo.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;TensorTrade is still in Beta, meaning it should be used very cautiously if used in production, as it may contain bugs.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TensorTrade is an open source Python framework for building, training, evaluating, and deploying robust trading algorithms using reinforcement learning. The framework focuses on being highly composable and extensible, to allow the system to scale from simple trading strategies on a single CPU, to complex investment strategies run on a distribution of HPC machines.&lt;/p&gt;
&lt;p&gt;Under the hood, the framework uses many of the APIs from existing machine learning libraries to maintain high quality data pipelines and learning models. One of the main goals of TensorTrade is to enable fast experimentation with algorithmic trading strategies, by leveraging the existing tools and pipelines provided by &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;gym&lt;/code&gt;, &lt;code&gt;keras&lt;/code&gt;, and &lt;code&gt;tensorflow&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Every piece of the framework is split up into re-usable components, allowing you to take advantage of the general use components built by the community, while keeping your proprietary features private. The aim is to simplify the process of testing and deploying robust trading agents using deep reinforcement learning, to allow you and I to focus on creating profitable strategies.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The goal of this framework is to enable fast experimentation, while maintaining production-quality data pipelines.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Read &lt;a href="http://tensortrade.org" rel="nofollow"&gt;the documentation&lt;/a&gt; or walk through &lt;a href="https://towardsdatascience.com/trade-smarter-w-reinforcement-learning-a5e91163f315?source=friends_link&amp;amp;sk=ea3afd0a305141eb9147be4718826dfb" rel="nofollow"&gt;the tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-guiding-principles" class="anchor" aria-hidden="true" href="#guiding-principles"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Guiding principles&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Inspired by &lt;a href="https://github.com/keras-team/keras"&gt;Keras' guiding principles&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;User friendliness.&lt;/strong&gt; TensorTrade is an API designed for human beings, not machines. It puts user experience front and center. TensorTrade follows best practices for reducing cognitive load: it offers consistent &amp;amp; simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Modularity.&lt;/strong&gt; A trading environment is a conglomeration of fully configurable modules that can be plugged together with as few restrictions as possible. In particular, exchanges, feature pipelines, action schemes, reward schemes, trading agents, and performance reports are all standalone modules that you can combine to create new trading environments.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Easy extensibility.&lt;/strong&gt; New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making TensorTrade suitable for advanced research and production use.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;You can get started testing on Google Colab or your local machine, by viewing our &lt;a href="https://github.com/notadamking/tensortrade/tree/master/examples"&gt;many examples&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;TensorTrade requires Python &amp;gt;= 3.6 for all functionality to work as expected.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-docker" class="anchor" aria-hidden="true" href="#docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker&lt;/h2&gt;
&lt;p&gt;To run the commands below, ensure Docker is installed. Visit &lt;a href="https://docs.docker.com/install/" rel="nofollow"&gt;https://docs.docker.com/install/&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run-jupyter-notebooks" class="anchor" aria-hidden="true" href="#run-jupyter-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Jupyter Notebooks&lt;/h3&gt;
&lt;p&gt;To run a jupyter notebook in your browser, execute the following command and visit the &lt;code&gt;http://127.0.0.1:8888/?token=...&lt;/code&gt; link printed to the command line.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;make run-notebook&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-build-documentation" class="anchor" aria-hidden="true" href="#build-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build Documentation&lt;/h3&gt;
&lt;p&gt;To build the HTML documentation, execute the following command.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;make run-docs&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-run-test-suite" class="anchor" aria-hidden="true" href="#run-test-suite"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Test Suite&lt;/h3&gt;
&lt;p&gt;To run the test suite, execute the following command.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;make run-tests&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h2&gt;
&lt;p&gt;You can ask questions and join the development discussion:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the &lt;a href="https://discord.gg/ZZ7BGWh" rel="nofollow"&gt;TensorTrade Discord server&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;On the &lt;a href="https://gitter.im/tensortrade-framework/community" rel="nofollow"&gt;TensorTrade Gitter&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also post &lt;strong&gt;bug reports and feature requests&lt;/strong&gt; in &lt;a href="https://github.com/notadamking/tensortrade/issues"&gt;GitHub issues&lt;/a&gt;. Make sure to read &lt;a href="https://github.com/notadamking/tensortrade/blob/master/CONTRIBUTING.md"&gt;our guidelines&lt;/a&gt; first.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sponsorship" class="anchor" aria-hidden="true" href="#sponsorship"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sponsorship&lt;/h2&gt;
&lt;p&gt;If you would like to support this project financially, there are a few ways you can contribute. Your contributions are greatly appreciated and help to keep TensorTrade maintained and always improving.&lt;/p&gt;
&lt;p&gt;Github Sponsors: &lt;a href="https://github.com/sponsors/notadamking"&gt;https://github.com/sponsors/notadamking&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;All Github Sponsors donations are matched 1:1 by Github up to $5,000!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Gitcoin Grants: &lt;a href="https://gitcoin.co/grants/155/tensortrade-trade-efficiently-with-reinforcement-l" rel="nofollow"&gt;https://gitcoin.co/grants/155/tensortrade-trade-efficiently-with-reinforcement-l&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;All Gitcoin Grants donations go directly towards funding our Gitcoin issues.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;BTC Address: &lt;code&gt;1Lc47bhYvdyKGk1qN8oBHdYQTkbFLL3PFw&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;ETH Address: &lt;code&gt;0x9907A0cF64Ec9Fbf6Ed8FD4971090DE88222a9aC&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;Contributions are encouraged and welcomed. This project is meant to grow as the community around it grows. Let me know on Discord in the #suggestions channel if there is anything that you would like to see in the future, or if there is anything you feel is missing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Working on your first Pull Request?&lt;/strong&gt; You can learn how from this &lt;em&gt;free&lt;/em&gt; series &lt;a href="https://egghead.io/series/how-to-contribute-to-an-open-source-project-on-github" rel="nofollow"&gt;How to Contribute to an Open Source Project on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/11e0e3968f7aad0c841cdab9a5d746dd02b9c394/68747470733a2f2f636f6e7472696275746f72732d696d672e66697265626173656170702e636f6d2f696d6167653f7265706f3d6e6f746164616d6b696e672f74656e736f727472616465"&gt;&lt;img src="https://camo.githubusercontent.com/11e0e3968f7aad0c841cdab9a5d746dd02b9c394/68747470733a2f2f636f6e7472696275746f72732d696d672e66697265626173656170702e636f6d2f696d6167653f7265706f3d6e6f746164616d6b696e672f74656e736f727472616465" alt="https://github.com/notadamking/tensortrade/graphs/contributors" data-canonical-src="https://contributors-img.firebaseapp.com/image?repo=notadamking/tensortrade" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensortrade-org</author><guid isPermaLink="false">https://github.com/tensortrade-org/tensortrade</guid><pubDate>Wed, 12 Feb 2020 00:16:00 GMT</pubDate></item><item><title>fendouai/PyTorchDocs #17 in Python, Today</title><link>https://github.com/fendouai/PyTorchDocs</link><description>&lt;p&gt;&lt;i&gt;PyTorch 官方中文教程包含 60 分钟快速入门教程，强化教程，计算机视觉，自然语言处理，生成对抗网络，强化学习。欢迎 Star，Fork！&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简介&lt;/h1&gt;
&lt;p&gt;目前研究人员正在使用的深度学习框架不尽相同，有 TensorFlow 、PyTorch、Keras等。这些深度学习框架被应用于计算机视觉、语音识别、自然语言处理与生物信息学等领域，并获取了极好的效果。其中，PyTorch是当前难得的简洁优雅且高效快速的框架，当前开源的框架中，没有哪一个框架能够在灵活性、易用性、速度这三个方面有两个能同时超过PyTorch。&lt;/p&gt;
&lt;p&gt;本文档的定位是 PyTorch 入门教程，主要针对想要学习PyTorch的学生群体或者深度学习爱好者。通过教程的学习，能够实现零基础想要了解和学习深度学习，降低自学的难度，快速学习PyTorch。&lt;/p&gt;
&lt;p&gt;官方教程包含了 PyTorch 介绍，安装教程；60分钟快速入门教程，可以迅速从小白阶段完成一个分类器模型；计算机视觉常用模型，方便基于自己的数据进行调整，不再需要从头开始写；自然语言处理模型，聊天机器人，文本生成等生动有趣的项目。&lt;/p&gt;
&lt;p&gt;总而言之：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果你想了解一下 PyTorch，可以看介绍部分。&lt;/li&gt;
&lt;li&gt;如果你想快速入门 PyTorch，可以看60分钟快速入门。&lt;/li&gt;
&lt;li&gt;如果你想解决计算机视觉问题，可以看计算机视觉部分。&lt;/li&gt;
&lt;li&gt;如果你想解决自然语言处理问题，可以看NLP 部分。&lt;/li&gt;
&lt;li&gt;还有强化学习和生成对抗网络部分内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作者：&lt;a href="http://www.panchuangai.com/" rel="nofollow"&gt;磐创AI&lt;/a&gt; &lt;a href="http://pytorch123.com/" rel="nofollow"&gt;PyTorch&lt;/a&gt; 翻译小组: News &amp;amp; PanChuang&lt;/p&gt;
&lt;p&gt;原文：&lt;a href="https://pytorch.org/tutorials/" rel="nofollow"&gt;https://pytorch.org/tutorials/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-第一章pytorch之简介与下载" class="anchor" aria-hidden="true" href="#第一章pytorch之简介与下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一章：PyTorch之简介与下载&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1pytorch简介" class="anchor" aria-hidden="true" href="#1pytorch简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/FirstSection/PyTorchIntro.md"&gt;PyTorch简介&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-2pytorch环境搭建" class="anchor" aria-hidden="true" href="#2pytorch环境搭建"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/FirstSection/InstallIutorial.md"&gt;PyTorch环境搭建&lt;/a&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-第二章pytorch之60min入门" class="anchor" aria-hidden="true" href="#第二章pytorch之60min入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二章：PyTorch之60min入门&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1pytorch-入门" class="anchor" aria-hidden="true" href="#1pytorch-入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/SecondSection/what_is_pytorch.md"&gt;PyTorch 入门&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-2pytorch-自动微分" class="anchor" aria-hidden="true" href="#2pytorch-自动微分"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/SecondSection/autograd_automatic_differentiation.md"&gt;PyTorch 自动微分&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-3pytorch-神经网络" class="anchor" aria-hidden="true" href="#3pytorch-神经网络"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/SecondSection/neural_networks.md"&gt;PyTorch 神经网络&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-4pytorch-图像分类器" class="anchor" aria-hidden="true" href="#4pytorch-图像分类器"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/SecondSection/training_a_classifier.md"&gt;PyTorch 图像分类器&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-5pytorch-数据并行处理" class="anchor" aria-hidden="true" href="#5pytorch-数据并行处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/SecondSection/optional_data_parallelism.md"&gt;PyTorch 数据并行处理&lt;/a&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-第三章pytorch之入门强化" class="anchor" aria-hidden="true" href="#第三章pytorch之入门强化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三章：PyTorch之入门强化&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1数据加载和处理" class="anchor" aria-hidden="true" href="#1数据加载和处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/ThirdSection/DataLoding.md"&gt;数据加载和处理&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-2pytorch小试牛刀" class="anchor" aria-hidden="true" href="#2pytorch小试牛刀"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/ThirdSection/LearningPyTorch.md"&gt;PyTorch小试牛刀&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-3迁移学习" class="anchor" aria-hidden="true" href="#3迁移学习"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/ThirdSection/TransferLearning.md"&gt;迁移学习&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-4混合前端的seq2seq模型部署" class="anchor" aria-hidden="true" href="#4混合前端的seq2seq模型部署"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/ThirdSection/DeployingSeq2SeqModelwithHybridFrontend.MD"&gt;混合前端的seq2seq模型部署&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-5保存和加载模型" class="anchor" aria-hidden="true" href="#5保存和加载模型"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/ThirdSection/SaveModel.md"&gt;保存和加载模型&lt;/a&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-第四章pytorch之图像篇" class="anchor" aria-hidden="true" href="#第四章pytorch之图像篇"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四章：PyTorch之图像篇&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1微调基于torchvision-03的目标检测模型" class="anchor" aria-hidden="true" href="#1微调基于torchvision-03的目标检测模型"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/fourSection/ObjectDetectionFinetuning.md"&gt;微调基于torchvision 0.3的目标检测模型&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-2微调torchvision模型" class="anchor" aria-hidden="true" href="#2微调torchvision模型"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/fourSection/FinetuningTorchVisionModel.md"&gt;微调TorchVision模型&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-3空间变换器网络" class="anchor" aria-hidden="true" href="#3空间变换器网络"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/fourSection/SpatialTranNet.md"&gt;空间变换器网络&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-4使用pytorch进行neural-transfer" class="anchor" aria-hidden="true" href="#4使用pytorch进行neural-transfer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/fourSection/NeuralTransfer.md"&gt;使用PyTorch进行Neural-Transfer&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-5生成对抗示例" class="anchor" aria-hidden="true" href="#5生成对抗示例"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/fourSection/AdversarialExampleGene.md"&gt;生成对抗示例&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-6使用onnx将模型转移至caffe2和移动端" class="anchor" aria-hidden="true" href="#6使用onnx将模型转移至caffe2和移动端"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;6.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/fourSection/ONNX.md"&gt;使用ONNX将模型转移至Caffe2和移动端&lt;/a&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-第五章pytorch之文本篇" class="anchor" aria-hidden="true" href="#第五章pytorch之文本篇"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五章：PyTorch之文本篇&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1聊天机器人教程" class="anchor" aria-hidden="true" href="#1聊天机器人教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/FifthSection/Chatbot.md"&gt;聊天机器人教程&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-2使用字符级rnn生成名字" class="anchor" aria-hidden="true" href="#2使用字符级rnn生成名字"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/FifthSection/Char%20RNN%20Generation.MD"&gt;使用字符级RNN生成名字&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-3使用字符级rnn进行名字分类" class="anchor" aria-hidden="true" href="#3使用字符级rnn进行名字分类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/FifthSection/Char%20RNN%20Classification.md"&gt;使用字符级RNN进行名字分类&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-4在深度学习和nlp中使用pytorch" class="anchor" aria-hidden="true" href="#4在深度学习和nlp中使用pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/FifthSection/DeepLearning%20NLP.md"&gt;在深度学习和NLP中使用Pytorch&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-5使用sequence2sequence网络和注意力进行翻译" class="anchor" aria-hidden="true" href="#5使用sequence2sequence网络和注意力进行翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/FifthSection/Translation_S2S%20Network.md"&gt;使用Sequence2Sequence网络和注意力进行翻译&lt;/a&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-第六章pytorch之生成对抗网络" class="anchor" aria-hidden="true" href="#第六章pytorch之生成对抗网络"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第六章：PyTorch之生成对抗网络&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1生成对抗网络generative-adversarial-networks" class="anchor" aria-hidden="true" href="#1生成对抗网络generative-adversarial-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/SixthSection/Dcgan.md"&gt;生成对抗网络（Generative Adversarial Networks）&lt;/a&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-第七章pytorch之强化学习" class="anchor" aria-hidden="true" href="#第七章pytorch之强化学习"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第七章：PyTorch之强化学习&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1强化学习dqn" class="anchor" aria-hidden="true" href="#1强化学习dqn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.&lt;a href="https://github.com/fendouai/PyTorchDocs/blob/master/SeventhSection/ReinforcementLearning.md"&gt;强化学习（DQN）&lt;/a&gt;&lt;/h3&gt;
&lt;h1&gt;&lt;a id="user-content-教程推荐" class="anchor" aria-hidden="true" href="#教程推荐"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;教程推荐&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;PyTorch 入门教程&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://pytorchchina.com" rel="nofollow"&gt;http://pytorchchina.com&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;磐创AI 聊天机器人，智能客服：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.panchuangai.com/" rel="nofollow"&gt;http://www.panchuangai.com/&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;磐创教程网站，TensorFlow，Pytorch，Keras：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://panchuang.net/" rel="nofollow"&gt;http://panchuang.net/&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;魔图互联 知识图谱推荐系统：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://motuhulian.com" rel="nofollow"&gt;http://motuhulian.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;由于译者水平有限，如有疏漏，欢迎提交 PR。&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fendouai</author><guid isPermaLink="false">https://github.com/fendouai/PyTorchDocs</guid><pubDate>Wed, 12 Feb 2020 00:17:00 GMT</pubDate></item><item><title>PrefectHQ/prefect #18 in Python, Today</title><link>https://github.com/PrefectHQ/prefect</link><description>&lt;p&gt;&lt;i&gt;The Prefect Core workflow engine&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/856f3511a080480174e3640d7677cb68d11f9566/68747470733a2f2f75706c6f6164732d73736c2e776562666c6f772e636f6d2f3562613434366230653738336532366435613266323338322f3563393432633963613933346563356338383538383239375f7072696d6172792d636f6c6f722d766572746963616c2e737667"&gt;&lt;img src="https://camo.githubusercontent.com/856f3511a080480174e3640d7677cb68d11f9566/68747470733a2f2f75706c6f6164732d73736c2e776562666c6f772e636f6d2f3562613434366230653738336532366435613266323338322f3563393432633963613933346563356338383538383239375f7072696d6172792d636f6c6f722d766572746963616c2e737667" height="350" data-canonical-src="https://uploads-ssl.webflow.com/5ba446b0e783e26d5a2f2382/5c942c9ca934ec5c88588297_primary-color-vertical.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;a href="https://circleci.com/gh/PrefectHQ/prefect/tree/master" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/b3d5ec9a28b072b288be263061973aa1e22577ac/68747470733a2f2f636972636c6563692e636f6d2f67682f5072656665637448512f707265666563742f747265652f6d61737465722e7376673f7374796c653d736869656c6426636972636c652d746f6b656e3d32383638396135356564633363333733343836616161356631316131616633653566633533333434" data-canonical-src="https://circleci.com/gh/PrefectHQ/prefect/tree/master.svg?style=shield&amp;amp;circle-token=28689a55edc3c373486aaa5f11a1af3e5fc53344" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://codecov.io/gh/PrefectHQ/prefect" rel="nofollow"&gt;
  &lt;img src="https://camo.githubusercontent.com/d0b6ec8716a3437ffd23c078cd793e4bd5ed10a2/68747470733a2f2f636f6465636f762e696f2f67682f5072656665637448512f707265666563742f6272616e63682f6d61737465722f67726170682f62616467652e737667" data-canonical-src="https://codecov.io/gh/PrefectHQ/prefect/branch/master/graph/badge.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://github.com/ambv/black"&gt;
    &lt;img src="https://camo.githubusercontent.com/28a51fe3a2c05048d8ca8ecd039d6b1619037326/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://pypi.org/project/prefect/" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/cf34b4cabb70450791a846393655799a4624f4b0/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f707265666563742e7376673f636f6c6f723d253233323742314646266c6162656c3d696e7374616c6c73266c6f676f436f6c6f723d253233344436303645" data-canonical-src="https://img.shields.io/pypi/dm/prefect.svg?color=%2327B1FF&amp;amp;label=installs&amp;amp;logoColor=%234D606E" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://hub.docker.com/r/prefecthq/prefect" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/980fdfd580fd14cf4245b2820c3c382e78f8cf3a/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f7072656665637468712f707265666563742e7376673f636f6c6f723d253233323742314646266c6f676f436f6c6f723d253233344436303645" data-canonical-src="https://img.shields.io/docker/pulls/prefecthq/prefect.svg?color=%2327B1FF&amp;amp;logoColor=%234D606E" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://join.slack.com/t/prefect-community/shared_invite/enQtODQ3MTA2MjI4OTgyLTliYjEyYzljNTc2OThlMDE4YmViYzk3NDU4Y2EzMWZiODM0NmU3NjM0NjIyNWY0MGIxOGQzODMxNDMxYWYyOTE" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/09d839a172d11dba8912f765293eee96b6068fd2/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76312e7376673f6c6162656c3d63686174266d6573736167653d6f6e253230736c61636b26636f6c6f723d323762316666267374796c653d666c6174" data-canonical-src="https://img.shields.io/static/v1.svg?label=chat&amp;amp;message=on%20slack&amp;amp;color=27b1ff&amp;amp;style=flat" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-hello-world-" class="anchor" aria-hidden="true" href="#hello-world-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hello, world! &lt;g-emoji class="g-emoji" alias="wave" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png"&gt;👋&lt;/g-emoji&gt;&lt;/h2&gt;
&lt;p&gt;We've rebuilt data engineering for the data science era.&lt;/p&gt;
&lt;p&gt;Prefect is a new workflow management system, designed for modern infrastructure and powered by the open-source Prefect Core workflow engine. Users organize &lt;code&gt;Tasks&lt;/code&gt; into &lt;code&gt;Flows&lt;/code&gt;, and Prefect takes care of the rest.&lt;/p&gt;
&lt;p&gt;Read the &lt;a href="https://docs.prefect.io" rel="nofollow"&gt;docs&lt;/a&gt;; get the &lt;a href="#installation"&gt;code&lt;/a&gt;; ask us &lt;a href="https://join.slack.com/t/prefect-community/shared_invite/enQtODQ3MTA2MjI4OTgyLTliYjEyYzljNTc2OThlMDE4YmViYzk3NDU4Y2EzMWZiODM0NmU3NjM0NjIyNWY0MGIxOGQzODMxNDMxYWYyOTE" rel="nofollow"&gt;anything&lt;/a&gt;!&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; prefect &lt;span class="pl-k"&gt;import&lt;/span&gt; task, Flow


&lt;span class="pl-en"&gt;@task&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;say_hello&lt;/span&gt;():
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Hello, world!&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)


&lt;span class="pl-k"&gt;with&lt;/span&gt; Flow(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;My First Flow&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; flow:
    say_hello()


flow.run() &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; "Hello, world!"&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-docs" class="anchor" aria-hidden="true" href="#docs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docs&lt;/h2&gt;
&lt;p&gt;Prefect's documentation -- including concepts, tutorials, and a full API reference -- is always available at &lt;a href="https://docs.prefect.io" rel="nofollow"&gt;docs.prefect.io&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Instructions for contributing to documentation can be found in the &lt;a href="https://docs.prefect.io/core/development/documentation.html" rel="nofollow"&gt;development guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-blog" class="anchor" aria-hidden="true" href="#blog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Blog&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://medium.com/the-prefect-blog" rel="nofollow"&gt;The Prefect Blog&lt;/a&gt; for updates and insights from the Prefect team.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;Read about Prefect's &lt;a href="https://docs.prefect.io/core/welcome/community.html" rel="nofollow"&gt;community&lt;/a&gt; or dive in to the &lt;a href="https://docs.prefect.io/core/development/overview.html" rel="nofollow"&gt;development guides&lt;/a&gt; for information about contributions, documentation, code style, and testing.&lt;/p&gt;
&lt;p&gt;Join our &lt;a href="https://join.slack.com/t/prefect-community/shared_invite/enQtODQ3MTA2MjI4OTgyLTliYjEyYzljNTc2OThlMDE4YmViYzk3NDU4Y2EzMWZiODM0NmU3NjM0NjIyNWY0MGIxOGQzODMxNDMxYWYyOTE" rel="nofollow"&gt;Slack&lt;/a&gt; to chat about Prefect, ask questions, and share tips.&lt;/p&gt;
&lt;p&gt;Prefect is committed to ensuring a positive environment. All interactions are governed by our &lt;a href="https://docs.prefect.io/core/welcome/code_of_conduct.html" rel="nofollow"&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-prefect" class="anchor" aria-hidden="true" href="#prefect"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;"...Prefect?"&lt;/h2&gt;
&lt;p&gt;From the Latin &lt;em&gt;praefectus&lt;/em&gt;, meaning "one who is in charge", a prefect is an official who oversees a domain and makes sure that the rules are followed. Similarly, Prefect is responsible for making sure that workflows execute properly.&lt;/p&gt;
&lt;p&gt;It also happens to be the name of a roving researcher for that wholly remarkable book, &lt;em&gt;The Hitchhiker's Guide to the Galaxy&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h3&gt;
&lt;p&gt;Prefect requires Python 3.5.2+.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-install-latest-release" class="anchor" aria-hidden="true" href="#install-latest-release"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install latest release&lt;/h3&gt;
&lt;p&gt;Using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install prefect&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c conda-forge prefect&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or &lt;code&gt;pipenv&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pipenv install --pre prefect
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-install-bleeding-edge" class="anchor" aria-hidden="true" href="#install-bleeding-edge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install bleeding edge&lt;/h3&gt;
&lt;p&gt;Please note that the master branch of Prefect is not guaranteed to be compatible with Prefect Cloud.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/PrefectHQ/prefect.git
pip install ./prefect&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Prefect is licensed under the Apache Software License version 2.0.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>PrefectHQ</author><guid isPermaLink="false">https://github.com/PrefectHQ/prefect</guid><pubDate>Wed, 12 Feb 2020 00:18:00 GMT</pubDate></item><item><title>nodejs/node-gyp #19 in Python, Today</title><link>https://github.com/nodejs/node-gyp</link><description>&lt;p&gt;&lt;i&gt;Node.js native addon build tool&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-node-gyp---nodejs-native-addon-build-tool" class="anchor" aria-hidden="true" href="#node-gyp---nodejs-native-addon-build-tool"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;node-gyp&lt;/code&gt; - Node.js native addon build tool&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.com/nodejs/node-gyp" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8d76e72e967546604d269a2066d89aa64cfcf17a/68747470733a2f2f7472617669732d63692e636f6d2f6e6f64656a732f6e6f64652d6779702e7376673f6272616e63683d6d6173746572" alt="Travis CI" data-canonical-src="https://travis-ci.com/nodejs/node-gyp.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/nodejs/node-gyp/actions?workflow=Python_tests"&gt;&lt;img src="https://github.com/nodejs/node-gyp/workflows/Python_tests/badge.svg" alt="Build Status" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;node-gyp&lt;/code&gt; is a cross-platform command-line tool written in Node.js for
compiling native addon modules for Node.js. It contains a fork of the
&lt;a href="https://gyp.gsrc.io" rel="nofollow"&gt;gyp&lt;/a&gt; project that was previously used by the Chromium
team, extended to support the development of Node.js native addons.&lt;/p&gt;
&lt;p&gt;Note that &lt;code&gt;node-gyp&lt;/code&gt; is &lt;em&gt;not&lt;/em&gt; used to build Node.js itself.&lt;/p&gt;
&lt;p&gt;Multiple target versions of Node.js are supported (i.e. &lt;code&gt;0.8&lt;/code&gt;, ..., &lt;code&gt;4&lt;/code&gt;, &lt;code&gt;5&lt;/code&gt;, &lt;code&gt;6&lt;/code&gt;,
etc.), regardless of what version of Node.js is actually installed on your system
(&lt;code&gt;node-gyp&lt;/code&gt; downloads the necessary development files or headers for the target version).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The same build commands work on any of the supported platforms&lt;/li&gt;
&lt;li&gt;Supports the targeting of different versions of Node.js&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;You can install &lt;code&gt;node-gyp&lt;/code&gt; using &lt;code&gt;npm&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ npm install -g node-gyp&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Depending on your operating system, you will need to install:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-on-unix" class="anchor" aria-hidden="true" href="#on-unix"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;On Unix&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python v2.7, v3.5, v3.6, or v3.7&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A proper C/C++ compiler toolchain, like &lt;a href="https://gcc.gnu.org" rel="nofollow"&gt;GCC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-on-macos" class="anchor" aria-hidden="true" href="#on-macos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;On macOS&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python v2.7, v3.5, v3.6, or v3.7&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.apple.com/xcode/download/" rel="nofollow"&gt;Xcode&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;You also need to install the &lt;code&gt;XCode Command Line Tools&lt;/code&gt; by running &lt;code&gt;xcode-select --install&lt;/code&gt;. Alternatively, if you already have the full Xcode installed, you can find them under the menu &lt;code&gt;Xcode -&amp;gt; Open Developer Tool -&amp;gt; More Developer Tools...&lt;/code&gt;. This step will install &lt;code&gt;clang&lt;/code&gt;, &lt;code&gt;clang++&lt;/code&gt;, and &lt;code&gt;make&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If your Mac has been &lt;em&gt;upgraded&lt;/em&gt; to macOS Catalina (10.15), please read &lt;a href="macOS_Catalina.md"&gt;macOS_Catalina.md&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-on-windows" class="anchor" aria-hidden="true" href="#on-windows"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;On Windows&lt;/h3&gt;
&lt;p&gt;Install the current version of Python from the &lt;a href="https://docs.python.org/3/using/windows.html#the-microsoft-store-package" rel="nofollow"&gt;Microsoft Store package&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-option-1" class="anchor" aria-hidden="true" href="#option-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Option 1&lt;/h4&gt;
&lt;p&gt;Install all the required tools and configurations using Microsoft's &lt;a href="https://github.com/felixrieseberg/windows-build-tools"&gt;windows-build-tools&lt;/a&gt; using &lt;code&gt;npm install --global --production windows-build-tools&lt;/code&gt; from an elevated PowerShell or CMD.exe (run as Administrator).&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-option-2" class="anchor" aria-hidden="true" href="#option-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Option 2&lt;/h4&gt;
&lt;p&gt;Install tools and configuration manually:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install Visual C++ Build Environment: &lt;a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools" rel="nofollow"&gt;Visual Studio Build Tools&lt;/a&gt;
(using "Visual C++ build tools" workload) or &lt;a href="https://visualstudio.microsoft.com/pl/thank-you-downloading-visual-studio/?sku=Community" rel="nofollow"&gt;Visual Studio 2017 Community&lt;/a&gt;
(using the "Desktop development with C++" workload)&lt;/li&gt;
&lt;li&gt;Launch cmd, &lt;code&gt;npm config set msvs_version 2017&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the above steps didn't work for you, please visit &lt;a href="https://github.com/Microsoft/nodejs-guidelines/blob/master/windows-environment.md#compiling-native-addon-modules"&gt;Microsoft's Node.js Guidelines for Windows&lt;/a&gt; for additional tips.&lt;/p&gt;
&lt;p&gt;To target native ARM64 Node.js on Windows 10 on ARM, add the components "Visual C++ compilers and libraries for ARM64" and "Visual C++ ATL for ARM64".&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-configuring-python-dependency" class="anchor" aria-hidden="true" href="#configuring-python-dependency"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuring Python Dependency&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;node-gyp&lt;/code&gt; requires that you have installed a compatible version of Python, one of: v2.7, v3.5, v3.6,
or v3.7. If you have multiple Python versions installed, you can identify which Python
version &lt;code&gt;node-gyp&lt;/code&gt; should use in one of the following ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;by setting the &lt;code&gt;--python&lt;/code&gt; command-line option, e.g.:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ node-gyp &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;command&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; --python /path/to/executable/python&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;If &lt;code&gt;node-gyp&lt;/code&gt; is called by way of &lt;code&gt;npm&lt;/code&gt;, &lt;em&gt;and&lt;/em&gt; you have multiple versions of
Python installed, then you can set &lt;code&gt;npm&lt;/code&gt;'s 'python' config key to the appropriate
value:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ npm config &lt;span class="pl-c1"&gt;set&lt;/span&gt; python /path/to/executable/python&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;
&lt;p&gt;If the &lt;code&gt;PYTHON&lt;/code&gt; environment variable is set to the path of a Python executable,
then that version will be used, if it is a compatible version.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the &lt;code&gt;NODE_GYP_FORCE_PYTHON&lt;/code&gt; environment variable is set to the path of a
Python executable, it will be used instead of any of the other configured or
builtin Python search paths. If it's not a compatible version, no further
searching will be done.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Use&lt;/h2&gt;
&lt;p&gt;To compile your native addon, first go to its root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ &lt;span class="pl-c1"&gt;cd&lt;/span&gt; my_node_addon&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The next step is to generate the appropriate project build files for the current
platform. Use &lt;code&gt;configure&lt;/code&gt; for that:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ node-gyp configure&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Auto-detection fails for Visual C++ Build Tools 2015, so &lt;code&gt;--msvs_version=2015&lt;/code&gt;
needs to be added (not needed when run by npm as configured above):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ node-gyp configure --msvs_version=2015&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;configure&lt;/code&gt; step looks for a &lt;code&gt;binding.gyp&lt;/code&gt; file in the current
directory to process. See below for instructions on creating a &lt;code&gt;binding.gyp&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Now you will have either a &lt;code&gt;Makefile&lt;/code&gt; (on Unix platforms) or a &lt;code&gt;vcxproj&lt;/code&gt; file
(on Windows) in the &lt;code&gt;build/&lt;/code&gt; directory. Next, invoke the &lt;code&gt;build&lt;/code&gt; command:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ node-gyp build&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now you have your compiled &lt;code&gt;.node&lt;/code&gt; bindings file! The compiled bindings end up
in &lt;code&gt;build/Debug/&lt;/code&gt; or &lt;code&gt;build/Release/&lt;/code&gt;, depending on the build mode. At this point,
you can require the &lt;code&gt;.node&lt;/code&gt; file with Node.js and run your tests!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; To create a &lt;em&gt;Debug&lt;/em&gt; build of the bindings file, pass the &lt;code&gt;--debug&lt;/code&gt; (or
&lt;code&gt;-d&lt;/code&gt;) switch when running either the &lt;code&gt;configure&lt;/code&gt;, &lt;code&gt;build&lt;/code&gt; or &lt;code&gt;rebuild&lt;/code&gt; commands.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-bindinggyp-file" class="anchor" aria-hidden="true" href="#the-bindinggyp-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The &lt;code&gt;binding.gyp&lt;/code&gt; file&lt;/h2&gt;
&lt;p&gt;A &lt;code&gt;binding.gyp&lt;/code&gt; file describes the configuration to build your module, in a
JSON-like format. This file gets placed in the root of your package, alongside
&lt;code&gt;package.json&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A barebones &lt;code&gt;gyp&lt;/code&gt; file appropriate for building a Node.js addon could look like:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;{
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;targets&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [
    {
      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;target_name&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;binding&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sources&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [ &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;src/binding.cc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; ]
    }
  ]
}&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-further-reading" class="anchor" aria-hidden="true" href="#further-reading"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Further reading&lt;/h2&gt;
&lt;p&gt;Some additional resources for Node.js native addons and writing &lt;code&gt;gyp&lt;/code&gt; configuration files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nodeschool.io/#goingnative" rel="nofollow"&gt;"Going Native" a nodeschool.io tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nodejs/node/tree/master/test/addons/hello-world"&gt;"Hello World" node addon example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gyp.gsrc.io/docs/UserDocumentation.md" rel="nofollow"&gt;gyp user documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gyp.gsrc.io/docs/InputFormatReference.md" rel="nofollow"&gt;gyp input format reference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nodejs/node-gyp/wiki/%22binding.gyp%22-files-out-in-the-wild"&gt;&lt;em&gt;"binding.gyp" files out in the wild&lt;/em&gt; wiki page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-commands" class="anchor" aria-hidden="true" href="#commands"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Commands&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;node-gyp&lt;/code&gt; responds to the following commands:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;strong&gt;Command&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;help&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Shows the help dialog&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;build&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Invokes &lt;code&gt;make&lt;/code&gt;/&lt;code&gt;msbuild.exe&lt;/code&gt; and builds the native addon&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;clean&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Removes the &lt;code&gt;build&lt;/code&gt; directory if it exists&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;configure&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Generates project build files for the current platform&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;rebuild&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Runs &lt;code&gt;clean&lt;/code&gt;, &lt;code&gt;configure&lt;/code&gt; and &lt;code&gt;build&lt;/code&gt; all in a row&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;install&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Installs Node.js header files for the given version&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;list&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Lists the currently installed Node.js header versions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;remove&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Removes the Node.js header files for the given version&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-command-options" class="anchor" aria-hidden="true" href="#command-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Command Options&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;node-gyp&lt;/code&gt; accepts the following command options:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;strong&gt;Command&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;-j n&lt;/code&gt;, &lt;code&gt;--jobs n&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Run &lt;code&gt;make&lt;/code&gt; in parallel. The value &lt;code&gt;max&lt;/code&gt; will use all available CPU cores&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--target=v6.2.1&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Node.js version to build for (default is &lt;code&gt;process.version&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--silly&lt;/code&gt;, &lt;code&gt;--loglevel=silly&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Log all progress to console&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--verbose&lt;/code&gt;, &lt;code&gt;--loglevel=verbose&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Log most progress to console&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--silent&lt;/code&gt;, &lt;code&gt;--loglevel=silent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Don't log anything to console&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;debug&lt;/code&gt;, &lt;code&gt;--debug&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Make Debug build (default is &lt;code&gt;Release&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--release&lt;/code&gt;, &lt;code&gt;--no-debug&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Make Release build&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;-C $dir&lt;/code&gt;, &lt;code&gt;--directory=$dir&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Run command in different directory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--make=$make&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Override &lt;code&gt;make&lt;/code&gt; command (e.g. &lt;code&gt;gmake&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--thin=yes&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Enable thin static libraries&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--arch=$arch&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Set target architecture (e.g. ia32)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--tarball=$path&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Get headers from a local tarball&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--devdir=$path&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;SDK download directory (default is OS cache directory)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--ensure&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Don't reinstall headers if already present&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--dist-url=$url&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Download header tarball from custom URL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--proxy=$url&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Set HTTP(S) proxy for downloading header tarball&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--noproxy=$urls&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Set urls to ignore proxies when downloading header tarball&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--cafile=$cafile&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Override default CA chain (to download tarball)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--nodedir=$path&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Set the path to the node source code&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--python=$path&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Set path to the Python binary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--msvs_version=$version&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Set Visual Studio version (Windows only)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;code&gt;--solution=$solution&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Set Visual Studio Solution version (Windows only)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-configuration" class="anchor" aria-hidden="true" href="#configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-environment-variables" class="anchor" aria-hidden="true" href="#environment-variables"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Environment variables&lt;/h3&gt;
&lt;p&gt;Use the form &lt;code&gt;npm_config_OPTION_NAME&lt;/code&gt; for any of the command options listed
above (dashes in option names should be replaced by underscores).&lt;/p&gt;
&lt;p&gt;For example, to set &lt;code&gt;devdir&lt;/code&gt; equal to &lt;code&gt;/tmp/.gyp&lt;/code&gt;, you would:&lt;/p&gt;
&lt;p&gt;Run this on Unix:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ &lt;span class="pl-k"&gt;export&lt;/span&gt; npm_config_devdir=/tmp/.gyp&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or this on Windows:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&amp;gt; &lt;span class="pl-s1"&gt;&lt;span class="pl-c1"&gt;set&lt;/span&gt; npm_config_devdir=c:&lt;span class="pl-cce"&gt;\t&lt;/span&gt;emp&lt;span class="pl-cce"&gt;\.&lt;/span&gt;gyp&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-npm-configuration" class="anchor" aria-hidden="true" href="#npm-configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;npm&lt;/code&gt; configuration&lt;/h3&gt;
&lt;p&gt;Use the form &lt;code&gt;OPTION_NAME&lt;/code&gt; for any of the command options listed above.&lt;/p&gt;
&lt;p&gt;For example, to set &lt;code&gt;devdir&lt;/code&gt; equal to &lt;code&gt;/tmp/.gyp&lt;/code&gt;, you would run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ npm config &lt;span class="pl-c1"&gt;set&lt;/span&gt; [--global] devdir /tmp/.gyp&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Configuration set via &lt;code&gt;npm&lt;/code&gt; will only be used when &lt;code&gt;node-gyp&lt;/code&gt;
is run via &lt;code&gt;npm&lt;/code&gt;, not when &lt;code&gt;node-gyp&lt;/code&gt; is run directly.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;node-gyp&lt;/code&gt; is available under the MIT license. See the &lt;a href="LICENSE"&gt;LICENSE
file&lt;/a&gt; for details.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>nodejs</author><guid isPermaLink="false">https://github.com/nodejs/node-gyp</guid><pubDate>Wed, 12 Feb 2020 00:19:00 GMT</pubDate></item><item><title>satoshiiizuka/siggraphasia2019_remastering #20 in Python, Today</title><link>https://github.com/satoshiiizuka/siggraphasia2019_remastering</link><description>&lt;p&gt;&lt;i&gt;Code for the paper "DeepRemaster: Temporal Source-Reference Attention Networks for Comprehensive Video Enhancement". http://iizuka.cs.tsukuba.ac.jp/projects/remastering/&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deepremaster-temporal-source-reference-attention-networks-for-comprehensive-video-enhancement" class="anchor" aria-hidden="true" href="#deepremaster-temporal-source-reference-attention-networks-for-comprehensive-video-enhancement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://hi.cs.waseda.ac.jp/~iizuka/projects/remastering/" rel="nofollow"&gt;DeepRemaster: Temporal Source-Reference Attention Networks for Comprehensive Video Enhancement&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://iizuka.cs.tsukuba.ac.jp/index_eng.html" rel="nofollow"&gt;Satoshi Iizuka&lt;/a&gt; and &lt;a href="https://esslab.jp/~ess/" rel="nofollow"&gt;Edgar Simo-Serra&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="teaser.jpg"&gt;&lt;img src="teaser.jpg" alt="Teaser Image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h2&gt;
&lt;p&gt;This code provides an implementation of the research paper:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  "DeepRemaster: Temporal Source-Reference Attention Networks for Comprehensive Video Enhancement"
  Satoshi Iizuka and Edgar Simo-Serra
  ACM Transaction on Graphics (Proc. of SIGGRAPH ASIA 2019), 2019
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We learn to semi-automatically remaster vintage videos with a deep convolutional network.
Our network is based on temporal convolutions with source-reference attention mechanisms
trained on videos with example-based deterioration simulation, which allows us to automatically
remove film noises, improve contrast and sharpness, and add color based on reference color frames created manually.
See our &lt;a href="http://iizuka.cs.tsukuba.ac.jp/projects/remastering/" rel="nofollow"&gt;project page&lt;/a&gt; for more detailed information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;  Copyright (C) &amp;lt;2019&amp;gt; &amp;lt;Satoshi Iizuka and Edgar Simo-Serra&amp;gt;

  This work is licensed under the Creative Commons
  Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
  of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
  send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

  Satoshi Iizuka, University of Tsukuba
  iizuka@cs.tsukuba.ac.jp, http://iizuka.cs.tsukuba.ac.jp/index_eng.html
  
  Edgar Simo-Serra, Waseda University
  ess@waseda.jp, https://esslab.jp/~ess/
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/" rel="nofollow"&gt;PyTorch (0.4.1+)&lt;/a&gt; &lt;a href="https://pytorch.org/docs/master/torchvision/" rel="nofollow"&gt;torchvision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ffmpeg.org/" rel="nofollow"&gt;FFmpeg (requires to be configured with --enable-libx264)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://opencv.org/" rel="nofollow"&gt;opencv (3.4.1+)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scikit-image.org/" rel="nofollow"&gt;scikit-image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tqdm/tqdm"&gt;tqdm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For information on how to install PyTorch, please refer to the &lt;a href="https://pytorch.org/" rel="nofollow"&gt;PyTorch website&lt;/a&gt;. FFmpeg should be installed with libx264 support, which can be installed in Anaconda by using &lt;code&gt;conda install x264 ffmpeg -c conda-forge&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;First, download the model by running the download script:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bash download_model.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Basic usage is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python remaster.py --input &amp;lt;input_video&amp;gt; --reference_dir &amp;lt;directory_of_reference_images&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The input video will be automatically restored and colorized based on the reference color frames using the model. If you want to perform restoration only, use &lt;code&gt;--disable_colorization&lt;/code&gt; option.&lt;/p&gt;
&lt;p&gt;Other options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--gpu&lt;/code&gt;: Use GPU for the computation (&lt;strong&gt;recommended&lt;/strong&gt;). Defaults to false.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--disable_colorization&lt;/code&gt;: Disable colorization and only perform restoration with enhancement. Defaults to false.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--mindim&lt;/code&gt;: Minimum edge dimension of the input video. Defaults to 320.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python remaster.py --input example/a-bomb_blast_effects_part.mp4 --reference_dir example/references --gpu
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-preparing-reference-images" class="anchor" aria-hidden="true" href="#preparing-reference-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preparing Reference Images&lt;/h3&gt;
&lt;p&gt;To prepare reference color images for your own video, it is recommended to first extract reference frames from the video using a scene detection technique such as &lt;a href="https://pyscenedetect.readthedocs.io/en/latest/" rel="nofollow"&gt;pyscenedetect&lt;/a&gt;. Afterwards, colorize them by leveraging image editing software or recent interactive colorization techniques such as the &lt;a href="https://github.com/junyanz/interactive-deep-colorization/"&gt;Interactive Deep Colorization [Zhang et al. 2017]&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-notes" class="anchor" aria-hidden="true" href="#notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This is developed on a Linux machine running Ubuntu 18.04 during late 2018.&lt;/li&gt;
&lt;li&gt;We recommend using GPU with 4GB+ memory for fast computation.&lt;/li&gt;
&lt;li&gt;Provided model and sample code are under a non-commercial creative commons license.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h2&gt;
&lt;p&gt;If you use this code please cite:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@Article{IizukaSIGGRAPHASIA2019,
  author = {Satoshi Iizuka and Edgar Simo-Serra},
  title = {{DeepRemaster: Temporal Source-Reference Attention Networks for Comprehensive Video Enhancement}},
  journal = "ACM Transactions on Graphics (Proc. of SIGGRAPH ASIA)",
  year = 2019,
  volume = 38,
  number = 6,
  pages = 1--13,
  articleno = 176,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>satoshiiizuka</author><guid isPermaLink="false">https://github.com/satoshiiizuka/siggraphasia2019_remastering</guid><pubDate>Wed, 12 Feb 2020 00:20:00 GMT</pubDate></item><item><title>clovaai/CRAFT-pytorch #21 in Python, Today</title><link>https://github.com/clovaai/CRAFT-pytorch</link><description>&lt;p&gt;&lt;i&gt;Official implementation of Character Region Awareness for Text Detection (CRAFT)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-craft-character-region-awareness-for-text-detection" class="anchor" aria-hidden="true" href="#craft-character-region-awareness-for-text-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CRAFT: Character-Region Awareness For Text detection&lt;/h2&gt;
&lt;p&gt;Official Pytorch implementation of CRAFT text detector | &lt;a href="https://arxiv.org/abs/1904.01941" rel="nofollow"&gt;Paper&lt;/a&gt; | &lt;a href="https://drive.google.com/open?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ" rel="nofollow"&gt;Pretrained Model&lt;/a&gt; | &lt;a href="https://youtu.be/HI8MzpY8KMI" rel="nofollow"&gt;Supplementary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="mailto:youngmin.baek@navercorp.com"&gt;Youngmin Baek&lt;/a&gt;, Bado Lee, Dongyoon Han, Sangdoo Yun, Hwalsuk Lee.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Clova AI Research, NAVER Corp.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-sample-results" class="anchor" aria-hidden="true" href="#sample-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample Results&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h3&gt;
&lt;p&gt;PyTorch implementation for CRAFT text detector that effectively detect text area by exploring each character region and affinity between characters. The bounding box of texts are obtained by simply finding minimum bounding rectangles on binary map after thresholding character region and affinity scores.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./figures/craft_example.gif"&gt;&lt;img width="1000" alt="teaser" src="./figures/craft_example.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;13 Jun, 2019&lt;/strong&gt;: Initial update
&lt;strong&gt;20 Jul, 2019&lt;/strong&gt;: Added post-processing for polygon result
&lt;strong&gt;28 Sep, 2019&lt;/strong&gt;: Added the trained model on IC15 and the link refiner&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting started&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-install-dependencies" class="anchor" aria-hidden="true" href="#install-dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install dependencies&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;PyTorch&amp;gt;=0.4.1&lt;/li&gt;
&lt;li&gt;torchvision&amp;gt;=0.2.1&lt;/li&gt;
&lt;li&gt;opencv-python&amp;gt;=3.4.2&lt;/li&gt;
&lt;li&gt;check requiremtns.txt&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h3&gt;
&lt;p&gt;The code for training is not included in this repository, and we cannot release the full training code for IP reason.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-test-instruction-using-pretrained-model" class="anchor" aria-hidden="true" href="#test-instruction-using-pretrained-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Test instruction using pretrained model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Download the trained models&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;em&gt;Model name&lt;/em&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;em&gt;Used datasets&lt;/em&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;em&gt;Languages&lt;/em&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;em&gt;Purpose&lt;/em&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;em&gt;Model Link&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;General&lt;/td&gt;
&lt;td align="left"&gt;SynthText, IC13, IC17&lt;/td&gt;
&lt;td align="left"&gt;Eng + MLT&lt;/td&gt;
&lt;td align="left"&gt;For general purpose&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://drive.google.com/open?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ" rel="nofollow"&gt;Click&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;IC15&lt;/td&gt;
&lt;td align="left"&gt;SynthText, IC15&lt;/td&gt;
&lt;td align="left"&gt;Eng&lt;/td&gt;
&lt;td align="left"&gt;For IC15 only&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://drive.google.com/open?id=1i2R7UIUqmkUtF0jv_3MXTqmQ_9wuAnLf" rel="nofollow"&gt;Click&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;LinkRefiner&lt;/td&gt;
&lt;td align="left"&gt;CTW1500&lt;/td&gt;
&lt;td align="left"&gt;-&lt;/td&gt;
&lt;td align="left"&gt;Used with the General Model&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://drive.google.com/open?id=1XSaFwBkOaFOdtk4Ane3DFyJGPRw6v5bO" rel="nofollow"&gt;Click&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Run with pretrained model&lt;/li&gt;
&lt;/ul&gt;
&lt;pre lang="(with"&gt;&lt;code&gt;python test.py --trained_model=[weightfile] --test_folder=[folder path to test images]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result image and socre maps will be saved to &lt;code&gt;./result&lt;/code&gt; by default.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-arguments" class="anchor" aria-hidden="true" href="#arguments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arguments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--trained_model&lt;/code&gt;: pretrained model&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--text_threshold&lt;/code&gt;: text confidence threshold&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--low_text&lt;/code&gt;: text low-bound score&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--link_threshold&lt;/code&gt;: link confidence threshold&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--cuda&lt;/code&gt;: use cuda for inference (default:True)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--canvas_size&lt;/code&gt;: max image size for inference&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--mag_ratio&lt;/code&gt;: image magnification ratio&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--poly&lt;/code&gt;: enable polygon type result&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--show_time&lt;/code&gt;: show processing time&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--test_folder&lt;/code&gt;: folder path to input images&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--refine&lt;/code&gt;: use link refiner for sentense-level dataset&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--refiner_model&lt;/code&gt;: pretrained refiner model&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;WebDemo : &lt;a href="https://demo.ocr.clova.ai/" rel="nofollow"&gt;https://demo.ocr.clova.ai/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Repo of recognition : &lt;a href="https://github.com/clovaai/deep-text-recognition-benchmark"&gt;https://github.com/clovaai/deep-text-recognition-benchmark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{baek2019character,
  title={Character Region Awareness for Text Detection},
  author={Baek, Youngmin and Lee, Bado and Han, Dongyoon and Yun, Sangdoo and Lee, Hwalsuk},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9365--9374},
  year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Copyright (c) 2019-present NAVER Corp.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>clovaai</author><guid isPermaLink="false">https://github.com/clovaai/CRAFT-pytorch</guid><pubDate>Wed, 12 Feb 2020 00:21:00 GMT</pubDate></item><item><title>programthink/zhao #22 in Python, Today</title><link>https://github.com/programthink/zhao</link><description>&lt;p&gt;&lt;i&gt;【编程随想】整理的《太子党关系网络》，专门揭露赵国的权贵&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="wiki" data-path="README.wiki"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;
&lt;p&gt;&lt;/p&gt;&lt;table id="user-content-toc" summary="Contents"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;div id="user-content-toctitle"&gt;&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="#"&gt;俺整理的《太子党关系网络》&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="#-2"&gt;简介&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-3"&gt;下载说明&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-4"&gt;多人协作说明&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-5"&gt;数据格式说明&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-6"&gt;目录说明&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="#data_"&gt;data 目录&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#bin_"&gt;bin 目录&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#download_"&gt;download 目录&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-7"&gt;编译脚本使用说明&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="#-8"&gt;脚本的命令行参数&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-9"&gt;依赖的软件&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="#-10"&gt;致“反对此项目的墙内程序员”&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;/p&gt;&lt;h1&gt;&lt;a id="user-content-俺整理的太子党关系网络" class="anchor" aria-hidden="true" href="#俺整理的太子党关系网络"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name=""&gt;&lt;/a&gt;&lt;span id=""&gt;俺整理的《太子党关系网络》&lt;/span&gt;&lt;/h1&gt;




&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--2"&gt;&lt;/a&gt;&lt;span id="user-content--2"&gt;简介&lt;/span&gt;&lt;/h2&gt;



&lt;p&gt;此项目创建于2016年2月，专门用来揭露天朝的权贵（也就是传说中的“赵家人”）。
&lt;/p&gt;
&lt;p&gt;俺把这几年收集整理的数据开源到 GitHub，便于多人协作——大伙儿群策群力，一起来曝光权贵家族。
&lt;/p&gt;
&lt;p&gt;初次上传的数据包括：700多个数据文件（ &lt;b&gt;对应700多人，130多个家族&lt;/b&gt; ），另有200多张图片（人物头像）。随着俺不断完善，数据会越来越多。
&lt;/p&gt;
&lt;p&gt;对这个项目，俺会【持续更新】。比如朝廷每次换届的时候，俺都会补充新的素材。
&lt;/p&gt;
&lt;p&gt;为了确保数据的可信度，俺主要参考“维基百科”以及一些国际权威媒体的报道（比如《纽约时报》、《华尔街日版》、《金融时报》等等）。
&lt;/p&gt;
&lt;p&gt;另外，对于某些客观事实（比如：生卒年月、简历、亲戚关系），俺也参考了天朝政府的官方网站，以及墙内的“百度百科”。
&lt;/p&gt;


&lt;h2&gt;&lt;a id="user-content-下载说明" class="anchor" aria-hidden="true" href="#下载说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--3"&gt;&lt;/a&gt;&lt;span id="user-content--3"&gt;下载说明&lt;/span&gt;&lt;/h2&gt;



&lt;p&gt;GitHub 提供了“下载整个项目”的功能，但是会比较大。
&lt;/p&gt;
&lt;p&gt;如果你仅仅想看《太子党关系网络》这份文档，只需在首页上方点击进入 &lt;b&gt;download&lt;/b&gt; 这个目录。
&lt;/p&gt;
&lt;p&gt;该目录下有 &lt;b&gt;pdf&lt;/b&gt; 和 &lt;b&gt;jpg&lt;/b&gt; 两个子目录，分别存放对应的 &lt;b&gt;【文件类型】&lt;/b&gt; 。你想要看哪一种文件格式，就进入哪个子目录里面。
&lt;/p&gt;
&lt;p&gt;进入【文件类型】的子目录之后，会看到一个文件列表（目前有13个文件）。先点击你想要的某个文件，会进入该文件的页面。
&lt;/p&gt;
&lt;p&gt;然后在【右上方】你会看到一个 &lt;b&gt;Raw 按钮&lt;/b&gt; ，在这个按钮上点【右键】，在【右键菜单】里面选“保存”或“另存为”，就可以把这个文件下载到你本机。
&lt;/p&gt;


&lt;h2&gt;&lt;a id="user-content-多人协作说明" class="anchor" aria-hidden="true" href="#多人协作说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--4"&gt;&lt;/a&gt;&lt;span id="user-content--4"&gt;多人协作说明&lt;/span&gt;&lt;/h2&gt;



&lt;p&gt;俺非常希望有更多的网友参与该项目，大伙儿一起来完善天朝权贵家族的资料。
&lt;/p&gt;
&lt;p&gt;想要参与的同学，可以通过如下方式：
&lt;/p&gt;






&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;到&lt;a href="https://program-think.blogspot.com/" rel="nofollow"&gt;俺博客&lt;/a&gt;留言进行反馈，补充信息或反馈错误。&lt;/li&gt;&lt;li&gt;在&lt;a href="https://github.com/programthink/zhao/issues"&gt;本项目发一个 issue&lt;/a&gt;，补充信息或反馈错误。&lt;/li&gt;&lt;li&gt;Fork 该项目，进行修改，然后向俺发一个 Pull Request&lt;/li&gt;&lt;/ul&gt;
（后面两种方式，你需要有 GitHub 的帐号）
&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;&lt;a id="user-content-数据格式说明" class="anchor" aria-hidden="true" href="#数据格式说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--5"&gt;&lt;/a&gt;&lt;span id="user-content--5"&gt;数据格式说明&lt;/span&gt;&lt;/h2&gt;



&lt;p&gt;本项目的数据文件，全部采用&lt;a href="https://zh.wikipedia.org/wiki/YAML" rel="nofollow"&gt;YAML 格式&lt;/a&gt;。这种格式非常简洁明了，有利于完全不懂技术的网友参与编辑。
&lt;/p&gt;
&lt;p&gt;而且俺在每一个 YAML 格式的文件中都写了详细的注释，便于其他网友修改。
&lt;/p&gt;


&lt;h2&gt;&lt;a id="user-content-目录说明" class="anchor" aria-hidden="true" href="#目录说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--6"&gt;&lt;/a&gt;&lt;span id="user-content--6"&gt;目录说明&lt;/span&gt;&lt;/h2&gt;




&lt;h3&gt;&lt;a id="user-content-data-目录" class="anchor" aria-hidden="true" href="#data-目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-data_"&gt;&lt;/a&gt;&lt;span id="user-content-data_"&gt;data 目录&lt;/span&gt;&lt;/h3&gt;



&lt;p&gt;data 目录用来保存数据文件，该目录下另有如下三个子目录：
&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;person&lt;/li&gt;&lt;/ul&gt;
这个目录存放个人的资料，每个人一个目录，目录名就是人名。对于偶尔有同名的情况，在目录名末尾追加数字序号来区分。
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;每个目录下都有一个 brief.yaml 文件，包含此人的简介。
&lt;/p&gt;
&lt;p&gt;有些目录下还有一个 portrait.png 文件，对应此人的头像。
&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;company&lt;/li&gt;&lt;/ul&gt;
这个目录存放与太子党有关的公司或组织机构。目录结构与 person 类似。
&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;family&lt;/li&gt;&lt;/ul&gt;
这个目录存放家族关系的信息。每个家族是一个 yaml 格式的文件。
&lt;p&gt;&lt;/p&gt;

&lt;h3&gt;&lt;a id="user-content-bin-目录" class="anchor" aria-hidden="true" href="#bin-目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-bin_"&gt;&lt;/a&gt;&lt;span id="user-content-bin_"&gt;bin 目录&lt;/span&gt;&lt;/h3&gt;



&lt;p&gt;该目录存放编译脚本。该脚本的使用参见下面的章节。
&lt;/p&gt;

&lt;h3&gt;&lt;a id="user-content-download-目录" class="anchor" aria-hidden="true" href="#download-目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-download_"&gt;&lt;/a&gt;&lt;span id="user-content-download_"&gt;download 目录&lt;/span&gt;&lt;/h3&gt;



&lt;p&gt;该目录存放制作好的文件，目前先提供 jpg 和 pdf 两种格式。
&lt;/p&gt;
&lt;p&gt;如果你需要其它格式，可以用 bin 目录下的编译脚本自行搞定（编译脚本的使用，参见下面的章节）。
&lt;/p&gt;


&lt;h2&gt;&lt;a id="user-content-编译脚本使用说明" class="anchor" aria-hidden="true" href="#编译脚本使用说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--7"&gt;&lt;/a&gt;&lt;span id="user-content--7"&gt;编译脚本使用说明&lt;/span&gt;&lt;/h2&gt;



&lt;p&gt;（俺是在 Linux 上编写该脚本，尚未在 Windows 上进行测试）
&lt;/p&gt;
&lt;p&gt;如果你在 Windows 上使用碰到问题，可以到&lt;a href="https://program-think.blogspot.com/" rel="nofollow"&gt;俺博客&lt;/a&gt;留言进行反馈。也可以在&lt;a href="https://github.com/programthink/zhao/issues"&gt;本项目发一个 issue&lt;/a&gt;。
&lt;/p&gt;

&lt;h3&gt;&lt;a id="user-content-脚本的命令行参数" class="anchor" aria-hidden="true" href="#脚本的命令行参数"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--8"&gt;&lt;/a&gt;&lt;span id="user-content--8"&gt;脚本的命令行参数&lt;/span&gt;&lt;/h3&gt;



&lt;p&gt;俺使用 python 作为编译脚本，该脚本位于 bin 目录下。
&lt;/p&gt;
&lt;p&gt;通过该脚本可以把原始数据生成为 dot 语言的脚本。然后再调用 Graphviz 把 dot 脚本生成各种格式（比如：pdf、jpeg）。
&lt;/p&gt;
&lt;p&gt;要使用该脚本，先在命令行模式下进入 bin 目录，然后运行如下命令：
&lt;/p&gt;
&lt;p&gt;（生成 pdf 格式的示例）
&lt;/p&gt;
&lt;p&gt;&lt;b&gt;python make.py pdf&lt;/b&gt;
&lt;/p&gt;
&lt;p&gt;（生成 jpg 格式的示例）
&lt;/p&gt;
&lt;p&gt;&lt;b&gt;python make.py jpg&lt;/b&gt;
&lt;/p&gt;

&lt;h3&gt;&lt;a id="user-content-依赖的软件" class="anchor" aria-hidden="true" href="#依赖的软件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--9"&gt;&lt;/a&gt;&lt;span id="user-content--9"&gt;依赖的软件&lt;/span&gt;&lt;/h3&gt;



&lt;p&gt;要使用上述脚本，你需要事先安装相关的软件（如下）
&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Python（2 或 3）&lt;/li&gt;&lt;/ul&gt;
因为俺用的是 Python 脚本，所以你需要先安装 python 软件。
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;目前 Python 有两种大版本——python2 和 python3——俺的编译脚本 &lt;b&gt;【同时兼容】&lt;/b&gt; 这两种 Python 的大版本。
&lt;/p&gt;
&lt;p&gt;对于 Python 的小版本，俺本人在 &lt;b&gt;2.7&lt;/b&gt; 和 &lt;b&gt;3.5&lt;/b&gt; 上测试通过。2.6 和 3.4 估计也可以。
&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;PyYAML&lt;/li&gt;&lt;/ul&gt;
这是一个基于 python 开发的软件包，专门用来处理 YAML 格式的文件。
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;你需要在你的 python 环境中安装该软件包。其官方链接如下：
&lt;/p&gt;
&lt;p&gt;&lt;a href="http://pyyaml.org/wiki/PyYAML" rel="nofollow"&gt;PyYAML 的官网的 wiki&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pypi.python.org/pypi/PyYAML" rel="nofollow"&gt;Python 官网的 PYPI&lt;/a&gt;
&lt;/p&gt;


&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Graphviz&lt;/li&gt;&lt;/ul&gt;
这个软件是用来生成【关系图】的。关于该这个软件，俺已经写了一篇扫盲教程：
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;《&lt;a href="https://program-think.blogspot.com/2016/02/opensource-review-graphviz.html" rel="nofollow"&gt;开源项目：【自动】绘图工具Graphviz——《太子党关系网络》就是用它制作&lt;/a&gt;》
&lt;/p&gt;


&lt;h2&gt;&lt;a id="user-content-致反对此项目的墙内程序员" class="anchor" aria-hidden="true" href="#致反对此项目的墙内程序员"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content--10"&gt;&lt;/a&gt;&lt;span id="user-content--10"&gt;致“反对此项目的墙内程序员”&lt;/span&gt;&lt;/h2&gt;



&lt;p&gt;本项目上线第二天，就收获 363 个 star 兼 88 个 fork，甚至还挤进 GitHub 的“当日 Trending”——俺很荣幸，也很高兴有这么多人给俺捧场。
&lt;/p&gt;
&lt;p&gt;但是在&lt;a href="https://github.com/programthink/zhao/issues"&gt;本项目的 issue 列表&lt;/a&gt;中也看到好几个反对此项目的程序员（应该都来自墙内），他们担心这个项目导致 GitHub 被 GFW 封杀。
&lt;/p&gt;
&lt;p&gt;这几年来，类似的言论俺已经看了不少。就好比强盗拿刀杀人，围观者不但没有谴责强盗，反而去谴责卖刀的店家——这就是传说中的“斯德哥尔摩综合症”。
&lt;/p&gt;
&lt;p&gt;有兴趣的同学，可以看俺之前的博文——《&lt;a href="https://program-think.blogspot.com/2012/06/stockholm-syndrome.html" rel="nofollow"&gt;天朝民众的心理分析：斯德哥尔摩综合症&lt;/a&gt;》&lt;/p&gt;&lt;/article&gt;&lt;/div&gt;</description><author>programthink</author><guid isPermaLink="false">https://github.com/programthink/zhao</guid><pubDate>Wed, 12 Feb 2020 00:22:00 GMT</pubDate></item><item><title>tensorflow/models #23 in Python, Today</title><link>https://github.com/tensorflow/models</link><description>&lt;p&gt;&lt;i&gt;Models and examples built with TensorFlow&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-models" class="anchor" aria-hidden="true" href="#tensorflow-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Models&lt;/h1&gt;
&lt;p&gt;This repository contains a number of different models implemented in &lt;a href="https://www.tensorflow.org" rel="nofollow"&gt;TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;The &lt;a href="official"&gt;official models&lt;/a&gt; are a collection of example models that use TensorFlow's high-level APIs. They are intended to be well-maintained, tested, and kept up to date with the latest stable TensorFlow API. They should also be reasonably optimized for fast performance while still being easy to read. We especially recommend newer TensorFlow users to start here.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/tensorflow/models/tree/master/research"&gt;research models&lt;/a&gt; are a large collection of models implemented in TensorFlow by researchers. They are not officially supported or available in release branches; it is up to the individual researchers to maintain the models and/or provide support on issues and pull requests.&lt;/p&gt;
&lt;p&gt;The &lt;a href="samples"&gt;samples folder&lt;/a&gt; contains code snippets and smaller models that demonstrate features of TensorFlow, including code presented in various blog posts.&lt;/p&gt;
&lt;p&gt;The &lt;a href="tutorials"&gt;tutorials folder&lt;/a&gt; is a collection of models described in the &lt;a href="https://www.tensorflow.org/tutorials/" rel="nofollow"&gt;TensorFlow tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-guidelines" class="anchor" aria-hidden="true" href="#contribution-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution guidelines&lt;/h2&gt;
&lt;p&gt;If you want to contribute to models, be sure to review the &lt;a href="CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensorflow</author><guid isPermaLink="false">https://github.com/tensorflow/models</guid><pubDate>Wed, 12 Feb 2020 00:23:00 GMT</pubDate></item><item><title>keras-team/keras #24 in Python, Today</title><link>https://github.com/keras-team/keras</link><description>&lt;p&gt;&lt;i&gt;Deep Learning for humans&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-keras-deep-learning-for-humans" class="anchor" aria-hidden="true" href="#keras-deep-learning-for-humans"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Keras: Deep Learning for humans&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0d08dc4f9466d347e8d28a951ea51e3430c6f92c/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6b657261732e696f2f696d672f6b657261732d6c6f676f2d323031382d6c617267652d313230302e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/0d08dc4f9466d347e8d28a951ea51e3430c6f92c/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6b657261732e696f2f696d672f6b657261732d6c6f676f2d323031382d6c617267652d313230302e706e67" alt="Keras logo" data-canonical-src="https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/keras-team/keras" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f8208f284c67046b0531446a3498b3e54c1a31e5/68747470733a2f2f7472617669732d63692e6f72672f6b657261732d7465616d2f6b657261732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/keras-team/keras.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/keras-team/keras/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/72b8fa08522b87c996b58d36be5132a346d434c5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6173686170652f6170697374617475732e7376673f6d61784167653d32353932303030" alt="license" data-canonical-src="https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-you-have-just-found-keras" class="anchor" aria-hidden="true" href="#you-have-just-found-keras"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;You have just found Keras.&lt;/h2&gt;
&lt;p&gt;Keras is a high-level neural networks API, written in Python and capable of running on top of &lt;a href="https://github.com/tensorflow/tensorflow"&gt;TensorFlow&lt;/a&gt;, &lt;a href="https://github.com/Microsoft/cntk"&gt;CNTK&lt;/a&gt;, or &lt;a href="https://github.com/Theano/Theano"&gt;Theano&lt;/a&gt;. It was developed with a focus on enabling fast experimentation. &lt;em&gt;Being able to go from idea to result with the least possible delay is key to doing good research.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Use Keras if you need a deep learning library that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).&lt;/li&gt;
&lt;li&gt;Supports both convolutional networks and recurrent networks, as well as combinations of the two.&lt;/li&gt;
&lt;li&gt;Runs seamlessly on CPU and GPU.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Read the documentation at &lt;a href="https://keras.io" rel="nofollow"&gt;Keras.io&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Keras is compatible with: &lt;strong&gt;Python 2.7-3.6&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-multi-backend-keras-and-tfkeras" class="anchor" aria-hidden="true" href="#multi-backend-keras-and-tfkeras"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-backend Keras and tf.keras:&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;At this time, we recommend that Keras users who use multi-backend Keras with the TensorFlow backend switch to &lt;code&gt;tf.keras&lt;/code&gt; in TensorFlow 2.0&lt;/strong&gt;. &lt;code&gt;tf.keras&lt;/code&gt; is better maintained and has better integration with TensorFlow features (eager execution, distribution support and other).&lt;/p&gt;
&lt;p&gt;Keras 2.2.5 was the last release of Keras implementing the 2.2.* API. It was the last release to only support TensorFlow 1 (as well as Theano and CNTK).&lt;/p&gt;
&lt;p&gt;The current release is Keras 2.3.0, which makes significant API changes and add support for TensorFlow 2.0. The 2.3.0 release will be the last major release of multi-backend Keras. Multi-backend Keras is superseded by &lt;code&gt;tf.keras&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Bugs present in multi-backend Keras will only be fixed until April 2020 (as part of minor releases).&lt;/p&gt;
&lt;p&gt;For more information about the future of Keras, see &lt;a href="http://bit.ly/keras-meeting-notes" rel="nofollow"&gt;the Keras meeting notes&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-guiding-principles" class="anchor" aria-hidden="true" href="#guiding-principles"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Guiding principles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;User friendliness.&lt;/strong&gt; Keras is an API designed for human beings, not machines. It puts user experience front and center. Keras follows best practices for reducing cognitive load: it offers consistent &amp;amp; simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Modularity.&lt;/strong&gt; A model is understood as a sequence or a graph of standalone, fully configurable modules that can be plugged together with as few restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions and regularization schemes are all standalone modules that you can combine to create new models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Easy extensibility.&lt;/strong&gt; New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Work with Python&lt;/strong&gt;. No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-30-seconds-to-keras" class="anchor" aria-hidden="true" href="#getting-started-30-seconds-to-keras"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting started: 30 seconds to Keras&lt;/h2&gt;
&lt;p&gt;The core data structure of Keras is a &lt;strong&gt;model&lt;/strong&gt;, a way to organize layers. The simplest type of model is the &lt;a href="https://keras.io/getting-started/sequential-model-guide" rel="nofollow"&gt;&lt;code&gt;Sequential&lt;/code&gt;&lt;/a&gt; model, a linear stack of layers. For more complex architectures, you should use the &lt;a href="https://keras.io/getting-started/functional-api-guide" rel="nofollow"&gt;Keras functional API&lt;/a&gt;, which allows to build arbitrary graphs of layers.&lt;/p&gt;
&lt;p&gt;Here is the &lt;code&gt;Sequential&lt;/code&gt; model:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; keras.models &lt;span class="pl-k"&gt;import&lt;/span&gt; Sequential

model &lt;span class="pl-k"&gt;=&lt;/span&gt; Sequential()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Stacking layers is as easy as &lt;code&gt;.add()&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; keras.layers &lt;span class="pl-k"&gt;import&lt;/span&gt; Dense

model.add(Dense(&lt;span class="pl-v"&gt;units&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;activation&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;relu&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;input_dim&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;))
model.add(Dense(&lt;span class="pl-v"&gt;units&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;10&lt;/span&gt;, &lt;span class="pl-v"&gt;activation&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;softmax&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once your model looks good, configure its learning process with &lt;code&gt;.compile()&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;model.compile(&lt;span class="pl-v"&gt;loss&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;categorical_crossentropy&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
              &lt;span class="pl-v"&gt;optimizer&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;sgd&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
              &lt;span class="pl-v"&gt;metrics&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;accuracy&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;model.compile(&lt;span class="pl-v"&gt;loss&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;keras.losses.categorical_crossentropy,
              &lt;span class="pl-v"&gt;optimizer&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;keras.optimizers.SGD(&lt;span class="pl-v"&gt;lr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.01&lt;/span&gt;, &lt;span class="pl-v"&gt;momentum&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.9&lt;/span&gt;, &lt;span class="pl-v"&gt;nesterov&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can now iterate on your training data in batches:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.&lt;/span&gt;
model.fit(x_train, y_train, &lt;span class="pl-v"&gt;epochs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Alternatively, you can feed batches to your model manually:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;model.train_on_batch(x_batch, y_batch)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Evaluate your performance in one line:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;loss_and_metrics &lt;span class="pl-k"&gt;=&lt;/span&gt; model.evaluate(x_test, y_test, &lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;128&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or generate predictions on new data:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;classes &lt;span class="pl-k"&gt;=&lt;/span&gt; model.predict(x_test, &lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;128&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Building a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?&lt;/p&gt;
&lt;p&gt;For a more in-depth tutorial about Keras, you can check out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://keras.io/getting-started/sequential-model-guide" rel="nofollow"&gt;Getting started with the Sequential model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://keras.io/getting-started/functional-api-guide" rel="nofollow"&gt;Getting started with the functional API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the &lt;a href="https://github.com/keras-team/keras/tree/master/examples"&gt;examples folder&lt;/a&gt; of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;Before installing Keras, please install one of its backend engines: TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/install/" rel="nofollow"&gt;TensorFlow installation instructions&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://deeplearning.net/software/theano/install.html#install" rel="nofollow"&gt;Theano installation instructions&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-on-your-machine" rel="nofollow"&gt;CNTK installation instructions&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You may also consider installing the following &lt;strong&gt;optional dependencies&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/" rel="nofollow"&gt;cuDNN&lt;/a&gt; (recommended if you plan on running Keras on GPU).&lt;/li&gt;
&lt;li&gt;HDF5 and &lt;a href="http://docs.h5py.org/en/latest/build.html" rel="nofollow"&gt;h5py&lt;/a&gt; (required if you plan on saving Keras models to disk).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://graphviz.gitlab.io/download/" rel="nofollow"&gt;graphviz&lt;/a&gt; and &lt;a href="https://github.com/erocarrera/pydot"&gt;pydot&lt;/a&gt; (used by &lt;a href="https://keras.io/visualization/" rel="nofollow"&gt;visualization utilities&lt;/a&gt; to plot model graphs).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, you can install Keras itself. There are two ways to install Keras:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Install Keras from PyPI (recommended):&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: These installation steps assume that you are on a Linux or Mac environment.
If you are on Windows, you will need to remove &lt;code&gt;sudo&lt;/code&gt; to run the commands below.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo pip install keras&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you are using a virtualenv, you may want to avoid using sudo:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install keras&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Alternatively: install Keras from the GitHub source:&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, clone Keras using &lt;code&gt;git&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/keras-team/keras.git&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, &lt;code&gt;cd&lt;/code&gt; to the Keras folder and run the install command:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; keras
sudo python setup.py install&lt;/pre&gt;&lt;/div&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-configuring-your-keras-backend" class="anchor" aria-hidden="true" href="#configuring-your-keras-backend"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuring your Keras backend&lt;/h2&gt;
&lt;p&gt;By default, Keras will use TensorFlow as its tensor manipulation library. &lt;a href="https://keras.io/backend/" rel="nofollow"&gt;Follow these instructions&lt;/a&gt; to configure the Keras backend.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h2&gt;
&lt;p&gt;You can ask questions and join the development discussion:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the &lt;a href="https://groups.google.com/forum/#!forum/keras-users" rel="nofollow"&gt;Keras Google group&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;On the &lt;a href="https://kerasteam.slack.com" rel="nofollow"&gt;Keras Slack channel&lt;/a&gt;. Use &lt;a href="https://keras-slack-autojoin.herokuapp.com/" rel="nofollow"&gt;this link&lt;/a&gt; to request an invitation to the channel.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also post &lt;strong&gt;bug reports and feature requests&lt;/strong&gt; (only) in &lt;a href="https://github.com/keras-team/keras/issues"&gt;GitHub issues&lt;/a&gt;. Make sure to read &lt;a href="https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md"&gt;our guidelines&lt;/a&gt; first.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-why-this-name-keras" class="anchor" aria-hidden="true" href="#why-this-name-keras"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why this name, Keras?&lt;/h2&gt;
&lt;p&gt;Keras (κέρας) means &lt;em&gt;horn&lt;/em&gt; in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the &lt;em&gt;Odyssey&lt;/em&gt;, where dream spirits (&lt;em&gt;Oneiroi&lt;/em&gt;, singular &lt;em&gt;Oneiros&lt;/em&gt;) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words κέρας (horn) / κραίνω (fulfill), and ἐλέφας (ivory) / ἐλεφαίρομαι (deceive).&lt;/p&gt;
&lt;p&gt;Keras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;"Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them."&lt;/em&gt; Homer, Odyssey 19. 562 ff (Shewring translation).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>keras-team</author><guid isPermaLink="false">https://github.com/keras-team/keras</guid><pubDate>Wed, 12 Feb 2020 00:24:00 GMT</pubDate></item><item><title>cloud-custodian/cloud-custodian #25 in Python, Today</title><link>https://github.com/cloud-custodian/cloud-custodian</link><description>&lt;p&gt;&lt;i&gt;Rules engine for cloud security, cost optimization, and governance, DSL in yaml for policies to query, filter, and take actions on resources&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cloud-custodian" class="anchor" aria-hidden="true" href="#cloud-custodian"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cloud Custodian&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/205826a1b28ffaebacffd403d9108b0a553f8b1f/68747470733a2f2f636c6f7564637573746f6469616e2e696f2f696d672f6c6f676f5f6361706f6e655f64657665785f636c6f75645f637573746f6469616e2e737667"&gt;&lt;img src="https://camo.githubusercontent.com/205826a1b28ffaebacffd403d9108b0a553f8b1f/68747470733a2f2f636c6f7564637573746f6469616e2e696f2f696d672f6c6f676f5f6361706f6e655f64657665785f636c6f75645f637573746f6469616e2e737667" alt="Cloud Custodian Logo" width="200px" height="200px" data-canonical-src="https://cloudcustodian.io/img/logo_capone_devex_cloud_custodian.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="https://gitter.im/cloud-custodian/cloud-custodian?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ba21f47b4a5e3958e6f16cb6a2c7732d3534fc66/68747470733a2f2f6261646765732e6769747465722e696d2f636c6f75642d637573746f6469616e2f636c6f75642d637573746f6469616e2e737667" alt="" data-canonical-src="https://badges.gitter.im/cloud-custodian/cloud-custodian.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://dev.azure.com/cloud-custodian/cloud-custodian/_build" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4d61bdbf3af99ca3f8a020554d8235562dfcf7ae/68747470733a2f2f6465762e617a7572652e636f6d2f636c6f75642d637573746f6469616e2f636c6f75642d637573746f6469616e2f5f617069732f6275696c642f7374617475732f437573746f6469616e2532302d25323043493f6272616e63684e616d653d6d6173746572" alt="" data-canonical-src="https://dev.azure.com/cloud-custodian/cloud-custodian/_apis/build/status/Custodian%20-%20CI?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8f54547853cfad57acfc8e06e6008cc296cda34d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/license-Apache%202-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://codecov.io/gh/cloud-custodian/cloud-custodian" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/20edabbd43d44df878bb6dca554316003083e03d/68747470733a2f2f636f6465636f762e696f2f67682f636c6f75642d637573746f6469616e2f636c6f75642d637573746f6469616e2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="" data-canonical-src="https://codecov.io/gh/cloud-custodian/cloud-custodian/branch/master/graph/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://requires.io/github/cloud-custodian/cloud-custodian/requirements/?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b0ac9d5f3cefc4eb4172530e92b4f7689b0f9c01/68747470733a2f2f72657175697265732e696f2f6769746875622f636c6f75642d637573746f6469616e2f636c6f75642d637573746f6469616e2f726571756972656d656e74732e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="https://requires.io/github/cloud-custodian/cloud-custodian/requirements.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cloud Custodian is a rules engine for managing public cloud accounts and
resources. It allows users to define policies to enable a well managed
cloud infrastructure, that's both secure and cost optimized. It
consolidates many of the adhoc scripts organizations have into a
lightweight and flexible tool, with unified metrics and reporting.&lt;/p&gt;
&lt;p&gt;Custodian can be used to manage AWS, Azure, and GCP environments by
ensuring real time compliance to security policies (like encryption and
access requirements), tag policies, and cost management via garbage
collection of unused resources and off-hours resource management.&lt;/p&gt;
&lt;p&gt;Custodian policies are written in simple YAML configuration files that
enable users to specify policies on a resource type (EC2, ASG, Redshift,
CosmosDB, PubSub Topic) and are constructed from a vocabulary of filters
and actions.&lt;/p&gt;
&lt;p&gt;It integrates with the cloud native serverless capabilities of each
provider to provide for real time enforcement of policies with builtin
provisioning. Or it can be run as a simple cron job on a server to
execute against large existing fleets.&lt;/p&gt;
&lt;p&gt;Cloud Custodian was originally developed at CapitalOne (by @kapilt et
al), but CapitalOne does not materially contribute or support this
project, nor do they have any active maintainers. They represent just
one of the thousands of users of this project. Like many opensource
projects, development is lead by the community of hundreds of
contributors and several cloud providers have dedicated teams working
on Custodian.&lt;/p&gt;
&lt;p&gt;"&lt;a href="https://cloudrumblings.io/cloud-adoption-engineering-the-next-generation-of-cloud-governance-21fb1a2eff60" rel="nofollow"&gt;Engineering the Next Generation of Cloud
Governance&lt;/a&gt;"
by @drewfirment&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Comprehensive support for public cloud services and resources with a
rich library of actions and filters to build policies with.&lt;/li&gt;
&lt;li&gt;Supports arbitrary filtering on resources with nested boolean
conditions.&lt;/li&gt;
&lt;li&gt;Dry run any policy to see what it would do.&lt;/li&gt;
&lt;li&gt;Automatically provisions serverless functions and event sources (
AWS CloudWatchEvents, AWS Config Rules, Azure EventGrid, GCP
AuditLog &amp;amp; Pub/Sub, etc)&lt;/li&gt;
&lt;li&gt;Cloud provider native metrics outputs on resources that matched a
policy&lt;/li&gt;
&lt;li&gt;Structured outputs into cloud native object storage of which
resources matched a policy.&lt;/li&gt;
&lt;li&gt;Intelligent cache usage to minimize api calls.&lt;/li&gt;
&lt;li&gt;Supports multi-account/subscription/project usage.&lt;/li&gt;
&lt;li&gt;Battle-tested - in production on some very large cloud environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://cloudcustodian.io" rel="nofollow"&gt;Homepage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cloudcustodian.io/docs/index.html" rel="nofollow"&gt;Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloudcustodian.io/docs/developer/installing.html" rel="nofollow"&gt;Developer Install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.google.com/search?q=cloud+custodian&amp;amp;source=lnms&amp;amp;tbm=vid" rel="nofollow"&gt;Presentations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-quick-install" class="anchor" aria-hidden="true" href="#quick-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Install&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python3 -m venv custodian
$ &lt;span class="pl-c1"&gt;source&lt;/span&gt; custodian/bin/activate
(custodian) $ pip install c7n&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;The first step to using Cloud Custodian is writing a YAML file
containing the policies that you want to run. Each policy specifies
the resource type that the policy will run on, a set of filters which
control resources will be affected by this policy, actions which the policy
with take on the matched resources, and a mode which controls which
how the policy will execute.&lt;/p&gt;
&lt;p&gt;The best getting started guides are the cloud provider specific tutorials.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloudcustodian.io/docs/aws/gettingstarted.html" rel="nofollow"&gt;AWS Getting Started&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloudcustodian.io/docs/azure/gettingstarted.html" rel="nofollow"&gt;Azure Getting Started&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloudcustodian.io/docs/gcp/gettingstarted.html" rel="nofollow"&gt;GCP Getting Started&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a quick walk through, below are some sample policies for AWS resources.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;will enforce that no S3 buckets have cross-account access enabled.&lt;/li&gt;
&lt;li&gt;will terminate any newly launched EC2 instance that do not have an encrypted EBS volume.&lt;/li&gt;
&lt;li&gt;will tag any EC2 instance that does not have the follow tags
"Environment", "AppId", and either "OwnerContact" or "DeptID" to be stopped
in four days.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-yaml"&gt;&lt;pre&gt;&lt;span class="pl-ent"&gt;policies&lt;/span&gt;:
 - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;s3-cross-account&lt;/span&gt;
   &lt;span class="pl-ent"&gt;description&lt;/span&gt;: &lt;span class="pl-s"&gt;|&lt;/span&gt;
&lt;span class="pl-s"&gt;     Checks S3 for buckets with cross-account access and&lt;/span&gt;
&lt;span class="pl-s"&gt;     removes the cross-account access.&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;   &lt;span class="pl-ent"&gt;resource&lt;/span&gt;: &lt;span class="pl-s"&gt;aws.s3&lt;/span&gt;
   &lt;span class="pl-ent"&gt;region&lt;/span&gt;: &lt;span class="pl-s"&gt;us-east-1&lt;/span&gt;
   &lt;span class="pl-ent"&gt;filters&lt;/span&gt;:
     - &lt;span class="pl-ent"&gt;type&lt;/span&gt;: &lt;span class="pl-s"&gt;cross-account&lt;/span&gt;
   &lt;span class="pl-ent"&gt;actions&lt;/span&gt;:
     - &lt;span class="pl-ent"&gt;type&lt;/span&gt;: &lt;span class="pl-s"&gt;remove-statements&lt;/span&gt;
       &lt;span class="pl-ent"&gt;statement_ids&lt;/span&gt;: &lt;span class="pl-s"&gt;matched&lt;/span&gt;

 - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;ec2-require-non-public-and-encrypted-volumes&lt;/span&gt;
   &lt;span class="pl-ent"&gt;resource&lt;/span&gt;: &lt;span class="pl-s"&gt;aws.ec2&lt;/span&gt;
   &lt;span class="pl-ent"&gt;description&lt;/span&gt;: &lt;span class="pl-s"&gt;|&lt;/span&gt;
&lt;span class="pl-s"&gt;    Provision a lambda and cloud watch event target&lt;/span&gt;
&lt;span class="pl-s"&gt;    that looks at all new instances and terminates those with&lt;/span&gt;
&lt;span class="pl-s"&gt;    unencrypted volumes.&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;   &lt;span class="pl-ent"&gt;mode&lt;/span&gt;:
    &lt;span class="pl-ent"&gt;type&lt;/span&gt;: &lt;span class="pl-s"&gt;cloudtrail&lt;/span&gt;
    &lt;span class="pl-ent"&gt;role&lt;/span&gt;: &lt;span class="pl-s"&gt;CloudCustodian-QuickStart&lt;/span&gt;
    &lt;span class="pl-ent"&gt;events&lt;/span&gt;:
      - &lt;span class="pl-s"&gt;RunInstances&lt;/span&gt;
   &lt;span class="pl-ent"&gt;filters&lt;/span&gt;:
    - &lt;span class="pl-ent"&gt;type&lt;/span&gt;: &lt;span class="pl-s"&gt;ebs&lt;/span&gt;
      &lt;span class="pl-ent"&gt;key&lt;/span&gt;: &lt;span class="pl-s"&gt;Encrypted&lt;/span&gt;
      &lt;span class="pl-ent"&gt;value&lt;/span&gt;: &lt;span class="pl-c1"&gt;false&lt;/span&gt;
   &lt;span class="pl-ent"&gt;actions&lt;/span&gt;:
    - &lt;span class="pl-s"&gt;terminate&lt;/span&gt;

 - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;tag-compliance&lt;/span&gt;
   &lt;span class="pl-ent"&gt;resource&lt;/span&gt;: &lt;span class="pl-s"&gt;aws.ec2&lt;/span&gt;
   &lt;span class="pl-ent"&gt;description&lt;/span&gt;: &lt;span class="pl-s"&gt;|&lt;/span&gt;
&lt;span class="pl-s"&gt;     Schedule a resource that does not meet tag compliance policies to be stopped in four days. Note a separate policy using the`marked-for-op` filter is required to actually stop the instances after four days.&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;   &lt;span class="pl-ent"&gt;filters&lt;/span&gt;:
    - &lt;span class="pl-ent"&gt;State.Name&lt;/span&gt;: &lt;span class="pl-s"&gt;running&lt;/span&gt;
    - &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-ent"&gt;tag:Environment&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;absent&lt;/span&gt;
    - &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-ent"&gt;tag:AppId&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;absent&lt;/span&gt;
    - &lt;span class="pl-ent"&gt;or&lt;/span&gt;:
      - &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-ent"&gt;tag:OwnerContact&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;absent&lt;/span&gt;
      - &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-ent"&gt;tag:DeptID&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;absent&lt;/span&gt;
   &lt;span class="pl-ent"&gt;actions&lt;/span&gt;:
    - &lt;span class="pl-ent"&gt;type&lt;/span&gt;: &lt;span class="pl-s"&gt;mark-for-op&lt;/span&gt;
      &lt;span class="pl-ent"&gt;op&lt;/span&gt;: &lt;span class="pl-s"&gt;stop&lt;/span&gt;
      &lt;span class="pl-ent"&gt;days&lt;/span&gt;: &lt;span class="pl-c1"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can validate, test, and run Cloud Custodian with the example policy with these commands:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Validate the configuration (note this happens by default on run)&lt;/span&gt;
$ custodian validate policy.yml

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Dryrun on the policies (no actions executed) to see what resources&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; match each policy.&lt;/span&gt;
$ custodian run --dryrun -s out policy.yml

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run the policy&lt;/span&gt;
$ custodian run -s out policy.yml&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can run Cloud Custodian via Docker as well:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download the image&lt;/span&gt;
$ docker pull cloudcustodian/c7n
$ mkdir output

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run the policy&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This will run the policy using only the environment variables for authentication&lt;/span&gt;
$ docker run -it \
  -v &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;pwd&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;/output:/home/custodian/output \
  -v &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;pwd&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;/policy.yml:/home/custodian/policy.yml \
  --env-file &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;&amp;lt;(&lt;/span&gt;env &lt;span class="pl-k"&gt;|&lt;/span&gt; grep &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;^AWS\|^AZURE\|^GOOGLE&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt; \
  cloudcustodian/c7n run -v -s /home/custodian/output /home/custodian/policy.yml

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run the policy (using AWS's generated credentials from STS)&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; NOTE: We mount the ``.aws/credentials`` and ``.aws/config`` directories to&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; the docker container to support authentication to AWS using the same credentials&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; credentials that are available to the local user if authenticating with STS.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This exposes your container to additional credentials than may be necessary,&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; i.e. additional credentials may be available inside of the container than is&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; minimally necessary.&lt;/span&gt;

$ docker run -it \
  -v &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;pwd&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;/output:/home/custodian/output \
  -v &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;pwd&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;/policy.yml:/home/custodian/policy.yml \
  -v &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;cd &lt;span class="pl-k"&gt;~&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; pwd&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;/.aws/credentials:/home/custodian/.aws/credentials \
  -v &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;cd &lt;span class="pl-k"&gt;~&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; pwd&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;/.aws/config:/home/custodian/.aws/config \
  --env-file &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;&amp;lt;(&lt;/span&gt;env &lt;span class="pl-k"&gt;|&lt;/span&gt; grep &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;^AWS&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt; \
  cloudcustodian/c7n run -v -s /home/custodian/output /home/custodian/policy.yml&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Custodian supports other useful subcommands and options, including
outputs to S3, CloudWatch metrics, STS role assumption. Policies go
together like Lego bricks with actions and filters.&lt;/p&gt;
&lt;p&gt;Consult the documentation for additional information, or reach out on gitter.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-cloud-provider-specific-help" class="anchor" aria-hidden="true" href="#cloud-provider-specific-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cloud Provider Specific Help&lt;/h2&gt;
&lt;p&gt;For specific instructions for AWS, Azure, and GCP, visit the relevant getting started page.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cloudcustodian.io/docs/aws/gettingstarted.html" rel="nofollow"&gt;AWS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloudcustodian.io/docs/azure/gettingstarted.html" rel="nofollow"&gt;Azure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloudcustodian.io/docs/gcp/gettingstarted.html" rel="nofollow"&gt;GCP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-get-involved" class="anchor" aria-hidden="true" href="#get-involved"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Get Involved&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://gitter.im/cloud-custodian/cloud-custodian" rel="nofollow"&gt;Gitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cloud-custodian/cloud-custodian"&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://groups.google.com/forum/#!forum/cloud-custodian" rel="nofollow"&gt;Mailing List&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://reddit.com/r/cloudcustodian" rel="nofollow"&gt;Reddit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/tagged/cloudcustodian" rel="nofollow"&gt;StackOverflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-additional-tools" class="anchor" aria-hidden="true" href="#additional-tools"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Additional Tools&lt;/h2&gt;
&lt;p&gt;The Custodian project also develops and maintains a suite of additional
tools here
&lt;a href="https://github.com/cloud-custodian/cloud-custodian/tree/master/tools"&gt;https://github.com/cloud-custodian/cloud-custodian/tree/master/tools&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://cloudcustodian.io/docs/tools/c7n-org.html" rel="nofollow"&gt;&lt;strong&gt;&lt;em&gt;Org&lt;/em&gt;:&lt;/strong&gt;&lt;/a&gt; Multi-account policy execution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://cloudcustodian.io/docs/tools/c7n-policystream.html" rel="nofollow"&gt;&lt;strong&gt;&lt;em&gt;PolicyStream&lt;/em&gt;:&lt;/strong&gt;&lt;/a&gt; Git history as stream of logical policy changes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://cloudcustodian.io/docs/tools/c7n-salactus.html" rel="nofollow"&gt;&lt;strong&gt;&lt;em&gt;Salactus&lt;/em&gt;:&lt;/strong&gt;&lt;/a&gt; Scale out s3 scanning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://cloudcustodian.io/docs/tools/c7n-mailer.html" rel="nofollow"&gt;&lt;strong&gt;&lt;em&gt;Mailer&lt;/em&gt;:&lt;/strong&gt;&lt;/a&gt; A reference implementation of sending messages to users to notify them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://cloudcustodian.io/docs/tools/c7n-trailcreator.html" rel="nofollow"&gt;&lt;strong&gt;&lt;em&gt;Trail Creator&lt;/em&gt;:&lt;/strong&gt;&lt;/a&gt; Retroactive tagging of resources creators from CloudTrail&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;TrailDB&lt;/em&gt;:&lt;/strong&gt; Cloudtrail indexing and time series generation for dashboarding.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://cloudcustodian.io/docs/tools/c7n-logexporter.html" rel="nofollow"&gt;&lt;strong&gt;&lt;em&gt;LogExporter&lt;/em&gt;:&lt;/strong&gt;&lt;/a&gt; Cloud watch log exporting to s3&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://cloudcustodian.io/docs/tools/cask.html" rel="nofollow"&gt;&lt;strong&gt;&lt;em&gt;Cask&lt;/em&gt;:&lt;/strong&gt;&lt;/a&gt; Easy custodian exec via docker&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://cloudcustodian.io/docs/tools/c7n-guardian.html" rel="nofollow"&gt;&lt;strong&gt;&lt;em&gt;Guardian&lt;/em&gt;:&lt;/strong&gt;&lt;/a&gt; Automated multi-account Guard Duty setup&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://cloudcustodian.io/docs/tools/omnissm.html" rel="nofollow"&gt;&lt;strong&gt;&lt;em&gt;Omni SSM&lt;/em&gt;:&lt;/strong&gt;&lt;/a&gt; EC2 Systems Manager Automation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Sentry&lt;/em&gt;:&lt;/strong&gt; Cloudwatch Log parsing for python tracebacks to integrate with
&lt;a href="https://sentry.io/welcome/" rel="nofollow"&gt;https://sentry.io/welcome/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/ops#mugc"&gt;&lt;strong&gt;&lt;em&gt;Mugc&lt;/em&gt;:&lt;/strong&gt;&lt;/a&gt; A utility used to clean up Cloud Custodian Lambda policies that are deployed in an AWS environment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;See &lt;a href="https://cloudcustodian.io/docs/contribute.html" rel="nofollow"&gt;https://cloudcustodian.io/docs/contribute.html&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-code-of-conduct" class="anchor" aria-hidden="true" href="#code-of-conduct"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code of Conduct&lt;/h2&gt;
&lt;p&gt;This project adheres to the &lt;a href="https://developer.capitalone.com/resources/code-of-conduct" rel="nofollow"&gt;Open Code of Conduct&lt;/a&gt;. By
participating, you are expected to honor this code.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>cloud-custodian</author><guid isPermaLink="false">https://github.com/cloud-custodian/cloud-custodian</guid><pubDate>Wed, 12 Feb 2020 00:25:00 GMT</pubDate></item></channel></rss>