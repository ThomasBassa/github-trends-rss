<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Python, Today</title><link>https://github.com/trending/python?since=daily</link><description>The top repositories on GitHub for python, measured daily</description><pubDate>Sat, 04 Jan 2020 01:10:50 GMT</pubDate><lastBuildDate>Sat, 04 Jan 2020 01:10:50 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>google-research/ALBERT #1 in Python, Today</title><link>https://github.com/google-research/ALBERT</link><description>&lt;p&gt;&lt;i&gt;ALBERT: A Lite BERT for Self-supervised Learning of Language Representations&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-albert" class="anchor" aria-hidden="true" href="#albert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ALBERT&lt;/h1&gt;
&lt;p&gt;***************New December 30, 2019 ***************&lt;/p&gt;
&lt;p&gt;Chinese models are released. We would like to thank &lt;a href="https://github.com/CLUEbenchmark/CLUE"&gt;CLUE team &lt;/a&gt; for providing the training data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://storage.googleapis.com/albert_models/albert_base_zh.tar.gz" rel="nofollow"&gt;Base&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://storage.googleapis.com/albert_models/albert_large_zh.tar.gz" rel="nofollow"&gt;Large&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://storage.googleapis.com/albert_models/albert_xlarge_zh.tar.gz" rel="nofollow"&gt;Xlarge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://storage.googleapis.com/albert_models/albert_xxlarge_zh.tar.gz" rel="nofollow"&gt;Xxlarge&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Version 2 of ALBERT models is released.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Base: [&lt;a href="https://storage.googleapis.com/albert_models/albert_base_v2.tar.gz" rel="nofollow"&gt;Tar file&lt;/a&gt;] [&lt;a href="https://tfhub.dev/google/albert_base/2" rel="nofollow"&gt;TF-Hub&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Large: [&lt;a href="https://storage.googleapis.com/albert_models/albert_large_v2.tar.gz" rel="nofollow"&gt;Tar file&lt;/a&gt;] [&lt;a href="https://tfhub.dev/google/albert_large/2" rel="nofollow"&gt;TF-Hub&lt;/a&gt; ]&lt;/li&gt;
&lt;li&gt;Xlarge: [&lt;a href="https://storage.googleapis.com/albert_models/albert_xlarge_v2.tar.gz" rel="nofollow"&gt;Tar file&lt;/a&gt;] [&lt;a href="https://tfhub.dev/google/albert_xlarge/2" rel="nofollow"&gt;TF-Hub&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Xxlarge: [&lt;a href="https://storage.googleapis.com/albert_models/albert_xxlarge_v2.tar.gz" rel="nofollow"&gt;Tar file&lt;/a&gt;] [&lt;a href="https://tfhub.dev/google/albert_xxlarge/2" rel="nofollow"&gt;TF-Hub&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this version, we apply 'no dropout', 'additional training data' and 'long training time' strategies to all models. We train ALBERT-base for 10M steps and other models for 3M steps.&lt;/p&gt;
&lt;p&gt;The result comparison to the v1 models is as followings:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Average&lt;/th&gt;
&lt;th&gt;SQuAD1.1&lt;/th&gt;
&lt;th&gt;SQuAD2.0&lt;/th&gt;
&lt;th&gt;MNLI&lt;/th&gt;
&lt;th&gt;SST-2&lt;/th&gt;
&lt;th&gt;RACE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;V2&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALBERT-base&lt;/td&gt;
&lt;td&gt;82.3&lt;/td&gt;
&lt;td&gt;90.2/83.2&lt;/td&gt;
&lt;td&gt;82.1/79.3&lt;/td&gt;
&lt;td&gt;84.6&lt;/td&gt;
&lt;td&gt;92.9&lt;/td&gt;
&lt;td&gt;66.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALBERT-large&lt;/td&gt;
&lt;td&gt;85.7&lt;/td&gt;
&lt;td&gt;91.8/85.2&lt;/td&gt;
&lt;td&gt;84.9/81.8&lt;/td&gt;
&lt;td&gt;86.5&lt;/td&gt;
&lt;td&gt;94.9&lt;/td&gt;
&lt;td&gt;75.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALBERT-xlarge&lt;/td&gt;
&lt;td&gt;87.9&lt;/td&gt;
&lt;td&gt;92.9/86.4&lt;/td&gt;
&lt;td&gt;87.9/84.1&lt;/td&gt;
&lt;td&gt;87.9&lt;/td&gt;
&lt;td&gt;95.4&lt;/td&gt;
&lt;td&gt;80.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALBERT-xxlarge&lt;/td&gt;
&lt;td&gt;90.9&lt;/td&gt;
&lt;td&gt;94.6/89.1&lt;/td&gt;
&lt;td&gt;89.8/86.9&lt;/td&gt;
&lt;td&gt;90.6&lt;/td&gt;
&lt;td&gt;96.8&lt;/td&gt;
&lt;td&gt;86.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;V1&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALBERT-base&lt;/td&gt;
&lt;td&gt;80.1&lt;/td&gt;
&lt;td&gt;89.3/82.3&lt;/td&gt;
&lt;td&gt;80.0/77.1&lt;/td&gt;
&lt;td&gt;81.6&lt;/td&gt;
&lt;td&gt;90.3&lt;/td&gt;
&lt;td&gt;64.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALBERT-large&lt;/td&gt;
&lt;td&gt;82.4&lt;/td&gt;
&lt;td&gt;90.6/83.9&lt;/td&gt;
&lt;td&gt;82.3/79.4&lt;/td&gt;
&lt;td&gt;83.5&lt;/td&gt;
&lt;td&gt;91.7&lt;/td&gt;
&lt;td&gt;68.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALBERT-xlarge&lt;/td&gt;
&lt;td&gt;85.5&lt;/td&gt;
&lt;td&gt;92.5/86.1&lt;/td&gt;
&lt;td&gt;86.1/83.1&lt;/td&gt;
&lt;td&gt;86.4&lt;/td&gt;
&lt;td&gt;92.4&lt;/td&gt;
&lt;td&gt;74.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALBERT-xxlarge&lt;/td&gt;
&lt;td&gt;91.0&lt;/td&gt;
&lt;td&gt;94.8/89.3&lt;/td&gt;
&lt;td&gt;90.2/87.4&lt;/td&gt;
&lt;td&gt;90.8&lt;/td&gt;
&lt;td&gt;96.9&lt;/td&gt;
&lt;td&gt;86.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The comparison shows that for ALBERT-base, ALBERT-large, and ALBERT-xlarge, v2 is much better than v1, indicating the importance of applying the above three strategies. On average, ALBERT-xxlarge is slightly worse than the v1, because of the following two reasons: 1) Training additional 1.5 M steps (the only difference between these two models is training for 1.5M steps and 3M steps) did not lead to significant performance improvement. 2) For v1, we did a little bit hyperparameter search among the parameters sets given by BERT, Roberta, and XLnet. For v2, we simply adopt the parameters from v1 except for RACE, where we use a learning rate of 1e-5 and 0 &lt;a href="https://arxiv.org/pdf/1909.11942.pdf" rel="nofollow"&gt;ALBERT DR&lt;/a&gt; (dropout rate for ALBERT in finetuning). The original (v1) RACE hyperparameter will cause model divergence for v2 models. Given that the downstream tasks are sensitive to the fine-tuning hyperparameters, we should be careful about so called slight improvements.&lt;/p&gt;
&lt;p&gt;ALBERT is "A Lite" version of BERT, a popular unsupervised language
representation learning algorithm. ALBERT uses parameter-reduction techniques
that allow for large-scale configurations, overcome previous memory limitations,
and achieve better behavior with respect to model degradation.&lt;/p&gt;
&lt;p&gt;For a technical description of the algorithm, see our paper:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1909.11942" rel="nofollow"&gt;ALBERT: A Lite BERT for Self-supervised Learning of Language Representations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-release-notes" class="anchor" aria-hidden="true" href="#release-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Release Notes&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Initial release: 10/9/2019&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-results" class="anchor" aria-hidden="true" href="#results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results&lt;/h1&gt;
&lt;p&gt;Performance of ALBERT on GLUE benchmark results using a single-model setup on
dev:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Models&lt;/th&gt;
&lt;th&gt;MNLI&lt;/th&gt;
&lt;th&gt;QNLI&lt;/th&gt;
&lt;th&gt;QQP&lt;/th&gt;
&lt;th&gt;RTE&lt;/th&gt;
&lt;th&gt;SST&lt;/th&gt;
&lt;th&gt;MRPC&lt;/th&gt;
&lt;th&gt;CoLA&lt;/th&gt;
&lt;th&gt;STS&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT-large&lt;/td&gt;
&lt;td&gt;86.6&lt;/td&gt;
&lt;td&gt;92.3&lt;/td&gt;
&lt;td&gt;91.3&lt;/td&gt;
&lt;td&gt;70.4&lt;/td&gt;
&lt;td&gt;93.2&lt;/td&gt;
&lt;td&gt;88.0&lt;/td&gt;
&lt;td&gt;60.6&lt;/td&gt;
&lt;td&gt;90.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XLNet-large&lt;/td&gt;
&lt;td&gt;89.8&lt;/td&gt;
&lt;td&gt;93.9&lt;/td&gt;
&lt;td&gt;91.8&lt;/td&gt;
&lt;td&gt;83.8&lt;/td&gt;
&lt;td&gt;95.6&lt;/td&gt;
&lt;td&gt;89.2&lt;/td&gt;
&lt;td&gt;63.6&lt;/td&gt;
&lt;td&gt;91.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RoBERTa-large&lt;/td&gt;
&lt;td&gt;90.2&lt;/td&gt;
&lt;td&gt;94.7&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;92.2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;86.6&lt;/td&gt;
&lt;td&gt;96.4&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;90.9&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;68.0&lt;/td&gt;
&lt;td&gt;92.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALBERT (1M)&lt;/td&gt;
&lt;td&gt;90.4&lt;/td&gt;
&lt;td&gt;95.2&lt;/td&gt;
&lt;td&gt;92.0&lt;/td&gt;
&lt;td&gt;88.1&lt;/td&gt;
&lt;td&gt;96.8&lt;/td&gt;
&lt;td&gt;90.2&lt;/td&gt;
&lt;td&gt;68.7&lt;/td&gt;
&lt;td&gt;92.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALBERT (1.5M)&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;90.8&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;95.3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;92.2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;89.2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;96.9&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;90.9&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;71.4&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;93.0&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Performance of ALBERT-xxl on SQuaD and RACE benchmarks using a single-model
setup:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Models&lt;/th&gt;
&lt;th&gt;SQuAD1.1 dev&lt;/th&gt;
&lt;th&gt;SQuAD2.0 dev&lt;/th&gt;
&lt;th&gt;SQuAD2.0 test&lt;/th&gt;
&lt;th&gt;RACE test (Middle/High)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT-large&lt;/td&gt;
&lt;td&gt;90.9/84.1&lt;/td&gt;
&lt;td&gt;81.8/79.0&lt;/td&gt;
&lt;td&gt;89.1/86.3&lt;/td&gt;
&lt;td&gt;72.0 (76.6/70.1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XLNet&lt;/td&gt;
&lt;td&gt;94.5/89.0&lt;/td&gt;
&lt;td&gt;88.8/86.1&lt;/td&gt;
&lt;td&gt;89.1/86.3&lt;/td&gt;
&lt;td&gt;81.8 (85.5/80.2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RoBERTa&lt;/td&gt;
&lt;td&gt;94.6/88.9&lt;/td&gt;
&lt;td&gt;89.4/86.5&lt;/td&gt;
&lt;td&gt;89.8/86.8&lt;/td&gt;
&lt;td&gt;83.2 (86.5/81.3)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UPM&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;89.9/87.2&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XLNet + SG-Net Verifier++&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;90.1/87.2&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALBERT (1M)&lt;/td&gt;
&lt;td&gt;94.8/89.2&lt;/td&gt;
&lt;td&gt;89.9/87.2&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;86.0 (88.2/85.1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ALBERT (1.5M)&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;94.8/89.3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;90.2/87.4&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;90.9/88.1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;86.5 (89.0/85.5)&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-pre-trained-models" class="anchor" aria-hidden="true" href="#pre-trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-trained Models&lt;/h1&gt;
&lt;p&gt;TF-Hub modules are available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Base: [&lt;a href="https://storage.googleapis.com/albert_models/albert_base_v1.tar.gz" rel="nofollow"&gt;Tar file&lt;/a&gt;] [&lt;a href="https://tfhub.dev/google/albert_base/1" rel="nofollow"&gt;TF-Hub&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Large: [&lt;a href="https://storage.googleapis.com/albert_models/albert_large_v1.tar.gz" rel="nofollow"&gt;Tar file&lt;/a&gt;] [&lt;a href="https://tfhub.dev/google/albert_large/1" rel="nofollow"&gt;TF-Hub&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Xlarge: [&lt;a href="https://storage.googleapis.com/albert_models/albert_xlarge_v1.tar.gz" rel="nofollow"&gt;Tar file&lt;/a&gt;] [&lt;a href="https://tfhub.dev/google/albert_xlarge/1" rel="nofollow"&gt;TF-Hub&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Xxlarge: [&lt;a href="https://storage.googleapis.com/albert_models/albert_xxlarge_v1.tar.gz" rel="nofollow"&gt;Tar file&lt;/a&gt;] [&lt;a href="https://tfhub.dev/google/albert_xxlarge/1" rel="nofollow"&gt;TF-Hub&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example usage of the TF-Hub module:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tags = set()
if is_training:
  tags.add("train")
albert_module = hub.Module("https://tfhub.dev/google/albert_base/1", tags=tags,
                           trainable=True)
albert_inputs = dict(
    input_ids=input_ids,
    input_mask=input_mask,
    segment_ids=segment_ids)
albert_outputs = albert_module(
    inputs=albert_inputs,
    signature="tokens",
    as_dict=True)

# If you want to use the token-level output, use
# albert_outputs["sequence_output"] instead.
output_layer = albert_outputs["pooled_output"]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For a full example, see &lt;code&gt;run_classifier_with_tfhub.py&lt;/code&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pre-training-instructions" class="anchor" aria-hidden="true" href="#pre-training-instructions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training Instructions&lt;/h1&gt;
&lt;p&gt;To pretrain ALBERT, use &lt;code&gt;run_pretraining.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install -r albert/requirements.txt
python -m albert.run_pretraining \
    --input_file=... \
    --output_dir=... \
    --init_checkpoint=... \
    --albert_config_file=... \
    --do_train \
    --do_eval \
    --train_batch_size=4096 \
    --eval_batch_size=64 \
    --max_seq_length=512 \
    --max_predictions_per_seq=20 \
    --optimizer='lamb' \
    --learning_rate=.00176 \
    --num_train_steps=125000 \
    --num_warmup_steps=3125 \
    --save_checkpoints_steps=5000
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-fine-tuning-on-glue" class="anchor" aria-hidden="true" href="#fine-tuning-on-glue"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning on GLUE&lt;/h1&gt;
&lt;p&gt;To fine-tune and evaluate a pretrained ALBERT on GLUE, please see the
convenience script &lt;code&gt;run_glue.sh&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Lower-level use cases may want to use the &lt;code&gt;run_classifier.py&lt;/code&gt; script directly.
The &lt;code&gt;run_classifier.py&lt;/code&gt; script is used both for fine-tuning and evaluation of
ALBERT on individual GLUE benchmark tasks, such as MNLI:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install -r albert/requirements.txt
python -m albert.run_classifier \
  --vocab_file=... \
  --data_dir=... \
  --output_dir=... \
  --init_checkpoint=... \
  --albert_config_file=... \
  --spm_model_file=... \
  --do_train \
  --do_eval \
  --do_predict \
  --do_lower_case \
  --max_seq_length=128 \
  --optimizer=adamw \
  --task_name=MNLI \
  --warmup_step=1000 \
  --learning_rate=3e-5 \
  --train_step=10000 \
  --save_checkpoints_steps=100 \
  --train_batch_size=128
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Good default flag values for each GLUE task can be found in &lt;code&gt;run_glue.sh&lt;/code&gt;.
You can fine-tune the model starting from TF-Hub modules instead of raw
checkpoints by setting e.g.
&lt;code&gt;--albert_hub_module_handle==https://tfhub.dev/google/albert_base/1&lt;/code&gt; instead
of &lt;code&gt;--init_checkpoint&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;After evaluation, the script should report some output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  global_step = ...
  loss = ...
  masked_lm_accuracy = ...
  masked_lm_loss = ...
  sentence_order_accuracy = ...
  sentence_order_loss = ...
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-fine-tuning-on-squad" class="anchor" aria-hidden="true" href="#fine-tuning-on-squad"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning on SQuAD&lt;/h1&gt;
&lt;p&gt;To fine-tune and evaluate a pretrained model on SQuAD v1, use the
&lt;code&gt;run_squad_v1.py&lt;/code&gt; script:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install -r albert/requirements.txt
python -m albert.run_squad_v1 \
  --albert_config_file=... \
  --vocab_file=... \
  --output_dir=... \
  --train_file=... \
  --predict_file=... \
  --train_feature_file=... \
  --predict_feature_file=... \
  --predict_feature_left_file=... \
  --init_checkpoint=... \
  --spm_model_file=... \
  --do_lower_case \
  --max_seq_length=384 \
  --doc_stride=128 \
  --max_query_length=64 \
  --do_train=true \
  --do_predict=true \
  --train_batch_size=48 \
  --predict_batch_size=8 \
  --learning_rate=5e-5 \
  --num_train_epochs=2.0 \
  --warmup_proportion=.1 \
  --save_checkpoints_steps=5000 \
  --n_best_size=20 \
  --max_answer_length=30
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For SQuAD v2, use the &lt;code&gt;run_squad_v2.py&lt;/code&gt; script:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install -r albert/requirements.txt
python -m albert.run_squad_v2 \
  --albert_config_file=... \
  --vocab_file=... \
  --output_dir=... \
  --train_file=... \
  --predict_file=... \
  --train_feature_file=... \
  --predict_feature_file=... \
  --predict_feature_left_file=... \
  --init_checkpoint=... \
  --spm_model_file=... \
  --do_lower_case \
  --max_seq_length=384 \
  --doc_stride=128 \
  --max_query_length=64 \
  --do_train \
  --do_predict \
  --train_batch_size=48 \
  --predict_batch_size=8 \
  --learning_rate=5e-5 \
  --num_train_epochs=2.0 \
  --warmup_proportion=.1 \
  --save_checkpoints_steps=5000 \
  --n_best_size=20 \
  --max_answer_length=30
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/ALBERT</guid><pubDate>Sat, 04 Jan 2020 00:01:00 GMT</pubDate></item><item><title>vinta/awesome-python #2 in Python, Today</title><link>https://github.com/vinta/awesome-python</link><description>&lt;p&gt;&lt;i&gt;A curated list of awesome Python frameworks, libraries, software and resources&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-awesome-python-" class="anchor" aria-hidden="true" href="#awesome-python-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome Python &lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://camo.githubusercontent.com/13c4e50d88df7178ae1882a203ed57b641674f94/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667" alt="Awesome" data-canonical-src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;A curated list of awesome Python frameworks, libraries, software and resources.&lt;/p&gt;
&lt;p&gt;Inspired by &lt;a href="https://github.com/ziadoz/awesome-php"&gt;awesome-php&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#awesome-python"&gt;Awesome Python&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#admin-panels"&gt;Admin Panels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#algorithms-and-design-patterns"&gt;Algorithms and Design Patterns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#asgi-servers"&gt;ASGI Servers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#asynchronous-programming"&gt;Asynchronous Programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#audio"&gt;Audio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#authentication"&gt;Authentication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#build-tools"&gt;Build Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#built-in-classes-enhancement"&gt;Built-in Classes Enhancement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#caching"&gt;Caching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#chatops-tools"&gt;ChatOps Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cms"&gt;CMS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#code-analysis"&gt;Code Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#command-line-interface-development"&gt;Command-line Interface Development&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#command-line-tools"&gt;Command-line Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#compatibility"&gt;Compatibility&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#computer-vision"&gt;Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#concurrency-and-parallelism"&gt;Concurrency and Parallelism&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#configuration"&gt;Configuration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cryptography"&gt;Cryptography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-analysis"&gt;Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-validation"&gt;Data Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-visualization"&gt;Data Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#database-drivers"&gt;Database Drivers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#database"&gt;Database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#date-and-time"&gt;Date and Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#debugging-tools"&gt;Debugging Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-learning"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#devops-tools"&gt;DevOps Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#distributed-computing"&gt;Distributed Computing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#distribution"&gt;Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#downloader"&gt;Downloader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#e-commerce"&gt;E-commerce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#editor-plugins-and-ides"&gt;Editor Plugins and IDEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#email"&gt;Email&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#environment-management"&gt;Environment Management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#files"&gt;Files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#foreign-function-interface"&gt;Foreign Function Interface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#forms"&gt;Forms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#functional-programming"&gt;Functional Programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#game-development"&gt;Game Development&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#geolocation"&gt;Geolocation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#gui-development"&gt;GUI Development&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hardware"&gt;Hardware&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#html-manipulation"&gt;HTML Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#http-clients"&gt;HTTP Clients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image-processing"&gt;Image Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#implementations"&gt;Implementations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#interactive-interpreter"&gt;Interactive Interpreter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#internationalization"&gt;Internationalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#job-scheduler"&gt;Job Scheduler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#logging"&gt;Logging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#machine-learning"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#miscellaneous"&gt;Miscellaneous&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#natural-language-processing"&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#network-virtualization"&gt;Network Virtualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#news-feed"&gt;News Feed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#orm"&gt;ORM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#package-management"&gt;Package Management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#package-repositories"&gt;Package Repositories&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#permissions"&gt;Permissions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#processes"&gt;Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#recommender-systems"&gt;Recommender Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#refactoring"&gt;Refactoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restful-api"&gt;RESTful API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#robotics"&gt;Robotics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rpc-servers"&gt;RPC Servers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#science"&gt;Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#search"&gt;Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#serialization"&gt;Serialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#serverless-frameworks"&gt;Serverless Frameworks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#specific-formats-processing"&gt;Specific Formats Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#static-site-generator"&gt;Static Site Generator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tagging"&gt;Tagging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#task-queues"&gt;Task Queues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#template-engine"&gt;Template Engine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#testing"&gt;Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#text-processing"&gt;Text Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#third-party-apis"&gt;Third-party APIs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#url-manipulation"&gt;URL Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#video"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#web-asset-management"&gt;Web Asset Management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#web-content-extracting"&gt;Web Content Extracting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#web-crawling"&gt;Web Crawling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#web-frameworks"&gt;Web Frameworks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#websocket"&gt;WebSocket&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#wsgi-servers"&gt;WSGI Servers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#resources"&gt;Resources&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#podcasts"&gt;Podcasts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#twitter"&gt;Twitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#websites"&gt;Websites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#weekly"&gt;Weekly&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-admin-panels" class="anchor" aria-hidden="true" href="#admin-panels"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Admin Panels&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for administrative interfaces.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ajenti/ajenti"&gt;ajenti&lt;/a&gt; - The admin panel your servers deserve.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://grappelliproject.com/" rel="nofollow"&gt;django-grappelli&lt;/a&gt; - A jazzy skin for the Django Admin-Interface.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/geex-arts/django-jet"&gt;django-jet&lt;/a&gt; - Modern responsive template for the Django admin interface with improved functionality.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://djangosuit.com/" rel="nofollow"&gt;django-suit&lt;/a&gt; - Alternative Django Admin-Interface (free only for Non-commercial use).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sshwsfc/xadmin"&gt;django-xadmin&lt;/a&gt; - Drop-in replacement of Django admin comes with lots of goodies.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jet-admin/jet-bridge"&gt;jet-bridge&lt;/a&gt; - Admin panel framework for any application with nice UI (ex Jet Django)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/flask-admin/flask-admin"&gt;flask-admin&lt;/a&gt; - Simple and extensible administrative interface framework for Flask.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mher/flower"&gt;flower&lt;/a&gt; - Real-time monitor and web admin for Celery.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wooey/wooey"&gt;wooey&lt;/a&gt; - A Django app which creates automatic web UIs for Python scripts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-algorithms-and-design-patterns" class="anchor" aria-hidden="true" href="#algorithms-and-design-patterns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Algorithms and Design Patterns&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Python implementation of algorithms and design patterns.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/keon/algorithms"&gt;algorithms&lt;/a&gt; - Minimal examples of data structures and algorithms in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tylerlaberge/PyPattyrn"&gt;PyPattyrn&lt;/a&gt; - A simple yet effective library for implementing common design patterns.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/faif/python-patterns"&gt;python-patterns&lt;/a&gt; - A collection of design patterns in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/grantjenks/python-sortedcontainers"&gt;sortedcontainers&lt;/a&gt; - Fast, pure-Python implementation of SortedList, SortedDict, and SortedSet types.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-asgi-servers" class="anchor" aria-hidden="true" href="#asgi-servers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ASGI Servers&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;ASGI-compatible web servers.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/encode/uvicorn"&gt;uvicorn&lt;/a&gt; - Uvicorn is a lightning-fast ASGI server implementation, using uvloop and httptools.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-asynchronous-programming" class="anchor" aria-hidden="true" href="#asynchronous-programming"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Asynchronous Programming&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/asyncio.html" rel="nofollow"&gt;asyncio&lt;/a&gt; - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks.
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/timofurrer/awesome-asyncio"&gt;awesome-asyncio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MagicStack/uvloop"&gt;uvloop&lt;/a&gt; - Ultra fast asyncio event loop.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twistedmatrix.com/trac/" rel="nofollow"&gt;Twisted&lt;/a&gt; - An event-driven networking engine.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-audio" class="anchor" aria-hidden="true" href="#audio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Audio&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for manipulating audio and its metadata.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Audio
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/beetbox/audioread"&gt;audioread&lt;/a&gt; - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/worldveil/dejavu"&gt;dejavu&lt;/a&gt; - Audio fingerprinting and recognition.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://bspaans.github.io/python-mingus/" rel="nofollow"&gt;mingus&lt;/a&gt; - An advanced music theory and notation package with MIDI file and playback support.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tyiannak/pyAudioAnalysis"&gt;pyAudioAnalysis&lt;/a&gt; - Audio feature extraction, classification, segmentation and applications.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jiaaro/pydub"&gt;pydub&lt;/a&gt; - Manipulate audio with a simple and easy high level interface.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Parisson/TimeSide"&gt;TimeSide&lt;/a&gt; - Open web audio processing framework.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Metadata
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/beetbox/beets"&gt;beets&lt;/a&gt; - A music library manager and &lt;a href="https://musicbrainz.org/" rel="nofollow"&gt;MusicBrainz&lt;/a&gt; tagger.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nicfit/eyeD3"&gt;eyeD3&lt;/a&gt; - A tool for working with audio files, specifically MP3 files containing ID3 metadata.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/quodlibet/mutagen"&gt;mutagen&lt;/a&gt; - A Python module to handle audio metadata.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/devsnd/tinytag"&gt;tinytag&lt;/a&gt; - A library for reading music meta data of MP3, OGG, FLAC and Wave files.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-authentication" class="anchor" aria-hidden="true" href="#authentication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authentication&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for implementing authentications schemes.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OAuth
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/lepture/authlib"&gt;authlib&lt;/a&gt; - JavaScript Object Signing and Encryption draft implementation.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pennersr/django-allauth"&gt;django-allauth&lt;/a&gt; - Authentication app for Django that "just works."&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/evonove/django-oauth-toolkit"&gt;django-oauth-toolkit&lt;/a&gt; - OAuth 2 goodies for Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/idan/oauthlib"&gt;oauthlib&lt;/a&gt; - A generic and thorough implementation of the OAuth request-signing logic.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/joestump/python-oauth2"&gt;python-oauth2&lt;/a&gt; - A fully tested, abstract interface to creating OAuth clients and servers.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/omab/python-social-auth"&gt;python-social-auth&lt;/a&gt; - An easy-to-setup social authentication mechanism.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;JWT
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jpadilla/pyjwt"&gt;pyjwt&lt;/a&gt; - JSON Web Token implementation in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mpdavis/python-jose/"&gt;python-jose&lt;/a&gt; - A JOSE implementation in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/davedoesdev/python-jwt"&gt;python-jwt&lt;/a&gt; - A module for generating and verifying JSON Web Tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-build-tools" class="anchor" aria-hidden="true" href="#build-tools"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build Tools&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Compile software from source code.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.yoctoproject.org/docs/1.6/bitbake-user-manual/bitbake-user-manual.html" rel="nofollow"&gt;BitBake&lt;/a&gt; - A make-like build tool for embedded Linux.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.buildout.org/en/latest/" rel="nofollow"&gt;buildout&lt;/a&gt; - A build system for creating, assembling and deploying applications from multiple parts.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/platformio/platformio-core"&gt;PlatformIO&lt;/a&gt; - A console tool to build code with different development platforms.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pybuilder/pybuilder"&gt;pybuilder&lt;/a&gt; - A continuous build tool written in pure Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.scons.org/" rel="nofollow"&gt;SCons&lt;/a&gt; - A software construction tool.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-built-in-classes-enhancement" class="anchor" aria-hidden="true" href="#built-in-classes-enhancement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Built-in Classes Enhancement&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for enhancing Python built-in classes.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/dataclasses.html" rel="nofollow"&gt;dataclasses&lt;/a&gt; - (Python standard library) Data classes.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/python-attrs/attrs"&gt;attrs&lt;/a&gt; - Replacement for &lt;code&gt;__init__&lt;/code&gt;, &lt;code&gt;__eq__&lt;/code&gt;, &lt;code&gt;__repr__&lt;/code&gt;, etc. boilerplate in class definitions.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jab/bidict"&gt;bidict&lt;/a&gt; - Efficient, Pythonic bidirectional map data structures and related functionality..&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cdgriffith/Box"&gt;Box&lt;/a&gt; - Python dictionaries with advanced dot notation access.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/carlosescri/DottedDict"&gt;DottedDict&lt;/a&gt; - A library that provides a method of accessing lists and dicts with a dotted path notation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-cms" class="anchor" aria-hidden="true" href="#cms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CMS&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Content Management Systems.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://wagtail.io/" rel="nofollow"&gt;wagtail&lt;/a&gt; - A Django content management system.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.django-cms.org/en/" rel="nofollow"&gt;django-cms&lt;/a&gt; - An Open source enterprise CMS based on the Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/feincms/feincms"&gt;feincms&lt;/a&gt; - One of the most advanced Content Management Systems built on Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kotti/Kotti"&gt;Kotti&lt;/a&gt; - A high-level, Pythonic web application framework built on Pyramid.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/stephenmcd/mezzanine"&gt;mezzanine&lt;/a&gt; - A powerful, consistent, and flexible content management platform.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://plone.org/" rel="nofollow"&gt;plone&lt;/a&gt; - A CMS built on top of the open source application server Zope.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rochacbruno/quokka"&gt;quokka&lt;/a&gt; - Flexible, extensible, small CMS powered by Flask and MongoDB.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-caching" class="anchor" aria-hidden="true" href="#caching"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Caching&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for caching data.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/bbangert/beaker"&gt;beaker&lt;/a&gt; - A WSGI middleware for sessions and caching.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/django-cache-machine/django-cache-machine"&gt;django-cache-machine&lt;/a&gt; - Automatic caching and invalidation for Django models.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Suor/django-cacheops"&gt;django-cacheops&lt;/a&gt; - A slick ORM cache with automatic granular event-driven invalidation.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dogpilecache.readthedocs.io/en/latest/" rel="nofollow"&gt;dogpile.cache&lt;/a&gt; - dogpile.cache is next generation replacement for Beaker made by same authors.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/HermesCache/" rel="nofollow"&gt;HermesCache&lt;/a&gt; - Python caching library with tag-based invalidation and dogpile effect prevention.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lericson/pylibmc"&gt;pylibmc&lt;/a&gt; - A Python wrapper around the &lt;a href="https://libmemcached.org/libMemcached.html" rel="nofollow"&gt;libmemcached&lt;/a&gt; interface.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.grantjenks.com/docs/diskcache/" rel="nofollow"&gt;python-diskcache&lt;/a&gt; - SQLite and file backed cache backend with faster lookups than memcached and redis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-chatops-tools" class="anchor" aria-hidden="true" href="#chatops-tools"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ChatOps Tools&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for chatbot development.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/errbotio/errbot/"&gt;errbot&lt;/a&gt; - The easiest and most popular chatbot to implement ChatOps.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-code-analysis" class="anchor" aria-hidden="true" href="#code-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code Analysis&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Tools of static analysis, linters and code quality checkers. Also see &lt;a href="https://github.com/mre/awesome-static-analysis"&gt;awesome-static-analysis&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Code Analysis
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/coala/coala/"&gt;coala&lt;/a&gt; - Language independent and easily extendable code analysis application.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/scottrogowski/code2flow"&gt;code2flow&lt;/a&gt; - Turn your Python and JavaScript code into DOT flowcharts.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PyCQA/prospector"&gt;prospector&lt;/a&gt; - A tool to analyse Python code.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/gak/pycallgraph"&gt;pycallgraph&lt;/a&gt; - A library that visualises the flow (call graph) of your Python application.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jendrikseipp/vulture"&gt;vulture&lt;/a&gt; - A tool for finding and analysing dead Python code.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Code Linters
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/flake8/" rel="nofollow"&gt;flake8&lt;/a&gt; - A wrapper around &lt;code&gt;pycodestyle&lt;/code&gt;, &lt;code&gt;pyflakes&lt;/code&gt; and McCabe.
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/DmytroLitvinov/awesome-flake8-extensions"&gt;awesome-flake8-extensions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pylint.org/" rel="nofollow"&gt;pylint&lt;/a&gt; - A fully customizable source code analyzer.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/klen/pylama"&gt;pylama&lt;/a&gt; - A code audit tool for Python and JavaScript.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wemake-services/wemake-python-styleguide"&gt;wemake-python-styleguide&lt;/a&gt; - The strictest and most opinionated python linter ever.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Code Formatters
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/python/black"&gt;black&lt;/a&gt; - The uncompromising Python code formatter.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/yapf"&gt;yapf&lt;/a&gt; - Yet another Python code formatter from Google.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Static Type Checkers, also see &lt;a href="https://github.com/typeddjango/awesome-python-typing"&gt;awesome-python-typing&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://mypy-lang.org/" rel="nofollow"&gt;mypy&lt;/a&gt; - Check variable types during compile time.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebook/pyre-check"&gt;pyre-check&lt;/a&gt; - Performant type checking.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Static Type Annotations Generators
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Instagram/MonkeyType"&gt;MonkeyType&lt;/a&gt; - A system for Python that generates static type annotations by collecting runtime types&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-command-line-interface-development" class="anchor" aria-hidden="true" href="#command-line-interface-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Command-line Interface Development&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for building command-line applications.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Command-line Application Development
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://builtoncement.com/" rel="nofollow"&gt;cement&lt;/a&gt; - CLI Application Framework for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://click.pocoo.org/dev/" rel="nofollow"&gt;click&lt;/a&gt; - A package for creating beautiful command line interfaces in a composable way.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.openstack.org/developer/cliff/" rel="nofollow"&gt;cliff&lt;/a&gt; - A framework for creating command-line programs with multi-level commands.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kennethreitz/clint"&gt;clint&lt;/a&gt; - Python Command-line Application Tools.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://docopt.org/" rel="nofollow"&gt;docopt&lt;/a&gt; - Pythonic command line arguments parser.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/python-fire"&gt;python-fire&lt;/a&gt; - A library for creating command line interfaces from absolutely any Python object.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jonathanslenders/python-prompt-toolkit"&gt;python-prompt-toolkit&lt;/a&gt; - A library for building powerful interactive command lines.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Terminal Rendering
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/peterbrittain/asciimatics"&gt;asciimatics&lt;/a&gt; - A package to create full-screen text UIs (from interactive forms to ASCII animations).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/glamp/bashplotlib"&gt;bashplotlib&lt;/a&gt; - Making basic plots in the terminal.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/colorama/" rel="nofollow"&gt;colorama&lt;/a&gt; - Cross-platform colored terminal text.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tqdm/tqdm"&gt;tqdm&lt;/a&gt; - Fast, extensible progress bar for loops and CLI.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-command-line-tools" class="anchor" aria-hidden="true" href="#command-line-tools"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Command-line Tools&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Useful CLI-based tools for productivity.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Productivity Tools
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/audreyr/cookiecutter"&gt;cookiecutter&lt;/a&gt; - A command-line utility that creates projects from cookiecutters (project templates).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sloria/doitlive"&gt;doitlive&lt;/a&gt; - A tool for live presentations in the terminal.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/gleitz/howdoi"&gt;howdoi&lt;/a&gt; - Instant coding answers via the command line.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebook/PathPicker"&gt;PathPicker&lt;/a&gt; - Select files out of bash output.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mooz/percol"&gt;percol&lt;/a&gt; - Adds flavor of interactive selection to the traditional pipe concept on UNIX.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nvbn/thefuck"&gt;thefuck&lt;/a&gt; - Correcting your previous console command.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tony/tmuxp"&gt;tmuxp&lt;/a&gt; - A &lt;a href="https://github.com/tmux/tmux"&gt;tmux&lt;/a&gt; session manager.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/timofurrer/try"&gt;try&lt;/a&gt; - A dead simple CLI to try out python packages - it's never been easier.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CLI Enhancements
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jakubroztocil/httpie"&gt;httpie&lt;/a&gt; - A command line HTTP client, a user-friendly cURL replacement.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cloudnativelabs/kube-shell"&gt;kube-shell&lt;/a&gt; - An integrated shell for working with the Kubernetes CLI.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dbcli/mycli"&gt;mycli&lt;/a&gt; - A Terminal Client for MySQL with AutoCompletion and Syntax Highlighting.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dbcli/pgcli"&gt;pgcli&lt;/a&gt; - Postgres CLI with autocompletion and syntax highlighting.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/donnemartin/saws"&gt;saws&lt;/a&gt; - A Supercharged &lt;a href="https://github.com/aws/aws-cli"&gt;aws-cli&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-compatibility" class="anchor" aria-hidden="true" href="#compatibility"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compatibility&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for migrating from Python 2 to 3.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://python-future.org/index.html" rel="nofollow"&gt;python-future&lt;/a&gt; - The missing compatibility layer between Python 2 and Python 3.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mitsuhiko/python-modernize"&gt;python-modernize&lt;/a&gt; - Modernizes Python code for eventual Python 3 migration.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/six/" rel="nofollow"&gt;six&lt;/a&gt; - Python 2 and 3 compatibility utilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-computer-vision" class="anchor" aria-hidden="true" href="#computer-vision"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Computer Vision&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for computer vision.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/kornia/kornia/"&gt;Kornia&lt;/a&gt; - Open Source Differentiable Computer Vision Library for PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://opencv.org/" rel="nofollow"&gt;OpenCV&lt;/a&gt; - Open Source Computer Vision Library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/madmaze/pytesseract"&gt;pytesseract&lt;/a&gt; - Another wrapper for &lt;a href="https://github.com/tesseract-ocr"&gt;Google Tesseract OCR&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://simplecv.org/" rel="nofollow"&gt;SimpleCV&lt;/a&gt; - An open source framework for building computer vision applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-concurrency-and-parallelism" class="anchor" aria-hidden="true" href="#concurrency-and-parallelism"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Concurrency and Parallelism&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for concurrent and parallel execution. Also see &lt;a href="https://github.com/timofurrer/awesome-asyncio"&gt;awesome-asyncio&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow"&gt;concurrent.futures&lt;/a&gt; - (Python standard library) A high-level interface for asynchronously executing callables.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/multiprocessing.html" rel="nofollow"&gt;multiprocessing&lt;/a&gt; - (Python standard library) Process-based parallelism.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://eventlet.net/" rel="nofollow"&gt;eventlet&lt;/a&gt; - Asynchronous framework with WSGI support.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.gevent.org/" rel="nofollow"&gt;gevent&lt;/a&gt; - A coroutine-based Python networking library that uses &lt;a href="https://github.com/python-greenlet/greenlet"&gt;greenlet&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MagicStack/uvloop"&gt;uvloop&lt;/a&gt; - Ultra fast implementation of &lt;code&gt;asyncio&lt;/code&gt; event loop on top of &lt;code&gt;libuv&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/soravux/scoop"&gt;scoop&lt;/a&gt; - Scalable Concurrent Operations in Python.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-configuration" class="anchor" aria-hidden="true" href="#configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for storing and parsing configuration options.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/DiffSK/configobj"&gt;configobj&lt;/a&gt; - INI file parser with validation.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/configparser.html" rel="nofollow"&gt;configparser&lt;/a&gt; - (Python standard library) INI file parser.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://profig.readthedocs.io/en/default/" rel="nofollow"&gt;profig&lt;/a&gt; - Config from multiple formats with value conversion.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/henriquebastos/python-decouple"&gt;python-decouple&lt;/a&gt; - Strict separation of settings from code.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-cryptography" class="anchor" aria-hidden="true" href="#cryptography"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cryptography&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cryptography.io/en/latest/" rel="nofollow"&gt;cryptography&lt;/a&gt; - A package designed to expose cryptographic primitives and recipes to Python developers.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/paramiko/paramiko"&gt;paramiko&lt;/a&gt; - The leading native Python SSHv2 protocol library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://passlib.readthedocs.io/en/stable/" rel="nofollow"&gt;passlib&lt;/a&gt; - Secure password storage/hashing library, very high level.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pyca/pynacl"&gt;pynacl&lt;/a&gt; - Python binding to the Networking and Cryptography (NaCl) library.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-data-analysis" class="anchor" aria-hidden="true" href="#data-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data Analysis&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for data analyzing.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/blaze/blaze"&gt;Blaze&lt;/a&gt; - NumPy and Pandas interface to Big Data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mining/mining"&gt;Open Mining&lt;/a&gt; - Business Intelligence (BI) in Pandas interface.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://orange.biolab.si/" rel="nofollow"&gt;Orange&lt;/a&gt; - Data mining, data visualization, analysis and machine learning through visual programming or scripts.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pandas.pydata.org/" rel="nofollow"&gt;Pandas&lt;/a&gt; - A library providing high-performance, easy-to-use data structures and data analysis tools.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ironmussa/Optimus"&gt;Optimus&lt;/a&gt; - Agile Data Science Workflows made easy with PySpark.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-data-validation" class="anchor" aria-hidden="true" href="#data-validation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data Validation&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for validating data. Used for forms in many cases.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/pyeve/cerberus"&gt;Cerberus&lt;/a&gt; - A lightweight and extensible data validation library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.pylonsproject.org/projects/colander/en/latest/" rel="nofollow"&gt;colander&lt;/a&gt; - Validating and deserializing data obtained via XML, JSON, an HTML form post.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Julian/jsonschema"&gt;jsonschema&lt;/a&gt; - An implementation of &lt;a href="http://json-schema.org/" rel="nofollow"&gt;JSON Schema&lt;/a&gt; for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/keleshev/schema"&gt;schema&lt;/a&gt; - A library for validating Python data structures.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/schematics/schematics"&gt;Schematics&lt;/a&gt; - Data Structure Validation.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/podio/valideer"&gt;valideer&lt;/a&gt; - Lightweight extensible data validation and adaptation library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/alecthomas/voluptuous"&gt;voluptuous&lt;/a&gt; - A Python data validation library.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-data-visualization" class="anchor" aria-hidden="true" href="#data-visualization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data Visualization&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for visualizing data. Also see &lt;a href="https://github.com/sorrycc/awesome-javascript#data-visualization"&gt;awesome-javascript&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/altair-viz/altair"&gt;Altair&lt;/a&gt; - Declarative statistical visualization library for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bokeh/bokeh"&gt;Bokeh&lt;/a&gt; - Interactive Web Plotting for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bloomberg/bqplot"&gt;bqplot&lt;/a&gt; - Interactive Plotting Library for the Jupyter Notebook&lt;/li&gt;
&lt;li&gt;&lt;a href="https://plot.ly/products/dash/" rel="nofollow"&gt;Dash&lt;/a&gt; - Built on top of Flask, React and Plotly aimed at analytical web applications.
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Acrotrend/awesome-dash"&gt;awesome-dash&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/has2k1/plotnine"&gt;plotnine&lt;/a&gt; - A grammar of graphics for Python based on ggplot2.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://matplotlib.org/" rel="nofollow"&gt;Matplotlib&lt;/a&gt; - A Python 2D plotting library.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.pygal.org/en/latest/" rel="nofollow"&gt;Pygal&lt;/a&gt; - A Python SVG Charts Creator.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/pygraphviz/" rel="nofollow"&gt;PyGraphviz&lt;/a&gt; - Python interface to &lt;a href="http://www.graphviz.org/" rel="nofollow"&gt;Graphviz&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.pyqtgraph.org/" rel="nofollow"&gt;PyQtGraph&lt;/a&gt; - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mwaskom/seaborn"&gt;Seaborn&lt;/a&gt; - Statistical data visualization using Matplotlib.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vispy/vispy"&gt;VisPy&lt;/a&gt; - High-performance scientific visualization based on OpenGL.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-database" class="anchor" aria-hidden="true" href="#database"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Database&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Databases implemented in Python.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/patx/pickledb"&gt;pickleDB&lt;/a&gt; - A simple and lightweight key-value store for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/msiemens/tinydb"&gt;tinydb&lt;/a&gt; - A tiny, document-oriented database.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zopefoundation/ZODB"&gt;ZODB&lt;/a&gt; - A native object database for Python. A key-value and object graph database.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-database-drivers" class="anchor" aria-hidden="true" href="#database-drivers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Database Drivers&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for connecting and operating databases.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MySQL - &lt;a href="http://shlomi-noach.github.io/awesome-mysql/" rel="nofollow"&gt;awesome-mysql&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/PyMySQL/mysqlclient-python"&gt;mysqlclient&lt;/a&gt; - MySQL connector with Python 3 support (&lt;a href="https://sourceforge.net/projects/mysql-python/" rel="nofollow"&gt;mysql-python&lt;/a&gt; fork).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PyMySQL/PyMySQL"&gt;PyMySQL&lt;/a&gt; - A pure Python MySQL driver compatible to mysql-python.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PostgreSQL - &lt;a href="https://github.com/dhamaniasad/awesome-postgres"&gt;awesome-postgres&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://initd.org/psycopg/" rel="nofollow"&gt;psycopg2&lt;/a&gt; - The most popular PostgreSQL adapter for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/gmr/queries"&gt;queries&lt;/a&gt; - A wrapper of the psycopg2 library for interacting with PostgreSQL.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Other Relational Databases
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.pymssql.org/en/latest/" rel="nofollow"&gt;pymssql&lt;/a&gt; - A simple database interface to Microsoft SQL Server.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/plasticityai/supersqlite"&gt;SuperSQLite&lt;/a&gt; - A supercharged SQLite library built on top of &lt;a href="https://github.com/rogerbinns/apsw"&gt;apsw&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NoSQL Databases
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/datastax/python-driver"&gt;cassandra-driver&lt;/a&gt; - The Python Driver for Apache Cassandra.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wbolster/happybase"&gt;happybase&lt;/a&gt; - A developer-friendly library for Apache HBase.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dpkp/kafka-python"&gt;kafka-python&lt;/a&gt; - The Python client for Apache Kafka.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://py2neo.org/" rel="nofollow"&gt;py2neo&lt;/a&gt; - A client library and toolkit for working with Neo4j.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mongodb/mongo-python-driver"&gt;pymongo&lt;/a&gt; - The official Python client for MongoDB.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/andymccurdy/redis-py"&gt;redis-py&lt;/a&gt; - The Python client for Redis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Asynchronous Clients
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/mongodb/motor"&gt;motor&lt;/a&gt; - The async Python driver for MongoDB.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-date-and-time" class="anchor" aria-hidden="true" href="#date-and-time"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Date and Time&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for working with dates and times.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/KoffeinFlummi/Chronyk"&gt;Chronyk&lt;/a&gt; - A Python 3 library for parsing human-written times and dates.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dateutil/dateutil"&gt;dateutil&lt;/a&gt; - Extensions to the standard Python &lt;a href="https://docs.python.org/3/library/datetime.html" rel="nofollow"&gt;datetime&lt;/a&gt; module.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/myusuf3/delorean/"&gt;delorean&lt;/a&gt; - A library for clearing up the inconvenient truths that arise dealing with datetimes.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zachwill/moment"&gt;moment&lt;/a&gt; - A Python library for dealing with dates/times. Inspired by &lt;a href="http://momentjs.com/" rel="nofollow"&gt;Moment.js&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sdispater/pendulum"&gt;Pendulum&lt;/a&gt; - Python datetimes made easy.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/shinux/PyTime"&gt;PyTime&lt;/a&gt; - An easy-to-use Python module which aims to operate date/time/datetime by string.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://launchpad.net/pytz" rel="nofollow"&gt;pytz&lt;/a&gt; - World timezone definitions, modern and historical. Brings the &lt;a href="https://en.wikipedia.org/wiki/Tz_database" rel="nofollow"&gt;tz database&lt;/a&gt; into Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dirn/When.py"&gt;when.py&lt;/a&gt; - Providing user-friendly functions to help perform common date and time actions.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kennethreitz/maya"&gt;maya&lt;/a&gt; - Datetimes for Humans.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-debugging-tools" class="anchor" aria-hidden="true" href="#debugging-tools"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Debugging Tools&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for debugging code.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pdb-like Debugger
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/gotcha/ipdb"&gt;ipdb&lt;/a&gt; - IPython-enabled &lt;a href="https://docs.python.org/3/library/pdb.html" rel="nofollow"&gt;pdb&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/antocuni/pdb"&gt;pdb++&lt;/a&gt; - Another drop-in replacement for pdb.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/inducer/pudb"&gt;pudb&lt;/a&gt; - A full-screen, console-based Python debugger.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kozea/wdb"&gt;wdb&lt;/a&gt; - An improbable web debugger through WebSockets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tracing
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/khamidou/lptrace"&gt;lptrace&lt;/a&gt; - &lt;a href="http://man7.org/linux/man-pages/man1/strace.1.html" rel="nofollow"&gt;strace&lt;/a&gt; for Python programs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ionelmc/python-manhole"&gt;manhole&lt;/a&gt; - Debugging UNIX socket connections and present the stacktraces for all threads and an interactive prompt.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/pyringe"&gt;pyringe&lt;/a&gt; - Debugger capable of attaching to and injecting code into Python processes.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ionelmc/python-hunter"&gt;python-hunter&lt;/a&gt; - A flexible code tracing toolkit.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Profiler
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/rkern/line_profiler"&gt;line_profiler&lt;/a&gt; - Line-by-line profiling.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fabianp/memory_profiler"&gt;memory_profiler&lt;/a&gt; - Monitor Memory usage of Python code.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/what-studio/profiling"&gt;profiling&lt;/a&gt; - An interactive Python profiler.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/benfred/py-spy"&gt;py-spy&lt;/a&gt; - A sampling profiler for Python programs. Written in Rust.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/uber/pyflame"&gt;pyflame&lt;/a&gt; - A ptracing profiler For Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nvdv/vprof"&gt;vprof&lt;/a&gt; - Visual Python profiler.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Others
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/gruns/icecream"&gt;icecream&lt;/a&gt; - Inspect variables, expressions, and program execution with a single, simple function call.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jazzband/django-debug-toolbar"&gt;django-debug-toolbar&lt;/a&gt; - Display various debug information for Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dcramer/django-devserver"&gt;django-devserver&lt;/a&gt; - A drop-in replacement for Django's runserver.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mgood/flask-debugtoolbar"&gt;flask-debugtoolbar&lt;/a&gt; - A port of the django-debug-toolbar to flask.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/eliben/pyelftools"&gt;pyelftools&lt;/a&gt; - Parsing and analyzing ELF files and DWARF debugging information.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-deep-learning" class="anchor" aria-hidden="true" href="#deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Frameworks for Neural Networks and Deep Learning. Also see &lt;a href="https://github.com/ChristosChristofidis/awesome-deep-learning"&gt;awesome-deep-learning&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/BVLC/caffe"&gt;caffe&lt;/a&gt; - A fast open framework for deep learning..&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/keras-team/keras"&gt;keras&lt;/a&gt; - A high-level neural networks library and capable of running on top of either TensorFlow or Theano.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dmlc/mxnet"&gt;mxnet&lt;/a&gt; - A deep learning framework designed for both efficiency and flexibility.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pytorch/pytorch"&gt;pytorch&lt;/a&gt; - Tensors and Dynamic neural networks in Python with strong GPU acceleration.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SerpentAI/SerpentAI"&gt;SerpentAI&lt;/a&gt; - Game agent framework. Use any video game as a deep learning sandbox.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/tensorflow"&gt;tensorflow&lt;/a&gt; - The most popular Deep Learning framework created by Google.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Theano/Theano"&gt;Theano&lt;/a&gt; - A library for fast numerical computation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-devops-tools" class="anchor" aria-hidden="true" href="#devops-tools"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DevOps Tools&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Software and libraries for DevOps.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ansible/ansible"&gt;ansible&lt;/a&gt; - A radically simple IT automation platform.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloudinit.readthedocs.io/en/latest/" rel="nofollow"&gt;cloudinit&lt;/a&gt; - A multi-distribution package that handles early initialization of a cloud instance.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sebastien/cuisine"&gt;cuisine&lt;/a&gt; - Chef-like functionality for Fabric.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.docker.com/compose/" rel="nofollow"&gt;docker-compose&lt;/a&gt; - Fast, isolated development environments using &lt;a href="https://www.docker.com/" rel="nofollow"&gt;Docker&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fabric/fabric"&gt;fabric&lt;/a&gt; - A simple, Pythonic tool for remote execution and deployment.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fabtools/fabtools"&gt;fabtools&lt;/a&gt; - Tools for writing awesome Fabric files.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nickstenning/honcho"&gt;honcho&lt;/a&gt; - A Python clone of &lt;a href="https://github.com/ddollar/foreman"&gt;Foreman&lt;/a&gt;, for managing Procfile-based applications.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.openstack.org/" rel="nofollow"&gt;OpenStack&lt;/a&gt; - Open source software for building private and public clouds.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pexpect/pexpect"&gt;pexpect&lt;/a&gt; - Controlling interactive programs in a pseudo-terminal like GNU expect.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giampaolo/psutil"&gt;psutil&lt;/a&gt; - A cross-platform process and system utilities module.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/saltstack/salt"&gt;saltstack&lt;/a&gt; - Infrastructure automation and management system.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Supervisor/supervisor"&gt;supervisor&lt;/a&gt; - Supervisor process control system for UNIX.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-distributed-computing" class="anchor" aria-hidden="true" href="#distributed-computing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Distributed Computing&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Frameworks and libraries for Distributed Computing.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Batch Processing
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/pyspark/" rel="nofollow"&gt;PySpark&lt;/a&gt; - &lt;a href="https://spark.apache.org/" rel="nofollow"&gt;Apache Spark&lt;/a&gt; Python API.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dask/dask"&gt;dask&lt;/a&gt; - A flexible parallel computing library for analytic computing.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/spotify/luigi"&gt;luigi&lt;/a&gt; - A module that helps you build complex pipelines of batch jobs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Yelp/mrjob"&gt;mrjob&lt;/a&gt; - Run MapReduce jobs on Hadoop or Amazon Web Services.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ray-project/ray/"&gt;Ray&lt;/a&gt; - A system for parallel and distributed Python that unifies the machine learning ecosystem.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stream Processing
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/robinhood/faust"&gt;faust&lt;/a&gt; - A stream processing library, porting the ideas from &lt;a href="https://kafka.apache.org/documentation/streams/" rel="nofollow"&gt;Kafka Streams&lt;/a&gt; to Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Parsely/streamparse"&gt;streamparse&lt;/a&gt; - Run Python code against real-time streams of data via &lt;a href="http://storm.apache.org/" rel="nofollow"&gt;Apache Storm&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-distribution" class="anchor" aria-hidden="true" href="#distribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Distribution&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries to create packaged executables for release distribution.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/spotify/dh-virtualenv"&gt;dh-virtualenv&lt;/a&gt; - Build and distribute a virtualenv as a Debian package.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nuitka.net/" rel="nofollow"&gt;Nuitka&lt;/a&gt; - Compile scripts, modules, packages to an executable or extension module.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pythonhosted.org/py2app/" rel="nofollow"&gt;py2app&lt;/a&gt; - Freezes Python scripts (Mac OS X).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.py2exe.org/" rel="nofollow"&gt;py2exe&lt;/a&gt; - Freezes Python scripts (Windows).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pyinstaller/pyinstaller"&gt;PyInstaller&lt;/a&gt; - Converts Python programs into stand-alone executables (cross-platform).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pynsist.readthedocs.io/en/latest/" rel="nofollow"&gt;pynsist&lt;/a&gt; - A tool to build Windows installers, installers bundle Python itself.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for generating project documentation.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/sphinx-doc/sphinx/"&gt;sphinx&lt;/a&gt; - Python Documentation generator.
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/yoloseem/awesome-sphinxdoc"&gt;awesome-sphinxdoc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mitmproxy/pdoc"&gt;pdoc&lt;/a&gt; - Epydoc replacement to auto generate API documentation for Python libraries.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pycco-docs/pycco"&gt;pycco&lt;/a&gt; - The literate-programming-style documentation generator.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-downloader" class="anchor" aria-hidden="true" href="#downloader"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloader&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for downloading.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/s3tools/s3cmd"&gt;s3cmd&lt;/a&gt; - A command line tool for managing Amazon S3 and CloudFront.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bloomreach/s4cmd"&gt;s4cmd&lt;/a&gt; - Super S3 command line tool, good for higher performance.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://you-get.org/" rel="nofollow"&gt;you-get&lt;/a&gt; - A YouTube/Youku/Niconico video downloader written in Python 3.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rg3.github.io/youtube-dl/" rel="nofollow"&gt;youtube-dl&lt;/a&gt; - A small command-line program to download videos from YouTube.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-e-commerce" class="anchor" aria-hidden="true" href="#e-commerce"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;E-commerce&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Frameworks and libraries for e-commerce and payments.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/lxneng/alipay"&gt;alipay&lt;/a&gt; - Unofficial Alipay API for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/stephenmcd/cartridge"&gt;Cartridge&lt;/a&gt; - A shopping cart app built using the Mezzanine.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://oscarcommerce.com/" rel="nofollow"&gt;django-oscar&lt;/a&gt; - An open-source e-commerce framework for Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/awesto/django-shop"&gt;django-shop&lt;/a&gt; - A Django based shop system.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/agiliq/merchant"&gt;merchant&lt;/a&gt; - A Django app to accept payments from various payment processors.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/carlospalol/money"&gt;money&lt;/a&gt; - &lt;code&gt;Money&lt;/code&gt; class with optional CLDR-backed locale-aware formatting and an extensible currency exchange.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Alir3z4/python-currencies"&gt;python-currencies&lt;/a&gt; - Display money format and its filthy currencies.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MicroPyramid/forex-python"&gt;forex-python&lt;/a&gt; - Foreign exchange rates, Bitcoin price index and currency conversion.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://getsaleor.com/" rel="nofollow"&gt;saleor&lt;/a&gt; - An e-commerce storefront for Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.shuup.com/en/" rel="nofollow"&gt;shoop&lt;/a&gt; - An open source E-Commerce platform based on Django.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-editor-plugins-and-ides" class="anchor" aria-hidden="true" href="#editor-plugins-and-ides"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Editor Plugins and IDEs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Emacs
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jorgenschaefer/elpy"&gt;elpy&lt;/a&gt; - Emacs Python Development Environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sublime Text
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/DamnWidget/anaconda"&gt;anaconda&lt;/a&gt; - Anaconda turns your Sublime Text 3 in a full featured Python development IDE.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/srusskih/SublimeJEDI"&gt;SublimeJEDI&lt;/a&gt; - A Sublime Text plugin to the awesome auto-complete library Jedi.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Vim
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/davidhalter/jedi-vim"&gt;jedi-vim&lt;/a&gt; - Vim bindings for the Jedi auto-completion library for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/python-mode/python-mode"&gt;python-mode&lt;/a&gt; - An all in one plugin for turning Vim into a Python IDE.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Valloric/YouCompleteMe"&gt;YouCompleteMe&lt;/a&gt; - Includes &lt;a href="https://github.com/davidhalter/jedi"&gt;Jedi&lt;/a&gt;-based completion engine for Python.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Visual Studio
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Microsoft/PTVS"&gt;PTVS&lt;/a&gt; - Python Tools for Visual Studio.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Visual Studio Code
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-python.python" rel="nofollow"&gt;Python&lt;/a&gt; - The official VSCode extension with rich support for Python.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;IDE
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.jetbrains.com/pycharm/" rel="nofollow"&gt;PyCharm&lt;/a&gt; - Commercial Python IDE by JetBrains. Has free community edition available.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/spyder-ide/spyder"&gt;spyder&lt;/a&gt; - Open Source Python IDE.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-email" class="anchor" aria-hidden="true" href="#email"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Email&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for sending and parsing email.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://tomekwojcik.github.io/envelopes/" rel="nofollow"&gt;envelopes&lt;/a&gt; - Mailing for human beings.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mailgun/flanker"&gt;flanker&lt;/a&gt; - An email address and Mime parsing library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/martinrusev/imbox"&gt;imbox&lt;/a&gt; - Python IMAP for Humans.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kennethreitz/inbox.py"&gt;inbox.py&lt;/a&gt; - Python SMTP Server for Humans.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zedshaw/lamson"&gt;lamson&lt;/a&gt; - Pythonic SMTP Application Server.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/marrow/mailer"&gt;Marrow Mailer&lt;/a&gt; - High-performance extensible mail delivery framework.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/modoboa/modoboa"&gt;modoboa&lt;/a&gt; - A mail hosting and management platform including a modern and simplified Web UI.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nylas/sync-engine"&gt;Nylas Sync Engine&lt;/a&gt; - Providing a RESTful API on top of a powerful email sync platform.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kootenpv/yagmail"&gt;yagmail&lt;/a&gt; - Yet another Gmail/SMTP client.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-environment-management" class="anchor" aria-hidden="true" href="#environment-management"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Environment Management&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for Python version and virtual environment management.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/pyenv/pyenv"&gt;pyenv&lt;/a&gt; - Simple Python version management.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pypa/virtualenv"&gt;virtualenv&lt;/a&gt; - A tool to create isolated Python environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-files" class="anchor" aria-hidden="true" href="#files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Files&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for file manipulation and MIME type detection.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/mimetypes.html" rel="nofollow"&gt;mimetypes&lt;/a&gt; - (Python standard library) Map filenames to MIME types.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jaraco/path.py"&gt;path.py&lt;/a&gt; - A module wrapper for &lt;a href="https://docs.python.org/3/library/os.path.html" rel="nofollow"&gt;os.path&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/pathlib.html" rel="nofollow"&gt;pathlib&lt;/a&gt; - (Python standard library) An cross-platform, object-oriented path library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pyfilesystem/pyfilesystem2"&gt;PyFilesystem2&lt;/a&gt; - Python's filesystem abstraction layer.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ahupp/python-magic"&gt;python-magic&lt;/a&gt; - A Python interface to the libmagic file type identification library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mikeorr/Unipath"&gt;Unipath&lt;/a&gt; - An object-oriented approach to file/directory operations.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/gorakhargosh/watchdog"&gt;watchdog&lt;/a&gt; - API and shell utilities to monitor file system events.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-foreign-function-interface" class="anchor" aria-hidden="true" href="#foreign-function-interface"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Foreign Function Interface&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for providing foreign function interface.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/cffi/" rel="nofollow"&gt;cffi&lt;/a&gt; - Foreign Function Interface for Python calling C code.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/ctypes.html" rel="nofollow"&gt;ctypes&lt;/a&gt; - (Python standard library) Foreign Function Interface for Python calling C code.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mathema.tician.de/software/pycuda/" rel="nofollow"&gt;PyCUDA&lt;/a&gt; - A Python wrapper for Nvidia's CUDA API.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.swig.org/Doc1.3/Python.html" rel="nofollow"&gt;SWIG&lt;/a&gt; - Simplified Wrapper and Interface Generator.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-forms" class="anchor" aria-hidden="true" href="#forms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Forms&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for working with forms.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Pylons/deform"&gt;Deform&lt;/a&gt; - Python HTML form generation library influenced by the formish form generation library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dyve/django-bootstrap3"&gt;django-bootstrap3&lt;/a&gt; - Bootstrap 3 integration with Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zostera/django-bootstrap4"&gt;django-bootstrap4&lt;/a&gt; - Bootstrap 4 integration with Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/django-crispy-forms/django-crispy-forms"&gt;django-crispy-forms&lt;/a&gt; - A Django app which lets you create beautiful forms in a very elegant and DRY way.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WiserTogether/django-remote-forms"&gt;django-remote-forms&lt;/a&gt; - A platform independent Django form serializer.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wtforms/wtforms"&gt;WTForms&lt;/a&gt; - A flexible forms validation and rendering library.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-functional-programming" class="anchor" aria-hidden="true" href="#functional-programming"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Functional Programming&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Functional Programming with Python.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://coconut-lang.org/" rel="nofollow"&gt;Coconut&lt;/a&gt; - Coconut is a variant of Python built for simple, elegant, Pythonic functional programming.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pytoolz/cytoolz/"&gt;CyToolz&lt;/a&gt; - Cython implementation of Toolz: High performance functional utilities.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kachayev/fn.py"&gt;fn.py&lt;/a&gt; - Functional programming in Python: implementation of missing features to enjoy FP.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Suor/funcy"&gt;funcy&lt;/a&gt; - A fancy and practical functional tools.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pytoolz/toolz"&gt;Toolz&lt;/a&gt; - A collection of functional utilities for iterators, functions, and dictionaries.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-gui-development" class="anchor" aria-hidden="true" href="#gui-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GUI Development&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for working with graphical user interface applications.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/curses.html" rel="nofollow"&gt;curses&lt;/a&gt; - Built-in wrapper for &lt;a href="http://www.gnu.org/software/ncurses/" rel="nofollow"&gt;ncurses&lt;/a&gt; used to create terminal GUI applications.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ChrisKnott/Eel"&gt;Eel&lt;/a&gt; - A library for making simple Electron-like offline HTML/JS GUI apps.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nucleic/enaml"&gt;enaml&lt;/a&gt; - Creating beautiful user-interfaces with Declarative Syntax like QML.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zoofIO/flexx"&gt;Flexx&lt;/a&gt; - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chriskiehl/Gooey"&gt;Gooey&lt;/a&gt; - Turn command line programs into a full GUI application with one line.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kivy.org/" rel="nofollow"&gt;kivy&lt;/a&gt; - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bitbucket.org/pyglet/pyglet/wiki/Home" rel="nofollow"&gt;pyglet&lt;/a&gt; - A cross-platform windowing and multimedia library for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wiki.gnome.org/Projects/PyGObject" rel="nofollow"&gt;PyGObject&lt;/a&gt; - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://riverbankcomputing.com/software/pyqt/intro" rel="nofollow"&gt;PyQt&lt;/a&gt; - Python bindings for the &lt;a href="https://www.qt.io/" rel="nofollow"&gt;Qt&lt;/a&gt; cross-platform application and UI framework.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PySimpleGUI/PySimpleGUI"&gt;PySimpleGUI&lt;/a&gt; - Wrapper for tkinter, Qt, WxPython and Remi.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/r0x0r/pywebview/"&gt;pywebview&lt;/a&gt; - A lightweight cross-platform native wrapper around a webview component.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wiki.python.org/moin/TkInter" rel="nofollow"&gt;Tkinter&lt;/a&gt; - Tkinter is Python's de-facto standard GUI package.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pybee/toga"&gt;Toga&lt;/a&gt; - A Python native, OS native GUI toolkit.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://urwid.org/" rel="nofollow"&gt;urwid&lt;/a&gt; - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wxpython.org/" rel="nofollow"&gt;wxPython&lt;/a&gt; - A blending of the wxWidgets C++ class library with the Python.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-graphql" class="anchor" aria-hidden="true" href="#graphql"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GraphQL&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for working with GraphQL.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://tartiflette.io" rel="nofollow"&gt;tartiflette&lt;/a&gt; - SDL-first GraphQL engine implementation for Python 3.6+ and asyncio.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tartiflette/tartiflette-aiohttp/"&gt;tartiflette-aiohttp&lt;/a&gt; - An &lt;code&gt;aiohttp&lt;/code&gt;-based wrapper for Tartiflette to expose GraphQL APIs over HTTP.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tartiflette/tartiflette-asgi/"&gt;tartiflette-asgi&lt;/a&gt; - ASGI support for the Tartiflette GraphQL engine.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-game-development" class="anchor" aria-hidden="true" href="#game-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Game Development&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Awesome game development libraries.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://cocos2d.org/" rel="nofollow"&gt;Cocos2d&lt;/a&gt; - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.harfang3d.com" rel="nofollow"&gt;Harfang3D&lt;/a&gt; - Python framework for 3D, VR and game development.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.panda3d.org/" rel="nofollow"&gt;Panda3D&lt;/a&gt; - 3D game engine developed by Disney.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.pygame.org/news.html" rel="nofollow"&gt;Pygame&lt;/a&gt; - Pygame is a set of Python modules designed for writing games.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ogre3d.org/tikiwiki/PyOgre" rel="nofollow"&gt;PyOgre&lt;/a&gt; - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pyopengl.sourceforge.net/" rel="nofollow"&gt;PyOpenGL&lt;/a&gt; - Python ctypes bindings for OpenGL and it's related APIs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pysdl2.readthedocs.io" rel="nofollow"&gt;PySDL2&lt;/a&gt; - A ctypes based wrapper for the SDL2 library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.renpy.org/" rel="nofollow"&gt;RenPy&lt;/a&gt; - A Visual Novel engine.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-geolocation" class="anchor" aria-hidden="true" href="#geolocation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Geolocation&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for geocoding addresses and working with latitudes and longitudes.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/SmileyChris/django-countries"&gt;django-countries&lt;/a&gt; - A Django app that provides a country field for models and forms.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.djangoproject.com/en/dev/ref/contrib/gis/" rel="nofollow"&gt;GeoDjango&lt;/a&gt; - A world-class geographic web framework.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/maxmind/geoip-api-python"&gt;GeoIP&lt;/a&gt; - Python API for MaxMind GeoIP Legacy Database.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/frewsxcv/python-geojson"&gt;geojson&lt;/a&gt; - Python bindings and utilities for GeoJSON.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/geopy/geopy"&gt;geopy&lt;/a&gt; - Python Geocoding Toolbox.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/appliedsec/pygeoip"&gt;pygeoip&lt;/a&gt; - Pure Python GeoIP API.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-html-manipulation" class="anchor" aria-hidden="true" href="#html-manipulation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTML Manipulation&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for working with HTML and XML.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="nofollow"&gt;BeautifulSoup&lt;/a&gt; - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mozilla/bleach"&gt;bleach&lt;/a&gt; - A whitelist-based HTML sanitization and text linkification library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/cssutils/" rel="nofollow"&gt;cssutils&lt;/a&gt; - A CSS library for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/html5lib/html5lib-python"&gt;html5lib&lt;/a&gt; - A standards-compliant library for parsing and serializing HTML documents and fragments.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://lxml.de/" rel="nofollow"&gt;lxml&lt;/a&gt; - A very fast, easy-to-use and versatile library for handling HTML and XML.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pallets/markupsafe"&gt;MarkupSafe&lt;/a&gt; - Implements a XML/HTML/XHTML Markup safe string for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/gawel/pyquery"&gt;pyquery&lt;/a&gt; - A jQuery-like library for parsing HTML.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/stchris/untangle"&gt;untangle&lt;/a&gt; - Converts XML documents to Python objects for easy access.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://weasyprint.org" rel="nofollow"&gt;WeasyPrint&lt;/a&gt; - A visual rendering engine for HTML and CSS that can export to PDF.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://xmldataset.readthedocs.io/en/latest/" rel="nofollow"&gt;xmldataset&lt;/a&gt; - Simple XML Parsing.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/martinblech/xmltodict"&gt;xmltodict&lt;/a&gt; - Working with XML feel like you are working with JSON.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-http-clients" class="anchor" aria-hidden="true" href="#http-clients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTTP Clients&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for working with HTTP.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/kennethreitz/grequests"&gt;grequests&lt;/a&gt; - requests + gevent for asynchronous HTTP requests.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/httplib2/httplib2"&gt;httplib2&lt;/a&gt; - Comprehensive HTTP client library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://requests.kennethreitz.org/en/master/" rel="nofollow"&gt;requests&lt;/a&gt; - HTTP Requests for Humans.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/twisted/treq"&gt;treq&lt;/a&gt; - Python requests like API built on top of Twisted's HTTP client.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/shazow/urllib3"&gt;urllib3&lt;/a&gt; - A HTTP library with thread-safe connection pooling, file post support, sanity friendly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-hardware" class="anchor" aria-hidden="true" href="#hardware"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hardware&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for programming with hardware.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://inotool.org/" rel="nofollow"&gt;ino&lt;/a&gt; - Command line toolkit for working with &lt;a href="https://www.arduino.cc/" rel="nofollow"&gt;Arduino&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/boppreh/keyboard"&gt;keyboard&lt;/a&gt; - Hook and simulate global keyboard events on Windows and Linux.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/boppreh/mouse"&gt;mouse&lt;/a&gt; - Hook and simulate global mouse events on Windows and Linux.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.pingo.io/" rel="nofollow"&gt;Pingo&lt;/a&gt; - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SavinaRoja/PyUserInput"&gt;PyUserInput&lt;/a&gt; - A module for cross-platform control of the mouse and keyboard.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/secdev/scapy"&gt;scapy&lt;/a&gt; - A brilliant packet manipulation library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rockymeza/wifi"&gt;wifi&lt;/a&gt; - A Python library and command line tool for working with WiFi on Linux.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-image-processing" class="anchor" aria-hidden="true" href="#image-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Processing&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for manipulating images.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/rossgoodwin/hmap"&gt;hmap&lt;/a&gt; - Image histogram remapping.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sourceforge.net/projects/imgseek/" rel="nofollow"&gt;imgSeek&lt;/a&gt; - A project for searching a collection of images using visual similarity.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hhatto/nude.py"&gt;nude.py&lt;/a&gt; - Nudity detection.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/daboth/pagan"&gt;pagan&lt;/a&gt; - Retro identicon (Avatar) generation based on input string and hash.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/python-pillow/Pillow"&gt;pillow&lt;/a&gt; - Pillow is the friendly &lt;a href="http://www.pythonware.com/products/pil/" rel="nofollow"&gt;PIL&lt;/a&gt; fork.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pythonhosted.org/pyBarcode/" rel="nofollow"&gt;pyBarcode&lt;/a&gt; - Create barcodes in Python without needing PIL.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ajkumar25/pygram"&gt;pygram&lt;/a&gt; - Instagram-like image filters.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lincolnloop/python-qrcode"&gt;python-qrcode&lt;/a&gt; - A pure Python QR Code generator.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fogleman/Quads"&gt;Quads&lt;/a&gt; - Computer art based on quadtrees.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-image.org/" rel="nofollow"&gt;scikit-image&lt;/a&gt; - A Python library for (scientific) image processing.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/thumbor/thumbor"&gt;thumbor&lt;/a&gt; - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dahlia/wand"&gt;wand&lt;/a&gt; - Python bindings for &lt;a href="http://www.imagemagick.org/script/magick-wand.php" rel="nofollow"&gt;MagickWand&lt;/a&gt;, C API for ImageMagick.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-implementations" class="anchor" aria-hidden="true" href="#implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Implementations of Python.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/python/cpython"&gt;CPython&lt;/a&gt; - &lt;strong&gt;Default, most widely used implementation of the Python programming language written in C.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cython.org/" rel="nofollow"&gt;Cython&lt;/a&gt; - Optimizing Static Compiler for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/metawilm/cl-python"&gt;CLPython&lt;/a&gt; - Implementation of the Python programming language written in Common Lisp.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/grumpy"&gt;Grumpy&lt;/a&gt; - More compiler than interpreter as more powerful CPython2.7 replacement (alpha).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/IronLanguages/ironpython3"&gt;IronPython&lt;/a&gt; - Implementation of the Python programming language written in C#.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://hg.python.org/jython" rel="nofollow"&gt;Jython&lt;/a&gt; - Implementation of Python programming language written in Java for the JVM.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/micropython/micropython"&gt;MicroPython&lt;/a&gt; - A lean and efficient Python programming language implementation.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://numba.pydata.org/" rel="nofollow"&gt;Numba&lt;/a&gt; - Python JIT compiler to LLVM aimed at scientific Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Maratyszcza/PeachPy"&gt;PeachPy&lt;/a&gt; - x86-64 assembler embedded in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Microsoft/Pyjion"&gt;Pyjion&lt;/a&gt; - A JIT for Python based upon CoreCLR.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bitbucket.org/pypy/pypy" rel="nofollow"&gt;PyPy&lt;/a&gt; - A very fast and compliant implementation of the Python language.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dropbox/pyston"&gt;Pyston&lt;/a&gt; - A Python implementation using JIT techniques.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/stackless-dev/stackless"&gt;Stackless Python&lt;/a&gt; - An enhanced version of the Python programming language.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-interactive-interpreter" class="anchor" aria-hidden="true" href="#interactive-interpreter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Interactive Interpreter&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Interactive Python interpreters (REPL).&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/bpython/bpython"&gt;bpython&lt;/a&gt; - A fancy interface to the Python interpreter.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jupyter.org" rel="nofollow"&gt;Jupyter Notebook (IPython)&lt;/a&gt; - A rich toolkit to help you make the most out of using Python interactively.
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/markusschanta/awesome-jupyter"&gt;awesome-jupyter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jonathanslenders/ptpython"&gt;ptpython&lt;/a&gt; - Advanced Python REPL built on top of the &lt;a href="https://github.com/jonathanslenders/python-prompt-toolkit"&gt;python-prompt-toolkit&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-internationalization" class="anchor" aria-hidden="true" href="#internationalization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Internationalization&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for working with i18n.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://babel.pocoo.org/en/latest/" rel="nofollow"&gt;Babel&lt;/a&gt; - An internationalization library for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ovalhub/pyicu"&gt;PyICU&lt;/a&gt; - A wrapper of International Components for Unicode C++ library (&lt;a href="http://site.icu-project.org/" rel="nofollow"&gt;ICU&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-job-scheduler" class="anchor" aria-hidden="true" href="#job-scheduler"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Job Scheduler&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for scheduling jobs.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://apscheduler.readthedocs.io/en/latest/" rel="nofollow"&gt;APScheduler&lt;/a&gt; - A light but powerful in-process task scheduler that lets you schedule functions.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/thauber/django-schedule"&gt;django-schedule&lt;/a&gt; - A calendaring app for Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pydoit.org/" rel="nofollow"&gt;doit&lt;/a&gt; - A task runner and build tool.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/gunnery/gunnery"&gt;gunnery&lt;/a&gt; - Multipurpose task execution tool for distributed systems with web-based interface.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://joblib.readthedocs.io/" rel="nofollow"&gt;Joblib&lt;/a&gt; - A set of tools to provide lightweight pipelining in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fengsp/plan"&gt;Plan&lt;/a&gt; - Writing crontab file in Python like a charm.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dbader/schedule"&gt;schedule&lt;/a&gt; - Python job scheduling for humans.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/knipknap/SpiffWorkflow"&gt;Spiff&lt;/a&gt; - A powerful workflow engine implemented in pure Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.openstack.org/developer/taskflow/" rel="nofollow"&gt;TaskFlow&lt;/a&gt; - A Python library that helps to make task execution easy, consistent and reliable.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://airflow.apache.org/" rel="nofollow"&gt;Airflow&lt;/a&gt; - Airflow is a platform to programmatically author, schedule and monitor workflows.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-logging" class="anchor" aria-hidden="true" href="#logging"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Logging&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for generating and working with logs.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ScatterHQ/eliot"&gt;Eliot&lt;/a&gt; - Logging for complex &amp;amp; distributed systems.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://logbook.readthedocs.io/en/stable/" rel="nofollow"&gt;logbook&lt;/a&gt; - Logging replacement for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/logging.html" rel="nofollow"&gt;logging&lt;/a&gt; - (Python standard library) Logging facility for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/raven-python"&gt;raven&lt;/a&gt; - Python client for Sentry, a log/error tracking, crash reporting and aggregation platform for web applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-machine-learning" class="anchor" aria-hidden="true" href="#machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for Machine Learning. Also see &lt;a href="https://github.com/josephmisiti/awesome-machine-learning#python"&gt;awesome-machine-learning&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/h2oai/h2o-3"&gt;H2O&lt;/a&gt; - Open Source Fast Scalable Machine Learning Platform.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/benhamner/Metrics"&gt;Metrics&lt;/a&gt; - Machine learning evaluation metrics.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/numenta/nupic"&gt;NuPIC&lt;/a&gt; - Numenta Platform for Intelligent Computing.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/" rel="nofollow"&gt;scikit-learn&lt;/a&gt; - The most popular Python library for Machine Learning.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark.apache.org/docs/latest/ml-guide.html" rel="nofollow"&gt;Spark ML&lt;/a&gt; - &lt;a href="http://spark.apache.org/" rel="nofollow"&gt;Apache Spark&lt;/a&gt;'s scalable Machine Learning library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/josephreisinger/vowpal_porpoise"&gt;vowpal_porpoise&lt;/a&gt; - A lightweight Python wrapper for &lt;a href="https://github.com/JohnLangford/vowpal_wabbit/"&gt;Vowpal Wabbit&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dmlc/xgboost"&gt;xgboost&lt;/a&gt; - A scalable, portable, and distributed gradient boosting library.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-microsoft-windows" class="anchor" aria-hidden="true" href="#microsoft-windows"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Microsoft Windows&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Python programming on Microsoft Windows.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://python-xy.github.io/" rel="nofollow"&gt;Python(x,y)&lt;/a&gt; - Scientific-applications-oriented Python Distribution based on Qt and Spyder.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/" rel="nofollow"&gt;pythonlibs&lt;/a&gt; - Unofficial Windows binaries for Python extension packages.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pythonnet/pythonnet"&gt;PythonNet&lt;/a&gt; - Python Integration with the .NET Common Language Runtime (CLR).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mhammond/pywin32"&gt;PyWin32&lt;/a&gt; - Python Extensions for Windows.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://winpython.github.io/" rel="nofollow"&gt;WinPython&lt;/a&gt; - Portable development environment for Windows 7/8.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-miscellaneous" class="anchor" aria-hidden="true" href="#miscellaneous"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Miscellaneous&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Useful libraries or tools that don't fit in the categories above.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jek/blinker"&gt;blinker&lt;/a&gt; - A fast Python in-process signal/event dispatching system.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mahmoud/boltons"&gt;boltons&lt;/a&gt; - A set of pure-Python utilities.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pallets/itsdangerous"&gt;itsdangerous&lt;/a&gt; - Various helpers to pass trusted data to untrusted environments.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mitsuhiko/pluginbase"&gt;pluginbase&lt;/a&gt; - A simple but flexible plugin system for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.tryton.org/" rel="nofollow"&gt;tryton&lt;/a&gt; - A general purpose business framework.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-natural-language-processing" class="anchor" aria-hidden="true" href="#natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natural Language Processing&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for working with human languages.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;General
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/RaRe-Technologies/gensim"&gt;gensim&lt;/a&gt; - Topic Modeling for Humans.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/saffsd/langid.py"&gt;langid.py&lt;/a&gt; - Stand-alone language identification system.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nltk.org/" rel="nofollow"&gt;nltk&lt;/a&gt; - A leading platform for building Python programs to work with human language data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/clips/pattern"&gt;pattern&lt;/a&gt; - A web mining module for the Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/aboSamoor/polyglot"&gt;polyglot&lt;/a&gt; - Natural language pipeline supporting hundreds of languages.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/pytext"&gt;pytext&lt;/a&gt; - A natural language modeling framework based on PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PetrochukM/PyTorch-NLP"&gt;PyTorch-NLP&lt;/a&gt; - A toolkit enabling rapid deep learning NLP prototyping for research.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spacy.io/" rel="nofollow"&gt;spacy&lt;/a&gt; - A library for industrial-strength natural language processing in Python and Cython.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/stanfordnlp/stanfordnlp"&gt;stanfordnlp&lt;/a&gt; - The Stanford NLP Group's official Python library, supporting 50+ languages.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Chinese
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/fxsjy/jieba"&gt;jieba&lt;/a&gt; - The most popular Chinese text segmentation library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lancopku/pkuseg-python"&gt;pkuseg-python&lt;/a&gt; - A toolkit for Chinese word segmentation in various domains.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/isnowfy/snownlp"&gt;snownlp&lt;/a&gt; - A library for processing Chinese text.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fighting41love/funNLP"&gt;funNLP&lt;/a&gt; - A collection of tools and datasets for Chinese NLP.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-network-virtualization" class="anchor" aria-hidden="true" href="#network-virtualization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network Virtualization&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Tools and libraries for Virtual Networking and SDN (Software Defined Networking).&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/mininet/mininet"&gt;mininet&lt;/a&gt; - A popular network emulator and API written in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/napalm-automation/napalm"&gt;napalm&lt;/a&gt; - Cross-vendor API to manipulate network devices.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/noxrepo/pox"&gt;pox&lt;/a&gt; - A Python-based SDN control applications, such as OpenFlow SDN controllers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-news-feed" class="anchor" aria-hidden="true" href="#news-feed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;News Feed&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for building user's activities.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/justquick/django-activity-stream"&gt;django-activity-stream&lt;/a&gt; - Generating generic activity streams from the actions on your site.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tschellenbach/Stream-Framework"&gt;Stream Framework&lt;/a&gt; - Building news feed and notification systems using Cassandra and Redis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-orm" class="anchor" aria-hidden="true" href="#orm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ORM&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries that implement Object-Relational Mapping or data mapping techniques.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Relational Databases
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.djangoproject.com/en/dev/topics/db/models/" rel="nofollow"&gt;Django Models&lt;/a&gt; - The Django ORM.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.sqlalchemy.org/" rel="nofollow"&gt;SQLAlchemy&lt;/a&gt; - The Python SQL Toolkit and Object Relational Mapper.
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/dahlia/awesome-sqlalchemy"&gt;awesome-sqlalchemy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pudo/dataset"&gt;dataset&lt;/a&gt; - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sdispater/orator"&gt;orator&lt;/a&gt; -  The Orator ORM provides a simple yet beautiful ActiveRecord implementation.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/encode/orm"&gt;orm&lt;/a&gt; - An async ORM.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/coleifer/peewee"&gt;peewee&lt;/a&gt; - A small, expressive ORM.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ponyorm/pony/"&gt;pony&lt;/a&gt; - ORM that provides a generator-oriented interface to SQL.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/web2py/pydal/"&gt;pydal&lt;/a&gt; - A pure Python Database Abstraction Layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NoSQL Databases
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/stephenmcd/hot-redis"&gt;hot-redis&lt;/a&gt; - Rich Python data types for Redis.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MongoEngine/mongoengine"&gt;mongoengine&lt;/a&gt; - A Python Object-Document-Mapper for working with MongoDB.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pynamodb/PynamoDB"&gt;PynamoDB&lt;/a&gt; - A Pythonic interface for &lt;a href="https://aws.amazon.com/dynamodb/" rel="nofollow"&gt;Amazon DynamoDB&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kiddouk/redisco"&gt;redisco&lt;/a&gt; - A Python Library for Simple Models and Containers Persisted in Redis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-package-management" class="anchor" aria-hidden="true" href="#package-management"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Package Management&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for package and dependency management.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pip.pypa.io/en/stable/" rel="nofollow"&gt;pip&lt;/a&gt; - The package installer for Python.
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pypi.org/" rel="nofollow"&gt;PyPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jazzband/pip-tools"&gt;pip-tools&lt;/a&gt; - A set of tools to keep your pinned Python dependencies fresh.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sdispater/poetry"&gt;poetry&lt;/a&gt; - Python dependency management and packaging made easy.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/conda/conda/"&gt;conda&lt;/a&gt; - Cross-platform, Python-agnostic binary package manager.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-package-repositories" class="anchor" aria-hidden="true" href="#package-repositories"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Package Repositories&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Local PyPI repository server and proxies.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/pypa/warehouse"&gt;warehouse&lt;/a&gt; - Next generation Python Package Repository (PyPI).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pypa/bandersnatch/"&gt;bandersnatch&lt;/a&gt; - PyPI mirroring tool provided by Python Packaging Authority (PyPA).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/devpi/devpi"&gt;devpi&lt;/a&gt; - PyPI server and packaging/testing/release tool.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jazzband/localshop"&gt;localshop&lt;/a&gt; - Local PyPI server (custom packages and auto-mirroring of pypi).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-permissions" class="anchor" aria-hidden="true" href="#permissions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Permissions&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries that allow or deny users access to data or functionality.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/django-guardian/django-guardian"&gt;django-guardian&lt;/a&gt; - Implementation of per object permissions for Django 1.2+&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dfunckt/django-rules"&gt;django-rules&lt;/a&gt; - A tiny but powerful app providing object-level permissions to Django, without requiring a database.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-processes" class="anchor" aria-hidden="true" href="#processes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Processes&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for starting and communicating with OS processes.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/amitt001/delegator.py"&gt;delegator.py&lt;/a&gt; - &lt;a href="https://docs.python.org/3/library/subprocess.html" rel="nofollow"&gt;Subprocesses&lt;/a&gt; for Humans 2.0.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sarge.readthedocs.io/en/latest/" rel="nofollow"&gt;sarge&lt;/a&gt; - Yet another wrapper for subprocess.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/amoffat/sh"&gt;sh&lt;/a&gt; - A full-fledged subprocess replacement for Python.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-recommender-systems" class="anchor" aria-hidden="true" href="#recommender-systems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recommender Systems&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for building recommender systems.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/spotify/annoy"&gt;annoy&lt;/a&gt; - Approximate Nearest Neighbors in C++/Python optimized for memory usage.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ibayer/fastFM"&gt;fastFM&lt;/a&gt; - A library for Factorization Machines.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/benfred/implicit"&gt;implicit&lt;/a&gt; - A fast Python implementation of collaborative filtering for implicit datasets.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/guestwalk/libffm"&gt;libffm&lt;/a&gt; - A library for Field-aware Factorization Machine (FFM).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lyst/lightfm"&gt;lightfm&lt;/a&gt; - A Python implementation of a number of popular recommendation algorithms.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/maciejkula/spotlight"&gt;spotlight&lt;/a&gt; - Deep recommender models using PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/NicolasHug/Surprise"&gt;Surprise&lt;/a&gt; - A scikit for building and analyzing recommender systems.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jfkirk/tensorrec"&gt;tensorrec&lt;/a&gt; - A Recommendation Engine Framework in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-refactoring" class="anchor" aria-hidden="true" href="#refactoring"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Refactoring&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Refactoring tools and libraries for Python&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://bicyclerepair.sourceforge.net/" rel="nofollow"&gt;Bicycle Repair Man&lt;/a&gt; - Bicycle Repair Man, a refactoring tool for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pybowler.io/" rel="nofollow"&gt;Bowler&lt;/a&gt; - Safe code refactoring for modern Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/python-rope/rope"&gt;Rope&lt;/a&gt; -  Rope is a python refactoring library.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-restful-api" class="anchor" aria-hidden="true" href="#restful-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RESTful API&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for building RESTful APIs.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Django
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.django-rest-framework.org/" rel="nofollow"&gt;django-rest-framework&lt;/a&gt; - A powerful and flexible toolkit to build web APIs.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tastypieapi.org/" rel="nofollow"&gt;django-tastypie&lt;/a&gt; - Creating delicious APIs for Django apps.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Flask
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/pyeve/eve"&gt;eve&lt;/a&gt; - REST API framework powered by Flask, MongoDB and good intentions.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/flask-api/flask-api"&gt;flask-api&lt;/a&gt; - Browsable Web APIs for Flask.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/flask-restful/flask-restful"&gt;flask-restful&lt;/a&gt; - Quickly building REST APIs for Flask.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pyramid
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Cornices/cornice"&gt;cornice&lt;/a&gt; - A RESTful framework for Pyramid.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Framework agnostic
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/encode/apistar"&gt;apistar&lt;/a&gt; - A smart Web API framework, designed for Python 3.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/falconry/falcon"&gt;falcon&lt;/a&gt; - A high-performance framework for building cloud APIs and web app backends.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tiangolo/fastapi"&gt;fastapi&lt;/a&gt; - A modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hugapi/hug"&gt;hug&lt;/a&gt; - A Python 3 framework for cleanly exposing APIs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jeffknupp/sandman2"&gt;sandman2&lt;/a&gt; - Automated REST APIs for existing database-driven systems.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huge-success/sanic"&gt;sanic&lt;/a&gt; - A Python 3.6+ web server and web framework that's written to go fast.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://vibora.io/" rel="nofollow"&gt;vibora&lt;/a&gt; - Fast, efficient and asynchronous Web framework inspired by Flask.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-robotics" class="anchor" aria-hidden="true" href="#robotics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Robotics&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for robotics.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics"&gt;PythonRobotics&lt;/a&gt; - This is a compilation of various robotics algorithms with visualizations.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://wiki.ros.org/rospy" rel="nofollow"&gt;rospy&lt;/a&gt; - This is a library for ROS (Robot Operating System).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-rpc-servers" class="anchor" aria-hidden="true" href="#rpc-servers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RPC Servers&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;RPC-compatible servers.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/0rpc/zerorpc-python"&gt;zeroRPC&lt;/a&gt; - zerorpc is a flexible RPC implementation based on &lt;a href="http://zeromq.org/" rel="nofollow"&gt;ZeroMQ&lt;/a&gt; and &lt;a href="http://msgpack.org/" rel="nofollow"&gt;MessagePack&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-science" class="anchor" aria-hidden="true" href="#science"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Science&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for scientific computing. Also see &lt;a href="https://github.com/TomNicholas/Python-for-Scientists"&gt;Python-for-Scientists&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.astropy.org/" rel="nofollow"&gt;astropy&lt;/a&gt; - A community Python library for Astronomy.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chapmanb/bcbio-nextgen"&gt;bcbio-nextgen&lt;/a&gt; - Providing best-practice pipelines for fully automated high throughput sequencing analysis.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chapmanb/bcbb"&gt;bccb&lt;/a&gt; - Collection of useful code related to biological analysis.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://biopython.org/wiki/Main_Page" rel="nofollow"&gt;Biopython&lt;/a&gt; - Biopython is a set of freely available tools for biological computation.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cclib.github.io/" rel="nofollow"&gt;cclib&lt;/a&gt; - A library for parsing and interpreting the results of computational chemistry packages.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://colour-science.org/" rel="nofollow"&gt;Colour&lt;/a&gt; - Implementing a comprehensive number of colour theory transformations and algorithms.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://networkx.github.io/" rel="nofollow"&gt;NetworkX&lt;/a&gt; - A high-productivity software for complex networks.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nipy.org" rel="nofollow"&gt;NIPY&lt;/a&gt; - A collection of neuroimaging toolkits.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.numpy.org/" rel="nofollow"&gt;NumPy&lt;/a&gt; - A fundamental package for scientific computing with Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://openbabel.org/wiki/Main_Page" rel="nofollow"&gt;Open Babel&lt;/a&gt; - A chemical toolbox designed to speak the many languages of chemical data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/obspy/obspy/wiki/"&gt;ObsPy&lt;/a&gt; - A Python toolbox for seismology.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.pydy.org/" rel="nofollow"&gt;PyDy&lt;/a&gt; - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pymc-devs/pymc3"&gt;PyMC&lt;/a&gt; - Markov Chain Monte Carlo sampling toolkit.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://qutip.org/" rel="nofollow"&gt;QuTiP&lt;/a&gt; - Quantum Toolbox in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.rdkit.org/" rel="nofollow"&gt;RDKit&lt;/a&gt; - Cheminformatics and Machine Learning Software.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.scipy.org/" rel="nofollow"&gt;SciPy&lt;/a&gt; - A Python-based ecosystem of open-source software for mathematics, science, and engineering.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/statsmodels/statsmodels"&gt;statsmodels&lt;/a&gt; - Statistical modeling and econometrics in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sympy/sympy"&gt;SymPy&lt;/a&gt; - A Python library for symbolic mathematics.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/quantopian/zipline"&gt;Zipline&lt;/a&gt; - A Pythonic algorithmic trading library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bitbucket.org/simpy/simpy" rel="nofollow"&gt;SimPy&lt;/a&gt; -  A process-based discrete-event simulation framework.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-search" class="anchor" aria-hidden="true" href="#search"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Search&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries and software for indexing and performing search queries on data.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html" rel="nofollow"&gt;elasticsearch-py&lt;/a&gt; - The official low-level Python client for &lt;a href="https://www.elastic.co/products/elasticsearch" rel="nofollow"&gt;Elasticsearch&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/elastic/elasticsearch-dsl-py"&gt;elasticsearch-dsl-py&lt;/a&gt; - The official high-level Python client for Elasticsearch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/django-haystack/django-haystack"&gt;django-haystack&lt;/a&gt; - Modular search for Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/django-haystack/pysolr"&gt;pysolr&lt;/a&gt; - A lightweight Python wrapper for &lt;a href="https://lucene.apache.org/solr/" rel="nofollow"&gt;Apache Solr&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://whoosh.readthedocs.io/en/latest/" rel="nofollow"&gt;whoosh&lt;/a&gt; - A fast, pure Python search engine library.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-serialization" class="anchor" aria-hidden="true" href="#serialization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Serialization&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for serializing complex data types&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/marshmallow-code/marshmallow"&gt;marshmallow&lt;/a&gt; - A lightweight library for converting complex objects to and from simple Python datatypes.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/TkTech/pysimdjson"&gt;pysimdjson&lt;/a&gt; - A Python bindings for &lt;a href="https://github.com/lemire/simdjson"&gt;simdjson&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/python-rapidjson/python-rapidjson"&gt;python-rapidjson&lt;/a&gt; - A Python wrapper around &lt;a href="https://github.com/Tencent/rapidjson"&gt;RapidJSON&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/esnme/ultrajson"&gt;ultrajson&lt;/a&gt; - A fast JSON decoder and encoder written in C with Python bindings.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-serverless-frameworks" class="anchor" aria-hidden="true" href="#serverless-frameworks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Serverless Frameworks&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Frameworks for developing serverless Python code.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nficano/python-lambda"&gt;python-lambda&lt;/a&gt; - A toolkit for developing and deploying Python code in AWS Lambda.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Miserlou/Zappa"&gt;Zappa&lt;/a&gt; - A tool for deploying WSGI applications on AWS Lambda and API Gateway.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-specific-formats-processing" class="anchor" aria-hidden="true" href="#specific-formats-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Specific Formats Processing&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for parsing and manipulating specific text formats.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;General
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/kennethreitz/tablib"&gt;tablib&lt;/a&gt; - A module for Tabular Datasets in XLS, CSV, JSON, YAML.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Office
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://openpyxl.readthedocs.io/en/stable/" rel="nofollow"&gt;openpyxl&lt;/a&gt; - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pyexcel/pyexcel"&gt;pyexcel&lt;/a&gt; - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/python-openxml/python-docx"&gt;python-docx&lt;/a&gt; - Reads, queries and modifies Microsoft Word 2007/2008 docx files.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/scanny/python-pptx"&gt;python-pptx&lt;/a&gt; - Python library for creating and updating PowerPoint (.pptx) files.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/unoconv/unoconv"&gt;unoconv&lt;/a&gt; - Convert between any document format supported by LibreOffice/OpenOffice.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jmcnamara/XlsxWriter"&gt;XlsxWriter&lt;/a&gt; - A Python module for creating Excel .xlsx files.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ZoomerAnalytics/xlwings"&gt;xlwings&lt;/a&gt; - A BSD-licensed library that makes it easy to call Python from Excel and vice versa.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/python-excel/xlwt"&gt;xlwt&lt;/a&gt; / &lt;a href="https://github.com/python-excel/xlrd"&gt;xlrd&lt;/a&gt; - Writing and reading data and formatting information from Excel files.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PDF
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/euske/pdfminer"&gt;PDFMiner&lt;/a&gt; - A tool for extracting information from PDF documents.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mstamy2/PyPDF2"&gt;PyPDF2&lt;/a&gt; - A library capable of splitting, merging and transforming PDF pages.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reportlab.com/opensource/" rel="nofollow"&gt;ReportLab&lt;/a&gt; - Allowing Rapid creation of rich PDF documents.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Markdown
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/lepture/mistune"&gt;Mistune&lt;/a&gt; - Fastest and full featured pure Python parsers of Markdown.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/waylan/Python-Markdown"&gt;Python-Markdown&lt;/a&gt; - A Python implementation of John Gruber’s Markdown.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;YAML
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://pyyaml.org/" rel="nofollow"&gt;PyYAML&lt;/a&gt; - YAML implementations for Python.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CSV
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/wireservice/csvkit"&gt;csvkit&lt;/a&gt; - Utilities for converting to and working with CSV.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Archive
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/mitsuhiko/unp"&gt;unp&lt;/a&gt; - A command line tool that can unpack archives easily.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-static-site-generator" class="anchor" aria-hidden="true" href="#static-site-generator"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Static Site Generator&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Static site generator is a software that takes some text + templates as input and produces HTML files on the output.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/mkdocs/mkdocs/"&gt;mkdocs&lt;/a&gt; - Markdown friendly documentation generator.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getpelican/pelican"&gt;pelican&lt;/a&gt; - Static site generator that supports Markdown and reST syntax.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lektor/lektor"&gt;lektor&lt;/a&gt; - An easy to use static CMS and blog engine.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getnikola/nikola"&gt;nikola&lt;/a&gt; - A static website and blog generator.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tagging" class="anchor" aria-hidden="true" href="#tagging"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tagging&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for tagging items.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jazzband/django-taggit"&gt;django-taggit&lt;/a&gt; - Simple tagging for Django.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-task-queues" class="anchor" aria-hidden="true" href="#task-queues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Task Queues&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for working with task queues.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.celeryproject.org/" rel="nofollow"&gt;celery&lt;/a&gt; - An asynchronous task queue/job queue based on distributed message passing.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/coleifer/huey"&gt;huey&lt;/a&gt; - Little multi-threaded task queue.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pricingassistant/mrq"&gt;mrq&lt;/a&gt; - A distributed worker task queue in Python using Redis &amp;amp; gevent.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rq/rq"&gt;rq&lt;/a&gt; - Simple job queues for Python.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-template-engine" class="anchor" aria-hidden="true" href="#template-engine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Template Engine&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries and tools for templating and lexing.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/pallets/jinja"&gt;Jinja2&lt;/a&gt; - A modern and designer friendly templating language.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://genshi.edgewall.org/" rel="nofollow"&gt;Genshi&lt;/a&gt; - Python templating toolkit for generation of web-aware output.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.makotemplates.org/" rel="nofollow"&gt;Mako&lt;/a&gt; - Hyperfast and lightweight templating for the Python platform.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for testing codebases and generating test data.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Testing Frameworks
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.pytest.org/en/latest/" rel="nofollow"&gt;pytest&lt;/a&gt; - A mature full-featured Python testing tool.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/HypothesisWorks/hypothesis"&gt;hypothesis&lt;/a&gt; - Hypothesis is an advanced Quickcheck style property based testing library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nose-devs/nose2"&gt;nose2&lt;/a&gt; - The successor to &lt;code&gt;nose&lt;/code&gt;, based on `unittest2.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robotframework/robotframework"&gt;Robot Framework&lt;/a&gt; - A generic test automation framework.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/unittest.html" rel="nofollow"&gt;unittest&lt;/a&gt; - (Python standard library) Unit testing framework.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Test Runners
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/CleanCut/green"&gt;green&lt;/a&gt; - A clean, colorful test runner.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nestorsalceda.github.io/mamba/" rel="nofollow"&gt;mamba&lt;/a&gt; - The definitive testing tool for Python. Born under the banner of BDD.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tox.readthedocs.io/en/latest/" rel="nofollow"&gt;tox&lt;/a&gt; - Auto builds and tests distributions in multiple Python versions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GUI / Web Testing
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/locustio/locust"&gt;locust&lt;/a&gt; - Scalable user load testing tool written in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/asweigart/pyautogui"&gt;PyAutoGUI&lt;/a&gt; - PyAutoGUI is a cross-platform GUI automation Python module for human beings.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/selenium/" rel="nofollow"&gt;Selenium&lt;/a&gt; - Python bindings for &lt;a href="http://www.seleniumhq.org/" rel="nofollow"&gt;Selenium&lt;/a&gt; WebDriver.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/seatgeek/sixpack"&gt;sixpack&lt;/a&gt; - A language-agnostic A/B Testing framework.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cobrateam/splinter"&gt;splinter&lt;/a&gt; - Open source tool for testing web applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mock
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/unittest.mock.html" rel="nofollow"&gt;mock&lt;/a&gt; - (Python standard library) A mocking and patching library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/doublex/" rel="nofollow"&gt;doublex&lt;/a&gt; - Powerful test doubles framework for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/spulec/freezegun"&gt;freezegun&lt;/a&gt; - Travel through time by mocking the datetime module.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/patrys/httmock"&gt;httmock&lt;/a&gt; - A mocking library for requests for Python 2.6+ and 3.2+.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/gabrielfalcao/HTTPretty"&gt;httpretty&lt;/a&gt; - HTTP request mock tool for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mindflayer/python-mocket"&gt;mocket&lt;/a&gt; - A socket mock framework with gevent/asyncio/SSL support.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/responses"&gt;responses&lt;/a&gt; - A utility library for mocking out the requests Python library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kevin1024/vcrpy"&gt;VCR.py&lt;/a&gt; - Record and replay HTTP interactions on your tests.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Object Factories
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/FactoryBoy/factory_boy"&gt;factory_boy&lt;/a&gt; - A test fixtures replacement for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/klen/mixer"&gt;mixer&lt;/a&gt; - Another fixtures replacement. Supported Django, Flask, SQLAlchemy, Peewee and etc.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vandersonmota/model_mommy"&gt;model_mommy&lt;/a&gt; - Creating random fixtures for testing in Django.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Code Coverage
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/coverage/" rel="nofollow"&gt;coverage&lt;/a&gt; - Code coverage measurement.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fake Data
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/lk-geimfari/mimesis"&gt;mimesis&lt;/a&gt; - is a Python library that help you generate fake data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/emirozer/fake2db"&gt;fake2db&lt;/a&gt; - Fake database generator.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/joke2k/faker"&gt;faker&lt;/a&gt; - A Python package that generates fake data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/radar/" rel="nofollow"&gt;radar&lt;/a&gt; - Generate random datetime / time.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-text-processing" class="anchor" aria-hidden="true" href="#text-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Text Processing&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for parsing and manipulating plain texts.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;General
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/chardet/chardet"&gt;chardet&lt;/a&gt; - Python 2/3 compatible character encoding detector.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/library/difflib.html" rel="nofollow"&gt;difflib&lt;/a&gt; - (Python standard library) Helpers for computing deltas.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/LuminosoInsight/python-ftfy"&gt;ftfy&lt;/a&gt; - Makes Unicode text less broken and more consistent automagically.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/seatgeek/fuzzywuzzy"&gt;fuzzywuzzy&lt;/a&gt; - Fuzzy String Matching.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ztane/python-Levenshtein/"&gt;Levenshtein&lt;/a&gt; - Fast computation of Levenshtein distance and string similarity.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vinta/pangu.py"&gt;pangu.py&lt;/a&gt; - Paranoid text spacing.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pwaller/pyfiglet"&gt;pyfiglet&lt;/a&gt; - An implementation of figlet written in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mozillazg/python-pinyin"&gt;pypinyin&lt;/a&gt; - Convert Chinese hanzi (漢字) to pinyin (拼音).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/orsinium/textdistance"&gt;textdistance&lt;/a&gt; - Compute distance between sequences with 30+ algorithms.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/Unidecode/" rel="nofollow"&gt;unidecode&lt;/a&gt; - ASCII transliterations of Unicode text.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Slugify
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/dimka665/awesome-slugify"&gt;awesome-slugify&lt;/a&gt; - A Python slugify library that can preserve unicode.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/un33k/python-slugify"&gt;python-slugify&lt;/a&gt; - A Python slugify library that translates unicode to ASCII.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mozilla/unicode-slugify"&gt;unicode-slugify&lt;/a&gt; - A slugifier that generates unicode slugs with Django as a dependency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unique identifiers
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/davidaurelio/hashids-python"&gt;hashids&lt;/a&gt; - Implementation of &lt;a href="http://hashids.org" rel="nofollow"&gt;hashids&lt;/a&gt; in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/skorokithakis/shortuuid"&gt;shortuuid&lt;/a&gt; - A generator library for concise, unambiguous and URL-safe UUIDs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parser
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/dabeaz/ply"&gt;ply&lt;/a&gt; - Implementation of lex and yacc parsing tools for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pygments.org/" rel="nofollow"&gt;pygments&lt;/a&gt; - A generic syntax highlighter.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pyparsing/pyparsing"&gt;pyparsing&lt;/a&gt; - A general purpose framework for generating parsers.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/derek73/python-nameparser"&gt;python-nameparser&lt;/a&gt; - Parsing human names into their individual components.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/daviddrysdale/python-phonenumbers"&gt;python-phonenumbers&lt;/a&gt; - Parsing, formatting, storing and validating international phone numbers.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/selwin/python-user-agents"&gt;python-user-agents&lt;/a&gt; - Browser user agent parser.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/andialbrecht/sqlparse"&gt;sqlparse&lt;/a&gt; - A non-validating SQL parser.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-third-party-apis" class="anchor" aria-hidden="true" href="#third-party-apis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Third-party APIs&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for accessing third party services APIs. Also see &lt;a href="https://github.com/realpython/list-of-python-api-wrappers"&gt;List of Python API Wrappers and Libraries&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://libcloud.apache.org/" rel="nofollow"&gt;apache-libcloud&lt;/a&gt; - One Python library for all clouds.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/boto/boto3"&gt;boto3&lt;/a&gt; - Python interface to Amazon Web Services.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/istrategylabs/django-wordpress"&gt;django-wordpress&lt;/a&gt; - WordPress models and views for Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mobolic/facebook-sdk"&gt;facebook-sdk&lt;/a&gt; - Facebook Platform Python SDK.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/google-api-python-client"&gt;google-api-python-client&lt;/a&gt; - Google APIs Client Library for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/burnash/gspread"&gt;gspread&lt;/a&gt; - Google Spreadsheets Python API.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ryanmcgrath/twython"&gt;twython&lt;/a&gt; - A Python wrapper for the Twitter API.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-url-manipulation" class="anchor" aria-hidden="true" href="#url-manipulation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;URL Manipulation&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for parsing URLs.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/gruns/furl"&gt;furl&lt;/a&gt; - A small Python library that makes parsing and manipulating URLs easy.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/codeinthehole/purl"&gt;purl&lt;/a&gt; - A simple, immutable URL class with a clean API for interrogation and manipulation.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ellisonleao/pyshorteners"&gt;pyshorteners&lt;/a&gt; - A pure Python URL shortening lib.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/marshmallow-code/webargs"&gt;webargs&lt;/a&gt; - A friendly library for parsing HTTP request arguments with built-in support for popular web frameworks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-video" class="anchor" aria-hidden="true" href="#video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for manipulating video and GIFs.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://zulko.github.io/moviepy/" rel="nofollow"&gt;moviepy&lt;/a&gt; - A module for script-based movie editing with many formats, including animated GIFs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/aizvorski/scikit-video"&gt;scikit-video&lt;/a&gt; - Video processing routines for SciPy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-web-asset-management" class="anchor" aria-hidden="true" href="#web-asset-management"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web Asset Management&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Tools for managing, compressing and minifying website assets.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/django-compressor/django-compressor"&gt;django-compressor&lt;/a&gt; - Compresses linked and inline JavaScript or CSS into a single cached file.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jazzband/django-pipeline"&gt;django-pipeline&lt;/a&gt; - An asset packaging library for Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jschneier/django-storages"&gt;django-storages&lt;/a&gt; - A collection of custom storage back ends for Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.fanstatic.org/en/latest/" rel="nofollow"&gt;fanstatic&lt;/a&gt; - Packages, optimizes, and serves static file dependencies as Python packages.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://wimleers.com/fileconveyor" rel="nofollow"&gt;fileconveyor&lt;/a&gt; - A daemon to detect and sync files to CDNs, S3 and FTP.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/miracle2k/flask-assets"&gt;flask-assets&lt;/a&gt; - Helps you integrate webassets into your Flask app.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/miracle2k/webassets"&gt;webassets&lt;/a&gt; - Bundles, optimizes, and manages unique cache-busting URLs for static resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-web-content-extracting" class="anchor" aria-hidden="true" href="#web-content-extracting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web Content Extracting&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for extracting web contents.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Alir3z4/html2text"&gt;html2text&lt;/a&gt; - Convert HTML to Markdown-formatted text.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/michaelhelmick/lassie"&gt;lassie&lt;/a&gt; - Web Content Retrieval for Humans.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/coleifer/micawber"&gt;micawber&lt;/a&gt; - A small library for extracting rich content from URLs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/codelucas/newspaper"&gt;newspaper&lt;/a&gt; - News extraction, article extraction and content curation in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/buriy/python-readability"&gt;python-readability&lt;/a&gt; - Fast Python port of arc90's readability tool.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kennethreitz/requests-html"&gt;requests-html&lt;/a&gt; - Pythonic HTML Parsing for Humans.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/miso-belica/sumy"&gt;sumy&lt;/a&gt; - A module for automatic summarization of text documents and HTML pages.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt; - Extract text from any document, Word, PowerPoint, PDFs, etc.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/gaojiuli/toapi"&gt;toapi&lt;/a&gt; - Every web site provides APIs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-web-crawling" class="anchor" aria-hidden="true" href="#web-crawling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web Crawling&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries to automate web scraping.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/chineking/cola"&gt;cola&lt;/a&gt; - A distributed crawling framework.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pythonhosted.org/feedparser/" rel="nofollow"&gt;feedparser&lt;/a&gt; - Universal feed parser.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lorien/grab"&gt;grab&lt;/a&gt; - Site scraping framework.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MechanicalSoup/MechanicalSoup"&gt;MechanicalSoup&lt;/a&gt; - A Python library for automating interaction with websites.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/binux/pyspider"&gt;pyspider&lt;/a&gt; - A powerful spider system.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jmcarp/robobrowser"&gt;robobrowser&lt;/a&gt; - A simple, Pythonic library for browsing the web without a standalone web browser.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scrapy.org/" rel="nofollow"&gt;scrapy&lt;/a&gt; - A fast high-level screen scraping and web crawling framework.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/scrapinghub/portia"&gt;portia&lt;/a&gt; - Visual scraping for Scrapy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-web-frameworks" class="anchor" aria-hidden="true" href="#web-frameworks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web Frameworks&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Traditional full stack web frameworks. Also see &lt;a href="https://github.com/vinta/awesome-python#restful-api"&gt;RESTful API&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Synchronous
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.djangoproject.com/" rel="nofollow"&gt;Django&lt;/a&gt; - The most popular web framework in Python.
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/shahraizali/awesome-django"&gt;awesome-django&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://flask.pocoo.org/" rel="nofollow"&gt;Flask&lt;/a&gt; - A microframework for Python.
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/humiaozuzu/awesome-flask"&gt;awesome-flask&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pylonsproject.org/" rel="nofollow"&gt;Pyramid&lt;/a&gt; - A small, fast, down-to-earth, open source Python web framework.
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/uralbash/awesome-pyramid"&gt;awesome-pyramid&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MasoniteFramework/masonite"&gt;Masonite&lt;/a&gt; - The modern and developer centric Python web framework.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Asynchronous
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.tornadoweb.org/en/latest/" rel="nofollow"&gt;Tornado&lt;/a&gt; - A web framework and asynchronous networking library.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-websocket" class="anchor" aria-hidden="true" href="#websocket"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WebSocket&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Libraries for working with WebSocket.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/crossbario/autobahn-python"&gt;autobahn-python&lt;/a&gt; - WebSocket &amp;amp; WAMP for Python on Twisted and &lt;a href="https://docs.python.org/3/library/asyncio.html" rel="nofollow"&gt;asyncio&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/django/channels"&gt;channels&lt;/a&gt; - Developer-friendly asynchrony for Django.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/aaugustin/websockets"&gt;websockets&lt;/a&gt; - A library for building WebSocket servers and clients with a focus on correctness and simplicity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-wsgi-servers" class="anchor" aria-hidden="true" href="#wsgi-servers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WSGI Servers&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;WSGI-compatible web servers.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jonashaag/bjoern"&gt;bjoern&lt;/a&gt; - Asynchronous, very fast and written in C.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/benoitc/gunicorn"&gt;gunicorn&lt;/a&gt; - Pre-forked, partly written in C.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://uwsgi-docs.readthedocs.io/en/latest/" rel="nofollow"&gt;uWSGI&lt;/a&gt; - A project aims at developing a full stack for building hosting services, written in C.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Pylons/waitress"&gt;waitress&lt;/a&gt; - Multi-threaded, powers Pyramid.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pallets/werkzeug"&gt;werkzeug&lt;/a&gt; - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h1&gt;
&lt;p&gt;Where to discover new Python libraries.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-podcasts" class="anchor" aria-hidden="true" href="#podcasts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Podcasts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://frompythonimportpodcast.com/" rel="nofollow"&gt;From Python Import Podcast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://podcastinit.com/" rel="nofollow"&gt;Podcast.init&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pythonbytes.fm" rel="nofollow"&gt;Python Bytes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pythontesting.net" rel="nofollow"&gt;Python Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://radiofreepython.com/" rel="nofollow"&gt;Radio Free Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://talkpython.fm/" rel="nofollow"&gt;Talk Python To Me&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://testandcode.com/" rel="nofollow"&gt;Test and Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-twitter" class="anchor" aria-hidden="true" href="#twitter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Twitter&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.com/codetengu" rel="nofollow"&gt;@codetengu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/getpy" rel="nofollow"&gt;@getpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/importpython" rel="nofollow"&gt;@importpython&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/planetpython" rel="nofollow"&gt;@planetpython&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/pycoders" rel="nofollow"&gt;@pycoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/pypi" rel="nofollow"&gt;@pypi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/pythontrending" rel="nofollow"&gt;@pythontrending&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/PythonWeekly" rel="nofollow"&gt;@PythonWeekly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/talkpython" rel="nofollow"&gt;@TalkPython&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/realpython" rel="nofollow"&gt;@realpython&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-websites" class="anchor" aria-hidden="true" href="#websites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Websites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/coolgithubprojects/" rel="nofollow"&gt;/r/CoolGithubProjects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/python" rel="nofollow"&gt;/r/Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://python.libhunt.com/" rel="nofollow"&gt;Awesome Python @LibHunt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://djangopackages.org/" rel="nofollow"&gt;Django Packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.fullstackpython.com/" rel="nofollow"&gt;Full Stack Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pythoncheatsheet.org/" rel="nofollow"&gt;Python Cheatsheet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://python.zeef.com/alan.richmond" rel="nofollow"&gt;Python ZEEF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ctolib.com/python/" rel="nofollow"&gt;Python 开发社区&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://realpython.com" rel="nofollow"&gt;Real Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/trending?l=python"&gt;Trending Python repositories on GitHub today&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://python-scripts.com/" rel="nofollow"&gt;Сообщество Python Программистов&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-weekly" class="anchor" aria-hidden="true" href="#weekly"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Weekly&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://weekly.codetengu.com/" rel="nofollow"&gt;CodeTengu Weekly 碼天狗週刊&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://importpython.com/newsletter/" rel="nofollow"&gt;Import Python Newsletter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pycoders.com/" rel="nofollow"&gt;Pycoder's Weekly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.pythonweekly.com/" rel="nofollow"&gt;Python Weekly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://realpython.com/python-tricks/" rel="nofollow"&gt;Python Tricks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h1&gt;
&lt;p&gt;Your contributions are always welcome! Please take a look at the &lt;a href="https://github.com/vinta/awesome-python/blob/master/CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt; first.&lt;/p&gt;
&lt;p&gt;I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could &lt;a href="https://github.com/vinta/awesome-python/pulls"&gt;vote for them&lt;/a&gt; by adding &lt;g-emoji class="g-emoji" alias="+1" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png"&gt;👍&lt;/g-emoji&gt; to them. Pull requests will be merged when their votes reach &lt;strong&gt;20&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;If you have any question about this opinionated list, do not hesitate to contact me &lt;a href="https://twitter.com/vinta" rel="nofollow"&gt;@vinta&lt;/a&gt; on Twitter or open an issue on GitHub.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>vinta</author><guid isPermaLink="false">https://github.com/vinta/awesome-python</guid><pubDate>Sat, 04 Jan 2020 00:02:00 GMT</pubDate></item><item><title>rq/rq #3 in Python, Today</title><link>https://github.com/rq/rq</link><description>&lt;p&gt;&lt;i&gt;Simple job queues for Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;RQ (&lt;em&gt;Redis Queue&lt;/em&gt;) is a simple Python library for queueing jobs and processing
them in the background with workers.  It is backed by Redis and it is designed
to have a low barrier to entry.  It should be integrated in your web stack
easily.&lt;/p&gt;
&lt;p&gt;RQ requires Redis &amp;gt;= 3.0.0.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://secure.travis-ci.org/rq/rq" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e7de6f8b96f166819cdd708a655c2721dc908330/68747470733a2f2f7472617669732d63692e6f72672f72712f72712e7376673f6272616e63683d6d6173746572" alt="Build status" data-canonical-src="https://travis-ci.org/rq/rq.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/rq" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b8215f7dc29d4761df6b4a7f51d3c965c7c44974/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f72712e737667" alt="PyPI" data-canonical-src="https://img.shields.io/pypi/pyversions/rq.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://codecov.io/gh/rq/rq" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/407a8bb07bd7765d6cc40c9c040de8059caa7a04/68747470733a2f2f636f6465636f762e696f2f67682f72712f72712f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/rq/rq/branch/master/graph/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Full documentation can be found &lt;a href="http://python-rq.org/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-support-rq" class="anchor" aria-hidden="true" href="#support-rq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support RQ&lt;/h2&gt;
&lt;p&gt;If you find RQ useful, please consider supporting this project via &lt;a href="https://tidelift.com/subscription/pkg/pypi-rq?utm_source=pypi-rq&amp;amp;utm_medium=referral&amp;amp;utm_campaign=readme" rel="nofollow"&gt;Tidelift&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting started&lt;/h2&gt;
&lt;p&gt;First, run a Redis server, of course:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;redis-server&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To put jobs on queues, you don't have to do anything special, just define
your typically lengthy or blocking function:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; requests

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;count_words_at_url&lt;/span&gt;(&lt;span class="pl-smi"&gt;url&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Just an example function that's called async.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    resp &lt;span class="pl-k"&gt;=&lt;/span&gt; requests.get(url)
    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(resp.text.split())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You do use the excellent &lt;a href="http://python-requests.org" rel="nofollow"&gt;requests&lt;/a&gt; package, don't you?&lt;/p&gt;
&lt;p&gt;Then, create an RQ queue:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; redis &lt;span class="pl-k"&gt;import&lt;/span&gt; Redis
&lt;span class="pl-k"&gt;from&lt;/span&gt; rq &lt;span class="pl-k"&gt;import&lt;/span&gt; Queue

q &lt;span class="pl-k"&gt;=&lt;/span&gt; Queue(&lt;span class="pl-v"&gt;connection&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;Redis())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And enqueue the function call:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; my_module &lt;span class="pl-k"&gt;import&lt;/span&gt; count_words_at_url
job &lt;span class="pl-k"&gt;=&lt;/span&gt; q.enqueue(count_words_at_url, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;http://nvie.com&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For a more complete example, refer to the &lt;a href="http://python-rq.org/" rel="nofollow"&gt;docs&lt;/a&gt;.  But this is the essence.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-the-worker" class="anchor" aria-hidden="true" href="#the-worker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The worker&lt;/h3&gt;
&lt;p&gt;To start executing enqueued function calls in the background, start a worker
from your project's directory:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;rq worker&lt;/span&gt;
&lt;span class="pl-c1"&gt;*** Listening for work on default&lt;/span&gt;
&lt;span class="pl-c1"&gt;Got count_words_at_url('http://nvie.com') from default&lt;/span&gt;
&lt;span class="pl-c1"&gt;Job result = 818&lt;/span&gt;
&lt;span class="pl-c1"&gt;*** Listening for work on default&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's about it.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;Simply use the following command to install the latest released version:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install rq
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want the cutting edge version (that may well be broken), use this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install -e git+https://github.com/nvie/rq.git@master#egg=rq
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-project-history" class="anchor" aria-hidden="true" href="#project-history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Project history&lt;/h2&gt;
&lt;p&gt;This project has been inspired by the good parts of &lt;a href="http://www.celeryproject.org/" rel="nofollow"&gt;Celery&lt;/a&gt;, &lt;a href="https://github.com/resque/resque"&gt;Resque&lt;/a&gt;
and &lt;a href="http://flask.pocoo.org/snippets/73/" rel="nofollow"&gt;this snippet&lt;/a&gt;, and has been created as a lightweight alternative to the
heaviness of Celery or other AMQP-based queueing implementations.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rq</author><guid isPermaLink="false">https://github.com/rq/rq</guid><pubDate>Sat, 04 Jan 2020 00:03:00 GMT</pubDate></item><item><title>huggingface/transformers #4 in Python, Today</title><link>https://github.com/huggingface/transformers</link><description>&lt;p&gt;&lt;i&gt;🤗 Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;br&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png"&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png" width="400" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;br&gt;
&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;&lt;p align="center"&gt;
    &lt;a href="https://circleci.com/gh/huggingface/transformers" rel="nofollow"&gt;
        &lt;img alt="Build" src="https://camo.githubusercontent.com/045b8639882280ff5cd38c403499977386c25134/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f6275696c642f6769746875622f68756767696e67666163652f7472616e73666f726d6572732f6d6173746572" data-canonical-src="https://img.shields.io/circleci/build/github/huggingface/transformers/master" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://github.com/huggingface/transformers/blob/master/LICENSE"&gt;
        &lt;img alt="GitHub" src="https://camo.githubusercontent.com/440e73b137335cc0088bb06e6c90cc7b503b14a2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f68756767696e67666163652f7472616e73666f726d6572732e7376673f636f6c6f723d626c7565" data-canonical-src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://huggingface.co/transformers/index.html" rel="nofollow"&gt;
        &lt;img alt="Documentation" src="https://camo.githubusercontent.com/b104c21f478c4d4a37f63292ab2898047f19ee24/68747470733a2f2f696d672e736869656c64732e696f2f776562736974652f687474702f68756767696e67666163652e636f2f7472616e73666f726d6572732f696e6465782e68746d6c2e7376673f646f776e5f636f6c6f723d72656426646f776e5f6d6573736167653d6f66666c696e652675705f6d6573736167653d6f6e6c696e65" data-canonical-src="https://img.shields.io/website/http/huggingface.co/transformers/index.html.svg?down_color=red&amp;amp;down_message=offline&amp;amp;up_message=online" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://github.com/huggingface/transformers/releases"&gt;
        &lt;img alt="GitHub release" src="https://camo.githubusercontent.com/8409fd8716dd1a11afa7ab38e1218b34918164eb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f68756767696e67666163652f7472616e73666f726d6572732e737667" data-canonical-src="https://img.shields.io/github/release/huggingface/transformers.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
&lt;/p&gt;
&lt;h3 align="center"&gt;&lt;a id="user-content-state-of-the-art-natural-language-processing-for-tensorflow-20-and-pytorch" class="anchor" aria-hidden="true" href="#state-of-the-art-natural-language-processing-for-tensorflow-20-and-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;p&gt;State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch
&lt;/p&gt;&lt;/h3&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers (formerly known as &lt;code&gt;pytorch-transformers&lt;/code&gt; and &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt;) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;As easy to use as pytorch-transformers&lt;/li&gt;
&lt;li&gt;As powerful and concise as Keras&lt;/li&gt;
&lt;li&gt;High performance on NLU and NLG tasks&lt;/li&gt;
&lt;li&gt;Low barrier to entry for educators and practitioners&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;State-of-the-art NLP for everyone&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep learning researchers&lt;/li&gt;
&lt;li&gt;Hands-on practitioners&lt;/li&gt;
&lt;li&gt;AI/ML/NLP teachers and educators&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lower compute costs, smaller carbon footprint&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Researchers can share trained models instead of always retraining&lt;/li&gt;
&lt;li&gt;Practitioners can reduce compute time and production costs&lt;/li&gt;
&lt;li&gt;10 architectures with over 30 pretrained models, some in more than 100 languages&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Choose the right framework for every part of a model's lifetime&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train state-of-the-art models in 3 lines of code&lt;/li&gt;
&lt;li&gt;Deep interoperability between TensorFlow 2.0 and PyTorch models&lt;/li&gt;
&lt;li&gt;Move a single model between TF2.0/PyTorch frameworks at will&lt;/li&gt;
&lt;li&gt;Seamlessly pick the right framework for training, evaluation, production&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Section&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;How to install the package&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#model-architectures"&gt;Model architectures&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Architectures (with pretrained weights)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#online-demo"&gt;Online demo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Experimenting with this repo’s text generation capabilities&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#quick-tour"&gt;Quick tour: Usage&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tokenizers &amp;amp; models usage: Bert and GPT-2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Quick-tour-TF-20-training-and-PyTorch-interoperability"&gt;Quick tour: TF 2.0 and PyTorch &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Train a TF 2.0 model in 10 lines of code, load it in PyTorch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#quick-tour-of-pipelines"&gt;Quick tour: pipelines&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Using Pipelines: Wrapper around tokenizer and models to use finetuned models&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#quick-tour-of-the-fine-tuningusage-scripts"&gt;Quick tour: Fine-tuning/usage scripts&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Using provided scripts: GLUE, SQuAD and Text generation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Quick-tour-of-model-sharing"&gt;Quick tour: Share your models &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Upload and share your fine-tuned models with the community&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Migrating-from-pytorch-transformers-to-transformers"&gt;Migrating from pytorch-transformers to transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Migrating your code from pytorch-transformers to transformers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Migrating-from-pytorch-pretrained-bert-to-transformers"&gt;Migrating from pytorch-pretrained-bert to pytorch-transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Migrating your code from pytorch-pretrained-bert to transformers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[Documentation]&lt;a href="https://huggingface.co/transformers/v2.3.0" rel="nofollow"&gt;(v2.3.0)&lt;/a&gt;&lt;a href="https://huggingface.co/transformers/v2.2.0" rel="nofollow"&gt;(v2.2.0/v2.2.1/v2.2.2)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v2.1.1" rel="nofollow"&gt;(v2.1.1)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v2.0.0" rel="nofollow"&gt;(v2.0.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.2.0" rel="nofollow"&gt;(v1.2.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.1.0" rel="nofollow"&gt;(v1.1.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.0.0" rel="nofollow"&gt;(v1.0.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers" rel="nofollow"&gt;(master)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Full API documentation and more&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;This repo is tested on Python 3.5+, PyTorch 1.0.0+ and TensorFlow 2.0.0-rc1&lt;/p&gt;
&lt;p&gt;You should install &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers in a &lt;a href="https://docs.python.org/3/library/venv.html" rel="nofollow"&gt;virtual environment&lt;/a&gt;. If you're unfamiliar with Python virtual environments, check out the &lt;a href="https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/" rel="nofollow"&gt;user guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Create a virtual environment with the version of Python you're going to use and activate it.&lt;/p&gt;
&lt;p&gt;Now, if you want to use &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers, you can install it with pip. If you'd like to play with the examples, you must install it from source.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-with-pip" class="anchor" aria-hidden="true" href="#with-pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;With pip&lt;/h3&gt;
&lt;p&gt;First you need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to &lt;a href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available" rel="nofollow"&gt;TensorFlow installation page&lt;/a&gt; and/or &lt;a href="https://pytorch.org/get-started/locally/#start-locally" rel="nofollow"&gt;PyTorch installation page&lt;/a&gt; regarding the specific install command for your platform.&lt;/p&gt;
&lt;p&gt;When TensorFlow 2.0 and/or PyTorch has been installed, &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers can be installed using pip as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install transformers&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-from-source" class="anchor" aria-hidden="true" href="#from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;From source&lt;/h3&gt;
&lt;p&gt;Here also, you first need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to &lt;a href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available" rel="nofollow"&gt;TensorFlow installation page&lt;/a&gt; and/or &lt;a href="https://pytorch.org/get-started/locally/#start-locally" rel="nofollow"&gt;PyTorch installation page&lt;/a&gt; regarding the specific install command for your platform.&lt;/p&gt;
&lt;p&gt;When TensorFlow 2.0 and/or PyTorch has been installed, you can install from source by cloning the repository and running:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/huggingface/transformers
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; transformers
pip install &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When you update the repository, you should upgrade the transformers installation and its dependencies as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git pull
pip install --upgrade &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-run-the-examples" class="anchor" aria-hidden="true" href="#run-the-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run the examples&lt;/h3&gt;
&lt;p&gt;Examples are included in the repository but are not shipped with the library.&lt;/p&gt;
&lt;p&gt;Therefore, in order to run the latest versions of the examples, you need to install from source, as described above.&lt;/p&gt;
&lt;p&gt;Look at the &lt;a href="https://github.com/huggingface/transformers/blob/master/examples/README.md"&gt;README&lt;/a&gt; for how to run examples.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h3&gt;
&lt;p&gt;A series of tests are included for the library and for some example scripts. Library tests can be found in the &lt;a href="https://github.com/huggingface/transformers/tree/master/tests"&gt;tests folder&lt;/a&gt; and examples tests in the &lt;a href="https://github.com/huggingface/transformers/tree/master/examples"&gt;examples folder&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Depending on which framework is installed (TensorFlow 2.0 and/or PyTorch), the irrelevant tests will be skipped. Ensure that both frameworks are installed if you want to execute all tests.&lt;/p&gt;
&lt;p&gt;Here's the easiest way to run tests for the library:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -e &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;.[testing]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
make &lt;span class="pl-c1"&gt;test&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and for the examples:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -e &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;.[testing]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
pip install -r examples/requirements.txt
make test-examples&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For details, refer to the &lt;a href="https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md#tests"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-do-you-want-to-run-a-transformer-model-on-a-mobile-device" class="anchor" aria-hidden="true" href="#do-you-want-to-run-a-transformer-model-on-a-mobile-device"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Do you want to run a Transformer model on a mobile device?&lt;/h3&gt;
&lt;p&gt;You should check out our &lt;a href="https://github.com/huggingface/swift-coreml-transformers"&gt;&lt;code&gt;swift-coreml-transformers&lt;/code&gt;&lt;/a&gt; repo.&lt;/p&gt;
&lt;p&gt;It contains a set of tools to convert PyTorch or TensorFlow 2.0 trained Transformer models (currently contains &lt;code&gt;GPT-2&lt;/code&gt;, &lt;code&gt;DistilGPT-2&lt;/code&gt;, &lt;code&gt;BERT&lt;/code&gt;, and &lt;code&gt;DistilBERT&lt;/code&gt;) to CoreML models that run on iOS devices.&lt;/p&gt;
&lt;p&gt;At some point in the future, you'll be able to seamlessly move from pre-training or fine-tuning models to productizing them in CoreML, or prototype a model or an app in CoreML then research its hyperparameters or architecture from TensorFlow 2.0 and/or PyTorch. Super exciting!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-architectures" class="anchor" aria-hidden="true" href="#model-architectures"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model architectures&lt;/h2&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers currently provides the following NLU/NLG architectures:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/google-research/bert"&gt;BERT&lt;/a&gt;&lt;/strong&gt; (from Google) released with the paper &lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt; by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/openai/finetune-transformer-lm"&gt;GPT&lt;/a&gt;&lt;/strong&gt; (from OpenAI) released with the paper &lt;a href="https://blog.openai.com/language-unsupervised/" rel="nofollow"&gt;Improving Language Understanding by Generative Pre-Training&lt;/a&gt; by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;GPT-2&lt;/a&gt;&lt;/strong&gt; (from OpenAI) released with the paper &lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt; by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/kimiyoung/transformer-xl"&gt;Transformer-XL&lt;/a&gt;&lt;/strong&gt; (from Google/CMU) released with the paper &lt;a href="https://arxiv.org/abs/1901.02860" rel="nofollow"&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/a&gt; by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/zihangdai/xlnet/"&gt;XLNet&lt;/a&gt;&lt;/strong&gt; (from Google/CMU) released with the paper &lt;a href="https://arxiv.org/abs/1906.08237" rel="nofollow"&gt;​XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/a&gt; by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/facebookresearch/XLM/"&gt;XLM&lt;/a&gt;&lt;/strong&gt; (from Facebook) released together with the paper &lt;a href="https://arxiv.org/abs/1901.07291" rel="nofollow"&gt;Cross-lingual Language Model Pretraining&lt;/a&gt; by Guillaume Lample and Alexis Conneau.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta"&gt;RoBERTa&lt;/a&gt;&lt;/strong&gt; (from Facebook), released together with the paper a &lt;a href="https://arxiv.org/abs/1907.11692" rel="nofollow"&gt;Robustly Optimized BERT Pretraining Approach&lt;/a&gt; by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/huggingface/transformers/tree/master/examples/distillation"&gt;DistilBERT&lt;/a&gt;&lt;/strong&gt; (from HuggingFace), released together with the paper &lt;a href="https://arxiv.org/abs/1910.01108" rel="nofollow"&gt;DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter&lt;/a&gt; by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into &lt;a href="https://github.com/huggingface/transformers/tree/master/examples/distillation"&gt;DistilGPT2&lt;/a&gt;, RoBERTa into &lt;a href="https://github.com/huggingface/transformers/tree/master/examples/distillation"&gt;DistilRoBERTa&lt;/a&gt;, Multilingual BERT into &lt;a href="https://github.com/huggingface/transformers/tree/master/examples/distillation"&gt;DistilmBERT&lt;/a&gt; and a German version of DistilBERT.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/salesforce/ctrl/"&gt;CTRL&lt;/a&gt;&lt;/strong&gt; (from Salesforce) released with the paper &lt;a href="https://arxiv.org/abs/1909.05858" rel="nofollow"&gt;CTRL: A Conditional Transformer Language Model for Controllable Generation&lt;/a&gt; by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://camembert-model.fr" rel="nofollow"&gt;CamemBERT&lt;/a&gt;&lt;/strong&gt; (from Inria/Facebook/Sorbonne) released with the paper &lt;a href="https://arxiv.org/abs/1911.03894" rel="nofollow"&gt;CamemBERT: a Tasty French Language Model&lt;/a&gt; by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/google-research/ALBERT"&gt;ALBERT&lt;/a&gt;&lt;/strong&gt; (from Google Research and the Toyota Technological Institute at Chicago) released with the paper &lt;a href="https://arxiv.org/abs/1909.11942" rel="nofollow"&gt;ALBERT: A Lite BERT for Self-supervised Learning of Language Representations&lt;/a&gt;, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/google-research/text-to-text-transfer-transformer"&gt;T5&lt;/a&gt;&lt;/strong&gt; (from Google AI) released with the paper &lt;a href="https://arxiv.org/abs/1910.10683" rel="nofollow"&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/a&gt; by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/pytorch/fairseq/tree/master/examples/xlmr"&gt;XLM-RoBERTa&lt;/a&gt;&lt;/strong&gt; (from Facebook AI), released together with the paper &lt;a href="https://arxiv.org/abs/1911.02116" rel="nofollow"&gt;Unsupervised Cross-lingual Representation Learning at Scale&lt;/a&gt; by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/facebookresearch/mmbt/"&gt;MMBT&lt;/a&gt;&lt;/strong&gt; (from Facebook), released together with the paper a &lt;a href="https://arxiv.org/pdf/1909.02950.pdf" rel="nofollow"&gt;Supervised Multimodal Bitransformers for Classifying Images and Text&lt;/a&gt; by Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Davide Testuggine.&lt;/li&gt;
&lt;li&gt;Want to contribute a new model? We have added a &lt;strong&gt;detailed guide and templates&lt;/strong&gt; to guide you in the process of adding a new model. You can find them in the &lt;a href="./templates"&gt;&lt;code&gt;templates&lt;/code&gt;&lt;/a&gt; folder of the repository. Be sure to check the &lt;a href="./CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; and contact the maintainers or open an issue to collect feedbacks before starting your PR.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the &lt;a href="https://huggingface.co/transformers/examples.html" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-online-demo" class="anchor" aria-hidden="true" href="#online-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online demo&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://transformer.huggingface.co" rel="nofollow"&gt;Write With Transformer&lt;/a&gt;&lt;/strong&gt;, built by the Hugging Face team at transformer.huggingface.co, is the official demo of this repo’s text generation capabilities.
You can use it to experiment with completions generated by &lt;code&gt;GPT2Model&lt;/code&gt;, &lt;code&gt;TransfoXLModel&lt;/code&gt;, and &lt;code&gt;XLNetModel&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“&lt;g-emoji class="g-emoji" alias="unicorn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f984.png"&gt;🦄&lt;/g-emoji&gt; Write with transformer is to writing what calculators are to calculus.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ba91bf4a35939363eca4ca83f3ad3f83248bbc60/68747470733a2f2f7472616e73666f726d65722e68756767696e67666163652e636f2f66726f6e742f6173736574732f7468756d626e61696c2d6c617267652e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/ba91bf4a35939363eca4ca83f3ad3f83248bbc60/68747470733a2f2f7472616e73666f726d65722e68756767696e67666163652e636f2f66726f6e742f6173736574732f7468756d626e61696c2d6c617267652e706e67" alt="write_with_transformer" data-canonical-src="https://transformer.huggingface.co/front/assets/thumbnail-large.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour" class="anchor" aria-hidden="true" href="#quick-tour"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour&lt;/h2&gt;
&lt;p&gt;Let's do a very quick overview of the model architectures in &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers. Detailed examples for each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the &lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;full documentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; torch
&lt;span class="pl-k"&gt;from&lt;/span&gt; transformers &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Transformers has a unified API&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; for 10 transformer architectures and 30 pretrained weights.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;          Model          | Tokenizer          | Pretrained weights shortcut&lt;/span&gt;
&lt;span class="pl-c1"&gt;MODELS&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; [(BertModel,       BertTokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (OpenAIGPTModel,  OpenAIGPTTokenizer,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;openai-gpt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (GPT2Model,       GPT2Tokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;gpt2&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (CTRLModel,       CTRLTokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ctrl&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (TransfoXLModel,  TransfoXLTokenizer,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;transfo-xl-wt103&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (XLNetModel,      XLNetTokenizer,      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xlnet-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (XLMModel,        XLMTokenizer,        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xlm-mlm-enfr-1024&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (DistilBertModel, DistilBertTokenizer, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;distilbert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (RobertaModel,    RobertaTokenizer,    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;roberta-base&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (XLMRobertaModel, XLMRobertaTokenizer, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xlm-roberta-base&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
         ]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Let's encode some text in a sequence of hidden-states using each model:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; model_class, tokenizer_class, pretrained_weights &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;MODELS&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load pretrained model/tokenizer&lt;/span&gt;
    tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer_class.from_pretrained(pretrained_weights)
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Encode text&lt;/span&gt;
    input_ids &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.tensor([tokenizer.encode(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Here is some text to encode&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)])  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Add special tokens takes care of adding [CLS], [SEP], &amp;lt;s&amp;gt;... tokens in the right way for each model.&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; torch.no_grad():
        last_hidden_states &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids)[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models outputs are now tuples&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.&lt;/span&gt;
&lt;span class="pl-c1"&gt;BERT_MODEL_CLASSES&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,
                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; All the classes for an architecture can be initiated from pretrained weights for this architecture&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Note that additional weights added for fine-tuning are only initialized&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; and need to be trained on the down-stream task&lt;/span&gt;
pretrained_weights &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(pretrained_weights)
&lt;span class="pl-k"&gt;for&lt;/span&gt; model_class &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;BERT_MODEL_CLASSES&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load pretrained model/tokenizer&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models can return full list of hidden-states &amp;amp; attentions weights at each layer&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights,
                                        &lt;span class="pl-v"&gt;output_hidden_states&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;,
                                        &lt;span class="pl-v"&gt;output_attentions&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
    input_ids &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.tensor([tokenizer.encode(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Let's see all hidden-states and attentions on this text&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)])
    all_hidden_states, all_attentions &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids)[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;:]

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models are compatible with Torchscript&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights, &lt;span class="pl-v"&gt;torchscript&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
    traced_model &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.jit.trace(model, (input_ids,))

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Simple serialization for models and tokenizers&lt;/span&gt;
    model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; save&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; re-load&lt;/span&gt;
    tokenizer.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; save&lt;/span&gt;
    tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; re-load&lt;/span&gt;

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; SOTA examples for GLUE, SQUAD, text generation...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour-tf-20-training-and-pytorch-interoperability" class="anchor" aria-hidden="true" href="#quick-tour-tf-20-training-and-pytorch-interoperability"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour TF 2.0 training and PyTorch interoperability&lt;/h2&gt;
&lt;p&gt;Let's do a quick example of how a TensorFlow 2.0 model can be trained in 12 lines of code with &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers and then loaded in PyTorch for fast inspection/tests.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow &lt;span class="pl-k"&gt;as&lt;/span&gt; tf
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow_datasets
&lt;span class="pl-k"&gt;from&lt;/span&gt; transformers &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load dataset, tokenizer, model from pretrained model/vocabulary&lt;/span&gt;
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
model &lt;span class="pl-k"&gt;=&lt;/span&gt; TFBertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
data &lt;span class="pl-k"&gt;=&lt;/span&gt; tensorflow_datasets.load(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;glue/mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Prepare dataset for GLUE as a tf.data.Dataset instance&lt;/span&gt;
train_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; glue_convert_examples_to_features(data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], tokenizer, &lt;span class="pl-v"&gt;max_length&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-v"&gt;task&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
valid_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; glue_convert_examples_to_features(data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;validation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], tokenizer, &lt;span class="pl-v"&gt;max_length&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-v"&gt;task&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
train_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; train_dataset.shuffle(&lt;span class="pl-c1"&gt;100&lt;/span&gt;).batch(&lt;span class="pl-c1"&gt;32&lt;/span&gt;).repeat(&lt;span class="pl-c1"&gt;2&lt;/span&gt;)
valid_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; valid_dataset.batch(&lt;span class="pl-c1"&gt;64&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule&lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.optimizers.Adam(&lt;span class="pl-v"&gt;learning_rate&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3e-5&lt;/span&gt;, &lt;span class="pl-v"&gt;epsilon&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1e-08&lt;/span&gt;, &lt;span class="pl-v"&gt;clipnorm&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1.0&lt;/span&gt;)
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.losses.SparseCategoricalCrossentropy(&lt;span class="pl-v"&gt;from_logits&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
metric &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.metrics.SparseCategoricalAccuracy(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;accuracy&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
model.compile(&lt;span class="pl-v"&gt;optimizer&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;optimizer, &lt;span class="pl-v"&gt;loss&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;loss, &lt;span class="pl-v"&gt;metrics&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[metric])

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train and evaluate using tf.keras.Model.fit()&lt;/span&gt;
history &lt;span class="pl-k"&gt;=&lt;/span&gt; model.fit(train_dataset, &lt;span class="pl-v"&gt;epochs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-v"&gt;steps_per_epoch&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;115&lt;/span&gt;,
                    &lt;span class="pl-v"&gt;validation_data&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;valid_dataset, &lt;span class="pl-v"&gt;validation_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;7&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load the TensorFlow model in PyTorch for inspection&lt;/span&gt;
model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
pytorch_model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;from_tf&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task&lt;/span&gt;
sentence_0 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;This research was consistent with his findings.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
sentence_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;His findings were compatible with this research.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
sentence_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;His findings were not compatible with this research.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
inputs_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer.encode_plus(sentence_0, sentence_1, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;return_tensors&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
inputs_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer.encode_plus(sentence_0, sentence_2, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;return_tensors&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

pred_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; pytorch_model(inputs_1[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;token_type_ids&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;inputs_1[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;token_type_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].argmax().item()
pred_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; pytorch_model(inputs_2[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;token_type_ids&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;inputs_2[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;token_type_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].argmax().item()

&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sentence_1 is&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; pred_1 &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;not a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;of sentence_0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sentence_2 is&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; pred_2 &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;not a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;of sentence_0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour-of-the-fine-tuningusage-scripts" class="anchor" aria-hidden="true" href="#quick-tour-of-the-fine-tuningusage-scripts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour of the fine-tuning/usage scripts&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;
Before running the fine-tuning scripts, please read the
&lt;a href="#run-the-examples"&gt;instructions&lt;/a&gt; on how to
setup your environment to run the examples.&lt;/p&gt;
&lt;p&gt;The library comprises several example scripts with SOTA performances for NLU and NLG tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;run_glue.py&lt;/code&gt;: an example fine-tuning Bert, XLNet and XLM on nine different GLUE tasks (&lt;em&gt;sequence-level classification&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run_squad.py&lt;/code&gt;: an example fine-tuning Bert, XLNet and XLM on the question answering dataset SQuAD 2.0 (&lt;em&gt;token-level classification&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run_generation.py&lt;/code&gt;: an example using GPT, GPT-2, CTRL, Transformer-XL and XLNet for conditional language generation&lt;/li&gt;
&lt;li&gt;other model-specific examples (see the documentation).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are three quick usage examples for these scripts:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run_gluepy-fine-tuning-on-glue-tasks-for-sequence-classification" class="anchor" aria-hidden="true" href="#run_gluepy-fine-tuning-on-glue-tasks-for-sequence-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_glue.py&lt;/code&gt;: Fine-tuning on GLUE tasks for sequence classification&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://gluebenchmark.com/" rel="nofollow"&gt;General Language Understanding Evaluation (GLUE) benchmark&lt;/a&gt; is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.&lt;/p&gt;
&lt;p&gt;Before running anyone of these GLUE tasks you should download the
&lt;a href="https://gluebenchmark.com/tasks" rel="nofollow"&gt;GLUE data&lt;/a&gt; by running
&lt;a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"&gt;this script&lt;/a&gt;
and unpack it to some directory &lt;code&gt;$GLUE_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You should also install the additional packages required by the examples:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -r ./examples/requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue
&lt;span class="pl-k"&gt;export&lt;/span&gt; TASK_NAME=MRPC

python ./examples/run_glue.py \
    --model_type bert \
    --model_name_or_path bert-base-uncased \
    --task_name &lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt; \
    --do_train \
    --do_eval \
    --do_lower_case \
    --data_dir &lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/&lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt; \
    --max_seq_length 128 \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --output_dir /tmp/&lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt;/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.&lt;/p&gt;
&lt;p&gt;The dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called '/tmp/MNLI-MM/' in addition to '/tmp/MNLI/'.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-fine-tuning-xlnet-model-on-the-sts-b-regression-task" class="anchor" aria-hidden="true" href="#fine-tuning-xlnet-model-on-the-sts-b-regression-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning XLNet model on the STS-B regression task&lt;/h4&gt;
&lt;p&gt;This example code fine-tunes XLNet on the STS-B corpus using parallel training on a server with 4 V100 GPUs.
Parallel training is a simple way to use several GPUs (but is slower and less flexible than distributed training, see below).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue

python ./examples/run_glue.py \
    --model_type xlnet \
    --model_name_or_path xlnet-large-cased \
    --do_train  \
    --do_eval   \
    --task_name=sts-b     \
    --data_dir=&lt;span class="pl-smi"&gt;${GLUE_DIR}&lt;/span&gt;/STS-B  \
    --output_dir=./proc_data/sts-b-110   \
    --max_seq_length=128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --gradient_accumulation_steps=1 \
    --max_steps=1200  \
    --model_name=xlnet-large-cased   \
    --overwrite_output_dir   \
    --overwrite_cache \
    --warmup_steps=120&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On this machine we thus have a batch size of 32, please increase &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; to reach the same batch size if you have a smaller machine. These hyper-parameters should result in a Pearson correlation coefficient of &lt;code&gt;+0.917&lt;/code&gt; on the development set.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-fine-tuning-bert-model-on-the-mrpc-classification-task" class="anchor" aria-hidden="true" href="#fine-tuning-bert-model-on-the-mrpc-classification-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning Bert model on the MRPC classification task&lt;/h4&gt;
&lt;p&gt;This example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 &amp;gt; 92.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m torch.distributed.launch --nproc_per_node 8 ./examples/run_glue.py   \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --task_name MRPC \
    --do_train   \
    --do_eval   \
    --do_lower_case   \
    --data_dir &lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC/   \
    --max_seq_length 128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5   \
    --num_train_epochs 3.0  \
    --output_dir /tmp/mrpc_output/ \
    --overwrite_output_dir   \
    --overwrite_cache \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Training with these hyper-parameters gave us the following results:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;  acc = 0.8823529411764706
  acc_and_f1 = 0.901702786377709
  eval_loss = 0.3418912578906332
  f1 = 0.9210526315789473
  global_step = 174
  loss = 0.07231863956341798&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-run_squadpy-fine-tuning-on-squad-for-question-answering" class="anchor" aria-hidden="true" href="#run_squadpy-fine-tuning-on-squad-for-question-answering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_squad.py&lt;/code&gt;: Fine-tuning on SQuAD for question-answering&lt;/h3&gt;
&lt;p&gt;This example code fine-tunes BERT on the SQuAD dataset using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 &amp;gt; 93 on SQuAD:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m torch.distributed.launch --nproc_per_node=8 ./examples/run_squad.py \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --do_train \
    --do_eval \
    --do_lower_case \
    --train_file &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
    --predict_file &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
    --learning_rate 3e-5 \
    --num_train_epochs 2 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir ../models/wwm_uncased_finetuned_squad/ \
    --per_gpu_eval_batch_size=3   \
    --per_gpu_train_batch_size=3   \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Training with these hyper-parameters gave us the following results:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/evaluate-v1.1.py &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json
{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 86.91579943235573, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 93.1532499015869}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the model provided as &lt;code&gt;bert-large-uncased-whole-word-masking-finetuned-squad&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run_generationpy-text-generation-with-gpt-gpt-2-ctrl-transformer-xl-and-xlnet" class="anchor" aria-hidden="true" href="#run_generationpy-text-generation-with-gpt-gpt-2-ctrl-transformer-xl-and-xlnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_generation.py&lt;/code&gt;: Text generation with GPT, GPT-2, CTRL, Transformer-XL and XLNet&lt;/h3&gt;
&lt;p&gt;A conditional generation script is also included to generate text from a prompt.
The generation script includes the &lt;a href="https://github.com/rusiaaman/XLNet-gen#methodology"&gt;tricks&lt;/a&gt; proposed by Aman Rusia to get high-quality generation with memory models like Transformer-XL and XLNet (include a predefined text to make short inputs longer).&lt;/p&gt;
&lt;p&gt;Here is how to run the script with the small version of OpenAI GPT-2 model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python ./examples/run_generation.py \
    --model_type=gpt2 \
    --length=20 \
    --model_name_or_path=gpt2 \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and from the Salesforce CTRL model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python ./examples/run_generation.py \
    --model_type=ctrl \
    --length=20 \
    --model_name_or_path=ctrl \
    --temperature=0 \
    --repetition_penalty=1.2 \&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour-of-model-sharing" class="anchor" aria-hidden="true" href="#quick-tour-of-model-sharing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour of model sharing&lt;/h2&gt;
&lt;p&gt;New in &lt;code&gt;v2.2.2&lt;/code&gt;: you can now upload and share your fine-tuned models with the community, using the CLI that's built-in to the library.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First, create an account on &lt;a href="https://huggingface.co/join" rel="nofollow"&gt;https://huggingface.co/join&lt;/a&gt;&lt;/strong&gt;. Then:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;transformers-cli login
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; log in using the same credentials as on huggingface.co&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Upload your model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;transformers-cli upload ./path/to/pretrained_model/

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ^^ Upload folder containing weights/tokenizer/config&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; saved via `.save_pretrained()`&lt;/span&gt;

transformers-cli upload ./config.json [--filename folder/foobar.json]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ^^ Upload a single file&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; (you can optionally override its filename, which can be nested inside a folder)&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Your model will then be accessible through its identifier, a concatenation of your username and the folder name above:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;username/model_name&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Anyone can load it from code:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; AutoTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;username/pretrained_model&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
model &lt;span class="pl-k"&gt;=&lt;/span&gt; AutoModel.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;username/pretrained_model&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, list all your files on S3:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;transformers-cli s3 ls
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; List all your S3 objects.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour-of-pipelines" class="anchor" aria-hidden="true" href="#quick-tour-of-pipelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour of pipelines&lt;/h2&gt;
&lt;p&gt;New in version &lt;code&gt;v2.3&lt;/code&gt;: &lt;code&gt;Pipeline&lt;/code&gt; are high-level objects which automatically handle tokenization, running your data through a transformers model
and outputting the result in a structured object.&lt;/p&gt;
&lt;p&gt;You can create &lt;code&gt;Pipeline&lt;/code&gt; objects for the following down-stream tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;feature-extraction&lt;/code&gt;: Generates a tensor representation for the input sequence&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ner&lt;/code&gt;: Generates named entity mapping for each word in the input sequence.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sentiment-analysis&lt;/code&gt;: Gives the polarity (positive / negative) of the whole input sequence.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;question-answering&lt;/code&gt;: Provided some context and a question refering to the context, it will extract the answer to the question
in the context.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; transformers &lt;span class="pl-k"&gt;import&lt;/span&gt; pipeline

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Allocate a pipeline for sentiment-analysis&lt;/span&gt;
nlp &lt;span class="pl-k"&gt;=&lt;/span&gt; pipeline(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;sentiment-analysis&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
nlp(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;We are very happy to include pipeline into the transformers repository.&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;label&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;POSITIVE&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;score&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.99893874&lt;/span&gt;}

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Allocate a pipeline for question-answering&lt;/span&gt;
nlp &lt;span class="pl-k"&gt;=&lt;/span&gt; pipeline(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;question-answering&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
nlp({
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;question&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;What is the name of the repository ?&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;context&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Pipeline have been included in the huggingface/transformers repository&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
})
&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;score&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.28756016668193496&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;start&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;35&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;end&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;59&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;answer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;huggingface/transformers&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;}&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-migrating-from-pytorch-transformers-to-transformers" class="anchor" aria-hidden="true" href="#migrating-from-pytorch-transformers-to-transformers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Migrating from pytorch-transformers to transformers&lt;/h2&gt;
&lt;p&gt;Here is a quick summary of what you should take care of when migrating from &lt;code&gt;pytorch-transformers&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-positional-order-of-some-models-keywords-inputs-attention_mask-token_type_ids-changed" class="anchor" aria-hidden="true" href="#positional-order-of-some-models-keywords-inputs-attention_mask-token_type_ids-changed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Positional order of some models' keywords inputs (&lt;code&gt;attention_mask&lt;/code&gt;, &lt;code&gt;token_type_ids&lt;/code&gt;...) changed&lt;/h3&gt;
&lt;p&gt;To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models &lt;strong&gt;keywords inputs&lt;/strong&gt; (&lt;code&gt;attention_mask&lt;/code&gt;, &lt;code&gt;token_type_ids&lt;/code&gt;...) has been changed.&lt;/p&gt;
&lt;p&gt;If you used to call the models with keyword names for keyword arguments, e.g. &lt;code&gt;model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)&lt;/code&gt;, this should not cause any change.&lt;/p&gt;
&lt;p&gt;If you used to call the models with positional inputs for keyword arguments, e.g. &lt;code&gt;model(inputs_ids, attention_mask, token_type_ids)&lt;/code&gt;, you may have to double check the exact order of input arguments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-migrating-from-pytorch-pretrained-bert-to-transformers" class="anchor" aria-hidden="true" href="#migrating-from-pytorch-pretrained-bert-to-transformers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Migrating from pytorch-pretrained-bert to transformers&lt;/h2&gt;
&lt;p&gt;Here is a quick summary of what you should take care of when migrating from &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-models-always-output-tuples" class="anchor" aria-hidden="true" href="#models-always-output-tuples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models always output &lt;code&gt;tuples&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The main breaking change when migrating from &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt; is that every model's forward method always outputs a &lt;code&gt;tuple&lt;/code&gt; with various elements depending on the model and the configuration parameters.&lt;/p&gt;
&lt;p&gt;The exact content of the tuples for each model is detailed in the models' docstrings and the &lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is a &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt; conversion example for a &lt;code&gt;BertForSequenceClassification&lt;/code&gt; classification model:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Let's load our model&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; If you used to have this line in pytorch-pretrained-bert:&lt;/span&gt;
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Now just use this line in transformers to extract the loss from the output tuple:&lt;/span&gt;
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; In transformers you can also have access to the logits:&lt;/span&gt;
loss, logits &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs[:&lt;span class="pl-c1"&gt;2&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;output_attentions&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)
loss, logits, attentions &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-using-hidden-states" class="anchor" aria-hidden="true" href="#using-hidden-states"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using hidden states&lt;/h3&gt;
&lt;p&gt;By enabling the configuration option &lt;code&gt;output_hidden_states&lt;/code&gt;, it was possible to retrieve the last hidden states of the encoder. In &lt;code&gt;pytorch-transformers&lt;/code&gt; as well as &lt;code&gt;transformers&lt;/code&gt; the return value has changed slightly: &lt;code&gt;all_hidden_states&lt;/code&gt; now also includes the hidden state of the embeddings in addition to those of the encoding layers. This allows users to easily access the embeddings final state.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-serialization" class="anchor" aria-hidden="true" href="#serialization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Serialization&lt;/h3&gt;
&lt;p&gt;Breaking change in the &lt;code&gt;from_pretrained()&lt;/code&gt; method:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Models are now set in evaluation mode by default when instantiated with the &lt;code&gt;from_pretrained()&lt;/code&gt; method. To train them, don't forget to set them back in training mode (&lt;code&gt;model.train()&lt;/code&gt;) to activate the dropout modules.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The additional &lt;code&gt;*input&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt; arguments supplied to the &lt;code&gt;from_pretrained()&lt;/code&gt; method used to be directly passed to the underlying model's class &lt;code&gt;__init__()&lt;/code&gt; method. They are now used to update the model configuration attribute instead, which can break derived model classes built based on the previous &lt;code&gt;BertForSequenceClassification&lt;/code&gt; examples. We are working on a way to mitigate this breaking change in &lt;a href="https://github.com/huggingface/transformers/pull/866"&gt;#866&lt;/a&gt; by forwarding the the model's &lt;code&gt;__init__()&lt;/code&gt; method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method &lt;code&gt;save_pretrained(save_directory)&lt;/code&gt; if you were using any other serialization method before.&lt;/p&gt;
&lt;p&gt;Here is an example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Let's load a model and tokenizer&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Do some stuff to our model and tokenizer&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Ex: add new tokens to the vocabulary and embeddings of our model&lt;/span&gt;
tokenizer.add_tokens([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;[SPECIAL_TOKEN_1]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;[SPECIAL_TOKEN_2]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])
model.resize_token_embeddings(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(tokenizer))
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train our model&lt;/span&gt;
train(model)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Now let's save our model and tokenizer to a directory&lt;/span&gt;
model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Reload the model and the tokenizer&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-optimizers-bertadam--openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules" class="anchor" aria-hidden="true" href="#optimizers-bertadam--openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimizers: BertAdam &amp;amp; OpenAIAdam are now AdamW, schedules are standard PyTorch schedules&lt;/h3&gt;
&lt;p&gt;The two optimizers previously included, &lt;code&gt;BertAdam&lt;/code&gt; and &lt;code&gt;OpenAIAdam&lt;/code&gt;, have been replaced by a single &lt;code&gt;AdamW&lt;/code&gt; optimizer which has a few differences:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it only implements weights decay correction,&lt;/li&gt;
&lt;li&gt;schedules are now externals (see below),&lt;/li&gt;
&lt;li&gt;gradient clipping is now also external (see below).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The new optimizer &lt;code&gt;AdamW&lt;/code&gt; matches PyTorch &lt;code&gt;Adam&lt;/code&gt; optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping.&lt;/p&gt;
&lt;p&gt;The schedules are now standard &lt;a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate" rel="nofollow"&gt;PyTorch learning rate schedulers&lt;/a&gt; and not part of the optimizer anymore.&lt;/p&gt;
&lt;p&gt;Here is a conversion examples from &lt;code&gt;BertAdam&lt;/code&gt; with a linear warmup and decay schedule to &lt;code&gt;AdamW&lt;/code&gt; and the same schedule:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Parameters:&lt;/span&gt;
lr &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1e-3&lt;/span&gt;
max_grad_norm &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1.0&lt;/span&gt;
num_training_steps &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1000&lt;/span&gt;
num_warmup_steps &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt;
warmup_proportion &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;float&lt;/span&gt;(num_warmup_steps) &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;float&lt;/span&gt;(num_training_steps)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 0.1&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Previously BertAdam optimizer was instantiated like this:&lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertAdam(model.parameters(), &lt;span class="pl-v"&gt;lr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;lr, &lt;span class="pl-v"&gt;schedule&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;warmup_linear&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;warmup&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;warmup_proportion, &lt;span class="pl-v"&gt;t_total&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_training_steps)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## and used like this:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; batch &lt;span class="pl-k"&gt;in&lt;/span&gt; train_data:
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(batch)
    loss.backward()
    optimizer.step()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## In Transformers, optimizer and schedules are splitted and instantiated like this:&lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; AdamW(model.parameters(), &lt;span class="pl-v"&gt;lr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;lr, &lt;span class="pl-v"&gt;correct_bias&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To reproduce BertAdam specific behavior set correct_bias=False&lt;/span&gt;
scheduler &lt;span class="pl-k"&gt;=&lt;/span&gt; get_linear_schedule_with_warmup(optimizer, &lt;span class="pl-v"&gt;num_warmup_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_warmup_steps, &lt;span class="pl-v"&gt;num_training_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_training_steps)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; PyTorch scheduler&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## and used like this:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; batch &lt;span class="pl-k"&gt;in&lt;/span&gt; train_data:
    model.train()
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(batch)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Gradient clipping is not in AdamW anymore (so you can use amp without issue)&lt;/span&gt;
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;We now have a paper you can cite for the &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers library:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>huggingface</author><guid isPermaLink="false">https://github.com/huggingface/transformers</guid><pubDate>Sat, 04 Jan 2020 00:04:00 GMT</pubDate></item><item><title>google-research/bert #5 in Python, Today</title><link>https://github.com/google-research/bert</link><description>&lt;p&gt;&lt;i&gt;TensorFlow code and pre-trained models for BERT&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-bert" class="anchor" aria-hidden="true" href="#bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BERT&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;***** New May 31st, 2019: Whole Word Masking Models *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a release of several new models which were the result of an improvement
the pre-processing code.&lt;/p&gt;
&lt;p&gt;In the original pre-processing code, we randomly select WordPiece tokens to
mask. For example:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Input Text: the man jumped up , put his basket on phil ##am ##mon ' s head&lt;/code&gt;
&lt;code&gt;Original Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The new technique is called Whole Word Masking. In this case, we always mask
&lt;em&gt;all&lt;/em&gt; of the the tokens corresponding to a word at once. The overall masking
rate remains the same.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The training is identical -- we still predict each masked WordPiece token
independently. The improvement comes from the fact that the original prediction
task was too 'easy' for words that had been split into multiple WordPieces.&lt;/p&gt;
&lt;p&gt;This can be enabled during data generation by passing the flag
&lt;code&gt;--do_whole_word_mask=True&lt;/code&gt; to &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Pre-trained models with Whole Word Masking are linked below. The data and
training were otherwise identical, and the models have identical structure and
vocab to the original models. We only include BERT-Large models. When using
these models, please make it clear in the paper that you are using the Whole
Word Masking variant of BERT-Large.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;SQUAD 1.1 F1/EM&lt;/th&gt;
&lt;th align="center"&gt;Multi NLI Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.0/84.3&lt;/td&gt;
&lt;td align="center"&gt;86.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.8/86.7&lt;/td&gt;
&lt;td align="center"&gt;87.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.5/84.8&lt;/td&gt;
&lt;td align="center"&gt;86.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.9/86.7&lt;/td&gt;
&lt;td align="center"&gt;86.46&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;***** New February 7th, 2019: TfHub Module *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;BERT has been uploaded to &lt;a href="https://tfhub.dev" rel="nofollow"&gt;TensorFlow Hub&lt;/a&gt;. See
&lt;code&gt;run_classifier_with_tfhub.py&lt;/code&gt; for an example of how to use the TF Hub module,
or run an example in the browser on
&lt;a href="https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb" rel="nofollow"&gt;Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 23rd, 2018: Un-normalized multilingual model + Thai +
Mongolian *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We uploaded a new multilingual model which does &lt;em&gt;not&lt;/em&gt; perform any normalization
on the input (no lower casing, accent stripping, or Unicode normalization), and
additionally inclues Thai and Mongolian.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It is recommended to use this version for developing multilingual models,
especially on languages with non-Latin alphabets.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This does not require any code changes, and can be downloaded here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;***** New November 15th, 2018: SOTA SQuAD 2.0 System *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which is
currently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of the
README for details.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 5th, 2018: Third-party PyTorch and Chainer versions of
BERT available *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NLP researchers from HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. Sosuke Kobayashi also made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
(Thanks!) We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 3rd, 2018: Multilingual and Chinese models available
*****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have made two new BERT models available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use character-based tokenization for Chinese, and WordPiece tokenization for
all other languages. Both models should work out-of-the-box without any code
changes. We did update the implementation of &lt;code&gt;BasicTokenizer&lt;/code&gt; in
&lt;code&gt;tokenization.py&lt;/code&gt; to support Chinese character tokenization, so please update if
you forked it. However, we did not change the tokenization API.&lt;/p&gt;
&lt;p&gt;For more, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** End new information *****&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BERT&lt;/strong&gt;, or &lt;strong&gt;B&lt;/strong&gt;idirectional &lt;strong&gt;E&lt;/strong&gt;ncoder &lt;strong&gt;R&lt;/strong&gt;epresentations from
&lt;strong&gt;T&lt;/strong&gt;ransformers, is a new method of pre-training language representations which
obtains state-of-the-art results on a wide array of Natural Language Processing
(NLP) tasks.&lt;/p&gt;
&lt;p&gt;Our academic paper which describes BERT in detail and provides full results on a
number of tasks can be found here:
&lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To give a few numbers, here are the results on the
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD v1.1&lt;/a&gt; question answering
task:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SQuAD v1.1 Leaderboard (Oct 8th 2018)&lt;/th&gt;
&lt;th align="center"&gt;Test EM&lt;/th&gt;
&lt;th align="center"&gt;Test F1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Ensemble - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;87.4&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;93.2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Ensemble - nlnet&lt;/td&gt;
&lt;td align="center"&gt;86.0&lt;/td&gt;
&lt;td align="center"&gt;91.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Single Model - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;85.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.8&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Single Model - nlnet&lt;/td&gt;
&lt;td align="center"&gt;83.5&lt;/td&gt;
&lt;td align="center"&gt;90.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And several natural language inference tasks:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th align="center"&gt;MultiNLI&lt;/th&gt;
&lt;th align="center"&gt;Question NLI&lt;/th&gt;
&lt;th align="center"&gt;SWAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.7&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.3&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenAI GPT (Prev. SOTA)&lt;/td&gt;
&lt;td align="center"&gt;82.2&lt;/td&gt;
&lt;td align="center"&gt;88.1&lt;/td&gt;
&lt;td align="center"&gt;75.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Plus many other tasks.&lt;/p&gt;
&lt;p&gt;Moreover, these results were all obtained with almost no task-specific neural
network architecture design.&lt;/p&gt;
&lt;p&gt;If you already know what BERT is and you just want to get started, you can
&lt;a href="#pre-trained-models"&gt;download the pre-trained models&lt;/a&gt; and
&lt;a href="#fine-tuning-with-bert"&gt;run a state-of-the-art fine-tuning&lt;/a&gt; in only a few
minutes.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-bert" class="anchor" aria-hidden="true" href="#what-is-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is BERT?&lt;/h2&gt;
&lt;p&gt;BERT is a method of pre-training language representations, meaning that we train
a general-purpose "language understanding" model on a large text corpus (like
Wikipedia), and then use that model for downstream NLP tasks that we care about
(like question answering). BERT outperforms previous methods because it is the
first &lt;em&gt;unsupervised&lt;/em&gt;, &lt;em&gt;deeply bidirectional&lt;/em&gt; system for pre-training NLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unsupervised&lt;/em&gt; means that BERT was trained using only a plain text corpus, which
is important because an enormous amount of plain text data is publicly available
on the web in many languages.&lt;/p&gt;
&lt;p&gt;Pre-trained representations can also either be &lt;em&gt;context-free&lt;/em&gt; or &lt;em&gt;contextual&lt;/em&gt;,
and contextual representations can further be &lt;em&gt;unidirectional&lt;/em&gt; or
&lt;em&gt;bidirectional&lt;/em&gt;. Context-free models such as
&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec" rel="nofollow"&gt;word2vec&lt;/a&gt; or
&lt;a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow"&gt;GloVe&lt;/a&gt; generate a single "word
embedding" representation for each word in the vocabulary, so &lt;code&gt;bank&lt;/code&gt; would have
the same representation in &lt;code&gt;bank deposit&lt;/code&gt; and &lt;code&gt;river bank&lt;/code&gt;. Contextual models
instead generate a representation of each word that is based on the other words
in the sentence.&lt;/p&gt;
&lt;p&gt;BERT was built upon recent work in pre-training contextual representations —
including &lt;a href="https://arxiv.org/abs/1511.01432" rel="nofollow"&gt;Semi-supervised Sequence Learning&lt;/a&gt;,
&lt;a href="https://blog.openai.com/language-unsupervised/" rel="nofollow"&gt;Generative Pre-Training&lt;/a&gt;,
&lt;a href="https://allennlp.org/elmo" rel="nofollow"&gt;ELMo&lt;/a&gt;, and
&lt;a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html" rel="nofollow"&gt;ULMFit&lt;/a&gt;
— but crucially these models are all &lt;em&gt;unidirectional&lt;/em&gt; or &lt;em&gt;shallowly
bidirectional&lt;/em&gt;. This means that each word is only contextualized using the words
to its left (or right). For example, in the sentence &lt;code&gt;I made a bank deposit&lt;/code&gt; the
unidirectional representation of &lt;code&gt;bank&lt;/code&gt; is only based on &lt;code&gt;I made a&lt;/code&gt; but not
&lt;code&gt;deposit&lt;/code&gt;. Some previous work does combine the representations from separate
left-context and right-context models, but only in a "shallow" manner. BERT
represents "bank" using both its left and right context — &lt;code&gt;I made a ... deposit&lt;/code&gt;
— starting from the very bottom of a deep neural network, so it is &lt;em&gt;deeply
bidirectional&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;BERT uses a simple approach for this: We mask out 15% of the words in the input,
run the entire sequence through a deep bidirectional
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; encoder, and then predict only
the masked words. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: the man went to the [MASK1] . he bought a [MASK2] of milk.
Labels: [MASK1] = store; [MASK2] = gallon
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to learn relationships between sentences, we also train on a simple
task which can be generated from any monolingual corpus: Given two sentences &lt;code&gt;A&lt;/code&gt;
and &lt;code&gt;B&lt;/code&gt;, is &lt;code&gt;B&lt;/code&gt; the actual next sentence that comes after &lt;code&gt;A&lt;/code&gt;, or just a random
sentence from the corpus?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: he bought a gallon of milk .
Label: IsNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: penguins are flightless .
Label: NotNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then train a large model (12-layer to 24-layer Transformer) on a large corpus
(Wikipedia + &lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt;) for a long time (1M
update steps), and that's BERT.&lt;/p&gt;
&lt;p&gt;Using BERT has two stages: &lt;em&gt;Pre-training&lt;/em&gt; and &lt;em&gt;fine-tuning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pre-training&lt;/strong&gt; is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a
one-time procedure for each language (current models are English-only, but
multilingual models will be released in the near future). We are releasing a
number of pre-trained models from the paper which were pre-trained at Google.
Most NLP researchers will never need to pre-train their own model from scratch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt; is inexpensive. All of the results in the paper can be
replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU,
starting from the exact same pre-trained model. SQuAD, for example, can be
trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of
91.0%, which is the single system state-of-the-art.&lt;/p&gt;
&lt;p&gt;The other important aspect of BERT is that it can be adapted to many types of
NLP tasks very easily. In the paper, we demonstrate state-of-the-art results on
sentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level
(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific
modifications.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-has-been-released-in-this-repository" class="anchor" aria-hidden="true" href="#what-has-been-released-in-this-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What has been released in this repository?&lt;/h2&gt;
&lt;p&gt;We are releasing the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow code for the BERT model architecture (which is mostly a standard
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; architecture).&lt;/li&gt;
&lt;li&gt;Pre-trained checkpoints for both the lowercase and cased version of
&lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; from the paper.&lt;/li&gt;
&lt;li&gt;TensorFlow code for push-button replication of the most important
fine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of the code in this repository works out-of-the-box with CPU, GPU, and Cloud
TPU.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-trained-models" class="anchor" aria-hidden="true" href="#pre-trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-trained models&lt;/h2&gt;
&lt;p&gt;We are releasing the &lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; models from the paper.
&lt;code&gt;Uncased&lt;/code&gt; means that the text has been lowercased before WordPiece tokenization,
e.g., &lt;code&gt;John Smith&lt;/code&gt; becomes &lt;code&gt;john smith&lt;/code&gt;. The &lt;code&gt;Uncased&lt;/code&gt; model also strips out any
accent markers. &lt;code&gt;Cased&lt;/code&gt; means that the true case and accent markers are
preserved. Typically, the &lt;code&gt;Uncased&lt;/code&gt; model is better unless you know that case
information is important for your task (e.g., Named Entity Recognition or
Part-of-Speech tagging).&lt;/p&gt;
&lt;p&gt;These models are all released under the same license as the source code (Apache
2.0).&lt;/p&gt;
&lt;p&gt;For information about the Multilingual and Chinese model, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When using a cased model, make sure to pass &lt;code&gt;--do_lower=False&lt;/code&gt; to the training
scripts. (Or pass &lt;code&gt;do_lower_case=False&lt;/code&gt; directly to &lt;code&gt;FullTokenizer&lt;/code&gt; if you're
using your own script.)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The links to the models are here (right-click, 'Save link as...' on the name):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads , 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased (New, recommended)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Uncased (Orig, not recommended)&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each .zip file contains three items:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A TensorFlow checkpoint (&lt;code&gt;bert_model.ckpt&lt;/code&gt;) containing the pre-trained
weights (which is actually 3 files).&lt;/li&gt;
&lt;li&gt;A vocab file (&lt;code&gt;vocab.txt&lt;/code&gt;) to map WordPiece to word id.&lt;/li&gt;
&lt;li&gt;A config file (&lt;code&gt;bert_config.json&lt;/code&gt;) which specifies the hyperparameters of
the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-fine-tuning-with-bert" class="anchor" aria-hidden="true" href="#fine-tuning-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with BERT&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: All results on the paper were fine-tuned on a single Cloud TPU,
which has 64GB of RAM. It is currently not possible to re-produce most of the
&lt;code&gt;BERT-Large&lt;/code&gt; results on the paper using a GPU with 12GB - 16GB of RAM, because
the maximum batch size that can fit in memory is too small. We are working on
adding code to this repository which allows for much larger effective batch size
on the GPU. See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for
more details.&lt;/p&gt;
&lt;p&gt;This code was tested with TensorFlow 1.11.0. It was tested with Python2 and
Python3 (but more thoroughly with Python2, since this is what's used internally
in Google).&lt;/p&gt;
&lt;p&gt;The fine-tuning examples which use &lt;code&gt;BERT-Base&lt;/code&gt; should be able to run on a GPU
that has at least 12GB of RAM using the hyperparameters given.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-fine-tuning-with-cloud-tpus" class="anchor" aria-hidden="true" href="#fine-tuning-with-cloud-tpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with Cloud TPUs&lt;/h3&gt;
&lt;p&gt;Most of the examples below assumes that you will be running training/evaluation
on your local machine, using a GPU like a Titan X or GTX 1080.&lt;/p&gt;
&lt;p&gt;However, if you have access to a Cloud TPU that you want to train on, just add
the following flags to &lt;code&gt;run_classifier.py&lt;/code&gt; or &lt;code&gt;run_squad.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --use_tpu=True \
  --tpu_name=$TPU_NAME
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please see the
&lt;a href="https://cloud.google.com/tpu/docs/tutorials/mnist" rel="nofollow"&gt;Google Cloud TPU tutorial&lt;/a&gt;
for how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;On Cloud TPUs, the pretrained model and the output directory will need to be on
Google Cloud Storage. For example, if you have a bucket named &lt;code&gt;some_bucket&lt;/code&gt;, you
might use the following flags instead:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --output_dir=gs://some_bucket/my_output_dir/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The unzipped pre-trained model files can also be found in the Google Cloud
Storage folder &lt;code&gt;gs://bert_models/2018_10_18&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export BERT_BASE_DIR=gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-sentence-and-sentence-pair-classification-tasks" class="anchor" aria-hidden="true" href="#sentence-and-sentence-pair-classification-tasks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sentence (and sentence-pair) classification tasks&lt;/h3&gt;
&lt;p&gt;Before running this example you must download the
&lt;a href="https://gluebenchmark.com/tasks" rel="nofollow"&gt;GLUE data&lt;/a&gt; by running
&lt;a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"&gt;this script&lt;/a&gt;
and unpack it to some directory &lt;code&gt;$GLUE_DIR&lt;/code&gt;. Next, download the &lt;code&gt;BERT-Base&lt;/code&gt;
checkpoint and unzip it to some directory &lt;code&gt;$BERT_BASE_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This example code fine-tunes &lt;code&gt;BERT-Base&lt;/code&gt; on the Microsoft Research Paraphrase
Corpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in a
few minutes on most GPUs.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue

python run_classifier.py \
  --task_name=MRPC \
  --do_train=true \
  --do_eval=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  eval_accuracy = 0.845588
  eval_loss = 0.505248
  global_step = 343
  loss = 0.505248
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that the Dev set accuracy was 84.55%. Small sets like MRPC have a
high variance in the Dev set accuracy, even when starting from the same
pre-training checkpoint. If you re-run multiple times (making sure to point to
different &lt;code&gt;output_dir&lt;/code&gt;), you should see results between 84% and 88%.&lt;/p&gt;
&lt;p&gt;A few other pre-trained models are implemented off-the-shelf in
&lt;code&gt;run_classifier.py&lt;/code&gt;, so it should be straightforward to follow those examples to
use BERT for any single-sentence or sentence-pair classification task.&lt;/p&gt;
&lt;p&gt;Note: You might see a message &lt;code&gt;Running train on CPU&lt;/code&gt;. This really just means
that it's running on something other than a Cloud TPU, which includes a GPU.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-prediction-from-classifier" class="anchor" aria-hidden="true" href="#prediction-from-classifier"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prediction from classifier&lt;/h4&gt;
&lt;p&gt;Once you have trained your classifier you can use it in inference mode by using
the --do_predict=true command. You need to have a file named test.tsv in the
input folder. Output will be created in file called test_results.tsv in the
output folder. Each line will contain output for each sample, columns are the
class probabilities.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue
&lt;span class="pl-k"&gt;export&lt;/span&gt; TRAINED_CLASSIFIER=/path/to/fine/tuned/classifier

python run_classifier.py \
  --task_name=MRPC \
  --do_predict=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$TRAINED_CLASSIFIER&lt;/span&gt; \
  --max_seq_length=128 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-squad-11" class="anchor" aria-hidden="true" href="#squad-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 1.1&lt;/h3&gt;
&lt;p&gt;The Stanford Question Answering Dataset (SQuAD) is a popular question answering
benchmark dataset. BERT (at the time of the release) obtains state-of-the-art
results on SQuAD with almost no task-specific network architecture modifications
or data augmentation. However, it does require semi-complex data pre-processing
and post-processing to deal with (a) the variable-length nature of SQuAD context
paragraphs, and (b) the character-level answer annotations which are used for
SQuAD training. This processing is implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD, you will first need to download the dataset. The
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD website&lt;/a&gt; does not seem to
link to the v1.1 datasets any longer, but the necessary files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json" rel="nofollow"&gt;train-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json" rel="nofollow"&gt;dev-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py"&gt;evaluate-v1.1.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The state-of-the-art SQuAD results from the paper currently cannot be reproduced
on a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 does
not seem to fit on a 12GB GPU using &lt;code&gt;BERT-Large&lt;/code&gt;). However, a reasonably strong
&lt;code&gt;BERT-Base&lt;/code&gt; model can be trained on the GPU with these hyperparameters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=12 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=/tmp/squad_base/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The dev set predictions will be saved into a file called &lt;code&gt;predictions.json&lt;/code&gt; in
the &lt;code&gt;output_dir&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/evaluate-v1.1.py &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json ./squad/predictions.json&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Which should produce an output like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 88.41249612335034, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 81.2488174077578}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see a result similar to the 88.5% reported in the paper for
&lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you have access to a Cloud TPU, you can train with &lt;code&gt;BERT-Large&lt;/code&gt;. Here is a
set of hyperparameters (slightly different than the paper) which consistently
obtain around 90.5%-91.0% F1 single-system trained only on SQuAD:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For example, one random run with these parameters produces the following Dev
scores:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 90.87081895814865, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 84.38978240302744}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you fine-tune for one epoch on
&lt;a href="http://nlp.cs.washington.edu/triviaqa/" rel="nofollow"&gt;TriviaQA&lt;/a&gt; before this the results will
be even better, but you will need to convert TriviaQA into the SQuAD json
format.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-squad-20" class="anchor" aria-hidden="true" href="#squad-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 2.0&lt;/h3&gt;
&lt;p&gt;This model is also implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD 2.0, you will first need to download the dataset. The necessary
files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json" rel="nofollow"&gt;train-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json" rel="nofollow"&gt;dev-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/" rel="nofollow"&gt;evaluate-v2.0.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On Cloud TPU you can run with BERT-Large as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We assume you have copied everything from the output directory to a local
directory called ./squad/. The initial dev set predictions will be at
./squad/predictions.json and the differences between the score of no answer ("")
and the best non-null answer for each question will be in the file
./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Run this script to tune a threshold for predicting null versus non-null answers:&lt;/p&gt;
&lt;p&gt;python $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json
./squad/predictions.json --na-prob-file ./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Assume the script outputs "best_f1_thresh" THRESH. (Typical values are between
-1.0 and -5.0). You can now re-run the model to generate predictions with the
derived threshold or alternatively you can extract the appropriate answers from
./squad/nbest_predictions.json.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=False \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True \
  --null_score_diff_threshold=&lt;span class="pl-smi"&gt;$THRESH&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-out-of-memory-issues" class="anchor" aria-hidden="true" href="#out-of-memory-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Out-of-memory issues&lt;/h3&gt;
&lt;p&gt;All experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB of
device RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely
to encounter out-of-memory issues if you use the same hyperparameters described
in the paper.&lt;/p&gt;
&lt;p&gt;The factors that affect memory usage are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;max_seq_length&lt;/code&gt;&lt;/strong&gt;: The released models were trained with sequence lengths
up to 512, but you can fine-tune with a shorter max sequence length to save
substantial memory. This is controlled by the &lt;code&gt;max_seq_length&lt;/code&gt; flag in our
example code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;train_batch_size&lt;/code&gt;&lt;/strong&gt;: The memory usage is also directly proportional to
the batch size.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model type, &lt;code&gt;BERT-Base&lt;/code&gt; vs. &lt;code&gt;BERT-Large&lt;/code&gt;&lt;/strong&gt;: The &lt;code&gt;BERT-Large&lt;/code&gt; model
requires significantly more memory than &lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimizer&lt;/strong&gt;: The default optimizer for BERT is Adam, which requires a lot
of extra memory to store the &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;v&lt;/code&gt; vectors. Switching to a more memory
efficient optimizer can reduce memory usage, but can also affect the
results. We have not experimented with other optimizers for fine-tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the default training scripts (&lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;run_squad.py&lt;/code&gt;), we
benchmarked the maximum batch size on single Titan X GPU (12GB RAM) with
TensorFlow 1.11.0:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Seq Length&lt;/th&gt;
&lt;th&gt;Max Batch Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Base&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Large&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unfortunately, these max batch sizes for &lt;code&gt;BERT-Large&lt;/code&gt; are so small that they
will actually harm the model accuracy, regardless of the learning rate used. We
are working on adding code to this repository which will allow much larger
effective batch sizes to be used on the GPU. The code will be based on one (or
both) of the following techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gradient accumulation&lt;/strong&gt;: The samples in a minibatch are typically
independent with respect to gradient computation (excluding batch
normalization, which is not used here). This means that the gradients of
multiple smaller minibatches can be accumulated before performing the weight
update, and this will be exactly equivalent to a single larger update.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/openai/gradient-checkpointing"&gt;&lt;strong&gt;Gradient checkpointing&lt;/strong&gt;&lt;/a&gt;:
The major use of GPU/TPU memory during DNN training is caching the
intermediate activations in the forward pass that are necessary for
efficient computation in the backward pass. "Gradient checkpointing" trades
memory for compute time by re-computing the activations in an intelligent
way.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;However, this is not implemented in the current release.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-to-extract-fixed-feature-vectors-like-elmo" class="anchor" aria-hidden="true" href="#using-bert-to-extract-fixed-feature-vectors-like-elmo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT to extract fixed feature vectors (like ELMo)&lt;/h2&gt;
&lt;p&gt;In certain cases, rather than fine-tuning the entire pre-trained model
end-to-end, it can be beneficial to obtained &lt;em&gt;pre-trained contextual
embeddings&lt;/em&gt;, which are fixed contextual representations of each input token
generated from the hidden layers of the pre-trained model. This should also
mitigate most of the out-of-memory issues.&lt;/p&gt;
&lt;p&gt;As an example, we include the script &lt;code&gt;extract_features.py&lt;/code&gt; which can be used
like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Sentence A and Sentence B are separated by the ||| delimiter for sentence&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pair tasks like question answering and entailment.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; For single sentence inputs, put one sentence per line and DON'T use the&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; delimiter.&lt;/span&gt;
&lt;span class="pl-c1"&gt;echo&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Who was Jim Henson ? ||| Jim Henson was a puppeteer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; /tmp/input.txt

python extract_features.py \
  --input_file=/tmp/input.txt \
  --output_file=/tmp/output.jsonl \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --layers=-1,-2,-3,-4 \
  --max_seq_length=128 \
  --batch_size=8&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will create a JSON file (one line per line of input) containing the BERT
activations from each Transformer layer specified by &lt;code&gt;layers&lt;/code&gt; (-1 is the final
hidden layer of the Transformer, etc.)&lt;/p&gt;
&lt;p&gt;Note that this script will produce very large output files (by default, around
15kb for every input token).&lt;/p&gt;
&lt;p&gt;If you need to maintain alignment between the original and tokenized words (for
projecting training labels), see the &lt;a href="#tokenization"&gt;Tokenization&lt;/a&gt; section
below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You may see a message like &lt;code&gt;Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict.&lt;/code&gt; This message is expected, it
just means that we are using the &lt;code&gt;init_from_checkpoint()&lt;/code&gt; API rather than the
saved model API. If you don't specify a checkpoint or specify an invalid
checkpoint, this script will complain.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tokenization" class="anchor" aria-hidden="true" href="#tokenization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;For sentence-level tasks (or sentence-pair) tasks, tokenization is very simple.
Just follow the example code in &lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;extract_features.py&lt;/code&gt;.
The basic procedure for sentence-level tasks is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Instantiate an instance of &lt;code&gt;tokenizer = tokenization.FullTokenizer&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tokenize the raw text with &lt;code&gt;tokens = tokenizer.tokenize(raw_text)&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Truncate to the maximum sequence length. (You can use up to 512, but you
probably want to use shorter if possible for memory and speed reasons.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[SEP]&lt;/code&gt; tokens in the right place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Word-level and span-level tasks (e.g., SQuAD and NER) are more complex, since
you need to maintain alignment between your input text and output text so that
you can project your training labels. SQuAD is a particularly complex example
because the input labels are &lt;em&gt;character&lt;/em&gt;-based, and SQuAD paragraphs are often
longer than our maximum sequence length. See the code in &lt;code&gt;run_squad.py&lt;/code&gt; to show
how we handle this.&lt;/p&gt;
&lt;p&gt;Before we describe the general recipe for handling word-level tasks, it's
important to understand what exactly our tokenizer is doing. It has three main
steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Text normalization&lt;/strong&gt;: Convert all whitespace characters to spaces, and
(for the &lt;code&gt;Uncased&lt;/code&gt; model) lowercase the input and strip out accent markers.
E.g., &lt;code&gt;John Johanson's, → john johanson's,&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Punctuation splitting&lt;/strong&gt;: Split &lt;em&gt;all&lt;/em&gt; punctuation characters on both sides
(i.e., add whitespace around all punctuation characters). Punctuation
characters are defined as (a) Anything with a &lt;code&gt;P*&lt;/code&gt; Unicode class, (b) any
non-letter/number/space ASCII character (e.g., characters like &lt;code&gt;$&lt;/code&gt; which are
technically not punctuation). E.g., &lt;code&gt;john johanson's, → john johanson ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;WordPiece tokenization&lt;/strong&gt;: Apply whitespace tokenization to the output of
the above procedure, and apply
&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py"&gt;WordPiece&lt;/a&gt;
tokenization to each token separately. (Our implementation is directly based
on the one from &lt;code&gt;tensor2tensor&lt;/code&gt;, which is linked). E.g., &lt;code&gt;john johanson ' s , → john johan ##son ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The advantage of this scheme is that it is "compatible" with most existing
English tokenizers. For example, imagine that you have a part-of-speech tagging
task which looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input:  John Johanson 's   house
Labels: NNP  NNP      POS NN
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tokenized output will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Tokens: john johan ##son ' s house
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Crucially, this would be the same output as if the raw text were &lt;code&gt;John Johanson's house&lt;/code&gt; (with no space before the &lt;code&gt;'s&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you have a pre-tokenized representation with word-level annotations, you can
simply tokenize each input word independently, and deterministically maintain an
original-to-tokenized alignment:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Input&lt;/span&gt;
orig_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;John&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Johanson&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;'s&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;house&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
labels      &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;POS&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Output&lt;/span&gt;
bert_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; []

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Token map will be an int -&amp;gt; int mapping between the `orig_tokens` index and&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; the `bert_tokens` index.&lt;/span&gt;
orig_to_tok_map &lt;span class="pl-k"&gt;=&lt;/span&gt; []

tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenization.FullTokenizer(
    &lt;span class="pl-v"&gt;vocab_file&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;vocab_file, &lt;span class="pl-v"&gt;do_lower_case&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[CLS]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;for&lt;/span&gt; orig_token &lt;span class="pl-k"&gt;in&lt;/span&gt; orig_tokens:
  orig_to_tok_map.append(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(bert_tokens))
  bert_tokens.extend(tokenizer.tokenize(orig_token))
bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[SEP]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; bert_tokens == ["[CLS]", "john", "johan", "##son", "'", "s", "house", "[SEP]"]&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; orig_to_tok_map == [1, 2, 4, 6]&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now &lt;code&gt;orig_to_tok_map&lt;/code&gt; can be used to project &lt;code&gt;labels&lt;/code&gt; to the tokenized
representation.&lt;/p&gt;
&lt;p&gt;There are common English tokenization schemes which will cause a slight mismatch
between how BERT was pre-trained. For example, if your input tokenization splits
off contractions like &lt;code&gt;do n't&lt;/code&gt;, this will cause a mismatch. If it is possible to
do so, you should pre-process your data to convert these back to raw-looking
text, but if it's not possible, this mismatch is likely not a big deal.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-training-with-bert" class="anchor" aria-hidden="true" href="#pre-training-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training with BERT&lt;/h2&gt;
&lt;p&gt;We are releasing code to do "masked LM" and "next sentence prediction" on an
arbitrary text corpus. Note that this is &lt;em&gt;not&lt;/em&gt; the exact code that was used for
the paper (the original code was written in C++, and had some additional
complexity), but this code does generate pre-training data as described in the
paper.&lt;/p&gt;
&lt;p&gt;Here's how to run the data generation. The input is a plain text file, with one
sentence per line. (It is important that these be actual sentences for the "next
sentence prediction" task). Documents are delimited by empty lines. The output
is a set of &lt;code&gt;tf.train.Example&lt;/code&gt;s serialized into &lt;code&gt;TFRecord&lt;/code&gt; file format.&lt;/p&gt;
&lt;p&gt;You can perform sentence segmentation with an off-the-shelf NLP toolkit such as
&lt;a href="https://spacy.io/" rel="nofollow"&gt;spaCy&lt;/a&gt;. The &lt;code&gt;create_pretraining_data.py&lt;/code&gt; script will
concatenate segments until they reach the maximum sequence length to minimize
computational waste from padding (see the script for more details). However, you
may want to intentionally add a slight amount of noise to your input data (e.g.,
randomly truncate 2% of input segments) to make it more robust to non-sentential
input during fine-tuning.&lt;/p&gt;
&lt;p&gt;This script stores all of the examples for the entire input file in memory, so
for large data files you should shard the input file and call the script
multiple times. (You can pass in a file glob to &lt;code&gt;run_pretraining.py&lt;/code&gt;, e.g.,
&lt;code&gt;tf_examples.tf_record*&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;max_predictions_per_seq&lt;/code&gt; is the maximum number of masked LM predictions per
sequence. You should set this to around &lt;code&gt;max_seq_length&lt;/code&gt; * &lt;code&gt;masked_lm_prob&lt;/code&gt; (the
script doesn't do that automatically because the exact value needs to be passed
to both scripts).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python create_pretraining_data.py \
  --input_file=./sample_text.txt \
  --output_file=/tmp/tf_examples.tfrecord \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's how to run the pre-training. Do not include &lt;code&gt;init_checkpoint&lt;/code&gt; if you are
pre-training from scratch. The model configuration (including vocab size) is
specified in &lt;code&gt;bert_config_file&lt;/code&gt;. This demo code only pre-trains for a small
number of steps (20), but in practice you will probably want to set
&lt;code&gt;num_train_steps&lt;/code&gt; to 10000 steps or more. The &lt;code&gt;max_seq_length&lt;/code&gt; and
&lt;code&gt;max_predictions_per_seq&lt;/code&gt; parameters passed to &lt;code&gt;run_pretraining.py&lt;/code&gt; must be the
same as &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_pretraining.py \
  --input_file=/tmp/tf_examples.tfrecord \
  --output_dir=/tmp/pretraining_output \
  --do_train=True \
  --do_eval=True \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --train_batch_size=32 \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --num_train_steps=20 \
  --num_warmup_steps=10 \
  --learning_rate=2e-5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will produce an output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  global_step = 20
  loss = 0.0979674
  masked_lm_accuracy = 0.985479
  masked_lm_loss = 0.0979328
  next_sentence_accuracy = 1.0
  next_sentence_loss = 3.45724e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that since our &lt;code&gt;sample_text.txt&lt;/code&gt; file is very small, this example training
will overfit that data in only a few steps and produce unrealistically high
accuracy numbers.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-tips-and-caveats" class="anchor" aria-hidden="true" href="#pre-training-tips-and-caveats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training tips and caveats&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;If using your own vocabulary, make sure to change &lt;code&gt;vocab_size&lt;/code&gt; in
&lt;code&gt;bert_config.json&lt;/code&gt;. If you use a larger vocabulary without changing this,
you will likely get NaNs when training on GPU or TPU due to unchecked
out-of-bounds access.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;If your task has a large domain-specific corpus available (e.g., "movie
reviews" or "scientific papers"), it will likely be beneficial to run
additional steps of pre-training on your corpus, starting from the BERT
checkpoint.&lt;/li&gt;
&lt;li&gt;The learning rate we used in the paper was 1e-4. However, if you are doing
additional steps of pre-training starting from an existing BERT checkpoint,
you should use a smaller learning rate (e.g., 2e-5).&lt;/li&gt;
&lt;li&gt;Current BERT models are English-only, but we do plan to release a
multilingual model which has been pre-trained on a lot of languages in the
near future (hopefully by the end of November 2018).&lt;/li&gt;
&lt;li&gt;Longer sequences are disproportionately expensive because attention is
quadratic to the sequence length. In other words, a batch of 64 sequences of
length 512 is much more expensive than a batch of 256 sequences of
length 128. The fully-connected/convolutional cost is the same, but the
attention cost is far greater for the 512-length sequences. Therefore, one
good recipe is to pre-train for, say, 90,000 steps with a sequence length of
128 and then for 10,000 additional steps with a sequence length of 512. The
very long sequences are mostly needed to learn positional embeddings, which
can be learned fairly quickly. Note that this does require generating the
data twice with different values of &lt;code&gt;max_seq_length&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you are pre-training from scratch, be prepared that pre-training is
computationally expensive, especially on GPUs. If you are pre-training from
scratch, our recommended recipe is to pre-train a &lt;code&gt;BERT-Base&lt;/code&gt; on a single
&lt;a href="https://cloud.google.com/tpu/docs/pricing" rel="nofollow"&gt;preemptible Cloud TPU v2&lt;/a&gt;, which
takes about 2 weeks at a cost of about $500 USD (based on the pricing in
October 2018). You will have to scale down the batch size when only training
on a single Cloud TPU, compared to what was used in the paper. It is
recommended to use the largest batch size that fits into TPU memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-data" class="anchor" aria-hidden="true" href="#pre-training-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training data&lt;/h3&gt;
&lt;p&gt;We will &lt;strong&gt;not&lt;/strong&gt; be able to release the pre-processed datasets used in the paper.
For Wikipedia, the recommended pre-processing is to download
&lt;a href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2" rel="nofollow"&gt;the latest dump&lt;/a&gt;,
extract the text with
&lt;a href="https://github.com/attardi/wikiextractor"&gt;&lt;code&gt;WikiExtractor.py&lt;/code&gt;&lt;/a&gt;, and then apply
any necessary cleanup to convert it into plain text.&lt;/p&gt;
&lt;p&gt;Unfortunately the researchers who collected the
&lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt; no longer have it available for
public download. The
&lt;a href="https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html" rel="nofollow"&gt;Project Guttenberg Dataset&lt;/a&gt;
is a somewhat smaller (200M word) collection of older books that are public
domain.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://commoncrawl.org/" rel="nofollow"&gt;Common Crawl&lt;/a&gt; is another very large collection of
text, but you will likely have to do substantial pre-processing and cleanup to
extract a usable corpus for pre-training BERT.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-learning-a-new-wordpiece-vocabulary" class="anchor" aria-hidden="true" href="#learning-a-new-wordpiece-vocabulary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learning a new WordPiece vocabulary&lt;/h3&gt;
&lt;p&gt;This repository does not include code for &lt;em&gt;learning&lt;/em&gt; a new WordPiece vocabulary.
The reason is that the code used in the paper was implemented in C++ with
dependencies on Google's internal libraries. For English, it is almost always
better to just start with our vocabulary and pre-trained models. For learning
vocabularies of other languages, there are a number of open source options
available. However, keep in mind that these are not compatible with our
&lt;code&gt;tokenization.py&lt;/code&gt; library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;Google's SentencePiece library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py"&gt;tensor2tensor's WordPiece generation script&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rsennrich/subword-nmt"&gt;Rico Sennrich's Byte Pair Encoding library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-in-colab" class="anchor" aria-hidden="true" href="#using-bert-in-colab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT in Colab&lt;/h2&gt;
&lt;p&gt;If you want to use BERT with &lt;a href="https://colab.research.google.com" rel="nofollow"&gt;Colab&lt;/a&gt;, you can
get started with the notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".
&lt;strong&gt;At the time of this writing (October 31st, 2018), Colab users can access a
Cloud TPU completely for free.&lt;/strong&gt; Note: One per user, availability limited,
requires a Google Cloud Platform account with storage (although storage may be
purchased with free credit for signing up with GCP), and this capability may not
longer be available in the future. Click on the BERT Colab that was just linked
for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-is-this-code-compatible-with-cloud-tpus-what-about-gpus" class="anchor" aria-hidden="true" href="#is-this-code-compatible-with-cloud-tpus-what-about-gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is this code compatible with Cloud TPUs? What about GPUs?&lt;/h4&gt;
&lt;p&gt;Yes, all of the code in this repository works out-of-the-box with CPU, GPU, and
Cloud TPU. However, GPU training is single-GPU only.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-i-am-getting-out-of-memory-errors-what-is-wrong" class="anchor" aria-hidden="true" href="#i-am-getting-out-of-memory-errors-what-is-wrong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I am getting out-of-memory errors, what is wrong?&lt;/h4&gt;
&lt;p&gt;See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for more
information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-pytorch-version-available" class="anchor" aria-hidden="true" href="#is-there-a-pytorch-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a PyTorch version available?&lt;/h4&gt;
&lt;p&gt;There is no official PyTorch implementation. However, NLP researchers from
HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-chainer-version-available" class="anchor" aria-hidden="true" href="#is-there-a-chainer-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a Chainer version available?&lt;/h4&gt;
&lt;p&gt;There is no official Chainer implementation. However, Sosuke Kobayashi made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the Chainer
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-in-other-languages-be-released" class="anchor" aria-hidden="true" href="#will-models-in-other-languages-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models in other languages be released?&lt;/h4&gt;
&lt;p&gt;Yes, we plan to release a multi-lingual BERT model in the near future. We cannot
make promises about exactly which languages will be included, but it will likely
be a single model which includes &lt;em&gt;most&lt;/em&gt; of the languages which have a
significantly-sized Wikipedia.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-larger-than-bert-large-be-released" class="anchor" aria-hidden="true" href="#will-models-larger-than-bert-large-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models larger than &lt;code&gt;BERT-Large&lt;/code&gt; be released?&lt;/h4&gt;
&lt;p&gt;So far we have not attempted to train anything larger than &lt;code&gt;BERT-Large&lt;/code&gt;. It is
possible that we will release larger models if we are able to obtain significant
improvements.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-what-license-is-this-library-released-under" class="anchor" aria-hidden="true" href="#what-license-is-this-library-released-under"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What license is this library released under?&lt;/h4&gt;
&lt;p&gt;All code &lt;em&gt;and&lt;/em&gt; models are released under the Apache 2.0 license. See the
&lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-how-do-i-cite-bert" class="anchor" aria-hidden="true" href="#how-do-i-cite-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I cite BERT?&lt;/h4&gt;
&lt;p&gt;For now, cite &lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;the Arxiv paper&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we submit the paper to a conference or journal, we will update the BibTeX.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;This is not an official Google product.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact-information" class="anchor" aria-hidden="true" href="#contact-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact information&lt;/h2&gt;
&lt;p&gt;For help or issues using BERT, please submit a GitHub issue.&lt;/p&gt;
&lt;p&gt;For personal communication related to BERT, please contact Jacob Devlin
(&lt;code&gt;jacobdevlin@google.com&lt;/code&gt;), Ming-Wei Chang (&lt;code&gt;mingweichang@google.com&lt;/code&gt;), or
Kenton Lee (&lt;code&gt;kentonl@google.com&lt;/code&gt;).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/bert</guid><pubDate>Sat, 04 Jan 2020 00:05:00 GMT</pubDate></item><item><title>tensorflow/models #6 in Python, Today</title><link>https://github.com/tensorflow/models</link><description>&lt;p&gt;&lt;i&gt;Models and examples built with TensorFlow&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-models" class="anchor" aria-hidden="true" href="#tensorflow-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Models&lt;/h1&gt;
&lt;p&gt;This repository contains a number of different models implemented in &lt;a href="https://www.tensorflow.org" rel="nofollow"&gt;TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;The &lt;a href="official"&gt;official models&lt;/a&gt; are a collection of example models that use TensorFlow's high-level APIs. They are intended to be well-maintained, tested, and kept up to date with the latest stable TensorFlow API. They should also be reasonably optimized for fast performance while still being easy to read. We especially recommend newer TensorFlow users to start here.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/tensorflow/models/tree/master/research"&gt;research models&lt;/a&gt; are a large collection of models implemented in TensorFlow by researchers. They are not officially supported or available in release branches; it is up to the individual researchers to maintain the models and/or provide support on issues and pull requests.&lt;/p&gt;
&lt;p&gt;The &lt;a href="samples"&gt;samples folder&lt;/a&gt; contains code snippets and smaller models that demonstrate features of TensorFlow, including code presented in various blog posts.&lt;/p&gt;
&lt;p&gt;The &lt;a href="tutorials"&gt;tutorials folder&lt;/a&gt; is a collection of models described in the &lt;a href="https://www.tensorflow.org/tutorials/" rel="nofollow"&gt;TensorFlow tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-guidelines" class="anchor" aria-hidden="true" href="#contribution-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution guidelines&lt;/h2&gt;
&lt;p&gt;If you want to contribute to models, be sure to review the &lt;a href="CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensorflow</author><guid isPermaLink="false">https://github.com/tensorflow/models</guid><pubDate>Sat, 04 Jan 2020 00:06:00 GMT</pubDate></item><item><title>hankcs/HanLP #7 in Python, Today</title><link>https://github.com/hankcs/HanLP</link><description>&lt;p&gt;&lt;i&gt;Natural Language Processing for the next decade&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-hanlp-han-language-processing" class="anchor" aria-hidden="true" href="#hanlp-han-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HanLP: Han Language Processing&lt;/h1&gt;
&lt;p&gt;The multilingual NLP library for researchers and companies, built on TensorFlow 2.0, for advancing state-of-the-art deep learning techniques in both academia and industry. HanLP was designed from day one to be efficient, user friendly and extendable. It comes with pretrained models for various human languages including English, Chinese and many more. Currently, HanLP 2.0 is in alpha stage with more killer features on the roadmap. Discussions are welcomed on our &lt;a href="https://bbs.hankcs.com/" rel="nofollow"&gt;forum&lt;/a&gt;, while bug reports and feature requests are reserved for GitHub issues. For Java users, please checkout the &lt;a href="https://github.com/hankcs/HanLP/tree/1.x"&gt;1.x&lt;/a&gt; branch.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip3 install hanlp&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;HanLP requires Python 3.6 or later. GPU/TPU is suggested but not mandatory.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-tokenization" class="anchor" aria-hidden="true" href="#tokenization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tokenization&lt;/h3&gt;
&lt;p&gt;For an end user, the basic flow starts with loading some pretrained models from disk or Internet. Each model has an identifier, which could be one path on your computer or an URL to any public servers. Here, let's load a tokenizer called &lt;code&gt;CTB6_CONVSEG&lt;/code&gt; with 2 lines of code.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; hanlp
&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; hanlp.load(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;CTB6_CONVSEG&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;HanLP will automatically resolve the identifier &lt;code&gt;CTB6_CONVSEG&lt;/code&gt; to an &lt;a href="https://file.hankcs.com/hanlp/cws/ctb6-convseg-cws_20191230_184525.zip" rel="nofollow"&gt;URL&lt;/a&gt;, then download it and unzip it, as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Downloading https://file.hankcs.com/hanlp/cws/ctb6-convseg-cws_20191230_184525.zip to /home/hankcs/.hanlp/cws/ctb6-convseg-cws_20191230_184525.zip
100.00%, 7.4 MB/7.4 MB, 64.1 MB/s, ETA 0 s      
Extracting /home/hankcs/.hanlp/cws/ctb6-convseg-cws_20191230_184525.zip to /home/hankcs/.hanlp/cws	
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Due to the huge network traffic (especially from mainland China), it could fail temporally then you need to retry or manually download and unzip it to the path shown in your terminal .&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once the model is loaded, you can then tokenize one sentence through &lt;code&gt;predict&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; tokenizer.predict(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;商品和服务&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;商品&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;和&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;服务&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-going-further" class="anchor" aria-hidden="true" href="#going-further"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Going Further&lt;/h4&gt;
&lt;p&gt;However, you can predict much faster. In the era of deep learning, batched computation usually gives a linear scale-up factor of &lt;code&gt;batch_size&lt;/code&gt;. So, you can predict multiple sentences at once, at the cost of GPU memory.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; tokenizer.predict([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;萨哈夫说，伊拉克将同联合国销毁伊拉克大规模杀伤性武器特别委员会继续保持合作。&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])
[[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;萨哈夫&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;说&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;，&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;伊拉克&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;将&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;同&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;联合国&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;销毁&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;伊拉克&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;大规模&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;杀伤性&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;武器&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;特别&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;委员会&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;继续&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;保持&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;合作&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;。&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
 [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;上海&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;华安&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;工业&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;（&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;集团&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;）&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;公司&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;董事长&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;谭旭光&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;和&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;秘书&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;张晚霞&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;来到&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;美国&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;纽约&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;现代&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;艺术&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;博物馆&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;参观&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;。&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's it! You're now ready to employ the latest DL models from HanLP in your research and work. Here are some tips if you want to go further.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Print &lt;code&gt;hanlp.pretrained.ALL&lt;/code&gt; to list all the pretrained models available in HanLP.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use &lt;code&gt;hanlp.pretrained.*&lt;/code&gt; to browse pretrained models by categories of NLP tasks. You can use the variables to identify them too.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; hanlp.pretrained.cws.&lt;span class="pl-c1"&gt;CTB6_CONVSEG&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https://file.hankcs.com/hanlp/cws/ctb6-convseg-cws_20191230_184525.zip&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-part-of-speech-tagging" class="anchor" aria-hidden="true" href="#part-of-speech-tagging"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part-of-Speech Tagging&lt;/h3&gt;
&lt;p&gt;Taggers take lists of tokens as input, then outputs one tag for each token.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; tagger &lt;span class="pl-k"&gt;=&lt;/span&gt; hanlp.load(hanlp.pretrained.pos.&lt;span class="pl-c1"&gt;CTB5_POS_RNN_FASTTEXT&lt;/span&gt;)
&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; tagger.predict([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;我&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;的&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;希望&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;是&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;希望&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;和平&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;PN&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;DEG&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NN&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VV&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NN&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Did you notice the different pos tags for the same word &lt;code&gt;希望&lt;/code&gt; ("hope")? The first one means "my dream" as a noun while the later means "want" as a verb. This tagger uses fasttext[^fasttext] as its embedding layer, which is free from OOV.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-named-entity-recognition" class="anchor" aria-hidden="true" href="#named-entity-recognition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Named Entity Recognition&lt;/h3&gt;
&lt;p&gt;The NER component requires tokenized tokens as input, then outputs the entities along with their types and spans.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; recognizer &lt;span class="pl-k"&gt;=&lt;/span&gt; hanlp.load(hanlp.pretrained.ner.&lt;span class="pl-c1"&gt;MSRA_NER_BERT_BASE_CN&lt;/span&gt;)
&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; recognizer.predict([&lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
                        &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;萨哈夫说，伊拉克将同联合国销毁伊拉克大规模杀伤性武器特别委员会继续保持合作。&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)])
[[(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;上海华安工业（集团）公司&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NT&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;12&lt;/span&gt;), (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;谭旭光&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NR&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;15&lt;/span&gt;, &lt;span class="pl-c1"&gt;18&lt;/span&gt;), (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;张晚霞&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NR&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;21&lt;/span&gt;, &lt;span class="pl-c1"&gt;24&lt;/span&gt;), (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;美国&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NS&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;26&lt;/span&gt;, &lt;span class="pl-c1"&gt;28&lt;/span&gt;), (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;纽约现代艺术博物馆&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NS&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;28&lt;/span&gt;, &lt;span class="pl-c1"&gt;37&lt;/span&gt;)], 
 [(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;萨哈夫&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NR&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;), (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;伊拉克&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NS&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;), (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;联合国销毁伊拉克大规模杀伤性武器特别委员会&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NT&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;10&lt;/span&gt;, &lt;span class="pl-c1"&gt;31&lt;/span&gt;)]]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Recognizers take lists of tokens as input, so don't forget to wrap your sentence with &lt;code&gt;list&lt;/code&gt;. For the outputs, each tuple stands for &lt;code&gt;(entity, type, begin, end)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This &lt;code&gt;MSRA_NER_BERT_BASE_CN&lt;/code&gt; is the state-of-the-art NER model based on BERT[^bert]. You can read its evaluation log through:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ cat /home/hankcs/.hanlp/ner/ner_bert_base_msra_20191230_205748/test.log 
19-12-30 02:37:28 INFO Evaluation results &lt;span class="pl-k"&gt;for&lt;/span&gt; test.tsv - loss: 1.5659 - f1: 0.9506 - speed: 48.40 sample/sec 
processed 168612 tokens with 5268 phrases&lt;span class="pl-k"&gt;;&lt;/span&gt; found: 5346 phrases&lt;span class="pl-k"&gt;;&lt;/span&gt; correct: 5045.
accuracy:  99.34%&lt;span class="pl-k"&gt;;&lt;/span&gt; precision:  94.37%&lt;span class="pl-k"&gt;;&lt;/span&gt; recall:  95.77%&lt;span class="pl-k"&gt;;&lt;/span&gt; FB1:  95.06
               NR: precision:  97.05%&lt;span class="pl-k"&gt;;&lt;/span&gt; recall:  98.43%&lt;span class="pl-k"&gt;;&lt;/span&gt; FB1:  97.73  1356
               NS: precision:  96.08%&lt;span class="pl-k"&gt;;&lt;/span&gt; recall:  95.83%&lt;span class="pl-k"&gt;;&lt;/span&gt; FB1:  95.95  2628
               NT: precision:  88.40%&lt;span class="pl-k"&gt;;&lt;/span&gt; recall:  92.90%&lt;span class="pl-k"&gt;;&lt;/span&gt; FB1:  90.59  1362&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-syntactic-dependency-parsing" class="anchor" aria-hidden="true" href="#syntactic-dependency-parsing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syntactic Dependency Parsing&lt;/h3&gt;
&lt;p&gt;Parsing lies in the core of NLP. Without parsing, one cannot claim to be a NLP researcher or engineer. But using HanLP, it takes no more than two lines of code.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; syntactic_parser &lt;span class="pl-k"&gt;=&lt;/span&gt; hanlp.load(hanlp.pretrained.dep.&lt;span class="pl-c1"&gt;CTB7_BIAFFINE_DEP&lt;/span&gt;)
&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;print&lt;/span&gt;(syntactic_parser.predict([(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;中国&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NR&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;批准&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VV&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;设立&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VV&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;了&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;AS&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;三十万&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;CD&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;家&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;M&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;外商&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NN&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;投资&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NN&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;), (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;企业&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NN&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)]))
&lt;span class="pl-c1"&gt;1&lt;/span&gt;	中国	_	&lt;span class="pl-c1"&gt;NR&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;2&lt;/span&gt;	nsubj	_	_
&lt;span class="pl-c1"&gt;2&lt;/span&gt;	批准	_	&lt;span class="pl-c1"&gt;VV&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;0&lt;/span&gt;	root	_	_
&lt;span class="pl-c1"&gt;3&lt;/span&gt;	设立	_	&lt;span class="pl-c1"&gt;VV&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;2&lt;/span&gt;	ccomp	_	_
&lt;span class="pl-c1"&gt;4&lt;/span&gt;	了	_	&lt;span class="pl-c1"&gt;AS&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;3&lt;/span&gt;	asp	_	_
&lt;span class="pl-c1"&gt;5&lt;/span&gt;	三十万	_	&lt;span class="pl-c1"&gt;CD&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;6&lt;/span&gt;	nummod	_	_
&lt;span class="pl-c1"&gt;6&lt;/span&gt;	家	_	M	_	_	&lt;span class="pl-c1"&gt;9&lt;/span&gt;	clf	_	_
&lt;span class="pl-c1"&gt;7&lt;/span&gt;	外商	_	&lt;span class="pl-c1"&gt;NN&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;9&lt;/span&gt;	nn	_	_
&lt;span class="pl-c1"&gt;8&lt;/span&gt;	投资	_	&lt;span class="pl-c1"&gt;NN&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;9&lt;/span&gt;	nn	_	_
&lt;span class="pl-c1"&gt;9&lt;/span&gt;	企业	_	&lt;span class="pl-c1"&gt;NN&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;3&lt;/span&gt;	dobj	_	_&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Parsers take both tokens and part-of-speech tags as input. The output is a tree in CoNLL-X format[^conllx], which can be manipulated through the &lt;code&gt;CoNLLSentence&lt;/code&gt; class.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-semantic-dependency-parsing" class="anchor" aria-hidden="true" href="#semantic-dependency-parsing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Semantic Dependency Parsing&lt;/h3&gt;
&lt;p&gt;A graph is a generalized tree, which conveys more information about the semantic relations between tokens. HanLP implements the biaffine[^biaffine] model which delivers the SOTA performance.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; semantic_parser &lt;span class="pl-k"&gt;=&lt;/span&gt; hanlp.load(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;SEMEVAL16_NEWS_BIAFFINE&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;print&lt;/span&gt;(semantic_parser.predict([(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;中国&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NR&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;批准&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VV&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;设立&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VV&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;了&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;AS&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;三十万&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;CD&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;家&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;M&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;外商&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NN&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;投资&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NN&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;), (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;企业&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NN&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)]))
&lt;span class="pl-c1"&gt;1&lt;/span&gt;	中国	_	&lt;span class="pl-c1"&gt;NR&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;2&lt;/span&gt;	Agt	_	_
&lt;span class="pl-c1"&gt;1&lt;/span&gt;	中国	_	&lt;span class="pl-c1"&gt;NR&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;3&lt;/span&gt;	Agt	_	_
&lt;span class="pl-c1"&gt;2&lt;/span&gt;	批准	_	&lt;span class="pl-c1"&gt;VV&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;0&lt;/span&gt;	Aft	_	_
&lt;span class="pl-c1"&gt;3&lt;/span&gt;	设立	_	&lt;span class="pl-c1"&gt;VV&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;2&lt;/span&gt;	eProg	_	_
&lt;span class="pl-c1"&gt;4&lt;/span&gt;	了	_	&lt;span class="pl-c1"&gt;AS&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;3&lt;/span&gt;	mTime	_	_
&lt;span class="pl-c1"&gt;5&lt;/span&gt;	三十万	_	&lt;span class="pl-c1"&gt;CD&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;6&lt;/span&gt;	Quan	_	_
&lt;span class="pl-c1"&gt;6&lt;/span&gt;	家	_	M	_	_	&lt;span class="pl-c1"&gt;9&lt;/span&gt;	Qp	_	_
&lt;span class="pl-c1"&gt;7&lt;/span&gt;	外商	_	&lt;span class="pl-c1"&gt;NN&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;8&lt;/span&gt;	Agt	_	_
&lt;span class="pl-c1"&gt;8&lt;/span&gt;	投资	_	&lt;span class="pl-c1"&gt;NN&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;9&lt;/span&gt;	rDatv	_	_
&lt;span class="pl-c1"&gt;9&lt;/span&gt;	企业	_	&lt;span class="pl-c1"&gt;NN&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;2&lt;/span&gt;	Pat	_	_
&lt;span class="pl-c1"&gt;9&lt;/span&gt;	企业	_	&lt;span class="pl-c1"&gt;NN&lt;/span&gt;	_	_	&lt;span class="pl-c1"&gt;3&lt;/span&gt;	Prod	_	_&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The output is a &lt;code&gt;CoNLLSentence&lt;/code&gt; too. However, it's not a tree but a graph in which one node can have multiple heads, e.g. &lt;code&gt;中国&lt;/code&gt; has two heads (ID 2 and 3).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pipelines" class="anchor" aria-hidden="true" href="#pipelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pipelines&lt;/h3&gt;
&lt;p&gt;Since parsers require part-of-speech tagging and tokenization, while taggers expects tokenization to be done beforehand, wouldn't it be nice if we have a pipeline to connect the inputs and outputs, like a computation graph?&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;pipeline &lt;span class="pl-k"&gt;=&lt;/span&gt; hanlp.pipeline() \
    .append(hanlp.utils.rules.split_sentence, &lt;span class="pl-v"&gt;output_key&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;sentences&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) \
    .append(tokenizer, &lt;span class="pl-v"&gt;output_key&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tokens&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) \
    .append(tagger, &lt;span class="pl-v"&gt;output_key&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;part_of_speech_tags&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) \
    .append(syntactic_parser, &lt;span class="pl-v"&gt;input_key&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tokens&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;part_of_speech_tags&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;), &lt;span class="pl-v"&gt;output_key&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;syntactic_dependencies&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) \
    .append(semantic_parser, &lt;span class="pl-v"&gt;input_key&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tokens&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;part_of_speech_tags&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;), &lt;span class="pl-v"&gt;output_key&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;semantic_dependencies&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Notice that the first pipe is an old-school Python function &lt;code&gt;split_sentence&lt;/code&gt;, which splits the input text into a list of sentences. Then the later DL components can utilize the batch processing seamlessly. This results in a pipeline with one input (text) pipe, multiple flow pipes and one output (parsed document). You can print out the pipeline to check its structure.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; pipeline
[&lt;span class="pl-c1"&gt;None&lt;/span&gt;&lt;span class="pl-ii"&gt;-&amp;gt;&lt;/span&gt;LambdaComponent&lt;span class="pl-ii"&gt;-&amp;gt;&lt;/span&gt;sentences, sentences&lt;span class="pl-ii"&gt;-&amp;gt;&lt;/span&gt;NgramConvTokenizer&lt;span class="pl-ii"&gt;-&amp;gt;&lt;/span&gt;tokens, tokens&lt;span class="pl-ii"&gt;-&amp;gt;&lt;/span&gt;RNNPartOfSpeechTagger&lt;span class="pl-ii"&gt;-&amp;gt;&lt;/span&gt;part_of_speech_tags, (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tokens&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;part_of_speech_tags&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;span class="pl-ii"&gt;-&amp;gt;&lt;/span&gt;BiaffineDependencyParser&lt;span class="pl-ii"&gt;-&amp;gt;&lt;/span&gt;syntactic_dependencies, (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tokens&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;part_of_speech_tags&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;span class="pl-ii"&gt;-&amp;gt;&lt;/span&gt;BiaffineSemanticDependencyParser&lt;span class="pl-ii"&gt;-&amp;gt;&lt;/span&gt;semantic_dependencies]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This time, let's feed in a whole document, which might be the scenario in your daily work.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;print&lt;/span&gt;(pipeline(text))
{
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sentences&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;HanLP是一系列模型与算法组成的自然语言处理工具包，目标是普及自然语言处理在生产环境中的应用。&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;内部算法经过工业界和学术界考验，配套书籍《自然语言处理入门》已经出版。&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
  ],
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;tokens&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;HanLP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;是&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;一&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;系列&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;模型&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;与&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;算法&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;组成&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;的&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;自然&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;语言&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;处理&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;工具包&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;，&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;目标&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;是&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;普及&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;自然&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;语言&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;处理&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;在&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;生产&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;环境&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;中&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;的&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;应用&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;。&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;],
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;HanLP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;具备&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;功能&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;完善&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;、&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;性能&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;高效&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;、&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;架构&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;清晰&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;、&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;语料&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;时&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;新&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;、&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;可&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;自&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;定义&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;的&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;特点&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;。&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;],
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;内部&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;算法&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;经过&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;工业界&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;和&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;学术界&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;考验&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;，&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;配套&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;书籍&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;《&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;自然&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;语言&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;处理&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;入门&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;》&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;已经&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;出版&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;。&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
  ],
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;part_of_speech_tags&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NR&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VC&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;CD&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;M&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;CC&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VV&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;DEC&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VV&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PU&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VC&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VV&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VV&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;P&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;LC&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;DEG&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PU&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;],
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NR&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VV&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VA&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PU&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VA&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PU&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VA&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PU&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;LC&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VA&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PU&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VV&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;P&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VV&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;DEC&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PU&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;],
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;P&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;CC&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PU&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VV&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PU&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PU&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;AD&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;VV&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PU&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
  ],
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;syntactic_dependencies&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [
    [[&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;top&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;root&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nummod&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;11&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;clf&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;cc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nsubj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;11&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;rcmod&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;cpm&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;11&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nn&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;12&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nsubj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;ccomp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;12&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;dobj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;punct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;16&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;top&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;16&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;ccomp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;19&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nn&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;20&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nsubj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;17&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;26&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;assmod&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;23&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nn&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;24&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;lobj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;21&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;plmod&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;21&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;assm&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;20&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;dobj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;punct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]],
    [[&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nsubj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;root&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nsubj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;20&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;rcmod&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;punct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nsubj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;punct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;10&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nsubj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;punct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;13&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;lobj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;14&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;loc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;punct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;18&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mmod&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;18&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;advmod&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;cpm&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;dobj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;punct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]],
    [[&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nn&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;18&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nsubj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;18&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;prep&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;cc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nn&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pobj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;18&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;punct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;10&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;rcmod&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;15&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nn&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;15&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;punct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;15&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nn&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;15&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nn&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;15&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nn&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;18&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;nsubj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;15&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;punct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;18&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;advmod&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;root&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-c1"&gt;18&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;punct&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]]
  ],
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;semantic_dependencies&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [
    [[[&lt;span class="pl-c1"&gt;2&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Exp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;4&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Quan&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;8&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Poss&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;7&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mConj&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;8&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Datv&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;11&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;rProd&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;8&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mAux&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;11&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Desc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;12&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Datv&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;2&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;dClas&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;12&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Clas&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Cont&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;12&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPunc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPunc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;16&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Exp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;17&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mMod&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;2&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;eSucc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;19&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Desc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;20&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Pat&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;26&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;rProd&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;23&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPrep&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;23&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Desc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;20&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Loc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;23&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mRang&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;16&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Clas&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;16&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPunc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]]],
    [[[&lt;span class="pl-c1"&gt;2&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Poss&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;4&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Exp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;4&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPunc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;4&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;eCoo&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPunc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPunc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;10&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPunc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPunc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;12&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mTime&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;14&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPunc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;20&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Desc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;18&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mAux&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]]],
    [[[&lt;span class="pl-c1"&gt;2&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Desc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;9&lt;/span&gt;, &lt;span class="pl-c1"&gt;18&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Exp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Agt&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Exp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;4&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPrep&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;6&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPrep&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;7&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Datv&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;7&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPunc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;7&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;eCoo&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;13&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Desc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;18&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mTime&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Aft&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]], [[&lt;span class="pl-c1"&gt;18&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mPunc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]]]
  ]
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The output is a json &lt;code&gt;dict&lt;/code&gt;, which most people are familiar with.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feel free to add more pre/post-processing to the pipeline, including cleaning, custom dictionary etc.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;pipeline.save('zh.json')&lt;/code&gt; to save your pipeline and deploy it to your production server.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-train-your-own-models" class="anchor" aria-hidden="true" href="#train-your-own-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Train Your Own Models&lt;/h2&gt;
&lt;p&gt;To write DL models is not hard, the real hard thing is to write a model able to reproduce the score in papers. The snippet below shows how to train a 97% F1 cws model on MSR corpus.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; NgramConvTokenizer()
save_dir &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;data/model/cws/convseg-msr-nocrf-noembed&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
tokenizer.fit(&lt;span class="pl-c1"&gt;SIGHAN2005_MSR_TRAIN&lt;/span&gt;,
              &lt;span class="pl-c1"&gt;SIGHAN2005_MSR_VALID&lt;/span&gt;,
              save_dir,
              &lt;span class="pl-v"&gt;word_embed&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;class_name&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;HanLP&amp;gt;Word2VecEmbedding&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                          &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;config&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: {
                              &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;trainable&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;True&lt;/span&gt;,
                              &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;filepath&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;CONVSEG_W2V_NEWS_TENSITE_CHAR&lt;/span&gt;,
                              &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;expand_vocab&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;False&lt;/span&gt;,
                              &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;lowercase&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;False&lt;/span&gt;,
                          }},
              &lt;span class="pl-v"&gt;optimizer&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.keras.optimizers.Adam(&lt;span class="pl-v"&gt;learning_rate&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.001&lt;/span&gt;,
                                                 &lt;span class="pl-v"&gt;epsilon&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1e-8&lt;/span&gt;, &lt;span class="pl-v"&gt;clipnorm&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;5&lt;/span&gt;),
              &lt;span class="pl-v"&gt;epochs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;,
              &lt;span class="pl-v"&gt;window_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;,
              &lt;span class="pl-v"&gt;metrics&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;f1&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
              &lt;span class="pl-v"&gt;weight_norm&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
tokenizer.evaluate(&lt;span class="pl-c1"&gt;SIGHAN2005_MSR_TEST&lt;/span&gt;, &lt;span class="pl-v"&gt;save_dir&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;save_dir)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The training and evaluation logs are as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Train for 783 steps, validate for 87 steps
Epoch 1/100
783/783 [==============================] - 177s 226ms/step - loss: 15.6354 - f1: 0.8506 - val_loss: 9.9109 - val_f1: 0.9081
Epoch 2/100
236/783 [========&amp;gt;.....................] - ETA: 1:41 - loss: 9.0359 - f1: 0.9126
...
19-12-28 20:55:59 INFO Trained 100 epochs in 3 h 55 m 42 s, each epoch takes 2 m 21 s
19-12-28 20:56:06 INFO Evaluation results for msr_test_gold.utf8 - loss: 3.6579 - f1: 0.9715 - speed: 1173.80 sample/sec
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, you can train a sentiment classifier to classify the comments of hotels.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;save_dir &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;data/model/classification/chnsenticorp_bert_base&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
classifier &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTextClassifier(BertTextTransform(&lt;span class="pl-v"&gt;y_column&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;))
classifier.fit(&lt;span class="pl-c1"&gt;CHNSENTICORP_ERNIE_TRAIN&lt;/span&gt;, &lt;span class="pl-c1"&gt;CHNSENTICORP_ERNIE_VALID&lt;/span&gt;, save_dir,
               &lt;span class="pl-v"&gt;bert&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-chinese&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
classifier.load(save_dir)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(classifier.classify(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;前台客房服务态度非常好！早餐很丰富，房价很干净。再接再厉！&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;))
classifier.evaluate(&lt;span class="pl-c1"&gt;CHNSENTICORP_ERNIE_TEST&lt;/span&gt;, &lt;span class="pl-v"&gt;save_dir&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;save_dir)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Due to the size of models, and the fact that corpora are domain specific, HanLP has limited plan to distribute pretrained text classification models.&lt;/p&gt;
&lt;p&gt;For more training scripts, please refer to &lt;a href="https://github.com/hankcs/HanLP/tree/master/tests/train"&gt;&lt;code&gt;tests/train&lt;/code&gt;&lt;/a&gt;. We are also working hard to release more examples in &lt;a href="https://github.com/hankcs/HanLP/tree/master/tests/demo"&gt;&lt;code&gt;tests/demo&lt;/code&gt;&lt;/a&gt;. Serving, documentations and more pretrained models are on the way too.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h2&gt;
&lt;p&gt;If you use HanLP in your research, please cite this repository.&lt;/p&gt;
&lt;div class="highlight highlight-text-tex-latex"&gt;&lt;pre&gt;@software{hanlp2,
  author = {Han He},
  title = {{HanLP: Han Language Processing}},
  year = {2020},
  url = {https://github.com/hankcs/HanLP},
}&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;HanLP is licensed under &lt;strong&gt;Apache License 2.0&lt;/strong&gt;. You can use HanLP in your commercial products for free. We would appreciate it if you add a link to HanLP on your website.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;p&gt;[^fasttext]:	A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, “Bag of Tricks for Efficient Text Classification,” vol. cs.CL. 07-Jul-2016.&lt;/p&gt;
&lt;p&gt;[^bert]: J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” arXiv.org, vol. cs.CL. 10-Oct-2018.bert &lt;/p&gt;
&lt;p&gt;[^biaffine]: T. Dozat and C. D. Manning, “Deep Biaffine Attention for Neural Dependency Parsing.,” ICLR, 2017.&lt;/p&gt;
&lt;p&gt;[^conllx]: Buchholz, S., &amp;amp; Marsi, E. (2006, June). CoNLL-X shared task on multilingual dependency parsing. In &lt;em&gt;Proceedings of the tenth conference on computational natural language learning&lt;/em&gt; (pp. 149-164). Association for Computational Linguistics.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hankcs</author><guid isPermaLink="false">https://github.com/hankcs/HanLP</guid><pubDate>Sat, 04 Jan 2020 00:07:00 GMT</pubDate></item><item><title>socialpoint-labs/sheetfu #8 in Python, Today</title><link>https://github.com/socialpoint-labs/sheetfu</link><description>&lt;p&gt;&lt;i&gt;Python library to interact with Google Sheets V4 API&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-sheetfu" class="anchor" aria-hidden="true" href="#sheetfu"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sheetfu&lt;/h1&gt;
&lt;a href="https://travis-ci.org/socialpoint-labs/sheetfu" rel="nofollow"&gt;&lt;img alt="https://travis-ci.org/socialpoint-labs/sheetfu.svg?branch=master" src="https://camo.githubusercontent.com/80e8bdf26a5b92d31029fff785c188d562c7e85c/68747470733a2f2f7472617669732d63692e6f72672f736f6369616c706f696e742d6c6162732f736865657466752e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.org/socialpoint-labs/sheetfu.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;p&gt;Sheetfu was built to interacts with Google Sheets with a simple, intuitive, and fast API.
The primary goal of this library is to adapt the Google App Script API for spreadsheets,
to Python. With Sheetfu, you can easily get or set cell values, background colors, font
colors or any other cell attributes.&lt;/p&gt;
&lt;a name="user-content-installing"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-installing" class="anchor" aria-hidden="true" href="#installing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing&lt;/h2&gt;
&lt;p&gt;Install and update using &lt;a href="https://pip.pypa.io/en/stable/quickstart/" rel="nofollow"&gt;pip&lt;/a&gt;:&lt;/p&gt;
&lt;pre lang="text"&gt;pip install -U Sheetfu
&lt;/pre&gt;
&lt;a name="user-content-a-simple-example"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-a-simple-example" class="anchor" aria-hidden="true" href="#a-simple-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A Simple Example&lt;/h2&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; sheetfu &lt;span class="pl-k"&gt;import&lt;/span&gt; SpreadsheetApp

spreadsheet &lt;span class="pl-k"&gt;=&lt;/span&gt; SpreadsheetApp(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path/to/secret.json&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;).open_by_id(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;insert spreadsheet id here&amp;gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; if ENV vars are defined, you can do initialize it with:&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; spreadsheet = SpreadsheetApp(from_env=True).open_by_id('&amp;lt;insert spreadsheet id here&amp;gt;')&lt;/span&gt;

sheet &lt;span class="pl-k"&gt;=&lt;/span&gt; spreadsheet.get_sheet_by_name(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Sheet1&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
data_range &lt;span class="pl-k"&gt;=&lt;/span&gt; sheet.get_data_range()           &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; returns the sheet range that contains data values.&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; this is how you get things&lt;/span&gt;
values &lt;span class="pl-k"&gt;=&lt;/span&gt; data_range.get_values()              &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; returns a 2D matrix of values.&lt;/span&gt;
backgrounds &lt;span class="pl-k"&gt;=&lt;/span&gt; data_range.get_backgrounds()    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; returns a 2D matrix of background colors in hex format.&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; this is how you set things&lt;/span&gt;
data_range.set_background(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;#000000&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)          &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; set every cell backgrounds to black&lt;/span&gt;
data_range.set_font_color(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;#ffffff&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)          &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; set every cell font colors to white&lt;/span&gt;

data_range.commit()                           &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; to commit your changes otherwise nothing gets changed.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To obtain your secret json file and know more about how to initialize your ENV vars, you can refer to &lt;a href="https://github.com/socialpoint-labs/sheetfu/blob/master/documentation/authentication.rst"&gt;the authentication tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can refer to the &lt;a href="https://github.com/socialpoint-labs/sheetfu/blob/master/documentation/usage.rst"&gt;sheetfu API documentation&lt;/a&gt; for a more detailed description.&lt;/p&gt;
&lt;a name="user-content-the-table-module"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-the-table-module" class="anchor" aria-hidden="true" href="#the-table-module"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Table module&lt;/h2&gt;
&lt;p&gt;Sheetfu also contains a table module that abstracts completely the coordinates
system for an ORM-like syntax. The example below is for a sheet with the 3
columns 'name', 'surname' and 'age'.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; sheetfu &lt;span class="pl-k"&gt;import&lt;/span&gt; Table

spreadsheet &lt;span class="pl-k"&gt;=&lt;/span&gt; SpreadsheetApp(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path/to/secret.json&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;).open_by_id(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;insert spreadsheet id here&amp;gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; if ENV vars are defined, you can do initialize it with:&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; spreadsheet = SpreadsheetApp(from_env=True).open_by_id('&amp;lt;insert spreadsheet id here&amp;gt;')&lt;/span&gt;

data_range &lt;span class="pl-k"&gt;=&lt;/span&gt; spreadsheet.get_sheet_by_name(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;people&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;).get_data_range()

table &lt;span class="pl-k"&gt;=&lt;/span&gt; Table(data_range, &lt;span class="pl-v"&gt;backgrounds&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

&lt;span class="pl-k"&gt;for&lt;/span&gt; item &lt;span class="pl-k"&gt;in&lt;/span&gt; table:
    &lt;span class="pl-k"&gt;if&lt;/span&gt; item.get_field_value(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;name&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;foo&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:
        item.set_field_value(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;surname&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bar&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)              &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; this set the surname field value&lt;/span&gt;
    age &lt;span class="pl-k"&gt;=&lt;/span&gt; item.get_field_value(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;age&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    item.set_field_value(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;age&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, age &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;)
    item.set_field_background(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;age&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;#ff0000&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)             &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; this set the field 'age' to red color&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Every set functions are batched for speed performance.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To send the batch update of every set requests you made,&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; you need to commit the table object as follow.&lt;/span&gt;
table.commit()&lt;/pre&gt;&lt;/div&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;For guidance on how to make a contribution to Sheetfu, see the &lt;a href="https://github.com/socialpoint-labs/sheetfu/blob/master/CONTRIBUTING.rst"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-links"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;License: &lt;a href="https://github.com/socialpoint-labs/sheetfu/blob/master/LICENSE"&gt;MIT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Releases: &lt;a href="https://pypi.org/project/sheetfu/" rel="nofollow"&gt;https://pypi.org/project/sheetfu/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code: &lt;a href="https://github.com/socialpoint-labs/sheetfu"&gt;https://github.com/socialpoint-labs/sheetfu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Issue tracker: &lt;a href="https://github.com/socialpoint-labs/sheetfu/issues"&gt;https://github.com/socialpoint-labs/sheetfu/issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are looking for the original sheetfu google apps script library, it has been relocated to &lt;a href="https://github.com/socialpoint-labs/sheetfu-apps-script"&gt;this page&lt;/a&gt;.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>socialpoint-labs</author><guid isPermaLink="false">https://github.com/socialpoint-labs/sheetfu</guid><pubDate>Sat, 04 Jan 2020 00:08:00 GMT</pubDate></item><item><title>apache/airflow #9 in Python, Today</title><link>https://github.com/apache/airflow</link><description>&lt;p&gt;&lt;i&gt;Apache Airflow - A platform to programmatically author, schedule, and monitor workflows&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;
&lt;h1&gt;&lt;a id="user-content-apache-airflow" class="anchor" aria-hidden="true" href="#apache-airflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Apache Airflow&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://badge.fury.io/py/apache-airflow" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0f108ec654c883218585d0045e654bc2d49b2bed/68747470733a2f2f62616467652e667572792e696f2f70792f6170616368652d616972666c6f772e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/apache-airflow.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/apache/airflow" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f22f5d352f57d42fa86e959568d637b135d3a675/68747470733a2f2f7472617669732d63692e6f72672f6170616368652f616972666c6f772e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/apache/airflow.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://codecov.io/github/apache/airflow?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8c5b43138631718758fa0e91b9db270daf51b147/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f6170616368652f616972666c6f772f6d61737465722e737667" alt="Coverage Status" data-canonical-src="https://img.shields.io/codecov/c/github/apache/airflow/master.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://airflow.readthedocs.io/en/latest/?badge=latest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/003c5318f36a13626071e12fc699dabf1fd8f11d/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f616972666c6f772f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/airflow/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="http://www.apache.org/licenses/LICENSE-2.0.txt" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a0f8662456135c05bf3117cb70a1aaa55c0d3604/687474703a2f2f696d672e736869656c64732e696f2f3a6c6963656e73652d417061636865253230322d626c75652e737667" alt="License" data-canonical-src="http://img.shields.io/:license-Apache%202-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/apache-airflow/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/80cdb6efdf45ca775e10a1e923dacf3b72b67ee5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6170616368652d616972666c6f772e737667" alt="PyPI - Python Version" data-canonical-src="https://img.shields.io/pypi/pyversions/apache-airflow.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/ApacheAirflow" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a6c2ad238bbc98ffa12bdd2b6aef6ab265125827/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f417061636865416972666c6f772e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/ApacheAirflow.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://apache-airflow-slack.herokuapp.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/260d99509084c1d8d8ddacdd14978003a0847a26/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d6a6f696e5f636861742d77686974652e7376673f6c6f676f3d736c61636b267374796c653d736f6369616c" alt="Slack Status" data-canonical-src="https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&amp;amp;style=social" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.&lt;/p&gt;
&lt;p&gt;When workflows are defined as code, they become more maintainable,
versionable, testable, and collaborative.&lt;/p&gt;
&lt;p&gt;Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#getting-started"&gt;Getting started&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#beyond-the-horizon"&gt;Beyond the Horizon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#principles"&gt;Principles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#user-interface"&gt;User Interface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#who-uses-apache-airflow"&gt;Who uses Apache Airflow?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#who-maintains-apache-airflow"&gt;Who Maintains Apache Airflow?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#can-i-use-the-apache-airflow-logo-in-my-presentation"&gt;Can I use the Apache Airflow logo in my presentation?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#links"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting started&lt;/h2&gt;
&lt;p&gt;Please visit the Airflow Platform documentation (latest &lt;strong&gt;stable&lt;/strong&gt; release) for help with &lt;a href="https://airflow.apache.org/installation.html" rel="nofollow"&gt;installing Airflow&lt;/a&gt;, getting a &lt;a href="https://airflow.apache.org/start.html" rel="nofollow"&gt;quick start&lt;/a&gt;, or a more complete &lt;a href="https://airflow.apache.org/tutorial.html" rel="nofollow"&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Documentation of GitHub master (latest development branch): &lt;a href="https://airflow.readthedocs.io/en/latest/" rel="nofollow"&gt;ReadTheDocs Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For further information, please visit the &lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Home" rel="nofollow"&gt;Airflow Wiki&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-beyond-the-horizon" class="anchor" aria-hidden="true" href="#beyond-the-horizon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Beyond the Horizon&lt;/h2&gt;
&lt;p&gt;Airflow &lt;strong&gt;is not&lt;/strong&gt; a data streaming solution. Tasks do not move data from
one to the other (though tasks can exchange metadata!). Airflow is not
in the &lt;a href="http://spark.apache.org/streaming/" rel="nofollow"&gt;Spark Streaming&lt;/a&gt;
or &lt;a href="https://storm.apache.org/" rel="nofollow"&gt;Storm&lt;/a&gt; space, it is more comparable to
&lt;a href="http://oozie.apache.org/" rel="nofollow"&gt;Oozie&lt;/a&gt; or
&lt;a href="https://azkaban.github.io/" rel="nofollow"&gt;Azkaban&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Workflows are expected to be mostly static or slowly changing. You can think
of the structure of the tasks in your workflow as slightly more dynamic
than a database structure would be. Airflow workflows are expected to look
similar from a run to the next, this allows for clarity around
unit of work and continuity.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-principles" class="anchor" aria-hidden="true" href="#principles"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Principles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;:  Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt;:  Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Elegant&lt;/strong&gt;:  Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful &lt;strong&gt;Jinja&lt;/strong&gt; templating engine.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt;:  Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-user-interface" class="anchor" aria-hidden="true" href="#user-interface"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;User Interface&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DAGs&lt;/strong&gt;: Overview of all DAGs in your environment.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/docs/img/dags.png"&gt;&lt;img src="/docs/img/dags.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tree View&lt;/strong&gt;: Tree representation of a DAG that spans across time.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/docs/img/tree.png"&gt;&lt;img src="/docs/img/tree.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph View&lt;/strong&gt;: Visualization of a DAG's dependencies and their current status for a specific run.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/docs/img/graph.png"&gt;&lt;img src="/docs/img/graph.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Task Duration&lt;/strong&gt;: Total time spent on different tasks over time.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/docs/img/duration.png"&gt;&lt;img src="/docs/img/duration.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gantt View&lt;/strong&gt;: Duration and overlap of a DAG.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/docs/img/gantt.png"&gt;&lt;img src="/docs/img/gantt.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Code View&lt;/strong&gt;:  Quick way to view source code of a DAG.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/docs/img/code.png"&gt;&lt;img src="/docs/img/code.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;Want to help build Apache Airflow? Check out our &lt;a href="https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst"&gt;contributing documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-who-uses-apache-airflow" class="anchor" aria-hidden="true" href="#who-uses-apache-airflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Who uses Apache Airflow?&lt;/h2&gt;
&lt;p&gt;As the Apache Airflow community grows, we'd like to keep track of who is using
the platform. Please send a PR with your company name and @githubhandle
if you may.&lt;/p&gt;
&lt;p&gt;Currently &lt;strong&gt;officially&lt;/strong&gt; using Airflow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.4g-capital.com/" rel="nofollow"&gt;4G Capital&lt;/a&gt; [&lt;a href="https://github.com/posei"&gt;@posei&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.6play.fr" rel="nofollow"&gt;6play&lt;/a&gt; [&lt;a href="https://github.com/lemoura"&gt;@lemourA&lt;/a&gt;, &lt;a href="https://github.com/achaussende"&gt;@achaussende&lt;/a&gt;, &lt;a href="https://github.com/d-nguyen"&gt;@d-nguyen&lt;/a&gt;, &lt;a href="https://github.com/julien-gm"&gt;@julien-gm&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://8fit.com/" rel="nofollow"&gt;8fit&lt;/a&gt; [&lt;a href="https://github.com/nicor88"&gt;@nicor88&lt;/a&gt;, &lt;a href="https://github.com/frnzska"&gt;@frnzska&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://90seconds.tv/" rel="nofollow"&gt;90 Seconds&lt;/a&gt; [&lt;a href="https://github.com/aaronmak"&gt;@aaronmak&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://99taxis.com" rel="nofollow"&gt;99&lt;/a&gt; [&lt;a href="https://github.com/fbenevides"&gt;@fbenevides&lt;/a&gt;, &lt;a href="https://github.com/gustavoamigo"&gt;@gustavoamigo&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/mmmaia"&gt;@mmmaia&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.adboost.sk" rel="nofollow"&gt;AdBOOST&lt;/a&gt; [&lt;a href="https://github.com/AdBOOST"&gt;AdBOOST&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.adobe.com/" rel="nofollow"&gt;Adobe&lt;/a&gt; [&lt;a href="https://github.com/mishikaSingh"&gt;@mishikaSingh&lt;/a&gt;, &lt;a href="https://github.com/ramandumcs"&gt;@ramandumcs&lt;/a&gt;, &lt;a href="https://github.com/vardancse"&gt;@vardancse&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/agaridata"&gt;Agari&lt;/a&gt; [&lt;a href="https://github.com/r39132"&gt;@r39132&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://agoda.com" rel="nofollow"&gt;Agoda&lt;/a&gt; [&lt;a href="https://github.com/akki"&gt;@akki&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://airbnb.io/" rel="nofollow"&gt;Airbnb&lt;/a&gt; [&lt;a href="https://github.com/mistercrunch"&gt;@mistercrunch&lt;/a&gt;, &lt;a href="https://github.com/artwr"&gt;@artwr&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.airdna.co" rel="nofollow"&gt;AirDNA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.airfinity.com" rel="nofollow"&gt;Airfinity&lt;/a&gt; [&lt;a href="https://github.com/sibowyer"&gt;@sibowyer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.airtel.in/" rel="nofollow"&gt;Airtel&lt;/a&gt; [&lt;a href="https://github.com/harishbisht"&gt;@harishbisht&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alan.eu" rel="nofollow"&gt;Alan&lt;/a&gt; [&lt;a href="https://github.com/charles-go"&gt;@charles-go&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://allegro.tech/" rel="nofollow"&gt;allegro.pl&lt;/a&gt; [&lt;a href="https://github.com/kretes"&gt;@kretes&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alopeyk.com" rel="nofollow"&gt;AloPeyk&lt;/a&gt; [&lt;a href="https://github.com/blcksrx"&gt;@blcksrx&lt;/a&gt;, &lt;a href="https://github.com/AloPeyk"&gt;@AloPeyk&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.getaltx.com/about" rel="nofollow"&gt;AltX&lt;/a&gt; [&lt;a href="https://github.com/pedromduarte"&gt;@pedromduarte&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ampathkenya.org/" rel="nofollow"&gt;AMPATH&lt;/a&gt;[&lt;a href="https://github.com/AMPATH"&gt;@AMPATH&lt;/a&gt;, &lt;a href="https://github.com/fatmali"&gt;@fatmali&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://apigee.com" rel="nofollow"&gt;Apigee&lt;/a&gt; [&lt;a href="https://github.com/btallman"&gt;@btallman&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.argolabs.org" rel="nofollow"&gt;ARGO Labs&lt;/a&gt; [&lt;a href="https://github.com/California-Data-Collaborative"&gt;@California Data Collaborative&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.armedangels.de" rel="nofollow"&gt;ARMEDANGELS&lt;/a&gt; [&lt;a href="https://github.com/swiffer"&gt;@swiffer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.arquivei.com.br/" rel="nofollow"&gt;Arquivei&lt;/a&gt; [&lt;a href="https://github.com/arquivei"&gt;@arquivei&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.arrive.com/" rel="nofollow"&gt;Arrive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://asana.com/" rel="nofollow"&gt;Asana&lt;/a&gt; [&lt;a href="https://github.com/chang"&gt;@chang&lt;/a&gt;, &lt;a href="https://github.com/dima-asana"&gt;@dima-asana&lt;/a&gt;, &lt;a href="https://github.com/jdavidheiser"&gt;@jdavidheiser&lt;/a&gt;, &lt;a href="https://github.com/ricardoandresrojas"&gt;@ricardoandresrojas&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.astronomer.io" rel="nofollow"&gt;Astronomer&lt;/a&gt; [&lt;a href="https://github.com/schnie"&gt;@schnie&lt;/a&gt;, &lt;a href="https://github.com/ashb"&gt;@ashb&lt;/a&gt;, &lt;a href="https://github.com/kaxil"&gt;@kaxil&lt;/a&gt;, &lt;a href="https://github.com/dimberman"&gt;@dimberman&lt;/a&gt;, &lt;a href="https://github.com/andriisoldatenko"&gt;@andriisoldatenko&lt;/a&gt;, &lt;a href="https://github.com/ryw"&gt;@ryw&lt;/a&gt;, &lt;a href="https://github.com/andrewhharmon"&gt;@andrewhharmon&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://auth0.com" rel="nofollow"&gt;Auth0&lt;/a&gt; [&lt;a href="https://github.com/sicarul"&gt;@sicarul&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://automattic.com/" rel="nofollow"&gt;Automattic&lt;/a&gt; [&lt;a href="https://github.com/anandnalya"&gt;@anandnalya&lt;/a&gt;, &lt;a href="https://github.com/bperson"&gt;@bperson&lt;/a&gt;, &lt;a href="https://github.com/Khrol"&gt;@khrol&lt;/a&gt;, &lt;a href="https://github.com/xyu"&gt;@xyu&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://awaytravel.com" rel="nofollow"&gt;Away&lt;/a&gt; [&lt;a href="https://github.com/trunsky"&gt;@trunsky&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.azrisolutions.com/" rel="nofollow"&gt;Azri Solutions&lt;/a&gt; [&lt;a href="https://github.com/userimack"&gt;@userimack&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://site.bagelcode.com/" rel="nofollow"&gt;Bagelcode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://truebalance.io/" rel="nofollow"&gt;BalanceHero&lt;/a&gt; [&lt;a href="https://github.com/swalloow"&gt;@swalloow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.bancodeformaturas.com.br" rel="nofollow"&gt;Banco de Formaturas&lt;/a&gt; [&lt;a href="https://github.com/guiligan"&gt;@guiligan&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.bandwidthx.com" rel="nofollow"&gt;BandwidthX&lt;/a&gt; [&lt;a href="https://github.com/dineshdsharma"&gt;@dineshdsharma&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.basetis.com" rel="nofollow"&gt;Basetis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.bbm.com/" rel="nofollow"&gt;BBM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.beamly.com/" rel="nofollow"&gt;Beamly&lt;/a&gt; [&lt;a href="https://github.com/christopheralcock"&gt;@christopheralcock&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://beeswax.com/" rel="nofollow"&gt;Beeswax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bellhops"&gt;Bellhops&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://belugadb.com" rel="nofollow"&gt;BelugaDB&lt;/a&gt; [&lt;a href="https://github.com/fabio-nukui"&gt;@fabio-nukui&lt;/a&gt; &amp;amp; &lt;a href="http://github.com/joao-sallaberry"&gt;@joao-sallaberry&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/lucianoviola"&gt;@lucianoviola&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/tmatuki"&gt;@tmatuki&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.betterment.com/" rel="nofollow"&gt;Betterment&lt;/a&gt; [&lt;a href="https://github.com/Betterment"&gt;@betterment&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.bexs.com.br/en" rel="nofollow"&gt;Bexs Bank&lt;/a&gt; [&lt;a href="https://github.com/felipefb"&gt;@felipefb&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/ishvann"&gt;@ilarsen&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bigquant.com/" rel="nofollow"&gt;BigQuant&lt;/a&gt; [&lt;a href="https://github.com/bigquant"&gt;@bigquant&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.birdz.com/en/" rel="nofollow"&gt;Birdz by Veolia&lt;/a&gt; [&lt;a href="https://github.com/benjamingrenier"&gt;@benjamingrenier&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.blablacar.com" rel="nofollow"&gt;BlaBlaCar&lt;/a&gt; [&lt;a href="https://github.com/puckel"&gt;@puckel&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/wmorin"&gt;@wmorin&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.blacklane.com" rel="nofollow"&gt;Blacklane&lt;/a&gt; [&lt;a href="https://github.com/serkef"&gt;@serkef&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.bloc.io" rel="nofollow"&gt;Bloc&lt;/a&gt; [&lt;a href="https://github.com/dpaola2"&gt;@dpaola2&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.techatbloomberg.com" rel="nofollow"&gt;Bloomberg&lt;/a&gt; [&lt;a href="https://github.com/dimberman"&gt;@dimberman&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.blue-yonder.com" rel="nofollow"&gt;Blue Yonder&lt;/a&gt; [&lt;a href="https://github.com/blue-yonder"&gt;@blue-yonder&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.blueapron.com" rel="nofollow"&gt;BlueApron&lt;/a&gt; [&lt;a href="https://github.com/jasonjho"&gt;@jasonjho&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/matthewdavidhauser"&gt;@matthewdavidhauser&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.bluecore.com" rel="nofollow"&gt;Bluecore&lt;/a&gt; [&lt;a href="https://github.com/JLDLaughlin"&gt;@JLDLaughlin&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bluekiri.com" rel="nofollow"&gt;Bluekiri&lt;/a&gt; [&lt;a href="https://github.com/bluekiri"&gt;@Bluekiri&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bodastage/bts-ce"&gt;Boda Telecom Suite - CE&lt;/a&gt; [&lt;a href="https://github.com/erssebaggala"&gt;@erssebaggala&lt;/a&gt;, &lt;a href="https://github.com/bodastage"&gt;@bodastage&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://bodastage.com" rel="nofollow"&gt;Bodastage Solutions&lt;/a&gt; [&lt;a href="https://github.com/erssebaggala"&gt;@erssebaggala&lt;/a&gt;, &lt;a href="https://github.com/bodastage"&gt;@bodastage&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bombora.com/" rel="nofollow"&gt;Bombora Inc&lt;/a&gt; [&lt;a href="https://github.com/jeffkpayne"&gt;@jeffkpayne&lt;/a&gt;, &lt;a href="https://github.com/pakelley"&gt;@pakelley&lt;/a&gt;, &lt;a href="https://github.com/dNavalta"&gt;@dNavalta&lt;/a&gt;, &lt;a href="https://github.com/austynh"&gt;@austynh&lt;/a&gt;, &lt;a href="https://github.com/TheOriginalAlex"&gt;@TheOriginalAlex&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.bonial.com/" rel="nofollow"&gt;Bonial International GmbH&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.bonnierbroadcasting.com" rel="nofollow"&gt;Bonnier Broadcasting&lt;/a&gt; [&lt;a href="https://github.com/wileeam"&gt;@wileeam&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.bouncex.com" rel="nofollow"&gt;BounceX&lt;/a&gt; [&lt;a href="https://github.com/JoshFerge"&gt;@JoshFerge&lt;/a&gt;, &lt;a href="https://github.com/hudsonrio"&gt;@hudsonrio&lt;/a&gt;, &lt;a href="https://github.com/ronniekritou"&gt;@ronniekritou&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.braintreepayments.com" rel="nofollow"&gt;Braintree&lt;/a&gt; [&lt;a href="https://github.com/coopergillan"&gt;@coopergillan&lt;/a&gt;, &lt;a href="https://github.com/curiousjazz77"&gt;@curiousjazz77&lt;/a&gt;, &lt;a href="https://github.com/raymondberg"&gt;@raymondberg&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://branch.io" rel="nofollow"&gt;Branch&lt;/a&gt; [&lt;a href="https://github.com/sdebarshi"&gt;@sdebarshi&lt;/a&gt;, &lt;a href="https://github.com/dmitrig01"&gt;@dmitrig01&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.caesars.com" rel="nofollow"&gt;Caesars Entertainment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/California-Data-Collaborative"&gt;California Data Collaborative&lt;/a&gt; powered by &lt;a href="http://www.argolabs.org" rel="nofollow"&gt;ARGO Labs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.capitalone.com" rel="nofollow"&gt;Capital One&lt;/a&gt; [&lt;a href="https://github.com/anoopengineer"&gt;@anoopengineer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.carbonite.com" rel="nofollow"&gt;Carbonite&lt;/a&gt; [&lt;a href="https://github.com/ajbosco"&gt;@ajbosco&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.carlabs.ai/" rel="nofollow"&gt;CarLabs&lt;/a&gt; [&lt;a href="https://github.com/sganz"&gt;@sganz&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/odannyc"&gt;@odannyc&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cava.com" rel="nofollow"&gt;CAVA&lt;/a&gt; [&lt;a href="http://github.com/minh5"&gt;@minh5&lt;/a&gt; &amp;amp; &lt;a href="http://github.com/patchus"&gt;@patchus&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.celect.com" rel="nofollow"&gt;Celect&lt;/a&gt; [&lt;a href="https://github.com/superdosh"&gt;@superdosh&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/chadcelect"&gt;@chadcelect&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://censys.io" rel="nofollow"&gt;Censys&lt;/a&gt; [&lt;a href="https://github.com/zakird"&gt;@zakird&lt;/a&gt;, &lt;a href="https://github.com/dadrian"&gt;@dadrian&lt;/a&gt;, &amp;amp; &lt;a href="https://github.com/andrewsardone"&gt;@andrewsardone&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.change.org" rel="nofollow"&gt;Change.org&lt;/a&gt; [&lt;a href="https://github.com/change"&gt;@change&lt;/a&gt;, &lt;a href="https://github.com/vijaykramesh"&gt;@vijaykramesh&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.chartboost.com" rel="nofollow"&gt;Chartboost&lt;/a&gt; [&lt;a href="https://github.com/cgelman"&gt;@cgelman&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/dclubb"&gt;@dclubb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://checkr.com" rel="nofollow"&gt;Checkr&lt;/a&gt; [&lt;a href="https://github.com/tongboh"&gt;@tongboh&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.chop.edu/centers-programs/division-genomic-diagnostics" rel="nofollow"&gt;Children's Hospital of Philadelphia Division of Genomic Diagnostics&lt;/a&gt; [&lt;a href="https://github.com/genomics-geek/"&gt;@genomics-geek&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cinimex.ru" rel="nofollow"&gt;Cinimex DataLab&lt;/a&gt; [&lt;a href="https://github.com/kdubovikov"&gt;@kdubovikov&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sandiego.gov" rel="nofollow"&gt;City of San Diego&lt;/a&gt; [&lt;a href="https://github.com/mrmaksimize"&gt;@MrMaksimize&lt;/a&gt;, &lt;a href="https://github.com/andrell81"&gt;@andrell81&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/arnaudvedy"&gt;@arnaudvedy&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.toronto.ca/" rel="nofollow"&gt;City of Toronto&lt;/a&gt; [&lt;a href="https://github.com/CityofToronto"&gt;@CityofToronto&lt;/a&gt;, &lt;a href="https://github.com/radumas"&gt;@radumas&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://civalue.com/" rel="nofollow"&gt;ciValue&lt;/a&gt; [&lt;a href="https://github.com/chencivalue"&gt;@chencivalue&lt;/a&gt;, &lt;a href="https://github.com/YoavGaudin"&gt;@YoavGaudin&lt;/a&gt;, &lt;a href="https://github.com/saleem-boshnak"&gt;@saleem-boshnak&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://civey.com/" rel="nofollow"&gt;Civey&lt;/a&gt; [&lt;a href="https://github.com/WesleyBatista"&gt;@WesleyBatista&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://clairvoyantsoft.com" rel="nofollow"&gt;Clairvoyant&lt;/a&gt; [&lt;a href="https://github.com/shekharv"&gt;@shekharv&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://classmethod.jp/" rel="nofollow"&gt;Classmethod, Inc.&lt;/a&gt; [&lt;a href="https://github.com/shoito"&gt;@shoito&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cleartax.in/" rel="nofollow"&gt;Cleartax&lt;/a&gt; [&lt;a href="https://github.com/anks"&gt;@anks&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/codebuff"&gt;@codebuff&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cloverhealth.com" rel="nofollow"&gt;Clover Health&lt;/a&gt; [&lt;a href="https://github.com/gwax"&gt;@gwax&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/vansivallab"&gt;@vansivallab&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.collectivehealth.com" rel="nofollow"&gt;Collectivehealth Inc.&lt;/a&gt; [&lt;a href="https://github.com/retornam"&gt;@retornam&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.compass.com" rel="nofollow"&gt;Compass&lt;/a&gt; [&lt;a href="https://github.com/wdhorton"&gt;@wdhorton&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.connectwise.com/" rel="nofollow"&gt;ConnectWise&lt;/a&gt; [&lt;a href="https://github.com/jacobeturpin"&gt;@jacobeturpin&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.contaazul.com" rel="nofollow"&gt;ContaAzul&lt;/a&gt; [&lt;a href="https://github.com/bern4rdelli"&gt;@bern4rdelli&lt;/a&gt;, &lt;a href="https://github.com/renanleme"&gt;@renanleme&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/sabino"&gt;@sabino&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cotap/"&gt;Cotap&lt;/a&gt; [&lt;a href="https://github.com/maraca"&gt;@maraca&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/richardchew"&gt;@richardchew&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.craigatwork.com" rel="nofollow"&gt;Craig@Work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://crealytics.com" rel="nofollow"&gt;Crealytics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.creditkarma.com/" rel="nofollow"&gt;Credit Karma&lt;/a&gt; [&lt;a href="https://github.com/preete-dixit-ck"&gt;@preete-dixit-ck&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/harish-gaggar-ck"&gt;@harish-gaggar-ck&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/greg-finley-ck"&gt;@greg-finley-ck&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.creditas.com.br" rel="nofollow"&gt;Creditas&lt;/a&gt; [&lt;a href="https://github.com/dcassiano"&gt;@dcassiano&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.creditcards.com/" rel="nofollow"&gt;CreditCards.com&lt;/a&gt;[&lt;a href="https://github.com/vmAggies"&gt;@vmAggies&lt;/a&gt; &amp;amp;  &lt;a href="https://github.com/jay-wallaby"&gt;@jay-wallaby&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cryptalizer.com/" rel="nofollow"&gt;Cryptalizer.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.customink.com/" rel="nofollow"&gt;Custom Ink&lt;/a&gt; [&lt;a href="https://github.com/david-dalisay"&gt;@david-dalisay&lt;/a&gt;, &lt;a href="https://github.com/dmartin11"&gt;@dmartin11&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/mpeteuil"&gt;@mpeteuil&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cyscale.com" rel="nofollow"&gt;Cyscale&lt;/a&gt; [&lt;a href="https://github.com/ocical"&gt;@ocical&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.dailymotion.com/fr" rel="nofollow"&gt;Dailymotion&lt;/a&gt; [&lt;a href="https://github.com/germaintanguy"&gt;@germaintanguy&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/hc"&gt;@hc&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.danamica.dk" rel="nofollow"&gt;Danamica&lt;/a&gt; [&lt;a href="https://github.com/testvinder"&gt;@testvinder&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.datareply.co.uk/" rel="nofollow"&gt;Data Reply&lt;/a&gt; [&lt;a href="https://github.com/kaxil"&gt;@kaxil&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://datacamp.com/" rel="nofollow"&gt;DataCamp&lt;/a&gt; [&lt;a href="https://github.com/dgrtwo"&gt;@dgrtwo&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.datafox.com/" rel="nofollow"&gt;DataFox&lt;/a&gt; [&lt;a href="https://github.com/sudowork"&gt;@sudowork&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.dentsu.com/" rel="nofollow"&gt;Dentsu Inc.&lt;/a&gt; [&lt;a href="https://github.com/bryan831"&gt;@bryan831&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/loozhengyuan"&gt;@loozhengyuan&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.digitalfirstmedia.com/" rel="nofollow"&gt;Digital First Media&lt;/a&gt; [&lt;a href="https://github.com/duffn"&gt;@duffn&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/mschmo"&gt;@mschmo&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/seanmuth"&gt;@seanmuth&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://digitalocean.com/" rel="nofollow"&gt;DigitalOcean&lt;/a&gt; [&lt;a href="https://github.com/ajbosco"&gt;@ajbosco&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.doordash.com/" rel="nofollow"&gt;DoorDash&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dotmodus.com" rel="nofollow"&gt;Dotmodus&lt;/a&gt; [&lt;a href="https://github.com/dannylee12"&gt;@dannylee12&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.drivy.com" rel="nofollow"&gt;Drivy&lt;/a&gt; [&lt;a href="https://github.com/AntoineAugusti"&gt;@AntoineAugusti&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.easytaxi.com/" rel="nofollow"&gt;Easy Taxi&lt;/a&gt; [&lt;a href="https://github.com/caique-lima"&gt;@caique-lima&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/diraol"&gt;@diraol&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ellisdon.com/" rel="nofollow"&gt;EllisDon&lt;/a&gt; [&lt;a href="https://github.com/d2kalra"&gt;@d2kalra&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/zbasama"&gt;@zbasama&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.endesa.com" rel="nofollow"&gt;Endesa&lt;/a&gt; [&lt;a href="https://github.com/drexpp"&gt;@drexpp&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.enigma.com" rel="nofollow"&gt;Enigma&lt;/a&gt; [&lt;a href="https://github.com/hydrosquall"&gt;@hydrosquall&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.datamaran.com" rel="nofollow"&gt;Datamaran&lt;/a&gt; [&lt;a href="https://github.com/valexharo"&gt;@valexharo&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.etsy.com" rel="nofollow"&gt;Etsy&lt;/a&gt; [&lt;a href="https://github.com/mchalek"&gt;@mchalek&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://evo.company/" rel="nofollow"&gt;evo.company&lt;/a&gt; [&lt;a href="https://github.com/orhideous"&gt;@orhideous&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.experityhealth.com/" rel="nofollow"&gt;Experity (formerly DocuTAP)&lt;/a&gt; [&lt;a href="https://github.com/cloneluke"&gt;@cloneluke&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/tobyjoliver"&gt;@tobyjoliver&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.fathomhealth.co/" rel="nofollow"&gt;Fathom Health&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.hsmap.com/" rel="nofollow"&gt;Firestone Inventing&lt;/a&gt; [&lt;a href="https://github.com/zihengCat"&gt;@zihengCat&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.flipp.com" rel="nofollow"&gt;Flipp&lt;/a&gt; [&lt;a href="https://github.com/sethwilsonwishabi"&gt;@sethwilsonwishabi&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.format.com" rel="nofollow"&gt;Format&lt;/a&gt; [&lt;a href="https://github.com/4ormat"&gt;@format&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/jasonicarter"&gt;@jasonicarter&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/freshbooks"&gt;FreshBooks&lt;/a&gt; [&lt;a href="https://github.com/DinoCow"&gt;@DinoCow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.freshworks.com/" rel="nofollow"&gt;Freshworks&lt;/a&gt; [&lt;a href="https://github.com/shaikshakeel"&gt;@shaikshakeel&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fullcontact"&gt;FullContact&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.fuller-inc.com/" rel="nofollow"&gt;Fuller, Inc.&lt;/a&gt; [&lt;a href="https://github.com/wutali"&gt;@wutali&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/sh-tech"&gt;@sh-tech&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fundera.com" rel="nofollow"&gt;Fundera&lt;/a&gt; [&lt;a href="https://github.com/andyxhadji"&gt;@andyxhadji&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gadventures.com" rel="nofollow"&gt;G Adventures&lt;/a&gt; [&lt;a href="https://github.com/chchtv11"&gt;@chchtv11&lt;/a&gt;, &lt;a href="https://github.com/tgumbley"&gt;@tgumbley&lt;/a&gt;, &lt;a href="https://github.com/tomwross"&gt;@tomwross&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gamewisp.com" rel="nofollow"&gt;GameWisp&lt;/a&gt; [&lt;a href="https://github.com/TJBIII"&gt;@tjbiii&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/theryanwalls"&gt;@theryanwalls&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.genecards.org" rel="nofollow"&gt;GeneCards&lt;/a&gt; [&lt;a href="https://github.com/oferze"&gt;@oferze&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/gentnerlab"&gt;Gentner Lab&lt;/a&gt; [&lt;a href="https://github.com/neuromusic"&gt;@neuromusic&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://getsimpl.com/" rel="nofollow"&gt;Get Simpl&lt;/a&gt; [&lt;a href="https://github.com/rootcss"&gt;@rootcss&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://about.gitlab.com/" rel="nofollow"&gt;GitLab&lt;/a&gt; [&lt;a href="https://gitlab.com/tlapiana" rel="nofollow"&gt;@tlapiana&lt;/a&gt; &amp;amp; &lt;a href="https://gitlab.com/tayloramurphy" rel="nofollow"&gt;@tayloramurphy&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Glassdoor"&gt;Glassdoor&lt;/a&gt; [&lt;a href="https://github.com/syvineckruyk"&gt;@syvineckruyk&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/sid88in"&gt;@sid88in&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://global-fashion-group.com" rel="nofollow"&gt;Global Fashion Group&lt;/a&gt; [&lt;a href="https://github.com/GFG"&gt;@GFG&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://godatadriven.com/" rel="nofollow"&gt;GoDataDriven&lt;/a&gt; [&lt;a href="https://github.com/basph"&gt;@BasPH&lt;/a&gt;, &lt;a href="https://github.com/danielvdende"&gt;@danielvdende&lt;/a&gt;, &lt;a href="https://github.com/ffinfo"&gt;@ffinfo&lt;/a&gt;, &lt;a href="https://github.com/Fokko"&gt;@Fokko&lt;/a&gt;, &lt;a href="https://github.com/gglanzani"&gt;@gglanzani&lt;/a&gt;, &lt;a href="https://github.com/hgrif"&gt;@hgrif&lt;/a&gt;, &lt;a href="https://github.com/jrderuiter"&gt;@jrderuiter&lt;/a&gt;, &lt;a href="https://github.com/NielsZeilemaker"&gt;@NielsZeilemaker&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gds-gov.tech" rel="nofollow"&gt;GovTech GDS&lt;/a&gt; [&lt;a href="https://github.com/chrissng"&gt;@chrissng&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/datagovsg"&gt;@datagovsg&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.grab.com/sg/" rel="nofollow"&gt;Grab&lt;/a&gt; [&lt;a href="https://github.com/canhtran"&gt;@calvintran&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gradeup.co" rel="nofollow"&gt;Gradeup&lt;/a&gt; [&lt;a href="https://github.com/gradeup"&gt;@gradeup&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.grandrounds.com/" rel="nofollow"&gt;Grand Rounds&lt;/a&gt; [&lt;a href="https://github.com/richddr"&gt;@richddr&lt;/a&gt;, &lt;a href="https://github.com/timz1290"&gt;@timz1290&lt;/a&gt;, &lt;a href="https://github.com/@wenever"&gt;@wenever&lt;/a&gt;, &amp;amp; &lt;a href="https://github.com/runongirlrunon"&gt;@runongirlrunon&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://es.groupalia.com" rel="nofollow"&gt;Groupalia&lt;/a&gt; [&lt;a href="https://github.com/jesusfcr"&gt;@jesusfcr&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://groupon.com" rel="nofollow"&gt;Groupon&lt;/a&gt; [&lt;a href="https://github.com/stevencasey"&gt;@stevencasey&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.growbots.com/" rel="nofollow"&gt;Growbots&lt;/a&gt;[&lt;a href="https://github.com/exploy"&gt;@exploy&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.gsngames.com" rel="nofollow"&gt;GSN Games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gusto.com" rel="nofollow"&gt;Gusto&lt;/a&gt; [&lt;a href="https://github.com/frankhsu"&gt;@frankhsu&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://joinhandshake.com/" rel="nofollow"&gt;Handshake&lt;/a&gt; [&lt;a href="https://github.com/mhickman"&gt;@mhickman&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.handy.com/careers/73115?gh_jid=73115&amp;amp;gh_src=o5qcxn" rel="nofollow"&gt;Handy&lt;/a&gt; [&lt;a href="https://github.com/marcintustin"&gt;@marcintustin&lt;/a&gt; / &lt;a href="https://github.com/mtustin-handy"&gt;@mtustin-handy&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.happn.com" rel="nofollow"&gt;happn&lt;/a&gt; [&lt;a href="https://github.com/pcorbel"&gt;@pcorbel&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.havan.com.br" rel="nofollow"&gt;HAVAN&lt;/a&gt; [&lt;a href="https://github.com/botbiz"&gt;@botbiz&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tech.hbc.com" rel="nofollow"&gt;HBC Digital&lt;/a&gt; [&lt;a href="https://github.com/tmccartan"&gt;@tmccartan&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/dmateusp"&gt;@dmateusp&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.hbo.com/" rel="nofollow"&gt;HBO&lt;/a&gt;[&lt;a href="https://github.com/yiwang"&gt;@yiwang&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.healthjump.com/" rel="nofollow"&gt;Healthjump&lt;/a&gt; [&lt;a href="https://github.com/miscbits"&gt;@miscbits&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.hellofresh.com" rel="nofollow"&gt;HelloFresh&lt;/a&gt; [&lt;a href="https://github.com/tammymendt"&gt;@tammymendt&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/davidsbatista"&gt;@davidsbatista&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/iuriinedostup"&gt;@iuriinedostup&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.hipages.com.au/" rel="nofollow"&gt;Hipages&lt;/a&gt; [&lt;a href="https://github.com/arihantsurana"&gt;@arihantsurana&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://holimetrix.com/" rel="nofollow"&gt;Holimetrix&lt;/a&gt; [&lt;a href="https://github.com/thibault-ketterer"&gt;@thibault-ketterer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hootsuite"&gt;Hootsuite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.hostnfly.com/" rel="nofollow"&gt;Hostnfly&lt;/a&gt; [&lt;a href="https://github.com/CyrilLeMat"&gt;@CyrilLeMat&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/pierrechopin"&gt;@pierrechopin&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/alexisrosuel"&gt;@alexisrosuel&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/HotelQuickly"&gt;HotelQuickly&lt;/a&gt; [&lt;a href="https://github.com/zinuzoid"&gt;@zinuzoid&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://huq.io" rel="nofollow"&gt;Huq Industries&lt;/a&gt; [&lt;a href="https://github.com/huq-industries"&gt;@huqindustries&lt;/a&gt;, &lt;a href="https://github.com/alepuccetti"&gt;@alepuccetti&lt;/a&gt;, &lt;a href="https://github.com/turbomerl"&gt;@turbomerl&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://piay.iflix.com" rel="nofollow"&gt;Iflix&lt;/a&gt; [&lt;a href="https://github.com/ChaturvediSulabh"&gt;@ChaturvediSulabh&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ifttt.com/" rel="nofollow"&gt;IFTTT&lt;/a&gt; [&lt;a href="https://github.com/apurvajoshi"&gt;@apurvajoshi&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.iheart.com/" rel="nofollow"&gt;iHeartRadio&lt;/a&gt;[&lt;a href="https://github.com/yiwang"&gt;@yiwang&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.imgix.com/" rel="nofollow"&gt;imgix&lt;/a&gt; [&lt;a href="https://github.com/dclubb"&gt;@dclubb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ing.com/" rel="nofollow"&gt;ING&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.instacart.com/" rel="nofollow"&gt;Instacart &lt;g-emoji class="g-emoji" alias="carrot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f955.png"&gt;🥕&lt;/g-emoji&gt;&lt;/a&gt; [&lt;a href="https://github.com/arp1t"&gt;@arp1t&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/code-sauce"&gt;@code-sauce&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/jasonlew"&gt;@jasonlew&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/j4p3"&gt;@j4p3&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/lubert"&gt;@lubert&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/mmontagna"&gt;@mmontagna&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/RyanAD"&gt;@RyanAD&lt;/a&gt; &amp;amp;&lt;a href="https://github.com/zzadeh"&gt;@zzadeh&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.intercom.com/" rel="nofollow"&gt;Intercom&lt;/a&gt; [&lt;a href="https://github.com/fox"&gt;@fox&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/paulvic"&gt;@paulvic&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.interia.pl" rel="nofollow"&gt;Interia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://investorise.com/" rel="nofollow"&gt;Investorise&lt;/a&gt; [&lt;a href="https://github.com/svenvarkel"&gt;@svenvarkel&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.is2.co" rel="nofollow"&gt;iS2.co&lt;/a&gt; [&lt;a href="https://github.com/iS2co"&gt;@iS2co&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jampp"&gt;Jampp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.jeitto.com.br" rel="nofollow"&gt;Jeitto&lt;/a&gt; [&lt;a href="https://github.com/BrennerPablo"&gt;@BrennerPablo&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/ds-mauri"&gt;@ds-mauri&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jetlore.com/" rel="nofollow"&gt;Jetlore&lt;/a&gt; [&lt;a href="https://github.com/bderose"&gt;@bderose&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.jobteaser.com" rel="nofollow"&gt;JobTeaser&lt;/a&gt; [&lt;a href="https://github.com/stefani75"&gt;@stefani75&lt;/a&gt; &amp;amp;  &lt;a href="https://github.com/knil-sama"&gt;@knil-sama&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.julo.co.id/" rel="nofollow"&gt;JULO&lt;/a&gt; [&lt;a href="https://github.com/sepam"&gt;@sepam&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/tenapril"&gt;@tenapril&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/verzqy"&gt;@verzqy&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kalibrr.com/" rel="nofollow"&gt;Kalibrr&lt;/a&gt; [&lt;a href="https://github.com/charlesverdad"&gt;@charlesverdad&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kargo.com" rel="nofollow"&gt;Kargo&lt;/a&gt; [&lt;a href="https://github.com/chaithra-yenikapati"&gt;@chaithra-yenikapati&lt;/a&gt;, &lt;a href="https://github.com/akarsh3007"&gt;@akarsh3007&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/dineshanchan"&gt;@dineshanchan&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://karmiclabs.com" rel="nofollow"&gt;Karmic&lt;/a&gt; [&lt;a href="https://github.com/hyw"&gt;@hyw&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://king.com" rel="nofollow"&gt;King&lt;/a&gt; [&lt;a href="https://github.com/nathadfield"&gt;@nathadfield&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kapsarc"&gt;King Abdullah Petroleum Studies and Research Center(KAPSARC)&lt;/a&gt; [&lt;a href="https://github.com/saianupkumarp"&gt;@saianupkumarp&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kiwi.com/" rel="nofollow"&gt;Kiwi.com&lt;/a&gt; [&lt;a href="https://github.com/underyx"&gt;@underyx&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kogan"&gt;Kogan.com&lt;/a&gt; [&lt;a href="https://github.com/geeknam"&gt;@geeknam&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.korbit.co.kr/" rel="nofollow"&gt;Korbit&lt;/a&gt; [&lt;a href="https://github.com/jensenity"&gt;@jensenity&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kpn.com/" rel="nofollow"&gt;KPN B.V.&lt;/a&gt; [&lt;a href="https://github.com/biyanisuraj"&gt;@biyanisuraj&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/gmic"&gt;@gmic&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.kroton.com.br/" rel="nofollow"&gt;Kroton Educacional&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://fundacaolemann.org.br" rel="nofollow"&gt;Lemann Foundation&lt;/a&gt; [&lt;a href="https://github.com/fernandosjp"&gt;@fernandosjp&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.parts-unlimited.com/" rel="nofollow"&gt;LeMans Corporation&lt;/a&gt; [&lt;a href="https://github.com/alloydwhitlock"&gt;@alloydwhitlock&lt;/a&gt;] &amp;amp; [&lt;a href="https://github.com/tinyrye"&gt;@tinyrye&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lendup.com/" rel="nofollow"&gt;LendUp&lt;/a&gt; [&lt;a href="https://github.com/lendup"&gt;@lendup&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.letsbonus.com" rel="nofollow"&gt;LetsBonus&lt;/a&gt; [&lt;a href="https://github.com/jesusfcr"&gt;@jesusfcr&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/OpringaoDoTurno"&gt;@OpringaoDoTurno&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.libertyglobal.com/" rel="nofollow"&gt;Liberty Global&lt;/a&gt; [&lt;a href="https://github.com/LibertyGlobal/"&gt;@LibertyGlobal&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://liligo.com/" rel="nofollow"&gt;liligo&lt;/a&gt; [&lt;a href="https://github.com/tromika"&gt;@tromika&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.liulishuo.com/" rel="nofollow"&gt;LingoChamp&lt;/a&gt; [&lt;a href="https://github.com/haitaoyao"&gt;@haitaoyao&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.logitravel.com/" rel="nofollow"&gt;Logitravel Group&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.latimes.com/" rel="nofollow"&gt;Los Angeles Times&lt;/a&gt; [&lt;a href="https://github.com/standyro"&gt;@standyro&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://loksuvidha.com/" rel="nofollow"&gt;LokSuvidha&lt;/a&gt; [&lt;a href="https://github.com/saurabhwahile"&gt;@saurabhwahile&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luc.id" rel="nofollow"&gt;Lucid&lt;/a&gt; [&lt;a href="https://github.com/jbrownlucid"&gt;@jbrownlucid&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/kkourtchikov"&gt;@kkourtchikov&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lumosity.com/" rel="nofollow"&gt;Lumos Labs&lt;/a&gt; [&lt;a href="https://github.com/rfroetscher/"&gt;@rfroetscher&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/zzztimbo/"&gt;@zzztimbo&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lyft.com/" rel="nofollow"&gt;Lyft&lt;/a&gt; [&lt;a href="https://github.com/feng-tao"&gt;@feng-tao&lt;/a&gt;, &lt;a href="https://github.com/milton0825"&gt;@milton0825&lt;/a&gt;, &lt;a href="https://github.com/astahlman"&gt;@astahlman&lt;/a&gt;,
&lt;a href="https://github.com/youngyjd"&gt;@youngyjd&lt;/a&gt;, &lt;a href="https://github.com/ArgentFalcon"&gt;@ArgentFalcon&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.m4u.com.br/" rel="nofollow"&gt;M4U&lt;/a&gt; [&lt;a href="https://github.com/msantino"&gt;@msantino&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://madroneco.com/" rel="nofollow"&gt;Madrone&lt;/a&gt; [&lt;a href="https://github.com/mbreining"&gt;@mbreining&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/scotthb"&gt;@scotthb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://markovian.com/" rel="nofollow"&gt;Markovian&lt;/a&gt; [&lt;a href="https://github.com/al-xv"&gt;@al-xv&lt;/a&gt;, &lt;a href="https://github.com/skogsbaeck"&gt;@skogsbaeck&lt;/a&gt;, &lt;a href="https://github.com/waltherg"&gt;@waltherg&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mercadoni.com.co" rel="nofollow"&gt;Mercadoni&lt;/a&gt; [&lt;a href="https://github.com/demorenoc"&gt;@demorenoc&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.mercari.com/" rel="nofollow"&gt;Mercari&lt;/a&gt; [&lt;a href="https://github.com/yu-iskw"&gt;@yu-iskw&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MfgLabs"&gt;MFG Labs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.minodes.com" rel="nofollow"&gt;MiNODES&lt;/a&gt; [&lt;a href="https://github.com/dice89"&gt;@dice89&lt;/a&gt;, &lt;a href="https://github.com/diazcelsa"&gt;@diazcelsa&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.modmed.com/" rel="nofollow"&gt;Modernizing Medicine&lt;/a&gt;[&lt;a href="https://github.com/kehv1n"&gt;@kehv1n&lt;/a&gt;, &lt;a href="https://github.com/dalupus"&gt;@dalupus&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.multiply.com" rel="nofollow"&gt;Multiply&lt;/a&gt; [&lt;a href="https://github.com/nrhvyc"&gt;@nrhvyc&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mytaxi.com" rel="nofollow"&gt;mytaxi&lt;/a&gt; [&lt;a href="https://github.com/mytaxi"&gt;@mytaxi&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbc.ca" rel="nofollow"&gt;National Bank of Canada&lt;/a&gt; [&lt;a href="https://github.com/brilhana"&gt;@brilhana&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.neoway.com.br/" rel="nofollow"&gt;Neoway&lt;/a&gt; [&lt;a href="https://github.com/orgs/NeowayLabs/people"&gt;@neowaylabs&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nerdwallet.com" rel="nofollow"&gt;Nerdwallet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.newrelic.com" rel="nofollow"&gt;New Relic&lt;/a&gt; [&lt;a href="https://github.com/marcweil"&gt;@marcweil&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.newzoo.com" rel="nofollow"&gt;Newzoo&lt;/a&gt; [&lt;a href="https://github.com/newzoo-nexus"&gt;@newzoo-nexus&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nexttrucking.com/" rel="nofollow"&gt;NEXT Trucking&lt;/a&gt; [&lt;a href="https://github.com/earthmancash2"&gt;@earthmancash2&lt;/a&gt;, &lt;a href="https://github.com/kppullin"&gt;@kppullin&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nextdoor.com" rel="nofollow"&gt;Nextdoor&lt;/a&gt; [&lt;a href="https://github.com/SivaPandeti"&gt;@SivaPandeti&lt;/a&gt;, &lt;a href="https://github.com/zshapiro"&gt;@zshapiro&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/jthomas123"&gt;@jthomas123&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nine.com.au" rel="nofollow"&gt;Nine&lt;/a&gt; [&lt;a href="https://github.com/TheZepto"&gt;@TheZepto&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.goprime.io/" rel="nofollow"&gt;OdysseyPrime&lt;/a&gt; [&lt;a href="https://github.com/davideberdin"&gt;@davideberdin&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://offerupnow.com" rel="nofollow"&gt;OfferUp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.onefinestay.com" rel="nofollow"&gt;OneFineStay&lt;/a&gt; [&lt;a href="https://github.com/slangwald"&gt;@slangwald&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://okfn.org" rel="nofollow"&gt;Open Knowledge International&lt;/a&gt; &lt;a href="https://github.com/vitorbaptista"&gt;@vitorbaptista&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.optum.com/" rel="nofollow"&gt;Optum&lt;/a&gt; - &lt;a href="https://www.unitedhealthgroup.com/" rel="nofollow"&gt;UnitedHealthGroup&lt;/a&gt; [&lt;a href="https://github.com/hiteshrd"&gt;@hiteshrd&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.outcomehealth.com/" rel="nofollow"&gt;Outcome Health&lt;/a&gt; [&lt;a href="https://github.com/mikethoun"&gt;@mikethoun&lt;/a&gt;, &lt;a href="https://github.com/rolandotribo"&gt;@rolandotribo&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.github.com/overstock"&gt;Overstock&lt;/a&gt; [&lt;a href="https://github.com/mhousley"&gt;@mhousley&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/mct0006"&gt;@mct0006&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ovh.com" rel="nofollow"&gt;OVH&lt;/a&gt; [&lt;a href="https://github.com/ncrocfer"&gt;@ncrocfer&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/anthonyolea"&gt;@anthonyolea&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pagar.me/" rel="nofollow"&gt;Pagar.me&lt;/a&gt; [&lt;a href="https://github.com/pagarme"&gt;@pagarme&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.paloaltonetworks.com/" rel="nofollow"&gt;Palo Alto Networks&lt;/a&gt; [&lt;a href="https://github.com/PaloAltoNetworks"&gt;@PaloAltoNetworks&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pandora.com/" rel="nofollow"&gt;Pandora Media&lt;/a&gt; [&lt;a href="https://github.com/Acehaidrey"&gt;@Acehaidrey&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/wolfier"&gt;@wolfier&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://payfit.com" rel="nofollow"&gt;PayFit&lt;/a&gt; [&lt;a href="https://github.com/pcorbel"&gt;@pcorbel&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.paymill.com/" rel="nofollow"&gt;PAYMILL&lt;/a&gt; [&lt;a href="https://github.com/paymill"&gt;@paymill&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/matthiashuschle"&gt;@matthiashuschle&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.paypal.com/" rel="nofollow"&gt;PayPal&lt;/a&gt; [&lt;a href="https://github.com/r39132"&gt;@r39132&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/jhsenjaliya"&gt;@jhsenjaliya&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pecan.ai" rel="nofollow"&gt;Pecan&lt;/a&gt; [&lt;a href="https://github.com/ohadmata"&gt;@ohadmata&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pernod-ricard.com/" rel="nofollow"&gt;Pernod-Ricard&lt;/a&gt; [&lt;a href="https://github.com/romain-nio"&gt;@romain-nio&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.plaid.com/" rel="nofollow"&gt;Plaid&lt;/a&gt; [&lt;a href="https://github.com/plaid"&gt;@plaid&lt;/a&gt;, &lt;a href="https://github.com/AustinBGibbons"&gt;@AustinBGibbons&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/jeeyoungk"&gt;@jeeyoungk&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.playbuzz.com/" rel="nofollow"&gt;Playbuzz&lt;/a&gt; [&lt;a href="https://github.com/clintonboys"&gt;@clintonboys&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/dbn"&gt;@dbn&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pmc.com/" rel="nofollow"&gt;PMC&lt;/a&gt; [&lt;a href="https://github.com/andrewm4894"&gt;@andrewm4894&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.poshmark.com" rel="nofollow"&gt;Poshmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.postmates.com" rel="nofollow"&gt;Postmates&lt;/a&gt; [&lt;a href="https://github.com/syeoryn"&gt;@syeoryn&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.premise.com" rel="nofollow"&gt;Premise&lt;/a&gt; [&lt;a href="https://github.com/jmccallum-premise"&gt;@jmccallum-premise&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.prontotools.io/" rel="nofollow"&gt;Pronto Tools&lt;/a&gt; [&lt;a href="https://github.com/zkan"&gt;@zkan&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/mesodiar"&gt;@mesodiar&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://proton.ai/" rel="nofollow"&gt;proton.ai&lt;/a&gt; [&lt;a href="https://github.com/prmsolutions"&gt;@prmsolutions&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.publicispixelpark.de/" rel="nofollow"&gt;Publicis Pixelpark&lt;/a&gt; [&lt;a href="https://github.com/feluelle"&gt;@feluelle&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pubnub.com" rel="nofollow"&gt;PubNub&lt;/a&gt; [&lt;a href="https://github.com/jzucker2"&gt;@jzucker2&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pxydata.com" rel="nofollow"&gt;PXYData&lt;/a&gt; [&lt;a href="http://github.com/patchus"&gt;@patchus&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://qplum.co" rel="nofollow"&gt;Qplum&lt;/a&gt; [&lt;a href="https://github.com/manti"&gt;@manti&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quantopian.com/" rel="nofollow"&gt;Quantopian&lt;/a&gt; [&lt;a href="http://github.com/eronarn"&gt;@eronarn&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://qubole.com" rel="nofollow"&gt;Qubole&lt;/a&gt; [&lt;a href="https://github.com/msumit"&gt;@msumit&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://quizlet.com" rel="nofollow"&gt;Quizlet&lt;/a&gt; [&lt;a href="https://github.com/quizlet"&gt;@quizlet&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/" rel="nofollow"&gt;Quora&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.raizen.com.br/" rel="nofollow"&gt;Raízen&lt;/a&gt; [&lt;a href="https://github.com/rudlac"&gt;@rudlac&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/guifneves"&gt;@guifneves&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.rea-group.com/" rel="nofollow"&gt;REA Group&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/" rel="nofollow"&gt;Reddit&lt;/a&gt; [&lt;a href="https://github.com/reddit/"&gt;@reddit&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://reverb.com" rel="nofollow"&gt;Reverb&lt;/a&gt;[&lt;a href="https://github.com/reverbdotcom"&gt;@reverbdotcom&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.revolut.com/" rel="nofollow"&gt;Revolut&lt;/a&gt; [&lt;a href="https://github.com/sztanko"&gt;@sztanko&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/nautilus28"&gt;@nautilus28&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://robinhood.com" rel="nofollow"&gt;Robinhood&lt;/a&gt; [&lt;a href="https://github.com/vineet-rh"&gt;@vineet-rh&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scaleway.com" rel="nofollow"&gt;Scaleway&lt;/a&gt; [&lt;a href="https://github.com/kdeldycke"&gt;@kdeldycke&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.seasoned.co/" rel="nofollow"&gt;Seasoned&lt;/a&gt; [&lt;a href="https://github.com/joshuacano"&gt;@joshuacano&lt;/a&gt;] &amp;amp; [&lt;a href="https://github.com/mmyers5"&gt;@mmyers&lt;/a&gt;] &amp;amp; [&lt;a href="https://github.com/tjward"&gt;@tjward&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.secretescapes.com" rel="nofollow"&gt;Secret Escapes&lt;/a&gt; [&lt;a href="https://github.com/secretescapes"&gt;@secretescapes&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.semantics3.com" rel="nofollow"&gt;Semantics3&lt;/a&gt; [&lt;a href="https://github.com/abishekk92"&gt;@abishekk92&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Sense360"&gt;Sense360&lt;/a&gt; [&lt;a href="https://github.com/KamilMroczek"&gt;@kamilmroczek&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.sentry.io" rel="nofollow"&gt;Sentry.io&lt;/a&gt; [&lt;a href="https://github.com/tiopi"&gt;@tiopi&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://shopkick.com/" rel="nofollow"&gt;Shopkick&lt;/a&gt; [&lt;a href="https://github.com/shopkick"&gt;@shopkick&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://hello.getsidecar.com/" rel="nofollow"&gt;Sidecar&lt;/a&gt; [&lt;a href="https://github.com/getsidecar"&gt;@getsidecar&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.similarweb.com/" rel="nofollow"&gt;SimilarWeb&lt;/a&gt; [&lt;a href="https://github.com/similarweb"&gt;@similarweb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.skyscanner.net/" rel="nofollow"&gt;Skyscanner&lt;/a&gt; [&lt;a href="https://github.com/Skyscanner"&gt;@skyscanner&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.smartnews.com/" rel="nofollow"&gt;SmartNews&lt;/a&gt; [&lt;a href="https://github.com/takus"&gt;@takus&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.snaptravel.com/" rel="nofollow"&gt;SnapTravel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.socialcops.com/" rel="nofollow"&gt;SocialCops&lt;/a&gt; [&lt;a href="https://github.com/vinayak-mehta"&gt;@vinayak-mehta&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/sharky93"&gt;@sharky93&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.societegenerale.fr/" rel="nofollow"&gt;Société générale&lt;/a&gt; [&lt;a href="https://github.com/medmrgh"&gt;@medmrgh&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/s83"&gt;@s83&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.spotahome.com/" rel="nofollow"&gt;Spotahome&lt;/a&gt; [&lt;a href="https://github.com/spotahome"&gt;@spotahome&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/spothero"&gt;SpotHero&lt;/a&gt; [&lt;a href="https://github.com/benjigoldberg"&gt;@benjigoldberg&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/spotify"&gt;Spotify&lt;/a&gt; [&lt;a href="https://github.com/znichols"&gt;@znichols&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://squareup.com/" rel="nofollow"&gt;Square&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://beta.stackspace.io/" rel="nofollow"&gt;Stackspace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.stone.co" rel="nofollow"&gt;StoneCo&lt;/a&gt; [&lt;a href="https://github.com/lgwacker"&gt;@lgwacker&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://strava.com" rel="nofollow"&gt;Strava&lt;/a&gt; [&lt;a href="https://github.com/strava"&gt;@strava&lt;/a&gt;, &lt;a href="https://github.com/dhuang"&gt;@dhuang&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/liamstewart"&gt;@liamstewart&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stripe.com" rel="nofollow"&gt;Stripe&lt;/a&gt; [&lt;a href="https://github.com/jbalogh"&gt;@jbalogh&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.strongmind.com" rel="nofollow"&gt;Strongmind&lt;/a&gt; [&lt;a href="https://github.com/tomchapin"&gt;@tomchapin&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/wongstein"&gt;@wongstein&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.surfline.com/" rel="nofollow"&gt;Surfline&lt;/a&gt; [&lt;a href="https://github.com/jawang35"&gt;@jawang35&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://t2systems.com" rel="nofollow"&gt;T2 Systems&lt;/a&gt; [&lt;a href="https://github.com/unclaimedpants"&gt;@unclaimedpants&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tails.com/" rel="nofollow"&gt;Tails.com&lt;/a&gt; [&lt;a href="https://github.com/alanmcruickshank"&gt;@alanmcruickshank&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tek.fi/en" rel="nofollow"&gt;TEK&lt;/a&gt; [&lt;a href="https://github.com/telac"&gt;@telac&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.alpha.company/" rel="nofollow"&gt;Telefonica Innovation Alpha&lt;/a&gt; [&lt;a href="https://github.com/Alpha-health"&gt;@Alpha-Health&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.teliacompany.com/en" rel="nofollow"&gt;Telia Company&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tesla.com/" rel="nofollow"&gt;Tesla&lt;/a&gt; [&lt;a href="https://github.com/thoralf-gutierrez"&gt;@thoralf-gutierrez&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.homedepot.com/" rel="nofollow"&gt;The Home Depot&lt;/a&gt;[&lt;a href="https://github.com/apekshithr"&gt;@apekshithr&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.theiconic.com.au/" rel="nofollow"&gt;THE ICONIC&lt;/a&gt; [&lt;a href="https://github.com/revathijay"&gt;@revathijay&lt;/a&gt;] [&lt;a href="https://github.com/ilikedata"&gt;@ilikedata&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://thinkingmachin.es" rel="nofollow"&gt;Thinking Machines&lt;/a&gt; [&lt;a href="https://github.com/marksteve"&gt;@marksteve&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.thinknear.com/" rel="nofollow"&gt;Thinknear&lt;/a&gt; [&lt;a href="https://github.com/d3cay1"&gt;@d3cay1&lt;/a&gt;, &lt;a href="https://github.com/ccson"&gt;@ccson&lt;/a&gt;, &amp;amp; &lt;a href="https://github.com/ababian"&gt;@ababian&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.thoughtworks.com/" rel="nofollow"&gt;ThoughtWorks&lt;/a&gt; [&lt;a href="https://github.com/sann3"&gt;@sann3&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.thumbtack.com/" rel="nofollow"&gt;Thumbtack&lt;/a&gt; [&lt;a href="https://github.com/natekupp"&gt;@natekupp&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tictail.com/" rel="nofollow"&gt;Tictail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tile.com/" rel="nofollow"&gt;Tile&lt;/a&gt; [&lt;a href="https://github.com/ranjanmanish"&gt;@ranjanmanish&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tinder.com/" rel="nofollow"&gt;Tinder&lt;/a&gt; [&lt;a href="https://github.com/kbendick"&gt;@kbendick&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tokenanalyst"&gt;TokenAnalyst&lt;/a&gt; [&lt;a href="https://github.com/simonohanlon101"&gt;@simonohanlon101&lt;/a&gt;, &lt;a href="https://github.com/ankitchiplunkar"&gt;@ankitchiplunkar&lt;/a&gt;, &lt;a href="https://github.com/sidshekhar"&gt;@sidshekhar&lt;/a&gt;, &lt;a href="https://github.com/sp6pe"&gt;@sp6pe&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tokopedia.com/" rel="nofollow"&gt;Tokopedia&lt;/a&gt; [&lt;a href="https://github.com/topedmaria"&gt;@topedmaria&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.trocafone.com/" rel="nofollow"&gt;Trocafone&lt;/a&gt; [&lt;a href="https://github.com/idontdomath"&gt;@idontdomath&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/gseva"&gt;@gseva&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/ordonezf"&gt;@ordonezf&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/PalmaLeandro"&gt;@PalmaLeandro&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.twinelabs.com/" rel="nofollow"&gt;Twine Labs&lt;/a&gt; [&lt;a href="https://github.com/ivorpeles"&gt;@ivorpeles&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.twitter.com/" rel="nofollow"&gt;Twitter&lt;/a&gt; [&lt;a href="https://github.com/aoen"&gt;@aoen&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ubisoft.com/" rel="nofollow"&gt;Ubisoft&lt;/a&gt; [&lt;a href="https://github.com/Walkoss"&gt;@Walkoss&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.udacity.com/" rel="nofollow"&gt;Udacity&lt;/a&gt; [&lt;a href="https://github.com/DandikUnited"&gt;@dandikunited&lt;/a&gt;, &lt;a href="https://github.com/simon-uc"&gt;@simon-uc&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.united.com/" rel="nofollow"&gt;United Airlines&lt;/a&gt; [&lt;a href="https://github.com/ilopezfr"&gt;@ilopezfr&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.upsight.com" rel="nofollow"&gt;Upsight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://veer.tv" rel="nofollow"&gt;VeeR VR&lt;/a&gt; [&lt;a href="https://github.com/pishilong"&gt;@pishilong&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.veikkaus.fi" rel="nofollow"&gt;Veikkaus&lt;/a&gt; [&lt;a href="https://github.com/hixus"&gt;@hixus&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.vente-exclusive.com/" rel="nofollow"&gt;Vente-Exclusive.com&lt;/a&gt; [&lt;a href="https://github.com/alexvanboxel"&gt;@alexvanboxel&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.vevo.com/" rel="nofollow"&gt;Vevo&lt;/a&gt; [&lt;a href="https://github.com/csetiawan"&gt;@csetiawan&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/jerrygillespie"&gt;@jerrygillespie&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.vidio.com/" rel="nofollow"&gt;Vidio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ville.montreal.qc.ca/" rel="nofollow"&gt;Ville de Montréal&lt;/a&gt;&lt;a href="https://github.com/VilledeMontreal/"&gt;@VilledeMontreal&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vnomics"&gt;Vnomics&lt;/a&gt; [&lt;a href="https://github.com/lpalum"&gt;@lpalum&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.walmartlabs.com" rel="nofollow"&gt;Walmart Labs&lt;/a&gt; [&lt;a href="https://github.com/bharathpalaksha"&gt;@bharathpalaksha&lt;/a&gt;, &lt;a href="https://github.com/vipul007ravi"&gt;@vipul007ravi&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.waze.com" rel="nofollow"&gt;Waze&lt;/a&gt; [&lt;a href="https://github.com/wazeHQ"&gt;@waze&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.wepay.com" rel="nofollow"&gt;WePay&lt;/a&gt; [&lt;a href="https://github.com/criccomini"&gt;@criccomini&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/mtagle"&gt;@mtagle&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WeTransfer"&gt;WeTransfer&lt;/a&gt; [&lt;a href="https://github.com/coredipper"&gt;@coredipper&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/higee"&gt;@higee&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/azclub"&gt;@azclub&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.whistle.com" rel="nofollow"&gt;Whistle Labs&lt;/a&gt; [&lt;a href="https://github.com/ananya77041"&gt;@ananya77041&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wisebanyan.com/" rel="nofollow"&gt;WiseBanyan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.wooga.com/" rel="nofollow"&gt;Wooga&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.wrike.com" rel="nofollow"&gt;Wrike&lt;/a&gt; [&lt;a href="https://github.com/eliseealex"&gt;@eliseealex&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/Teoretic6"&gt;teoretic6&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.xero.com/" rel="nofollow"&gt;Xero&lt;/a&gt; [&lt;a href="https://github.com/yan9yu"&gt;@yan9yu&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/adamantnz/"&gt;adamantnz&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.xoom.com/" rel="nofollow"&gt;Xoom&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.yahoo.com/" rel="nofollow"&gt;Yahoo!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.yieldr.com/" rel="nofollow"&gt;Yieldr&lt;/a&gt; [&lt;a href="https://github.com/ggeorgiadis"&gt;@ggeorgiadis&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.zapier.com" rel="nofollow"&gt;Zapier&lt;/a&gt; [&lt;a href="https://github.com/drknexus"&gt;@drknexus&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/statwonk"&gt;@statwonk&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.zego.com/" rel="nofollow"&gt;Zego&lt;/a&gt; [&lt;a href="https://github.com/ruimffl"&gt;@ruimffl&lt;/a&gt;, &lt;a href="https://github.com/james-welly"&gt;@james-welly&lt;/a&gt;, &lt;a href="https://github.com/ken-payne"&gt;@ken-payne&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.github.com/zendesk"&gt;Zendesk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zen.ly" rel="nofollow"&gt;Zenly&lt;/a&gt; [&lt;a href="https://github.com/cerisier"&gt;@cerisier&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/jbdalido"&gt;@jbdalido&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.zymergen.com/" rel="nofollow"&gt;Zymergen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.zynga.com" rel="nofollow"&gt;Zynga&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-who-maintains-apache-airflow" class="anchor" aria-hidden="true" href="#who-maintains-apache-airflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Who Maintains Apache Airflow?&lt;/h2&gt;
&lt;p&gt;Airflow is the work of the &lt;a href="https://github.com/apache/airflow/graphs/contributors"&gt;community&lt;/a&gt;,
but the &lt;a href="https://people.apache.org/committers-by-project.html#airflow" rel="nofollow"&gt;core committers/maintainers&lt;/a&gt;
are responsible for reviewing and merging PRs as well as steering conversation around new feature requests.
If you would like to become a maintainer, please review the Apache Airflow
&lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Committers" rel="nofollow"&gt;committer requirements&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-can-i-use-the-apache-airflow-logo-in-my-presentation" class="anchor" aria-hidden="true" href="#can-i-use-the-apache-airflow-logo-in-my-presentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Can I use the Apache Airflow logo in my presentation?&lt;/h2&gt;
&lt;p&gt;Yes! Be sure to abide by the Apache Foundation &lt;a href="https://www.apache.org/foundation/marks/#books" rel="nofollow"&gt;trademark policies&lt;/a&gt; and the Apache Airflow &lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook" rel="nofollow"&gt;Brandbook&lt;/a&gt;. The most up to date logos are found in &lt;a href="/docs/img/logos"&gt;this repo&lt;/a&gt; and on the Apache Software Foundation &lt;a href="https://www.apache.org/logos/about.html" rel="nofollow"&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://airflow.apache.org/" rel="nofollow"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://apache-airflow-slack.herokuapp.com/" rel="nofollow"&gt;Chat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Links" rel="nofollow"&gt;More&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>apache</author><guid isPermaLink="false">https://github.com/apache/airflow</guid><pubDate>Sat, 04 Jan 2020 00:09:00 GMT</pubDate></item><item><title>localstack/localstack #10 in Python, Today</title><link>https://github.com/localstack/localstack</link><description>&lt;p&gt;&lt;i&gt;💻  A fully functional local AWS cloud stack. Develop and test your cloud &amp; Serverless apps offline!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://travis-ci.org/localstack/localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9776494b9a6b388dc743ef4f1fe0f48418996403/68747470733a2f2f7472617669732d63692e6f72672f6c6f63616c737461636b2f6c6f63616c737461636b2e737667" alt="Build Status" data-canonical-src="https://travis-ci.org/localstack/localstack.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="#backers"&gt;&lt;img src="https://camo.githubusercontent.com/7503d4e605e56494b94f7e899b59c12d6869e6d4/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f6261636b6572732f62616467652e737667" alt="Backers on Open Collective" data-canonical-src="https://opencollective.com/localstack/backers/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="#sponsors"&gt;&lt;img src="https://camo.githubusercontent.com/4ce5d939a7baa05f5513a28bced276b40163e726/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f72732f62616467652e737667" alt="Sponsors on Open Collective" data-canonical-src="https://opencollective.com/localstack/sponsors/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/localstack/localstack?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a25d81482ec1f47ecef8ab8f5c6ea87316d0df71/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6c6f63616c737461636b2f6c6f63616c737461636b2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/localstack/localstack/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/localstack/Platform" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ef7f49d6c9d82d4762efd93e6c5190ed7ff070e8/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6c6f63616c737461636b2f506c6174666f726d2e737667" alt="Gitter" data-canonical-src="https://img.shields.io/gitter/room/localstack/Platform.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://badge.fury.io/py/localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3f7ff1cf090f0a7a2ca744acb42a565188b1e51f/68747470733a2f2f62616467652e667572792e696f2f70792f6c6f63616c737461636b2e737667" alt="PyPI Version" data-canonical-src="https://badge.fury.io/py/localstack.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://img.shields.io/pypi/l/localstack.svg" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8c44673b3399efcaa024ef3f64e031acd53422f5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f6c6f63616c737461636b2e737667" alt="PyPI License" data-canonical-src="https://img.shields.io/pypi/l/localstack.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://codeclimate.com/github/localstack/localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/bb304a7e024bd89e75e3ee5922f276d69b0399f0/68747470733a2f2f636f6465636c696d6174652e636f6d2f6769746875622f6c6f63616c737461636b2f6c6f63616c737461636b2f6261646765732f6770612e737667" alt="Code Climate" data-canonical-src="https://codeclimate.com/github/localstack/localstack/badges/gpa.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/_localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/83d4084f7b71558e33b08844da5c773a8657e271/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-localstack---a-fully-functional-local-aws-cloud-stack" class="anchor" aria-hidden="true" href="#localstack---a-fully-functional-local-aws-cloud-stack"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LocalStack - A fully functional local AWS cloud stack&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/localstack/localstack/raw/master/localstack/dashboard/web/img/localstack.png"&gt;&lt;img src="https://github.com/localstack/localstack/raw/master/localstack/dashboard/web/img/localstack.png" alt="LocalStack" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;LocalStack&lt;/em&gt; provides an easy-to-use test/mocking framework for developing Cloud applications.&lt;/p&gt;
&lt;p&gt;Currently, the focus is primarily on supporting the AWS cloud stack.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-announcements" class="anchor" aria-hidden="true" href="#announcements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Announcements&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;2019-10-09&lt;/strong&gt;: &lt;strong&gt;LocalStack Pro is out!&lt;/strong&gt; We're incredibly excited to announce the launch of LocalStack Pro - the enterprise version of LocalStack with additional APIs and advanced features. Check out the free trial at &lt;a href="https://localstack.cloud" rel="nofollow"&gt;https://localstack.cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2018-01-10&lt;/strong&gt;: &lt;strong&gt;Help wanted!&lt;/strong&gt; Please &lt;a href="https://lambdastudy.typeform.com/to/kDUvvy?source=localstack-github" rel="nofollow"&gt;fill out this survey&lt;/a&gt; to support a research study on the usage of Serverless and Function-as-a-Service (FaaS) services, conducted at Chalmers University of Technology. The survey only takes 5-10 minutes of your time. Many thanks for your participation!!
&lt;ul&gt;
&lt;li&gt;The result from this study can be found &lt;a href="https://research.chalmers.se/en/publication/508147" rel="nofollow"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2017-08-27&lt;/strong&gt;: &lt;strong&gt;We need your support!&lt;/strong&gt; LocalStack is growing fast, we now have thousands of developers using the platform on a regular basis. Last month we have recorded a staggering 100k test runs, with 25k+ DynamoDB tables, 20k+ SQS queues, 15k+ Kinesis streams, 13k+ S3 buckets, and 10k+ Lambda functions created locally - for 0$ costs (more details to be published soon). Bug and feature requests are pouring in, and we now need some support from &lt;em&gt;you&lt;/em&gt; to keep the open source version actively maintained. Please check out &lt;a href="https://opencollective.com/localstack" rel="nofollow"&gt;Open Collective&lt;/a&gt; and become a &lt;a href="https://github.com/localstack/localstack#backers"&gt;backer&lt;/a&gt; or &lt;a href="https://github.com/localstack/localstack#backers"&gt;supporter&lt;/a&gt; of the project today! Thanks everybody for contributing. ♥&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2017-07-20&lt;/strong&gt;: Please note: Starting with version &lt;code&gt;0.7.0&lt;/code&gt;, the Docker image will be pushed
and kept up to date under the &lt;strong&gt;new name&lt;/strong&gt; &lt;code&gt;localstack/localstack&lt;/code&gt;. (This means that you may
have to update your CI configurations.) Please refer to the updated
&lt;strong&gt;&lt;a href="doc/end_user_license_agreement"&gt;End-User License Agreement (EULA)&lt;/a&gt;&lt;/strong&gt; for the new versions.
The old Docker image (&lt;code&gt;atlassianlabs/localstack&lt;/code&gt;) is still available but will not be maintained
any longer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h1&gt;
&lt;p&gt;LocalStack spins up the following core Cloud APIs on your local machine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;API Gateway&lt;/strong&gt; at &lt;a href="http://localhost:4567" rel="nofollow"&gt;http://localhost:4567&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kinesis&lt;/strong&gt; at &lt;a href="http://localhost:4568" rel="nofollow"&gt;http://localhost:4568&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DynamoDB&lt;/strong&gt; at &lt;a href="http://localhost:4569" rel="nofollow"&gt;http://localhost:4569&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DynamoDB Streams&lt;/strong&gt; at &lt;a href="http://localhost:4570" rel="nofollow"&gt;http://localhost:4570&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;S3&lt;/strong&gt; at &lt;a href="http://localhost:4572" rel="nofollow"&gt;http://localhost:4572&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Firehose&lt;/strong&gt; at &lt;a href="http://localhost:4573" rel="nofollow"&gt;http://localhost:4573&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lambda&lt;/strong&gt; at &lt;a href="http://localhost:4574" rel="nofollow"&gt;http://localhost:4574&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SNS&lt;/strong&gt; at &lt;a href="http://localhost:4575" rel="nofollow"&gt;http://localhost:4575&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQS&lt;/strong&gt; at &lt;a href="http://localhost:4576" rel="nofollow"&gt;http://localhost:4576&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Redshift&lt;/strong&gt; at &lt;a href="http://localhost:4577" rel="nofollow"&gt;http://localhost:4577&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Elasticsearch Service&lt;/strong&gt; at &lt;a href="http://localhost:4578" rel="nofollow"&gt;http://localhost:4578&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SES&lt;/strong&gt; at &lt;a href="http://localhost:4579" rel="nofollow"&gt;http://localhost:4579&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Route53&lt;/strong&gt; at &lt;a href="http://localhost:4580" rel="nofollow"&gt;http://localhost:4580&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CloudFormation&lt;/strong&gt; at &lt;a href="http://localhost:4581" rel="nofollow"&gt;http://localhost:4581&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CloudWatch&lt;/strong&gt; at &lt;a href="http://localhost:4582" rel="nofollow"&gt;http://localhost:4582&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSM&lt;/strong&gt; at &lt;a href="http://localhost:4583" rel="nofollow"&gt;http://localhost:4583&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SecretsManager&lt;/strong&gt; at &lt;a href="http://localhost:4584" rel="nofollow"&gt;http://localhost:4584&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;StepFunctions&lt;/strong&gt; at &lt;a href="http://localhost:4585" rel="nofollow"&gt;http://localhost:4585&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CloudWatch Logs&lt;/strong&gt; at &lt;a href="http://localhost:4586" rel="nofollow"&gt;http://localhost:4586&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EventBridge (CloudWatch Events)&lt;/strong&gt; at &lt;a href="http://localhost:4587" rel="nofollow"&gt;http://localhost:4587&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;STS&lt;/strong&gt; at &lt;a href="http://localhost:4592" rel="nofollow"&gt;http://localhost:4592&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IAM&lt;/strong&gt; at &lt;a href="http://localhost:4593" rel="nofollow"&gt;http://localhost:4593&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EC2&lt;/strong&gt; at &lt;a href="http://localhost:4597" rel="nofollow"&gt;http://localhost:4597&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KMS&lt;/strong&gt; at &lt;a href="http://localhost:4599" rel="nofollow"&gt;http://localhost:4599&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to the above, the &lt;a href="https://localstack.cloud/#pricing" rel="nofollow"&gt;&lt;strong&gt;Pro version&lt;/strong&gt; of LocalStack&lt;/a&gt; supports additional APIs and advanced features, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AppSync&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Athena&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CloudFront&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cognito&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ECS/EKS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ElastiCache&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EMR&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IoT&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lambda Layers&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RDS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;XRay&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interactive UIs to manage resources&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test report dashboards&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;...and much, much more to come!&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-why-localstack" class="anchor" aria-hidden="true" href="#why-localstack"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why LocalStack?&lt;/h2&gt;
&lt;p&gt;LocalStack builds on existing best-of-breed mocking/testing tools, most notably
&lt;a href="https://github.com/mhart/kinesalite"&gt;kinesalite&lt;/a&gt;/&lt;a href="https://github.com/mhart/dynalite"&gt;dynalite&lt;/a&gt;
and &lt;a href="https://github.com/spulec/moto"&gt;moto&lt;/a&gt;. While these tools are &lt;em&gt;awesome&lt;/em&gt; (!), they lack functionality
for certain use cases. LocalStack combines the tools, makes them interoperable, and adds important
missing functionality on top of them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Error injection:&lt;/strong&gt; LocalStack allows to inject errors frequently occurring in real Cloud environments,
for instance &lt;code&gt;ProvisionedThroughputExceededException&lt;/code&gt; which is thrown by Kinesis or DynamoDB if the amount of
read/write throughput is exceeded.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Isolated processes&lt;/strong&gt;: All services in LocalStack run in separate processes. The overhead of additional
processes is negligible, and the entire stack can easily be executed on any developer machine and CI server.
In moto, components are often hard-wired in RAM (e.g., when forwarding a message on an SNS topic to an SQS queue,
the queue endpoint is looked up in a local hash map). In contrast, LocalStack services live in isolation
(separate processes available via HTTP), which fosters true decoupling and more closely resembles the real
cloud environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pluggable services&lt;/strong&gt;: All services in LocalStack are easily pluggable (and replaceable), due to the fact that
we are using isolated processes for each service. This allows us to keep the framework up-to-date and select
best-of-breed mocks for each individual service.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;python&lt;/code&gt; (both Python 2.x and 3.x supported)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pip&lt;/code&gt; (python package manager)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Docker&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installing" class="anchor" aria-hidden="true" href="#installing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing&lt;/h2&gt;
&lt;p&gt;The easiest way to install LocalStack is via &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install localstack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please do &lt;strong&gt;not&lt;/strong&gt; use &lt;code&gt;sudo&lt;/code&gt; or the &lt;code&gt;root&lt;/code&gt; user - LocalStack
should be installed and started entirely under a local non-root user. If you have problems
with permissions in MacOS X Sierra, install with &lt;code&gt;pip install --user localstack&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-in-docker" class="anchor" aria-hidden="true" href="#running-in-docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running in Docker&lt;/h2&gt;
&lt;p&gt;By default, LocalStack gets started inside a Docker container using this command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localstack start
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that on MacOS you may have to run &lt;code&gt;TMPDIR=/private$TMPDIR localstack start --docker&lt;/code&gt; if
&lt;code&gt;$TMPDIR&lt;/code&gt; contains a symbolic link that cannot be mounted by Docker.)&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-using-docker-compose" class="anchor" aria-hidden="true" href="#using-docker-compose"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using &lt;code&gt;docker-compose&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;You can also use the &lt;code&gt;docker-compose.yml&lt;/code&gt; file from the repository and use this command (currently requires &lt;code&gt;docker-compose&lt;/code&gt; version 2.1+):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker-compose up
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that on MacOS you may have to run &lt;code&gt;TMPDIR=/private$TMPDIR docker-compose up&lt;/code&gt; if
&lt;code&gt;$TMPDIR&lt;/code&gt; contains a symbolic link that cannot be mounted by Docker.)&lt;/p&gt;
&lt;p&gt;Use on existing docker-compose project. Add in existing services. The project can be found in docker hub, no need to download or clone source:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;version: '2.1'
services:
...
  localstack:
    image: localstack/localstack
    ports:
      - "4567-4584:4567-4584"
      - "${PORT_WEB_UI-8080}:${PORT_WEB_UI-8080}"
    environment:
      - SERVICES=${SERVICES- }
      - DEBUG=${DEBUG- }
      - DATA_DIR=${DATA_DIR- }
      - PORT_WEB_UI=${PORT_WEB_UI- }
      - LAMBDA_EXECUTOR=${LAMBDA_EXECUTOR- }
      - KINESIS_ERROR_PROBABILITY=${KINESIS_ERROR_PROBABILITY- }
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - "${TMPDIR:-/tmp/localstack}:/tmp/localstack"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To facilitate interoperability, configuration variables can be prefixed with &lt;code&gt;LOCALSTACK_&lt;/code&gt; in docker. For instance, setting &lt;code&gt;LOCALSTACK_SERVICES=s3&lt;/code&gt; is equivalent to &lt;code&gt;SERVICES=s3&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-starting-locally-non-docker-mode" class="anchor" aria-hidden="true" href="#starting-locally-non-docker-mode"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting locally (non-Docker mode)&lt;/h2&gt;
&lt;p&gt;Alternatively, the infrastructure can be spun up on the local host machine (without using Docker) using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localstack start --host
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that this will require &lt;a href="#Developing"&gt;additional dependencies&lt;/a&gt;, and currently is not supported on some operating systems, including Windows.)&lt;/p&gt;
&lt;p&gt;LocalStack will attempt to automatically fetch the missing dependencies when you first start it up in "host" mode; alternatively, you can use the &lt;code&gt;full&lt;/code&gt; profile to install all dependencies at &lt;code&gt;pip&lt;/code&gt; installation time:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install "localstack[full]"
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-configurations" class="anchor" aria-hidden="true" href="#configurations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configurations&lt;/h2&gt;
&lt;p&gt;You can pass the following environment variables to LocalStack:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;SERVICES&lt;/code&gt;: Comma-separated list of service names and (optional) ports they should run on.
If no port is specified, a default port is used. Service names basically correspond to the
&lt;a href="http://docs.aws.amazon.com/cli/latest/reference/#available-services" rel="nofollow"&gt;service names of the AWS CLI&lt;/a&gt;
(&lt;code&gt;kinesis&lt;/code&gt;, &lt;code&gt;lambda&lt;/code&gt;, &lt;code&gt;sqs&lt;/code&gt;, etc), although LocalStack only supports a subset of them.
Example value: &lt;code&gt;kinesis,lambda:4569,sqs:4570&lt;/code&gt; to start Kinesis on the default port,
Lambda on port 4569, and SQS on port 4570. In addition, the following shorthand values can be
specified to run a predefined ensemble of services:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;serverless&lt;/code&gt;: run services often used for Serverless apps (&lt;code&gt;iam&lt;/code&gt;, &lt;code&gt;lambda&lt;/code&gt;, &lt;code&gt;dynamodb&lt;/code&gt;, &lt;code&gt;apigateway&lt;/code&gt;, &lt;code&gt;s3&lt;/code&gt;, &lt;code&gt;sns&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DEFAULT_REGION&lt;/code&gt;: AWS region to use when talking to the API (defaults to &lt;code&gt;us-east-1&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HOSTNAME&lt;/code&gt;: Name of the host to expose the services internally (defaults to &lt;code&gt;localhost&lt;/code&gt;).
Use this to customize the framework-internal communication, e.g., if services are
started in different containers using docker-compose.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HOSTNAME_EXTERNAL&lt;/code&gt;: Name of the host to expose the services externally (defaults to &lt;code&gt;localhost&lt;/code&gt;).
This host is used, e.g., when returning queue URLs from the SQS service to the client.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;SERVICE&amp;gt;_PORT&lt;/code&gt;: Port number to bind a specific service to (defaults to service ports above).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;SERVICE&amp;gt;_PORT_EXTERNAL&lt;/code&gt;: Port number to expose a specific service externally (defaults to service ports above). &lt;code&gt;SQS_PORT_EXTERNAL&lt;/code&gt;, for example, is used when returning queue URLs from the SQS service to the client.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;USE_SSL&lt;/code&gt;: Whether to use &lt;code&gt;https://...&lt;/code&gt; URLs with SSL encryption (defaults to &lt;code&gt;false&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;KINESIS_ERROR_PROBABILITY&lt;/code&gt;: Decimal value between 0.0 (default) and 1.0 to randomly
inject &lt;code&gt;ProvisionedThroughputExceededException&lt;/code&gt; errors into Kinesis API responses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;KINESIS_SHARD_LIMIT&lt;/code&gt;: Integer value (defaults to &lt;code&gt;100&lt;/code&gt;) or &lt;code&gt;Infinity&lt;/code&gt; (to disable), in which to kinesalite will start throwing exceptions to mimick the &lt;a href="https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html" rel="nofollow"&gt;default shard limit&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;KINESIS_LATENCY&lt;/code&gt;: Integer value (defaults to &lt;code&gt;500&lt;/code&gt;) or &lt;code&gt;0&lt;/code&gt; (to disable), in which to kinesalite will delay returning a response in order to mimick latency from a live AWS call.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DYNAMODB_ERROR_PROBABILITY&lt;/code&gt;: Decimal value between 0.0 (default) and 1.0 to randomly
inject &lt;code&gt;ProvisionedThroughputExceededException&lt;/code&gt; errors into DynamoDB API responses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_EXECUTOR&lt;/code&gt;: Method to use for executing Lambda functions. Possible values are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;local&lt;/code&gt;: run Lambda functions in a temporary directory on the local machine&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker&lt;/code&gt;: run each function invocation in a separate Docker container&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker-reuse&lt;/code&gt;: create one Docker container per function and reuse it across invocations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For &lt;code&gt;docker&lt;/code&gt; and &lt;code&gt;docker-reuse&lt;/code&gt;, if LocalStack itself is started inside Docker, then
the &lt;code&gt;docker&lt;/code&gt; command needs to be available inside the container (usually requires to run the
container in privileged mode). Default is &lt;code&gt;docker&lt;/code&gt;, fallback to &lt;code&gt;local&lt;/code&gt; if Docker is not available.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_REMOTE_DOCKER&lt;/code&gt; determines whether Lambda code is copied or mounted into containers.
Possible values are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;true&lt;/code&gt; (default): your Lambda function definitions will be passed to the container by
copying the zip file (potentially slower). It allows for remote execution, where the host
and the client are not on the same machine.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;false&lt;/code&gt;: your Lambda function definitions will be passed to the container by mounting a
volume (potentially faster). This requires to have the Docker client and the Docker
host on the same machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_DOCKER_NETWORK&lt;/code&gt;: Optional Docker network for the container running your lambda function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_CONTAINER_REGISTRY&lt;/code&gt; Use an alternative docker registry to pull lambda execution containers (default: &lt;code&gt;lambci/lambda&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_REMOVE_CONTAINERS&lt;/code&gt;: Whether to remove containers after Lambdas finished executing (default: &lt;code&gt;true&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DATA_DIR&lt;/code&gt;: Local directory for saving persistent data (currently only supported for these services:
Kinesis, DynamoDB, Elasticsearch, S3). Set it to &lt;code&gt;/tmp/localstack/data&lt;/code&gt; to enable persistence
(&lt;code&gt;/tmp/localstack&lt;/code&gt; is mounted into the Docker container), leave blank to disable
persistence (default).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;PORT_WEB_UI&lt;/code&gt;: Port for the Web user interface (dashboard). Default is &lt;code&gt;8080&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;SERVICE&amp;gt;_BACKEND&lt;/code&gt;: Custom endpoint URL to use for a specific service, where &lt;code&gt;&amp;lt;SERVICE&amp;gt;&lt;/code&gt; is the uppercase
service name (currently works for: &lt;code&gt;APIGATEWAY&lt;/code&gt;, &lt;code&gt;CLOUDFORMATION&lt;/code&gt;, &lt;code&gt;DYNAMODB&lt;/code&gt;, &lt;code&gt;ELASTICSEARCH&lt;/code&gt;,
&lt;code&gt;KINESIS&lt;/code&gt;, &lt;code&gt;S3&lt;/code&gt;, &lt;code&gt;SNS&lt;/code&gt;, &lt;code&gt;SQS&lt;/code&gt;). This allows to easily integrate third-party services into LocalStack.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;FORCE_NONINTERACTIVE&lt;/code&gt;: when running with Docker, disables the &lt;code&gt;--interactive&lt;/code&gt; and &lt;code&gt;--tty&lt;/code&gt; flags. Useful when running headless.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DOCKER_FLAGS&lt;/code&gt;: Allows to pass custom flags (e.g., volume mounts) to "docker run" when running LocalStack in Docker.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DOCKER_CMD&lt;/code&gt;: Shell command used to run Docker containers, e.g., set to &lt;code&gt;"sudo docker"&lt;/code&gt; to run as sudo (default: &lt;code&gt;docker&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;START_WEB&lt;/code&gt;: Flag to control whether the Web API should be started in Docker (values: &lt;code&gt;0&lt;/code&gt;/&lt;code&gt;1&lt;/code&gt;; default: &lt;code&gt;1&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_FALLBACK_URL&lt;/code&gt;: Fallback URL to use when a non-existing Lambda is invoked. Either records invocations in DynamoDB (value &lt;code&gt;dynamodb://&amp;lt;table_name&amp;gt;&lt;/code&gt;) or forwards invocations as a POST request (value &lt;code&gt;http(s)://...&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;EXTRA_CORS_ALLOWED_HEADERS&lt;/code&gt;: Comma-separated list of header names to be be added to &lt;code&gt;Access-Control-Allow-Headers&lt;/code&gt; CORS header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;EXTRA_CORS_EXPOSE_HEADERS&lt;/code&gt;: Comma-separated list of header names to be be added to &lt;code&gt;Access-Control-Expose-Headers&lt;/code&gt; CORS header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_JAVA_OPTS&lt;/code&gt;: Allow passing custom JVM options (e.g., &lt;code&gt;-Xmx512M&lt;/code&gt;) to Java Lambdas executed in Docker. Use &lt;code&gt;_debug_port_&lt;/code&gt; placeholder to configure the debug port (e.g., &lt;code&gt;-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=_debug_port_&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, the following &lt;em&gt;read-only&lt;/em&gt; environment variables are available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LOCALSTACK_HOSTNAME&lt;/code&gt;: Name of the host where LocalStack services are available.
This is needed in order to access the services from within your Lambda functions
(e.g., to store an item to DynamoDB or S3 from Lambda).
The variable &lt;code&gt;LOCALSTACK_HOSTNAME&lt;/code&gt; is available for both, local Lambda execution
(&lt;code&gt;LAMBDA_EXECUTOR=local&lt;/code&gt;) and execution inside separate Docker containers (&lt;code&gt;LAMBDA_EXECUTOR=docker&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-dynamically-updating-configuration-at-runtime" class="anchor" aria-hidden="true" href="#dynamically-updating-configuration-at-runtime"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dynamically updating configuration at runtime&lt;/h3&gt;
&lt;p&gt;Each of the service APIs listed &lt;a href="https://github.com/localstack/localstack#overview"&gt;above&lt;/a&gt; defines
a backdoor API under the path &lt;code&gt;/?_config_&lt;/code&gt; which allows to dynamically update configuration variables
defined in &lt;a href="https://github.com/localstack/localstack/blob/master/localstack/config.py"&gt;&lt;code&gt;config.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, to dynamically set &lt;code&gt;KINESIS_ERROR_PROBABILITY=1&lt;/code&gt; at runtime, use the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -v -d '{"variable":"KINESIS_ERROR_PROBABILITY","value":1}' 'http://localhost:4568/?_config_'
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-initializing-a-fresh-instance" class="anchor" aria-hidden="true" href="#initializing-a-fresh-instance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Initializing a fresh instance&lt;/h3&gt;
&lt;p&gt;When a container is started for the first time, it will execute files with extensions .sh that are found in &lt;code&gt;/docker-entrypoint-initaws.d&lt;/code&gt;. Files will be executed in alphabetical order. You can easily create aws resources on localstack using &lt;code&gt;awslocal&lt;/code&gt; (or &lt;code&gt;aws&lt;/code&gt;) cli tool in the initialization scripts.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-a-note-about-using-custom-ssl-certificates-for-use_ssl1" class="anchor" aria-hidden="true" href="#a-note-about-using-custom-ssl-certificates-for-use_ssl1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A note about using custom SSL certificates (for &lt;code&gt;USE_SSL=1&lt;/code&gt;)&lt;/h2&gt;
&lt;p&gt;If you need to use your own SSL Certificate and keep it persistent and not use the random automatic generated Certificate, you can place into the localstack temporary directory :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/tmp/localstack/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the three named files below :&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;server.test.pem
server.test.pem.crt
server.test.pem.key&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;the file &lt;code&gt;server.test.pem&lt;/code&gt; must contains your key file content, your certificate and chain certificate files contents (do a cat in this order)&lt;/li&gt;
&lt;li&gt;the file &lt;code&gt;server.test.pem.crt&lt;/code&gt; must contains your certificate and chains files contents (do a 'cat' in this order)&lt;/li&gt;
&lt;li&gt;the file server.test.pem.key must contains your key file content&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-using-use_ssl-and-own-persistent-certificate-with-docker-compose" class="anchor" aria-hidden="true" href="#using-use_ssl-and-own-persistent-certificate-with-docker-compose"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using USE_SSL and own persistent certificate with docker-compose&lt;/h3&gt;
&lt;p&gt;Typically with docker-compose you can add into docker-compose.yml this volume to the localstack services :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;volumes:
      - "${PWD}/ls_tmp:/tmp/localstack"
      - "/var/run/docker.sock:/var/run/docker.sock"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;local directory &lt;strong&gt;ls_tmp&lt;/strong&gt; must contains the three files (server.test.pem, server.test.pem.crt, server.test.pem.key)&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-accessing-the-infrastructure-via-cli-or-code" class="anchor" aria-hidden="true" href="#accessing-the-infrastructure-via-cli-or-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Accessing the infrastructure via CLI or code&lt;/h2&gt;
&lt;p&gt;You can point your &lt;code&gt;aws&lt;/code&gt; CLI to use the local infrastructure, for example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aws --endpoint-url=http://localhost:4568 kinesis list-streams
{
    "StreamNames": []
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NEW&lt;/strong&gt;: Check out &lt;a href="https://github.com/localstack/awscli-local"&gt;awslocal&lt;/a&gt;, a thin CLI wrapper
that runs commands directly against LocalStack (no need to specify &lt;code&gt;--endpoint-url&lt;/code&gt; anymore).
Install it via &lt;code&gt;pip install awscli-local&lt;/code&gt;, and then use it as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;awslocal kinesis list-streams
{
    "StreamNames": []
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: Use the environment variable &lt;code&gt;$LOCALSTACK_HOSTNAME&lt;/code&gt; to determine the target host
inside your Lambda function. See &lt;a href="#Configurations"&gt;Configurations&lt;/a&gt; section for more details.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-client-libraries" class="anchor" aria-hidden="true" href="#client-libraries"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Client Libraries&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python: &lt;a href="https://github.com/localstack/localstack-python-client"&gt;https://github.com/localstack/localstack-python-client&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;alternatively, you can also use &lt;code&gt;boto3&lt;/code&gt; and use the &lt;code&gt;endpoint_url&lt;/code&gt; parameter when creating a connection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(more coming soon...)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-integration-with-nosetests" class="anchor" aria-hidden="true" href="#integration-with-nosetests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Integration with nosetests&lt;/h2&gt;
&lt;p&gt;If you want to use LocalStack in your integration tests (e.g., nosetests), simply fire up the
infrastructure in your test setup method and then clean up everything in your teardown method:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from localstack.services import infra

def setup():
    infra.start_infra(asynchronous=True)

def teardown():
    infra.stop_infra()

def my_app_test():
    # here goes your test logic
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See the example test file &lt;code&gt;tests/integration/test_integration.py&lt;/code&gt; for more details.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-integration-with-serverless" class="anchor" aria-hidden="true" href="#integration-with-serverless"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Integration with Serverless&lt;/h2&gt;
&lt;p&gt;You can use the &lt;a href="https://www.npmjs.com/package/serverless-localstack" rel="nofollow"&gt;&lt;code&gt;serverless-localstack&lt;/code&gt;&lt;/a&gt; plugin to easily run &lt;a href="https://serverless.com/framework/" rel="nofollow"&gt;Serverless&lt;/a&gt; applications on LocalStack.
For more information, please check out the plugin repository here:
&lt;a href="https://github.com/localstack/serverless-localstack"&gt;https://github.com/localstack/serverless-localstack&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-local-code-with-lambda" class="anchor" aria-hidden="true" href="#using-local-code-with-lambda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using local code with Lambda&lt;/h2&gt;
&lt;p&gt;In order to mount a local folder, ensure that &lt;code&gt;LAMBDA_REMOTE_DOCKER&lt;/code&gt; is set to &lt;code&gt;false&lt;/code&gt; then set the S3 bucket name to &lt;code&gt;__local__&lt;/code&gt; and the S3 key to your local path:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    awslocal lambda create-function --function-name myLambda \
      --code S3Bucket="__local__",S3Key="/my/local/lambda/folder" \
      --handler index.myHandler \
      --runtime nodejs8.10 \
      --role whatever
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-integration-with-javajunit" class="anchor" aria-hidden="true" href="#integration-with-javajunit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Integration with Java/JUnit&lt;/h2&gt;
&lt;p&gt;In order to use LocalStack with Java, the project ships with a simple JUnit runner and a JUnit 5
extension. Take a look at the example JUnit tests in &lt;code&gt;ext/java&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;By default, the JUnit Test Runner starts LocalStack in a Docker container, for the duration of the test.
The container can be configured by using the &lt;code&gt;@LocalstackDockerProperties&lt;/code&gt; annotation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;...
import cloud.localstack.LocalstackTestRunner;
import cloud.localstack.TestUtils;
import cloud.localstack.docker.annotation.LocalstackDockerProperties;

@RunWith(LocalstackTestRunner.class)
@LocalstackDockerProperties(services = { "s3", "sqs", "kinesis:77077" })
public class MyCloudAppTest {

  @Test
  public void testLocalS3API() {
    AmazonS3 s3 = TestUtils.getClientS3()
    List&amp;lt;Bucket&amp;gt; buckets = s3.listBuckets();
    ...
  }

}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or with JUnit 5 :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@ExtendWith(LocalstackExtension.class)
@LocalstackDockerProperties(...)
public class MyCloudAppTest {
   ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The LocalStack JUnit test runner is published as an artifact in Maven Central.
Simply add the following dependency to your &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;cloud.localstack&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;localstack-utils&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.2.0&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can configure the Docker behaviour using the &lt;code&gt;@LocalstackDockerProperties&lt;/code&gt; annotation with the following parameters:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;property&lt;/th&gt;
&lt;th&gt;usage&lt;/th&gt;
&lt;th&gt;type&lt;/th&gt;
&lt;th&gt;default value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pullNewImage&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Determines if a new image is pulled from the docker repo before the tests are run.&lt;/td&gt;
&lt;td&gt;boolean&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;randomizePorts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Determines if the container should expose the default local stack ports (4567-4583) or if it should expose randomized ports.&lt;/td&gt;
&lt;td&gt;boolean&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;services&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Determines which services should be run when the localstack starts.&lt;/td&gt;
&lt;td&gt;String[]&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;imageTag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Use a specific image tag for docker container&lt;/td&gt;
&lt;td&gt;String&lt;/td&gt;
&lt;td&gt;&lt;code&gt;latest&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;hostNameResolver&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Used for determining the host name of the machine running the docker containers so that the containers can be addressed.&lt;/td&gt;
&lt;td&gt;IHostNameResolver&lt;/td&gt;
&lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;environmentVariableProvider&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Used for injecting environment variables into the container.&lt;/td&gt;
&lt;td&gt;IEnvironmentVariableProvider&lt;/td&gt;
&lt;td&gt;Empty Map&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;useSingleDockerContainer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Whether a singleton container should be used by all test classes.&lt;/td&gt;
&lt;td&gt;boolean&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Note: When specifying the port in the &lt;code&gt;services&lt;/code&gt; property, you cannot use &lt;code&gt;randomizePorts = true&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-troubleshooting" class="anchor" aria-hidden="true" href="#troubleshooting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Troubleshooting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you're using AWS Java libraries with Kinesis, please, refer to &lt;a href="https://github.com/mhart/kinesalite#cbor-protocol-issues-with-the-java-sdk"&gt;CBOR protocol issues with the Java SDK guide&lt;/a&gt; how to disable CBOR protocol which is not supported by kinesalite.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accessing local S3 from Java: To avoid domain name resolution issues, you need to enable &lt;strong&gt;path style access&lt;/strong&gt; on your client:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;s3.setS3ClientOptions(S3ClientOptions.builder().setPathStyleAccess(true).build());
// There is also an option to do this if you're using any of the client builder classes:
AmazonS3ClientBuilder builder = AmazonS3ClientBuilder.standard();
builder.withPathStyleAccessEnabled(true);
...
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Mounting the temp. directory: Note that on MacOS you may have to run &lt;code&gt;TMPDIR=/private$TMPDIR docker-compose up&lt;/code&gt; if
&lt;code&gt;$TMPDIR&lt;/code&gt; contains a symbolic link that cannot be mounted by Docker.
(See details here: &lt;a href="https://bitbucket.org/atlassian/localstack/issues/40/getting-mounts-failed-on-docker-compose-up" rel="nofollow"&gt;https://bitbucket.org/atlassian/localstack/issues/40/getting-mounts-failed-on-docker-compose-up&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you run into file permission issues on &lt;code&gt;pip install&lt;/code&gt; under Mac OS (e.g., &lt;code&gt;Permission denied: '/Library/Python/2.7/site-packages/six.py'&lt;/code&gt;), then you may have to re-install &lt;code&gt;pip&lt;/code&gt; via Homebrew (see &lt;a href="https://github.com/localstack/localstack/issues/260#issuecomment-334458631"&gt;this discussion thread&lt;/a&gt;). Alternatively, try installing
with the &lt;code&gt;--user&lt;/code&gt; flag: &lt;code&gt;pip install --user localstack&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you are deploying within OpenShift, please be aware: the pod must run as &lt;code&gt;root&lt;/code&gt;, and the user must have capabilities added to the running pod, in order to allow Elasticsearch to be run as the non-root &lt;code&gt;localstack&lt;/code&gt; user.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The environment variable &lt;code&gt;no_proxy&lt;/code&gt; is rewritten by LocalStack.
(Internal requests will go straight via localhost, bypassing any proxy configuration).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For troubleshooting LocalStack start issues, you can check debug logs by running &lt;code&gt;DEBUG=1 localstack start&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In case you get errors related to node/nodejs, you may find (this issue comment: &lt;a href="https://github.com/localstack/localstack/issues/227#issuecomment-319938530"&gt;https://github.com/localstack/localstack/issues/227#issuecomment-319938530&lt;/a&gt;) helpful.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you are using AWS Java libraries and need to disable SSL certificate checking, add &lt;code&gt;-Dcom.amazonaws.sdk.disableCertChecking&lt;/code&gt; to the java invocation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-developing" class="anchor" aria-hidden="true" href="#developing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Developing&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-requirements-for-developing-or-starting-locally" class="anchor" aria-hidden="true" href="#requirements-for-developing-or-starting-locally"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements for developing or starting locally&lt;/h3&gt;
&lt;p&gt;To develop new features, or to start the stack locally (outside of Docker), the following additional tools are required:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;make&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;npm&lt;/code&gt; (node.js package manager)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;java&lt;/code&gt;/&lt;code&gt;javac&lt;/code&gt; (Java 8 runtime environment and compiler)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mvn&lt;/code&gt; (Maven, the build system for Java)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-development-environment" class="anchor" aria-hidden="true" href="#development-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development Environment&lt;/h3&gt;
&lt;p&gt;If you pull the repo in order to extend/modify LocalStack, run this command to install
all the dependencies:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will install the required pip dependencies in a local Python virtualenv directory
&lt;code&gt;.venv&lt;/code&gt; (your global python packages will remain untouched), as well as some node modules
in &lt;code&gt;./localstack/node_modules/&lt;/code&gt;. Depending on your system, some pip/npm modules may require
additional native libs installed.&lt;/p&gt;
&lt;p&gt;The Makefile contains a target to conveniently run the local infrastructure for development:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make infra
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out the
&lt;a href="https://github.com/localstack/localstack/tree/master/doc/developer_guides"&gt;developer guide&lt;/a&gt; which
contains a few instructions on how to get started with developing (and debugging) features for
LocalStack.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h2&gt;
&lt;p&gt;The project contains a set of unit and integration tests that can be kicked off via a make
target:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make test
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-web-dashboard" class="anchor" aria-hidden="true" href="#web-dashboard"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web Dashboard&lt;/h2&gt;
&lt;p&gt;The projects also comes with a simple Web dashboard that allows to view the deployed AWS
components and the relationship between them.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localstack web
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-other-ui-clients" class="anchor" aria-hidden="true" href="#other-ui-clients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other UI Clients&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://getcommandeer.com" rel="nofollow"&gt;Commandeer desktop app&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.npmjs.com/package/dynamodb-admin" rel="nofollow"&gt;DynamoDB Admin Web UI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-change-log" class="anchor" aria-hidden="true" href="#change-log"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Change Log&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;v0.10.6: Fix MD5 hash of message attributes on SQS-&amp;gt;Lambda integration; add &lt;code&gt;LAMBDA_REMOVE_CONTAINERS&lt;/code&gt; config; bump moto to latest version; fix LocationConstraint markup in S3 responses; fix success_action_status for S3 POST Object; add UUIDs to CW events; update SSL certificate to new requirements in MacOS Catalina; fix Docker port mapping for JUnit test runner; add test for CW logs multi-byte message; fix S3 bucket name validation, auto-convert uppercase characters in bucket names; fix CF deployment of step functions; fix DOCKER_HOST_FROM_CONTAINER config for local exec; fix object ACLs for multipart uploads; add initial support for Kinesis stream consumers; enhance proxy to transparently accept both HTTP/HTTPS on the same port; support TemplateURL for CF ValidateTemplate; support Marker for S3 ListObjects; fix passing of metadata on S3 presigned URL put; allow trailing slashes in Elasticsearch API; refactor Java libs, make Docker the default JUnit executor; update ElasticMQ version; add initial support for KMS via &lt;code&gt;local-kms&lt;/code&gt;; fix Terraform deployment for Lambdas; fix stack name in CloudFormation resource names; fix S3 bucket name checks for domain based addressing; add locking for SSL cert creation; support JARs in lib/ folder for Java lambdas; disable SSL verification for CF template URLs; add S3 website ErrorDocument emulation; fix return type of Lambda GetPolicy; fix API Gateway authorizer implementation; fix permission issue for cert files; fix non-JSON content types for API Gateway; fix DynamoDB error for Put on non-existing table; fix notification triggers on S3 presigned URL upload; add CloudFormation support for deployment of SAM resources; fix SNS FilterPolicy configuration; add kinesis/ListStreams API Gateway integration&lt;/li&gt;
&lt;li&gt;v0.10.5: Various CloudFormation fixes: deployment of API GW method integrations, properly skip resource updates, Lambda SQS event source mapping, avoid duplicate resource creation, support for ApiGateway::GatewayResponse and Events::Rule, log groups for Lambdas; support adding Lambda policies; customize Docker registry for Lambda images; support multiple configurations in S3 notifications; fix encoding of non-ASCII results from API Gateway; allow docker-reuse to use mounted volumes; support presigned S3 URL upload notifications; fix lookup of Python Lambda handler in sub directories; upgrade kinesalite; fix duplicate CORS headers; fix mapping of Lambda versions and ARNs; fix SNS x-amz-sns-message-type header; send SNS confirmation message for HTTP(S) subscriptions; fix DynamoDB local libs for Docker Alpine; add CF support for SNS subscriptions; fix RecordId for firehose put-record-batch; fix SQS messages with multi-byte characters; avoid creating multiple SNS subscriptions; add .bat script and support running under Windows; fix S3 location constraint for CF&lt;/li&gt;
&lt;li&gt;v0.10.4: Add checks for open UDP ports; fix S3 chunked encoding uploads; fix LatestStreamLabel; fix CORS headers for SQS/SNS; set Java lambda debug port only when needed; expose default region in a util function; fix MacOS tmp folder; clear tmp supervisord logs at container startup; fix signed header requests for S3; expose Web UI via HTTPS; add Timestamp to SNS messages; fix attributes for SQS queues addressed via URL&lt;/li&gt;
&lt;li&gt;v0.10.3: Allow specifying data types for CF attributes; add API for service status and starting services at runtime; support NextShardIterator in DDB streams; add mock responses for S3 encryption and replication; fix rendering of resources in web UI; custom SQS queue attributes; fix Lambda docker command and imports; fix SQS queue physical ID in CF; allow proxy listener to define custom backend per request; support Lambda event body over stdin; exclude &lt;code&gt;ingest-geoip&lt;/code&gt; ES module to optimize image size; skip checking MD5 on S3 copy; fix DynamoDB table ARN for CF; fix CF deployment of StepFunction activities; fix uploading of Java Lambda as JAR in ZIP; fix installing libs for plugins; added &lt;code&gt;LAMBDA_JAVA_OPTS&lt;/code&gt; for Java Lambda debugging; bump Maven dependency versions; refactor Lambda API; fix boolean strings in CF templates; allow overriding AWS account id with &lt;code&gt;TEST_AWS_ACCOUNT_ID&lt;/code&gt;; fix incorrect region for API GW resources created via CF; fix permissions for cache files in &lt;code&gt;/tmp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;v0.10.2: Fix logging issue with async Lambdas; fix kinesis records processing; add basic support for &lt;code&gt;Ref&lt;/code&gt; in CloudFormation; fix ddb streams uuid generation; upgrade travis CI setup; fix DynamoDB error messages; cache server processes&lt;/li&gt;
&lt;li&gt;v0.10.0: Lazy loading of libraries; fix handling of regions; add API multiserver; improve CPU profiling; fix ES xpack installation; add basic EventBridge support; refactor Lambda API and executor; add MessageAttributes on SNS payloads; tagging for SNS; ability to customize docker command&lt;/li&gt;
&lt;li&gt;v0.9.6: Add API Gateway SQS proxy; fix command to push Docker image; fix Docker bridge IP configuration; fix SSL issue in dashboard infra; updates to README&lt;/li&gt;
&lt;li&gt;v0.9.5: Reduce Docker image size by squashing; fix response body for presigned URL S3 PUT requests; fix CreateDate returned by IAM; fix account IDs for CF and SNS; fix topic checks for SMS using SNS; improve documentation around &lt;code&gt;@LocalstackDockerProperties&lt;/code&gt;; add basic EC2 support; upgrade to ElasticSearch 6.7; set Last-Modified header in S3; preserve logic with uppercase event keys in Java; add support for nodejs 10.x Lambdas&lt;/li&gt;
&lt;li&gt;v0.9.4: Fix ARNs in CloudFormation deployments; write stderr to file in supervisord; fix Lambda invocation times; fix canonicalization of service names when running in Docker; add support for &lt;code&gt;@Nested&lt;/code&gt; in Junit5; add support for batch/transaction in DynamoDB; fix output buffering for subprocesses; assign unique ports under docker-reuse; check if topic ARN exists before publish&lt;/li&gt;
&lt;li&gt;v0.9.3: Fix output buffering of child processes; new release of Java libs; add imageTag attribute for Java annotation&lt;/li&gt;
&lt;li&gt;v0.9.2: Update to Python 3 in Dockerfile; preserve attributes when SNS Subscribe; fix event source mapping in Lambda; fix CORS ExposeHeaders; set Lambda timeout in secs; add tags support for Lambda/Firehose; add message attributes for SQS/Lambda; fix shard count support for Kinesis; fix port mappings for CloudFormation&lt;/li&gt;
&lt;li&gt;v0.9.1: Define dependent and composite services in config; forward Lambda logs to CloudWatch Logs; add SQS event deserializing for Lambda; fix AWS_PROXY for JSON list payload; add START_WEB config parameter; return correct location for S3 multipart uploads; add support for Lambda custom runtime; fix account ID for IAM responses; fix using correct SSL cert; limit memory usage for Java processes; fix unicode encoding for SNS messages; allow using &lt;code&gt;LOCALSTACK_&lt;/code&gt; prefix in Docker environment variables; enable request forwarding for non-existing Lambdas; fix large downloads for S3; add API endpoint for dynamically updating config variables; fix CloudFormation stack update&lt;/li&gt;
&lt;li&gt;v0.9.0: Enhance integration with Serverless; refactor CloudFormation implementation; add support for Step Functions, IAM, STS; fix CloudFormation integration; support mounting Lambda code locally; add &lt;code&gt;docker-entrypoint-initaws.d&lt;/code&gt; dir for initializing resources; add S3Event Parser for Lambda; fix S3 chunk encoding; fix S3 multipart upload notification; add dotnetcore2.1 and ruby2.5 Lambda runtimes; fix issues with JDK 9; install ES plugins available in AWS&lt;/li&gt;
&lt;li&gt;v0.8.10: Add kclpy to pip package; fix badges in README&lt;/li&gt;
&lt;li&gt;v0.8.9: Replace moto-ext with upstream moto; fix SNS message attributes; fix swagger; make external SQS port configurable; support for SNS DeleteTopic; S3 notifications for multipart uploads; support requestContext in AWS_PROXY integration; update docs for SSL usage&lt;/li&gt;
&lt;li&gt;v0.8.8: Support Docker network config for Lambda containers; support queryStringParameters for Lambda AWS_PROXY apigateway; add AWS SecretsManager service; add SQS/Lambda integration; add support for Firehose Kinesis source; add GetAlias to Lambda API; add function properties to LambdaContext for invocations; fix extraction of Java Lambda archives; check region headers for SNS; fix Lambda output buffering; fix S3 download of gzip; bump ElasticMQ to 0.14.5; fix Lambda response codes; fix syntax issues for Python 3.7&lt;/li&gt;
&lt;li&gt;v0.8.7: Support .Net Core 2.0 and nodejs8.10 Lambdas; refactor Java libs and integrate with JUnit 5; support tags for ES domains; add CloudFormation support for SNS topics; fix kinesis error injection; fix override of &lt;code&gt;ES_JAVA_OPTS&lt;/code&gt;; fix SQS CORS preflight response; fix S3 content md5 checks and Host header; fix ES startup issue; Bump elasticmq to 0.13.10; bump kinesalite version&lt;/li&gt;
&lt;li&gt;v0.8.6: Fixes for Windows installation; bump ES to 6.2.0; support filter policy for SNS; upgrade kinesalite; refactor JUnit runner; support Lambda PutFunctionConcurrency and GetEventSourceMapping; fixes for Terraform; add golang support to Lambda; fix file permission issue in Java Lambda tests; fix S3 bucket notification config&lt;/li&gt;
&lt;li&gt;v0.8.5: Fix DDB streams event type; implement CF Fn::GetAZs; async lambda for DDB events; fix S3 content-type; fix CF deployer for SQS; fix S3 ExposePorts; fix message subject in SNS; support for Firehose -&amp;gt; ES; pass external env vars to containers from Java; add mock for list-queue-tags; enhance docker test runner; fix Windows installation issues; new version of Java libs&lt;/li&gt;
&lt;li&gt;v0.8.4: Fix &lt;code&gt;pipenv&lt;/code&gt; dependency issue; Docker JUnit test runner; POJO type for Java Lambda RequestHandler; Java Lambda DynamoDB event; reuse Docker containers for Lambda invocations; API Gateway wildcard path segments; fix SNS RawMessageDelivery&lt;/li&gt;
&lt;li&gt;v0.8.3: Fix DDB stream events for UPDATE operations; fix DDB streams sequence numbers; fix transfer-encoding for DDB; fix requests with missing content-length header; support non-ascii content in DynamoDB items; map external port for SQS queue URLs; default to LAMBDA_REMOTE_DOCKER=true if running in Docker; S3 lifecycle support; reduce Docker image size&lt;/li&gt;
&lt;li&gt;v0.8.2: Fix S3 bucket notification configuration; CORS headers for API Gateway; fix &amp;gt;128k S3 multipart uploads; return valid ShardIDs in DynamoDB Streams; fix hardcoded "ddblocal" DynamoDB TableARN; import default service ports from localstack-client; fix S3 bucket policy response; Execute lambdas asynchronously if the source is a topic&lt;/li&gt;
&lt;li&gt;v0.8.1: Improvements in Lambda API: publish-version, list-version, function aliases; use single map with Lambda function details; workaround for SQS .fifo queues; add test for S3 upload; initial support for SSM; fix regex to replace SQS queue URL hostnames; update linter (single quotes); use &lt;code&gt;docker.for.mac.localhost&lt;/code&gt; to connect to LocalStack from Docker on Mac; fix b64 encoding for Java Lambdas; fix path of moto_server command&lt;/li&gt;
&lt;li&gt;v0.8.0: Fix request data in &lt;code&gt;GenericProxyHandler&lt;/code&gt;; add &lt;code&gt;$PORT_WEB_UI&lt;/code&gt; and &lt;code&gt;$HOSTNAME_EXTERNAL&lt;/code&gt; configs; API Gateway path parameters; enable flake8 linting; add config for service backend URLs; use ElasticMQ instead of moto for SQS; expose &lt;code&gt;$LOCALSTACK_HOSTNAME&lt;/code&gt;; custom environment variable support for Lambda; improve error logging and installation for Java/JUnit; add support for S3 REST Object POST&lt;/li&gt;
&lt;li&gt;v0.7.5: Fix issue with incomplete parallel downloads; bypass http_proxy for internal requests; use native Python code to unzip archives; download KCL client libs only for testing and not on pip install&lt;/li&gt;
&lt;li&gt;v0.7.4: Refactor CLI and enable plugins; support unicode names for S3; fix SQS names containing a dot character; execute Java Lambda functions in Docker containers; fix DynamoDB error handling; update docs&lt;/li&gt;
&lt;li&gt;v0.7.3: Extract proxy listeners into (sub-)classes; put java libs into a single "fat" jar; fix issue with non-daemonized threads; refactor code to start flask services&lt;/li&gt;
&lt;li&gt;v0.7.2: Fix DATA_DIR config when running in Docker; fix Maven dependencies; return 'ConsumedCapacity' from DynamoDB get-item; use Queue ARN instead of URL for S3 bucket notifications&lt;/li&gt;
&lt;li&gt;v0.7.1: Fix S3 API to GET bucket notifications; release Java artifacts to Maven Central; fix S3 file access from Spark; create DDB stream on UpdateTable; remove AUI dependency, optimize size of Docker image&lt;/li&gt;
&lt;li&gt;v0.7.0: Support for Kinesis in CloudFormation; extend and integrate Java tests in CI; publish Docker image under new name; update READMEs and license agreements&lt;/li&gt;
&lt;li&gt;v0.6.2: Major refactoring of installation process, lazy loading of dependencies&lt;/li&gt;
&lt;li&gt;v0.6.1: Add CORS headers; platform compatibility fixes (remove shell commands and sh module); add CloudFormation validate-template; fix Lambda execution in Docker; basic domain handling in ES API; API Gateway authorizers&lt;/li&gt;
&lt;li&gt;v0.6.0: Load services as plugins; fix service default ports; fix SQS-&amp;gt;SNS and MD5 of message attributes; fix Host header for S3&lt;/li&gt;
&lt;li&gt;v0.5.5: Enable SSL encryption for all service endpoints (&lt;code&gt;USE_SSL&lt;/code&gt; config); create Docker base image; fix issue with DATA_DIR&lt;/li&gt;
&lt;li&gt;v0.5.4: Remove hardcoded /tmp/ for Windows-compat.; update CLI and docs; fix S3/SNS notifications; disable Elasticsearch compression&lt;/li&gt;
&lt;li&gt;v0.5.3: Add CloudFormation support for serverless / API Gateway deployments; fix installation via pypi; minor fix for Java (passing of environment variables)&lt;/li&gt;
&lt;li&gt;v0.5.0: Extend DynamoDB Streams API; fix keep-alive connection for S3; fix deadlock in nested Lambda executions; add integration SNS-&amp;gt;Lambda; CloudFormation serverless example; replace dynalite with DynamoDBLocal; support Lambda execution in remote Docker container; fix CloudWatch metrics for Lambda invocation errors&lt;/li&gt;
&lt;li&gt;v0.4.3: Initial support for CloudWatch metrics (for Lambda functions); HTTP forwards for API Gateway; fix S3 message body signatures; download Lambda archive from S3 bucket; fix/extend ES tests&lt;/li&gt;
&lt;li&gt;v0.4.2: Initial support for Java Lambda functions; CloudFormation deployments; API Gateway tests&lt;/li&gt;
&lt;li&gt;v0.4.1: Python 3 compatibility; data persistence; add seq. numbers in Kinesis events; limit Elasticsearch memory&lt;/li&gt;
&lt;li&gt;v0.4.0: Execute Lambda functions in Docker containers; CORS headers for S3&lt;/li&gt;
&lt;li&gt;v0.3.11: Add Route53, SES, CloudFormation; DynamoDB fault injection; UI tweaks; refactor config&lt;/li&gt;
&lt;li&gt;v0.3.10: Add initial support for S3 bucket notifications; fix subprocess32 installation&lt;/li&gt;
&lt;li&gt;v0.3.9: Make services/ports configurable via $SERVICES; add tests for Firehose+S3&lt;/li&gt;
&lt;li&gt;v0.3.8: Fix Elasticsearch via local bind and proxy; refactoring; improve error logging&lt;/li&gt;
&lt;li&gt;v0.3.5: Fix lambda handler name; fix host name for S3 API; install web libs on pip install&lt;/li&gt;
&lt;li&gt;v0.3.4: Fix file permissions in build; fix and add UI to Docker image; add stub of ES API&lt;/li&gt;
&lt;li&gt;v0.3.3: Add version tags to Docker images&lt;/li&gt;
&lt;li&gt;v0.3.2: Add support for Redshift API; code refactoring&lt;/li&gt;
&lt;li&gt;v0.3.1: Add Dockerfile and push image to Docker Hub&lt;/li&gt;
&lt;li&gt;v0.3.0: Add simple integration for JUnit; improve process signal handling&lt;/li&gt;
&lt;li&gt;v0.2.11: Refactored the AWS assume role function&lt;/li&gt;
&lt;li&gt;v0.2.10: Added AWS assume role functionality.&lt;/li&gt;
&lt;li&gt;v0.2.9: Kinesis error response formatting&lt;/li&gt;
&lt;li&gt;v0.2.7: Throw Kinesis errors randomly&lt;/li&gt;
&lt;li&gt;v0.2.6: Decouple SNS/SQS: intercept SNS calls and forward to subscribed SQS queues&lt;/li&gt;
&lt;li&gt;v0.2.5: Return error response from Kinesis if flag is set&lt;/li&gt;
&lt;li&gt;v0.2.4: Allow Lambdas to use &lt;strong&gt;file&lt;/strong&gt; (import from file instead of exec'ing)&lt;/li&gt;
&lt;li&gt;v0.2.3: Improve Kinesis/KCL auto-checkpointing (leases in DDB)&lt;/li&gt;
&lt;li&gt;v0.2.0: Speed up installation time by lazy loading libraries&lt;/li&gt;
&lt;li&gt;v0.1.19: Pass shard_id in records sent from KCL process&lt;/li&gt;
&lt;li&gt;v0.1.16: Minor restructuring and refactoring (create separate kinesis_util.py)&lt;/li&gt;
&lt;li&gt;v0.1.14: Fix AWS tokens when creating Elasticsearch client&lt;/li&gt;
&lt;li&gt;v0.1.11: Add startup/initialization notification for KCL process&lt;/li&gt;
&lt;li&gt;v0.1.10: Bump version of amazon_kclpy to 1.4.1&lt;/li&gt;
&lt;li&gt;v0.1.9: Add initial support for SQS/SNS&lt;/li&gt;
&lt;li&gt;v0.1.8: Fix installation of JARs in amazon_kclpy if localstack is installed transitively&lt;/li&gt;
&lt;li&gt;v0.1.7: Bump version of amazon_kclpy to 1.4.0&lt;/li&gt;
&lt;li&gt;v0.1.6: Add travis-ci and coveralls configuration&lt;/li&gt;
&lt;li&gt;v0.1.5: Refactor Elasticsearch utils; fix bug in method to delete all ES indexes&lt;/li&gt;
&lt;li&gt;v0.1.4: Enhance logging; extend java KCL credentials provider (support STS assumed roles)&lt;/li&gt;
&lt;li&gt;v0.1.2: Add configurable KCL log output&lt;/li&gt;
&lt;li&gt;v0.1.0: Initial release&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We welcome feedback, bug reports, and pull requests!&lt;/p&gt;
&lt;p&gt;For pull requests, please stick to the following guidelines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add tests for any new features and bug fixes. Ideally, each PR should increase the test coverage.&lt;/li&gt;
&lt;li&gt;Follow the existing code style (e.g., indents). A PEP8 code linting target is included in the Makefile.&lt;/li&gt;
&lt;li&gt;Put a reasonable amount of comments into the code.&lt;/li&gt;
&lt;li&gt;Separate unrelated changes into multiple pull requests.&lt;/li&gt;
&lt;li&gt;1 commit per PR: Please squash/rebase multiple commits into one single commit (to keep the history clean).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please note that by contributing any code or documentation to this repository (by
raising pull requests, or otherwise) you explicitly agree to
the &lt;a href="doc/contributor_license_agreement"&gt;&lt;strong&gt;Contributor License Agreement&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;This project exists thanks to all the people who contribute.
&lt;a href="https://github.com/localstack/localstack/graphs/contributors"&gt;&lt;img src="https://camo.githubusercontent.com/70c617534022062f30a0679c0c2422a23ec605ee/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f636f6e7472696275746f72732e7376673f77696474683d383930" data-canonical-src="https://opencollective.com/localstack/contributors.svg?width=890" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-backers" class="anchor" aria-hidden="true" href="#backers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Backers&lt;/h2&gt;
&lt;p&gt;Thank you to all our backers! &lt;g-emoji class="g-emoji" alias="pray" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64f.png"&gt;🙏&lt;/g-emoji&gt; [&lt;a href="https://opencollective.com/localstack#backer" rel="nofollow"&gt;Become a backer&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://opencollective.com/localstack#backers" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/baded82797613a9dfee852dc9b83e810dbb99e07/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f6261636b6572732e7376673f77696474683d383930" data-canonical-src="https://opencollective.com/localstack/backers.svg?width=890" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sponsors" class="anchor" aria-hidden="true" href="#sponsors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sponsors&lt;/h2&gt;
&lt;p&gt;Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [&lt;a href="https://opencollective.com/localstack#sponsor" rel="nofollow"&gt;Become a sponsor&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://opencollective.com/localstack/sponsor/0/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5447958470aa26c2d09b668bf4b8e3946f94908f/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f302f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/0/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/1/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0fb41b881ff9a6f2545bf210a1b5eeefe1c2f9e0/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f312f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/1/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/2/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/105ab894cafcccc622161e8a0f070bb2e1edc38f/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f322f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/2/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/3/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/12dcc5d9654145e5609701ac986ced8a34155fd9/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f332f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/3/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/4/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/99c081e4fa906ad24a9ba4a430ce1b8f6b47f9b2/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f342f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/4/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/5/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/024967ca75aecc89725c4be25d8951eb637cc30b/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f352f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/5/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/6/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2b266eeed387c32eb4e2c42dd5e5d005967c4024/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f362f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/6/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/7/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c677209769bb3f28660b6ebd0c87b2e2be8d6103/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f372f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/7/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/8/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/902262e1aeccb957ef75bcbf00de78bd0ba60fb6/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f382f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/8/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/9/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/574b420b1fe419c38dd36d0c0a2a2d78031a3af5/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f392f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/9/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-stargazers-over-time" class="anchor" aria-hidden="true" href="#stargazers-over-time"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stargazers over time&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://starchart.cc/localstack/localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9d274070627cd092149bd6ed66d1e8401cd966fa/68747470733a2f2f7374617263686172742e63632f6c6f63616c737461636b2f6c6f63616c737461636b2e737667" alt="Stargazers over time" data-canonical-src="https://starchart.cc/localstack/localstack.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright (c) 2017-2019 LocalStack maintainers and contributors.&lt;/p&gt;
&lt;p&gt;Copyright (c) 2016 Atlassian and others.&lt;/p&gt;
&lt;p&gt;This version of LocalStack is released under the Apache License, Version 2.0 (see LICENSE.txt).
By downloading and using this software you agree to the
&lt;a href="doc/end_user_license_agreement"&gt;End-User License Agreement (EULA)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We build on a number of third-party software tools, including the following:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Third-Party software&lt;/th&gt;
&lt;th&gt;License&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Python/pip modules:&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;airspeed&lt;/td&gt;
&lt;td&gt;BSD License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;amazon_kclpy&lt;/td&gt;
&lt;td&gt;Amazon Software License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;boto3&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;coverage&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;docopt&lt;/td&gt;
&lt;td&gt;MIT License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;elasticsearch&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;flask&lt;/td&gt;
&lt;td&gt;BSD License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;flask_swagger&lt;/td&gt;
&lt;td&gt;MIT License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;jsonpath-rw&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;moto&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;requests&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;subprocess32&lt;/td&gt;
&lt;td&gt;PSF License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Node.js/npm modules:&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kinesalite&lt;/td&gt;
&lt;td&gt;MIT License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Other tools:&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Elasticsearch&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;local-kms&lt;/td&gt;
&lt;td&gt;MIT License&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>localstack</author><guid isPermaLink="false">https://github.com/localstack/localstack</guid><pubDate>Sat, 04 Jan 2020 00:10:00 GMT</pubDate></item><item><title>DavidBuchanan314/dlinject #11 in Python, Today</title><link>https://github.com/DavidBuchanan314/dlinject</link><description>&lt;p&gt;&lt;i&gt;Inject a shared library (i.e. arbitrary code) into a live linux process, without ptrace&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-dlinjectpy" class="anchor" aria-hidden="true" href="#dlinjectpy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;dlinject.py&lt;/h1&gt;
&lt;p&gt;Inject a shared library (i.e. arbitrary code) into a live linux process, without ptrace. Inspired by &lt;a href="https://github.com/AonCyberLabs/Cexigua"&gt;Cexigua&lt;/a&gt; and &lt;a href="https://github.com/gaffe23/linux-inject"&gt;linux-inject&lt;/a&gt;, among other things.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://asciinema.org/a/290906" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/bdd555dcf0b5e77912c74cda1eb5f7ca23075e0b/68747470733a2f2f61736369696e656d612e6f72672f612f3239303930362e737667" alt="asciicast" data-canonical-src="https://asciinema.org/a/290906.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h1&gt;
&lt;p&gt;Note: currently requires the python3 branch of &lt;a href="https://github.com/Gallopsled/pwntools"&gt;pwntools&lt;/a&gt;. It's a fairly heavyweight dependency that I only use a few features of, so I might be able to remove it in the future.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    .___.__  .__            __               __
  __| _/|  | |__| ____     |__| ____   _____/  |_  ______ ___.__.
 / __ | |  | |  |/    \    |  |/ __ \_/ ___\   __\ \____ &amp;lt;   |  |
/ /_/ | |  |_|  |   |  \   |  \  ___/\  \___|  |   |  |_&amp;gt; &amp;gt;___  |
\____ | |____/__|___|  /\__|  |\___  &amp;gt;\___  &amp;gt;__| /\|   __// ____|
     \/              \/\______|    \/     \/     \/|__|   \/

source: https://github.com/DavidBuchanan314/dlinject

usage: dlinject.py [-h] [--stopmethod {sigstop,cgroup_freeze,none}]
                   pid /path/to/lib.so

Inject a shared library into a live process.

positional arguments:
  pid                   The pid of the target process
  /path/to/lib.so       Path of the shared library to load (note: must be
                        relative to the target process's cwd, or absolute)

optional arguments:
  -h, --help            show this help message and exit
  --stopmethod {sigstop,cgroup_freeze,none}
                        How to stop the target process prior to shellcode
                        injection. SIGSTOP (default) can have side-effects.
                        cgroup freeze requires root. 'none' is likely to cause
                        race conditions.

&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-why" class="anchor" aria-hidden="true" href="#why"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Because I can.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are various &lt;a href="https://www.aldeid.com/wiki/Ptrace-anti-debugging" rel="nofollow"&gt;anti-ptrace techniques&lt;/a&gt;, which this evades by simply not using ptrace.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I don't like ptrace.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using &lt;code&gt;LD_PRELOAD&lt;/code&gt; can sometimes be fiddly or impossible, if the process you want to inject into is spawned by another process with a clean environment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-how-it-works" class="anchor" aria-hidden="true" href="#how-it-works"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How it Works&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Send the stop signal to the target process. (optional)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Locate the &lt;code&gt;_dl_open()&lt;/code&gt; symbol.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Retreive &lt;code&gt;RIP&lt;/code&gt; and &lt;code&gt;RSP&lt;/code&gt; via &lt;code&gt;/proc/[pid]/syscall&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a backup of part of the stack, and the code we're about to overwrite with our shellcode, by reading from &lt;code&gt;/proc/[pid]/mem&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generate primary and secondary shellcode buffers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Insert primary shellcode at &lt;code&gt;RIP&lt;/code&gt;, by writing to &lt;code&gt;/proc/[pid]/mem&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The primary shellcode:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pushes common registers to the stack.&lt;/li&gt;
&lt;li&gt;Loads the secondary shellcode via &lt;code&gt;mmap()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Jumps to the secondary shellcode.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The secondary shellcode:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Restores the stack and program code to their original states.&lt;/li&gt;
&lt;li&gt;Pivots the stack (so we don't touch the original one at all).&lt;/li&gt;
&lt;li&gt;Calls &lt;code&gt;_dl_open()&lt;/code&gt; to load the user-specified library. Any constructors will be executed on load, as usual.&lt;/li&gt;
&lt;li&gt;Restores register state, un-pivots the stack, and jumps back to where it was at the time of the original &lt;code&gt;SIGSTOP&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-limitations" class="anchor" aria-hidden="true" href="#limitations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Limitations:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sending &lt;code&gt;SIGSTOP&lt;/code&gt; may cause unwanted side-effects, for example if another thread is waiting on &lt;code&gt;waitpid()&lt;/code&gt;. The &lt;code&gt;--stopmethod=cgroup_freeze&lt;/code&gt; option avoids this, but requires root (on most distros, at least).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I'm not entirely sure how this will interact with complex multi-threaded applications. There's certainly potential for breakage.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;x86-64&lt;/code&gt; Linux only (for now - 32-bit support could potentially be added).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Requires root, or relaxed YAMA configuration (&lt;code&gt;echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope&lt;/code&gt; is useful when testing).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the target process is sandboxed (e.g. seccomp filters), it might not have permission to &lt;code&gt;mmap()&lt;/code&gt; the second stage shellcode, or to &lt;code&gt;dlopen()&lt;/code&gt; the library.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>DavidBuchanan314</author><guid isPermaLink="false">https://github.com/DavidBuchanan314/dlinject</guid><pubDate>Sat, 04 Jan 2020 00:11:00 GMT</pubDate></item><item><title>alexey-goloburdin/telegram-finance-bot #12 in Python, Today</title><link>https://github.com/alexey-goloburdin/telegram-finance-bot</link><description>&lt;p&gt;&lt;i&gt;Телеграм бот для учёта личных расходов и ведения бюджета&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;Telegram бот для учёта личных расходов и ведения бюджета, &lt;a href="https://www.youtube.com/watch?v=Kh16iosOTIQ" rel="nofollow"&gt;видео с пояснениями по коду и описание&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;В переменных окружения надо проставить API токен бота, а также адрес proxy и логин-пароль к ней.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TELEGRAM_API_TOKEN&lt;/code&gt; — API токен бота&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TELEGRAM_PROXY_URL&lt;/code&gt; — URL прокси сервера&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TELEGRAM_PROXY_LOGIN&lt;/code&gt; — логин прокси сервера&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TELEGRAM_PROXY_PASSWORD&lt;/code&gt; — пароль прокси сервера&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TELEGRAM_ACCESS_ID&lt;/code&gt; — ID Telegram аккаунта, от которого будут приниматься сообщения (сообщения от остальных аккаунтов игнорируются)&lt;/p&gt;
&lt;p&gt;Использование с Docker показано ниже. Предварительно заполните ENV переменные, указанные выше, в Dockerfile, а также в команде запуска укажите локальную директорию с проектом вместо &lt;code&gt;local_project_path&lt;/code&gt;. SQLite база данных будет лежать в папке проекта &lt;code&gt;db/finance.db&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build -t tgfinance ./
docker run -d --name tg -v /local_project_path/db:/home/db tgfinance
docker run -ti --name tg tgfinance
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>alexey-goloburdin</author><guid isPermaLink="false">https://github.com/alexey-goloburdin/telegram-finance-bot</guid><pubDate>Sat, 04 Jan 2020 00:12:00 GMT</pubDate></item><item><title>TheKingOfDuck/fuzzDicts #13 in Python, Today</title><link>https://github.com/TheKingOfDuck/fuzzDicts</link><description>&lt;p&gt;&lt;i&gt;Web Pentesting Fuzz 字典,一个就够了。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-fuzzdicts" class="anchor" aria-hidden="true" href="#fuzzdicts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;fuzzDicts&lt;/h1&gt;
&lt;p&gt;Web Pentesting Fuzz 字典,一个就够了。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-log" class="anchor" aria-hidden="true" href="#log"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;log&lt;/h2&gt;
&lt;p&gt;不定期更新，使用前建议git pull一下，同步更新。&lt;/p&gt;
&lt;p&gt;20191219:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用正则&lt;code&gt;(\W)&lt;/code&gt;过滤了很多无效的参数,如空格(){}等等,并允许-的存在，重新合并去重了一下参数字典，均放在AllParam.txt，感谢奶权师傅的反馈。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;20191214:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最近在整理各CMS的漏洞，前前后后下载了50多个CMS,顺便重新采集了一下参数，parameter.txt的体积增加到5859条。（原2800+）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;20191106:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在密码字典下新增加了华为安全产品默认用户名密码速查表.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;20191026:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用过程中发现参数字典冗杂了,所以将最近采集的到的以及一些优秀的工具中的字典合并去重复放进了AllParam.txt，共51219条，推荐使用.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;20191022:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在参数字典下新增了&lt;a href="https://github.com/s0md3v/Arjun"&gt;Arjun&lt;/a&gt;的一个工具,比原先的脚本要强大得多,字典在db目录下.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;20190928:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在passwordDict下新增了从&lt;a href="https://github.com/ring04h"&gt;猪猪侠师傅Github&lt;/a&gt;复制的wifi密码top2000字典。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;20190819:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在directoryDicts下新增了常见漏洞目录，推荐直接使用all.txt&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在passwodDict下新增了常见安全设备/路由器/中间件/服务弱口令清单。不过还是推荐使用RW_Password这个强弱口令字典，因为等保的强压之下很多单位不得不将密码设置的复杂，为了方便记忆这些密码又基本都是有规律的，从而诞生了强弱口令，真的很好用啊。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其他更新，本次更新部分字典采集自&lt;a href="https://github.com/Stardustsky/SaiDict"&gt;SaiDict&lt;/a&gt;,合并的时候仔细去重了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;20190811：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上传了自己平常爆破子域名用的字典(从subDomainsBrute,layer等工具中提取出来合并去重，再和自己生成的部分字典合并)，推荐使用main.txt,另一个比较弟弟。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;20190801：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;合并了一个&lt;a href="https://github.com/r35tart/RW_Password"&gt;r35tart&lt;/a&gt;师傅整理的很好的“强弱口令”字典（即看起来很复杂，单但实际上很多人在用的密码）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;20190615：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;合并了一个&lt;a href="https://github.com/emadshanab/WordLists-20111129"&gt;国外的字典&lt;/a&gt; 感觉分类有点乱 考完试再重新整理一下咯。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-content" class="anchor" aria-hidden="true" href="#content"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;content&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%E5%8F%82%E6%95%B0fuzz%E5%AD%97%E5%85%B8"&gt;参数Fuzz字典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#xss-fuzz%E5%AD%97%E5%85%B8"&gt;Xss Fuzz字典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E7%94%A8%E6%88%B7%E5%90%8D%E5%AD%97%E5%85%B8"&gt;用户名字典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E5%AF%86%E7%A0%81%E5%AD%97%E5%85%B8"&gt;密码字典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E7%9B%AE%E5%BD%95%E5%AD%97%E5%85%B8"&gt;目录字典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sql-fuzz%E5%AD%97%E5%85%B8"&gt;sql-fuzz字典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ssrf-fuzz%E5%AD%97%E5%85%B8"&gt;ssrf-fuzz字典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#XXE%E5%AD%97%E5%85%B8"&gt;XXE字典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ctf%E5%AD%97%E5%85%B8"&gt;ctf字典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Api%E5%AD%97%E5%85%B8"&gt;Api字典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E8%B7%AF%E7%94%B1%E5%99%A8%E5%90%8E%E5%8F%B0%E5%AD%97%E5%85%B8"&gt;路由器后台字典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E6%96%87%E4%BB%B6%E5%90%8E%E7%BC%80Fuzz"&gt;文件后缀Fuzz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#js%E6%96%87%E4%BB%B6%E5%AD%97%E5%85%B8"&gt;js文件字典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/tree/master/subdomainDicts"&gt;子域名字典&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;工具推荐：&lt;a href="https://portswigger.net/burp/" rel="nofollow"&gt;burpsuite&lt;/a&gt;,&lt;a href="https://github.com/sqlmapproject/sqlmap"&gt;sqlmap&lt;/a&gt;,&lt;a href="https://github.com/bsmali4/xssfork"&gt;xssfork&lt;/a&gt;,&lt;a href="https://github.com/xmendez/wfuzz/"&gt;Wfuzz&lt;/a&gt;,&lt;a href="https://github.com/TuuuNya/webdirscan"&gt;webdirscan&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;如果有什么的好字典或是建议欢迎提交issue给我。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-参数fuzz字典" class="anchor" aria-hidden="true" href="#参数fuzz字典"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/paramDict"&gt;参数Fuzz字典&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/TheKingOfDuck/fuzzDicts/blob/master/paramDict/parameter.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/images/parameter.jpg"&gt;&lt;img src="https://github.com/TheKingOfDuck/fuzzDicts/raw/master/images/parameter.jpg" alt="CoolCat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;采集自&lt;code&gt;ThinkPHP&lt;/code&gt;,&lt;code&gt;yii2&lt;/code&gt;,&lt;code&gt;phphub&lt;/code&gt;,&lt;code&gt;Zblog&lt;/code&gt;,&lt;code&gt;DiscuzX&lt;/code&gt;,&lt;code&gt;WordPress&lt;/code&gt;等常见PHP框架/CMS。&lt;/p&gt;
&lt;p&gt;使用技巧：如&lt;a href="http://127.0.0.1/1.php" rel="nofollow"&gt;http://127.0.0.1/1.php&lt;/a&gt; ,视为可疑文件，进行fuzz param 选择GET,POST AND (POST JSON) AND (GET Route) AND cookie param&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-xss-fuzz字典" class="anchor" aria-hidden="true" href="#xss-fuzz字典"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/easyXssPayload/blob/master/easyXssPayload.txt"&gt;Xss Fuzz字典&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/TheKingOfDuck/easyXssPayload/blob/master/easyXssPayload.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/images/xss.jpg"&gt;&lt;img src="https://github.com/TheKingOfDuck/fuzzDicts/raw/master/images/xss.jpg" alt="CoolCat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;采集自&lt;code&gt;github&lt;/code&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-用户名字典" class="anchor" aria-hidden="true" href="#用户名字典"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/tree/master/userNameDict"&gt;用户名字典&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/TheKingOfDuck/fuzzDicts/tree/master/userNameDict
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/images/username.jpg"&gt;&lt;img src="https://github.com/TheKingOfDuck/fuzzDicts/raw/master/images/username.jpg" alt="CoolCat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-密码字典" class="anchor" aria-hidden="true" href="#密码字典"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/tree/master/passwordDict"&gt;密码字典&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/TheKingOfDuck/fuzzDicts/tree/master/passwordDict
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/images/password.jpg"&gt;&lt;img src="https://github.com/TheKingOfDuck/fuzzDicts/raw/master/images/password.jpg" alt="CoolCat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-目录字典" class="anchor" aria-hidden="true" href="#目录字典"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/tree/master/directoryDicts"&gt;目录字典&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/TheKingOfDuck/fuzzDicts/tree/master/directoryDicts
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/images/directory.jpg"&gt;&lt;img src="https://github.com/TheKingOfDuck/fuzzDicts/raw/master/images/directory.jpg" alt="CoolCat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sql-fuzz字典" class="anchor" aria-hidden="true" href="#sql-fuzz字典"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/sqlDict/sql.txt"&gt;SQL Fuzz字典&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/TheKingOfDuck/fuzzDicts/blob/master/sqlDict/sql.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/images/sql.jpg"&gt;&lt;img src="https://github.com/TheKingOfDuck/fuzzDicts/raw/master/images/sql.jpg" alt="CoolCat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ssrf-fuzz字典" class="anchor" aria-hidden="true" href="#ssrf-fuzz字典"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/ssrfDicts"&gt;ssrf fuzz字典&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/TheKingOfDuck/fuzzDicts/blob/master/ssrfDicts
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/images/ssrf.jpg"&gt;&lt;img src="https://github.com/TheKingOfDuck/fuzzDicts/raw/master/images/ssrf.jpg" alt="CoolCat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;由&lt;a href="https://github.com/doge-dog"&gt;\xeb\xfe&lt;/a&gt;师傅提供。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-xxe字典" class="anchor" aria-hidden="true" href="#xxe字典"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/tree/master/XXEDicts"&gt;XXE字典&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/TheKingOfDuck/fuzzDicts/tree/master/XXEDicts
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/images/xxe.jpg"&gt;&lt;img src="https://github.com/TheKingOfDuck/fuzzDicts/raw/master/images/xxe.jpg" alt="CoolCat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;收集自百度。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ctf字典" class="anchor" aria-hidden="true" href="#ctf字典"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/tree/master/ctfDict"&gt;ctf字典&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/TheKingOfDuck/fuzzDicts/tree/master/ctfDict
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/ctfDict/ctf-wscan/1.gif"&gt;&lt;img src="https://github.com/TheKingOfDuck/fuzzDicts/raw/master/ctfDict/ctf-wscan/1.gif" alt="CoolCat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;采集自&lt;a href="https://github.com/kingkaki/ctf-wscan"&gt;kingkaki&lt;/a&gt;，原先收集时百度直接下载的压缩包，没看到github链接，所以没标记来源，抱歉抱歉&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-api字典" class="anchor" aria-hidden="true" href="#api字典"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/tree/master/apiDict"&gt;Api字典&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/TheKingOfDuck/fuzzDicts/tree/master/apiDict/api.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/images/api.jpg"&gt;&lt;img src="https://github.com/TheKingOfDuck/fuzzDicts/raw/master/images/api.jpg" alt="CoolCat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;钟馗采集的代码写得很cxk 我真弟弟。。。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-路由器后台字典" class="anchor" aria-hidden="true" href="#路由器后台字典"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/tree/master/routerDicts"&gt;路由器后台字典&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/TheKingOfDuck/fuzzDicts/tree/master/routerDicts/pass.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/images/router.jpg"&gt;&lt;img src="https://github.com/TheKingOfDuck/fuzzDicts/raw/master/images/router.jpg" alt="CoolCat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-文件后缀fuzz" class="anchor" aria-hidden="true" href="#文件后缀fuzz"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/tree/master/uploadFileExtDicts"&gt;文件后缀Fuzz&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/TheKingOfDuck/fuzzDicts/tree/master/uploadFileExtDicts
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheKingOfDuck/fuzzDicts/blob/master/images/fileExt.png"&gt;&lt;img src="https://github.com/TheKingOfDuck/fuzzDicts/raw/master/images/fileExt.png" alt="CoolCat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;采集自&lt;a href="https://github.com/c0ny1/upload-fuzz-dic-builder"&gt;https://github.com/c0ny1/upload-fuzz-dic-builder&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-js文件字典" class="anchor" aria-hidden="true" href="#js文件字典"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/TheKingOfDuck/fuzzDicts/tree/master/js"&gt;js文件字典&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;采集自:&lt;a href="https://github.com/7dog7/bottleneckOsmosis"&gt;https://github.com/7dog7/bottleneckOsmosis&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>TheKingOfDuck</author><guid isPermaLink="false">https://github.com/TheKingOfDuck/fuzzDicts</guid><pubDate>Sat, 04 Jan 2020 00:13:00 GMT</pubDate></item><item><title>abhiTronix/vidgear #14 in Python, Today</title><link>https://github.com/abhiTronix/vidgear</link><description>&lt;p&gt;&lt;i&gt;Most Powerful multi-threaded Video Processing Python framework powerpacked with unique trailblazing features. &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;
&lt;h1 align="center"&gt;&lt;a id="user-content---" class="anchor" aria-hidden="true" href="#--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/631e0d44d47ce2f834d26022272def4a030bdfc8/68747470733a2f2f6162686974726f6e69782e6769746875622e696f2f696d672f766964676561722f76696467656172206c6f676f2e737667"&gt;&lt;img src="https://camo.githubusercontent.com/631e0d44d47ce2f834d26022272def4a030bdfc8/68747470733a2f2f6162686974726f6e69782e6769746875622e696f2f696d672f766964676561722f76696467656172206c6f676f2e737667" alt="VidGear Logo" width="70%" data-canonical-src="https://abhitronix.github.io/img/vidgear/vidgear logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/h1&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content----1" class="anchor" aria-hidden="true" href="#---1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/b3f97d3980eb1ac9fbbf0861b67a21f696195eba/68747470733a2f2f6162686974726f6e69782e6769746875622e696f2f696d672f766964676561722f766964676561722062616e6e65722e737667"&gt;&lt;img src="https://camo.githubusercontent.com/b3f97d3980eb1ac9fbbf0861b67a21f696195eba/68747470733a2f2f6162686974726f6e69782e6769746875622e696f2f696d672f766964676561722f766964676561722062616e6e65722e737667" alt="VidGear Banner" width="40%" data-canonical-src="https://abhitronix.github.io/img/vidgear/vidgear banner.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;div align="center"&gt;
&lt;p&gt;&lt;a href="https://github.com/abhiTronix/vidgear/releases/latest"&gt;Releases&lt;/a&gt;   |   &lt;a href="#gears"&gt;Gears&lt;/a&gt;   |   &lt;a href="https://github.com/abhiTronix/vidgear/wiki"&gt;Wiki Documentation&lt;/a&gt;   |   &lt;a href="#installation"&gt;Installation&lt;/a&gt;   |   &lt;a href="#license"&gt;License&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/abhiTronix/vidgear" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d738cb06ece315cb96d373b6d82799e066357904/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f6162686954726f6e69782f766964676561722e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d747261766973" alt="Build Status" data-canonical-src="https://img.shields.io/travis/abhiTronix/vidgear.svg?style=for-the-badge&amp;amp;logo=travis" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/abhiTronix/vidgear" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b92fe938a10b5ed74fc4da7c6328a2cb7ae90f2c/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f6162686954726f6e69782f766964676561722f74657374696e673f7374796c653d666f722d7468652d6261646765266c6f676f3d636f6465636f76" alt="Codecov branch" data-canonical-src="https://img.shields.io/codecov/c/github/abhiTronix/vidgear/testing?style=for-the-badge&amp;amp;logo=codecov" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://ci.appveyor.com/project/abhiTronix/vidgear" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a808f9d793525276d3e8ddbc3bc9b8baab0ac9c7/68747470733a2f2f696d672e736869656c64732e696f2f6170707665796f722f63692f6162686974726f6e69782f766964676561722e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d6170707665796f72" alt="Build Status" data-canonical-src="https://img.shields.io/appveyor/ci/abhitronix/vidgear.svg?style=for-the-badge&amp;amp;logo=appveyor" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pypi.org/project/vidgear/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4204a7bdb10d1e157552c17bead0e6f376284c2e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f766964676561722e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d646174613a696d6167652f706e673b6261736536342c6956424f5277304b47676f414141414e53556845556741414143414141414167434159414141427a656e72304141414243306c45515652596864575650516f434d5243465836485932676861695a5558734c573045447942726257744e2f45554873485457466e5979434c3467786962565a5a6c5a7a4b546e577a30515a706b3572307649646b462f6b4250414d4f4b656464452b4351504b6f633559743563546a424d645153774451546f5767424a416e336a6d6871676c746170415636453662355531374d4747415561556a3037546669634d6649425a44563676786f77426d314250395762535145346f356839496a504a6d793733544550444478566d6f5a64517251356a52686c7939513874674d55586b4949576e306f4734475951664158517a7a3150476f4369516e644d37623452674a61792f68377a424c5433684153676f4b6a616d514a4d72654b66306766754147795974584549414b634c2f44737331356971366f685867686f7a4c5969414d78507541437774495434796551557841614c725a77416f7147524b476b377144535954665951384c75596e4141414141456c46546b5375516d4343" alt="PyPi version" data-canonical-src="https://img.shields.io/pypi/v/vidgear.svg?style=for-the-badge&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAABC0lEQVRYhdWVPQoCMRCFX6HY2ghaiZUXsLW0EDyBrbWtN/EUHsHTWFnYyCL4gxibVZZlZzKTnWz0QZpk5r0vIdkF/kBPAMOKeddE+CQPKoc5Yt5cTjBMdQSwDQToWgBJAn3jmhqgltapAV6E6b5U17MGGAUaUj07TficMfIBZDV6vxowBm1BP9WbSQE4o5h9IjPJmy73TEPDDxVmoZdQrQ5jRhly9Q8tgMUXkIIWn0oG4GYQfAXQzz1PGoCiQndM7b4RgJay/h7zBLT3hASgoKjamQJMreKf0gfuAGyYtXEIAKcL/Dss15iq6ohXghozLYiAMxPuACwtIT4yeQUxAaLrZwAoqGRKGk7qDSYTfYQ8LuYnAAAAAElFTkSuQmCC" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://saythanks.io/to/abhiTronix" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2a4a9ebacfbd2cc4567bb525bbf4155afbfbe2d7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d64617461253341696d616765253246737667253242786d6c2533426261736536342532435044393462577767646d567963326c76626a30694d5334774969426c626d4e765a476c755a7a3069565652474c546769507a343863335a6e49476c6b50534a7a646d63794969423361575230614430694e6a51314969426f5a576c6e61485139496a55344e534967646d567963326c76626a30694d5334774969423462577875637a30696148523063446f764c336433647935334d793576636d63764d6a41774d43397a646d6369506941385a7942705a443069624746355a584978496a3467494478775958526f49476c6b50534a775958526f4d6a51784e7949675a443069625449354e79347a494455314d4334344e324d744d544d754e7a63314c5445314c6a517a4e6930304f4334784e7a45744e4455754e544d744e7a59754e444d314c5459324c6a67334e4330344d7934334e4451744e6a4d754d6a51794c546b314c6a45304d6930334d69347a4f5451744d5449354c6a45304c5445774d7934334c5459794c6a59344e5330314e7934334d6930344f53347a4d4459744d5445314c6a63784c5467354c6a49784e4330784f5451754d7a51674d4334774e4451314d5449744d7a67754d7a6730494449754e6a59774f4330314d7934784e7a49674d544d754e4445744e7a55754e7a6b33494445344c6a497a4e79307a4f43347a4f4459674e4455754d5330324e6934354d446b674e7a6b754e4451314c5467304c6a4d314e5341794e43347a4d6a55744d5449754d7a553249444d324c6a4d794d7930784e7934344e4455674e7a59754f5451304c5445344c6a4133494451794c6a51354d7930774c6a497a4e44677a494455784c6a517a4f5341304c6a63784f5463674e7a59754e444d31494445344c6a51314d69417a4d4334304d6a55674d5459754e7a4530494459784c6a6330494455794c6a517a4e6941324f4334794d544d674e7a63754f44457862444d754f546b344d5341784e5334324e7a49674f5334344e546b324c5449784c6a55344e574d314e5334334d5459744d5449784c6a6b334944497a4d7934324c5445794d4334784e5341794f5455754e53417a4c6a417a4d5459674d546b754e6a4d3449444d354c6a41334e6941794d5334334f5451674d5449794c6a5578494451754d7a67774d5341784e6a6b754e5445744d6a49754e7a4531494459784c6a4d774f5330324e53347a4f4341784d4467754d4455744d5459304c6a4178494445334f5334324f4330324e4334324f4445674e4459754f5463304c54457a4e7934344f4341784d5467754d4455744d5451794c6a6b34494445794f4334774d7930314c6a6b784e5455674d5445754e5467344c5441754d6a67794d5459674d5334344d5455354c5449324c6a51774f4330794e7934304e6a46364969426d615778735053496a5a4751314d44526d496938253242494477765a7a34384c334e325a7a34253344" alt="Say Thank you" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg?style=for-the-badge&amp;amp;logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz48c3ZnIGlkPSJzdmcyIiB3aWR0aD0iNjQ1IiBoZWlnaHQ9IjU4NSIgdmVyc2lvbj0iMS4wIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPiA8ZyBpZD0ibGF5ZXIxIj4gIDxwYXRoIGlkPSJwYXRoMjQxNyIgZD0ibTI5Ny4zIDU1MC44N2MtMTMuNzc1LTE1LjQzNi00OC4xNzEtNDUuNTMtNzYuNDM1LTY2Ljg3NC04My43NDQtNjMuMjQyLTk1LjE0Mi03Mi4zOTQtMTI5LjE0LTEwMy43LTYyLjY4NS01Ny43Mi04OS4zMDYtMTE1LjcxLTg5LjIxNC0xOTQuMzQgMC4wNDQ1MTItMzguMzg0IDIuNjYwOC01My4xNzIgMTMuNDEtNzUuNzk3IDE4LjIzNy0zOC4zODYgNDUuMS02Ni45MDkgNzkuNDQ1LTg0LjM1NSAyNC4zMjUtMTIuMzU2IDM2LjMyMy0xNy44NDUgNzYuOTQ0LTE4LjA3IDQyLjQ5My0wLjIzNDgzIDUxLjQzOSA0LjcxOTcgNzYuNDM1IDE4LjQ1MiAzMC40MjUgMTYuNzE0IDYxLjc0IDUyLjQzNiA2OC4yMTMgNzcuODExbDMuOTk4MSAxNS42NzIgOS44NTk2LTIxLjU4NWM1NS43MTYtMTIxLjk3IDIzMy42LTEyMC4xNSAyOTUuNSAzLjAzMTYgMTkuNjM4IDM5LjA3NiAyMS43OTQgMTIyLjUxIDQuMzgwMSAxNjkuNTEtMjIuNzE1IDYxLjMwOS02NS4zOCAxMDguMDUtMTY0LjAxIDE3OS42OC02NC42ODEgNDYuOTc0LTEzNy44OCAxMTguMDUtMTQyLjk4IDEyOC4wMy01LjkxNTUgMTEuNTg4LTAuMjgyMTYgMS44MTU5LTI2LjQwOC0yNy40NjF6IiBmaWxsPSIjZGQ1MDRmIi8%2BIDwvZz48L3N2Zz4%3D" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fgithub.com%2FabhiTronix%2Fvidgear&amp;amp;via%20%40abhi_una12&amp;amp;text=VidGear%20-%20A%20simple%2C%20powerful%2C%20flexible%20%26%20threaded%20Python%20Video%20Processing%20Library&amp;amp;hashtags=vidgear%20%23multithreaded%20%23python%20%23video-processing%20%23github" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c2e5a6804b268ffea5cd981698721bcdc28d33d/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d74776974746572" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/url/http/shields.io.svg?style=for-the-badge&amp;amp;logo=twitter" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/2twOXFvlA" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/44ddf085277c353428cb4203a14fc7718a39e1a3/68747470733a2f2f6162686974726f6e69782e6769746875622e696f2f696d672f766964676561722f6f72616e67655f696d672e706e67" alt="Buy Me A Coffee" data-canonical-src="https://abhitronix.github.io/img/vidgear/orange_img.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;VidGear is a powerful python Video Processing library built with multi-threaded &lt;a href="#gears"&gt;&lt;strong&gt;Gears&lt;/strong&gt;&lt;/a&gt; each with a unique set of trailblazing features. These APIs provides a easy-to-use, highly extensible, and multi-threaded wrapper around many underlying state-of-the-art libraries such as &lt;em&gt;&lt;a href="https://github.com/opencv/opencv"&gt;OpenCV ➶&lt;/a&gt;, &lt;a href="https://www.ffmpeg.org/" rel="nofollow"&gt;FFmpeg ➶&lt;/a&gt;, &lt;a href="https://github.com/waveform80/picamera"&gt;picamera ➶&lt;/a&gt;, &lt;a href="https://github.com/mps-youtube/pafy"&gt;pafy ➶&lt;/a&gt;, &lt;a href="https://github.com/zeromq/pyzmq"&gt;pyzmq ➶&lt;/a&gt; and &lt;a href="https://github.com/BoboTiG/python-mss"&gt;python-mss ➶&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;The following &lt;strong&gt;functional block diagram&lt;/strong&gt; clearly depicts the functioning of VidGear library:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/7eb66b31e45072cd99ee988690242b93388705db/68747470733a2f2f6162686974726f6e69782e6769746875622e696f2f696d672f766964676561722f766964676561725f66756e6374696f6e322d30312e737667"&gt;&lt;img src="https://camo.githubusercontent.com/7eb66b31e45072cd99ee988690242b93388705db/68747470733a2f2f6162686974726f6e69782e6769746875622e696f2f696d672f766964676561722f766964676561725f66756e6374696f6e322d30312e737667" alt="@Vidgear Functional Block Diagram" data-canonical-src="https://abhitronix.github.io/img/vidgear/vidgear_function2-01.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h1&gt;
&lt;p&gt;&lt;a href="#tldr"&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="#gears"&gt;&lt;strong&gt;Gears: What are these?&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#camgear"&gt;&lt;strong&gt;CamGear&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pigear"&gt;&lt;strong&gt;PiGear&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#videogear"&gt;&lt;strong&gt;VideoGear&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#screengear"&gt;&lt;strong&gt;ScreenGear&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#writegear"&gt;&lt;strong&gt;WriteGear&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#netgear"&gt;&lt;strong&gt;NetGear&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#installation"&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#prerequisites"&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#option-1-pypi-install"&gt;&lt;strong&gt;1 - PyPI Install&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#option-2-release-archive-download"&gt;&lt;strong&gt;2 - Release Archive Download&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#option-3-clone-the-repo"&gt;&lt;strong&gt;3 - Clone Repo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#new-release-sneekpeak--vidgear-016"&gt;&lt;strong&gt;New-Release SneekPeak: v0.1.6&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="#documentation"&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For Developers/Contributors&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#testing"&gt;&lt;strong&gt;Testing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contributing"&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Additional Info&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#supported-python-legacies"&gt;&lt;strong&gt;Supported Python legacies&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#changelog"&gt;&lt;strong&gt;Changelog&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citing"&gt;&lt;strong&gt;Citing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-tldr" class="anchor" aria-hidden="true" href="#tldr"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TL;DR&lt;/h1&gt;
&lt;h4&gt;&lt;a id="user-content-what-is-vidgear" class="anchor" aria-hidden="true" href="#what-is-vidgear"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is vidgear?&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;"VidGear is an &lt;a href="https://github.com/abhiTronix/vidgear/wiki/FAQ-&amp;amp;-Troubleshooting#2-vidgear-is-ultrafast-but-how"&gt;ultrafast➶&lt;/a&gt;, compact, flexible and easy-to-adapt complete Video Processing Python Library."&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-what-does-it-do" class="anchor" aria-hidden="true" href="#what-does-it-do"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What does it do?&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;"VidGear can read, write, process, send &amp;amp; receive video frames from various devices in real-time."&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-what-is-its-purpose" class="anchor" aria-hidden="true" href="#what-is-its-purpose"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is its purpose?&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;"Built with simplicity in mind, VidGear lets programmers and software developers to easily integrate and perform complex Video Processing tasks in their existing or new applications, without going through various underlying library's documentation and using just a few lines of code. Beneficial for both, if you're new to programming with Python language or already a pro at it."&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For more advanced information, see the &lt;a href="https://github.com/abhiTronix/vidgear/wiki"&gt;&lt;em&gt;Wiki Documentation ➶&lt;/em&gt;&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-gears" class="anchor" aria-hidden="true" href="#gears"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gears&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;VidGear is built with various &lt;strong&gt;Multi-Threaded APIs&lt;/strong&gt; &lt;em&gt;(a.k.a Gears)&lt;/em&gt; each with some unique function/mechanism.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Each of these API is designed exclusively to handle/control different device-specific video streams, network streams, and media encoders. These APIs provides an easy-to-use, highly extensible, and a multi-threaded wrapper around various underlying libraries to exploit their features and functions directly while providing robust error-handling.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;These Gears can be classified as follows:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A. VideoCapture Gears:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#camgear"&gt;&lt;strong&gt;CamGear:&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;Targets various IP-USB-Cameras/Network-Streams/YouTube-Video-URL.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pigear"&gt;&lt;strong&gt;PiGear:&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;Targets various Raspberry Pi Camera Modules.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#screengear"&gt;&lt;strong&gt;ScreenGear:&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;Enables ultra-fast Screen Casting.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#videogear"&gt;&lt;strong&gt;VideoGear:&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;A common API with Video Stabilizer wrapper.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;B. VideoWriter Gear:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#writegear"&gt;&lt;strong&gt;WriteGear:&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;Handles easy Lossless Video Encoding and Compression.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;C. Network Gear:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#netgear"&gt;&lt;strong&gt;NetGear:&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;Targets synchronous/asynchronous video frames transferring between interconnecting systems over the network.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-camgear" class="anchor" aria-hidden="true" href="#camgear"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CamGear&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;CamGear can grab ultra-fast frames from diverse range of devices/streams, which includes almost any IP/USB Cameras, multimedia video file format (&lt;a href="https://github.com/abhiTronix/vidgear/blob/e0843720202b0921d1c26e2ce5b11fadefbec892/vidgear/tests/benchmark_tests/test_benchmark_playback.py#L65"&gt;&lt;em&gt;upto 4k tested&lt;/em&gt;&lt;/a&gt;), various network stream protocols such as &lt;code&gt;http(s), rtp, rstp, rtmp, mms, etc.&lt;/code&gt;, plus support for live Gstreamer's stream pipeline and YouTube video/livestreams URLs.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;CamGear provides a flexible, high-level multi-threaded wrapper around &lt;code&gt;OpenCV's&lt;/code&gt; &lt;a href="https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html#a57c0e81e83e60f36c83027dc2a188e80" rel="nofollow"&gt;VideoCapture class&lt;/a&gt; with access almost all of its available parameters and also employs &lt;a href="https://github.com/mps-youtube/pafy"&gt;&lt;code&gt;pafy&lt;/code&gt;&lt;/a&gt; python APIs for live &lt;a href="https://github.com/abhiTronix/vidgear/wiki/CamGear#2-camgear-api-with-live-youtube-piplineing-using-video-url"&gt;YouTube streaming&lt;/a&gt;. Furthermore, CamGear implements exclusively on &lt;a href="https://github.com/abhiTronix/vidgear/wiki/Threaded-Queue-Mode"&gt;&lt;strong&gt;Threaded Queue mode&lt;/strong&gt;&lt;/a&gt; for ultra-fast, error-free and synchronized frame handling.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Following simplified functional block diagram depicts CamGear API's generalized working:&lt;/strong&gt;&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/abhiTronix/Imbakup/raw/master/Images/CamGear.png"&gt;&lt;img src="https://github.com/abhiTronix/Imbakup/raw/master/Images/CamGear.png" alt="CamGear Functional Block Diagram" width="60%/" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-camgear-api-guide" class="anchor" aria-hidden="true" href="#camgear-api-guide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CamGear API Guide:&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/abhiTronix/vidgear/wiki/CamGear#camgear-api"&gt;&lt;strong&gt;&amp;gt;&amp;gt;&amp;gt; Usage Guide&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-videogear" class="anchor" aria-hidden="true" href="#videogear"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VideoGear&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;VideoGear API provides a special internal wrapper around VidGear's exclusive &lt;a href="https://github.com/abhiTronix/vidgear/wiki/Stabilizer-Class"&gt;&lt;strong&gt;Video Stabilizer&lt;/strong&gt;&lt;/a&gt; class.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Furthermore, VideoGear API can provide internal access to both &lt;a href="#camgear"&gt;CamGear&lt;/a&gt; and &lt;a href="#pigear"&gt;PiGear&lt;/a&gt; APIs separated by a special flag. Thereby, &lt;em&gt;this API holds the exclusive power for any incoming VideoStream from any source, whether it is live or not, to access and stabilize it directly with minimum latency and memory requirements.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Below is a snapshot of a VideoGear Stabilizer in action:&lt;/strong&gt;&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/abhiTronix/Imbakup/raw/master/Images/stabilizer.gif"&gt;&lt;img src="https://github.com/abhiTronix/Imbakup/raw/master/Images/stabilizer.gif" alt="VideoGear Stabilizer in action!" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;sub&gt;&lt;i&gt;Original Video Courtesy &lt;a href="http://liushuaicheng.org/SIGGRAPH2013/database.html" title="opensourced video samples database" rel="nofollow"&gt;@SIGGRAPH2013&lt;/a&gt;&lt;/i&gt;&lt;/sub&gt;
&lt;/p&gt;
&lt;p&gt;Code to generate above VideoGear API Stabilized Video(&lt;em&gt;See more detailed usage examples &lt;a href="https://github.com/abhiTronix/vidgear/wiki/Real-time-Video-Stabilization#usage"&gt;here&lt;/a&gt;&lt;/em&gt;):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; import libraries&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; vidgear.gears &lt;span class="pl-k"&gt;import&lt;/span&gt; VideoGear
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;import&lt;/span&gt; cv2

stream_stab &lt;span class="pl-k"&gt;=&lt;/span&gt; VideoGear(&lt;span class="pl-v"&gt;source&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;test.mp4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;stabilize&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;).start() &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To open any valid video stream with `stabilize` flag set to True.&lt;/span&gt;
stream_org &lt;span class="pl-k"&gt;=&lt;/span&gt; VideoGear(&lt;span class="pl-v"&gt;source&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;test.mp4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;).start() &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; open same stream without stabilization for comparison&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; infinite loop&lt;/span&gt;
&lt;span class="pl-k"&gt;while&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;:
  
  frame_stab &lt;span class="pl-k"&gt;=&lt;/span&gt; stream_stab.read()
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; read stabilized frames&lt;/span&gt;

  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; check if frame is None&lt;/span&gt;
  &lt;span class="pl-k"&gt;if&lt;/span&gt; frame_stab &lt;span class="pl-k"&gt;is&lt;/span&gt; &lt;span class="pl-c1"&gt;None&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;if True break the infinite loop&lt;/span&gt;
    &lt;span class="pl-k"&gt;break&lt;/span&gt;
  
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;read original frame&lt;/span&gt;
  frame_org &lt;span class="pl-k"&gt;=&lt;/span&gt; stream_org.read()

  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;concatenate both frames&lt;/span&gt;
  output_frame &lt;span class="pl-k"&gt;=&lt;/span&gt; np.concatenate((frame_org, frame_stab), &lt;span class="pl-v"&gt;axis&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;)

  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;put text&lt;/span&gt;
  cv2.putText(output_frame, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Before&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, (&lt;span class="pl-c1"&gt;10&lt;/span&gt;, output_frame.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;] &lt;span class="pl-k"&gt;-&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;),cv2.&lt;span class="pl-c1"&gt;FONT_HERSHEY_SIMPLEX&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.6&lt;/span&gt;, (&lt;span class="pl-c1"&gt;0&lt;/span&gt;,&lt;span class="pl-c1"&gt;255&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;), &lt;span class="pl-c1"&gt;2&lt;/span&gt;)
  cv2.putText(output_frame, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;After&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, (output_frame.shape[&lt;span class="pl-c1"&gt;1&lt;/span&gt;]&lt;span class="pl-k"&gt;//&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;10&lt;/span&gt;, frame.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;] &lt;span class="pl-k"&gt;-&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;),cv2.&lt;span class="pl-c1"&gt;FONT_HERSHEY_SIMPLEX&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.6&lt;/span&gt;, (&lt;span class="pl-c1"&gt;0&lt;/span&gt;,&lt;span class="pl-c1"&gt;255&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;), &lt;span class="pl-c1"&gt;2&lt;/span&gt;)
  
  cv2.imshow(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Stabilized Frame&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, output_frame)
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Show output window&lt;/span&gt;

  key &lt;span class="pl-k"&gt;=&lt;/span&gt; cv2.waitKey(&lt;span class="pl-c1"&gt;1&lt;/span&gt;) &lt;span class="pl-k"&gt;&amp;amp;&lt;/span&gt; &lt;span class="pl-c1"&gt;&lt;span class="pl-k"&gt;0x&lt;/span&gt;FF&lt;/span&gt;
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; check for 'q' key-press&lt;/span&gt;
  &lt;span class="pl-k"&gt;if&lt;/span&gt; key &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-c1"&gt;ord&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;q&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;):
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;if 'q' key-pressed break out&lt;/span&gt;
    &lt;span class="pl-k"&gt;break&lt;/span&gt;

cv2.destroyAllWindows()
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; close output window&lt;/span&gt;
stream_org.stop()
stream_stab.stop()
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; safely close video streams.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-videogear-api-guide" class="anchor" aria-hidden="true" href="#videogear-api-guide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VideoGear API Guide:&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/abhiTronix/vidgear/wiki/VideoGear#videogear-api"&gt;&lt;strong&gt;&amp;gt;&amp;gt;&amp;gt; Usage Guide&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pigear" class="anchor" aria-hidden="true" href="#pigear"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PiGear&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;PiGear is similar to CamGear but made to support various Raspberry Pi Camera Modules &lt;em&gt;(such as &lt;a href="https://github.com/techyian/MMALSharp/wiki/OmniVision-OV5647-Camera-Module"&gt;OmniVision OV5647 Camera Module&lt;/a&gt; and &lt;a href="https://github.com/techyian/MMALSharp/wiki/Sony-IMX219-Camera-Module"&gt;Sony IMX219 Camera Module&lt;/a&gt;)&lt;/em&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;PiGear provides a flexible multi-threaded wrapper around complete &lt;a href="https://github.com/waveform80/picamera"&gt;&lt;strong&gt;picamera&lt;/strong&gt;&lt;/a&gt; python library to interface with these modules correctly, and also grants the ability to exploit its various features like &lt;code&gt;brightness, saturation, sensor_mode, etc.&lt;/code&gt; effortlessly.&lt;/p&gt;
&lt;p&gt;Best of all, PiGear API provides excellent Error-Handling with features like a threaded internal timer that keeps active track of any frozen threads and handles hardware failures/frozen threads robustly thereby will exit safely if any failure occurs. So now if you accidently pulled your camera module cable out when running PiGear API in your script, instead of going into possible kernel panic/frozen threads, API exit safely to save resources.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Following simplified functional block diagram depicts PiGear API:&lt;/strong&gt;&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/abhiTronix/Imbakup/raw/master/Images/PiGear.png"&gt;&lt;img src="https://github.com/abhiTronix/Imbakup/raw/master/Images/PiGear.png" alt="PiGear Functional Block Diagram" width="40%/" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pigear-api-guide" class="anchor" aria-hidden="true" href="#pigear-api-guide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PiGear API Guide:&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/abhiTronix/vidgear/wiki/PiGear#pigear-api"&gt;&lt;strong&gt;&amp;gt;&amp;gt;&amp;gt; Usage Guide&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-screengear" class="anchor" aria-hidden="true" href="#screengear"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ScreenGear&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;ScreenGear API act as Screen Recorder, that can grab frames from your monitor in real-time either by define an area on the computer screen or fullscreen at the expense of insignificant latency. It also provide seemless support for capturing frames from multiple monitors.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ScreenGear provides a high-level multi-threaded wrapper around &lt;a href="https://github.com/BoboTiG/python-mss"&gt;&lt;strong&gt;python-mss&lt;/strong&gt;&lt;/a&gt; python library API and also supports a easy and flexible direct internal parameter manipulation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Below is a snapshot of a ScreenGear API in action:&lt;/strong&gt;&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/abhiTronix/Imbakup/raw/master/Images/screengear.gif"&gt;&lt;img src="https://github.com/abhiTronix/Imbakup/raw/master/Images/screengear.gif" alt="ScreenGear in action!" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Code to generate the above result:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; import libraries&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; vidgear.gears &lt;span class="pl-k"&gt;import&lt;/span&gt; ScreenGear
&lt;span class="pl-k"&gt;import&lt;/span&gt; cv2

stream &lt;span class="pl-k"&gt;=&lt;/span&gt; ScreenGear().start()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; infinite loop&lt;/span&gt;
&lt;span class="pl-k"&gt;while&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;:
  
  frame &lt;span class="pl-k"&gt;=&lt;/span&gt; stream.read()
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; read frames&lt;/span&gt;

  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; check if frame is None&lt;/span&gt;
  &lt;span class="pl-k"&gt;if&lt;/span&gt; frame &lt;span class="pl-k"&gt;is&lt;/span&gt; &lt;span class="pl-c1"&gt;None&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;if True break the infinite loop&lt;/span&gt;
    &lt;span class="pl-k"&gt;break&lt;/span&gt;
  
  cv2.imshow(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Output Frame&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, frame)
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Show output window&lt;/span&gt;

  key &lt;span class="pl-k"&gt;=&lt;/span&gt; cv2.waitKey(&lt;span class="pl-c1"&gt;1&lt;/span&gt;) &lt;span class="pl-k"&gt;&amp;amp;&lt;/span&gt; &lt;span class="pl-c1"&gt;&lt;span class="pl-k"&gt;0x&lt;/span&gt;FF&lt;/span&gt;
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; check for 'q' key-press&lt;/span&gt;
  &lt;span class="pl-k"&gt;if&lt;/span&gt; key &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-c1"&gt;ord&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;q&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;):
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;if 'q' key-pressed break out&lt;/span&gt;
    &lt;span class="pl-k"&gt;break&lt;/span&gt;

cv2.destroyAllWindows()
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; close output window&lt;/span&gt;

stream.stop()
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; safely close video stream.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-screengear-api-guide" class="anchor" aria-hidden="true" href="#screengear-api-guide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ScreenGear API Guide:&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/abhiTronix/vidgear/wiki/ScreenGear#screengear-api"&gt;&lt;strong&gt;&amp;gt;&amp;gt;&amp;gt; Usage Guide&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-writegear" class="anchor" aria-hidden="true" href="#writegear"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WriteGear&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;WriteGear handles various powerful Writer Tools that provide us the freedom to do almost anything imagine with multimedia files.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;WriteGear API provide a complete, flexible &amp;amp; robust wrapper around &lt;a href="https://www.ffmpeg.org/" rel="nofollow"&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;&lt;/a&gt;, a leading multimedia framework. With WriteGear, we can process real-time video frames into a lossless compressed format with any suitable specification in just few easy &lt;a href="https://github.com/abhiTronix/vidgear/wiki/Compression-Mode:-FFmpeg#1-writegear-bare-minimum-examplecompression-mode"&gt;lines of codes&lt;/a&gt;. These specifications include setting any video/audio property such as &lt;code&gt;bitrate, codec, framerate, resolution, subtitles,  etc.&lt;/code&gt; easily as well complex tasks such as multiplexing video with audio in real-time(see this &lt;a href="https://github.com/abhiTronix/vidgear/wiki/Working-with-Audio#a-live-audio-input-to-writegear-class"&gt;example wiki&lt;/a&gt;). Best of all, WriteGear grants the freedom to play with any FFmpeg parameter with its exclusive custom Command function(see this &lt;a href="https://github.com/abhiTronix/vidgear/wiki/Custom-FFmpeg-Commands-in-WriteGear-API#custom-ffmpeg-commands-in-writegear-api"&gt;example wiki&lt;/a&gt;), while handling all errors robustly.&lt;/p&gt;
&lt;p&gt;In addition to this, WriteGear also provides flexible access to &lt;a href="https://docs.opencv.org/master/dd/d9e/classcv_1_1VideoWriter.html#ad59c61d8881ba2b2da22cff5487465b5" rel="nofollow"&gt;&lt;strong&gt;OpenCV's VideoWriter API&lt;/strong&gt;&lt;/a&gt; which provides some basic tools for video frames encoding but without compression.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;WriteGear primarily operates in the following two modes:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Compression Mode:&lt;/strong&gt; In this mode, WriteGear utilizes &lt;a href="https://www.ffmpeg.org/" rel="nofollow"&gt;&lt;strong&gt;&lt;code&gt;FFmpeg's&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; inbuilt encoders to encode lossless multimedia files. It provides us the ability to exploit almost any available parameters available within FFmpeg, with so much ease and flexibility and while doing that it robustly handles all errors/warnings quietly. &lt;strong&gt;You can find more about this mode &lt;a href="https://github.com/abhiTronix/vidgear/wiki/Compression-Mode:-FFmpeg"&gt;here&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non-Compression Mode:&lt;/strong&gt; In this mode, WriteGear utilizes basic OpenCV's inbuilt &lt;a href="https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html" rel="nofollow"&gt;&lt;strong&gt;VideoWriter API&lt;/strong&gt;&lt;/a&gt;. Similar to compression mode, WriteGear also supports all parameters manipulation available within OpenCV's VideoWriter API. But this mode lacks the ability to manipulate encoding parameters and other important features like video compression, audio encoding, etc. &lt;strong&gt;You can learn about this mode &lt;a href="https://github.com/abhiTronix/vidgear/wiki/Non-Compression-Mode:-OpenCV"&gt;here&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Following functional block diagram depicts WriteGear API's generalized working:&lt;/strong&gt;&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/abhiTronix/Imbakup/raw/master/Images/WriteGear.png"&gt;&lt;img src="https://github.com/abhiTronix/Imbakup/raw/master/Images/WriteGear.png" alt="WriteGear Functional Block Diagram" width="70%/" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-writegear-api-guide" class="anchor" aria-hidden="true" href="#writegear-api-guide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WriteGear API Guide:&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/abhiTronix/vidgear/wiki/WriteGear#writegear-api"&gt;&lt;strong&gt;&amp;gt;&amp;gt;&amp;gt; Usage Guide&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-netgear" class="anchor" aria-hidden="true" href="#netgear"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NetGear&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;NetGear is exclusively designed to transfer video frames synchronously and asynchronously between interconnecting systems over the network in real-time.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;NetGear implements a high-level wrapper around &lt;a href="https://github.com/zeromq/pyzmq"&gt;&lt;strong&gt;PyZmQ&lt;/strong&gt;&lt;/a&gt; python library that contains python bindings for &lt;a href="http://zeromq.org/" rel="nofollow"&gt;ZeroMQ&lt;/a&gt; - a high-performance asynchronous distributed messaging library that aim to be used in distributed or concurrent applications. It provides a message queue, but unlike message-oriented middleware, a ZeroMQ system can run without a dedicated message broker.&lt;/p&gt;
&lt;p&gt;NetGear provides seamless support for bidirectional data transmission between receiver(client) and sender(server) through bi-directional synchronous messaging patterns such as zmq.PAIR &lt;em&gt;(ZMQ Pair Pattern)&lt;/em&gt; &amp;amp; zmq.REQ/zmq.REP &lt;em&gt;(ZMQ Request/Reply Pattern)&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;NetGear also supports real-time frame Encoding/Decoding compression capabilities for optimizing performance while sending the frames directly over the network, by encoding the frame before sending it and decoding it on the client's end automatically in real-time.&lt;/p&gt;
&lt;p&gt;For security, NetGear implements easy access to ZeroMQ's powerful, smart &amp;amp; secure Security Layers, that enables strong encryption on data, and unbreakable authentication between the Server and the Client with the help of custom certificates/keys and brings easy, standardized privacy and authentication for distributed systems over the network.&lt;/p&gt;
&lt;p&gt;Best of all, NetGear can robustly handle Multiple Servers devices at once, thereby providing access to seamless Live Streaming of the multiple device in a network at the same time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NetGear as of now seamlessly supports three ZeroMQ messaging patterns:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/pair.html" rel="nofollow"&gt;&lt;strong&gt;&lt;code&gt;zmq.PAIR&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;(ZMQ Pair Pattern)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/client_server.html" rel="nofollow"&gt;&lt;strong&gt;&lt;code&gt;zmq.REQ/zmq.REP&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;(ZMQ Request/Reply Pattern)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/pubsub.html" rel="nofollow"&gt;&lt;strong&gt;&lt;code&gt;zmq.PUB/zmq.SUB&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;(ZMQ Publish/Subscribe Pattern)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Following functional block diagram depicts generalized functioning of NetGear API:&lt;/strong&gt;&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/abhiTronix/Imbakup/raw/master/Images/NetGear.png"&gt;&lt;img src="https://github.com/abhiTronix/Imbakup/raw/master/Images/NetGear.png" alt="NetGear Functional Block Diagram" width="80%/" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-netgear-api-guide" class="anchor" aria-hidden="true" href="#netgear-api-guide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NetGear API Guide:&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/abhiTronix/vidgear/wiki/NetGear#netgear-api"&gt;&lt;strong&gt;&amp;gt;&amp;gt;&amp;gt; Usage Guide&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-new-release-sneekpeak--vidgear-016" class="anchor" aria-hidden="true" href="#new-release-sneekpeak--vidgear-016"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New Release SneekPeak : VidGear 0.1.6&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;g-emoji class="g-emoji" alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png"&gt;⚠️&lt;/g-emoji&gt; Python 2.7 legacy support &lt;a href="https://github.com/abhiTronix/vidgear/issues/29"&gt;dropped in v0.1.6&lt;/a&gt; !&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NetGear API:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Added powerful ZMQ Authentication &amp;amp; Data Encryption features for NetGear API&lt;/li&gt;
&lt;li&gt;Added robust Multi-Server support for NetGear API.&lt;/li&gt;
&lt;li&gt;Added exclusive Bi-Directional Mode for bidirectional data transmission.&lt;/li&gt;
&lt;li&gt;Added frame-compression support with on-the-fly flexible encoding/decoding.&lt;/li&gt;
&lt;li&gt;Implemented new &lt;em&gt;Publish/Subscribe(&lt;code&gt;zmq.PUB/zmq.SUB&lt;/code&gt;)&lt;/em&gt; pattern for seamless Live Streaming in NetGear API.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PiGear API:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Added new threaded internal timing function for PiGear to handle any hardware failures/frozen threads&lt;/li&gt;
&lt;li&gt;PiGear will not exit safely with &lt;code&gt;SystemError&lt;/code&gt; if Picamera ribbon cable is pulled out to save resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;WriteGear API:&lt;/strong&gt; Added new &lt;code&gt;execute_ffmpeg_cmd&lt;/code&gt; function to pass a custom command to its internal FFmpeg pipeline.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stabilizer class:&lt;/strong&gt; Added new &lt;em&gt;Crop and Zoom&lt;/em&gt; feature.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Added VidGear's official native support for MacOS environment and &lt;a href="changelog.md"&gt;many more...&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites:&lt;/h2&gt;
&lt;p&gt;Before installing VidGear, you must verify that the following dependencies are met:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png"&gt;⚠️&lt;/g-emoji&gt; Must be using only &lt;a href="#supported-python-legacies"&gt;&lt;strong&gt;supported Python legacies&lt;/strong&gt;&lt;/a&gt; and also &lt;a href="https://pip.pypa.io/en/stable/installing/" rel="nofollow"&gt;&lt;strong&gt;pip&lt;/strong&gt;&lt;/a&gt; already installed and configured.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;OpenCV:&lt;/code&gt;&lt;/strong&gt; VidGear must require OpenCV(3.0+) python enabled binaries to be installed on your machine for its core functions. For its installation, you can follow these online tutorials for &lt;a href="https://www.pyimagesearch.com/2018/05/28/ubuntu-18-04-how-to-install-opencv/" rel="nofollow"&gt;linux&lt;/a&gt; and &lt;a href="https://www.pyimagesearch.com/2018/09/26/install-opencv-4-on-your-raspberry-pi/" rel="nofollow"&gt;raspberry pi&lt;/a&gt;, otherwise, install it via pip:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;  pip3 install -U opencv-python       &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;or install opencv-contrib-python similarly&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;FFmpeg:&lt;/code&gt;&lt;/strong&gt; VidGear must require FFmpeg for its powerful video compression and encoding capabilities. &lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;🌟&lt;/g-emoji&gt; Follow this &lt;a href="https://github.com/abhiTronix/vidgear/wiki/FFmpeg-Installation"&gt;&lt;strong&gt;FFmpeg wiki page&lt;/strong&gt;&lt;/a&gt; for its installation. &lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;🌟&lt;/g-emoji&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;picamera:&lt;/code&gt;&lt;/strong&gt; Required if using Raspberry Pi Camera Modules(&lt;em&gt;such as OmniVision OV5647 Camera Module&lt;/em&gt;) with your Raspberry Pi machine. You can easily install it via pip:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;  pip3 install picamera&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also, make sure to enable Raspberry Pi hardware-specific settings prior to using this library.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;mss:&lt;/code&gt;&lt;/strong&gt; Required for using Screen Casting. Install it via pip:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;  pip3 install mss&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;pyzmq:&lt;/code&gt;&lt;/strong&gt; Required for transferring live video frames through &lt;em&gt;ZeroMQ messaging system&lt;/em&gt; over the network. Install it via pip:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;  pip3 install pyzmq&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;pafy:&lt;/code&gt;&lt;/strong&gt; Required for direct YouTube Video streaming capabilities. Both &lt;a href="https://github.com/mps-youtube/pafy"&gt;&lt;code&gt;pafy&lt;/code&gt;&lt;/a&gt; and latest only &lt;a href="https://github.com/ytdl-org/youtube-dl/"&gt;&lt;code&gt;youtube-dl&lt;/code&gt;&lt;/a&gt;(&lt;em&gt;as pafy's backend&lt;/em&gt;) libraries must be installed via pip as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;  pip3 install pafy
  pip3 install -U youtube-dl&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-available-installation-options" class="anchor" aria-hidden="true" href="#available-installation-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Available Installation Options:&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-option-1-pypi-install" class="anchor" aria-hidden="true" href="#option-1-pypi-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Option 1: PyPI Install&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Best option for &lt;strong&gt;quickly&lt;/strong&gt; getting VidGear installed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;  pip3 install vidgear&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-option-2-release-archive-download" class="anchor" aria-hidden="true" href="#option-2-release-archive-download"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Option 2: Release Archive Download&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Best option if you want a &lt;strong&gt;compressed archive&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;VidGear releases are available for download as packages in the &lt;a href="https://github.com/abhiTronix/vidgear/releases/latest"&gt;latest release&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-option-3-clone-the-repository" class="anchor" aria-hidden="true" href="#option-3-clone-the-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Option 3: Clone the Repository&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Best option for trying &lt;strong&gt;latest patches(&lt;em&gt;maybe experimental&lt;/em&gt;), Pull Requests&lt;/strong&gt;, or &lt;strong&gt;contributing&lt;/strong&gt; to development.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can clone this repository's &lt;code&gt;testing&lt;/code&gt; branch for development and thereby can install as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt; git clone https://github.com/abhiTronix/vidgear.git
 &lt;span class="pl-c1"&gt;cd&lt;/span&gt; vidgear
 git checkout testing
 sudo pip3 install &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h1&gt;
&lt;p&gt;The full documentation for all VidGear classes and functions can be found in the link below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/abhiTronix/vidgear/wiki"&gt;Wiki Documentation - English&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt; Testing VidGear require some &lt;em&gt;additional dependencies &amp;amp; data&lt;/em&gt; which can be downloaded manually as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Clone &amp;amp; Install &lt;a href="#option-3-clone-the-repo"&gt;Testing Branch&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Download few additional python libraries:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt; pip3 install six
 pip3 install pytest&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Download Test Dataset:&lt;/strong&gt; To perform tests, additional &lt;em&gt;test dataset&lt;/em&gt; is required, which can be downloaded &lt;em&gt;(to temp dir)&lt;/em&gt; by running &lt;a href="https://github.com/abhiTronix/vidgear/blob/testing/scripts/bash/prepare_dataset.sh"&gt;&lt;em&gt;bash script&lt;/em&gt;&lt;/a&gt; as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt; chmod +x scripts/bash/prepare_dataset.sh
 .scripts/bash/prepare_dataset.sh               &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;for Windows, use `sh scripts/bash/prepare_dataset.sh`&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Run Tests:&lt;/strong&gt; Then various VidGear tests can be run with &lt;code&gt;pytest&lt;/code&gt;(&lt;em&gt;in VidGear's root folder&lt;/em&gt;) as below:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt; pytest -sv                                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;-sv for verbose output.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h1&gt;
&lt;p&gt;See &lt;a href="contributing.md"&gt;&lt;strong&gt;contributing.md&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-supported-python-legacies" class="anchor" aria-hidden="true" href="#supported-python-legacies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supported Python legacies&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Python 3+ are only supported legacies for installing Vidgear v0.1.6 and above.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;g-emoji class="g-emoji" alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png"&gt;⚠️&lt;/g-emoji&gt; Python 2.7 legacy support &lt;a href="https://github.com/abhiTronix/vidgear/issues/29"&gt;dropped in v0.1.6&lt;/a&gt;.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h1&gt;
&lt;p&gt;See &lt;a href="changelog.md"&gt;&lt;strong&gt;changelog.md&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Here is a Bibtex entry you can use to cite this project in a publication:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-text-tex-latex"&gt;&lt;pre&gt;@misc{vidgear,
    Title = {vidgear},
    Author = {Abhishek Thakur},
    howpublished = {&lt;span class="pl-c1"&gt;\url&lt;/span&gt;{https://github.com/abhiTronix/vidgear}}   
  }&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Copyright © abhiTronix 2019&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This library is licensed under the &lt;strong&gt;&lt;a href="https://github.com/abhiTronix/vidgear/blob/master/LICENSE"&gt;Apache 2.0 License&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;



&lt;/article&gt;&lt;/div&gt;</description><author>abhiTronix</author><guid isPermaLink="false">https://github.com/abhiTronix/vidgear</guid><pubDate>Sat, 04 Jan 2020 00:14:00 GMT</pubDate></item><item><title>pre-commit/pre-commit #15 in Python, Today</title><link>https://github.com/pre-commit/pre-commit</link><description>&lt;p&gt;&lt;i&gt;A framework for managing and maintaining multi-language pre-commit hooks.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://dev.azure.com/asottile/asottile/_build/latest?definitionId=21&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/77efae7dd03b989344eec8a930a32102bd94f5d4/68747470733a2f2f6465762e617a7572652e636f6d2f61736f7474696c652f61736f7474696c652f5f617069732f6275696c642f7374617475732f7072652d636f6d6d69742e7072652d636f6d6d69743f6272616e63684e616d653d6d6173746572" alt="Build Status" data-canonical-src="https://dev.azure.com/asottile/asottile/_apis/build/status/pre-commit.pre-commit?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://dev.azure.com/asottile/asottile/_build/latest?definitionId=21&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/691a9bcfbc45a261221340f41216946cbc8a9597/68747470733a2f2f696d672e736869656c64732e696f2f617a7572652d6465766f70732f636f7665726167652f61736f7474696c652f61736f7474696c652f32312f6d61737465722e737667" alt="Azure DevOps coverage" data-canonical-src="https://img.shields.io/azure-devops/coverage/asottile/asottile/21/master.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-commit" class="anchor" aria-hidden="true" href="#pre-commit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pre-commit&lt;/h2&gt;
&lt;p&gt;A framework for managing and maintaining multi-language pre-commit hooks.&lt;/p&gt;
&lt;p&gt;For more information see: &lt;a href="https://pre-commit.com/" rel="nofollow"&gt;https://pre-commit.com/&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pre-commit</author><guid isPermaLink="false">https://github.com/pre-commit/pre-commit</guid><pubDate>Sat, 04 Jan 2020 00:15:00 GMT</pubDate></item><item><title>williamFalcon/pytorch-lightning #16 in Python, Today</title><link>https://github.com/williamFalcon/pytorch-lightning</link><description>&lt;p&gt;&lt;i&gt;The lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/source/_static/images/lightning_logo_small.png"&gt;&lt;img src="docs/source/_static/images/lightning_logo_small.png" alt="Logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pytorch-lightning" class="anchor" aria-hidden="true" href="#pytorch-lightning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Lightning&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://badge.fury.io/py/pytorch-lightning" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d25ca5d127f8476c9ecaad392df653c27731a470/68747470733a2f2f62616467652e667572792e696f2f70792f7079746f7263682d6c696768746e696e672e737667" alt="PyPI Status" data-canonical-src="https://badge.fury.io/py/pytorch-lightning.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pepy.tech/project/pytorch-lightning" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/dc8cc8bbf178ec000198332c49f6009a23bb7cd2/68747470733a2f2f706570792e746563682f62616467652f7079746f7263682d6c696768746e696e67" alt="PyPI Status" data-canonical-src="https://pepy.tech/badge/pytorch-lightning" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/williamFalcon/pytorch-lightning" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ce09370f927389d2d30c607c6f0b8a422cb8ae02/68747470733a2f2f7472617669732d63692e6f72672f77696c6c69616d46616c636f6e2f7079746f7263682d6c696768746e696e672e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/williamFalcon/pytorch-lightning.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/williamFalcon/pytorch-lightning" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e819308819d8312b203956b1d2ddc469932115a7/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f4e45572d50524f4a4543542d49443f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/NEW-PROJECT-ID?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/williamFalcon/pytorch-lightning/tree/master/tests#running-coverage"&gt;&lt;img src="docs/source/_static/images/coverage.svg" alt="Coverage" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.codefactor.io/repository/github/borda/pytorch-lightning" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a25dbb85804bffafcf05a87baa47711f8a87f842/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f626f7264612f7079746f7263682d6c696768746e696e672f6261646765" alt="CodeFactor" data-canonical-src="https://www.codefactor.io/repository/github/borda/pytorch-lightning/badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch-lightning.readthedocs.io/en/latest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6cac0339fcf42cdf38ec923d12df75fcc4ed647f/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7079746f7263682d6c696768746e696e672f62616467652f3f76657273696f6e3d6c6174657374" alt="ReadTheDocs" data-canonical-src="https://readthedocs.org/projects/pytorch-lightning/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://join.slack.com/t/pytorch-lightning/shared_invite/enQtODU5ODIyNTUzODQwLTFkMDg5Mzc1MDBmNjEzMDgxOTVmYTdhYjA1MDdmODUyOTg2OGQ1ZWZkYTQzODhhNzdhZDA3YmNhMDhlMDY4YzQ" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/42eaba71efb1cd9904693fee132c8d11d70088d2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d636861742d677265656e2e7376673f6c6f676f3d736c61636b" alt="Slack" data-canonical-src="https://img.shields.io/badge/slack-chat-green.svg?logo=slack" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/williamFalcon/pytorch-lightning/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/8051e9938a1ab39cf002818dfceb6b6092f34d68/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667" alt="license" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://shields.io/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f7fc1fcdca56d91498138ec975f06316a0e4db3d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e65787425323052656c656173652d446563253230362d253343434f4c4f522533452e737667" alt="Next Release" data-canonical-src="https://img.shields.io/badge/Next%20Release-Dec%206-%3CCOLOR%3E.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;p&gt;Simple installation from PyPI&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install pytorch-lightning  &lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-docs" class="anchor" aria-hidden="true" href="#docs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docs&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/" rel="nofollow"&gt;View the docs here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-demo" class="anchor" aria-hidden="true" href="#demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Demo&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=HOk9c4_35FKg" rel="nofollow"&gt;Copy and run this COLAB!&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-it" class="anchor" aria-hidden="true" href="#what-is-it"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is it?&lt;/h2&gt;
&lt;p&gt;Lightning is a very lightweight wrapper on PyTorch that decouples the science code from the engineering code. It's more of a style-guide than a framework. By refactoring your code, we can automate most of the non-research code.&lt;/p&gt;
&lt;p&gt;To use Lightning, simply refactor your research code into the &lt;a href="https://github.com/williamFalcon/pytorch-lightning#how-do-i-do-use-it"&gt;LightningModule&lt;/a&gt; format (the science) and Lightning will automate the rest (the engineering). Lightning guarantees tested, correct, modern best practices for the automated parts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you are a researcher, Lightning is infinitely flexible, you can modify everything down to the way .backward is called or distributed is set up.&lt;/li&gt;
&lt;li&gt;If you are a scientist or production team, lightning is very simple to use with best practice defaults.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-what-does-lightning-control-for-me" class="anchor" aria-hidden="true" href="#what-does-lightning-control-for-me"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What does lightning control for me?&lt;/h2&gt;
&lt;p&gt;Everything in Blue!&lt;br&gt;
This is how lightning separates the science (red) from the engineering (blue).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/source/_static/images/pl.gif"&gt;&lt;img src="docs/source/_static/images/pl.gif" alt="Overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-much-effort-is-it-to-convert" class="anchor" aria-hidden="true" href="#how-much-effort-is-it-to-convert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How much effort is it to convert?&lt;/h2&gt;
&lt;p&gt;You're probably tired of switching frameworks at this point. But it is a very quick process to refactor into the Lightning format (ie: hours). &lt;a href="https://towardsdatascience.com/how-to-refactor-your-pytorch-code-to-get-these-42-benefits-of-pytorch-lighting-6fdd0dc97538" rel="nofollow"&gt;Check out this tutorial&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-starting-a-new-project" class="anchor" aria-hidden="true" href="#starting-a-new-project"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting a new project?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning-conference-seed"&gt;Use our seed-project aimed at reproducibility!&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-why-do-i-want-to-use-lightning" class="anchor" aria-hidden="true" href="#why-do-i-want-to-use-lightning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why do I want to use lightning?&lt;/h2&gt;
&lt;p&gt;Every research project starts the same, a model, a training loop, validation loop, etc. As your research advances, you're likely to need distributed training, 16-bit precision, checkpointing, gradient accumulation, etc.&lt;/p&gt;
&lt;p&gt;Lightning sets up all the boilerplate state-of-the-art training for you so you can focus on the research.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-readme-table-of-contents" class="anchor" aria-hidden="true" href="#readme-table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;README Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning#how-do-i-do-use-it"&gt;How do I use it&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning#what-does-lightning-control-for-me"&gt;What lightning automates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning#tensorboard"&gt;Tensorboard integration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning#lightning-automates-all-of-the-following-each-is-also-configurable"&gt;Lightning features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning#tutorials"&gt;Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning/blob/master/.github/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning#bleeding-edge"&gt;Bleeding edge install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning#lightning-design-principles"&gt;Lightning Design Principles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning#asking-for-help"&gt;Asking for help&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-how-do-i-do-use-it" class="anchor" aria-hidden="true" href="#how-do-i-do-use-it"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I do use it?&lt;/h2&gt;
&lt;p&gt;Think about Lightning as refactoring your research code instead of using a new framework. The research code goes into a &lt;a href="https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/" rel="nofollow"&gt;LightningModule&lt;/a&gt; which you fit using a Trainer.&lt;/p&gt;
&lt;p&gt;The LightningModule defines a &lt;em&gt;system&lt;/em&gt; such as seq-2-seq, GAN, etc... It can ALSO define a simple classifier such as the example below.&lt;/p&gt;
&lt;p&gt;To use lightning do 2 things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/" rel="nofollow"&gt;Define a LightningModule&lt;/a&gt;
&lt;strong&gt;WARNING:&lt;/strong&gt; This syntax is for version 0.5.0+ where abbreviations were removed.
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; os

&lt;span class="pl-k"&gt;import&lt;/span&gt; torch
&lt;span class="pl-k"&gt;from&lt;/span&gt; torch.nn &lt;span class="pl-k"&gt;import&lt;/span&gt; functional &lt;span class="pl-k"&gt;as&lt;/span&gt; F
&lt;span class="pl-k"&gt;from&lt;/span&gt; torch.utils.data &lt;span class="pl-k"&gt;import&lt;/span&gt; DataLoader
&lt;span class="pl-k"&gt;from&lt;/span&gt; torchvision.datasets &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-c1"&gt;MNIST&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; torchvision &lt;span class="pl-k"&gt;import&lt;/span&gt; transforms

&lt;span class="pl-k"&gt;import&lt;/span&gt; pytorch_lightning &lt;span class="pl-k"&gt;as&lt;/span&gt; pl

&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;CoolSystem&lt;/span&gt;(&lt;span class="pl-e"&gt;pl&lt;/span&gt;.&lt;span class="pl-e"&gt;LightningModule&lt;/span&gt;):

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;super&lt;/span&gt;(CoolSystem, &lt;span class="pl-c1"&gt;self&lt;/span&gt;).&lt;span class="pl-c1"&gt;__init__&lt;/span&gt;()
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; not the best model...&lt;/span&gt;
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.l1 &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.nn.Linear(&lt;span class="pl-c1"&gt;28&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt; &lt;span class="pl-c1"&gt;28&lt;/span&gt;, &lt;span class="pl-c1"&gt;10&lt;/span&gt;)

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;forward&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;x&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; torch.relu(&lt;span class="pl-c1"&gt;self&lt;/span&gt;.l1(x.view(x.size(&lt;span class="pl-c1"&gt;0&lt;/span&gt;), &lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;)))

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;training_step&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;batch&lt;/span&gt;, &lt;span class="pl-smi"&gt;batch_idx&lt;/span&gt;):
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; REQUIRED&lt;/span&gt;
        x, y &lt;span class="pl-k"&gt;=&lt;/span&gt; batch
        y_hat &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.forward(x)
        loss &lt;span class="pl-k"&gt;=&lt;/span&gt; F.cross_entropy(y_hat, y)
        tensorboard_logs &lt;span class="pl-k"&gt;=&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_loss&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: loss}
        &lt;span class="pl-k"&gt;return&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;loss&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: loss, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;log&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: tensorboard_logs}

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;validation_step&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;batch&lt;/span&gt;, &lt;span class="pl-smi"&gt;batch_idx&lt;/span&gt;):
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; OPTIONAL&lt;/span&gt;
        x, y &lt;span class="pl-k"&gt;=&lt;/span&gt; batch
        y_hat &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.forward(x)
        &lt;span class="pl-k"&gt;return&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;val_loss&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: F.cross_entropy(y_hat, y)}

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;validation_end&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;outputs&lt;/span&gt;):
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; OPTIONAL&lt;/span&gt;
        avg_loss &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.stack([x[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;val_loss&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;for&lt;/span&gt; x &lt;span class="pl-k"&gt;in&lt;/span&gt; outputs]).mean()
        tensorboard_logs &lt;span class="pl-k"&gt;=&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;val_loss&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: avg_loss}
        &lt;span class="pl-k"&gt;return&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;avg_val_loss&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: avg_loss, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;log&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: tensorboard_logs}

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;configure_optimizers&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; REQUIRED&lt;/span&gt;
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; can return multiple optimizers and learning_rate schedulers&lt;/span&gt;
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; (LBFGS it is automatically supported, no need for closure function)&lt;/span&gt;
        &lt;span class="pl-k"&gt;return&lt;/span&gt; torch.optim.Adam(&lt;span class="pl-c1"&gt;self&lt;/span&gt;.parameters(), &lt;span class="pl-v"&gt;lr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.02&lt;/span&gt;)

    &lt;span class="pl-en"&gt;@pl.data_loader&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;train_dataloader&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; REQUIRED&lt;/span&gt;
        &lt;span class="pl-k"&gt;return&lt;/span&gt; DataLoader(MNIST(os.getcwd(), &lt;span class="pl-v"&gt;train&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;download&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;transform&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;transforms.ToTensor()), &lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;)

    &lt;span class="pl-en"&gt;@pl.data_loader&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;val_dataloader&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; OPTIONAL&lt;/span&gt;
        &lt;span class="pl-k"&gt;return&lt;/span&gt; DataLoader(MNIST(os.getcwd(), &lt;span class="pl-v"&gt;train&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;download&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;transform&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;transforms.ToTensor()), &lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;)

    &lt;span class="pl-en"&gt;@pl.data_loader&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;test_dataloader&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; OPTIONAL&lt;/span&gt;
        &lt;span class="pl-k"&gt;return&lt;/span&gt; DataLoader(MNIST(os.getcwd(), &lt;span class="pl-v"&gt;train&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;, &lt;span class="pl-v"&gt;download&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;transform&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;transforms.ToTensor()), &lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Fit with a &lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/" rel="nofollow"&gt;trainer&lt;/a&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; pytorch_lightning &lt;span class="pl-k"&gt;import&lt;/span&gt; Trainer

model &lt;span class="pl-k"&gt;=&lt;/span&gt; CoolSystem()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; most basic trainer, uses good defaults&lt;/span&gt;
trainer &lt;span class="pl-k"&gt;=&lt;/span&gt; Trainer()    
trainer.fit(model)   &lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Trainer sets up a tensorboard logger, early stopping and checkpointing by default (you can modify all of them or
use something other than tensorboard).&lt;/p&gt;
&lt;p&gt;Here are more advanced examples&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train on cpu using only 10% of the data (for demo purposes)&lt;/span&gt;
trainer &lt;span class="pl-k"&gt;=&lt;/span&gt; Trainer(&lt;span class="pl-v"&gt;max_epochs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-v"&gt;train_percent_check&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.1&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train on 4 gpus (lightning chooses GPUs for you)&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; trainer = Trainer(max_epochs=1, gpus=4, distributed_backend='ddp')  &lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train on 4 gpus (you choose GPUs)&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; trainer = Trainer(max_epochs=1, gpus=[0, 1, 3, 7], distributed_backend='ddp')   &lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train on 32 gpus across 4 nodes (make sure to submit appropriate SLURM job)&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; trainer = Trainer(max_epochs=1, gpus=8, num_gpu_nodes=4, distributed_backend='ddp')&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train (1 epoch only here for demo)&lt;/span&gt;
trainer.fit(model)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; view tensorboard logs &lt;/span&gt;
logging.info(&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;View tensorboard logs by running&lt;/span&gt;&lt;span class="pl-cce"&gt;\n&lt;/span&gt;&lt;span class="pl-s"&gt;tensorboard --logdir &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;os.getcwd()&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;)
logging.info(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;and going to http://localhost:6006 on your browser&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When you're all done you can even run the test set separately.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;trainer.test()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Could be as complex as seq-2-seq + attention&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; define what happens for training here&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;training_step&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;batch&lt;/span&gt;, &lt;span class="pl-smi"&gt;batch_idx&lt;/span&gt;):
    x, y &lt;span class="pl-k"&gt;=&lt;/span&gt; batch
    
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; define your own forward and loss calculation&lt;/span&gt;
    hidden_states &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.encoder(x)
     
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; even as complex as a seq-2-seq + attn model&lt;/span&gt;
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; (this is just a toy, non-working example to illustrate)&lt;/span&gt;
    start_token &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;SOS&amp;gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
    last_hidden &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.zeros(&lt;span class="pl-c1"&gt;...&lt;/span&gt;)
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;
    &lt;span class="pl-k"&gt;for&lt;/span&gt; step &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(max_seq_len):
        attn_context &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.attention_nn(hidden_states, start_token)
        pred &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.decoder(start_token, attn_context, last_hidden) 
        last_hidden &lt;span class="pl-k"&gt;=&lt;/span&gt; pred
        pred &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.predict_nn(pred)
        loss &lt;span class="pl-k"&gt;+=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.loss(last_hidden, y[step])
        
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;toy example as well&lt;/span&gt;
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; loss &lt;span class="pl-k"&gt;/&lt;/span&gt; max_seq_len
    &lt;span class="pl-k"&gt;return&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;loss&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: loss} &lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Or as basic as CNN image classification&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; define what happens for validation here&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;validation_step&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;batch&lt;/span&gt;, &lt;span class="pl-smi"&gt;batch_idx&lt;/span&gt;):    
    x, y &lt;span class="pl-k"&gt;=&lt;/span&gt; batch
    
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; or as basic as a CNN classification&lt;/span&gt;
    out &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.forward(x)
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; my_loss(out, y)
    &lt;span class="pl-k"&gt;return&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;loss&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: loss} &lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;And you also decide how to collate the output of all validation steps&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;validation_end&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;outputs&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;    Called at the end of validation to aggregate outputs&lt;/span&gt;
&lt;span class="pl-s"&gt;    :param outputs: list of individual outputs of each validation step&lt;/span&gt;
&lt;span class="pl-s"&gt;    :return:&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    val_loss_mean &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;
    val_acc_mean &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;
    &lt;span class="pl-k"&gt;for&lt;/span&gt; output &lt;span class="pl-k"&gt;in&lt;/span&gt; outputs:
        val_loss_mean &lt;span class="pl-k"&gt;+=&lt;/span&gt; output[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;val_loss&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
        val_acc_mean &lt;span class="pl-k"&gt;+=&lt;/span&gt; output[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;val_acc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]

    val_loss_mean &lt;span class="pl-k"&gt;/=&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(outputs)
    val_acc_mean &lt;span class="pl-k"&gt;/=&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(outputs)
    logs &lt;span class="pl-k"&gt;=&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;val_loss&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: val_loss_mean.item(), &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;val_acc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: val_acc_mean.item()}
    result &lt;span class="pl-k"&gt;=&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;log&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: logs}
    &lt;span class="pl-k"&gt;return&lt;/span&gt; result&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-tensorboard" class="anchor" aria-hidden="true" href="#tensorboard"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tensorboard&lt;/h2&gt;
&lt;p&gt;Lightning is fully integrated with tensorboard, MLFlow and supports any logging module.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/source/_static/images/tf_loss.png"&gt;&lt;img src="docs/source/_static/images/tf_loss.png" alt="tensorboard-support" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lightning also adds a text column with all the hyperparameters for this experiment.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/source/_static/images/tf_tags.png"&gt;&lt;img src="docs/source/_static/images/tf_tags.png" alt="tensorboard-support" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-lightning-automates-all-of-the-following-each-is-also-configurable" class="anchor" aria-hidden="true" href="#lightning-automates-all-of-the-following-each-is-also-configurable"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lightning automates all of the following (&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/" rel="nofollow"&gt;each is also configurable&lt;/a&gt;):&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-checkpointing" class="anchor" aria-hidden="true" href="#checkpointing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Checkpointing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Checkpointing/#model-saving" rel="nofollow"&gt;Checkpoint callback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Checkpointing/#model-saving" rel="nofollow"&gt;Model saving&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/LightningModule/methods/#load-from-metrics" rel="nofollow"&gt;Model loading&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Checkpointing/#restoring-training-session" rel="nofollow"&gt;Restoring training session&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-computing-cluster-slurm" class="anchor" aria-hidden="true" href="#computing-cluster-slurm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Computing cluster (SLURM)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/SLURM%20Managed%20Cluster#running-grid-search-on-a-cluster" rel="nofollow"&gt;Running grid search on a cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/SLURM%20Managed%20Cluster#walltime-auto-resubmit" rel="nofollow"&gt;Walltime auto-resubmit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-debugging" class="anchor" aria-hidden="true" href="#debugging"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Debugging&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/debugging/#fast-dev-run" rel="nofollow"&gt;Fast dev run&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/debugging/#inspect-gradient-norms" rel="nofollow"&gt;Inspect gradient norms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/debugging/#Log-gpu-usage" rel="nofollow"&gt;Log GPU usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/debugging/#make-model-overfit-on-subset-of-data" rel="nofollow"&gt;Make model overfit on subset of data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/debugging/#print-the-parameter-count-by-layer" rel="nofollow"&gt;Print the parameter count by layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/debugging/#print-which-gradients-are-nan" rel="nofollow"&gt;Print which gradients are nan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/LightningModule/properties/#example_input_array" rel="nofollow"&gt;Print input and output size of every module in system&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-distributed-training" class="anchor" aria-hidden="true" href="#distributed-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Distributed training&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/hooks/#init_ddp_connection" rel="nofollow"&gt;Implement Your Own Distributed (DDP) training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Distributed%20training/#16-bit-mixed-precision" rel="nofollow"&gt;16-bit mixed precision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Distributed%20training/#Multi-GPU" rel="nofollow"&gt;Multi-GPU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Distributed%20training/#Multi-node" rel="nofollow"&gt;Multi-node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Distributed%20training/#single-gpu" rel="nofollow"&gt;Single GPU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Distributed%20training/#self-balancing-architecture" rel="nofollow"&gt;Self-balancing architecture&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-experiment-logging" class="anchor" aria-hidden="true" href="#experiment-logging"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Experiment Logging&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Logging/#display-metrics-in-progress-bar" rel="nofollow"&gt;Display metrics in progress bar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Logging/#log-metric-row-every-k-batches" rel="nofollow"&gt;Log metric row every k batches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Logging/#process-position" rel="nofollow"&gt;Process position&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Logging/#tensorboard-support" rel="nofollow"&gt;Tensorboard support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Logging/#save-a-snapshot-of-all-hyperparameters" rel="nofollow"&gt;Save a snapshot of all hyperparameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Logging/#snapshot-code-for-a-training-run" rel="nofollow"&gt;Snapshot code for a training run&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Logging/#write-logs-file-to-csv-every-k-batches" rel="nofollow"&gt;Write logs file to csv every k batches&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-loop" class="anchor" aria-hidden="true" href="#training-loop"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training loop&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Training%20Loop/#accumulated-gradients" rel="nofollow"&gt;Accumulate gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Training%20Loop/#force-training-for-min-or-max-epochs" rel="nofollow"&gt;Force training for min or max epochs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Training%20Loop/#early-stopping" rel="nofollow"&gt;Early stopping callback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Training%20Loop/#force-disable-early-stop" rel="nofollow"&gt;Force disable early stop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Training%20Loop/#gradient-clipping" rel="nofollow"&gt;Gradient Clipping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/hooks/" rel="nofollow"&gt;Hooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/#configure_optimizers" rel="nofollow"&gt;Learning rate scheduling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/#configure_optimizers" rel="nofollow"&gt;Use multiple optimizers (like GANs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Training%20Loop/#set-how-much-of-the-training-set-to-check" rel="nofollow"&gt;Set how much of the training set to check (1-100%)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/hooks/#optimizer_step" rel="nofollow"&gt;Step optimizers at arbitrary intervals&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-validation-loop" class="anchor" aria-hidden="true" href="#validation-loop"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Validation loop&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Validation%20loop/#check-validation-every-n-epochs" rel="nofollow"&gt;Check validation every n epochs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/hooks/" rel="nofollow"&gt;Hooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Validation%20loop/#set-how-much-of-the-validation-set-to-check" rel="nofollow"&gt;Set how much of the validation set to check&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Validation%20loop/#set-how-much-of-the-test-set-to-check" rel="nofollow"&gt;Set how much of the test set to check&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Validation%20loop/#set-validation-check-frequency-within-1-training-epoch" rel="nofollow"&gt;Set validation check frequency within 1 training epoch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Validation%20loop/#set-the-number-of-validation-sanity-steps" rel="nofollow"&gt;Set the number of validation sanity steps&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-testing-loop" class="anchor" aria-hidden="true" href="#testing-loop"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing loop&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/Trainer/Testing%20loop/" rel="nofollow"&gt;Run test set&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning/tree/master/pl_examples/domain_templates/gan.py"&gt;GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning/tree/master/pl_examples/basic_examples"&gt;MNIST&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning/network/dependents?package_id=UGFja2FnZS0zNzE3NDU4OTM%3D"&gt;Other projects using Lightning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning/tree/master/pl_examples/multi_node_examples"&gt;Multi-node&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/supercharge-your-ai-research-with-pytorch-lightning-337948a99eec" rel="nofollow"&gt;Basic Lightning use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/9-tips-for-training-lightning-fast-neural-networks-in-pytorch-8e63a502f565" rel="nofollow"&gt;9 key speed features in Pytorch-Lightning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/trivial-multi-node-training-with-pytorch-lightning-ff75dfb809bd" rel="nofollow"&gt;SLURM, multi-node training with Lightning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-asking-for-help" class="anchor" aria-hidden="true" href="#asking-for-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Asking for help&lt;/h2&gt;
&lt;p&gt;Welcome to the Lightning community!&lt;/p&gt;
&lt;p&gt;If you have any questions, feel free to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://williamfalcon.github.io/pytorch-lightning/" rel="nofollow"&gt;read the docs&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/williamFalcon/pytorch-lightning/issues?utf8=%E2%9C%93&amp;amp;q=my++question"&gt;Search through the issues&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/ask?guided=false" rel="nofollow"&gt;Ask on stackoverflow&lt;/a&gt; with the tag pytorch-lightning.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If no one replies to you quickly enough, feel free to post the stackoverflow link to our Gitter chat!&lt;/p&gt;
&lt;p&gt;To chat with the rest of us visit our &lt;a href="https://gitter.im/PyTorch-Lightning/community" rel="nofollow"&gt;gitter channel&lt;/a&gt;!&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;How do I use Lightning for rapid research?&lt;/strong&gt;&lt;br&gt;
&lt;a href="https://williamfalcon.github.io/pytorch-lightning/" rel="nofollow"&gt;Here's a walk-through&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why was Lightning created?&lt;/strong&gt;&lt;br&gt;
Lightning has 3 goals in mind:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Maximal flexibility while abstracting out the common boilerplate across research projects.&lt;/li&gt;
&lt;li&gt;Reproducibility. If all projects use the LightningModule template, it will be much much easier to understand what's going on and where to look! It will also mean every implementation follows a standard format.&lt;/li&gt;
&lt;li&gt;Democratizing PyTorch power user features. Distributed training? 16-bit? know you need them but don't want to take the time to implement? All good... these come built into Lightning.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;How does Lightning compare with Ignite and fast.ai?&lt;/strong&gt;&lt;br&gt;
&lt;a href="https://medium.com/@_willfalcon/pytorch-lightning-vs-pytorch-ignite-vs-fast-ai-61dc7480ad8a" rel="nofollow"&gt;Here's a thorough comparison&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is this another library I have to learn?&lt;/strong&gt;&lt;br&gt;
Nope! We use pure Pytorch everywhere and don't add unecessary abstractions!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Are there plans to support Python 2?&lt;/strong&gt;&lt;br&gt;
Nope.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Are there plans to support virtualenv?&lt;/strong&gt;&lt;br&gt;
Nope. Please use anaconda or miniconda.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Which PyTorch versions do you support?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PyTorch 1.1.0&lt;/strong&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install pytorch 1.1.0 using the official instructions   &lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install test-tube 0.6.7.6 which supports 1.1.0   &lt;/span&gt;
pip install test-tube==0.6.7.6   

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install latest Lightning version without upgrading deps    &lt;/span&gt;
pip install -U --no-deps pytorch-lightning&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch 1.2.0, 1.3.0,&lt;/strong&gt;
Install via pip as normal&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-custom-installation" class="anchor" aria-hidden="true" href="#custom-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom installation&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-bleeding-edge" class="anchor" aria-hidden="true" href="#bleeding-edge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bleeding edge&lt;/h3&gt;
&lt;p&gt;If you can't wait for the next release, install the most up to date code with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;using GIT (locally clone whole repo with full history)
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install git+https://github.com/williamFalcon/pytorch-lightning.git@master --upgrade&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;using instant zip (last state of the repo without git history)
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install https://github.com/williamFalcon/pytorch-lightning/archive/master.zip --upgrade&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-any-release-installation" class="anchor" aria-hidden="true" href="#any-release-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Any release installation&lt;/h3&gt;
&lt;p&gt;You can also install any past release from this repository:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install https://github.com/williamFalcon/pytorch-lightning/archive/0.4.4.zip --upgrade&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-bibtex" class="anchor" aria-hidden="true" href="#bibtex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bibtex&lt;/h2&gt;
&lt;p&gt;If you want to cite the framework feel free to use this (but only if you loved it &lt;g-emoji class="g-emoji" alias="blush" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60a.png"&gt;😊&lt;/g-emoji&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@misc{Falcon2019,
  author = {Falcon, W.A.},
  title = {PyTorch Lightning},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/williamFalcon/pytorch-lightning}}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>williamFalcon</author><guid isPermaLink="false">https://github.com/williamFalcon/pytorch-lightning</guid><pubDate>Sat, 04 Jan 2020 00:16:00 GMT</pubDate></item><item><title>Chakazul/Lenia #17 in Python, Today</title><link>https://github.com/Chakazul/Lenia</link><description>&lt;p&gt;&lt;i&gt;Lenia - Mathematical Life Forms&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/Chakazul/Lenia/blob/master/Screencap/icon2.png"&gt;&lt;img src="https://github.com/Chakazul/Lenia/raw/master/Screencap/icon2.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[2019-12-14] Lifeforms found in 3D and 4D Lenia
&lt;a href="https://twitter.com/BertChakovsky/status/1211010576631005184" rel="nofollow"&gt;https://twitter.com/BertChakovsky/status/1211010576631005184&lt;/a&gt;
&lt;a href="https://twitter.com/BertChakovsky/status/1209177017096163328" rel="nofollow"&gt;https://twitter.com/BertChakovsky/status/1209177017096163328&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[2019-10-16] Paper published in Complex Systems journal at &lt;a href="https://www.complex-systems.com/abstracts/v28_i03_a01/" rel="nofollow"&gt;https://www.complex-systems.com/abstracts/v28_i03_a01/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[2019-05-04] Preprint paper updated on arXiv, accepted by Complex Systems journal and waiting for publication&lt;/li&gt;
&lt;li&gt;[2018-12-27] Preprint paper "Lenia - Biology of Artificial Life" available at &lt;a href="https://arxiv.org/abs/1812.05433" rel="nofollow"&gt;https://arxiv.org/abs/1812.05433&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[2018-07-19] Ready to run in your browser!  &lt;a href="https://chakazul.github.io/Lenia/JavaScript/Lenia.html" rel="nofollow"&gt;https://chakazul.github.io/Lenia/JavaScript/Lenia.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-lenia" class="anchor" aria-hidden="true" href="#lenia"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lenia&lt;/h1&gt;
&lt;p&gt;Lenia is a 2D &lt;a href="https://en.wikipedia.org/wiki/Cellular_automaton" rel="nofollow"&gt;cellular automata&lt;/a&gt; with continuous space, time and states. It produces a huge variety of interesting life forms.&lt;/p&gt;
&lt;p&gt;There are various versions available. Python, Matlab and web (JavaScript) versions are real-time, interactive, and equipped with statistics tools. Jupyter and R versions are non-interactive and just for demonstration purposes.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-showcase-video" class="anchor" aria-hidden="true" href="#showcase-video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Showcase video&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://vimeo.com/277328815" rel="nofollow"&gt;&lt;img src="https://github.com/Chakazul/Lenia/raw/master/Screencap/Video.png" alt="screen cap" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://vimeo.com/277328815" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ab220fad362dbf41cd2eb2e4e1687436f1b527c9/68747470733a2f2f6368616b617a756c2e6769746875622e696f2f69636f6e732f76696d656f2e706e67" alt="Watch in Vimeo" data-canonical-src="https://chakazul.github.io/icons/vimeo.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://youtu.be/iE46jKYcI4Y" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/18814c6ed2177a12bf8ec5b2e2412a5e28f0cd2e/68747470733a2f2f6368616b617a756c2e6769746875622e696f2f69636f6e732f796f75747562652e706e67" alt="Watch in YouTube" data-canonical-src="https://chakazul.github.io/icons/youtube.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-python-version" class="anchor" aria-hidden="true" href="#python-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Version&lt;/h2&gt;
&lt;p&gt;Fastest version, minimalist layout. Now with GPU support! (Needs Python3 and various libraries)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/Chakazul/Lenia/blob/master/Screencap/Python3.png"&gt;&lt;img src="https://github.com/Chakazul/Lenia/raw/master/Screencap/Python3.png" alt="screen cap" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/Chakazul/Lenia/blob/master/Screencap/Python4.png"&gt;&lt;img src="https://github.com/Chakazul/Lenia/raw/master/Screencap/Python4.png" alt="screen cap" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/Chakazul/Lenia/blob/master/Screencap/Python-stats.png"&gt;&lt;img src="https://github.com/Chakazul/Lenia/raw/master/Screencap/Python-stats.png" alt="screen cap" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-matlab-version" class="anchor" aria-hidden="true" href="#matlab-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Matlab Version&lt;/h2&gt;
&lt;p&gt;Fast version, great tools for statistical analysis. (Needs purchased copy of Matlab)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/Chakazul/Lenia/blob/master/Screencap/Matlab.png"&gt;&lt;img src="https://github.com/Chakazul/Lenia/raw/master/Screencap/Matlab.png" alt="screen cap" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-javascript-version" class="anchor" aria-hidden="true" href="#javascript-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;JavaScript Version&lt;/h2&gt;
&lt;p&gt;The original program, slow but with most features.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/Chakazul/Lenia/blob/master/Screencap/JavaScript.png"&gt;&lt;img src="https://github.com/Chakazul/Lenia/raw/master/Screencap/JavaScript.png" alt="screen cap 1" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/Chakazul/Lenia/blob/master/Screencap/JavaScript2.png"&gt;&lt;img src="https://github.com/Chakazul/Lenia/raw/master/Screencap/JavaScript2.png" alt="screen cap 2" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/Chakazul/Lenia/blob/master/Screencap/JavaScript3.png"&gt;&lt;img src="https://github.com/Chakazul/Lenia/raw/master/Screencap/JavaScript3.png" alt="screen cap 3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3D rendering using plot.ly&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/Chakazul/Lenia/blob/master/Screencap/orbium-ezgif.gif"&gt;&lt;img src="https://github.com/Chakazul/Lenia/raw/master/Screencap/orbium-ezgif.gif" alt="orbium" style="max-width:100%;"&gt;&lt;/a&gt;      
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/Chakazul/Lenia/blob/master/Screencap/gyrorbium-ezgif.gif"&gt;&lt;img src="https://github.com/Chakazul/Lenia/raw/master/Screencap/gyrorbium-ezgif.gif" alt="gyrorbium" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Chakazul</author><guid isPermaLink="false">https://github.com/Chakazul/Lenia</guid><pubDate>Sat, 04 Jan 2020 00:17:00 GMT</pubDate></item><item><title>microsoft/nlp-recipes #18 in Python, Today</title><link>https://github.com/microsoft/nlp-recipes</link><description>&lt;p&gt;&lt;i&gt;Natural Language Processing Best Practices &amp; Examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="NLP-Logo.png"&gt;&lt;img src="NLP-Logo.png" align="right" alt="" width="300" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-nlp-best-practices" class="anchor" aria-hidden="true" href="#nlp-best-practices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP Best Practices&lt;/h1&gt;
&lt;p&gt;In recent years, natural language processing (NLP) has seen quick growth in quality and usability, and this has helped to drive business adoption of artificial intelligence (AI) solutions. In the last few years, researchers have been applying newer deep learning methods to NLP. Data scientists started moving from traditional methods to state-of-the-art (SOTA) deep neural network (DNN) algorithms which use language models pretrained on large text corpora.&lt;/p&gt;
&lt;p&gt;This repository contains examples and best practices for building NLP systems, provided as &lt;a href="examples"&gt;Jupyter notebooks&lt;/a&gt; and &lt;a href="utils_nlp"&gt;utility functions&lt;/a&gt;. The focus of the repository is on state-of-the-art methods and common scenarios that are popular among researchers and practitioners working on problems involving text and language.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h2&gt;
&lt;p&gt;The goal of this repository is to build a comprehensive set of tools and examples that leverage recent advances in NLP algorithms, neural architectures, and distributed machine learning systems.
The content is based on our past and potential future engagements with customers as well as collaboration with partners, researchers, and the open source community.&lt;/p&gt;
&lt;p&gt;We hope that the tools can significantly reduce the “time to market” by simplifying the experience from defining the business problem to development of solution by orders of magnitude. In addition, the example notebooks would serve as guidelines and showcase best practices and usage of the tools in a wide variety of languages.&lt;/p&gt;
&lt;p&gt;In an era of transfer learning, transformers, and deep architectures, we believe that pretrained models provide a unified solution to many real-world problems and allow handling different tasks and languages easily. We will, therefore, prioritize such models, as they achieve state-of-the-art results on several NLP benchmarks like &lt;a href="https://gluebenchmark.com/leaderboard" rel="nofollow"&gt;&lt;em&gt;GLUE&lt;/em&gt;&lt;/a&gt; and &lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;&lt;em&gt;SQuAD&lt;/em&gt;&lt;/a&gt; leaderboards. The models can be used in a number of applications ranging from simple text classification to sophisticated intelligent chat bots.&lt;/p&gt;
&lt;p&gt;Note that for certain kind of NLP problems, you may not need to build your own models. Instead, pre-built or easily customizable solutions exist which do not require any custom coding or machine learning expertise. We strongly recommend evaluating if these can sufficiently solve your problem. If these solutions are not applicable, or the accuracy of these solutions is not sufficient, then resorting to more complex and time-consuming custom approaches may be necessary. The following cognitive services offer simple solutions to address common NLP tasks:
&lt;br&gt;&lt;br&gt;&lt;b&gt;&lt;a href="https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/" rel="nofollow"&gt;Text Analytics&lt;/a&gt; &lt;/b&gt; are a set of pre-trained REST APIs which can be called for Sentiment Analysis, Key phrase extraction, Language detection and Named Entity Detection and more. These APIs work out of the box and require minimal expertise in machine learning, but have limited customization capabilities.
&lt;br&gt;&lt;br&gt;&lt;b&gt;&lt;a href="https://azure.microsoft.com/en-us/services/cognitive-services/qna-maker/" rel="nofollow"&gt;QnA Maker&lt;/a&gt; &lt;/b&gt;is a cloud-based API service that lets you create a conversational question-and-answer layer over your existing data. Use it to build a knowledge base by extracting questions and answers from your semi-structured content, including FAQs, manuals, and documents.
&lt;br&gt;&lt;br&gt;&lt;b&gt;&lt;a href="https://azure.microsoft.com/en-us/services/cognitive-services/language-understanding-intelligent-service/" rel="nofollow"&gt;Language Understanding&lt;/a&gt;&lt;/b&gt; is a SaaS service to train and deploy a model as a REST API given a user-provided training set. You could do Intent Classification as well as Named Entity Extraction by performing simple steps of providing example utterances and labelling them. It supports Active Learning, so your model always keeps learning and improving.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-target-audience" class="anchor" aria-hidden="true" href="#target-audience"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Target Audience&lt;/h2&gt;
&lt;p&gt;For this repository our target audience includes data scientists and machine learning engineers with varying levels of NLP knowledge as our content is source-only and targets custom machine learning modelling. The utilities and examples provided are intended to be solution accelerators for real-world NLP problems.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-focus-areas" class="anchor" aria-hidden="true" href="#focus-areas"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Focus areas&lt;/h2&gt;
&lt;p&gt;The repository aims to expand NLP capabilities along three separate dimensions&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-scenarios" class="anchor" aria-hidden="true" href="#scenarios"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Scenarios&lt;/h3&gt;
&lt;p&gt;We aim to have end-to-end examples of common tasks and scenarios such as text classification, named entity recognition etc.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-algorithms" class="anchor" aria-hidden="true" href="#algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Algorithms&lt;/h3&gt;
&lt;p&gt;We aim to support multiple models for each of the supported scenarios. Currently, transformer-based models are supported across most scenarios. We have been working on integrating the &lt;a href="https://github.com/huggingface/transformers"&gt;transformers package&lt;/a&gt; from &lt;a href="https://huggingface.co/" rel="nofollow"&gt;Hugging Face&lt;/a&gt; which allows users to easily load pretrained models and fine-tune them for different tasks.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-languages" class="anchor" aria-hidden="true" href="#languages"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Languages&lt;/h3&gt;
&lt;p&gt;We strongly subscribe to the multi-language principles laid down by &lt;a href="http://faculty.washington.edu/ebender/papers/Bender-SDSS-2019.pdf" rel="nofollow"&gt;"Emily Bender"&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"Natural language is not a synonym for English"&lt;/li&gt;
&lt;li&gt;"English isn't generic for language, despite what NLP papers might lead you to believe"&lt;/li&gt;
&lt;li&gt;"Always name the language you are working on" (&lt;a href="https://www.aclweb.org/anthology/Q18-1041/" rel="nofollow"&gt;Bender rule&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The repository aims to support non-English languages  across all the scenarios. Pre-trianed models used in the repository such as BERT, FastText support 100+ languages out of the box. Our goal is to provide end-to-end examples in as many languages as possible. We encourage community contributions in this area.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-content" class="anchor" aria-hidden="true" href="#content"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Content&lt;/h2&gt;
&lt;p&gt;The following is a summary of the commonly used NLP scenarios covered in the repository. Each scenario is demonstrated in one or more &lt;a href="examples"&gt;Jupyter notebook examples&lt;/a&gt; that make use of the core code base of models and repository utilities.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Scenario&lt;/th&gt;
&lt;th&gt;Models&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Languages&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Text Classification&lt;/td&gt;
&lt;td&gt;BERT, XLNet, RoBERTa&lt;/td&gt;
&lt;td&gt;Text classification is a supervised learning method of learning and predicting the category or the class of a document given its text content.&lt;/td&gt;
&lt;td&gt;English, Hindi, Arabic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Named Entity Recognition&lt;/td&gt;
&lt;td&gt;BERT&lt;/td&gt;
&lt;td&gt;Named entity recognition (NER) is the task of classifying words or key phrases of a text into predefined entities of interest.&lt;/td&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Entailment&lt;/td&gt;
&lt;td&gt;BERT, XLNet, RoBERTa&lt;/td&gt;
&lt;td&gt;Textual entailment is the task of classifying the binary relation between two natural-language texts,  &lt;em&gt;text&lt;/em&gt; and &lt;em&gt;hypothesis&lt;/em&gt;, to determine if the &lt;em&gt;text&lt;/em&gt; agrees with the &lt;em&gt;hypothesis&lt;/em&gt; or not.&lt;/td&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Question Answering&lt;/td&gt;
&lt;td&gt;BiDAF, BERT, XLNet&lt;/td&gt;
&lt;td&gt;Question answering (QA) is the task of retrieving or generating a valid answer for a given query in natural language, provided with a passage related to the query.&lt;/td&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sentence Similarity&lt;/td&gt;
&lt;td&gt;BERT, GenSen&lt;/td&gt;
&lt;td&gt;Sentence similarity is the process of computing a similarity score given a pair of text documents.&lt;/td&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Embeddings&lt;/td&gt;
&lt;td&gt;Word2Vec&lt;br&gt;fastText&lt;br&gt;GloVe&lt;/td&gt;
&lt;td&gt;Embedding is the process of converting a word or a piece of text to a continuous vector space of real number, usually, in low dimension.&lt;/td&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sentiment Analysis&lt;/td&gt;
&lt;td&gt;Dependency Parser &lt;br&gt;GloVe&lt;/td&gt;
&lt;td&gt;Provides an example of train and use Aspect Based Sentiment Analysis with Azure ML and &lt;a href="http://nlp_architect.nervanasys.com/absa.html" rel="nofollow"&gt;Intel NLP Architect&lt;/a&gt; .&lt;/td&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;While solving NLP problems, it is always good to start with the prebuilt &lt;a href="https://azure.microsoft.com/en-us/services/cognitive-services/directory/lang/" rel="nofollow"&gt;Cognitive Services&lt;/a&gt;. When the needs are beyond the bounds of the prebuilt cognitive service and when you want to search for custom machine learning methods,  you will find this repository  very useful. To get started, navigate to the &lt;a href="SETUP.md"&gt;Setup Guide&lt;/a&gt;, which lists instructions on how to setup your environment and dependencies.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-azure-machine-learning-service" class="anchor" aria-hidden="true" href="#azure-machine-learning-service"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Azure Machine Learning service&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://azure.microsoft.com/en-us/services/machine-learning-service/" rel="nofollow"&gt;Azure Machine Learning service&lt;/a&gt; is a cloud service used to train, deploy, automate, and manage machine learning models, all at the broad scale that the cloud provides. AzureML is presented in notebooks across different scenarios to enhance the efficiency of developing Natural Language systems at scale and for various AI model development related tasks like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data" rel="nofollow"&gt;&lt;strong&gt;Accessing Datastores&lt;/strong&gt;&lt;/a&gt; to easily read and write your data in Azure storage services such as blob storage or file share.&lt;/li&gt;
&lt;li&gt;Scaling up and out on &lt;a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute" rel="nofollow"&gt;&lt;strong&gt;Azure Machine Learning Compute&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train" rel="nofollow"&gt;&lt;strong&gt;Automated Machine Learning&lt;/strong&gt;&lt;/a&gt; which builds high quality machine learning models by automating model and hyperparameter selection. AutoML explores BERT, BiLSTM, bag-of-words, and word embeddings on the user's dataset to handle text columns.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-track-experiments" rel="nofollow"&gt;&lt;strong&gt;Tracking experiments and monitoring metrics&lt;/strong&gt;&lt;/a&gt; to enhance the model creation process.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-ml-models#distributed-training-and-custom-docker-images" rel="nofollow"&gt;&lt;strong&gt;Distributed Training&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters" rel="nofollow"&gt;&lt;strong&gt;Hyperparameter tuning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deploying the trained machine learning model as a web service to &lt;a href="https://azure.microsoft.com/en-us/services/container-instances/" rel="nofollow"&gt;&lt;strong&gt;Azure Container Instance&lt;/strong&gt;&lt;/a&gt; for deveopment and test,  or for low scale, CPU-based workloads.&lt;/li&gt;
&lt;li&gt;Deploying the trained machine learning model as a web service to &lt;a href="https://azure.microsoft.com/en-us/services/kubernetes-service/" rel="nofollow"&gt;&lt;strong&gt;Azure Kubernetes Service&lt;/strong&gt;&lt;/a&gt; for high-scale production deployments and provides autoscaling, and fast response times.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To successfully run these notebooks, you will need an &lt;a href="https://azure.microsoft.com/en-us/" rel="nofollow"&gt;&lt;strong&gt;Azure subscription&lt;/strong&gt;&lt;/a&gt; or can &lt;a href="https://azure.microsoft.com/en-us/free/" rel="nofollow"&gt;&lt;strong&gt;try Azure for free&lt;/strong&gt;&lt;/a&gt;. There may be other Azure services or products used in the notebooks. Introduction and/or reference of those will be provided in the notebooks themselves.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We hope that the open source community would contribute to the content and bring in the latest SOTA algorithm. This project welcomes contributions and suggestions. Before contributing, please see our &lt;a href="CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;p&gt;The following is a list of related repositories that we like and think are useful for NLP tasks.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A great PyTorch library from Hugging Face with implementations of popular transformer-based models. We've been using their package extensively in this repo and greatly appreciate their effort.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/Azure/MachineLearningNotebooks/"&gt;Azure Machine Learning Notebooks&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ML and deep learning examples with Azure Machine Learning.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/Microsoft/AzureML-BERT"&gt;AzureML-BERT&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;End-to-end recipes for pre-training and fine-tuning BERT using Azure Machine Learning service.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/microsoft/MASS"&gt;MASS&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;MASS: Masked Sequence to Sequence Pre-training for Language Generation.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/namisan/mt-dnn"&gt;MT-DNN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Multi-Task Deep Neural Networks for Natural Language Understanding.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/microsoft/unilm"&gt;UniLM&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Unified Language Model Pre-training.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-build-status" class="anchor" aria-hidden="true" href="#build-status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build Status&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Build&lt;/th&gt;
&lt;th&gt;Branch&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux CPU&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;td&gt;&lt;a href="https://dev.azure.com/best-practices/nlp/_build/latest?definitionId=50&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a596d9d36c1eb7fb78ba58ecba08c9fe507f5a52/68747470733a2f2f6465762e617a7572652e636f6d2f626573742d7072616374696365732f6e6c702f5f617069732f6275696c642f7374617475732f6370755f696e746567726174696f6e5f74657374735f6c696e75783f6272616e63684e616d653d6d6173746572" alt="Build Status" data-canonical-src="https://dev.azure.com/best-practices/nlp/_apis/build/status/cpu_integration_tests_linux?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux CPU&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;staging&lt;/td&gt;
&lt;td&gt;&lt;a href="https://dev.azure.com/best-practices/nlp/_build/latest?definitionId=50&amp;amp;branchName=staging" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/078a4b1cb2cf83563e2f18cb89045bde266d018d/68747470733a2f2f6465762e617a7572652e636f6d2f626573742d7072616374696365732f6e6c702f5f617069732f6275696c642f7374617475732f6370755f696e746567726174696f6e5f74657374735f6c696e75783f6272616e63684e616d653d73746167696e67" alt="Build Status" data-canonical-src="https://dev.azure.com/best-practices/nlp/_apis/build/status/cpu_integration_tests_linux?branchName=staging" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux GPU&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;td&gt;&lt;a href="https://dev.azure.com/best-practices/nlp/_build/latest?definitionId=51&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/83c620b2297b4790ef5625f08323c6bfcb83a335/68747470733a2f2f6465762e617a7572652e636f6d2f626573742d7072616374696365732f6e6c702f5f617069732f6275696c642f7374617475732f6770755f696e746567726174696f6e5f74657374735f6c696e75783f6272616e63684e616d653d6d6173746572" alt="Build Status" data-canonical-src="https://dev.azure.com/best-practices/nlp/_apis/build/status/gpu_integration_tests_linux?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux GPU&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;staging&lt;/td&gt;
&lt;td&gt;&lt;a href="https://dev.azure.com/best-practices/nlp/_build/latest?definitionId=51&amp;amp;branchName=staging" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3b516ad5ded4f921fa1752ef02f789591e84aca5/68747470733a2f2f6465762e617a7572652e636f6d2f626573742d7072616374696365732f6e6c702f5f617069732f6275696c642f7374617475732f6770755f696e746567726174696f6e5f74657374735f6c696e75783f6272616e63684e616d653d73746167696e67" alt="Build Status" data-canonical-src="https://dev.azure.com/best-practices/nlp/_apis/build/status/gpu_integration_tests_linux?branchName=staging" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>microsoft</author><guid isPermaLink="false">https://github.com/microsoft/nlp-recipes</guid><pubDate>Sat, 04 Jan 2020 00:18:00 GMT</pubDate></item><item><title>clovaai/ext_portrait_segmentation #19 in Python, Today</title><link>https://github.com/clovaai/ext_portrait_segmentation</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-extreme-lightweight-portrait-segmentation" class="anchor" aria-hidden="true" href="#extreme-lightweight-portrait-segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Extreme Lightweight Portrait Segmentation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;python 3&lt;/li&gt;
&lt;li&gt;pytorch &amp;gt;= 0.4.1&lt;/li&gt;
&lt;li&gt;torchvision==0.2.1&lt;/li&gt;
&lt;li&gt;opencv-python==3.4.2.17&lt;/li&gt;
&lt;li&gt;numpy&lt;/li&gt;
&lt;li&gt;tensorflow &amp;gt;=1.13.0&lt;/li&gt;
&lt;li&gt;visdom&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-model" class="anchor" aria-hidden="true" href="#model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model&lt;/h2&gt;
&lt;p&gt;ExtremeC3Net (&lt;a href="https://arxiv.org/abs/1908.03093" rel="nofollow"&gt;paper&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Hyojin Park, Lars Lowe Sjösund, YoungJoon Yoo, Jihwan Bang, Nojun Kwak.&lt;/p&gt;
&lt;p&gt;"ExtremeC3Net: Extreme Lightweight Portrait Segmentation Networks using Advanced C3-modules"&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;config file : extremeC3Net.json&lt;/li&gt;
&lt;li&gt;Param : 0.038 M&lt;/li&gt;
&lt;li&gt;Flop : 0.128 G&lt;/li&gt;
&lt;li&gt;IoU : 94.98&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SINet (&lt;a href="https://arxiv.org/abs/1911.09099" rel="nofollow"&gt;paper&lt;/a&gt;) Accepted in WACV2020&lt;/p&gt;
&lt;p&gt;Hyojin Park, Lars Lowe Sjösund, YoungJoon Yoo, Nicolas Monet, Jihwan Bang, Nojun Kwak&lt;/p&gt;
&lt;p&gt;SINet: Extreme Lightweight Portrait Segmentation Networks with Spatial Squeeze Modules and Information Blocking Decoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;config file : SINet.json&lt;/li&gt;
&lt;li&gt;Param : 0.087 M&lt;/li&gt;
&lt;li&gt;Flop : 0.064 G&lt;/li&gt;
&lt;li&gt;IoU : 95.2&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-run-example" class="anchor" aria-hidden="true" href="#run-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run example&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Preparing dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download datasets
if you use audgmented dataset, fix the code in dataloader.py in line 20 depending on location of augmented dataset.
Also, please make different pickle file for Augmented dataset and baseline dataset.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1 . ExtremeC3Net&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python main.py --c ExtremeC3Net.json&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;2 . SINet&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python main.py --c SINet.json&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-additonal-dataset" class="anchor" aria-hidden="true" href="#additonal-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Additonal Dataset&lt;/h2&gt;
&lt;p&gt;We make augmented dataset from Baidu fashion dataset.&lt;/p&gt;
&lt;p&gt;The original Baidu dataset link is &lt;a href="http://www.cbsr.ia.ac.cn/users/ynyu/dataset/" rel="nofollow"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;EG1800 dataset link what I used in &lt;a href="https://drive.google.com/file/d/1QmMrv7h-NJHYMnFfsqzqAM8d-G1Tz7VV/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Our augmented dataset is &lt;a href="https://drive.google.com/file/d/1e9nJtGQYy1zdVLIDP7_xALUR1iwOaeuN/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.
We use all train and val dataset for training segmentation model.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-cityscape" class="anchor" aria-hidden="true" href="#cityscape"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CityScape&lt;/h2&gt;
&lt;p&gt;If you want SINet code for cityscapes dataset, please go to this &lt;a href="https://github.com/clovaai/c3_sinet"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If our works is useful to you, please add two papers.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;@article{park2019extremec3net,
  title={ExtremeC3Net: Extreme Lightweight Portrait Segmentation Networks using Advanced C3-modules},
  author={Park, Hyojin and Sj{&lt;span class="pl-cce"&gt;\"&lt;/span&gt;o}sund, Lars Lowe and Yoo, YoungJoon and Kwak, Nojun},
  journal={arXiv preprint arXiv:1908.03093},
  year={2019}
}

@article{park2019sinet,
  title={SINet: Extreme Lightweight Portrait Segmentation Networks with Spatial Squeeze Modules and Information Blocking Decoder},
  author={Park, Hyojin and Sj{&lt;span class="pl-cce"&gt;\"&lt;/span&gt;o}sund, Lars Lowe and Monet, Nicolas and Yoo, YoungJoon and Kwak, Nojun},
  journal={arXiv preprint arXiv:1911.09099},
  year={2019}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledge" class="anchor" aria-hidden="true" href="#acknowledge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledge&lt;/h2&gt;
&lt;p&gt;We are grateful to &lt;a href="https://github.com/clovaai"&gt;Clova AI, NAVER&lt;/a&gt; with valuable discussions.&lt;/p&gt;
&lt;p&gt;I also appreciate my co-authors Lars Lowe Sjösund and YoungJoon Yoo from  &lt;a href="https://clova.ai/en/research/research-areas.html" rel="nofollow"&gt;Clova AI, NAVER&lt;/a&gt;,
Nicolas Monet from &lt;a href="https://europe.naverlabs.com/" rel="nofollow"&gt;NAVER LABS Europe&lt;/a&gt;
and Jihwan Bang from &lt;a href="https://www.searchsolutions.co.kr/" rel="nofollow"&gt;Search Solutions, Inc&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Copyright (c) 2019-present NAVER Corp.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>clovaai</author><guid isPermaLink="false">https://github.com/clovaai/ext_portrait_segmentation</guid><pubDate>Sat, 04 Jan 2020 00:19:00 GMT</pubDate></item><item><title>junyanz/pytorch-CycleGAN-and-pix2pix #20 in Python, Today</title><link>https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</link><description>&lt;p&gt;&lt;i&gt;Image-to-Image Translation in PyTorch&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="imgs/horse2zebra.gif"&gt;&lt;img src="imgs/horse2zebra.gif" align="right" width="384" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-cyclegan-and-pix2pix-in-pytorch" class="anchor" aria-hidden="true" href="#cyclegan-and-pix2pix-in-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CycleGAN and pix2pix in PyTorch&lt;/h1&gt;
&lt;p&gt;We provide PyTorch implementations for both unpaired and paired image-to-image translation.&lt;/p&gt;
&lt;p&gt;The code was written by &lt;a href="https://github.com/junyanz"&gt;Jun-Yan Zhu&lt;/a&gt; and &lt;a href="https://github.com/taesung"&gt;Taesung Park&lt;/a&gt;, and supported by &lt;a href="https://ssnl.github.io/" rel="nofollow"&gt;Tongzhou Wang&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This PyTorch implementation produces results comparable to or better than our original Torch software. If you would like to reproduce the same results as in the papers, check out the original &lt;a href="https://github.com/junyanz/CycleGAN"&gt;CycleGAN Torch&lt;/a&gt; and &lt;a href="https://github.com/phillipi/pix2pix"&gt;pix2pix Torch&lt;/a&gt; code&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The current software works well with PyTorch 0.41+. Check out the older &lt;a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/pytorch0.3.1"&gt;branch&lt;/a&gt; that supports PyTorch 0.1-0.3.&lt;/p&gt;
&lt;p&gt;You may find useful information in &lt;a href="docs/tips.md"&gt;training/test tips&lt;/a&gt; and &lt;a href="docs/qa.md"&gt;frequently asked questions&lt;/a&gt;. To implement custom models and datasets, check out our &lt;a href="#custom-model-and-dataset"&gt;templates&lt;/a&gt;. To help users better understand and adapt our codebase, we provide an &lt;a href="docs/overview.md"&gt;overview&lt;/a&gt; of the code structure of this repository.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CycleGAN: &lt;a href="https://junyanz.github.io/CycleGAN/" rel="nofollow"&gt;Project&lt;/a&gt; |  &lt;a href="https://arxiv.org/pdf/1703.10593.pdf" rel="nofollow"&gt;Paper&lt;/a&gt; |  &lt;a href="https://github.com/junyanz/CycleGAN"&gt;Torch&lt;/a&gt; |
&lt;a href="https://www.tensorflow.org/tutorials/generative/cyclegan" rel="nofollow"&gt;Tensorflow Core Tutorial&lt;/a&gt; | &lt;a href="https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb" rel="nofollow"&gt;PyTorch Colab&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/69cbc0371777fba5d251a564e2f8a8f38d1bf43f/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f7465617365725f686967685f7265732e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/69cbc0371777fba5d251a564e2f8a8f38d1bf43f/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f7465617365725f686967685f7265732e6a7067" width="800" data-canonical-src="https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pix2pix:  &lt;a href="https://phillipi.github.io/pix2pix/" rel="nofollow"&gt;Project&lt;/a&gt; |  &lt;a href="https://arxiv.org/pdf/1611.07004.pdf" rel="nofollow"&gt;Paper&lt;/a&gt; |  &lt;a href="https://github.com/phillipi/pix2pix"&gt;Torch&lt;/a&gt; |
&lt;a href="https://www.tensorflow.org/tutorials/generative/cyclegan" rel="nofollow"&gt;Tensorflow Core Tutorial&lt;/a&gt; | &lt;a href="https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb" rel="nofollow"&gt;PyTorch Colab&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/6f486f0501ce4eef6b6050a0acedee8664c718b8/68747470733a2f2f7068696c6c6970692e6769746875622e696f2f706978327069782f696d616765732f7465617365725f76332e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/6f486f0501ce4eef6b6050a0acedee8664c718b8/68747470733a2f2f7068696c6c6970692e6769746875622e696f2f706978327069782f696d616765732f7465617365725f76332e706e67" width="800px" data-canonical-src="https://phillipi.github.io/pix2pix/images/teaser_v3.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://affinelayer.com/pixsrv/" rel="nofollow"&gt;EdgesCats Demo&lt;/a&gt; | &lt;a href="https://github.com/affinelayer/pix2pix-tensorflow"&gt;pix2pix-tensorflow&lt;/a&gt; | by &lt;a href="https://twitter.com/christophrhesse" rel="nofollow"&gt;Christopher Hesse&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="imgs/edges2cats.jpg"&gt;&lt;img src="imgs/edges2cats.jpg" width="400px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you use this code for your research, please cite:&lt;/p&gt;
&lt;p&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.&lt;br&gt;
&lt;a href="https://people.eecs.berkeley.edu/~junyanz/" rel="nofollow"&gt;Jun-Yan Zhu&lt;/a&gt;*,  &lt;a href="https://taesung.me/" rel="nofollow"&gt;Taesung Park&lt;/a&gt;*, &lt;a href="https://people.eecs.berkeley.edu/~isola/" rel="nofollow"&gt;Phillip Isola&lt;/a&gt;, &lt;a href="https://people.eecs.berkeley.edu/~efros" rel="nofollow"&gt;Alexei A. Efros&lt;/a&gt;. In ICCV 2017. (* equal contributions) &lt;a href="https://junyanz.github.io/CycleGAN/CycleGAN.txt" rel="nofollow"&gt;[Bibtex]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Image-to-Image Translation with Conditional Adversarial Networks.&lt;br&gt;
&lt;a href="https://people.eecs.berkeley.edu/~isola" rel="nofollow"&gt;Phillip Isola&lt;/a&gt;, &lt;a href="https://people.eecs.berkeley.edu/~junyanz" rel="nofollow"&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href="https://people.eecs.berkeley.edu/~tinghuiz" rel="nofollow"&gt;Tinghui Zhou&lt;/a&gt;, &lt;a href="https://people.eecs.berkeley.edu/~efros" rel="nofollow"&gt;Alexei A. Efros&lt;/a&gt;. In CVPR 2017. &lt;a href="http://people.csail.mit.edu/junyanz/projects/pix2pix/pix2pix.bib" rel="nofollow"&gt;[Bibtex]&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-talks-and-course" class="anchor" aria-hidden="true" href="#talks-and-course"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Talks and Course&lt;/h2&gt;
&lt;p&gt;pix2pix slides: &lt;a href="http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.key" rel="nofollow"&gt;keynote&lt;/a&gt; | &lt;a href="http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.pdf" rel="nofollow"&gt;pdf&lt;/a&gt;,
CycleGAN slides: &lt;a href="http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pptx" rel="nofollow"&gt;pptx&lt;/a&gt; | &lt;a href="http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pdf" rel="nofollow"&gt;pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;CycleGAN course assignment &lt;a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip" rel="nofollow"&gt;code&lt;/a&gt; and &lt;a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-handout.pdf" rel="nofollow"&gt;handout&lt;/a&gt; designed by Prof. &lt;a href="http://www.cs.toronto.edu/~rgrosse/" rel="nofollow"&gt;Roger Grosse&lt;/a&gt; for &lt;a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/" rel="nofollow"&gt;CSC321&lt;/a&gt; "Intro to Neural Networks and Machine Learning" at University of Toronto. Please contact the instructor if you would like to adopt it in your course.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-colab-notebook" class="anchor" aria-hidden="true" href="#colab-notebook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Colab Notebook&lt;/h2&gt;
&lt;p&gt;TensorFlow Core CycleGAN Tutorial: &lt;a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt; | &lt;a href="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb"&gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TensorFlow Core pix2pix Tutorial: &lt;a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt; | &lt;a href="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb"&gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PyTorch Colab notebook: &lt;a href="https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb" rel="nofollow"&gt;CycleGAN&lt;/a&gt; and &lt;a href="https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb" rel="nofollow"&gt;pix2pix&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-other-implementations" class="anchor" aria-hidden="true" href="#other-implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other implementations&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-cyclegan" class="anchor" aria-hidden="true" href="#cyclegan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CycleGAN&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/leehomyc/cyclegan-1"&gt; [Tensorflow]&lt;/a&gt; (by Harry Yang),
&lt;a href="https://github.com/architrathore/CycleGAN/"&gt;[Tensorflow]&lt;/a&gt; (by Archit Rathore),
&lt;a href="https://github.com/vanhuyz/CycleGAN-TensorFlow"&gt;[Tensorflow]&lt;/a&gt; (by Van Huy),
&lt;a href="https://github.com/XHUJOY/CycleGAN-tensorflow"&gt;[Tensorflow]&lt;/a&gt; (by Xiaowei Hu),
&lt;a href="https://github.com/LynnHo/CycleGAN-Tensorflow-Simple"&gt; [Tensorflow-simple]&lt;/a&gt; (by Zhenliang He),
&lt;a href="https://github.com/luoxier/CycleGAN_Tensorlayer"&gt; [TensorLayer1.0]&lt;/a&gt; (by luoxier),
&lt;a href="https://github.com/tensorlayer/cyclegan"&gt; [TensorLayer2.0]&lt;/a&gt; (by zsdonghao),
&lt;a href="https://github.com/Aixile/chainer-cyclegan"&gt;[Chainer]&lt;/a&gt; (by Yanghua Jin),
&lt;a href="https://github.com/yunjey/mnist-svhn-transfer"&gt;[Minimal PyTorch]&lt;/a&gt; (by yunjey),
&lt;a href="https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN"&gt;[Mxnet]&lt;/a&gt; (by Ldpe2G),
&lt;a href="https://github.com/tjwei/GANotebooks"&gt;[lasagne/Keras]&lt;/a&gt; (by tjwei),
&lt;a href="https://github.com/simontomaskarlsson/CycleGAN-Keras"&gt;[Keras]&lt;/a&gt; (by Simon Karlsson)
&lt;/p&gt;

&lt;h3&gt;&lt;a id="user-content-pix2pix" class="anchor" aria-hidden="true" href="#pix2pix"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pix2pix&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/affinelayer/pix2pix-tensorflow"&gt; [Tensorflow]&lt;/a&gt; (by Christopher Hesse),
&lt;a href="https://github.com/Eyyub/tensorflow-pix2pix"&gt;[Tensorflow]&lt;/a&gt; (by Eyyüb Sariu),
&lt;a href="https://github.com/datitran/face2face-demo"&gt; [Tensorflow (face2face)]&lt;/a&gt; (by Dat Tran),
&lt;a href="https://github.com/awjuliani/Pix2Pix-Film"&gt; [Tensorflow (film)]&lt;/a&gt; (by Arthur Juliani),
&lt;a href="https://github.com/kaonashi-tyc/zi2zi"&gt;[Tensorflow (zi2zi)]&lt;/a&gt; (by Yuchen Tian),
&lt;a href="https://github.com/pfnet-research/chainer-pix2pix"&gt;[Chainer]&lt;/a&gt; (by mattya),
&lt;a href="https://github.com/tjwei/GANotebooks"&gt;[tf/torch/keras/lasagne]&lt;/a&gt; (by tjwei),
&lt;a href="https://github.com/taey16/pix2pixBEGAN.pytorch"&gt;[Pytorch]&lt;/a&gt; (by taey16)
&lt;/p&gt;

&lt;h2&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Linux or macOS&lt;/li&gt;
&lt;li&gt;Python 3&lt;/li&gt;
&lt;li&gt;CPU or NVIDIA GPU + CUDA CuDNN&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Clone this repo:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; pytorch-CycleGAN-and-pix2pix&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Install &lt;a href="http://pytorch.org" rel="nofollow"&gt;PyTorch&lt;/a&gt; and 0.4+ and other dependencies (e.g., torchvision, &lt;a href="https://github.com/facebookresearch/visdom"&gt;visdom&lt;/a&gt; and &lt;a href="https://github.com/Knio/dominate"&gt;dominate&lt;/a&gt;).
&lt;ul&gt;
&lt;li&gt;For pip users, please type the command &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;For Conda users, we provide a installation script &lt;code&gt;./scripts/conda_deps.sh&lt;/code&gt;. Alternatively, you can create a new Conda environment using &lt;code&gt;conda env create -f environment.yml&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;For Docker users, we provide the pre-built Docker image and Dockerfile. Please refer to our &lt;a href="docs/docker.md"&gt;Docker&lt;/a&gt; page.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-cyclegan-traintest" class="anchor" aria-hidden="true" href="#cyclegan-traintest"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CycleGAN train/test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Download a CycleGAN dataset (e.g. maps):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./datasets/download_cyclegan_dataset.sh maps&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;To view training results and loss plots, run &lt;code&gt;python -m visdom.server&lt;/code&gt; and click the URL &lt;a href="http://localhost:8097" rel="nofollow"&gt;http://localhost:8097&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Train a model:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#!&lt;/span&gt;./scripts/train_cyclegan.sh&lt;/span&gt;
python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To see more intermediate results, check out &lt;code&gt;./checkpoints/maps_cyclegan/web/index.html&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Test the model:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#!&lt;/span&gt;./scripts/test_cyclegan.sh&lt;/span&gt;
python test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;The test results will be saved to a html file here: &lt;code&gt;./results/maps_cyclegan/latest_test/index.html&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pix2pix-traintest" class="anchor" aria-hidden="true" href="#pix2pix-traintest"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pix2pix train/test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Download a pix2pix dataset (e.g.&lt;a href="http://cmp.felk.cvut.cz/~tylecr1/facade/" rel="nofollow"&gt;facades&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./datasets/download_pix2pix_dataset.sh facades&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;To view training results and loss plots, run &lt;code&gt;python -m visdom.server&lt;/code&gt; and click the URL &lt;a href="http://localhost:8097" rel="nofollow"&gt;http://localhost:8097&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Train a model:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#!&lt;/span&gt;./scripts/train_pix2pix.sh&lt;/span&gt;
python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To see more intermediate results, check out  &lt;code&gt;./checkpoints/facades_pix2pix/web/index.html&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Test the model (&lt;code&gt;bash ./scripts/test_pix2pix.sh&lt;/code&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#!&lt;/span&gt;./scripts/test_pix2pix.sh&lt;/span&gt;
python test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;The test results will be saved to a html file here: &lt;code&gt;./results/facades_pix2pix/test_latest/index.html&lt;/code&gt;. You can find more scripts at &lt;code&gt;scripts&lt;/code&gt; directory.&lt;/li&gt;
&lt;li&gt;To train and test pix2pix-based colorization models, please add &lt;code&gt;--model colorization&lt;/code&gt; and &lt;code&gt;--dataset_mode colorization&lt;/code&gt;. See our training &lt;a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#notes-on-colorization"&gt;tips&lt;/a&gt; for more details.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-apply-a-pre-trained-model-cyclegan" class="anchor" aria-hidden="true" href="#apply-a-pre-trained-model-cyclegan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Apply a pre-trained model (CycleGAN)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;You can download a pretrained model (e.g. horse2zebra) with the following script:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./scripts/download_cyclegan_model.sh horse2zebra&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;The pretrained model is saved at &lt;code&gt;./checkpoints/{name}_pretrained/latest_net_G.pth&lt;/code&gt;. Check &lt;a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/download_cyclegan_model.sh#L3"&gt;here&lt;/a&gt; for all the available CycleGAN models.&lt;/li&gt;
&lt;li&gt;To test the model, you also need to download the  horse2zebra dataset:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./datasets/download_cyclegan_dataset.sh horse2zebra&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Then generate the results using&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model &lt;span class="pl-c1"&gt;test&lt;/span&gt; --no_dropout&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The option &lt;code&gt;--model test&lt;/code&gt; is used for generating results of CycleGAN only for one side. This option will automatically set &lt;code&gt;--dataset_mode single&lt;/code&gt;, which only loads the images from one set. On the contrary, using &lt;code&gt;--model cycle_gan&lt;/code&gt; requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at &lt;code&gt;./results/&lt;/code&gt;. Use &lt;code&gt;--results_dir {directory_path_to_save_result}&lt;/code&gt; to specify the results directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For your own experiments, you might want to specify &lt;code&gt;--netG&lt;/code&gt;, &lt;code&gt;--norm&lt;/code&gt;, &lt;code&gt;--no_dropout&lt;/code&gt; to match the generator architecture of the trained model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-apply-a-pre-trained-model-pix2pix" class="anchor" aria-hidden="true" href="#apply-a-pre-trained-model-pix2pix"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Apply a pre-trained model (pix2pix)&lt;/h3&gt;
&lt;p&gt;Download a pre-trained model with &lt;code&gt;./scripts/download_pix2pix_model.sh&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Check &lt;a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/download_pix2pix_model.sh#L3"&gt;here&lt;/a&gt; for all the available pix2pix models. For example, if you would like to download label2photo model on the Facades dataset,&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./scripts/download_pix2pix_model.sh facades_label2photo&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Download the pix2pix facades datasets:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./datasets/download_pix2pix_dataset.sh facades&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Then generate the results using&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python test.py --dataroot ./datasets/facades/ --direction BtoA --model pix2pix --name facades_label2photo_pretrained&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Note that we specified &lt;code&gt;--direction BtoA&lt;/code&gt; as Facades dataset's A to B direction is photos to labels.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use &lt;code&gt;--model test&lt;/code&gt; option. See &lt;code&gt;./scripts/test_single.sh&lt;/code&gt; for how to apply a model to Facade label maps (stored in the directory &lt;code&gt;facades/testB&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;See a list of currently available models at &lt;code&gt;./scripts/download_pix2pix_model.sh&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-docker" class="anchor" aria-hidden="true" href="#docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="docs/docker.md"&gt;Docker&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We provide the pre-built Docker image and Dockerfile that can run this code repo. See &lt;a href="docs/docker.md"&gt;docker&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="docs/datasets.md"&gt;Datasets&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Download pix2pix/CycleGAN datasets and create your own datasets.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-trainingtest-tips" class="anchor" aria-hidden="true" href="#trainingtest-tips"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="docs/tips.md"&gt;Training/Test Tips&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Best practice for training and testing your models.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-frequently-asked-questions" class="anchor" aria-hidden="true" href="#frequently-asked-questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="docs/qa.md"&gt;Frequently Asked Questions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Before you post a new question, please first look at the above Q &amp;amp; A and existing GitHub issues.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-custom-model-and-dataset" class="anchor" aria-hidden="true" href="#custom-model-and-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Model and Dataset&lt;/h2&gt;
&lt;p&gt;If you plan to implement custom models and dataset for your new applications, we provide a dataset &lt;a href="data/template_dataset.py"&gt;template&lt;/a&gt; and a model &lt;a href="models/template_model.py"&gt;template&lt;/a&gt; as a starting point.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-code-structure" class="anchor" aria-hidden="true" href="#code-structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="docs/overview.md"&gt;Code structure&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To help users better understand and use our code, we briefly overview the functionality and implementation of each package and each module.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pull-request" class="anchor" aria-hidden="true" href="#pull-request"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pull Request&lt;/h2&gt;
&lt;p&gt;You are always welcome to contribute to this repository by sending a &lt;a href="https://help.github.com/articles/about-pull-requests/"&gt;pull request&lt;/a&gt;.
Please run &lt;code&gt;flake8 --ignore E501 .&lt;/code&gt; and &lt;code&gt;python ./scripts/test_before_push.py&lt;/code&gt; before you commit the code. Please also update the code structure &lt;a href="docs/overview.md"&gt;overview&lt;/a&gt; accordingly if you add or remove files.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use this code for your research, please cite our papers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{CycleGAN2017,
  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networkss},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},
  year={2017}
}


@inproceedings{isola2017image,
  title={Image-to-Image Translation with Conditional Adversarial Networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},
  year={2017}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-other-languages" class="anchor" aria-hidden="true" href="#other-languages"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other Languages&lt;/h2&gt;
&lt;p&gt;&lt;a href="docs/README_es.md"&gt;Spanish&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-related-projects" class="anchor" aria-hidden="true" href="#related-projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related Projects&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/junyanz/CycleGAN"&gt;CycleGAN-Torch&lt;/a&gt; |
&lt;a href="https://github.com/phillipi/pix2pix"&gt;pix2pix-Torch&lt;/a&gt; | &lt;a href="https://github.com/NVIDIA/pix2pixHD"&gt;pix2pixHD&lt;/a&gt;|
&lt;a href="https://github.com/junyanz/BicycleGAN"&gt;BicycleGAN&lt;/a&gt; | &lt;a href="https://tcwang0509.github.io/vid2vid/" rel="nofollow"&gt;vid2vid&lt;/a&gt; | &lt;a href="https://github.com/NVlabs/SPADE"&gt;SPADE/GauGAN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;&lt;a href="https://github.com/junyanz/iGAN"&gt;iGAN&lt;/a&gt; | &lt;a href="https://github.com/CSAILVision/GANDissect"&gt;GAN Dissection&lt;/a&gt; | &lt;a href="http://ganpaint.io/" rel="nofollow"&gt;GAN Paint&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-cat-paper-collection" class="anchor" aria-hidden="true" href="#cat-paper-collection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cat Paper Collection&lt;/h2&gt;
&lt;p&gt;If you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper &lt;a href="https://github.com/junyanz/CatPapers"&gt;Collection&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgments" class="anchor" aria-hidden="true" href="#acknowledgments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;Our code is inspired by &lt;a href="https://github.com/pytorch/examples/tree/master/dcgan"&gt;pytorch-DCGAN&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>junyanz</author><guid isPermaLink="false">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</guid><pubDate>Sat, 04 Jan 2020 00:20:00 GMT</pubDate></item><item><title>facebookresearch/LASER #21 in Python, Today</title><link>https://github.com/facebookresearch/LASER</link><description>&lt;p&gt;&lt;i&gt;Language-Agnostic SEntence Representations&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-laser--language-agnostic-sentence-representations" class="anchor" aria-hidden="true" href="#laser--language-agnostic-sentence-representations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LASER  Language-Agnostic SEntence Representations&lt;/h1&gt;
&lt;p&gt;LASER is a library to calculate and use multilingual sentence embeddings.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NEWS&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2019/11/08 &lt;a href="tasks/CCMatrix"&gt;&lt;strong&gt;CCMatrix is available&lt;/strong&gt;&lt;/a&gt;: Mining billions of high-quality parallel sentences on the WEB [8]&lt;/li&gt;
&lt;li&gt;2019/07/31 Gilles Bodard and Jérémy Rapin provided a &lt;a href="docker"&gt;&lt;strong&gt;Docker environment&lt;/strong&gt;&lt;/a&gt; to use LASER&lt;/li&gt;
&lt;li&gt;2019/07/11 &lt;a href="tasks/WikiMatrix"&gt;&lt;strong&gt;WikiMatrix is available&lt;/strong&gt;&lt;/a&gt;: bitext extraction for 1620 language pairs in WikiPedia [7]&lt;/li&gt;
&lt;li&gt;2019/03/18 switch to BSD license&lt;/li&gt;
&lt;li&gt;2019/02/13 The code to perform bitext mining is &lt;a href="tasks/bucc"&gt;&lt;strong&gt;now available&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;CURRENT VERSION:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We now provide an encoder which was trained on &lt;a href="#supported-languages"&gt;&lt;strong&gt;93 languages&lt;/strong&gt;&lt;/a&gt;, written in 23 different alphabets [6].
This includes all European languages, many Asian and Indian languages, Arabic, Persian, Hebrew, ...,
as well as various minority languages and dialects.&lt;/li&gt;
&lt;li&gt;We provide a &lt;a href="data/tatoeba/v1"&gt;&lt;em&gt;test set for more than 100 languages&lt;/em&gt;&lt;/a&gt;
based on the &lt;a href="https://tatoeba.org/eng" rel="nofollow"&gt;&lt;em&gt;Tatoeba corpus&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Switch to PyTorch 1.0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these languages are encoded by the same BiLSTM encoder, and there is no need
to specify the input language (but tokenization is language specific).
According to our experience, the sentence encoder also supports code-switching, i.e.
the same sentences can contain words in several different languages.&lt;/p&gt;
&lt;p&gt;We have also some evidence that the encoder can generalizes to other
languages which have not been seen during training, but which are in
a language family which is covered by other languages.&lt;/p&gt;
&lt;p&gt;A detailed description how the multilingual sentence embeddings are trained can
be found in [6], together with an extensive experimental evaluation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Python 3.6&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pytorch.org/" rel="nofollow"&gt;PyTorch 1.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.numpy.org/" rel="nofollow"&gt;NumPy&lt;/a&gt;, tested with 1.15.4&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/Cython/" rel="nofollow"&gt;Cython&lt;/a&gt;, needed by Python wrapper of FastBPE, tested with 0.29.6&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/faiss"&gt;Faiss&lt;/a&gt;, for fast similarity search and bitext mining&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/transliterate" rel="nofollow"&gt;transliterate 1.10.2&lt;/a&gt;, only used for Greek (&lt;code&gt;pip install transliterate&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/jieba/" rel="nofollow"&gt;jieba 0.39&lt;/a&gt;, Chinese segmenter (&lt;code&gt;pip install jieba&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/JapaneseTokenizer/" rel="nofollow"&gt;mecab 0.996&lt;/a&gt;, Japanese segmenter&lt;/li&gt;
&lt;li&gt;tokenization from the Moses encoder (installed automatically)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/glample/fastBPE"&gt;FastBPE&lt;/a&gt;, fast C++ implementation of byte-pair encoding (installed automatically)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;set the environment variable 'LASER' to the root of the installation, e.g.
&lt;code&gt;export LASER="${HOME}/projects/laser"&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;download encoders from Amazon s3 by &lt;code&gt;bash ./install_models.sh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;download third party software by &lt;code&gt;bash ./install_external_tools.sh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;download the data used in the example tasks (see description for each task)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-applications" class="anchor" aria-hidden="true" href="#applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Applications&lt;/h2&gt;
&lt;p&gt;We showcase several applications of multilingual sentence embeddings
with code to reproduce our results (in the directory "tasks").&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="tasks/mldoc"&gt;&lt;strong&gt;Cross-lingual document classification&lt;/strong&gt;&lt;/a&gt; using the
&lt;a href="https://github.com/facebookresearch/MLDoc"&gt;&lt;em&gt;MLDoc&lt;/em&gt;&lt;/a&gt; corpus [2,6]&lt;/li&gt;
&lt;li&gt;&lt;a href="tasks/WikiMatrix"&gt;&lt;strong&gt;WikiMatrix&lt;/strong&gt;&lt;/a&gt;
Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia [7]&lt;/li&gt;
&lt;li&gt;&lt;a href="tasks/bucc"&gt;&lt;strong&gt;Bitext mining&lt;/strong&gt;&lt;/a&gt; using the
&lt;a href="https://comparable.limsi.fr/bucc2018/bucc2018-task.html" rel="nofollow"&gt;&lt;em&gt;BUCC&lt;/em&gt;&lt;/a&gt; corpus [3,5]&lt;/li&gt;
&lt;li&gt;&lt;a href="tasks/xnli"&gt;&lt;strong&gt;Cross-lingual NLI&lt;/strong&gt;&lt;/a&gt;
using the &lt;a href="https://www.nyu.edu/projects/bowman/xnli/" rel="nofollow"&gt;&lt;em&gt;XNLI&lt;/em&gt;&lt;/a&gt; corpus [4,5,6]&lt;/li&gt;
&lt;li&gt;&lt;a href="tasks/similarity"&gt;&lt;strong&gt;Multilingual similarity search&lt;/strong&gt;&lt;/a&gt; [1,6]&lt;/li&gt;
&lt;li&gt;&lt;a href="tasks/embed"&gt;&lt;strong&gt;Sentence embedding of text files&lt;/strong&gt;&lt;/a&gt;
example how to calculate sentence embeddings for arbitrary text files in any of the supported language.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;For all tasks, we use exactly the same multilingual encoder, without any task specific optimization or fine-tuning.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;LASER is BSD-licensed, as found in the &lt;a href="LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; file in the root directory of this source tree.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-supported-languages" class="anchor" aria-hidden="true" href="#supported-languages"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supported languages&lt;/h2&gt;
&lt;p&gt;Our model was trained on the following languages:&lt;/p&gt;
&lt;p&gt;Afrikaans, Albanian, Amharic, Arabic, Armenian, Aymara, Azerbaijani, Basque, Belarusian, Bengali,
Berber languages, Bosnian, Breton, Bulgarian, Burmese, Catalan, Central/Kadazan Dusun, Central Khmer,
Chavacano, Chinese, Coastal Kadazan, Cornish, Croatian, Czech, Danish, Dutch, Eastern Mari, English,
Esperanto, Estonian, Finnish, French, Galician, Georgian, German, Greek, Hausa, Hebrew, Hindi,
Hungarian, Icelandic, Ido, Indonesian, Interlingua, Interlingue, Irish, Italian, Japanese, Kabyle,
Kazakh, Korean, Kurdish, Latvian, Latin, Lingua Franca Nova, Lithuanian, Low German/Saxon,
Macedonian, Malagasy, Malay, Malayalam, Maldivian (Divehi), Marathi, Norwegian (Bokmål), Occitan,
Persian (Farsi), Polish, Portuguese, Romanian, Russian, Serbian, Sindhi, Sinhala, Slovak, Slovenian,
Somali, Spanish, Swahili, Swedish, Tagalog, Tajik, Tamil, Tatar, Telugu, Thai, Turkish, Uighur,
Ukrainian, Urdu, Uzbek, Vietnamese, Wu Chinese and Yue Chinese.&lt;/p&gt;
&lt;p&gt;We have also observed that the model seems to generalize well to other (minority) languages or dialects, e.g.&lt;/p&gt;
&lt;p&gt;Asturian, Egyptian Arabic, Faroese, Kashubian, North Moluccan Malay, Nynorsk Norwegian, Piedmontese, Sorbian, Swabian,
Swiss German or Western Frisian.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Holger Schwenk and Matthijs Douze,
&lt;a href="https://aclanthology.info/papers/W17-2619/w17-2619" rel="nofollow"&gt;&lt;em&gt;Learning Joint Multilingual Sentence Representations with Neural Machine Translation&lt;/em&gt;&lt;/a&gt;,
ACL workshop on Representation Learning for NLP, 2017&lt;/p&gt;
&lt;p&gt;[2] Holger Schwenk and Xian Li,
&lt;a href="http://www.lrec-conf.org/proceedings/lrec2018/pdf/658.pdf" rel="nofollow"&gt;&lt;em&gt;A Corpus for Multilingual Document Classification in Eight Languages&lt;/em&gt;&lt;/a&gt;,
LREC, pages 3548-3551, 2018.&lt;/p&gt;
&lt;p&gt;[3] Holger Schwenk,
&lt;a href="http://aclweb.org/anthology/P18-2037" rel="nofollow"&gt;&lt;em&gt;Filtering and Mining Parallel Data in a Joint Multilingual Space&lt;/em&gt;&lt;/a&gt;
ACL, July 2018&lt;/p&gt;
&lt;p&gt;[4] Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman, Holger Schwenk and Veselin Stoyanov,
&lt;a href="https://aclweb.org/anthology/D18-1269" rel="nofollow"&gt;&lt;em&gt;XNLI: Cross-lingual Sentence Understanding through Inference&lt;/em&gt;&lt;/a&gt;,
EMNLP, 2018.&lt;/p&gt;
&lt;p&gt;[5] Mikel Artetxe and Holger Schwenk,
&lt;a href="https://arxiv.org/abs/1811.01136" rel="nofollow"&gt;&lt;em&gt;Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings&lt;/em&gt;&lt;/a&gt;
arXiv, Nov 3 2018.&lt;/p&gt;
&lt;p&gt;[6] Mikel Artetxe and Holger Schwenk,
&lt;a href="https://arxiv.org/abs/1812.10464" rel="nofollow"&gt;&lt;em&gt;Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond&lt;/em&gt;&lt;/a&gt;
arXiv, Dec 26 2018.&lt;/p&gt;
&lt;p&gt;[7] Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong and Paco Guzman,
&lt;a href="https://arxiv.org/abs/1907.05791" rel="nofollow"&gt;&lt;em&gt;WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia&lt;/em&gt;&lt;/a&gt;
arXiv, July 11  2019.&lt;/p&gt;
&lt;p&gt;[8] Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave and Armand Joulin
&lt;a href="https://arxiv.org/abs/1911.04944" rel="nofollow"&gt;&lt;em&gt;CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>facebookresearch</author><guid isPermaLink="false">https://github.com/facebookresearch/LASER</guid><pubDate>Sat, 04 Jan 2020 00:21:00 GMT</pubDate></item><item><title>microsoft/macaw #22 in Python, Today</title><link>https://github.com/microsoft/macaw</link><description>&lt;p&gt;&lt;i&gt;An Extensible Conversational Information Seeking Platform&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-macaw-an-extensible-conversational-information-seeking-platform" class="anchor" aria-hidden="true" href="#macaw-an-extensible-conversational-information-seeking-platform"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Macaw: An Extensible Conversational Information Seeking Platform&lt;/h1&gt;
&lt;p&gt;Conversational information seeking (CIS) has been recognized as a major emerging research area in information retrieval.
Such research will require data and tools, to allow the implementation and study of conversational systems. Macaw is
an open-source framework with a modular architecture for CIS research. Macaw supports &lt;em&gt;multi-turn&lt;/em&gt;, &lt;em&gt;multi-modal&lt;/em&gt;, and
&lt;em&gt;mixed-initiative&lt;/em&gt; interactions, for tasks such as document retrieval, question answering, recommendation, and
structured data exploration. It has a modular design to encourage the study of new CIS algorithms, which can be
evaluated in batch mode. It can also integrate with a user interface, which allows user studies and data collection in
an interactive mode, where the back end can be &lt;em&gt;fully algorithmic&lt;/em&gt; or a &lt;em&gt;wizard of oz&lt;/em&gt; setup.&lt;/p&gt;
&lt;p&gt;Macaw could be of interest to the researchers and practitioners working on information retrieval, natural language
processing, and dialogue systems.&lt;/p&gt;
&lt;p&gt;For more information on Macaw, please refer to &lt;a href="https://arxiv.org/pdf/1912.08904.pdf" rel="nofollow"&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Table of content:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#macaw-architecture"&gt;Macaw Architecture&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#interfaces"&gt;Interfaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#retrieval"&gt;Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#answer-selection-and-generation"&gt;Answer Selection and Generation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#running-macaw"&gt;Running Macaw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bug-report-and-feature-request"&gt;Bug Report and Feature Request&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-macaw-architecture" class="anchor" aria-hidden="true" href="#macaw-architecture"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Macaw Architecture&lt;/h2&gt;
&lt;p&gt;Macaw has a modular architecture, which allows further development and extension. The high-level architecture of Macaw
is presented below:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="macaw-arch.jpg"&gt;&lt;img src="macaw-arch.jpg" alt="The high-level architecture of Macaw" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For more information on each module in Macaw, refer to this paper.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-interfaces" class="anchor" aria-hidden="true" href="#interfaces"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Interfaces&lt;/h4&gt;
&lt;p&gt;Macaw supports the following interfaces:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Standard IO: For &lt;em&gt;development&lt;/em&gt; purposes&lt;/li&gt;
&lt;li&gt;File IO: For &lt;em&gt;batch experiments&lt;/em&gt; (see the examples in the &lt;code&gt;data&lt;/code&gt; folder for input and output file formats)&lt;/li&gt;
&lt;li&gt;Telegram bot: For interaction with real users&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an example of the Telegram interface for Macaw. It supports multi-modal interactions (text, speech, click, etc).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="macaw-example-tax.jpg"&gt;&lt;img src="macaw-example-tax.jpg" alt="Telegram interface for Macaw" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="macaw-example-shakespeare.jpg"&gt;&lt;img src="macaw-example-shakespeare.jpg" alt="Telegram interface for Macaw" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-retrieval" class="anchor" aria-hidden="true" href="#retrieval"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Retrieval&lt;/h4&gt;
&lt;p&gt;Macaw features the following search engines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://lemurproject.org/indri.php" rel="nofollow"&gt;Indri&lt;/a&gt;: an open-source search engine that can be used for any arbitrary text
collection.&lt;/li&gt;
&lt;li&gt;Bing web search API: sending a request to the Bing API and getting the results.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-answer-selection-and-generation" class="anchor" aria-hidden="true" href="#answer-selection-and-generation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Answer Selection and Generation&lt;/h4&gt;
&lt;p&gt;For question answering, Macaw only features &lt;a href="https://github.com/facebookresearch/DrQA"&gt;the DrQA model&lt;/a&gt; in its current
version.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;Macaw requires &lt;code&gt;Python &amp;gt;= 3.6&lt;/code&gt; and &lt;code&gt;pip3&lt;/code&gt;. If you don't have &lt;code&gt;setuptools&lt;/code&gt;, run &lt;code&gt;sudo pip3 install setuptools&lt;/code&gt;.
To install Macaw, first &lt;strong&gt;clone macaw&lt;/strong&gt; from this repo and then follow the following installation steps. The
mentioned installation commands can be executed on Ubuntu. You can use the same or similar commands on other Linux
distribution. If you are using Windows 10, we recommend installing Macaw and all the required packages on
&lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10" rel="nofollow"&gt;Windows Subsystem for Linux&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-step-1-installing-mongodb-server" class="anchor" aria-hidden="true" href="#step-1-installing-mongodb-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 1: Installing MongoDB server&lt;/h4&gt;
&lt;p&gt;Macaw uses MongoDB for storing and retrieving user interactions (conversations). To install MongoDB server, run the
following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install mongodb-server-core
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-step-2-installing-indri-and-pyndri" class="anchor" aria-hidden="true" href="#step-2-installing-indri-and-pyndri"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 2: Installing Indri and Pyndri&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://lemurproject.org/indri.php" rel="nofollow"&gt;Indri&lt;/a&gt; is an open-source search engine for information retrieval research,
implemented as part of the &lt;a href="http://lemurproject.org/" rel="nofollow"&gt;Lemur Project&lt;/a&gt;.
&lt;a href="https://github.com/cvangysel/pyndri"&gt;Pyndri&lt;/a&gt; is a python interface to Indri. Macaw uses Indri for retrieving documents
from an arbitrary text collection.
To install Indri, first download Indri from &lt;a href="https://sourceforge.net/projects/lemur/files/lemur/" rel="nofollow"&gt;https://sourceforge.net/projects/lemur/files/lemur/&lt;/a&gt;. As suggested by pyndri,
we have used Indri-5.11. This Indri version can be installed as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# download indri-5.11.tar.gz
sudo apt install g++ zlib1g-dev
tar xzvf indri-5.11.tar.gz
rm indri-5.11.tar.gz
cd indri-5.11
./configure CXX="g++ -D_GLIBCXX_USE_CXX11_ABI=0"
make
sudo make install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, clone the pyndri repository from &lt;a href="https://github.com/cvangysel/pyndri"&gt;https://github.com/cvangysel/pyndri&lt;/a&gt; and run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python3 setup.py install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this step, you can make sure your installation is complete by running the pyndri tests.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-step-3-installing-stanford-core-nlp" class="anchor" aria-hidden="true" href="#step-3-installing-stanford-core-nlp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 3: Installing Stanford Core NLP&lt;/h4&gt;
&lt;p&gt;Stanford Core NLP can be used for tokenization and most importantly for co-reference resolution. If you do not need
co-reference resolution, you can ignore this step. Stanford Core NLP requires &lt;code&gt;java&lt;/code&gt;. Get it by following these
commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget -O "stanford-corenlp-full-2017-06-09.zip" "http://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip"
sudo apt-get install unzip
unzip "stanford-corenlp-full-2017-06-09.zip"
rm "stanford-corenlp-full-2017-06-09.zip"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you don't have &lt;code&gt;java&lt;/code&gt;, install it using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install default-jre
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-step-4-installing-drqa" class="anchor" aria-hidden="true" href="#step-4-installing-drqa"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 4: Installing DrQA&lt;/h4&gt;
&lt;p&gt;Macaw also supports answer extraction / generation for user queries from retrieved documents. For this purpose, it
features &lt;a href="https://github.com/facebookresearch/DrQA"&gt;DrQA&lt;/a&gt;. If you do not need this functionality, ignore this step (you
can also install this later).
To install DrQA, run the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/facebookresearch/DrQA.git
cd DrQA
pip3 install -r requirements.txt
pip3 install torch
sudo python3 setup.py develop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To use pre-trained DrQA model, use the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./download.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This downloads a 7.5GB (compressed) file and requires 25GB (uncompressed) space. This may take a while!&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-step-5-installing-ffmpeg" class="anchor" aria-hidden="true" href="#step-5-installing-ffmpeg"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 5: Installing FFmpeg&lt;/h4&gt;
&lt;p&gt;To support speech interactions with users, Macaw requires FFmpeg for some multimedia processing steps. If you don't
need a speech support from Macaw, you can skip this step. To install FFmpeg, run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install 
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-step-6-installing-macaw" class="anchor" aria-hidden="true" href="#step-6-installing-macaw"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Step 6: Installing Macaw&lt;/h4&gt;
&lt;p&gt;After cloning Macaw, use the following commands for installation:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd macaw
sudo pip3 install -r requirements.txt
sudo python3 setup.py install
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-running-macaw" class="anchor" aria-hidden="true" href="#running-macaw"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Macaw&lt;/h2&gt;
&lt;p&gt;If you run macaw with interactive (or live) mode, you should first run MongoDB server using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo mongod
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this command uses the default database directory (&lt;code&gt;/data/db&lt;/code&gt;) for storing the data. You may need to create
this directory if you haven't. You can also use other locations using the &lt;code&gt;--dbpath&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;We provide three different main scripts (i.e., app):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;live_main.py&lt;/code&gt;: An interactive conversational search and question answering system. It can use both STDIO and Telegram
interfaces.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;batch_ext_main.py&lt;/code&gt;: A model for running experiments on a reusable dataset. This main script uses FILEIO as the
interface.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wizard_of_oz_main.py&lt;/code&gt;: A main script for Wizard of Oz experiments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After selecting the desired main script, open the python file and provide the required parameters. For example, you need
to use your Bing subscription key (if using Bing), the path to Indri index (if using Indri), Telegram bot token (if
using Telegram interface), etc. in order to run the &lt;code&gt;live_main.py&lt;/code&gt; script. You can further run the favorite main script
as below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python3 live_main.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-bug-report-and-feature-request" class="anchor" aria-hidden="true" href="#bug-report-and-feature-request"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bug Report and Feature Request&lt;/h2&gt;
&lt;p&gt;For bug report and feature request, you can open an issue in github, or send an email to
&lt;a href="http://hamedz.ir" rel="nofollow"&gt;Hamed Zamani&lt;/a&gt; at &lt;code&gt;hazamani@microsoft.com&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you found Macaw useful, you can cite the following article:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Hamed Zamani and Nick Craswell, "Macaw: An Extensible Conversational Information Seeking System", arxiv pre-print.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;bibtex:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{macaw,
  title={Macaw: An Extensible Conversational Information Seeking Platform},
  author={Zamani, Hamed and Craswell, Nick},
  journal={arXiv preprint arXiv:1912.08904},
  year={2019},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Macaw is distributed under the &lt;strong&gt;MIT License&lt;/strong&gt;. See the &lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h2&gt;
&lt;p&gt;This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com" rel="nofollow"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt;
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;.
For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow"&gt;Code of Conduct FAQ&lt;/a&gt; or
contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>microsoft</author><guid isPermaLink="false">https://github.com/microsoft/macaw</guid><pubDate>Sat, 04 Jan 2020 00:22:00 GMT</pubDate></item><item><title>amdegroot/ssd.pytorch #23 in Python, Today</title><link>https://github.com/amdegroot/ssd.pytorch</link><description>&lt;p&gt;&lt;i&gt;A PyTorch Implementation of Single Shot MultiBox Detector&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ssd-single-shot-multibox-object-detector-in-pytorch" class="anchor" aria-hidden="true" href="#ssd-single-shot-multibox-object-detector-in-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SSD: Single Shot MultiBox Object Detector, in PyTorch&lt;/h1&gt;
&lt;p&gt;A &lt;a href="http://pytorch.org/" rel="nofollow"&gt;PyTorch&lt;/a&gt; implementation of &lt;a href="http://arxiv.org/abs/1512.02325" rel="nofollow"&gt;Single Shot MultiBox Detector&lt;/a&gt; from the 2016 paper by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang, and Alexander C. Berg.  The official and original Caffe code can be found &lt;a href="https://github.com/weiliu89/caffe/tree/ssd"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/amdegroot/ssd.pytorch/blob/master/doc/ssd.png"&gt;&lt;img align="right" src="https://github.com/amdegroot/ssd.pytorch/raw/master/doc/ssd.png" height="400/" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#datasets"&gt;Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#training-ssd"&gt;Train&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#evaluation"&gt;Evaluate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#performance"&gt;Performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#demos"&gt;Demos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#todo"&gt;Future Work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; 
 
 
 &lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Install &lt;a href="http://pytorch.org/" rel="nofollow"&gt;PyTorch&lt;/a&gt; by selecting your environment on the website and running the appropriate command.&lt;/li&gt;
&lt;li&gt;Clone this repository.
&lt;ul&gt;
&lt;li&gt;Note: We currently only support Python 3+.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Then download the dataset by following the &lt;a href="#datasets"&gt;instructions&lt;/a&gt; below.&lt;/li&gt;
&lt;li&gt;We now support &lt;a href="https://github.com/facebookresearch/visdom"&gt;Visdom&lt;/a&gt; for real-time loss visualization during training!
&lt;ul&gt;
&lt;li&gt;To use Visdom in the browser:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; First install Python server and client&lt;/span&gt;
pip install visdom
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Start the server (probably in a screen or tmux)&lt;/span&gt;
python -m visdom.server&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Then (during training) navigate to &lt;a href="http://localhost:8097/" rel="nofollow"&gt;http://localhost:8097/&lt;/a&gt; (see the Train section below for training details).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Note: For training, we currently support &lt;a href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="nofollow"&gt;VOC&lt;/a&gt; and &lt;a href="http://mscoco.org/" rel="nofollow"&gt;COCO&lt;/a&gt;, and aim to add &lt;a href="http://www.image-net.org/" rel="nofollow"&gt;ImageNet&lt;/a&gt; support soon.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h2&gt;
&lt;p&gt;To make things easy, we provide bash scripts to handle the dataset downloads and setup for you.  We also provide simple dataset loaders that inherit &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt;, making them fully compatible with the &lt;code&gt;torchvision.datasets&lt;/code&gt; &lt;a href="http://pytorch.org/docs/torchvision/datasets.html" rel="nofollow"&gt;API&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-coco" class="anchor" aria-hidden="true" href="#coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;COCO&lt;/h3&gt;
&lt;p&gt;Microsoft COCO: Common Objects in Context&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-download-coco-2014" class="anchor" aria-hidden="true" href="#download-coco-2014"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download COCO 2014&lt;/h5&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; specify a directory for dataset to be downloaded into, else default is ~/data/&lt;/span&gt;
sh data/scripts/COCO2014.sh&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-voc-dataset" class="anchor" aria-hidden="true" href="#voc-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VOC Dataset&lt;/h3&gt;
&lt;p&gt;PASCAL VOC: Visual Object Classes&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-download-voc2007-trainval--test" class="anchor" aria-hidden="true" href="#download-voc2007-trainval--test"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download VOC2007 trainval &amp;amp; test&lt;/h5&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; specify a directory for dataset to be downloaded into, else default is ~/data/&lt;/span&gt;
sh data/scripts/VOC2007.sh &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; &amp;lt;directory&amp;gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-download-voc2012-trainval" class="anchor" aria-hidden="true" href="#download-voc2012-trainval"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download VOC2012 trainval&lt;/h5&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; specify a directory for dataset to be downloaded into, else default is ~/data/&lt;/span&gt;
sh data/scripts/VOC2012.sh &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; &amp;lt;directory&amp;gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-training-ssd" class="anchor" aria-hidden="true" href="#training-ssd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training SSD&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First download the fc-reduced &lt;a href="https://arxiv.org/abs/1409.1556" rel="nofollow"&gt;VGG-16&lt;/a&gt; PyTorch base network weights at:              &lt;a href="https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth" rel="nofollow"&gt;https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;By default, we assume you have downloaded the file in the &lt;code&gt;ssd.pytorch/weights&lt;/code&gt; dir:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;mkdir weights
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; weights
wget https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;To train SSD using the train script simply specify the parameters listed in &lt;code&gt;train.py&lt;/code&gt; as a flag or manually change them.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Note:
&lt;ul&gt;
&lt;li&gt;For training, an NVIDIA GPU is strongly recommended for speed.&lt;/li&gt;
&lt;li&gt;For instructions on Visdom usage/installation, see the &lt;a href="#installation"&gt;Installation&lt;/a&gt; section.&lt;/li&gt;
&lt;li&gt;You can pick-up training from a checkpoint by specifying the path as one of the training parameters (again, see &lt;code&gt;train.py&lt;/code&gt; for options)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-evaluation" class="anchor" aria-hidden="true" href="#evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;To evaluate a trained network:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python eval.py&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can specify the parameters listed in the &lt;code&gt;eval.py&lt;/code&gt; file by flagging them or manually changing them.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/amdegroot/ssd.pytorch/blob/master/doc/detection_examples.png"&gt;&lt;img align="left" src="https://github.com/amdegroot/ssd.pytorch/raw/master/doc/detection_examples.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-performance" class="anchor" aria-hidden="true" href="#performance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Performance&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-voc2007-test" class="anchor" aria-hidden="true" href="#voc2007-test"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VOC2007 Test&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-map" class="anchor" aria-hidden="true" href="#map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;mAP&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Original&lt;/th&gt;
&lt;th align="center"&gt;Converted weiliu89 weights&lt;/th&gt;
&lt;th align="center"&gt;From scratch w/o data aug&lt;/th&gt;
&lt;th align="center"&gt;From scratch w/ data aug&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;77.2 %&lt;/td&gt;
&lt;td align="center"&gt;77.26 %&lt;/td&gt;
&lt;td align="center"&gt;58.12%&lt;/td&gt;
&lt;td align="center"&gt;77.43 %&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5&gt;&lt;a id="user-content-fps" class="anchor" aria-hidden="true" href="#fps"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FPS&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;GTX 1060:&lt;/strong&gt; ~45.45 FPS&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-demos" class="anchor" aria-hidden="true" href="#demos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Demos&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-use-a-pre-trained-ssd-network-for-detection" class="anchor" aria-hidden="true" href="#use-a-pre-trained-ssd-network-for-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use a pre-trained SSD network for detection&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-download-a-pre-trained-network" class="anchor" aria-hidden="true" href="#download-a-pre-trained-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download a pre-trained network&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;We are trying to provide PyTorch &lt;code&gt;state_dicts&lt;/code&gt; (dict of weight tensors) of the latest SSD model definitions trained on different datasets.&lt;/li&gt;
&lt;li&gt;Currently, we provide the following PyTorch models:
&lt;ul&gt;
&lt;li&gt;SSD300 trained on VOC0712 (newest PyTorch weights)
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth" rel="nofollow"&gt;https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SSD300 trained on VOC0712 (original Caffe weights)
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://s3.amazonaws.com/amdegroot-models/ssd_300_VOC0712.pth" rel="nofollow"&gt;https://s3.amazonaws.com/amdegroot-models/ssd_300_VOC0712.pth&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Our goal is to reproduce this table from the &lt;a href="http://arxiv.org/abs/1512.02325" rel="nofollow"&gt;original paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="left"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2342b1174a343d30802dc8fc9acd8b59dc2556d5/687474703a2f2f7777772e63732e756e632e6564752f7e776c69752f7061706572732f7373645f726573756c74732e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/2342b1174a343d30802dc8fc9acd8b59dc2556d5/687474703a2f2f7777772e63732e756e632e6564752f7e776c69752f7061706572732f7373645f726573756c74732e706e67" alt="SSD results on multiple datasets" width="800px" data-canonical-src="http://www.cs.unc.edu/~wliu/papers/ssd_results.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-try-the-demo-notebook" class="anchor" aria-hidden="true" href="#try-the-demo-notebook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Try the demo notebook&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Make sure you have &lt;a href="http://jupyter.readthedocs.io/en/latest/install.html" rel="nofollow"&gt;jupyter notebook&lt;/a&gt; installed.&lt;/li&gt;
&lt;li&gt;Two alternatives for installing jupyter notebook:
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;If you installed PyTorch with &lt;a href="https://www.continuum.io/downloads" rel="nofollow"&gt;conda&lt;/a&gt; (recommended), then you should already have it.  (Just  navigate to the ssd.pytorch cloned repo and run):
&lt;code&gt;jupyter notebook&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If using &lt;a href="https://pypi.python.org/pypi/pip" rel="nofollow"&gt;pip&lt;/a&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; make sure pip is upgraded&lt;/span&gt;
pip3 install --upgrade pip
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install jupyter notebook&lt;/span&gt;
pip install jupyter
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run this inside ssd.pytorch&lt;/span&gt;
jupyter notebook&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Now navigate to &lt;code&gt;demo/demo.ipynb&lt;/code&gt; at &lt;a href="http://localhost:8888" rel="nofollow"&gt;http://localhost:8888&lt;/a&gt; (by default) and have at it!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-try-the-webcam-demo" class="anchor" aria-hidden="true" href="#try-the-webcam-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Try the webcam demo&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Works on CPU (may have to tweak &lt;code&gt;cv2.waitkey&lt;/code&gt; for optimal fps) or on an NVIDIA GPU&lt;/li&gt;
&lt;li&gt;This demo currently requires opencv2+ w/ python bindings and an onboard webcam
&lt;ul&gt;
&lt;li&gt;You can change the default webcam in &lt;code&gt;demo/live.py&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Install the &lt;a href="https://github.com/jrosebr1/imutils"&gt;imutils&lt;/a&gt; package to leverage multi-threading on CPU:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pip install imutils&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Running &lt;code&gt;python -m demo.live&lt;/code&gt; opens the webcam and begins detecting!&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;p&gt;We have accumulated the following to-do list, which we hope to complete in the near future&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Still to come:
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Support for the MS COCO dataset&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Support for SSD512 training and testing&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Support for training on custom datasets&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/amdegroot"&gt;&lt;strong&gt;Max deGroot&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/ellisbrown"&gt;&lt;strong&gt;Ellis Brown&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/em&gt; Unfortunately, this is just a hobby of ours and not a full-time job, so we'll do our best to keep things up to date, but no guarantees.  That being said, thanks to everyone for your continued help and feedback as it is really appreciated. We will try to address everything as soon as possible.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Wei Liu, et al. "SSD: Single Shot MultiBox Detector." ECCV2016.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/weiliu89/caffe/tree/ssd"&gt;Original Implementation (CAFFE)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A huge thank you to &lt;a href="https://github.com/alexkoltun"&gt;Alex Koltun&lt;/a&gt; and his team at &lt;a href="http://www.webyclip.com" rel="nofollow"&gt;Webyclip&lt;/a&gt; for their help in finishing the data augmentation portion.&lt;/li&gt;
&lt;li&gt;A list of other great SSD ports that were sources of inspiration (especially the Chainer repo):
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Hakuyume/chainer-ssd"&gt;Chainer&lt;/a&gt;, &lt;a href="https://github.com/rykov8/ssd_keras"&gt;Keras&lt;/a&gt;, &lt;a href="https://github.com/zhreshold/mxnet-ssd"&gt;MXNet&lt;/a&gt;, &lt;a href="https://github.com/balancap/SSD-Tensorflow"&gt;Tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>amdegroot</author><guid isPermaLink="false">https://github.com/amdegroot/ssd.pytorch</guid><pubDate>Sat, 04 Jan 2020 00:23:00 GMT</pubDate></item><item><title>oppia/oppia #24 in Python, Today</title><link>https://github.com/oppia/oppia</link><description>&lt;p&gt;&lt;i&gt;Tool for collaboratively building interactive lessons.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path=".github/README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-oppia----" class="anchor" aria-hidden="true" href="#oppia----"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.oppia.org" rel="nofollow"&gt;Oppia&lt;/a&gt; &lt;a href="https://travis-ci.org/oppia/oppia" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/54efdeb529fc905c38a5ecf5f5846eb20101871f/68747470733a2f2f6170692e7472617669732d63692e6f72672f6f707069612f6f707069612e7376673f6272616e63683d646576656c6f70" alt="Build Status" data-canonical-src="https://api.travis-ci.org/oppia/oppia.svg?branch=develop" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://codeclimate.com/github/oppia/oppia/test_coverage" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/79132c4527d701e6fc3ed4e81c394139a75ed66d/68747470733a2f2f6170692e636f6465636c696d6174652e636f6d2f76312f6261646765732f65616139646665383963373630343831303739642f746573745f636f766572616765" alt="Code Coverage" data-canonical-src="https://api.codeclimate.com/v1/badges/eaa9dfe89c760481079d/test_coverage" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/oppia/oppia" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/779deb490aa71e0fdabdbbfee87b6660337b65f9/68747470733a2f2f636f6465636f762e696f2f67682f6f707069612f6f707069612f6272616e63682f646576656c6f702f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/oppia/oppia/branch/develop/graph/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://gitter.im/oppia/oppia-chat?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" alt="Join the chat at https://gitter.im/oppia/oppia-chat" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Oppia is an online learning tool that enables anyone to easily create and share interactive activities (called 'explorations'). These activities simulate a one-on-one conversation with a tutor, making it possible for students to learn by doing while getting feedback.&lt;/p&gt;
&lt;p&gt;In addition to developing the Oppia platform, the team is also developing and piloting a set of free and effective &lt;a href="https://www.oppia.org/fractions" rel="nofollow"&gt;lessons&lt;/a&gt; on basic mathematics. These lessons are targeted at learners who lack access to educational resources.&lt;/p&gt;
&lt;p&gt;Oppia is written using Python and AngularJS, and is built on top of Google App Engine.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.oppia.org" rel="nofollow"&gt;Oppia.org community site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://oppia.github.io/" rel="nofollow"&gt;User Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/oppia/oppia/wiki"&gt;Contributors' wiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://groups.google.com/group/oppia-dev" rel="nofollow"&gt;Developer mailing list&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/oppia/oppia/issues/new/choose"&gt;File an issue&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
  &lt;a href="http://www.youtube.com/watch?v=Ntcw0H0hwPU" rel="nofollow"&gt;
    &lt;img src="https://cloud.githubusercontent.com/assets/8845039/16814722/b219cac0-4954-11e6-9573-c37557d1b410.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;Please refer to the &lt;a href="https://github.com/oppia/oppia/wiki"&gt;developer wiki&lt;/a&gt; for full installation instructions. This is just a short summary for developers who would like to contribute:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a new, empty folder called &lt;code&gt;opensource/&lt;/code&gt; within your home folder. Navigate to it (&lt;code&gt;cd opensource&lt;/code&gt;), then &lt;a href="https://github.com/oppia/oppia/wiki/Fork-and-Clone-Oppia"&gt;fork and clone&lt;/a&gt; the Oppia repo. This will create a new folder named &lt;code&gt;opensource/oppia&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to &lt;code&gt;opensource/oppia/&lt;/code&gt; and run:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;  git checkout develop
  python -m scripts.start
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="3"&gt;
&lt;li&gt;To run tests:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;  python -m scripts.run_tests
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;The Oppia project is built by the community for the community. We welcome contributions from everyone, especially new contributors.&lt;/p&gt;
&lt;p&gt;You can help with Oppia's development in many ways, including art, coding, design and documentation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Developers&lt;/strong&gt;: please see &lt;a href="https://github.com/oppia/oppia/wiki/Contributing-code-to-Oppia#setting-things-up"&gt;this wiki page&lt;/a&gt; for instructions on how to set things up and commit changes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;All other contributors&lt;/strong&gt;: please see our &lt;a href="https://github.com/oppia/oppia/wiki"&gt;general contributor guidelines&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h2&gt;
&lt;p&gt;If you have any feature requests or bug reports, please log them on our &lt;a href="https://github.com/oppia/oppia/issues/new/choose"&gt;issue tracker&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Please report security issues directly to &lt;a href="mailto:admin@oppia.org"&gt;admin@oppia.org&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;The Oppia code is released under the &lt;a href="https://github.com/oppia/oppia/blob/develop/LICENSE"&gt;Apache v2 license&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-keeping-in-touch" class="anchor" aria-hidden="true" href="#keeping-in-touch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Keeping in touch&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/oppia-org" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://groups.google.com/group/oppia" rel="nofollow"&gt;Discussion forum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://groups.google.com/group/oppia-announce" rel="nofollow"&gt;Announcements mailing list&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Social media: &lt;a href="https://www.youtube.com/channel/UC5c1G7BNDCfv1rczcBp9FPw" rel="nofollow"&gt;YouTube&lt;/a&gt;, &lt;a href="https://www.facebook.com/oppiaorg" rel="nofollow"&gt;FB&lt;/a&gt;, &lt;a href="https://twitter.com/oppiaorg" rel="nofollow"&gt;Twitter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also have public chat rooms on Gitter: &lt;a href="https://gitter.im/oppia/oppia-chat" rel="nofollow"&gt;https://gitter.im/oppia/oppia-chat&lt;/a&gt; and the #oppia channel on Freenode IRC. Drop by and say hello!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>oppia</author><guid isPermaLink="false">https://github.com/oppia/oppia</guid><pubDate>Sat, 04 Jan 2020 00:24:00 GMT</pubDate></item><item><title>pallets/flask #25 in Python, Today</title><link>https://github.com/pallets/flask</link><description>&lt;p&gt;&lt;i&gt;The Python micro framework for building web applications.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-flask" class="anchor" aria-hidden="true" href="#flask"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Flask&lt;/h1&gt;
&lt;p&gt;Flask is a lightweight &lt;a href="https://wsgi.readthedocs.io" rel="nofollow"&gt;WSGI&lt;/a&gt; web application framework. It is designed
to make getting started quick and easy, with the ability to scale up to
complex applications. It began as a simple wrapper around &lt;a href="https://www.palletsprojects.com/p/werkzeug/" rel="nofollow"&gt;Werkzeug&lt;/a&gt;
and &lt;a href="https://www.palletsprojects.com/p/jinja/" rel="nofollow"&gt;Jinja&lt;/a&gt; and has become one of the most popular Python web
application frameworks.&lt;/p&gt;
&lt;p&gt;Flask offers suggestions, but doesn't enforce any dependencies or
project layout. It is up to the developer to choose the tools and
libraries they want to use. There are many extensions provided by the
community that make adding new functionality easy.&lt;/p&gt;
&lt;a name="user-content-installing"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-installing" class="anchor" aria-hidden="true" href="#installing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing&lt;/h2&gt;
&lt;p&gt;Install and update using &lt;a href="https://pip.pypa.io/en/stable/quickstart/" rel="nofollow"&gt;pip&lt;/a&gt;:&lt;/p&gt;
&lt;pre lang="text"&gt;pip install -U Flask
&lt;/pre&gt;
&lt;a name="user-content-a-simple-example"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-a-simple-example" class="anchor" aria-hidden="true" href="#a-simple-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A Simple Example&lt;/h2&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; flask &lt;span class="pl-k"&gt;import&lt;/span&gt; Flask

app &lt;span class="pl-k"&gt;=&lt;/span&gt; Flask(&lt;span class="pl-c1"&gt;__name__&lt;/span&gt;)

&lt;span class="pl-en"&gt;@app.route&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;hello&lt;/span&gt;():
    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Hello, World!&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre lang="text"&gt;$ env FLASK_APP=hello.py flask run
 * Serving Flask app "hello"
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
&lt;/pre&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;For guidance on setting up a development environment and how to make a
contribution to Flask, see the &lt;a href="https://github.com/pallets/flask/blob/master/CONTRIBUTING.rst"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-donate"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-donate" class="anchor" aria-hidden="true" href="#donate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Donate&lt;/h2&gt;
&lt;p&gt;The Pallets organization develops and supports Flask and the libraries
it uses. In order to grow the community of contributors and users, and
allow the maintainers to devote more time to the projects, &lt;a href="https://psfmember.org/civicrm/contribute/transact?reset=1&amp;amp;id=20" rel="nofollow"&gt;please
donate today&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-links"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Website: &lt;a href="https://palletsprojects.com/p/flask/" rel="nofollow"&gt;https://palletsprojects.com/p/flask/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Documentation: &lt;a href="https://flask.palletsprojects.com/" rel="nofollow"&gt;https://flask.palletsprojects.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Releases: &lt;a href="https://pypi.org/project/Flask/" rel="nofollow"&gt;https://pypi.org/project/Flask/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code: &lt;a href="https://github.com/pallets/flask"&gt;https://github.com/pallets/flask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Issue tracker: &lt;a href="https://github.com/pallets/flask/issues"&gt;https://github.com/pallets/flask/issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Test status: &lt;a href="https://dev.azure.com/pallets/flask/_build" rel="nofollow"&gt;https://dev.azure.com/pallets/flask/_build&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Official chat: &lt;a href="https://discord.gg/t6rrQZH" rel="nofollow"&gt;https://discord.gg/t6rrQZH&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>pallets</author><guid isPermaLink="false">https://github.com/pallets/flask</guid><pubDate>Sat, 04 Jan 2020 00:25:00 GMT</pubDate></item></channel></rss>