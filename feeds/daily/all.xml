<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: All Languages, Today</title><link>https://github.com/trending/all?since=daily</link><description>The top repositories on GitHub for all, measured daily</description><pubDate>Tue, 26 Nov 2019 01:07:31 GMT</pubDate><lastBuildDate>Tue, 26 Nov 2019 01:07:31 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>algorithm-visualizer/algorithm-visualizer #1 in All Languages, Today</title><link>https://github.com/algorithm-visualizer/algorithm-visualizer</link><description>&lt;p&gt;&lt;i&gt;:fireworks:Interactive Online Platform that Visualizes Algorithms from Code&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-algorithm-visualizer" class="anchor" aria-hidden="true" href="#algorithm-visualizer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Algorithm Visualizer&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Algorithm Visualizer is an interactive online platform that visualizes algorithms from code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/algorithm-visualizer/algorithm-visualizer/graphs/contributors"&gt;&lt;img src="https://camo.githubusercontent.com/f88cba7c2c657c1120f84e9b26185d4464b8d14b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f616c676f726974686d2d76697375616c697a65722f616c676f726974686d2d76697375616c697a65722e7376673f7374796c653d666c61742d737175617265" alt="GitHub contributors" data-canonical-src="https://img.shields.io/github/contributors/algorithm-visualizer/algorithm-visualizer.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/algorithm-visualizer/algorithm-visualizer/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/b09432c4f695803aa7d0115a5a50d79ce6a5fad9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f616c676f726974686d2d76697375616c697a65722f616c676f726974686d2d76697375616c697a65722e7376673f7374796c653d666c61742d737175617265" alt="GitHub license" data-canonical-src="https://img.shields.io/github/license/algorithm-visualizer/algorithm-visualizer.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Learning an algorithm gets much easier with visualizing it. Don't get what we mean? Check it out:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://algorithm-visualizer.org/" rel="nofollow"&gt;&lt;strong&gt;algorithm-visualizer.org&lt;/strong&gt;&lt;img src="https://raw.githubusercontent.com/algorithm-visualizer/algorithm-visualizer/master/branding/screenshot.png" alt="Screenshot" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We have multiple repositories under the hood that comprise the website. Take a look at the contributing guidelines in the repository you want to contribute to.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/algorithm-visualizer/algorithm-visualizer"&gt;&lt;strong&gt;&lt;code&gt;algorithm-visualizer&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; is a web app written in React. It contains UI components and interprets commands into visualizations. Check out &lt;a href="CONTRIBUTING.md"&gt;the contributing guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/algorithm-visualizer/server"&gt;&lt;strong&gt;&lt;code&gt;server&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; serves the web app and provides APIs that it needs on the fly. (e.g., GitHub sign in, compiling/running code, etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/algorithm-visualizer/algorithms"&gt;&lt;strong&gt;&lt;code&gt;algorithms&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; contains visualizations of algorithms shown on the side menu of the website.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/search?q=topic%3Avisualization-library+org%3Aalgorithm-visualizer&amp;amp;type=Repositories"&gt;&lt;strong&gt;&lt;code&gt;tracers.*&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; are visualization libraries written in each supported language. They extract visualizing commands from code.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>algorithm-visualizer</author><guid isPermaLink="false">https://github.com/algorithm-visualizer/algorithm-visualizer</guid><pubDate>Tue, 26 Nov 2019 00:01:00 GMT</pubDate></item><item><title>Anuken/Mindustry #2 in All Languages, Today</title><link>https://github.com/Anuken/Mindustry</link><description>&lt;p&gt;&lt;i&gt;A sandbox tower defense game&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="core/assets/sprites/logo.png"&gt;&lt;img src="core/assets/sprites/logo.png" alt="Logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/Anuken/Mindustry" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4045163cf028de26f77c5b4cca8a2e6a047729d8/68747470733a2f2f7472617669732d63692e6f72672f416e756b656e2f4d696e6475737472792e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/Anuken/Mindustry.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://discord.gg/mindustry" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cf28333f4842576ffba5fbae8d2d54d329060da1/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3339313032303531303236393636393337362e737667" alt="Discord" data-canonical-src="https://img.shields.io/discord/391020510269669376.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A sandbox tower defense game written in Java.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://trello.com/b/aE2tcUwF/mindustry-40-plans" rel="nofollow"&gt;Trello Board&lt;/a&gt;&lt;/em&gt;&lt;br&gt;
&lt;em&gt;&lt;a href="https://mindustrygame.github.io/wiki" rel="nofollow"&gt;Wiki&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h3&gt;
&lt;p&gt;See &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-building" class="anchor" aria-hidden="true" href="#building"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building&lt;/h3&gt;
&lt;p&gt;Bleeding-edge live builds are generated automatically for every commit. You can see them &lt;a href="https://github.com/Anuken/MindustryBuilds/releases"&gt;here&lt;/a&gt;. Old builds might still be on &lt;a href="https://jenkins.hellomouse.net/job/mindustry/" rel="nofollow"&gt;jenkins&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you'd rather compile on your own, follow these instructions.
First, make sure you have &lt;a href="https://www.java.com/en/download/" rel="nofollow"&gt;Java 8&lt;/a&gt; and &lt;a href="https://adoptopenjdk.net/" rel="nofollow"&gt;JDK 8&lt;/a&gt; installed. Open a terminal in the root directory, &lt;code&gt;cd&lt;/code&gt; to the Mindustry folder and run the following commands:&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-windows" class="anchor" aria-hidden="true" href="#windows"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Windows&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Running:&lt;/em&gt; &lt;code&gt;gradlew desktop:run&lt;/code&gt;&lt;br&gt;
&lt;em&gt;Building:&lt;/em&gt; &lt;code&gt;gradlew desktop:dist&lt;/code&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-linuxmac-os" class="anchor" aria-hidden="true" href="#linuxmac-os"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linux/Mac OS&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Running:&lt;/em&gt; &lt;code&gt;./gradlew desktop:run&lt;/code&gt;&lt;br&gt;
&lt;em&gt;Building:&lt;/em&gt; &lt;code&gt;./gradlew desktop:dist&lt;/code&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-server" class="anchor" aria-hidden="true" href="#server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Server&lt;/h4&gt;
&lt;p&gt;Server builds are bundled with each released build (in Releases). If you'd rather compile on your own, replace 'desktop' with 'server', e.g. &lt;code&gt;gradlew server:dist&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-android" class="anchor" aria-hidden="true" href="#android"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Android&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Install the Android SDK &lt;a href="https://developer.android.com/studio#downloads" rel="nofollow"&gt;here.&lt;/a&gt; Make sure you're downloading the "Command line tools only", as Android Studio is not required.&lt;/li&gt;
&lt;li&gt;Create a file named &lt;code&gt;local.properties&lt;/code&gt; inside the Mindustry directory, with its contents looking like this: &lt;code&gt;sdk.dir=&amp;lt;Path to Android SDK you just downloaded, without these bracket&amp;gt;&lt;/code&gt;. For example, if you're on Windows and installed the tools to C:\tools, your local.properties would contain &lt;code&gt;sdk.dir=C:\\tools&lt;/code&gt; (&lt;em&gt;note the double backslashes are required instead of single ones!&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;gradlew android:assembleDebug&lt;/code&gt; (or &lt;code&gt;./gradlew&lt;/code&gt; if on linux/mac). This will create an unsigned APK in &lt;code&gt;android/build/outputs/apk&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;(Optional) To debug the application on a connected phone, do &lt;code&gt;gradlew android:installDebug android:run&lt;/code&gt;. It is &lt;strong&gt;highly recommended&lt;/strong&gt; to use IntelliJ for this instead, however.&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-troubleshooting" class="anchor" aria-hidden="true" href="#troubleshooting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Troubleshooting&lt;/h5&gt;
&lt;p&gt;If the terminal returns &lt;code&gt;Permission denied&lt;/code&gt; or &lt;code&gt;Command not found&lt;/code&gt; on Mac/Linux, run &lt;code&gt;chmod +x ./gradlew&lt;/code&gt; before running &lt;code&gt;./gradlew&lt;/code&gt;. &lt;em&gt;This is a one-time procedure.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Gradle may take up to several minutes to download files. Be patient. &lt;br&gt;
After building, the output .JAR file should be in &lt;code&gt;/desktop/build/libs/Mindustry.jar&lt;/code&gt; for desktop builds, and in &lt;code&gt;/server/build/libs/server-release.jar&lt;/code&gt; for server builds.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-feature-requests" class="anchor" aria-hidden="true" href="#feature-requests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Feature Requests&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://feathub.com/Anuken/Mindustry" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cf561bb2279b8cfad6bc2f98ababf6e5a0d7df41/68747470733a2f2f666561746875622e636f6d2f416e756b656e2f4d696e6475737472793f666f726d61743d737667" alt="Feature Requests" data-canonical-src="https://feathub.com/Anuken/Mindustry?format=svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-downloads" class="anchor" aria-hidden="true" href="#downloads"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloads&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://anuke.itch.io/mindustry" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f0a65e7bc4221accc62c5acd4c82a20571968278/68747470733a2f2f7374617469632e697463682e696f2f696d616765732f62616467652e737667" alt="Get it on Itch.io" height="60" data-canonical-src="https://static.itch.io/images/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://play.google.com/store/apps/details?id=io.anuke.mindustry" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/bdaf711a93d64d0bb5e5abfc346a8b84ea47f164/68747470733a2f2f706c61792e676f6f676c652e636f6d2f696e746c2f656e5f75732f6261646765732f696d616765732f67656e657269632f656e2d706c61792d62616467652e706e67" alt="Get it on Google Play" height="80" data-canonical-src="https://play.google.com/intl/en_us/badges/images/generic/en-play-badge.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://f-droid.org/packages/io.anuke.mindustry/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/376ad76c59e6f46c3069c2765c722d5aa65b2d31/68747470733a2f2f6664726f69642e6769746c61622e696f2f617274776f726b2f62616467652f6765742d69742d6f6e2e706e67" alt="Get it on F-Droid" height="80" data-canonical-src="https://fdroid.gitlab.io/artwork/badge/get-it-on.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Anuken</author><guid isPermaLink="false">https://github.com/Anuken/Mindustry</guid><pubDate>Tue, 26 Nov 2019 00:02:00 GMT</pubDate></item><item><title>leandromoreira/digital_video_introduction #3 in All Languages, Today</title><link>https://github.com/leandromoreira/digital_video_introduction</link><description>&lt;p&gt;&lt;i&gt;A hands-on introduction to video technology: image, video, codec (av1, vp9, h265) and more (ffmpeg encoding).&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="/README-cn.md" title="Simplified Chinese"&gt;&lt;g-emoji class="g-emoji" alias="cn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1e8-1f1f3.png"&gt;🇨🇳&lt;/g-emoji&gt;&lt;/a&gt;
&lt;a href="/README-ja.md" title="Japanese"&gt;&lt;g-emoji class="g-emoji" alias="jp" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1ef-1f1f5.png"&gt;🇯🇵&lt;/g-emoji&gt;&lt;/a&gt;
&lt;a href="/README-it.md" title="Italian"&gt;&lt;g-emoji class="g-emoji" alias="it" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1ee-1f1f9.png"&gt;🇮🇹&lt;/g-emoji&gt;&lt;/a&gt;
&lt;a href="/README-ko.md" title="Korean"&gt;&lt;g-emoji class="g-emoji" alias="kr" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f0-1f1f7.png"&gt;🇰🇷&lt;/g-emoji&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://img.shields.io/badge/license-BSD--3--Clause-blue.svg" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/274d07206c413193cf01e32de7f897d98da66ca2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4253442d2d332d2d436c617573652d626c75652e737667" alt="license" data-canonical-src="https://img.shields.io/badge/license-BSD--3--Clause-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-intro" class="anchor" aria-hidden="true" href="#intro"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Intro&lt;/h1&gt;
&lt;p&gt;A gentle introduction to video technology, although it's aimed at software developers / engineers, we want to make it easy &lt;strong&gt;for anyone to learn&lt;/strong&gt;. This idea was born during a &lt;a href="https://docs.google.com/presentation/d/17Z31kEkl_NGJ0M66reqr9_uTG6tI5EDDVXpdPKVuIrs/edit#slide=id.p" rel="nofollow"&gt;mini workshop for newcomers to video technology&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The goal is to introduce some digital video concepts with a &lt;strong&gt;simple vocabulary, lots of visual elements and practical examples&lt;/strong&gt; when possible, and make this knowledge available everywhere. Please, feel free to send corrections, suggestions and improve it.&lt;/p&gt;
&lt;p&gt;There will be &lt;strong&gt;hands-on&lt;/strong&gt; sections which require you to have &lt;strong&gt;docker installed&lt;/strong&gt; and this repository cloned.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/leandromoreira/digital_video_introduction.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; digital_video_introduction
./setup.sh&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: when you see a &lt;code&gt;./s/ffmpeg&lt;/code&gt; or &lt;code&gt;./s/mediainfo&lt;/code&gt; command, it means we're running a &lt;strong&gt;containerized version&lt;/strong&gt; of that program, which already includes all the needed requirements.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All the &lt;strong&gt;hands-on should be performed from the folder you cloned&lt;/strong&gt; this repository. For the &lt;strong&gt;jupyter examples&lt;/strong&gt; you must start the server &lt;code&gt;./s/start_jupyter.sh&lt;/code&gt; and copy the URL and use it in your browser.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;added DRM system&lt;/li&gt;
&lt;li&gt;released version 1.0.0&lt;/li&gt;
&lt;li&gt;added simplified Chinese translation&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-index" class="anchor" aria-hidden="true" href="#index"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Index&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#intro"&gt;Intro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#index"&gt;Index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#basic-terminology"&gt;Basic terminology&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#other-ways-to-encode-a-color-image"&gt;Other ways to encode a color image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hands-on-play-around-with-image-and-color"&gt;Hands-on: play around with image and color&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dvd-is-dar-43"&gt;DVD is DAR 4:3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hands-on-check-video-properties"&gt;Hands-on: Check video properties&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#redundancy-removal"&gt;Redundancy removal&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#colors-luminance-and-our-eyes"&gt;Colors, Luminance and our eyes&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#color-model"&gt;Color model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#converting-between-ycbcr-and-rgb"&gt;Converting between YCbCr and RGB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#chroma-subsampling"&gt;Chroma subsampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hands-on-check-ycbcr-histogram"&gt;Hands-on: Check YCbCr histogram&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#frame-types"&gt;Frame types&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#i-frame-intra-keyframe"&gt;I Frame (intra, keyframe)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#p-frame-predicted"&gt;P Frame (predicted)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-a-video-with-a-single-i-frame"&gt;Hands-on: A video with a single I-frame&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#b-frame-bi-predictive"&gt;B Frame (bi-predictive)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-compare-videos-with-b-frame"&gt;Hands-on: Compare videos with B-frame&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#temporal-redundancy-inter-prediction"&gt;Temporal redundancy (inter prediction)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-see-the-motion-vectors"&gt;Hands-on: See the motion vectors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#spatial-redundancy-intra-prediction"&gt;Spatial redundancy (intra prediction)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-check-intra-predictions"&gt;Hands-on: Check intra predictions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-does-a-video-codec-work"&gt;How does a video codec work?&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-why-how"&gt;What? Why? How?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#history"&gt;History&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#the-birth-of-av1"&gt;The birth of AV1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#a-generic-codec"&gt;A generic codec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1st-step---picture-partitioning"&gt;1st step - picture partitioning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-check-partitions"&gt;Hands-on: Check partitions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#2nd-step---predictions"&gt;2nd step - predictions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3rd-step---transform"&gt;3rd step - transform&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-throwing-away-different-coefficients"&gt;Hands-on: throwing away different coefficients&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4th-step---quantization"&gt;4th step - quantization&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-quantization"&gt;Hands-on: quantization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5th-step---entropy-coding"&gt;5th step - entropy coding&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#vlc-coding"&gt;VLC coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#arithmetic-coding"&gt;Arithmetic coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hands-on-cabac-vs-cavlc"&gt;Hands-on: CABAC vs CAVLC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#6th-step---bitstream-format"&gt;6th step - bitstream format&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#h264-bitstream"&gt;H.264 bitstream&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hands-on-inspect-the-h264-bitstream"&gt;Hands-on: Inspect the H.264 bitstream&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#review"&gt;Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-does-h265-achieve-a-better-compression-ratio-than-h264"&gt;How does H.265 achieve a better compression ratio than H.264?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#online-streaming"&gt;Online streaming&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#general-architecture"&gt;General architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#progressive-download-and-adaptive-streaming"&gt;Progressive download and adaptive streaming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#content-protection"&gt;Content protection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-use-jupyter"&gt;How to use jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conferences"&gt;Conferences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-basic-terminology" class="anchor" aria-hidden="true" href="#basic-terminology"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic terminology&lt;/h1&gt;
&lt;p&gt;An &lt;strong&gt;image&lt;/strong&gt; can be thought of as a &lt;strong&gt;2D matrix&lt;/strong&gt;. If we think about &lt;strong&gt;colors&lt;/strong&gt;, we can extrapolate this idea seeing this image as a &lt;strong&gt;3D matrix&lt;/strong&gt; where the &lt;strong&gt;additional dimensions&lt;/strong&gt; are used to provide &lt;strong&gt;color data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If we chose to represent these colors using the &lt;a href="https://en.wikipedia.org/wiki/Primary_color" rel="nofollow"&gt;primary colors (red, green and blue)&lt;/a&gt;, we define three planes: the first one for &lt;strong&gt;red&lt;/strong&gt;, the second for &lt;strong&gt;green&lt;/strong&gt;, and the last one for the &lt;strong&gt;blue&lt;/strong&gt; color.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/image_3d_matrix_rgb.png"&gt;&lt;img src="/i/image_3d_matrix_rgb.png" alt="an image is a 3d matrix RGB" title="An image is a 3D matrix" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We'll call each point in this matrix &lt;strong&gt;a pixel&lt;/strong&gt; (picture element). One pixel represents the &lt;strong&gt;intensity&lt;/strong&gt; (usually a numeric value) of a given color. For example, a &lt;strong&gt;red pixel&lt;/strong&gt; means 0 of green, 0 of blue and maximum of red. The &lt;strong&gt;pink color pixel&lt;/strong&gt; can be formed with a combination of the three colors. Using a representative numeric range from 0 to 255, the pink pixel is defined by &lt;strong&gt;Red=255, Green=192 and Blue=203&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-other-ways-to-encode-a-color-image" class="anchor" aria-hidden="true" href="#other-ways-to-encode-a-color-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other ways to encode a color image&lt;/h4&gt;
&lt;p&gt;Many other possible models may be used to represent the colors that make up an image. We could, for instance, use an indexed palette where we'd only need a single byte to represent each pixel instead of the 3 needed when using the RGB model. In such a model we could use a 2D matrix instead of a 3D matrix to represent our color, this would save on memory but yield fewer color options.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/nes-color-palette.png"&gt;&lt;img src="/i/nes-color-palette.png" alt="NES palette" title="NES palette" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For instance, look at the picture down below. The first face is fully colored. The others are the red, green, and blue planes (shown as gray tones).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/rgb_channels_intensity.png"&gt;&lt;img src="/i/rgb_channels_intensity.png" alt="RGB channels intensity" title="RGB channels intensity" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can see that the &lt;strong&gt;red color&lt;/strong&gt; will be the one that &lt;strong&gt;contributes more&lt;/strong&gt; (the brightest parts in the second face) to the final color while the &lt;strong&gt;blue color&lt;/strong&gt; contribution can be mostly &lt;strong&gt;only seen in Mario's eyes&lt;/strong&gt; (last face) and part of his clothes, see how &lt;strong&gt;all planes contribute less&lt;/strong&gt; (darkest parts) to the &lt;strong&gt;Mario's mustache&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;And each color intensity requires a certain amount of bits, this quantity is known as &lt;strong&gt;bit depth&lt;/strong&gt;. Let's say we spend &lt;strong&gt;8 bits&lt;/strong&gt; (accepting values from 0 to 255) per color (plane), therefore we have a &lt;strong&gt;color depth&lt;/strong&gt; of &lt;strong&gt;24 bits&lt;/strong&gt; (8 bits * 3 planes R/G/B), and we can also infer that we could use 2 to the power of 24 different colors.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;It's great&lt;/strong&gt; to learn &lt;a href="http://www.cambridgeincolour.com/tutorials/camera-sensors.htm" rel="nofollow"&gt;how an image is captured from the world to the bits&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another property of an image is the &lt;strong&gt;resolution&lt;/strong&gt;, which is the number of pixels in one dimension. It is often presented as width × height, for example, the &lt;strong&gt;4×4&lt;/strong&gt; image below.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/resolution.png"&gt;&lt;img src="/i/resolution.png" alt="image resolution" title="image resolution" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-hands-on-play-around-with-image-and-color" class="anchor" aria-hidden="true" href="#hands-on-play-around-with-image-and-color"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: play around with image and color&lt;/h4&gt;
&lt;p&gt;You can &lt;a href="/image_as_3d_array.ipynb"&gt;play around with image and colors&lt;/a&gt; using &lt;a href="#how-to-use-jupyter"&gt;jupyter&lt;/a&gt; (python, numpy, matplotlib and etc).&lt;/p&gt;
&lt;p&gt;You can also learn &lt;a href="/filters_are_easy.ipynb"&gt;how image filters (edge detection, sharpen, blur...) work&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another property we can see while working with images or video is the &lt;strong&gt;aspect ratio&lt;/strong&gt; which simply describes the proportional relationship between width and height of an image or pixel.&lt;/p&gt;
&lt;p&gt;When people says this movie or picture is &lt;strong&gt;16x9&lt;/strong&gt; they usually are referring to the &lt;strong&gt;Display Aspect Ratio (DAR)&lt;/strong&gt;, however we also can have different shapes of individual pixels, we call this &lt;strong&gt;Pixel Aspect Ratio (PAR)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/DAR.png"&gt;&lt;img src="/i/DAR.png" alt="display aspect ratio" title="display aspect ratio" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/PAR.png"&gt;&lt;img src="/i/PAR.png" alt="pixel aspect ratio" title="pixel aspect ratio" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-dvd-is-dar-43" class="anchor" aria-hidden="true" href="#dvd-is-dar-43"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DVD is DAR 4:3&lt;/h4&gt;
&lt;p&gt;Although the real resolution of a DVD is 704x480 it still keeps a 4:3 aspect ratio because it has a PAR of 10:11 (704x10/480x11)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Finally, we can define a &lt;strong&gt;video&lt;/strong&gt; as a &lt;strong&gt;succession of &lt;em&gt;n&lt;/em&gt; frames&lt;/strong&gt; in &lt;strong&gt;time&lt;/strong&gt; which can be seen as another dimension, &lt;em&gt;n&lt;/em&gt; is the frame rate or frames per second (FPS).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/video.png"&gt;&lt;img src="/i/video.png" alt="video" title="video" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The number of bits per second needed to show a video is its &lt;strong&gt;bit rate&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;bit rate = width * height * bit depth * frames per second&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, a video with 30 frames per second, 24 bits per pixel, resolution of 480x240 will need &lt;strong&gt;82,944,000 bits per second&lt;/strong&gt; or 82.944 Mbps (30x480x240x24) if we don't employ any kind of compression.&lt;/p&gt;
&lt;p&gt;When the &lt;strong&gt;bit rate&lt;/strong&gt; is nearly constant it's called constant bit rate (&lt;strong&gt;CBR&lt;/strong&gt;) but it also can vary then called variable bit rate (&lt;strong&gt;VBR&lt;/strong&gt;).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This graph shows a constrained VBR which doesn't spend too many bits while the frame is black.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/vbr.png"&gt;&lt;img src="/i/vbr.png" alt="constrained vbr" title="constrained vbr" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the early days, engineers came up with a technique for doubling the perceived frame rate of a video display &lt;strong&gt;without consuming extra bandwidth&lt;/strong&gt;. This technique is known as &lt;strong&gt;interlaced video&lt;/strong&gt;; it basically sends half of the screen in 1 "frame" and the other half in the next "frame".&lt;/p&gt;
&lt;p&gt;Today screens render mostly using &lt;strong&gt;progressive scan technique&lt;/strong&gt;. Progressive is a way of displaying, storing, or transmitting moving images in which all the lines of each frame are drawn in sequence.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/interlaced_vs_progressive.png"&gt;&lt;img src="/i/interlaced_vs_progressive.png" alt="interlaced vs progressive" title="interlaced vs progressive" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now we have an idea about how an &lt;strong&gt;image&lt;/strong&gt; is represented digitally, how its &lt;strong&gt;colors&lt;/strong&gt; are arranged, how many &lt;strong&gt;bits per second&lt;/strong&gt; do we spend to show a video, if it's constant (CBR)  or variable (VBR), with a given &lt;strong&gt;resolution&lt;/strong&gt; using a given &lt;strong&gt;frame rate&lt;/strong&gt; and many other terms such as interlaced, PAR and others.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-hands-on-check-video-properties" class="anchor" aria-hidden="true" href="#hands-on-check-video-properties"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: Check video properties&lt;/h4&gt;
&lt;p&gt;You can &lt;a href="https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#inspect-stream"&gt;check most of the  explained properties with ffmpeg or mediainfo.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-redundancy-removal" class="anchor" aria-hidden="true" href="#redundancy-removal"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Redundancy removal&lt;/h1&gt;
&lt;p&gt;We learned that it's not feasible to use video without any compression; &lt;strong&gt;a single one hour video&lt;/strong&gt; at 720p resolution with 30fps would &lt;strong&gt;require 278GB&lt;sup&gt;*&lt;/sup&gt;&lt;/strong&gt;. Since &lt;strong&gt;using solely lossless data compression algorithms&lt;/strong&gt; like DEFLATE (used in PKZIP, Gzip, and PNG), &lt;strong&gt;won't&lt;/strong&gt; decrease the required bandwidth sufficiently we need to find other ways to compress the video.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; We found this number by multiplying 1280 x 720 x 24 x 30 x 3600 (width, height, bits per pixel, fps and time in seconds)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In order to do this, we can &lt;strong&gt;exploit how our vision works&lt;/strong&gt;. We're better at distinguishing brightness than colors, the &lt;strong&gt;repetitions in time&lt;/strong&gt;, a video contains a lot of images with few changes, and the &lt;strong&gt;repetitions within the image&lt;/strong&gt;, each frame also contains many areas using the same or similar color.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-colors-luminance-and-our-eyes" class="anchor" aria-hidden="true" href="#colors-luminance-and-our-eyes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Colors, Luminance and our eyes&lt;/h2&gt;
&lt;p&gt;Our eyes are &lt;a href="http://vanseodesign.com/web-design/color-luminance/" rel="nofollow"&gt;more sensitive to brightness than colors&lt;/a&gt;, you can test it for yourself, look at this picture.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/luminance_vs_color.png"&gt;&lt;img src="/i/luminance_vs_color.png" alt="luminance vs color" title="luminance vs color" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you are unable to see that the colors of the &lt;strong&gt;squares A and B are identical&lt;/strong&gt; on the left side, that's fine, it's our brain playing tricks on us to &lt;strong&gt;pay more attention to light and dark than color&lt;/strong&gt;. There is a connector, with the same color, on the right side so we (our brain) can easily spot that in fact, they're the same color.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Simplistic explanation of how our eyes work&lt;/strong&gt;
The &lt;a href="http://www.biologymad.com/nervoussystem/eyenotes.htm" rel="nofollow"&gt;eye is a complex organ&lt;/a&gt;, it is composed of many parts but we are mostly interested in the cones and rods cells. The eye &lt;a href="https://en.wikipedia.org/wiki/Photoreceptor_cell" rel="nofollow"&gt;contains about 120 million rod cells and 6 million cone cells&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To &lt;strong&gt;oversimplify&lt;/strong&gt;, let's try to put colors and brightness in the eye's parts function. The &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Rod_cell" rel="nofollow"&gt;rod cells&lt;/a&gt; are mostly responsible for brightness&lt;/strong&gt; while the &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Cone_cell" rel="nofollow"&gt;cone cells&lt;/a&gt; are responsible for color&lt;/strong&gt;, there are three types of cones, each with different pigment, namely: &lt;a href="https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg" rel="nofollow"&gt;S-cones (Blue), M-cones (Green) and L-cones (Red)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since we have many more rod cells (brightness) than cone cells (color), one can infer that we are more capable of distinguishing dark and light than colors.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/eyes.jpg"&gt;&lt;img src="/i/eyes.jpg" alt="eyes composition" title="eyes composition" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contrast sensitivity functions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Researchers of experimental psychology and many other fields have developed many theories on human vision. And one of them is called Contrast sensitivity functions. They are related to spatio and temporal of the light and their value presents at given init light, how much change is required before an observer reported there was a change. Notice the plural of the word "function", this is for the reason that we can measure Contrast sensitivity functions with not only black-white but also colors. The result of these experiments shows that in most cases our eyes are more sensitive to brightness than color.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Once we know that we're more sensitive to &lt;strong&gt;luma&lt;/strong&gt; (the brightness in an image) we can try to exploit it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-color-model" class="anchor" aria-hidden="true" href="#color-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Color model&lt;/h3&gt;
&lt;p&gt;We first learned &lt;a href="#basic-terminology"&gt;how to color images&lt;/a&gt; work using the &lt;strong&gt;RGB model&lt;/strong&gt;, but there are other models too. In fact, there is a model that separates luma (brightness) from  chrominance (colors) and it is known as &lt;strong&gt;YCbCr&lt;/strong&gt;&lt;sup&gt;*&lt;/sup&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; there are more models which do the same separation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This color model uses &lt;strong&gt;Y&lt;/strong&gt; to represent the brightness and two color channels &lt;strong&gt;Cb&lt;/strong&gt; (chroma blue) and &lt;strong&gt;Cr&lt;/strong&gt; (chroma red). The &lt;a href="https://en.wikipedia.org/wiki/YCbCr" rel="nofollow"&gt;YCbCr&lt;/a&gt; can be derived from RGB and it also can be converted back to RGB. Using this model we can create full colored images as we can see down below.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/ycbcr.png"&gt;&lt;img src="/i/ycbcr.png" alt="ycbcr example" title="ycbcr example" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-converting-between-ycbcr-and-rgb" class="anchor" aria-hidden="true" href="#converting-between-ycbcr-and-rgb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Converting between YCbCr and RGB&lt;/h3&gt;
&lt;p&gt;Some may argue, how can we produce all the &lt;strong&gt;colors without using the green&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;To answer this question, we'll walk through a conversion from RGB to YCbCr. We'll use the coefficients from the &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Rec._601" rel="nofollow"&gt;standard BT.601&lt;/a&gt;&lt;/strong&gt; that was recommended by the &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/ITU-R" rel="nofollow"&gt;group ITU-R&lt;sup&gt;*&lt;/sup&gt;&lt;/a&gt;&lt;/strong&gt; . The first step is to &lt;strong&gt;calculate the luma&lt;/strong&gt;, we'll use the constants suggested by ITU and replace the RGB values.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Y = 0.299R + 0.587G + 0.114B
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we had the luma, we can &lt;strong&gt;split the colors&lt;/strong&gt; (chroma blue and red):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Cb = 0.564(B - Y)
Cr = 0.713(R - Y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can also &lt;strong&gt;convert it back&lt;/strong&gt; and even get the &lt;strong&gt;green by using YCbCr&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;R = Y + 1.402Cr
B = Y + 1.772Cb
G = Y - 0.344Cb - 0.714Cr
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; groups and standards are common in digital video, they usually define what are the standards, for instance, &lt;a href="https://en.wikipedia.org/wiki/Rec._2020" rel="nofollow"&gt;what is 4K? what frame rate should we use? resolution? color model?&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Generally, &lt;strong&gt;displays&lt;/strong&gt; (monitors, TVs, screens and etc) utilize &lt;strong&gt;only the RGB model&lt;/strong&gt;, organized in different manners, see some of them magnified below:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/new_pixel_geometry.jpg"&gt;&lt;img src="/i/new_pixel_geometry.jpg" alt="pixel geometry" title="pixel geometry" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-chroma-subsampling" class="anchor" aria-hidden="true" href="#chroma-subsampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Chroma subsampling&lt;/h3&gt;
&lt;p&gt;With the image represented as luma and chroma components, we can take advantage of the human visual system's greater sensitivity for luma resolution rather than chroma to selectively remove information. &lt;strong&gt;Chroma subsampling&lt;/strong&gt; is the technique of encoding images using &lt;strong&gt;less resolution for chroma than for luma&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/ycbcr_subsampling_resolution.png"&gt;&lt;img src="/i/ycbcr_subsampling_resolution.png" alt="ycbcr subsampling resolutions" title="ycbcr subsampling resolutions" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;How much should we reduce the chroma resolution?! It turns out that there are already some schemas that describe how to handle resolution and the merge (&lt;code&gt;final color = Y + Cb + Cr&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;These schemas are known as subsampling systems and are expressed as a 3 part ratio - &lt;code&gt;a:x:y&lt;/code&gt; which defines the chroma resolution in relation to a &lt;code&gt;a x 2&lt;/code&gt; block of luma pixels.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt; is the horizontal sampling reference (usually 4)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is the number of chroma samples in the first row of &lt;code&gt;a&lt;/code&gt; pixels (horizontal resolution in relation to &lt;code&gt;a&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; is the number of changes of chroma samples between the first and seconds rows of &lt;code&gt;a&lt;/code&gt; pixels.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;An exception to this exists with 4:1:0, which provides a single chroma sample within each &lt;code&gt;4 x 4&lt;/code&gt; block of luma resolution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Common schemes used in modern codecs are: &lt;strong&gt;4:4:4&lt;/strong&gt; &lt;em&gt;(no subsampling)&lt;/em&gt;, &lt;strong&gt;4:2:2, 4:1:1, 4:2:0, 4:1:0 and 3:1:1&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;YCbCr 4:2:0 merge&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here's a merged piece of an image using YCbCr 4:2:0, notice that we only spend 12 bits per pixel.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/ycbcr_420_merge.png"&gt;&lt;img src="/i/ycbcr_420_merge.png" alt="YCbCr 4:2:0 merge" title="YCbCr 4:2:0 merge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can see the same image encoded by the main chroma subsampling types, images in the first row are the final YCbCr while the last row of images shows the chroma resolution. It's indeed a great win for such small loss.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/chroma_subsampling_examples.jpg"&gt;&lt;img src="/i/chroma_subsampling_examples.jpg" alt="chroma subsampling examples" title="chroma subsampling examples" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Previously we had calculated that we needed &lt;a href="#redundancy-removal"&gt;278GB of storage to keep a video file with one hour at 720p resolution and 30fps&lt;/a&gt;. If we use &lt;strong&gt;YCbCr 4:2:0&lt;/strong&gt; we can cut &lt;strong&gt;this size in half (139 GB)&lt;/strong&gt;&lt;sup&gt;*&lt;/sup&gt; but it is still far from ideal.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; we found this value by multiplying width, height, bits per pixel and fps. Previously we needed 24 bits, now we only need 12.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-hands-on-check-ycbcr-histogram" class="anchor" aria-hidden="true" href="#hands-on-check-ycbcr-histogram"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: Check YCbCr histogram&lt;/h3&gt;
&lt;p&gt;You can &lt;a href="/encoding_pratical_examples.md#generates-yuv-histogram"&gt;check the YCbCr histogram with ffmpeg.&lt;/a&gt; This scene has a higher blue contribution, which is showed by the &lt;a href="https://en.wikipedia.org/wiki/Histogram" rel="nofollow"&gt;histogram&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/yuv_histogram.png"&gt;&lt;img src="/i/yuv_histogram.png" alt="ycbcr color histogram" title="ycbcr color histogram" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-frame-types" class="anchor" aria-hidden="true" href="#frame-types"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Frame types&lt;/h2&gt;
&lt;p&gt;Now we can move on and try to eliminate the &lt;strong&gt;redundancy in time&lt;/strong&gt; but before that let's establish some basic terminology. Suppose we have a movie with 30fps, here are its first 4 frames.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_1.png"&gt;&lt;img src="/i/smw_background_ball_1.png" alt="ball 1" title="ball 1" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_2.png"&gt;&lt;img src="/i/smw_background_ball_2.png" alt="ball 2" title="ball 2" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_3.png"&gt;&lt;img src="/i/smw_background_ball_3.png" alt="ball 3" title="ball 3" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_4.png"&gt;&lt;img src="/i/smw_background_ball_4.png" alt="ball 4" title="ball 4" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can see &lt;strong&gt;lots of repetitions&lt;/strong&gt; within frames like &lt;strong&gt;the blue background&lt;/strong&gt;, it doesn't change from frame 0 to frame 3. To tackle this problem, we can &lt;strong&gt;abstractly categorize&lt;/strong&gt; them as three types of frames.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-i-frame-intra-keyframe" class="anchor" aria-hidden="true" href="#i-frame-intra-keyframe"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I Frame (intra, keyframe)&lt;/h3&gt;
&lt;p&gt;An I-frame (reference, keyframe, intra) is a &lt;strong&gt;self-contained frame&lt;/strong&gt;. It doesn't rely on anything to be rendered, an I-frame looks similar to a static photo. The first frame is usually an I-frame but we'll see I-frames inserted regularly among other types of frames.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_1.png"&gt;&lt;img src="/i/smw_background_ball_1.png" alt="ball 1" title="ball 1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-p-frame-predicted" class="anchor" aria-hidden="true" href="#p-frame-predicted"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;P Frame (predicted)&lt;/h3&gt;
&lt;p&gt;A P-frame takes advantage of the fact that almost always the current picture can be &lt;strong&gt;rendered using the previous frame.&lt;/strong&gt; For instance, in the second frame, the only change was the ball that moved forward. We can &lt;strong&gt;rebuild frame 1, only using the difference and referencing to the previous frame&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_1.png"&gt;&lt;img src="/i/smw_background_ball_1.png" alt="ball 1" title="ball 1" style="max-width:100%;"&gt;&lt;/a&gt; &amp;lt;-  &lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_2_diff.png"&gt;&lt;img src="/i/smw_background_ball_2_diff.png" alt="ball 2" title="ball 2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-hands-on-a-video-with-a-single-i-frame" class="anchor" aria-hidden="true" href="#hands-on-a-video-with-a-single-i-frame"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: A video with a single I-frame&lt;/h4&gt;
&lt;p&gt;Since a P-frame uses less data why can't we encode an entire &lt;a href="/encoding_pratical_examples.md#1-i-frame-and-the-rest-p-frames"&gt;video with a single I-frame and all the rest being P-frames?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After you encoded this video, start to watch it and do a &lt;strong&gt;seek for an advanced&lt;/strong&gt; part of the video, you'll notice &lt;strong&gt;it takes some time&lt;/strong&gt; to really move to that part. That's because a &lt;strong&gt;P-frame needs a reference frame&lt;/strong&gt; (I-frame for instance) to be rendered.&lt;/p&gt;
&lt;p&gt;Another quick test you can do is to encode a video using a single I-Frame and then &lt;a href="/encoding_pratical_examples.md#1-i-frames-per-second-vs-05-i-frames-per-second"&gt;encode it inserting an I-frame each 2s&lt;/a&gt; and &lt;strong&gt;check the size of each rendition&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-b-frame-bi-predictive" class="anchor" aria-hidden="true" href="#b-frame-bi-predictive"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;B Frame (bi-predictive)&lt;/h3&gt;
&lt;p&gt;What about referencing the past and future frames to provide even a better compression?! That's basically what a B-frame is.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_1.png"&gt;&lt;img src="/i/smw_background_ball_1.png" alt="ball 1" title="ball 1" style="max-width:100%;"&gt;&lt;/a&gt; &amp;lt;-  &lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_2_diff.png"&gt;&lt;img src="/i/smw_background_ball_2_diff.png" alt="ball 2" title="ball 2" style="max-width:100%;"&gt;&lt;/a&gt; -&amp;gt; &lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_3.png"&gt;&lt;img src="/i/smw_background_ball_3.png" alt="ball 3" title="ball 3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-hands-on-compare-videos-with-b-frame" class="anchor" aria-hidden="true" href="#hands-on-compare-videos-with-b-frame"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: Compare videos with B-frame&lt;/h4&gt;
&lt;p&gt;You can generate two renditions, first with B-frames and other with &lt;a href="/encoding_pratical_examples.md#no-b-frames-at-all"&gt;no B-frames at all&lt;/a&gt; and check the size of the file as well as the quality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-summary" class="anchor" aria-hidden="true" href="#summary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Summary&lt;/h3&gt;
&lt;p&gt;These frames types are used to &lt;strong&gt;provide better compression&lt;/strong&gt;. We'll look how this happens in the next section, but for now we can think of &lt;strong&gt;I-frame as expensive while P-frame is cheaper but the cheapest is the B-frame.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/frame_types.png"&gt;&lt;img src="/i/frame_types.png" alt="frame types example" title="frame types example" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-temporal-redundancy-inter-prediction" class="anchor" aria-hidden="true" href="#temporal-redundancy-inter-prediction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Temporal redundancy (inter prediction)&lt;/h2&gt;
&lt;p&gt;Let's explore the options we have to reduce the &lt;strong&gt;repetitions in time&lt;/strong&gt;, this type of redundancy can be solved with techniques of &lt;strong&gt;inter prediction&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We will try to &lt;strong&gt;spend fewer bits&lt;/strong&gt; to encode the sequence of frames 0 and 1.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/original_frames.png"&gt;&lt;img src="/i/original_frames.png" alt="original frames" title="original frames" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;One thing we can do it's a subtraction, we simply &lt;strong&gt;subtract frame 1 from frame 0&lt;/strong&gt; and we get just what we need to &lt;strong&gt;encode the residual&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/difference_frames.png"&gt;&lt;img src="/i/difference_frames.png" alt="delta frames" title="delta frames" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But what if I tell you that there is a &lt;strong&gt;better method&lt;/strong&gt; which uses even fewer bits?! First, let's treat the &lt;code&gt;frame 0&lt;/code&gt; as a collection of well-defined partitions and then we'll try to match the blocks from &lt;code&gt;frame 0&lt;/code&gt; on &lt;code&gt;frame 1&lt;/code&gt;. We can think of it as &lt;strong&gt;motion estimation&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-wikipedia---block-motion-compensation" class="anchor" aria-hidden="true" href="#wikipedia---block-motion-compensation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wikipedia - block motion compensation&lt;/h3&gt;
&lt;p&gt;"&lt;strong&gt;Block motion compensation&lt;/strong&gt; divides up the current frame into non-overlapping blocks, and the motion compensation vector &lt;strong&gt;tells where those blocks come from&lt;/strong&gt; (a common misconception is that the previous frame is divided up into non-overlapping blocks, and the motion compensation vectors tell where those blocks move to). The source blocks typically overlap in the source frame. Some video compression algorithms assemble the current frame out of pieces of several different previously-transmitted frames."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/original_frames_motion_estimation.png"&gt;&lt;img src="/i/original_frames_motion_estimation.png" alt="delta frames" title="delta frames" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We could estimate that the ball moved from &lt;code&gt;x=0, y=25&lt;/code&gt; to &lt;code&gt;x=6, y=26&lt;/code&gt;, the &lt;strong&gt;x&lt;/strong&gt; and &lt;strong&gt;y&lt;/strong&gt; values are the &lt;strong&gt;motion vectors&lt;/strong&gt;. One &lt;strong&gt;further step&lt;/strong&gt; we can do to save bits is to &lt;strong&gt;encode only the motion vector difference&lt;/strong&gt; between the last block position and the predicted, so the final motion vector would be &lt;code&gt;x=6 (6-0), y=1 (26-25)&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In a real-world situation, this &lt;strong&gt;ball would be sliced into n partitions&lt;/strong&gt; but the process is the same.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The objects on the frame &lt;strong&gt;move in a 3D way&lt;/strong&gt;, the ball can become smaller when it moves to the background. It's normal that &lt;strong&gt;we won't find the perfect match&lt;/strong&gt; to the block we tried to find a match. Here's a superposed view of our estimation vs the real picture.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/motion_estimation.png"&gt;&lt;img src="/i/motion_estimation.png" alt="motion estimation" title="motion estimation" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But we can see that when we apply &lt;strong&gt;motion estimation&lt;/strong&gt; the &lt;strong&gt;data to encode is smaller&lt;/strong&gt; than using simply delta frame techniques.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/comparison_delta_vs_motion_estimation.png"&gt;&lt;img src="/i/comparison_delta_vs_motion_estimation.png" alt="motion estimation vs delta " title="motion estimation delta" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-how-real-motion-compensation-would-look" class="anchor" aria-hidden="true" href="#how-real-motion-compensation-would-look"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How real motion compensation would look&lt;/h3&gt;
&lt;p&gt;This technique is applied to all blocks, very often a ball would be partitioned in more than one block.
&lt;a target="_blank" rel="noopener noreferrer" href="/i/real_world_motion_compensation.png"&gt;&lt;img src="/i/real_world_motion_compensation.png" alt="real world motion compensation" title="real world motion compensation" style="max-width:100%;"&gt;&lt;/a&gt;
Source: &lt;a href="https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf" rel="nofollow"&gt;https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can &lt;a href="/frame_difference_vs_motion_estimation_plus_residual.ipynb"&gt;play around with these concepts using jupyter&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-hands-on-see-the-motion-vectors" class="anchor" aria-hidden="true" href="#hands-on-see-the-motion-vectors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: See the motion vectors&lt;/h4&gt;
&lt;p&gt;We can &lt;a href="/encoding_pratical_examples.md#generate-debug-video"&gt;generate a video with the inter prediction (motion vectors)  with ffmpeg.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/motion_vectors_ffmpeg.png"&gt;&lt;img src="/i/motion_vectors_ffmpeg.png" alt="inter prediction (motion vectors) with ffmpeg" title="inter prediction (motion vectors) with ffmpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Or we can use the &lt;a href="https://software.intel.com/en-us/intel-video-pro-analyzer" rel="nofollow"&gt;Intel Video Pro Analyzer&lt;/a&gt; (which is paid but there is a free trial version which limits you to only the first 10 frames).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/inter_prediction_intel_video_pro_analyzer.png"&gt;&lt;img src="/i/inter_prediction_intel_video_pro_analyzer.png" alt="inter prediction intel video pro analyzer" title="inter prediction intel video pro analyzer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-spatial-redundancy-intra-prediction" class="anchor" aria-hidden="true" href="#spatial-redundancy-intra-prediction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Spatial redundancy (intra prediction)&lt;/h2&gt;
&lt;p&gt;If we analyze &lt;strong&gt;each frame&lt;/strong&gt; in a video we'll see that there are also &lt;strong&gt;many areas that are correlated&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/repetitions_in_space.png"&gt;&lt;img src="/i/repetitions_in_space.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let's walk through an example. This scene is mostly composed of blue and white colors.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_bg.png"&gt;&lt;img src="/i/smw_bg.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is an &lt;code&gt;I-frame&lt;/code&gt; and we &lt;strong&gt;can't use previous frames&lt;/strong&gt; to predict from but we still can compress it. We will encode the red block selection. If we &lt;strong&gt;look at its neighbors&lt;/strong&gt;, we can &lt;strong&gt;estimate&lt;/strong&gt; that there is a &lt;strong&gt;trend of colors around it&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_bg_block.png"&gt;&lt;img src="/i/smw_bg_block.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We will &lt;strong&gt;predict&lt;/strong&gt; that the frame will continue to &lt;strong&gt;spread the colors vertically&lt;/strong&gt;, it means that the colors of the &lt;strong&gt;unknown pixels will hold the values of its neighbors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_bg_prediction.png"&gt;&lt;img src="/i/smw_bg_prediction.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Our &lt;strong&gt;prediction can be wrong&lt;/strong&gt;, for that reason we need to apply this technique (&lt;strong&gt;intra prediction&lt;/strong&gt;) and then &lt;strong&gt;subtract the real values&lt;/strong&gt; which gives us the residual block, resulting in a much more compressible matrix compared to the original.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_residual.png"&gt;&lt;img src="/i/smw_residual.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-hands-on-check-intra-predictions" class="anchor" aria-hidden="true" href="#hands-on-check-intra-predictions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: Check intra predictions&lt;/h4&gt;
&lt;p&gt;You can &lt;a href="/encoding_pratical_examples.md#generate-debug-video"&gt;generate a video with macro blocks and their predictions with ffmpeg.&lt;/a&gt; Please check the ffmpeg documentation to understand the &lt;a href="https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors#AnalyzingMacroblockTypes" rel="nofollow"&gt;meaning of each block color&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/macro_blocks_ffmpeg.png"&gt;&lt;img src="/i/macro_blocks_ffmpeg.png" alt="intra prediction (macro blocks) with ffmpeg" title="inter prediction (motion vectors) with ffmpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Or we can use the &lt;a href="https://software.intel.com/en-us/intel-video-pro-analyzer" rel="nofollow"&gt;Intel Video Pro Analyzer&lt;/a&gt; (which is paid but there is a free trial version which limits you to only the first 10 frames).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/intra_prediction_intel_video_pro_analyzer.png"&gt;&lt;img src="/i/intra_prediction_intel_video_pro_analyzer.png" alt="intra prediction intel video pro analyzer" title="intra prediction intel video pro analyzer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-how-does-a-video-codec-work" class="anchor" aria-hidden="true" href="#how-does-a-video-codec-work"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How does a video codec work?&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-what-why-how" class="anchor" aria-hidden="true" href="#what-why-how"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What? Why? How?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;What?&lt;/strong&gt; It's a piece of software / hardware that compresses or decompresses digital video. &lt;strong&gt;Why?&lt;/strong&gt; Market and society demands higher quality videos with limited bandwidth or storage. Remember when we &lt;a href="#basic-terminology"&gt;calculated the needed bandwidth&lt;/a&gt; for 30 frames per second, 24 bits per pixel, resolution of a 480x240 video? It was &lt;strong&gt;82.944 Mbps&lt;/strong&gt; with no compression applied. It's the only way to deliver HD/FullHD/4K in TVs and the Internet. &lt;strong&gt;How?&lt;/strong&gt; We'll take a brief look at the major techniques here.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;CODEC vs Container&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One common mistake that beginners often do is to confuse digital video CODEC and &lt;a href="https://en.wikipedia.org/wiki/Digital_container_format" rel="nofollow"&gt;digital video container&lt;/a&gt;. We can think of &lt;strong&gt;containers&lt;/strong&gt; as a wrapper format which contains metadata of the video (and possible audio too), and the &lt;strong&gt;compressed video&lt;/strong&gt; can be seen as its payload.&lt;/p&gt;
&lt;p&gt;Usually, the extension of a video file defines its video container. For instance, the file &lt;code&gt;video.mp4&lt;/code&gt; is probably a &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/MPEG-4_Part_14" rel="nofollow"&gt;MPEG-4 Part 14&lt;/a&gt;&lt;/strong&gt; container and a file named &lt;code&gt;video.mkv&lt;/code&gt; it's probably a &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Matroska" rel="nofollow"&gt;matroska&lt;/a&gt;&lt;/strong&gt;. To be completely sure about the codec and container format we can use &lt;a href="/encoding_pratical_examples.md#inspect-stream"&gt;ffmpeg or mediainfo&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-history" class="anchor" aria-hidden="true" href="#history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;History&lt;/h2&gt;
&lt;p&gt;Before we jump into the inner workings of a generic codec, let's look back to understand a little better about some old video codecs.&lt;/p&gt;
&lt;p&gt;The video codec &lt;a href="https://en.wikipedia.org/wiki/H.261" rel="nofollow"&gt;H.261&lt;/a&gt;  was born in 1990 (technically 1988), and it was designed to work with &lt;strong&gt;data rates of 64 kbit/s&lt;/strong&gt;. It already uses ideas such as chroma subsampling, macro block, etc. In the year of 1995, the &lt;strong&gt;H.263&lt;/strong&gt; video codec standard was published and continued to be extended until 2001.&lt;/p&gt;
&lt;p&gt;In 2003 the first version of &lt;strong&gt;H.264/AVC&lt;/strong&gt; was completed. In the same year, a company called &lt;strong&gt;TrueMotion&lt;/strong&gt; released their video codec as a &lt;strong&gt;royalty-free&lt;/strong&gt; lossy video compression called &lt;strong&gt;VP3&lt;/strong&gt;. In 2008, &lt;strong&gt;Google bought&lt;/strong&gt; this company, releasing &lt;strong&gt;VP8&lt;/strong&gt; in the same year. In December of 2012, Google released the &lt;strong&gt;VP9&lt;/strong&gt; and it's  &lt;strong&gt;supported by roughly ¾ of the browser market&lt;/strong&gt; (mobile included).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/AOMedia_Video_1" rel="nofollow"&gt;AV1&lt;/a&gt;&lt;/strong&gt; is a new &lt;strong&gt;royalty-free&lt;/strong&gt; and open source video codec that's being designed by the &lt;a href="http://aomedia.org/" rel="nofollow"&gt;Alliance for Open Media (AOMedia)&lt;/a&gt;, which is composed of the &lt;strong&gt;companies: Google, Mozilla, Microsoft, Amazon, Netflix, AMD, ARM, NVidia, Intel and Cisco&lt;/strong&gt; among others. The &lt;strong&gt;first version&lt;/strong&gt; 0.1.0 of the reference codec was &lt;strong&gt;published on April 7, 2016&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/codec_history_timeline.png"&gt;&lt;img src="/i/codec_history_timeline.png" alt="codec history timeline" title="codec history timeline" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-the-birth-of-av1" class="anchor" aria-hidden="true" href="#the-birth-of-av1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The birth of AV1&lt;/h4&gt;
&lt;p&gt;Early 2015, Google was working on &lt;a href="https://en.wikipedia.org/wiki/VP9#Successor:_from_VP10_to_AV1" rel="nofollow"&gt;VP10&lt;/a&gt;, Xiph (Mozilla) was working on &lt;a href="https://xiph.org/daala/" rel="nofollow"&gt;Daala&lt;/a&gt; and Cisco open-sourced its royalty-free video codec called &lt;a href="https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03" rel="nofollow"&gt;Thor&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then MPEG LA first announced annual caps for HEVC (H.265) and fees 8 times higher than H.264 but soon they changed the rules again:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;no annual cap&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;content fee&lt;/strong&gt; (0.5% of revenue) and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;per-unit fees about 10 times higher than h264&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href="http://aomedia.org/about-us/" rel="nofollow"&gt;alliance for open media&lt;/a&gt; was created by companies from hardware manufacturer (Intel, AMD, ARM , Nvidia, Cisco), content delivery (Google, Netflix, Amazon), browser maintainers (Google, Mozilla), and others.&lt;/p&gt;
&lt;p&gt;The companies had a common goal, a royalty-free video codec and then AV1 was born with a much &lt;a href="http://aomedia.org/license/patent/" rel="nofollow"&gt;simpler patent license&lt;/a&gt;. &lt;strong&gt;Timothy B. Terriberry&lt;/strong&gt; did an awesome presentation, which is the source of this section, about the &lt;a href="https://www.youtube.com/watch?v=lzPaldsmJbk" rel="nofollow"&gt;AV1 conception, license model and its current state&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You'll be surprised to know that you can &lt;strong&gt;analyze the AV1 codec through your browser&lt;/strong&gt;, go to &lt;a href="http://aomanalyzer.org/" rel="nofollow"&gt;http://aomanalyzer.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/av1_browser_analyzer.png"&gt;&lt;img src="/i/av1_browser_analyzer.png" alt="av1 browser analyzer" title="av1 browser analyzer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PS: If you want to learn more about the history of the codecs you must learn the basics behind &lt;a href="https://www.vcodex.com/video-compression-patents/" rel="nofollow"&gt;video compression patents&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-a-generic-codec" class="anchor" aria-hidden="true" href="#a-generic-codec"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A generic codec&lt;/h2&gt;
&lt;p&gt;We're going to introduce the &lt;strong&gt;main mechanics behind a generic video codec&lt;/strong&gt; but most of these concepts are useful and used in modern codecs such as VP9, AV1 and HEVC. Be sure to understand that we're going to simplify things a LOT. Sometimes we'll use a real example (mostly H.264) to demonstrate a technique.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-1st-step---picture-partitioning" class="anchor" aria-hidden="true" href="#1st-step---picture-partitioning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1st step - picture partitioning&lt;/h2&gt;
&lt;p&gt;The first step is to &lt;strong&gt;divide the frame&lt;/strong&gt; into several &lt;strong&gt;partitions, sub-partitions&lt;/strong&gt; and beyond.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/picture_partitioning.png"&gt;&lt;img src="/i/picture_partitioning.png" alt="picture partitioning" title="picture partitioning" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But why?&lt;/strong&gt; There are many reasons, for instance, when we split the picture we can work the predictions more precisely, using small partitions for the small moving parts while using bigger partitions to a static background.&lt;/p&gt;
&lt;p&gt;Usually, the CODECs &lt;strong&gt;organize these partitions&lt;/strong&gt; into slices (or tiles), macro (or coding tree units) and many sub-partitions. The max size of these partitions varies, HEVC sets 64x64 while AVC uses 16x16 but the sub-partitions can reach sizes of 4x4.&lt;/p&gt;
&lt;p&gt;Remember that we learned how &lt;strong&gt;frames are typed&lt;/strong&gt;?! Well, you can &lt;strong&gt;apply those ideas to blocks&lt;/strong&gt; too, therefore we can have I-Slice, B-Slice, I-Macroblock and etc.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-hands-on-check-partitions" class="anchor" aria-hidden="true" href="#hands-on-check-partitions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: Check partitions&lt;/h3&gt;
&lt;p&gt;We can also use the &lt;a href="https://software.intel.com/en-us/intel-video-pro-analyzer" rel="nofollow"&gt;Intel Video Pro Analyzer&lt;/a&gt; (which is paid but there is a free trial version which limits you to only the first 10 frames). Here are &lt;a href="/encoding_pratical_examples.md#transcoding"&gt;VP9 partitions&lt;/a&gt; analyzed.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/paritions_view_intel_video_pro_analyzer.png"&gt;&lt;img src="/i/paritions_view_intel_video_pro_analyzer.png" alt="VP9 partitions view intel video pro analyzer " title="VP9 partitions view intel video pro analyzer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-2nd-step---predictions" class="anchor" aria-hidden="true" href="#2nd-step---predictions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2nd step - predictions&lt;/h2&gt;
&lt;p&gt;Once we have the partitions, we can make predictions over them. For the &lt;a href="#temporal-redundancy-inter-prediction"&gt;inter prediction&lt;/a&gt; we need &lt;strong&gt;to send the motion vectors and the residual&lt;/strong&gt; and the &lt;a href="#spatial-redundancy-intra-prediction"&gt;intra prediction&lt;/a&gt; we'll &lt;strong&gt;send the prediction direction and the residual&lt;/strong&gt; as well.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-3rd-step---transform" class="anchor" aria-hidden="true" href="#3rd-step---transform"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3rd step - transform&lt;/h2&gt;
&lt;p&gt;After we get the residual block (&lt;code&gt;predicted partition - real partition&lt;/code&gt;), we can &lt;strong&gt;transform&lt;/strong&gt; it in a way that lets us know which &lt;strong&gt;pixels we can discard&lt;/strong&gt; while keeping the &lt;strong&gt;overall quality&lt;/strong&gt;. There are some transformations for this exact behavior.&lt;/p&gt;
&lt;p&gt;Although there are &lt;a href="https://en.wikipedia.org/wiki/List_of_Fourier-related_transforms#Discrete_transforms" rel="nofollow"&gt;other transformations&lt;/a&gt;, we'll look more closely at the discrete cosine transform (DCT). The &lt;a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform" rel="nofollow"&gt;&lt;strong&gt;DCT&lt;/strong&gt;&lt;/a&gt; main features are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;converts&lt;/strong&gt; blocks of &lt;strong&gt;pixels&lt;/strong&gt; into  same-sized blocks of &lt;strong&gt;frequency coefficients&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;compacts&lt;/strong&gt; energy, making it easy to eliminate spatial redundancy.&lt;/li&gt;
&lt;li&gt;is &lt;strong&gt;reversible&lt;/strong&gt;, a.k.a. you can reverse to pixels.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;On 2 Feb 2017, Cintra, R. J. and Bayer, F. M have published their paper &lt;a href="https://arxiv.org/abs/1702.00817" rel="nofollow"&gt;DCT-like Transform for Image Compression
Requires 14 Additions Only&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Don't worry if you didn't understand the benefits from every bullet point, we'll try to make some experiments in order to see the real value from it.&lt;/p&gt;
&lt;p&gt;Let's take the following &lt;strong&gt;block of pixels&lt;/strong&gt; (8x8):&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/pixel_matrice.png"&gt;&lt;img src="/i/pixel_matrice.png" alt="pixel values matrix" title="pixel values matrix" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Which renders to the following block image (8x8):&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/gray_image.png"&gt;&lt;img src="/i/gray_image.png" alt="pixel values matrix" title="pixel values matrix" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When we &lt;strong&gt;apply the DCT&lt;/strong&gt; over this block of pixels and we get the &lt;strong&gt;block of coefficients&lt;/strong&gt; (8x8):&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/dct_coefficient_values.png"&gt;&lt;img src="/i/dct_coefficient_values.png" alt="coefficients values" title="coefficients values" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And if we render this block of coefficients, we'll get this image:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/dct_coefficient_image.png"&gt;&lt;img src="/i/dct_coefficient_image.png" alt="dct coefficients image" title="dct coefficients image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As you can see it looks nothing like the original image, we might notice that the &lt;strong&gt;first coefficient&lt;/strong&gt; is very different from all the others. This first coefficient is known as the DC coefficient which represents of &lt;strong&gt;all the samples&lt;/strong&gt; in the input array, something &lt;strong&gt;similar to an average&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This block of coefficients has an interesting property which is that it separates the high-frequency components from the low frequency.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/dctfrequ.jpg"&gt;&lt;img src="/i/dctfrequ.jpg" alt="dct frequency coefficients property" title="dct frequency coefficients property" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In an image, &lt;strong&gt;most of the energy&lt;/strong&gt; will be concentrated in the &lt;a href="https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm" rel="nofollow"&gt;&lt;strong&gt;lower frequencies&lt;/strong&gt;&lt;/a&gt;, so if we transform an image into its frequency components and &lt;strong&gt;throw away the higher frequency coefficients&lt;/strong&gt;, we can &lt;strong&gt;reduce the amount of data&lt;/strong&gt; needed to describe the image without sacrificing too much image quality.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;frequency means how fast a signal is changing&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's try to apply the knowledge we acquired in the test by converting the original image to its frequency (block of coefficients) using DCT and then throwing away part of the least important coefficients.&lt;/p&gt;
&lt;p&gt;First, we convert it to its &lt;strong&gt;frequency domain&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/dct_coefficient_values.png"&gt;&lt;img src="/i/dct_coefficient_values.png" alt="coefficients values" title="coefficients values" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Next, we discard part (67%) of the coefficients, mostly the bottom right part of it.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/dct_coefficient_zeroed.png"&gt;&lt;img src="/i/dct_coefficient_zeroed.png" alt="zeroed coefficients" title="zeroed coefficients" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Finally, we reconstruct the image from this discarded block of coefficients (remember, it needs to be reversible) and compare it to the original.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/original_vs_quantized.png"&gt;&lt;img src="/i/original_vs_quantized.png" alt="original vs quantized" title="original vs quantized" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As we can see it resembles the original image but it introduced lots of differences from the original, we &lt;strong&gt;throw away 67.1875%&lt;/strong&gt; and we still were able to get at least something similar to the original. We could more intelligently discard the coefficients to have a better image quality but that's the next topic.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Each coefficient is formed using all the pixels&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It's important to note that each coefficient doesn't directly map to a single pixel but it's a weighted sum of all pixels. This amazing graph shows how the first and second coefficient is calculated, using weights which are unique for each index.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/applicat.jpg"&gt;&lt;img src="/i/applicat.jpg" alt="dct calculation" title="dct calculation" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Source: &lt;a href="https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm" rel="nofollow"&gt;https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also try to &lt;a href="/dct_better_explained.ipynb"&gt;visualize the DCT by looking at a simple image&lt;/a&gt; formation over the DCT basis. For instance, here's the &lt;a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform#Example_of_IDCT" rel="nofollow"&gt;A character being formed&lt;/a&gt; using each coefficient weight.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ac2f3ff3a6e29112f3e7f51325cb7d3a2f08e377/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f352f35652f496463742d616e696d6174696f6e2e676966"&gt;&lt;img src="https://camo.githubusercontent.com/ac2f3ff3a6e29112f3e7f51325cb7d3a2f08e377/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f352f35652f496463742d616e696d6174696f6e2e676966" alt="" data-canonical-src="https://upload.wikimedia.org/wikipedia/commons/5/5e/Idct-animation.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-hands-on-throwing-away-different-coefficients" class="anchor" aria-hidden="true" href="#hands-on-throwing-away-different-coefficients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: throwing away different coefficients&lt;/h3&gt;
&lt;p&gt;You can play around with the &lt;a href="/uniform_quantization_experience.ipynb"&gt;DCT transform&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-4th-step---quantization" class="anchor" aria-hidden="true" href="#4th-step---quantization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4th step - quantization&lt;/h2&gt;
&lt;p&gt;When we throw away some of the coefficients, in the last step (transform), we kinda did some form of quantization. This step is where we chose to lose information (the &lt;strong&gt;lossy part&lt;/strong&gt;) or in simple terms, we'll &lt;strong&gt;quantize coefficients to achieve compression&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;How can we quantize a block of coefficients? One simple method would be a uniform quantization, where we take a block, &lt;strong&gt;divide it by a single value&lt;/strong&gt; (10) and round this value.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/quantize.png"&gt;&lt;img src="/i/quantize.png" alt="quantize" title="quantize" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;How can we &lt;strong&gt;reverse&lt;/strong&gt; (re-quantize) this block of coefficients? We can do that by &lt;strong&gt;multiplying the same value&lt;/strong&gt; (10) we divide it first.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/re-quantize.png"&gt;&lt;img src="/i/re-quantize.png" alt="re-quantize" title="re-quantize" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This &lt;strong&gt;approach isn't the best&lt;/strong&gt; because it doesn't take into account the importance of each coefficient, we could use a &lt;strong&gt;matrix of quantizers&lt;/strong&gt; instead of a single value, this matrix can exploit the property of the DCT, quantizing most the bottom right and less the upper left, the &lt;a href="https://www.hdm-stuttgart.de/~maucher/Python/MMCodecs/html/jpegUpToQuant.html" rel="nofollow"&gt;JPEG uses a similar approach&lt;/a&gt;, you can check &lt;a href="https://github.com/google/guetzli/blob/master/guetzli/jpeg_data.h#L40"&gt;source code to see this matrix&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-hands-on-quantization" class="anchor" aria-hidden="true" href="#hands-on-quantization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: quantization&lt;/h3&gt;
&lt;p&gt;You can play around with the &lt;a href="/dct_experiences.ipynb"&gt;quantization&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-5th-step---entropy-coding" class="anchor" aria-hidden="true" href="#5th-step---entropy-coding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5th step - entropy coding&lt;/h2&gt;
&lt;p&gt;After we quantized the data (image blocks/slices/frames) we still can compress it in a lossless way. There are many ways (algorithms) to compress data. We're going to briefly experience some of them, for a deeper understanding you can read the amazing book &lt;a href="https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/" rel="nofollow"&gt;Understanding Compression: Data Compression for Modern Developers&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-vlc-coding" class="anchor" aria-hidden="true" href="#vlc-coding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VLC coding:&lt;/h3&gt;
&lt;p&gt;Let's suppose we have a stream of the symbols: &lt;strong&gt;a&lt;/strong&gt;, &lt;strong&gt;e&lt;/strong&gt;, &lt;strong&gt;r&lt;/strong&gt; and &lt;strong&gt;t&lt;/strong&gt; and their probability (from 0 to 1) is represented by this table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;a&lt;/th&gt;
&lt;th&gt;e&lt;/th&gt;
&lt;th&gt;r&lt;/th&gt;
&lt;th&gt;t&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;probability&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can assign unique binary codes (preferable small) to the most probable and bigger codes to the least probable ones.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;a&lt;/th&gt;
&lt;th&gt;e&lt;/th&gt;
&lt;th&gt;r&lt;/th&gt;
&lt;th&gt;t&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;probability&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;binary code&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;110&lt;/td&gt;
&lt;td&gt;1110&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let's compress the stream &lt;strong&gt;eat&lt;/strong&gt;, assuming we would spend 8 bits for each symbol, we would spend &lt;strong&gt;24 bits&lt;/strong&gt; without any compression. But in case we replace each symbol for its code we can save space.&lt;/p&gt;
&lt;p&gt;The first step is to encode the symbol &lt;strong&gt;e&lt;/strong&gt; which is &lt;code&gt;10&lt;/code&gt; and the second symbol is &lt;strong&gt;a&lt;/strong&gt; which is added (not in a mathematical way) &lt;code&gt;[10][0]&lt;/code&gt; and finally the third symbol &lt;strong&gt;t&lt;/strong&gt; which makes our final compressed bitstream to be &lt;code&gt;[10][0][1110]&lt;/code&gt; or &lt;code&gt;1001110&lt;/code&gt; which only requires &lt;strong&gt;7 bits&lt;/strong&gt; (3.4 times less space than the original).&lt;/p&gt;
&lt;p&gt;Notice that each code must be a unique prefixed code &lt;a href="https://en.wikipedia.org/wiki/Huffman_coding" rel="nofollow"&gt;Huffman can help you to find these numbers&lt;/a&gt;. Though it has some issues there are &lt;a href="https://en.wikipedia.org/wiki/Context-adaptive_variable-length_coding" rel="nofollow"&gt;video codecs that still offers&lt;/a&gt; this method and it's the  algorithm for many applications which requires compression.&lt;/p&gt;
&lt;p&gt;Both encoder and decoder &lt;strong&gt;must know&lt;/strong&gt; the symbol table with its code, therefore, you need to send the table too.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-arithmetic-coding" class="anchor" aria-hidden="true" href="#arithmetic-coding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arithmetic coding:&lt;/h3&gt;
&lt;p&gt;Let's suppose we have a stream of the symbols: &lt;strong&gt;a&lt;/strong&gt;, &lt;strong&gt;e&lt;/strong&gt;, &lt;strong&gt;r&lt;/strong&gt;, &lt;strong&gt;s&lt;/strong&gt; and &lt;strong&gt;t&lt;/strong&gt; and their probability is represented by this table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;a&lt;/th&gt;
&lt;th&gt;e&lt;/th&gt;
&lt;th&gt;r&lt;/th&gt;
&lt;th&gt;s&lt;/th&gt;
&lt;th&gt;t&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;probability&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.15&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With this table in mind, we can build ranges containing all the possible symbols sorted by the most frequents.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/range.png"&gt;&lt;img src="/i/range.png" alt="initial arithmetic range" title="initial arithmetic range" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now let's encode the stream &lt;strong&gt;eat&lt;/strong&gt;, we pick the first symbol &lt;strong&gt;e&lt;/strong&gt; which is located within the subrange &lt;strong&gt;0.3 to 0.6&lt;/strong&gt; (but not included) and we take this subrange and split it again using the same proportions used before but within this new range.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/second_subrange.png"&gt;&lt;img src="/i/second_subrange.png" alt="second sub range" title="second sub range" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let's continue to encode our stream &lt;strong&gt;eat&lt;/strong&gt;, now we take the second symbol &lt;strong&gt;a&lt;/strong&gt; which is within the new subrange &lt;strong&gt;0.3 to 0.39&lt;/strong&gt; and then we take our last symbol &lt;strong&gt;t&lt;/strong&gt; and we do the same process again and we get the last subrange &lt;strong&gt;0.354 to 0.372&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/arithimetic_range.png"&gt;&lt;img src="/i/arithimetic_range.png" alt="final arithmetic range" title="final arithmetic range" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We just need to pick a number within the last subrange &lt;strong&gt;0.354 to 0.372&lt;/strong&gt;, let's choose &lt;strong&gt;0.36&lt;/strong&gt; but we could choose any number within this subrange. With &lt;strong&gt;only&lt;/strong&gt; this number we'll be able to recover our original stream &lt;strong&gt;eat&lt;/strong&gt;. If you think about it, it's like if we were drawing a line within ranges of ranges to encode our stream.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/range_show.png"&gt;&lt;img src="/i/range_show.png" alt="final range traverse" title="final range traverse" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;reverse process&lt;/strong&gt; (A.K.A. decoding) is equally easy, with our number &lt;strong&gt;0.36&lt;/strong&gt; and our original range we can run the same process but now using this number to reveal the stream encoded behind this number.&lt;/p&gt;
&lt;p&gt;With the first range, we notice that our number fits at the slice, therefore, it's our first symbol, now we split this subrange again, doing the same process as before, and we'll notice that &lt;strong&gt;0.36&lt;/strong&gt; fits the symbol &lt;strong&gt;a&lt;/strong&gt; and after we repeat the process we came to the last symbol &lt;strong&gt;t&lt;/strong&gt; (forming our original encoded stream &lt;em&gt;eat&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Both encoder and decoder &lt;strong&gt;must know&lt;/strong&gt; the symbol probability table, therefore you need to send the table.&lt;/p&gt;
&lt;p&gt;Pretty neat, isn't it? People are damn smart to come up with a such solution, some &lt;a href="https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding" rel="nofollow"&gt;video codecs use&lt;/a&gt; this technique (or at least offer it as an option).&lt;/p&gt;
&lt;p&gt;The idea is to lossless compress the quantized bitstream, for sure this article is missing tons of details, reasons, trade-offs and etc. But &lt;a href="https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/" rel="nofollow"&gt;you should learn more&lt;/a&gt; as a developer. Newer codecs are trying to use different &lt;a href="https://en.wikipedia.org/wiki/Asymmetric_Numeral_Systems" rel="nofollow"&gt;entropy coding algorithms like ANS.&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-hands-on-cabac-vs-cavlc" class="anchor" aria-hidden="true" href="#hands-on-cabac-vs-cavlc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: CABAC vs CAVLC&lt;/h3&gt;
&lt;p&gt;You can &lt;a href="https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#cabac-vs-cavlc"&gt;generate two streams, one with CABAC and other with CAVLC&lt;/a&gt; and &lt;strong&gt;compare the time&lt;/strong&gt; it took to generate each of them as well as &lt;strong&gt;the final size&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-6th-step---bitstream-format" class="anchor" aria-hidden="true" href="#6th-step---bitstream-format"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;6th step - bitstream format&lt;/h2&gt;
&lt;p&gt;After we did all these steps we need to &lt;strong&gt;pack the compressed frames and context to these steps&lt;/strong&gt;. We need to explicitly inform to the decoder about &lt;strong&gt;the decisions taken by the encoder&lt;/strong&gt;, such as bit depth, color space, resolution, predictions info (motion vectors, intra prediction direction), profile, level, frame rate, frame type, frame number and much more.&lt;/p&gt;
&lt;p&gt;We're going to study, superficially, the H.264 bitstream. Our first step is to &lt;a href="/encoding_pratical_examples.md#generate-a-single-frame-h264-bitstream"&gt;generate a minimal  H.264 &lt;sup&gt;*&lt;/sup&gt; bitstream&lt;/a&gt;, we can do that using our own repository and &lt;a href="http://ffmpeg.org/" rel="nofollow"&gt;ffmpeg&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./s/ffmpeg -i /files/i/minimal.png -pix_fmt yuv420p /files/v/minimal_yuv420.h264
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; ffmpeg adds, by default, all the encoding parameter as a &lt;strong&gt;SEI NAL&lt;/strong&gt;, soon we'll define what is a NAL.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This command will generate a raw h264 bitstream with a &lt;strong&gt;single frame&lt;/strong&gt;, 64x64, with color space yuv420 and using the following image as the frame.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/minimal.png"&gt;&lt;img src="/i/minimal.png" alt="used frame to generate minimal h264 bitstream" title="used frame to generate minimal h264 bitstream" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-h264-bitstream" class="anchor" aria-hidden="true" href="#h264-bitstream"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;H.264 bitstream&lt;/h3&gt;
&lt;p&gt;The AVC (H.264) standard defines that the information will be sent in &lt;strong&gt;macro frames&lt;/strong&gt; (in the network sense), called &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Network_Abstraction_Layer" rel="nofollow"&gt;NAL&lt;/a&gt;&lt;/strong&gt; (Network Abstraction Layer). The main goal of the NAL is the provision of a "network-friendly" video representation, this standard must work on TVs (stream based), the Internet (packet based) among others.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/nal_units.png"&gt;&lt;img src="/i/nal_units.png" alt="NAL units H.264" title="NAL units H.264" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There is a &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Frame_synchronization" rel="nofollow"&gt;synchronization marker&lt;/a&gt;&lt;/strong&gt; to define the boundaries of the NAL's units. Each synchronization marker holds a value of &lt;code&gt;0x00 0x00 0x01&lt;/code&gt; except to the very first one which is &lt;code&gt;0x00 0x00 0x00 0x01&lt;/code&gt;. If we run the &lt;strong&gt;hexdump&lt;/strong&gt; on the generated h264 bitstream, we can identify at least three NALs in the beginning of the file.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/minimal_yuv420_hex.png"&gt;&lt;img src="/i/minimal_yuv420_hex.png" alt="synchronization marker on NAL units" title="synchronization marker on NAL units" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As we said before, the decoder needs to know not only the picture data but also the details of the video, frame, colors, used parameters, and others. The &lt;strong&gt;first byte&lt;/strong&gt; of each NAL defines its category and &lt;strong&gt;type&lt;/strong&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;NAL type id&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;Undefined&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Coded slice of a non-IDR picture&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Coded slice data partition A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Coded slice data partition B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Coded slice data partition C&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;IDR&lt;/strong&gt; Coded slice of an IDR picture&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;SEI&lt;/strong&gt; Supplemental enhancement information&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;SPS&lt;/strong&gt; Sequence parameter set&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;PPS&lt;/strong&gt; Picture parameter set&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;Access unit delimiter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;End of sequence&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;End of stream&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Usually, the first NAL of a bitstream is a &lt;strong&gt;SPS&lt;/strong&gt;, this type of NAL is responsible for informing the general encoding variables like &lt;strong&gt;profile&lt;/strong&gt;, &lt;strong&gt;level&lt;/strong&gt;, &lt;strong&gt;resolution&lt;/strong&gt; and others.&lt;/p&gt;
&lt;p&gt;If we skip the first synchronization marker we can decode the &lt;strong&gt;first byte&lt;/strong&gt; to know what &lt;strong&gt;type of NAL&lt;/strong&gt; is the first one.&lt;/p&gt;
&lt;p&gt;For instance the first byte after the synchronization marker is &lt;code&gt;01100111&lt;/code&gt;, where the first bit (&lt;code&gt;0&lt;/code&gt;) is to the field &lt;strong&gt;forbidden_zero_bit&lt;/strong&gt;, the next 2 bits (&lt;code&gt;11&lt;/code&gt;) tell us the field &lt;strong&gt;nal_ref_idc&lt;/strong&gt; which indicates whether this NAL is a reference field or not and the rest 5 bits (&lt;code&gt;00111&lt;/code&gt;) inform us the field &lt;strong&gt;nal_unit_type&lt;/strong&gt;, in this case, it's a &lt;strong&gt;SPS&lt;/strong&gt; (7) NAL unit.&lt;/p&gt;
&lt;p&gt;The second byte (&lt;code&gt;binary=01100100, hex=0x64, dec=100&lt;/code&gt;) of an SPS NAL is the field &lt;strong&gt;profile_idc&lt;/strong&gt; which shows the profile that the encoder has used, in this case, we used  the &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Profiles" rel="nofollow"&gt;constrained high-profile&lt;/a&gt;&lt;/strong&gt;, it's a high profile without the support of B (bi-predictive) slices.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/minimal_yuv420_bin.png"&gt;&lt;img src="/i/minimal_yuv420_bin.png" alt="SPS binary view" title="SPS binary view" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When we read the H.264 bitstream spec for an SPS NAL we'll find many values for the &lt;strong&gt;parameter name&lt;/strong&gt;, &lt;strong&gt;category&lt;/strong&gt; and a &lt;strong&gt;description&lt;/strong&gt;, for instance, let's look at &lt;code&gt;pic_width_in_mbs_minus_1&lt;/code&gt; and &lt;code&gt;pic_height_in_map_units_minus_1&lt;/code&gt; fields.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter name&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pic_width_in_mbs_minus_1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;ue(v)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pic_height_in_map_units_minus_1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;ue(v)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;ue(v)&lt;/strong&gt;: unsigned integer &lt;a href="https://pythonhosted.org/bitstring/exp-golomb.html" rel="nofollow"&gt;Exp-Golomb-coded&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If we do some math with the value of these fields we will end up with the &lt;strong&gt;resolution&lt;/strong&gt;. We can represent a &lt;code&gt;1920 x 1080&lt;/code&gt; using a &lt;code&gt;pic_width_in_mbs_minus_1&lt;/code&gt; with the value of &lt;code&gt;119 ( (119 + 1) * macroblock_size = 120 * 16 = 1920) &lt;/code&gt;, again saving space, instead of encode &lt;code&gt;1920&lt;/code&gt; we did it with &lt;code&gt;119&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If we continue to examine our created video with a binary view (ex: &lt;code&gt;xxd -b -c 11 v/minimal_yuv420.h264&lt;/code&gt;), we can skip to the last NAL which is the frame itself.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/slice_nal_idr_bin.png"&gt;&lt;img src="/i/slice_nal_idr_bin.png" alt="h264 idr slice header" title="h264 idr slice header" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can see its first 6 bytes values: &lt;code&gt;01100101 10001000 10000100 00000000 00100001 11111111&lt;/code&gt;. As we already know the first byte tell us about what type of NAL it is, in this case, (&lt;code&gt;00101&lt;/code&gt;) it's an &lt;strong&gt;IDR Slice (5)&lt;/strong&gt; and we can further inspect it:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/slice_header.png"&gt;&lt;img src="/i/slice_header.png" alt="h264 slice header spec" title="h264 slice header spec" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Using the spec info we can decode what type of slice (&lt;strong&gt;slice_type&lt;/strong&gt;), the frame number (&lt;strong&gt;frame_num&lt;/strong&gt;) among others important fields.&lt;/p&gt;
&lt;p&gt;In order to get the values of some fields (&lt;code&gt;ue(v), me(v), se(v) or te(v)&lt;/code&gt;) we need to decode it using a special decoder called &lt;a href="https://pythonhosted.org/bitstring/exp-golomb.html" rel="nofollow"&gt;Exponential-Golomb&lt;/a&gt;, this method is &lt;strong&gt;very efficient to encode variable values&lt;/strong&gt;, mostly when there are many default values.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The values of &lt;strong&gt;slice_type&lt;/strong&gt; and &lt;strong&gt;frame_num&lt;/strong&gt; of this video are 7 (I slice) and 0 (the first frame).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can see the &lt;strong&gt;bitstream as a protocol&lt;/strong&gt; and if you want or need to learn more about this bitstream please refer to the &lt;a href="http://www.itu.int/rec/T-REC-H.264-201610-I" rel="nofollow"&gt;ITU H.264 spec.&lt;/a&gt; Here's a macro diagram which shows where the picture data (compressed YUV) resides.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/h264_bitstream_macro_diagram.png"&gt;&lt;img src="/i/h264_bitstream_macro_diagram.png" alt="h264 bitstream macro diagram" title="h264 bitstream macro diagram" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can explore others bitstreams like the &lt;a href="https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf" rel="nofollow"&gt;VP9 bitstream&lt;/a&gt;, &lt;a href="http://handle.itu.int/11.1002/1000/11885-en?locatt=format:pdf" rel="nofollow"&gt;H.265 (HEVC)&lt;/a&gt; or even our &lt;strong&gt;new best friend&lt;/strong&gt; &lt;a href="https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8" rel="nofollow"&gt;&lt;strong&gt;AV1&lt;/strong&gt; bitstream&lt;/a&gt;, &lt;a href="http://www.gpac-licensing.com/2016/07/12/vp9-av1-bitstream-format/" rel="nofollow"&gt;do they all look similar? No&lt;/a&gt;, but once you learned one you can easily get the others.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-hands-on-inspect-the-h264-bitstream" class="anchor" aria-hidden="true" href="#hands-on-inspect-the-h264-bitstream"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: Inspect the H.264 bitstream&lt;/h3&gt;
&lt;p&gt;We can &lt;a href="https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-video"&gt;generate a single frame video&lt;/a&gt; and use  &lt;a href="https://en.wikipedia.org/wiki/MediaInfo" rel="nofollow"&gt;mediainfo&lt;/a&gt; to inspect its H.264 bitstream. In fact, you can even see the &lt;a href="https://github.com/MediaArea/MediaInfoLib/blob/master/Source/MediaInfo/Video/File_Avc.cpp"&gt;source code that parses h264 (AVC)&lt;/a&gt; bitstream.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/mediainfo_details_1.png"&gt;&lt;img src="/i/mediainfo_details_1.png" alt="mediainfo details h264 bitstream" title="mediainfo details h264 bitstream" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can also use the &lt;a href="https://software.intel.com/en-us/intel-video-pro-analyzer" rel="nofollow"&gt;Intel Video Pro Analyzer&lt;/a&gt; which is paid but there is a free trial version which limits you to only the first 10 frames but that's okay for learning purposes.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/intel-video-pro-analyzer.png"&gt;&lt;img src="/i/intel-video-pro-analyzer.png" alt="intel video pro analyzer details h264 bitstream" title="intel video pro analyzer details h264 bitstream" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-review" class="anchor" aria-hidden="true" href="#review"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Review&lt;/h2&gt;
&lt;p&gt;We'll notice that many of the &lt;strong&gt;modern codecs uses this same model we learned&lt;/strong&gt;. In fact, let's look at the Thor video codec block diagram, it contains all the steps we studied. The idea is that you now should be able to at least understand better the innovations and papers for the area.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/thor_codec_block_diagram.png"&gt;&lt;img src="/i/thor_codec_block_diagram.png" alt="thor_codec_block_diagram" title="thor_codec_block_diagram" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Previously we had calculated that we needed &lt;a href="#chroma-subsampling"&gt;139GB of storage to keep a video file with one hour at 720p resolution and 30fps&lt;/a&gt; if we use the techniques we learned here, like &lt;strong&gt;inter and intra prediction, transform, quantization, entropy coding and other&lt;/strong&gt; we can achieve, assuming we are spending &lt;strong&gt;0.031 bit per pixel&lt;/strong&gt;, the same perceivable quality video &lt;strong&gt;requiring only 367.82MB vs 139GB&lt;/strong&gt; of store.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We choose to use &lt;strong&gt;0.031 bit per pixel&lt;/strong&gt; based on the example video provided here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-how-does-h265-achieve-a-better-compression-ratio-than-h264" class="anchor" aria-hidden="true" href="#how-does-h265-achieve-a-better-compression-ratio-than-h264"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How does H.265 achieve a better compression ratio than H.264?&lt;/h2&gt;
&lt;p&gt;Now that we know more about how codecs work, then it is easy to understand how new codecs are able to deliver higher resolutions with fewer bits.&lt;/p&gt;
&lt;p&gt;We will compare AVC and HEVC, let's keep in mind that it is almost always a trade-off between more CPU cycles (complexity) and compression rate.&lt;/p&gt;
&lt;p&gt;HEVC has bigger and more &lt;strong&gt;partitions&lt;/strong&gt; (and &lt;strong&gt;sub-partitions&lt;/strong&gt;) options than AVC, more &lt;strong&gt;intra predictions directions&lt;/strong&gt;, &lt;strong&gt;improved entropy coding&lt;/strong&gt; and more, all these improvements made H.265 capable to compress 50% more than H.264.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/avc_vs_hevc.png"&gt;&lt;img src="/i/avc_vs_hevc.png" alt="h264 vs h265" title="H.264 vs H.265" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-online-streaming" class="anchor" aria-hidden="true" href="#online-streaming"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online streaming&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-general-architecture" class="anchor" aria-hidden="true" href="#general-architecture"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;General architecture&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/general_architecture.png"&gt;&lt;img src="/i/general_architecture.png" alt="general architecture" title="general architecture" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TODO]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-progressive-download-and-adaptive-streaming" class="anchor" aria-hidden="true" href="#progressive-download-and-adaptive-streaming"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Progressive download and adaptive streaming&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/progressive_download.png"&gt;&lt;img src="/i/progressive_download.png" alt="progressive download" title="progressive download" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/adaptive_streaming.png"&gt;&lt;img src="/i/adaptive_streaming.png" alt="adaptive streaming" title="adaptive streaming" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TODO]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-content-protection" class="anchor" aria-hidden="true" href="#content-protection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Content protection&lt;/h2&gt;
&lt;p&gt;We can use &lt;strong&gt;a simple token system&lt;/strong&gt; to protect the content. The user without a token tries to request a video and the CDN forbids her or him while a user with a valid token can play the content, it works pretty similarly to most of the web authentication systems.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/token_protection.png"&gt;&lt;img src="/i/token_protection.png" alt="token protection" title="token_protection" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The sole use of this token system still allows a user to download a video and distribute it. Then the &lt;strong&gt;DRM (digital rights management)&lt;/strong&gt; systems can be used to try to avoid this.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/drm.png"&gt;&lt;img src="/i/drm.png" alt="drm" title="drm" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In real life production systems, people often use both techniques to provide authorization and authentication.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-drm" class="anchor" aria-hidden="true" href="#drm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DRM&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-main-systems" class="anchor" aria-hidden="true" href="#main-systems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main systems&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;FPS - &lt;a href="https://developer.apple.com/streaming/fps/" rel="nofollow"&gt;&lt;strong&gt;FairPlay Streaming&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PR - &lt;a href="https://www.microsoft.com/playready/" rel="nofollow"&gt;&lt;strong&gt;PlayReady&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WV - &lt;a href="http://www.widevine.com/" rel="nofollow"&gt;&lt;strong&gt;Widevine&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-what" class="anchor" aria-hidden="true" href="#what"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What?&lt;/h4&gt;
&lt;p&gt;DRM means Digital rights management, it's a way &lt;strong&gt;to provide copyright protection for digital media&lt;/strong&gt;, for instance, digital video and audio. Although it's used in many places &lt;a href="https://en.wikipedia.org/wiki/Digital_rights_management#DRM-free_works" rel="nofollow"&gt;it's not universally accepted&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-why" class="anchor" aria-hidden="true" href="#why"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why?&lt;/h4&gt;
&lt;p&gt;Content creator (mostly studios) want to protect its intelectual property against copy to prevent unauthorized redistribution of digital media.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-how" class="anchor" aria-hidden="true" href="#how"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How?&lt;/h4&gt;
&lt;p&gt;We're going to describe an abstract and generic form of DRM in a very simplified way.&lt;/p&gt;
&lt;p&gt;Given a &lt;strong&gt;content C1&lt;/strong&gt; (i.e. an hls or dash video streaming), with a &lt;strong&gt;player P1&lt;/strong&gt; (i.e. shaka-clappr, exo-player or ios) in a &lt;strong&gt;device D1&lt;/strong&gt; (i.e. a smartphone, TV, tablet or desktop/notebook) using a &lt;strong&gt;DRM system DRM1&lt;/strong&gt; (widevine, playready or FairPlay).&lt;/p&gt;
&lt;p&gt;The content C1 is encrypted with a &lt;strong&gt;symmetric-key K1&lt;/strong&gt; from the system DRM1, generating the &lt;strong&gt;encrypted content C'1&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/drm_general_flow.jpeg"&gt;&lt;img src="/i/drm_general_flow.jpeg" alt="drm general flow" title="drm general flow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The player P1, of a device D1, has two keys (asymmetric), a &lt;strong&gt;private key PRK1&lt;/strong&gt; (this key is protected&lt;sup&gt;1&lt;/sup&gt; and only known by &lt;strong&gt;D1&lt;/strong&gt;) and a &lt;strong&gt;public key PUK1&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;sup&gt;1&lt;/sup&gt;protected&lt;/strong&gt;: this protection can be &lt;strong&gt;via hardware&lt;/strong&gt;, for instance, this key can be stored inside a special (read-only) chip that works like &lt;a href="https://en.wikipedia.org/wiki/Black_box" rel="nofollow"&gt;a black-box&lt;/a&gt; to provide decryption, or &lt;strong&gt;by software&lt;/strong&gt; (less safe), the DRM system provides means to know which type of protection a given device has.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When the &lt;strong&gt;player P1 wants to play&lt;/strong&gt; the &lt;strong&gt;content C'1&lt;/strong&gt;, it needs to deal with the &lt;strong&gt;DRM system DRM1&lt;/strong&gt;, giving its public key &lt;strong&gt;PUK1&lt;/strong&gt;. The DRM system DRM1 returns the &lt;strong&gt;key K1 encrypted&lt;/strong&gt; with the client''s public key &lt;strong&gt;PUK1&lt;/strong&gt;. In theory, this response is something that &lt;strong&gt;only D1 is capable of decrypting&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;K1P1D1 = enc(K1, PUK1)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P1&lt;/strong&gt; uses its DRM local system (it could be a &lt;a href="https://en.wikipedia.org/wiki/System_on_a_chip" rel="nofollow"&gt;SOC&lt;/a&gt;, a specialized hardware or software), this system is &lt;strong&gt;able to decrypt&lt;/strong&gt; the content using its private key PRK1, it can decrypt &lt;strong&gt;the symmetric-key K1 from the K1P1D1&lt;/strong&gt; and &lt;strong&gt;play C'1&lt;/strong&gt;. At best case, the keys are not exposed through RAM.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;K1 = dec(K1P1D1, PRK1)

P1.play(dec(C'1, K1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/drm_decoder_flow.jpeg"&gt;&lt;img src="/i/drm_decoder_flow.jpeg" alt="drm decoder flow" title="drm decoder flow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-how-to-use-jupyter" class="anchor" aria-hidden="true" href="#how-to-use-jupyter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use jupyter&lt;/h1&gt;
&lt;p&gt;Make sure you have &lt;strong&gt;docker installed&lt;/strong&gt; and just run &lt;code&gt;./s/start_jupyter.sh&lt;/code&gt; and follow the instructions on the terminal.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-conferences" class="anchor" aria-hidden="true" href="#conferences"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conferences&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://demuxed.com/" rel="nofollow"&gt;DEMUXED&lt;/a&gt; - you can &lt;a href="https://www.youtube.com/channel/UCIc_DkRxo9UgUSTvWVNCmpA" rel="nofollow"&gt;check the last 2 events presentations.&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h1&gt;
&lt;p&gt;The richest content is here, it's where all the info we saw in this text was extracted, based or inspired by. You can deepen your knowledge with these amazing links, books, videos and etc.&lt;/p&gt;
&lt;p&gt;Online Courses and Tutorials:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/digital/" rel="nofollow"&gt;https://www.coursera.org/learn/digital/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf" rel="nofollow"&gt;https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://xiph.org/video/vid1.shtml" rel="nofollow"&gt;https://xiph.org/video/vid1.shtml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://xiph.org/video/vid2.shtml" rel="nofollow"&gt;https://xiph.org/video/vid2.shtml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://slhck.info/ffmpeg-encoding-course" rel="nofollow"&gt;http://slhck.info/ffmpeg-encoding-course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cambridgeincolour.com/tutorials/camera-sensors.htm" rel="nofollow"&gt;http://www.cambridgeincolour.com/tutorials/camera-sensors.htm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/vcodex/a-short-history-of-video-coding" rel="nofollow"&gt;http://www.slideshare.net/vcodex/a-short-history-of-video-coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338" rel="nofollow"&gt;http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.android.com/guide/topics/media/media-formats.html" rel="nofollow"&gt;https://developer.android.com/guide/topics/media/media-formats.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/MadhawaKasun/audio-compression-23398426" rel="nofollow"&gt;http://www.slideshare.net/MadhawaKasun/audio-compression-23398426&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf" rel="nofollow"&gt;http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Books:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486395327&amp;amp;sr=1-1" rel="nofollow"&gt;https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486395327&amp;amp;sr=1-1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925" rel="nofollow"&gt;https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486396914&amp;amp;sr=1-3&amp;amp;keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO" rel="nofollow"&gt;https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486396914&amp;amp;sr=1-3&amp;amp;keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486396940&amp;amp;sr=1-1&amp;amp;keywords=jan+ozer" rel="nofollow"&gt;https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486396940&amp;amp;sr=1-1&amp;amp;keywords=jan+ozer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Onboarding material:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Eyevinn/streaming-onboarding"&gt;https://github.com/Eyevinn/streaming-onboarding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://howvideo.works/" rel="nofollow"&gt;https://howvideo.works/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.aws.training/Details/eLearning?id=17775" rel="nofollow"&gt;https://www.aws.training/Details/eLearning?id=17775&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.aws.training/Details/eLearning?id=17887" rel="nofollow"&gt;https://www.aws.training/Details/eLearning?id=17887&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.aws.training/Details/Video?id=24750" rel="nofollow"&gt;https://www.aws.training/Details/Video?id=24750&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bitstream Specifications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.itu.int/rec/T-REC-H.264-201610-I" rel="nofollow"&gt;http://www.itu.int/rec/T-REC-H.264-201610-I&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&amp;amp;lang=en" rel="nofollow"&gt;http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&amp;amp;lang=en&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf" rel="nofollow"&gt;https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf" rel="nofollow"&gt;http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243" rel="nofollow"&gt;http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html" rel="nofollow"&gt;http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://forum.doom9.org/showthread.php?t=167081" rel="nofollow"&gt;https://forum.doom9.org/showthread.php?t=167081&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://forum.doom9.org/showthread.php?t=168947" rel="nofollow"&gt;https://forum.doom9.org/showthread.php?t=168947&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Software:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ffmpeg.org/" rel="nofollow"&gt;https://ffmpeg.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ffmpeg.org/ffmpeg-all.html" rel="nofollow"&gt;https://ffmpeg.org/ffmpeg-all.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ffmpeg.org/ffprobe.html" rel="nofollow"&gt;https://ffmpeg.org/ffprobe.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://trac.ffmpeg.org/wiki/" rel="nofollow"&gt;https://trac.ffmpeg.org/wiki/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://software.intel.com/en-us/intel-video-pro-analyzer" rel="nofollow"&gt;https://software.intel.com/en-us/intel-video-pro-analyzer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8" rel="nofollow"&gt;https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Non-ITU Codecs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://aomedia.googlesource.com/" rel="nofollow"&gt;https://aomedia.googlesource.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/webmproject/libvpx/tree/master/vp9"&gt;https://github.com/webmproject/libvpx/tree/master/vp9&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml" rel="nofollow"&gt;https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://people.xiph.org/~jm/daala/revisiting/" rel="nofollow"&gt;https://people.xiph.org/~jm/daala/revisiting/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=lzPaldsmJbk" rel="nofollow"&gt;https://www.youtube.com/watch?v=lzPaldsmJbk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fosdem.org/2017/schedule/event/om_av1/" rel="nofollow"&gt;https://fosdem.org/2017/schedule/event/om_av1/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jmvalin.ca/papers/AV1_tools.pdf" rel="nofollow"&gt;https://jmvalin.ca/papers/AV1_tools.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Encoding Concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://x265.org/hevc-h265/" rel="nofollow"&gt;http://x265.org/hevc-h265/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://slhck.info/video/2017/03/01/rate-control.html" rel="nofollow"&gt;http://slhck.info/video/2017/03/01/rate-control.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://slhck.info/video/2017/02/24/vbr-settings.html" rel="nofollow"&gt;http://slhck.info/video/2017/02/24/vbr-settings.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://slhck.info/video/2017/02/24/crf-guide.html" rel="nofollow"&gt;http://slhck.info/video/2017/02/24/crf-guide.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1702.00817v1.pdf" rel="nofollow"&gt;https://arxiv.org/pdf/1702.00817v1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors" rel="nofollow"&gt;https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html" rel="nofollow"&gt;http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html" rel="nofollow"&gt;http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/" rel="nofollow"&gt;https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blogs.gnome.org/rbultje/2016/12/13/overview-of-the-vp9-video-codec/" rel="nofollow"&gt;https://blogs.gnome.org/rbultje/2016/12/13/overview-of-the-vp9-video-codec/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://videoblerg.wordpress.com/2017/11/10/ffmpeg-and-how-to-use-it-wrong/" rel="nofollow"&gt;https://videoblerg.wordpress.com/2017/11/10/ffmpeg-and-how-to-use-it-wrong/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Video Sequences for Testing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://bbb3d.renderfarming.net/download.html" rel="nofollow"&gt;http://bbb3d.renderfarming.net/download.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx" rel="nofollow"&gt;https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Miscellaneous:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Eyevinn/streaming-onboarding"&gt;https://github.com/Eyevinn/streaming-onboarding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/a/24890903" rel="nofollow"&gt;http://stackoverflow.com/a/24890903&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264" rel="nofollow"&gt;http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html" rel="nofollow"&gt;http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://vanseodesign.com/web-design/color-luminance/" rel="nofollow"&gt;http://vanseodesign.com/web-design/color-luminance/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.biologymad.com/nervoussystem/eyenotes.htm" rel="nofollow"&gt;http://www.biologymad.com/nervoussystem/eyenotes.htm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf" rel="nofollow"&gt;http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.csc.villanova.edu/~rschumey/csc4800/dct.html" rel="nofollow"&gt;http://www.csc.villanova.edu/~rschumey/csc4800/dct.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.explainthatstuff.com/digitalcameras.html" rel="nofollow"&gt;http://www.explainthatstuff.com/digitalcameras.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.hkvstar.com" rel="nofollow"&gt;http://www.hkvstar.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.hometheatersound.com/" rel="nofollow"&gt;http://www.hometheatersound.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.lighterra.com/papers/videoencodingh264/" rel="nofollow"&gt;http://www.lighterra.com/papers/videoencodingh264/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.red.com/learn/red-101/video-chroma-subsampling" rel="nofollow"&gt;http://www.red.com/learn/red-101/video-chroma-subsampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/ManoharKuse/hevc-intra-coding" rel="nofollow"&gt;http://www.slideshare.net/ManoharKuse/hevc-intra-coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/mwalendo/h264vs-hevc" rel="nofollow"&gt;http://www.slideshare.net/mwalendo/h264vs-hevc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/rvarun7777/final-seminar-46117193" rel="nofollow"&gt;http://www.slideshare.net/rvarun7777/final-seminar-46117193&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf" rel="nofollow"&gt;http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx" rel="nofollow"&gt;http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&amp;amp;PageNum=1" rel="nofollow"&gt;http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&amp;amp;PageNum=1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/" rel="nofollow"&gt;http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/" rel="nofollow"&gt;https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/" rel="nofollow"&gt;https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://codesequoia.wordpress.com/category/video/" rel="nofollow"&gt;https://codesequoia.wordpress.com/category/video/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.apple.com/library/content/technotes/tn2224/_index.html" rel="nofollow"&gt;https://developer.apple.com/library/content/technotes/tn2224/_index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikibooks.org/wiki/MeGUI/x264_Settings" rel="nofollow"&gt;https://en.wikibooks.org/wiki/MeGUI/x264_Settings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/AOMedia_Video_1" rel="nofollow"&gt;https://en.wikipedia.org/wiki/AOMedia_Video_1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Cone_cell" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Cone_cell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg" rel="nofollow"&gt;https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Inter_frame" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Inter_frame&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Intra-frame_coding" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Intra-frame_coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Photoreceptor_cell" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Photoreceptor_cell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Pixel_aspect_ratio" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Pixel_aspect_ratio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Presentation_timestamp" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Presentation_timestamp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Rod_cell" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Rod_cell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg" rel="nofollow"&gt;https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/" rel="nofollow"&gt;https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping" rel="nofollow"&gt;https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/" rel="nofollow"&gt;https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03" rel="nofollow"&gt;https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.encoding.com/android/" rel="nofollow"&gt;https://www.encoding.com/android/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.encoding.com/http-live-streaming-hls/" rel="nofollow"&gt;https://www.encoding.com/http-live-streaming-hls/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm" rel="nofollow"&gt;https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lifewire.com/cmos-image-sensor-493271" rel="nofollow"&gt;https://www.lifewire.com/cmos-image-sensor-493271&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ" rel="nofollow"&gt;https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar" rel="nofollow"&gt;https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.vcodex.com/h264avc-intra-precition/" rel="nofollow"&gt;https://www.vcodex.com/h264avc-intra-precition/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=9vgtJJ2wwMA" rel="nofollow"&gt;https://www.youtube.com/watch?v=9vgtJJ2wwMA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=LFXN9PiOGtY" rel="nofollow"&gt;https://www.youtube.com/watch?v=LFXN9PiOGtY&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=Lto-ajuqW3w&amp;amp;list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6" rel="nofollow"&gt;https://www.youtube.com/watch?v=Lto-ajuqW3w&amp;amp;list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=LWxu4rkZBLw" rel="nofollow"&gt;https://www.youtube.com/watch?v=LWxu4rkZBLw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf" rel="nofollow"&gt;https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>leandromoreira</author><guid isPermaLink="false">https://github.com/leandromoreira/digital_video_introduction</guid><pubDate>Tue, 26 Nov 2019 00:03:00 GMT</pubDate></item><item><title>alirezadir/Production-Level-Deep-Learning #4 in All Languages, Today</title><link>https://github.com/alirezadir/Production-Level-Deep-Learning</link><description>&lt;p&gt;&lt;i&gt;This repo attempts to serve as a guideline for building practical production-level deep learning systems to be deployed in real world applications. &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-bulb-a-guide-to-production-level-deep-learning-clapper-scroll--ferry" class="anchor" aria-hidden="true" href="#bulb-a-guide-to-production-level-deep-learning-clapper-scroll--ferry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="bulb" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png"&gt;💡&lt;/g-emoji&gt; A Guide to Production Level Deep Learning &lt;g-emoji class="g-emoji" alias="clapper" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ac.png"&gt;🎬&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt;  &lt;g-emoji class="g-emoji" alias="ferry" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26f4.png"&gt;⛴&lt;/g-emoji&gt;&lt;/h1&gt;
&lt;p&gt;[NOTE: This repo is still under development, and any feedback to make it better is welcome &lt;g-emoji class="g-emoji" alias="blush" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60a.png"&gt;😊&lt;/g-emoji&gt; ]&lt;/p&gt;
&lt;p&gt;Deploying deep learning models in production could be challenging, as it's far beyond just training models with good performance. As you can see in the following figure, there are several components that need to be properly designed and developed in order to deploy a production level deep learning system:&lt;/p&gt;
&lt;p align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/alirezadir/Production-Level-Deep-Learning/blob/master/images/components.png"&gt;&lt;img src="https://github.com/alirezadir/Production-Level-Deep-Learning/raw/master/images/components.png" title="" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;This repo aims to serve as a an engineering guideline for building production-level deep learning systems to be deployed in real world applications.&lt;/p&gt;
&lt;p&gt;The material presented here is mostly borrowed from &lt;a href="https://fullstackdeeplearning.com" rel="nofollow"&gt;Full Stack Deep Learning Bootcamp&lt;/a&gt; (by &lt;a href="https://people.eecs.berkeley.edu/~pabbeel/" rel="nofollow"&gt;Pieter Abbeel&lt;/a&gt; at UC Berkeley, &lt;a href="http://josh-tobin.com/" rel="nofollow"&gt;Josh Tobin&lt;/a&gt; at OpenAI, and &lt;a href="https://sergeykarayev.com/" rel="nofollow"&gt;Sergey Karayev&lt;/a&gt; at Turnitin), &lt;a href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/detail/79327" rel="nofollow"&gt;TFX workshop&lt;/a&gt; by &lt;a href="https://www.linkedin.com/in/robert-crowe/" rel="nofollow"&gt;Robert Crowe&lt;/a&gt;, and &lt;a href="https://pipeline.ai/" rel="nofollow"&gt;Pipeline.ai&lt;/a&gt;'s &lt;a href="https://www.meetup.com/Advanced-KubeFlow/" rel="nofollow"&gt;Advanced KubeFlow Meetup&lt;/a&gt; by &lt;a href="https://www.linkedin.com/in/cfregly/" rel="nofollow"&gt;Chris Fregly&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The following figure represent a high level overview of different components in a production level deep learning system:&lt;/p&gt;
&lt;p align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/alirezadir/Production-Level-Deep-Learning/blob/master/images/infra_tooling.png"&gt;&lt;img src="https://github.com/alirezadir/Production-Level-Deep-Learning/raw/master/images/infra_tooling.png" title="" width="95%" height="95%" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
In the following, we will go through each module and recommend toolsets and frameworks as well as best practices from practitioners that fit each component. 
&lt;h1&gt;&lt;a id="user-content-full-stack-pipeline" class="anchor" aria-hidden="true" href="#full-stack-pipeline"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Full stack pipeline&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-1-data-management" class="anchor" aria-hidden="true" href="#1-data-management"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Data Management&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-11-data-sources" class="anchor" aria-hidden="true" href="#11-data-sources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.1 Data Sources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Supervised deep learning requires a lot of labeled data&lt;/li&gt;
&lt;li&gt;Labeling own data is costly!&lt;/li&gt;
&lt;li&gt;Here are some resources for data:
&lt;ul&gt;
&lt;li&gt;Open source data (good to start with, but not an advantage)&lt;/li&gt;
&lt;li&gt;Data augmentation (a MUST for computer vision, an option for NLP)&lt;/li&gt;
&lt;li&gt;Synthetic data (almost always worth starting with, esp. in NLP)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-12--data-labeling" class="anchor" aria-hidden="true" href="#12--data-labeling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.2  Data Labeling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Requires: separate software stack (labeling platforms), temporary labor, and QC&lt;/li&gt;
&lt;li&gt;Sources of labor for labeling:
&lt;ul&gt;
&lt;li&gt;Crowdsourcing (Mechanical Turk): cheap and scalable, less reliable, needs QC&lt;/li&gt;
&lt;li&gt;Hiring own annotators: less QC needed, expensive, slow to scale&lt;/li&gt;
&lt;li&gt;Data labeling service companies:
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.figure-eight.com/" rel="nofollow"&gt;FigureEight&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Labeling platforms:
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://prodi.gy/" rel="nofollow"&gt;Prodigy&lt;/a&gt;: An annotation tool powered
by active learning (by developers of Spacy), text and image&lt;/li&gt;
&lt;li&gt;&lt;a href="https://thehive.ai/" rel="nofollow"&gt;HIVE&lt;/a&gt;: AI as a Service platform for computer vision&lt;/li&gt;
&lt;li&gt;&lt;a href="https://supervise.ly/" rel="nofollow"&gt;Supervisely&lt;/a&gt;: entire computer vision platform&lt;/li&gt;
&lt;li&gt;&lt;a href="https://labelbox.com/" rel="nofollow"&gt;Labelbox&lt;/a&gt;: computer vision&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scale.com/" rel="nofollow"&gt;Scale&lt;/a&gt; AI data platform (computer vision &amp;amp; NLP)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-13-data-storage" class="anchor" aria-hidden="true" href="#13-data-storage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.3. Data Storage&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data storage options:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Object store&lt;/strong&gt;: Store binary data (images, sound files, compressed texts)
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/s3/" rel="nofollow"&gt;Amazon S3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ceph.io/" rel="nofollow"&gt;Ceph&lt;/a&gt; Object Store&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: Store metadata (file paths, labels, user activity, etc).
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.postgresql.org/" rel="nofollow"&gt;Postgres&lt;/a&gt; is the right choice for most of applications, with the best-in-class SQL and great support for unstructured JSON.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Lake&lt;/strong&gt;: to aggregate features which are not obtainable from database (e.g. logs)
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/redshift/" rel="nofollow"&gt;Amazon Redshift&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feature Store&lt;/strong&gt;: store, access, and share machine learning features
(Feature extraction could be computationally expensive and nearly impossible to scale, hence re-using features by different models and teams is a key to high performance ML teams).
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/gojek/feast"&gt;FEAST&lt;/a&gt; (Google cloud, Open Source)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://eng.uber.com/michelangelo/" rel="nofollow"&gt;Michelangelo Palette&lt;/a&gt; (Uber)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Suggestion: At training time, copy data into a local or networked &lt;strong&gt;filesystem&lt;/strong&gt; (NFS). &lt;sup&gt;&lt;a href="#fsdl"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-14-data-versioning" class="anchor" aria-hidden="true" href="#14-data-versioning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.4. Data Versioning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It's a "MUST" for deployed ML models:&lt;br&gt;
&lt;strong&gt;Deployed ML models are part code, part data&lt;/strong&gt;. &lt;sup&gt;&lt;a href="#fsdl"&gt;1&lt;/a&gt;&lt;/sup&gt;  No data versioning means no model versioning.&lt;/li&gt;
&lt;li&gt;Data versioning platforms:
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://dvc.org/" rel="nofollow"&gt;DVC&lt;/a&gt;: Open source version control system for ML projects&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pachyderm.com/" rel="nofollow"&gt;Pachyderm&lt;/a&gt;: version control for data&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.liquidata.co/" rel="nofollow"&gt;Dolt&lt;/a&gt;: versioning for SQL database&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-15-data-processing" class="anchor" aria-hidden="true" href="#15-data-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.5. Data Processing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Training data for production models may come from different sources, including &lt;em&gt;Stored data in db and object stores&lt;/em&gt;, &lt;em&gt;log processing&lt;/em&gt;, and &lt;em&gt;outputs of other classifiers&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;There are dependencies between tasks, each needs to be kicked off after its dependencies are finished. For example, training on new log data, requires a preprocessing step before training.&lt;/li&gt;
&lt;li&gt;Makefiles are not scalable. "Workflow manager"s become pretty essential in this regard.&lt;/li&gt;
&lt;li&gt;Workflow managers:
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://airflow.apache.org/" rel="nofollow"&gt;Airflow&lt;/a&gt; by Airbnb: Dynamic, extensible, elegant, and scalable (the most commonly used)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DAG workflow&lt;/li&gt;
&lt;li&gt;Robust conditional execution: retry in case of failure&lt;/li&gt;
&lt;li&gt;Pusher supports docker images with tensorflow serving&lt;/li&gt;
&lt;li&gt;Whole workflow in a single .py file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/spotify/luigi"&gt;Luigi&lt;/a&gt; by Spotify&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-2-development-training-and-evaluation" class="anchor" aria-hidden="true" href="#2-development-training-and-evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Development, Training, and Evaluation&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-21-software-engineering" class="anchor" aria-hidden="true" href="#21-software-engineering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.1. Software engineering&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Winner language: Python&lt;/li&gt;
&lt;li&gt;Editors:
&lt;ul&gt;
&lt;li&gt;Vim&lt;/li&gt;
&lt;li&gt;Emacs&lt;/li&gt;
&lt;li&gt;&lt;a href="https://code.visualstudio.com/" rel="nofollow"&gt;VS Code&lt;/a&gt; (Recommended by the author): Built-in git staging and diff, Lint code, open projects remotely through ssh&lt;/li&gt;
&lt;li&gt;Jupyter Notebooks: Great as starting point of the projects, hard to scale&lt;/li&gt;
&lt;li&gt;&lt;a href="https://streamlit.io/" rel="nofollow"&gt;Streamlit&lt;/a&gt;: interactive data science tool with applets&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compute recommendations &lt;sup&gt;&lt;a href="#fsdl"&gt;1&lt;/a&gt;&lt;/sup&gt;:
&lt;ul&gt;
&lt;li&gt;For &lt;em&gt;individuals&lt;/em&gt; or &lt;em&gt;startups&lt;/em&gt;:
&lt;ul&gt;
&lt;li&gt;Development: a 4x Turing-architecture PC&lt;/li&gt;
&lt;li&gt;Training/Evaluation: Use the same 4x GPU PC. When running many experiments, either buy shared servers or use cloud instances.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For &lt;em&gt;large companies:&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;Development: Buy a 4x Turing-architecture PC per ML scientist or let them use V100 instances&lt;/li&gt;
&lt;li&gt;Training/Evaluation: Use cloud instances with proper provisioning and handling of failures&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud Providers:
&lt;ul&gt;
&lt;li&gt;GCP: option to connect GPUs to any instance + has TPUs&lt;/li&gt;
&lt;li&gt;AWS:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-22-resource-management" class="anchor" aria-hidden="true" href="#22-resource-management"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.2. Resource Management&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Allocating free resources to programs&lt;/li&gt;
&lt;li&gt;Resource management options:
&lt;ul&gt;
&lt;li&gt;Old school cluster job scheduler ( e.g. &lt;a href="https://slurm.schedmd.com/" rel="nofollow"&gt;Slurm&lt;/a&gt; workload manager )&lt;/li&gt;
&lt;li&gt;Docker + Kubernetes&lt;/li&gt;
&lt;li&gt;Kubeflow&lt;/li&gt;
&lt;li&gt;&lt;a href="https://polyaxon.com/" rel="nofollow"&gt;Polyaxon&lt;/a&gt; (paid features)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-23-dl-frameworks" class="anchor" aria-hidden="true" href="#23-dl-frameworks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.3. DL Frameworks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Unless having a good reason not to, use Tensorflow/Keras or PyTorch. &lt;sup&gt;&lt;a href="#fsdl"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;The following figure shows a comparison between different frameworks on how they stand for &lt;em&gt;"developement"&lt;/em&gt; and &lt;em&gt;"production"&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
  &lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/alirezadir/Production-Level-Deep-Learning/blob/master/images/frameworks.png"&gt;&lt;img src="https://github.com/alirezadir/Production-Level-Deep-Learning/raw/master/images/frameworks.png" title="" width="45%" height="45%" style="max-width:100%;"&gt;&lt;/a&gt;
   &lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-24-experiment-management" class="anchor" aria-hidden="true" href="#24-experiment-management"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.4. Experiment management&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Development, training, and evaluation strategy:
&lt;ul&gt;
&lt;li&gt;Always start &lt;strong&gt;simple&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Train a small model on a small batch. Only if it works, scale to larger data and models, and hyperparameter tuning!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Experiment management tools:&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/tensorboard" rel="nofollow"&gt;Tensorboard&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;provides the visualization and tooling needed for ML experimentation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://losswise.com/" rel="nofollow"&gt;Losswise&lt;/a&gt; (Monitoring for ML)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.comet.ml/" rel="nofollow"&gt;Comet&lt;/a&gt;: lets you track code, experiments, and results on ML projects&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.wandb.com/" rel="nofollow"&gt;Weights &amp;amp; Biases&lt;/a&gt;: Record and visualize every detail of your research with easy collaboration&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mlflow.org/docs/latest/tracking.html#tracking" rel="nofollow"&gt;MLFlow Tracking&lt;/a&gt;: for logging parameters, code versions, metrics, and output files, and  visualization of the results.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-25-hyperparameter-tuning" class="anchor" aria-hidden="true" href="#25-hyperparameter-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.5. Hyperparameter Tuning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/kubeflow/katib"&gt;Katib&lt;/a&gt;: Kubernete's Native System for Hyperparameter Tuning and Neural Architecture Search, inspired by &lt;a href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/bcb15507f4b52991a0783013df4222240e942381.pdf" rel="nofollow"&gt;Google vizier&lt;/a&gt; and supports multiple ML/DL frameworks (e.g. TensorFlow, MXNet, and PyTorch).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://maxpumperla.com/hyperas/" rel="nofollow"&gt;Hyperas&lt;/a&gt;: a simple wrapper around hyperopt for Keras, with a simple template notation to define hyper-parameter ranges to tune.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sigopt.com/" rel="nofollow"&gt;SIGOPT&lt;/a&gt;:  a scalable, enterprise-grade optimization platform&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ray-project/ray/tree/master/python/ray/tune"&gt;Ray-Tune&lt;/a&gt;: A scalable research platform for distributed model selection (with a focus on deep learning and deep reinforcement learning)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.wandb.com/library/sweeps" rel="nofollow"&gt;Sweeps&lt;/a&gt; from &lt;a href="https://www.wandb.com/" rel="nofollow"&gt;Weights &amp;amp; Biases&lt;/a&gt;: Parameters are not explicitly specified by a developer. Instead they are approximated and learned by a machine learning model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-26-distributed-training" class="anchor" aria-hidden="true" href="#26-distributed-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.6. Distributed Training&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data parallelism: Use it when iteration time is too long (both tensorflow and PyTorch support)&lt;/li&gt;
&lt;li&gt;Model parallelism: when model does not fit on a single GPU&lt;/li&gt;
&lt;li&gt;Other solutions:
&lt;ul&gt;
&lt;li&gt;Ray&lt;/li&gt;
&lt;li&gt;Horovod&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-3-troubleshooting-tbd" class="anchor" aria-hidden="true" href="#3-troubleshooting-tbd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. Troubleshooting [TBD]&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-4-testing-and-deployment" class="anchor" aria-hidden="true" href="#4-testing-and-deployment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4. Testing and Deployment&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-41-testing-and-cicd" class="anchor" aria-hidden="true" href="#41-testing-and-cicd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.1. Testing and CI/CD&lt;/h3&gt;
&lt;p&gt;Machine Learning production software requires a more diverse set of test suites than traditional software:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/alirezadir/Production-Level-Deep-Learning/blob/master/images/testing.png"&gt;&lt;img src="https://github.com/alirezadir/Production-Level-Deep-Learning/raw/master/images/testing.png" title="" width="75%" height="75%" style="max-width:100%;"&gt;&lt;/a&gt;
   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unit and Integration Testing:
&lt;ul&gt;
&lt;li&gt;Types of test:
&lt;ul&gt;
&lt;li&gt;Training system tests: testing training pipeline&lt;/li&gt;
&lt;li&gt;Validation tests: testing prediction system on validation set&lt;/li&gt;
&lt;li&gt;Functionality tests: testing prediction system on few important examples&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Continuous Integration: Running tests after each new code change pushed to the repo&lt;/li&gt;
&lt;li&gt;SaaS for continuous integration:
&lt;ul&gt;
&lt;li&gt;CircleCI, Travis&lt;/li&gt;
&lt;li&gt;Jenkins, Buildkite&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-42-web-depolyment" class="anchor" aria-hidden="true" href="#42-web-depolyment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.2. Web Depolyment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Consists of a &lt;strong&gt;Prediction System&lt;/strong&gt; and a &lt;strong&gt;Serving System&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Prediction System: Process input data, make predictions&lt;/li&gt;
&lt;li&gt;Serving System (Web server):
&lt;ul&gt;
&lt;li&gt;Serve prediction with scale in mind&lt;/li&gt;
&lt;li&gt;Use REST API to serve prediction HTTP requests&lt;/li&gt;
&lt;li&gt;Calls the prediction system to respond&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Serving options:
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Deploy to VMs, scale by adding instances&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Deploy as containers, scale via orchestration&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Containers
&lt;ul&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Container Orchestration:
&lt;ul&gt;
&lt;li&gt;Kubernetes (the most popular now)&lt;/li&gt;
&lt;li&gt;MESOS&lt;/li&gt;
&lt;li&gt;Marathon&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Deploy code as a "serverless function"&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Deploy via a &lt;strong&gt;model serving&lt;/strong&gt; solution&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model serving:
&lt;ul&gt;
&lt;li&gt;Specialized web deployment for ML models&lt;/li&gt;
&lt;li&gt;Batches request for GPU inference&lt;/li&gt;
&lt;li&gt;Frameworks:
&lt;ul&gt;
&lt;li&gt;Tensorflow serving&lt;/li&gt;
&lt;li&gt;MXNet Model server&lt;/li&gt;
&lt;li&gt;Clipper (Berkeley)&lt;/li&gt;
&lt;li&gt;SaaS solutions (Seldon, Algorithma)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Decision making:
&lt;ul&gt;
&lt;li&gt;CPU inference:
&lt;ul&gt;
&lt;li&gt;CPU inference is preferable if it meets the requirements.&lt;/li&gt;
&lt;li&gt;Scale by adding more servers, or going serverless.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GPU inference:
&lt;ul&gt;
&lt;li&gt;TF serving or Clipper&lt;/li&gt;
&lt;li&gt;Adaptive batching is useful&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-45-service-mesh-and-traffic-routing" class="anchor" aria-hidden="true" href="#45-service-mesh-and-traffic-routing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.5 Service Mesh and Traffic Routing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Transition from monolithic applications towards a distributed microservice architecture could be challenging.&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;Service mesh&lt;/strong&gt; (consisting of a network of microservices) reduces the complexity of such deployments, and eases the strain on development teams.
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://istio.io/" rel="nofollow"&gt;Istio&lt;/a&gt;: a service mesh to ease creation of  a network of deployed services with load balancing, service-to-service authentication, monitoring, with few or no code changes in service code.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-44-monitoring" class="anchor" aria-hidden="true" href="#44-monitoring"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.4. Monitoring:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Purpose of monitoring:
&lt;ul&gt;
&lt;li&gt;Alerts for downtime, errors, and distribution shifts&lt;/li&gt;
&lt;li&gt;Catching service and data regressions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud providers solutions are decent&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-45-deploying-on-embedded-and-mobile-devices" class="anchor" aria-hidden="true" href="#45-deploying-on-embedded-and-mobile-devices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.5. Deploying on Embedded and Mobile Devices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Main challenge: memory footprint and compute constraints&lt;/li&gt;
&lt;li&gt;Solutions:
&lt;ul&gt;
&lt;li&gt;Quantization&lt;/li&gt;
&lt;li&gt;Reduced model size
&lt;ul&gt;
&lt;li&gt;MobileNets&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge Distillation
&lt;ul&gt;
&lt;li&gt;DistillBERT (for NLP)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Embedded and Mobile Frameworks:
&lt;ul&gt;
&lt;li&gt;Tensorflow Lite&lt;/li&gt;
&lt;li&gt;PyTorch Mobile&lt;/li&gt;
&lt;li&gt;Core ML&lt;/li&gt;
&lt;li&gt;ML Kit&lt;/li&gt;
&lt;li&gt;FRITZ&lt;/li&gt;
&lt;li&gt;OpenVINO&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model Conversion:
&lt;ul&gt;
&lt;li&gt;Open Neural Network Exchange (ONNX): open-source format for deep learning models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-46-all-in-one-solutions" class="anchor" aria-hidden="true" href="#46-all-in-one-solutions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.6. All-in-one solutions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Tensorflow Extended (TFX)&lt;/li&gt;
&lt;li&gt;Michelangelo (Uber)&lt;/li&gt;
&lt;li&gt;Google Cloud AI Platform&lt;/li&gt;
&lt;li&gt;Amazon SageMaker&lt;/li&gt;
&lt;li&gt;Neptune&lt;/li&gt;
&lt;li&gt;FLOYD&lt;/li&gt;
&lt;li&gt;Paperspace&lt;/li&gt;
&lt;li&gt;Determined AI&lt;/li&gt;
&lt;li&gt;Domino data lab&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
   &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/alirezadir/Production-Level-Deep-Learning/blob/master/images/infra-cmp.png"&gt;&lt;img src="https://github.com/alirezadir/Production-Level-Deep-Learning/raw/master/images/infra-cmp.png" title="" width="95%" height="95%" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-tensorflow-extended-tfx" class="anchor" aria-hidden="true" href="#tensorflow-extended-tfx"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tensorflow Extended (TFX)&lt;/h1&gt;
&lt;h1&gt;&lt;a id="user-content-airflow-and-kubeflow-ml-pipelines" class="anchor" aria-hidden="true" href="#airflow-and-kubeflow-ml-pipelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Airflow and KubeFlow ML Pipelines&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-other-useful-links" class="anchor" aria-hidden="true" href="#other-useful-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other useful links:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/xamat/lessons-learned-from-building-practical-deep-learning-systems" rel="nofollow"&gt;Lessons learned from building practical deep learning systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ai.google/research/pubs/pub43146" rel="nofollow"&gt;Machine Learning: The High Interest Credit Card of Technical Debt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/alirezadir/Production-Level-Deep-Learning/blob/master/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References:&lt;/h2&gt;
&lt;p&gt;&lt;a name="user-content-fsdl"&gt;[1]&lt;/a&gt;: &lt;a href="https://fullstackdeeplearning.com/" rel="nofollow"&gt;Full Stack Deep Learning Bootcamp&lt;/a&gt;, Nov 2019.&lt;/p&gt;
&lt;p&gt;&lt;a name="user-content-pipe"&gt;[2]&lt;/a&gt;: &lt;a href="https://www.meetup.com/Advanced-KubeFlow/" rel="nofollow"&gt;Advanced KubeFlow Workshop&lt;/a&gt; by &lt;a href="https://pipeline.ai/" rel="nofollow"&gt;Pipeline.ai&lt;/a&gt;, 2019.&lt;/p&gt;
&lt;p&gt;&lt;a name="user-content-pipe"&gt;[3]&lt;/a&gt;: &lt;a href="https://cdn.oreillystatic.com/en/assets/1/event/298/TFX_%20Production%20ML%20pipelines%20with%20TensorFlow%20Presentation.pdf" rel="nofollow"&gt;TFX: Real World Machine Learning in Production&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>alirezadir</author><guid isPermaLink="false">https://github.com/alirezadir/Production-Level-Deep-Learning</guid><pubDate>Tue, 26 Nov 2019 00:04:00 GMT</pubDate></item><item><title>rasbt/deeplearning-models #5 in All Languages, Today</title><link>https://github.com/rasbt/deeplearning-models</link><description>&lt;p&gt;&lt;i&gt;A collection of various deep learning architectures, models, and tips&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667" alt="Python 3.7" data-canonical-src="https://img.shields.io/badge/Python-3.7-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-deep-learning-models" class="anchor" aria-hidden="true" href="#deep-learning-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning Models&lt;/h1&gt;
&lt;p&gt;A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-traditional-machine-learning" class="anchor" aria-hidden="true" href="#traditional-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Traditional Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Perceptron&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/perceptron.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/perceptron.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/perceptron.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/perceptron.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Logistic Regression&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/logistic-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/logistic-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/logistic-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/logistic-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Softmax Regression (Multinomial Logistic Regression)&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/softmax-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/softmax-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/softmax-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/softmax-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-multilayer-perceptrons" class="anchor" aria-hidden="true" href="#multilayer-perceptrons"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multilayer Perceptrons&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Multilayer Perceptron&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Dropout&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-dropout.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-dropout.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-dropout.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-dropout.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Batch Normalization&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Backpropagation from Scratch&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-lowlevel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-lowlevel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-fromscratch__sigmoid-mse.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-fromscratch__sigmoid-mse.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-basic" class="anchor" aria-hidden="true" href="#basic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/cnn/cnn-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/cnn/cnn-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network with He Initialization&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-he-init.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-he-init.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-concepts" class="anchor" aria-hidden="true" href="#concepts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Concepts&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Replacing Fully-Connnected by Equivalent Convolutional Layers&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/fc-to-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/fc-to-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-fully-convolutional" class="anchor" aria-hidden="true" href="#fully-convolutional"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fully Convolutional&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Fully Convolutional Neural Network&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-allconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-allconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-lenet" class="anchor" aria-hidden="true" href="#lenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LeNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;LeNet-5 on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LeNet-5 on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LeNet-5 on QuickDraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-alexnet" class="anchor" aria-hidden="true" href="#alexnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AlexNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;AlexNet on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-vgg" class="anchor" aria-hidden="true" href="#vgg"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VGG&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network VGG-16&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/cnn/cnn-vgg16.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/cnn/cnn-vgg16.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;VGG-16 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network VGG-19&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg19.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg19.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-densenet" class="anchor" aria-hidden="true" href="#densenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DenseNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DenseNet-121 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-densenet121-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-densenet121-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;DenseNet-121 Image Classifier Trained on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-densenet121-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-densenet121-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-resnet" class="anchor" aria-hidden="true" href="#resnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ResNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ResNet and Residual Blocks&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/resnet-ex-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/resnet-ex-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-18 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-18 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Object Classifier Trained on QuickDraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-50 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet50-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet50-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-50 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-101 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet101-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-101 Trained on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-152 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet152-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet152-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-network-in-network" class="anchor" aria-hidden="true" href="#network-in-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network in Network&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Network in Network CIFAR-10 Classifier&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-metric-learning" class="anchor" aria-hidden="true" href="#metric-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Metric Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Siamese Network with Multilayer Perceptrons&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/metric/siamese-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/metric/siamese-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-autoencoders" class="anchor" aria-hidden="true" href="#autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Autoencoders&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-fully-connected-autoencoders" class="anchor" aria-hidden="true" href="#fully-connected-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fully-connected Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Autoencoder (MNIST)&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Autoencoder (MNIST) + Scikit-Learn Random Forest Classifier&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-basic-with-rf.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-basic-with-rf.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-convolutional-autoencoders" class="anchor" aria-hidden="true" href="#convolutional-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions / Transposed Convolutions&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-deconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-deconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions and Continuous Jaccard Distance&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv-jaccard.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv-jaccard.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions (without pooling operations)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv-nopool.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv-nopool.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-conv-nneighbor.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-conv-nneighbor.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation -- Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation -- Trained on Quickdraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor-quickdraw-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor-quickdraw-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-variational-autoencoders" class="anchor" aria-hidden="true" href="#variational-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Variational Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Variational Autoencoder&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-var.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-var.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Variational Autoencoder&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-var.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-var.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-conditional-variational-autoencoders" class="anchor" aria-hidden="true" href="#conditional-variational-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conditional Variational Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Conditional Variational Autoencoder (with labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cvae.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cvae.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Conditional Variational Autoencoder (without labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cvae_no-out-concat.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cvae_no-out-concat.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Conditional Variational Autoencoder (with labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cnn-cvae.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cnn-cvae.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Conditional Variational Autoencoder (without labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cnn-cvae_no-out-concat.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cnn-cvae_no-out-concat.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-generative-adversarial-networks-gans" class="anchor" aria-hidden="true" href="#generative-adversarial-networks-gans"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks (GANs)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fully Connected GAN on MNIST&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Fully Connected Wasserstein GAN on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/wgan-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/wgan-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional GAN on MNIST&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional GAN on MNIST with Label Smoothing&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan-conv-smoothing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan-conv-smoothing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan-conv-smoothing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan-conv-smoothing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Wasserstein GAN on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/dc-wgan-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/dc-wgan-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-graph-neural-networks-gnns" class="anchor" aria-hidden="true" href="#graph-neural-networks-gnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Graph Neural Networks (GNNs)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most Basic Graph Neural Network with Gaussian Filter on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Basic Graph Neural Network with Edge Prediction on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-edge-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-edge-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Basic Graph Neural Network with Spectral Graph Convolution on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-graph-spectral-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-graph-spectral-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-recurrent-neural-networks-rnns" class="anchor" aria-hidden="true" href="#recurrent-neural-networks-rnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks (RNNs)&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-many-to-one-sentiment-analysis--classification" class="anchor" aria-hidden="true" href="#many-to-one-sentiment-analysis--classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Many-to-one: Sentiment Analysis / Classification&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple single-layer RNN (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_simple_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_simple_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A simple single-layer RNN with packed sequences to ignore padding characters (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_simple_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_simple_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells (IMDB) and pre-trained GloVe word vectors&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells and Own Dataset in CSV Format (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with GRU cells (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer bi-directional RNN (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-many-to-many--sequence-to-sequence" class="anchor" aria-hidden="true" href="#many-to-many--sequence-to-sequence"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Many-to-Many / Sequence-to-Sequence&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple character RNN to generate new text (Charles Dickens)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ordinal-regression" class="anchor" aria-hidden="true" href="#ordinal-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ordinal Regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ordinal Regression CNN -- CORAL w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-coral-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-coral-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ordinal Regression CNN -- Niu et al. 2016 w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-niu-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-niu-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ordinal Regression CNN -- Beckham and Pal 2016 w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-beckham2016-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-beckham2016-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tips-and-tricks" class="anchor" aria-hidden="true" href="#tips-and-tricks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tips and Tricks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cyclical Learning Rate&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/cyclical-learning-rate.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/cyclical-learning-rate.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Annealing with Increasing the Batch Size (w. CIFAR-10 &amp;amp; AlexNet)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/cnn-alexnet-cifar10-batchincrease.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/cnn-alexnet-cifar10-batchincrease.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Gradient Clipping (w. MLP on MNIST)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/gradclipping_mlp.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/gradclipping_mlp.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-transfer-learning" class="anchor" aria-hidden="true" href="#transfer-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transfer Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Transfer Learning Example (VGG16 pre-trained on ImageNet for Cifar-10)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;   [PyTorch: &lt;a href="pytorch_ipynb/transfer/transferlearning-vgg16-cifar10-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/transfer/transferlearning-vgg16-cifar10-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pytorch-workflows-and-mechanics" class="anchor" aria-hidden="true" href="#pytorch-workflows-and-mechanics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Workflows and Mechanics&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-custom-datasets" class="anchor" aria-hidden="true" href="#custom-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- CSV files converted to HDF5&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Face Images from CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from Quickdraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from the Street View House Number (SVHN) Dataset&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Asian Face Dataset (AFAD)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Dating Historical Color Images&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-and-preprocessing" class="anchor" aria-hidden="true" href="#training-and-preprocessing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Preprocessing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Generating Validation Set Splits&lt;br&gt;
[PyTorch]: &lt;a href="pytorch_ipynb/mechanics/validation-splits.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/validation-splits.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Dataloading with Pinned Memory&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-cifar10-pinmem.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-cifar10-pinmem.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Standardizing Images&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-standardized.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-standardized.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Image Transformation Examples&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/torchvision-transform-examples.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/torchvision-transform-examples.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Char-RNN with Own Text File&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sentiment Classification RNN with Own CSV File&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-parallel-computing" class="anchor" aria-hidden="true" href="#parallel-computing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Parallel Computing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Using Multiple GPUs with DataParallel -- VGG-16 Gender Classifier on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-other" class="anchor" aria-hidden="true" href="#other"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Sequential API and hooks&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/mlp-sequential.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/mlp-sequential.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Weight Sharing Within a Layer&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/cnn-weight-sharing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/cnn-weight-sharing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Plotting Live Training Performance in Jupyter Notebooks with just Matplotlib&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/plot-jupyter-matplotlib.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/plot-jupyter-matplotlib.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-autograd" class="anchor" aria-hidden="true" href="#autograd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Autograd&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Getting Gradients of an Intermediate Variable in PyTorch&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/manual-gradients.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/manual-gradients.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tensorflow-workflows-and-mechanics" class="anchor" aria-hidden="true" href="#tensorflow-workflows-and-mechanics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Workflows and Mechanics&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-custom-datasets-1" class="anchor" aria-hidden="true" href="#custom-datasets-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Chunking an Image Dataset for Minibatch Training using NumPy NPZ Archives&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Storing an Image Dataset for Minibatch Training using HDF5&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using Input Pipelines to Read Data from TFRecords Files&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/tfrecords.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/tfrecords.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using Queue Runners to Feed Images Directly from Disk&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/file-queues.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/file-queues.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using TensorFlow's Dataset API&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/dataset-api.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/dataset-api.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-and-preprocessing-1" class="anchor" aria-hidden="true" href="#training-and-preprocessing-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Preprocessing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Saving and Loading Trained Models -- from TensorFlow Checkpoint Files and NumPy NPZ Archives&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/saving-and-reloading-models.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/saving-and-reloading-models.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rasbt</author><guid isPermaLink="false">https://github.com/rasbt/deeplearning-models</guid><pubDate>Tue, 26 Nov 2019 00:05:00 GMT</pubDate></item><item><title>ginuerzh/gost #6 in All Languages, Today</title><link>https://github.com/ginuerzh/gost</link><description>&lt;p&gt;&lt;i&gt;GO Simple Tunnel - a simple tunnel written in golang&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-gost---go-simple-tunnel" class="anchor" aria-hidden="true" href="#gost---go-simple-tunnel"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;gost - GO Simple Tunnel&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-go语言实现的安全隧道" class="anchor" aria-hidden="true" href="#go语言实现的安全隧道"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GO语言实现的安全隧道&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://godoc.org/github.com/ginuerzh/gost" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/1f3e3ccd44f64950e39566e5b93e26269254dbf5/68747470733a2f2f676f646f632e6f72672f6769746875622e636f6d2f67696e7565727a682f676f73743f7374617475732e737667" alt="GoDoc" data-canonical-src="https://godoc.org/github.com/ginuerzh/gost?status.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/ginuerzh/gost" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9eb6af9deb543509958eaebc77dd559aa6f01f3c/68747470733a2f2f7472617669732d63692e6f72672f67696e7565727a682f676f73742e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/ginuerzh/gost.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://goreportcard.com/report/github.com/ginuerzh/gost" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/38033d3aaf10530c0a1d3b69232cb544a0f0187f/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f67696e7565727a682f676f7374" alt="Go Report Card" data-canonical-src="https://goreportcard.com/badge/github.com/ginuerzh/gost" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://codecov.io/gh/ginuerzh/gost/branch/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/487d35ed4036279b88394f88742f45515a58cc56/68747470733a2f2f636f6465636f762e696f2f67682f67696e7565727a682f676f73742f6272616e63682f6d61737465722f6772617068732f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/ginuerzh/gost/branch/master/graphs/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/ginuerzh/gost/releases/latest"&gt;&lt;img src="https://camo.githubusercontent.com/8286e90a83669f7e4309ab1518019669c7694d4b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f67696e7565727a682f676f73742e737667" alt="GitHub release" data-canonical-src="https://img.shields.io/github/release/ginuerzh/gost.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://build.snapcraft.io/user/ginuerzh/gost" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d3776ee2e56489119db504b50cf5b7e3b95383e4/68747470733a2f2f6275696c642e736e617063726166742e696f2f62616467652f67696e7565727a682f676f73742e737667" alt="Snap Status" data-canonical-src="https://build.snapcraft.io/badge/ginuerzh/gost.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://hub.docker.com/r/ginuerzh/gost/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cf9dfa0c62d0f15bd24636a813670f077fa8eb38/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6275696c642f67696e7565727a682f676f73742e737667" alt="Docker Build Status" data-canonical-src="https://img.shields.io/docker/build/ginuerzh/gost.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="README_en.md"&gt;English README&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-特性" class="anchor" aria-hidden="true" href="#特性"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;特性&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;多端口监听&lt;/li&gt;
&lt;li&gt;可设置转发代理，支持多级转发(代理链)&lt;/li&gt;
&lt;li&gt;支持标准HTTP/HTTPS/HTTP2/SOCKS4(A)/SOCKS5代理协议&lt;/li&gt;
&lt;li&gt;Web代理支持&lt;a href="https://docs.ginuerzh.xyz/gost/probe_resist/" rel="nofollow"&gt;探测防御&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.ginuerzh.xyz/gost/configuration/" rel="nofollow"&gt;支持多种隧道类型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.ginuerzh.xyz/gost/socks/" rel="nofollow"&gt;SOCKS5代理支持TLS协商加密&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.ginuerzh.xyz/gost/socks/" rel="nofollow"&gt;Tunnel UDP over TCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.ginuerzh.xyz/gost/redirect/" rel="nofollow"&gt;TCP透明代理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.ginuerzh.xyz/gost/port-forwarding/" rel="nofollow"&gt;本地/远程TCP/UDP端口转发&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.ginuerzh.xyz/gost/ss/" rel="nofollow"&gt;支持Shadowsocks(TCP/UDP)协议&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.ginuerzh.xyz/gost/sni/" rel="nofollow"&gt;支持SNI代理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.ginuerzh.xyz/gost/permission/" rel="nofollow"&gt;权限控制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.ginuerzh.xyz/gost/load-balancing/" rel="nofollow"&gt;负载均衡&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.ginuerzh.xyz/gost/bypass/" rel="nofollow"&gt;路由控制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.ginuerzh.xyz/gost/dns/" rel="nofollow"&gt;DNS控制&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Wiki站点: &lt;a href="https://docs.ginuerzh.xyz/gost/" rel="nofollow"&gt;https://docs.ginuerzh.xyz/gost/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Google讨论组: &lt;a href="https://groups.google.com/d/forum/go-gost" rel="nofollow"&gt;https://groups.google.com/d/forum/go-gost&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Telegram讨论群: &lt;a href="https://t.me/gogost" rel="nofollow"&gt;https://t.me/gogost&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-安装" class="anchor" aria-hidden="true" href="#安装"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;安装&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-二进制文件" class="anchor" aria-hidden="true" href="#二进制文件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;二进制文件&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/ginuerzh/gost/releases"&gt;https://github.com/ginuerzh/gost/releases&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-源码编译" class="anchor" aria-hidden="true" href="#源码编译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;源码编译&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;go get -u github.com/ginuerzh/gost/cmd/gost&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-docker" class="anchor" aria-hidden="true" href="#docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker pull ginuerzh/gost&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-ubuntu商店" class="anchor" aria-hidden="true" href="#ubuntu商店"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ubuntu商店&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo snap install gost&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-快速上手" class="anchor" aria-hidden="true" href="#快速上手"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;快速上手&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-不设置转发代理" class="anchor" aria-hidden="true" href="#不设置转发代理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;不设置转发代理&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/30855083688ca3dd1fc150de4e539025e945adb8/68747470733a2f2f67696e7565727a682e6769746875622e696f2f696d616765732f676f73745f30312e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/30855083688ca3dd1fc150de4e539025e945adb8/68747470733a2f2f67696e7565727a682e6769746875622e696f2f696d616765732f676f73745f30312e706e67" data-canonical-src="https://ginuerzh.github.io/images/gost_01.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;作为标准HTTP/SOCKS5代理&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;设置代理认证信息&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=admin:123456@localhost:8080&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;多端口监听&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=http2://:443 -L=socks5://:1080 -L=ss://aes-128-cfb:123456@:8338&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-设置转发代理" class="anchor" aria-hidden="true" href="#设置转发代理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;设置转发代理&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/08a5bd6673567539e63f163dcf2e05360dcda94b/68747470733a2f2f67696e7565727a682e6769746875622e696f2f696d616765732f676f73745f30322e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/08a5bd6673567539e63f163dcf2e05360dcda94b/68747470733a2f2f67696e7565727a682e6769746875622e696f2f696d616765732f676f73745f30322e706e67" data-canonical-src="https://ginuerzh.github.io/images/gost_02.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=192.168.1.1:8081&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;转发代理认证&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=http://admin:123456@192.168.1.1:8081&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-设置多级转发代理代理链" class="anchor" aria-hidden="true" href="#设置多级转发代理代理链"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;设置多级转发代理(代理链)&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/fa8c9555277cf27b2931a57e1ae0d5b94fffb157/68747470733a2f2f67696e7565727a682e6769746875622e696f2f696d616765732f676f73745f30332e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/fa8c9555277cf27b2931a57e1ae0d5b94fffb157/68747470733a2f2f67696e7565727a682e6769746875622e696f2f696d616765732f676f73745f30332e706e67" data-canonical-src="https://ginuerzh.github.io/images/gost_03.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=quic://192.168.1.1:6121 -F=socks5+wss://192.168.1.2:1080 -F=http2://192.168.1.3:443 ... -F=a.b.c.d:NNNN&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;gost按照-F设置的顺序通过代理链将请求最终转发给a.b.c.d:NNNN处理，每一个转发代理可以是任意HTTP/HTTPS/HTTP2/SOCKS4/SOCKS5/Shadowsocks类型代理。&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-本地端口转发tcp" class="anchor" aria-hidden="true" href="#本地端口转发tcp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;本地端口转发(TCP)&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=tcp://:2222/192.168.1.1:22 [-F&lt;span class="pl-k"&gt;=&lt;/span&gt;...]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;将本地TCP端口2222上的数据(通过代理链)转发到192.168.1.1:22上。当代理链末端(最后一个-F参数)为SSH转发通道类型时，gost会直接使用SSH的本地端口转发功能:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=tcp://:2222/192.168.1.1:22 -F forward+ssh://:2222&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-本地端口转发udp" class="anchor" aria-hidden="true" href="#本地端口转发udp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;本地端口转发(UDP)&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=udp://:5353/192.168.1.1:53&lt;span class="pl-k"&gt;?&lt;/span&gt;ttl=60 [-F&lt;span class="pl-k"&gt;=&lt;/span&gt;...]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;将本地UDP端口5353上的数据(通过代理链)转发到192.168.1.1:53上。
每条转发通道都有超时时间，当超过此时间，且在此时间段内无任何数据交互，则此通道将关闭。可以通过&lt;code&gt;ttl&lt;/code&gt;参数来设置超时时间，默认值为60秒。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注:&lt;/strong&gt; 转发UDP数据时，如果有代理链，则代理链的末端(最后一个-F参数)必须是gost SOCKS5类型代理，gost会使用UDP over TCP方式进行转发。&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-远程端口转发tcp" class="anchor" aria-hidden="true" href="#远程端口转发tcp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;远程端口转发(TCP)&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=rtcp://:2222/192.168.1.1:22 [-F&lt;span class="pl-k"&gt;=&lt;/span&gt;... -F&lt;span class="pl-k"&gt;=&lt;/span&gt;socks5://172.24.10.1:1080]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;将172.24.10.1:2222上的数据(通过代理链)转发到192.168.1.1:22上。当代理链末端(最后一个-F参数)为SSH转发通道类型时，gost会直接使用SSH的远程端口转发功能:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=rtcp://:2222/192.168.1.1:22 -F forward+ssh://:2222&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-远程端口转发udp" class="anchor" aria-hidden="true" href="#远程端口转发udp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;远程端口转发(UDP)&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=rudp://:5353/192.168.1.1:53&lt;span class="pl-k"&gt;?&lt;/span&gt;ttl=60 [-F&lt;span class="pl-k"&gt;=&lt;/span&gt;... -F&lt;span class="pl-k"&gt;=&lt;/span&gt;socks5://172.24.10.1:1080]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;将172.24.10.1:5353上的数据(通过代理链)转发到192.168.1.1:53上。
每条转发通道都有超时时间，当超过此时间，且在此时间段内无任何数据交互，则此通道将关闭。可以通过&lt;code&gt;ttl&lt;/code&gt;参数来设置超时时间，默认值为60秒。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注:&lt;/strong&gt; 转发UDP数据时，如果有代理链，则代理链的末端(最后一个-F参数)必须是GOST SOCKS5类型代理，gost会使用UDP-over-TCP方式进行转发。&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-http2" class="anchor" aria-hidden="true" href="#http2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTTP2&lt;/h4&gt;
&lt;p&gt;gost的HTTP2支持两种模式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;作为标准的HTTP2代理，并向下兼容HTTPS代理。&lt;/li&gt;
&lt;li&gt;作为通道传输其他协议。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-代理模式" class="anchor" aria-hidden="true" href="#代理模式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;代理模式&lt;/h5&gt;
&lt;p&gt;服务端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=http2://:443&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;客户端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=http2://server_ip:443&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-通道模式" class="anchor" aria-hidden="true" href="#通道模式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;通道模式&lt;/h5&gt;
&lt;p&gt;服务端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=h2://:443&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;客户端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=h2://server_ip:443&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-quic" class="anchor" aria-hidden="true" href="#quic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;QUIC&lt;/h4&gt;
&lt;p&gt;gost对QUIC的支持是基于&lt;a href="https://github.com/lucas-clemente/quic-go"&gt;quic-go&lt;/a&gt;库。&lt;/p&gt;
&lt;p&gt;服务端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=quic://:6121&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;客户端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=quic://server_ip:6121&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt; QUIC模式只能作为代理链的第一个节点。&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-kcp" class="anchor" aria-hidden="true" href="#kcp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;KCP&lt;/h4&gt;
&lt;p&gt;gost对KCP的支持是基于&lt;a href="https://github.com/xtaci/kcp-go"&gt;kcp-go&lt;/a&gt;和&lt;a href="https://github.com/xtaci/kcptun"&gt;kcptun&lt;/a&gt;库。&lt;/p&gt;
&lt;p&gt;服务端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=kcp://:8388&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;客户端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=kcp://server_ip:8388&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;gost会自动加载当前工作目录中的kcp.json(如果存在)配置文件，或者可以手动通过参数指定配置文件路径：&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=kcp://:8388&lt;span class="pl-k"&gt;?&lt;/span&gt;c=/path/to/conf/file&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt; KCP模式只能作为代理链的第一个节点。&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-ssh" class="anchor" aria-hidden="true" href="#ssh"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SSH&lt;/h4&gt;
&lt;p&gt;gost的SSH支持两种模式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;作为转发通道，配合本地/远程TCP端口转发使用。&lt;/li&gt;
&lt;li&gt;作为通道传输其他协议。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-转发模式" class="anchor" aria-hidden="true" href="#转发模式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;转发模式&lt;/h5&gt;
&lt;p&gt;服务端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=forward+ssh://:2222&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;客户端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=rtcp://:1222/:22 -F=forward+ssh://server_ip:2222&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-通道模式-1" class="anchor" aria-hidden="true" href="#通道模式-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;通道模式&lt;/h5&gt;
&lt;p&gt;服务端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=ssh://:2222&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;客户端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=ssh://server_ip:2222&lt;span class="pl-k"&gt;?&lt;/span&gt;ping=60&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;可以通过&lt;code&gt;ping&lt;/code&gt;参数设置心跳包发送周期，单位为秒。默认不发送心跳包。&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-透明代理" class="anchor" aria-hidden="true" href="#透明代理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;透明代理&lt;/h4&gt;
&lt;p&gt;基于iptables的透明代理。&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=redirect://:12345 -F=http2://server_ip:443&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-obfs4" class="anchor" aria-hidden="true" href="#obfs4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;obfs4&lt;/h4&gt;
&lt;p&gt;此功能由&lt;a href="https://github.com/isofew"&gt;@isofew&lt;/a&gt;贡献。&lt;/p&gt;
&lt;p&gt;服务端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=obfs4://:443&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;当服务端运行后会在控制台打印出连接地址供客户端使用:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;obfs4://:443/?cert=4UbQjIfjJEQHPOs8vs5sagrSXx1gfrDCGdVh2hpIPSKH0nklv1e4f29r7jb91VIrq4q5Jw&amp;amp;iat-mode=0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;客户端:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gost -L=:8888 -F='obfs4://server_ip:443?cert=4UbQjIfjJEQHPOs8vs5sagrSXx1gfrDCGdVh2hpIPSKH0nklv1e4f29r7jb91VIrq4q5Jw&amp;amp;iat-mode=0'
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-加密机制" class="anchor" aria-hidden="true" href="#加密机制"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;加密机制&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-http" class="anchor" aria-hidden="true" href="#http"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTTP&lt;/h4&gt;
&lt;p&gt;对于HTTP可以使用TLS加密整个通讯过程，即HTTPS代理：&lt;/p&gt;
&lt;p&gt;服务端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=https://:443&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;客户端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=http+tls://server_ip:443&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-http2-1" class="anchor" aria-hidden="true" href="#http2-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTTP2&lt;/h4&gt;
&lt;p&gt;gost的HTTP2代理模式仅支持使用TLS加密的HTTP2协议，不支持明文HTTP2传输。&lt;/p&gt;
&lt;p&gt;gost的HTTP2通道模式支持加密(h2)和明文(h2c)两种模式。&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-socks5" class="anchor" aria-hidden="true" href="#socks5"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SOCKS5&lt;/h4&gt;
&lt;p&gt;gost支持标准SOCKS5协议的no-auth(0x00)和user/pass(0x02)方法，并在此基础上扩展了两个：tls(0x80)和tls-auth(0x82)，用于数据加密。&lt;/p&gt;
&lt;p&gt;服务端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=socks5://:1080&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;客户端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=socks5://server_ip:1080&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果两端都是gost(如上)则数据传输会被加密(协商使用tls或tls-auth方法)，否则使用标准SOCKS5进行通讯(no-auth或user/pass方法)。&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-shadowsocks" class="anchor" aria-hidden="true" href="#shadowsocks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Shadowsocks&lt;/h4&gt;
&lt;p&gt;gost对shadowsocks的支持是基于&lt;a href="https://github.com/shadowsocks/shadowsocks-go"&gt;shadowsocks-go&lt;/a&gt;库。&lt;/p&gt;
&lt;p&gt;服务端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=ss://chacha20:123456@:8338&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;客户端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=ss://chacha20:123456@server_ip:8338&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-shadowsocks-udp-relay" class="anchor" aria-hidden="true" href="#shadowsocks-udp-relay"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Shadowsocks UDP relay&lt;/h5&gt;
&lt;p&gt;目前仅服务端支持UDP Relay。&lt;/p&gt;
&lt;p&gt;服务端:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=ssu://chacha20:123456@:8338&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-tls" class="anchor" aria-hidden="true" href="#tls"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TLS&lt;/h4&gt;
&lt;p&gt;gost内置了TLS证书，如果需要使用其他TLS证书，有两种方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在gost运行目录放置cert.pem(公钥)和key.pem(私钥)两个文件即可，gost会自动加载运行目录下的cert.pem和key.pem文件。&lt;/li&gt;
&lt;li&gt;使用参数指定证书文件路径：&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;http2://:443?cert=/path/to/my/cert/file&amp;amp;key=/path/to/my/key/file&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;对于客户端可以通过&lt;code&gt;secure&lt;/code&gt;参数开启服务器证书和域名校验:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;http2://server_domain_name:443?secure=true&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;对于客户端可以指定CA证书进行&lt;a href="https://en.wikipedia.org/wiki/Transport_Layer_Security#Certificate_pinning" rel="nofollow"&gt;证书锁定&lt;/a&gt;(Certificate Pinning):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;gost -L=:8080 -F=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;http2://:443?ca=ca.pem&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;证书锁定功能由&lt;a href="https://github.com/sheerun"&gt;@sheerun&lt;/a&gt;贡献&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ginuerzh</author><guid isPermaLink="false">https://github.com/ginuerzh/gost</guid><pubDate>Tue, 26 Nov 2019 00:06:00 GMT</pubDate></item><item><title>harismuneer/Ultimate-Facebook-Scraper #7 in All Languages, Today</title><link>https://github.com/harismuneer/Ultimate-Facebook-Scraper</link><description>&lt;p&gt;&lt;i&gt;🤖 A bot which scrapes almost everything about a Facebook user's profile including all public posts/statuses available on the user's timeline, uploaded photos, tagged photos, videos, friends list and their profile photos (including Followers, Following, Work Friends, College Friends etc).&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;a href="#"&gt;
  &lt;div align="center"&gt;
    &lt;img src="images/ufs_icon.png" width="154" style="max-width:100%;"&gt;
  &lt;/div&gt;
&lt;/a&gt;
&lt;h1 align="center"&gt;&lt;a id="user-content-ultimate-facebook-scraper-ufs" class="anchor" aria-hidden="true" href="#ultimate-facebook-scraper-ufs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ultimate Facebook Scraper (UFS)&lt;/h1&gt;
&lt;p align="center"&gt;
  Tooling that &lt;b&gt;automates&lt;/b&gt; your social media interactions to collect posts, photos, videos, friends, followers and much more on Facebook.
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://www.codacy.com/manual/harismuneer/Ultimate-Facebook-Scraper?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=harismuneer/Ultimate-Facebook-Scraper&amp;amp;utm_campaign=Badge_Grade" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/99af5d6fbf3b3412b5d86dd7a51267cd1932f568/68747470733a2f2f6170692e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3766343137393063336333613466643239323933373737633637366338363137" data-canonical-src="https://api.codacy.com/project/badge/Grade/7f41790c3c3a4fd29293777c676c8617" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/91e0907406753ea1e935b1bad0ec812126e19fe7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4275696c642d50617373696e672d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265266c6f676f3d6170707665796f72" data-canonical-src="https://img.shields.io/badge/Build-Passing-brightgreen.svg?style=flat-square&amp;amp;logo=appveyor" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/d41b9884bd102b525c8fb9a8c3c8d3bbed2b67f0/68747470733a2f2f6261646765732e66726170736f66742e636f6d2f6f732f76312f6f70656e2d736f757263652e7376673f763d313033" data-canonical-src="https://badges.frapsoft.com/os/v1/open-source.svg?v=103" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://www.github.com/harismuneer/Ultimate-Facebook-Scraper/fork"&gt;
    &lt;img src="https://camo.githubusercontent.com/e1e2221a649724fbdb4bf206770d7f7b374b6895/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f68617269736d756e6565722f556c74696d6174652d46616365626f6f6b2d536372617065722e7376673f7374796c653d736f6369616c266c6162656c3d466f726b266d61784167653d32353932303030" data-canonical-src="https://img.shields.io/github/forks/harismuneer/Ultimate-Facebook-Scraper.svg?style=social&amp;amp;label=Fork&amp;amp;maxAge=2592000" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/e2adf2e1c6a9764f7b398c9588f8d8dd010a76c3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c6174266c6162656c3d436f6e747269627574696f6e7326636f6c6f72413d72656426636f6c6f72423d626c61636b" data-canonical-src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat&amp;amp;label=Contributions&amp;amp;colorA=red&amp;amp;colorB=black" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p align="center"&gt;
  Developers from following organizations have so far joined the quest and contributed to UFS.
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/cb47d8b8488c5a9294bb8c7bee1bbb0e55cf189b/68747470733a2f2f7777772e616462697264732e676c6f62616c2f77702d636f6e74656e742f75706c6f6164732f323031362f30372f4d6963726f736f66742d4c6f676f2d7371756172652e706e67" width="150" alt="Microsoft" data-canonical-src="https://www.adbirds.global/wp-content/uploads/2016/07/Microsoft-Logo-square.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href=""&gt;&lt;img hspace="5" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/c865e9d21e8b1f793908ef76b5a39e755995afde/68747470733a2f2f6175746f706169722e6e65742f696d616765732f65617379626c6f675f61727469636c65732f352f4d49542d4c6f676f2e706e67" width="165" alt="MIT" data-canonical-src="https://autopair.net/images/easyblog_articles/5/MIT-Logo.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/7aa438fc9aab9a289b8ca3b06e05a226ee6fb4d9/68747470733a2f2f7777772e676c6f62616c66696e616e6369616c646174612e636f6d2f77702d636f6e74656e742f75706c6f6164732f636c69656e742d6c6f676f732f486172766172642d556e69766572736974792d6c6f676f2d2e6a7067" width="163" alt="Harvard" data-canonical-src="https://www.globalfinancialdata.com/wp-content/uploads/client-logos/Harvard-University-logo-.jpg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href=""&gt;&lt;img hspace="10" style="max-width:100%;"&gt;&lt;/a&gt;  
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/e9bffdd7c658416ab6e041712e603b8efd9cf033/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f656e2f652f65342f4e6174696f6e616c5f556e69766572736974795f6f665f436f6d70757465725f616e645f456d657267696e675f536369656e6365735f6c6f676f2e706e67" width="120" alt="NUCES" data-canonical-src="https://upload.wikimedia.org/wikipedia/en/e/e4/National_University_of_Computer_and_Emerging_Sciences_logo.png" style="max-width:100%;"&gt;
  &lt;/a&gt;&lt;br&gt;      
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/056cfdefc6f173df43487548359503523340575e/68747470733a2f2f7374617469632e7769787374617469632e636f6d2f6d656469612f6537303738375f63306334386434353662306434323162396330643435343738663966343232647e6d76322e676966" width="165" alt="UCLA" data-canonical-src="https://static.wixstatic.com/media/e70787_c0c48d456b0d421b9c0d45478f9f422d~mv2.gif" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href=""&gt;&lt;img hspace="20" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/4a92f8294c47c3413d19e77d40a6fecb4f913928/687474703a2f2f7777772e6765656b7765656b2e706b2f323031362f77702d636f6e74656e742f75706c6f6164732f323031362f30312f41434d2d6c6f676f2e706e67" width="280" alt="ACM" data-canonical-src="http://www.geekweek.pk/2016/wp-content/uploads/2016/01/ACM-logo.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href=""&gt;&lt;img hspace="20" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/553f90c470ce1a5a1720febffca2f06e05b7df75/68747470733a2f2f69322e77702e636f6d2f7777772e6f7066626c6f672e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031342f31302f4c756d732d4c6f676f2e6a70673f7a6f6f6d3d312e33343939393939303436333235363834266669743d3530302532433430392673736c3d31" width="120" alt="LUMS" data-canonical-src="https://i2.wp.com/www.opfblog.com/wp-content/uploads/2014/10/Lums-Logo.jpg?zoom=1.3499999046325684&amp;amp;fit=500%2C409&amp;amp;ssl=1" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;p&gt;A bot which scrapes almost everything about a facebook user's profile including&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;uploaded photos&lt;/li&gt;
&lt;li&gt;tagged photos&lt;/li&gt;
&lt;li&gt;videos&lt;/li&gt;
&lt;li&gt;friends list and their profile photos (including Followers, Following, Work Friends, College Friends etc)&lt;/li&gt;
&lt;li&gt;and all public posts/statuses available on the user's timeline.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The best thing about this scraper is that the data is scraped in an organized format so that it can be used for educational/research purpose by researchers. Moreover, this scraper does not use Facebook's Graph API so there are no rate limiting issues as such.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This tool is being used by thousands of developers weekly and we are pretty amazed at this response! Thank you guys!&lt;g-emoji class="g-emoji" alias="tada" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png"&gt;🎉&lt;/g-emoji&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For details regarding &lt;strong&gt;citing/referencing&lt;/strong&gt; this tool for your research, check the 'Citation' section below.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-note" class="anchor" aria-hidden="true" href="#note"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note&lt;/h2&gt;
&lt;p&gt;At its core, this tool uses xpaths of &lt;strong&gt;'divs'&lt;/strong&gt; to extract data from them. Since Facebook keeps on updating its site frequently and the 'divs' get changed. Consequently, we have to update the divs accordingly to correctly scrape the data.&lt;/p&gt;
&lt;p&gt;The developers of this tool have devoted a lot of time and effort in developing and most importantly maintaining this tool for quite a lot time now. &lt;strong&gt;In order to keep this amazing tool alive, we need support from you geeks.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The code is pretty intuitive and easy to understand, so you can update the relevant xpaths in the code when you feel that you have tried many profiles and the data isn't being scraped for any of them (that's a hint that Facebook has updated their site) and generate a pull request. That's quite an easy thing to do. Thanks!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sample" class="anchor" aria-hidden="true" href="#sample"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample&lt;/h2&gt;
&lt;p align="middle"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/main.png"&gt;&lt;img src="images/main.png" width="700" style="max-width:100%;"&gt;&lt;/a&gt;
 &lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-screenshot" class="anchor" aria-hidden="true" href="#screenshot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Screenshot&lt;/h2&gt;
&lt;p align="middle"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/screenshot.png"&gt;&lt;img src="images/screenshot.png" width="700" style="max-width:100%;"&gt;&lt;/a&gt;
 &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;p&gt;You will need to install latest version of &lt;a href="https://www.google.com/chrome/" rel="nofollow"&gt;Google Chrome&lt;/a&gt;. Moreover, you need to install selenium module as well using&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install selenium
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the code using Python 3. Also, the code is multi-platform and is tested on both Windows and Linux.
The tool uses latest version of &lt;a href="http://chromedriver.chromium.org/downloads" rel="nofollow"&gt;Chrome Web Driver&lt;/a&gt;. I have placed the webdriver along with the code but if that version doesn't work then replace the chrome web driver with the latest one.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-to-run" class="anchor" aria-hidden="true" href="#how-to-run"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Run&lt;/h3&gt;
&lt;p&gt;There's a file named "input.txt". You can add as many profiles as you want in the following format with each link on a new line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://www.facebook.com/andrew.ng.96
https://www.facebook.com/zuck
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make sure the link only contains the username or id number at the end and not any other stuff. Make sure its in the format mentioned above.&lt;/p&gt;
&lt;p&gt;Note: There are two modes to download Friends Profile Pics and the user's Photos: Large Size and Small Size. You can change the following variables. By default they are set to Small Sized Pics because its really quick while Large Size Mode takes time depending on the number of pictures to download&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# whether to download the full image or its thumbnail (small size)
# if small size is True then it will be very quick else if its False then it will open each photo to download it
# and it will take much more time
friends_small_size = True
photos_small_size = True
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;a href="https://zenodo.org/badge/latestdoi/145763277" rel="nofollow"&gt;
  &lt;img src="https://camo.githubusercontent.com/273879e6bcd018c853ef3a5d1a4adb87427df933/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3134353736333237372e737667" data-canonical-src="https://zenodo.org/badge/145763277.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;p&gt;If you use this tool for your research, then kindly cite it. Click the above badge for more information regarding the complete citation for this tool and diffferent citation formats like IEEE, APA etc.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-important-message" class="anchor" aria-hidden="true" href="#important-message"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Important Message&lt;/h2&gt;
&lt;p&gt;This tool is for research purposes only. Hence, the developers of this tool won't be responsible for any misuse of data collected using this tool. Used by many researchers and open source intelligence (OSINT) analysts.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;p&gt;You can get in touch with us on our LinkedIn Profiles:&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-haris-muneer" class="anchor" aria-hidden="true" href="#haris-muneer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Haris Muneer&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/harismuneer" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3c9c6ad9c5e217bfdf0540ec0d035be81117a100/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e6e6563742d68617269736d756e6565722d626c75652e7376673f6c6f676f3d6c696e6b6564696e266c6f6e6743616368653d74727565267374796c653d736f6369616c266c6162656c3d436f6e6e656374" alt="LinkedIn Link" data-canonical-src="https://img.shields.io/badge/Connect-harismuneer-blue.svg?logo=linkedin&amp;amp;longCache=true&amp;amp;style=social&amp;amp;label=Connect" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also follow my GitHub Profile to stay updated about my latest projects: &lt;a href="https://github.com/harismuneer"&gt;&lt;img src="https://camo.githubusercontent.com/f1eda7d92b24897b9917c1b16ebfc72be16e160b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e6e6563742d68617269736d756e6565722d626c75652e7376673f6c6f676f3d476974687562266c6f6e6743616368653d74727565267374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="GitHub Follow" data-canonical-src="https://img.shields.io/badge/Connect-harismuneer-blue.svg?logo=Github&amp;amp;longCache=true&amp;amp;style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-hassaan-elahi" class="anchor" aria-hidden="true" href="#hassaan-elahi"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hassaan Elahi&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/hassaan-elahi/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/016097a1448ba96812eb5abe38d730705d89f7bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e6e6563742d4861737361616e2d2d456c6168692d626c75652e7376673f6c6f676f3d6c696e6b6564696e266c6f6e6743616368653d74727565267374796c653d736f6369616c266c6162656c3d436f6e6e656374" alt="LinkedIn Link" data-canonical-src="https://img.shields.io/badge/Connect-Hassaan--Elahi-blue.svg?logo=linkedin&amp;amp;longCache=true&amp;amp;style=social&amp;amp;label=Connect" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also follow my GitHub Profile to stay updated about my latest projects:&lt;a href="https://github.com/Hassaan-Elahi"&gt;&lt;img src="https://camo.githubusercontent.com/fa3d8c58ddd3ac5746969c8f2a397de737851b0c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e6e6563742d4861737361616e2d2d456c6168692d626c75652e7376673f6c6f676f3d476974687562266c6f6e6743616368653d74727565267374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="GitHub Follow" data-canonical-src="https://img.shields.io/badge/Connect-Hassaan--Elahi-blue.svg?logo=Github&amp;amp;longCache=true&amp;amp;style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you liked the repo then kindly support it by giving it a star ⭐!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributions-welcome" class="anchor" aria-hidden="true" href="#contributions-welcome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions Welcome&lt;/h2&gt;
&lt;p&gt;&lt;a href="#"&gt;&lt;img src="https://camo.githubusercontent.com/d24f2f8414437a9491ea3145cafd373167315d50/68747470733a2f2f666f7274686562616467652e636f6d2f696d616765732f6261646765732f6275696c742d776974682d6c6f76652e737667" alt="forthebadge" data-canonical-src="https://forthebadge.com/images/badges/built-with-love.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you find any bug in the code or have any improvements in mind then feel free to generate a pull request.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-issues" class="anchor" aria-hidden="true" href="#issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Issues&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.github.com/harismuneer/Ultimate-Facebook-Scraper/issues"&gt;&lt;img src="https://camo.githubusercontent.com/b3ece733dc678aecdcf9479bb26adfeee59c963e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f68617269736d756e6565722f556c74696d6174652d46616365626f6f6b2d536372617065722e7376673f7374796c653d666c6174266c6162656c3d497373756573266d61784167653d32353932303030" alt="GitHub Issues" data-canonical-src="https://img.shields.io/github/issues/harismuneer/Ultimate-Facebook-Scraper.svg?style=flat&amp;amp;label=Issues&amp;amp;maxAge=2592000" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you face any issue, you can create a new issue in the Issues Tab and I will be glad to help you out.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="../master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/3406d0ad8f23c602988ff89dee4d145529a503ce/68747470733a2f2f696d672e736869656c64732e696f2f636f636f61706f64732f6c2f41464e6574776f726b696e672e7376673f7374796c653d7374796c65266c6162656c3d4c6963656e7365266d61784167653d32353932303030" alt="MIT" data-canonical-src="https://img.shields.io/cocoapods/l/AFNetworking.svg?style=style&amp;amp;label=License&amp;amp;maxAge=2592000" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Copyright (c) 2018-present, harismuneer, Hassaan-Elahi&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>harismuneer</author><guid isPermaLink="false">https://github.com/harismuneer/Ultimate-Facebook-Scraper</guid><pubDate>Tue, 26 Nov 2019 00:07:00 GMT</pubDate></item><item><title>qingshuisiyuan/electron-ssr-backup #8 in All Languages, Today</title><link>https://github.com/qingshuisiyuan/electron-ssr-backup</link><description>&lt;p&gt;&lt;i&gt;electron-ssr原作者删除了这个伟大的项目，故备份了下来，不继续开发,且用且珍惜&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-写在前面" class="anchor" aria-hidden="true" href="#写在前面"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;写在前面&lt;/h1&gt;
&lt;p&gt;原作者（不具名）于2019年5月15日删除了并停止开发这个伟大的项目，不管因为什么原因，我们都应该感谢原作者的付出，希望有缘再见！&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-使用时注意事项" class="anchor" aria-hidden="true" href="#使用时注意事项"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;使用时注意事项：&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;本应用内置http_proxy服务，如果您系统中或浏览器中安装有其他代理工具，服务，插件等请暂时停用或关闭服务或卸载后使用本软件。&lt;/li&gt;
&lt;li&gt;本应用使用&lt;code&gt;gsetting&lt;/code&gt;设置系统代理，所以有些Linux系统无法使用该功能，请参考&lt;a href="https://github.com/qingshuisiyuan/electron-ssr-backup/blob/master/Ubuntu.md"&gt;Ubuntu.md&lt;/a&gt;手动设置系统代理。&lt;/li&gt;
&lt;li&gt;火狐浏览器请注意在浏览器设置中更改代理方式为使用系统代理或手动设置。&lt;/li&gt;
&lt;li&gt;Chrome浏览器默认使用系统代理&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-2019-08-05更新" class="anchor" aria-hidden="true" href="#2019-08-05更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019-08-05更新&lt;/h2&gt;
&lt;p&gt;更新内容：&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;增加原作者发布的0.2.5安装包，未在release发布请访问项目文件夹&lt;a href="https://github.com/qingshuisiyuan/electron-ssr-backup/tree/master/0.2.5App"&gt;0.2.5App&lt;/a&gt;&lt;br&gt;&lt;/li&gt;
&lt;li&gt;更新并收集issues并制成文档&lt;a href="https://github.com/qingshuisiyuan/electron-ssr-backup/blob/master/issue.md"&gt;issue.md&lt;/a&gt;&lt;br&gt;&lt;/li&gt;
&lt;li&gt;说明issues作用&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-下载" class="anchor" aria-hidden="true" href="#下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载&lt;/h2&gt;
&lt;p&gt;提供0.2.6版本与0.2.5版本，建议优先使用0.2.6版本
0.2.6下载地址见&lt;a href="https://github.com/qingshuisiyuan/electron-ssr-backup/releases"&gt;Github release&lt;/a&gt;，对应的操作系统下载的文件为&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Windows &lt;code&gt;electron-ssr-setup-x.x.x.exe&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Mac &lt;code&gt;electron-ssr-x.x.x.dmg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Linux 优先建议下载&lt;code&gt;electron-ssr-x.x.x.AppImage&lt;/code&gt;，直接双击运行。如果无法使用或者想直接下载自己系统专用包请看下一条&lt;/li&gt;
&lt;li&gt;Arch或者基于Arch的系统，下载&lt;code&gt;electron-ssr-x.x.x.pacman&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;RedHat系列的系统，下载&lt;code&gt;electron-ssr-x.x.x.rpm&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Debian系列的系统，下载&lt;code&gt;electron-ssr-x.x.x.deb&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;其他Linux系统或者通用Linux系统可下载&lt;code&gt;electron-ssr-x.x.x.tar.gz&lt;/code&gt;（不会使用.tar.gz的请自行百度）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-debian系列安装与配置ubuntumd" class="anchor" aria-hidden="true" href="#debian系列安装与配置ubuntumd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Debian系列安装与配置&lt;a href="https://github.com/qingshuisiyuan/electron-ssr-backup/blob/master/Ubuntu.md"&gt;Ubuntu.md&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-收集已知问题和解决方案issuemd" class="anchor" aria-hidden="true" href="#收集已知问题和解决方案issuemd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;收集已知问题和解决方案&lt;a href="https://github.com/qingshuisiyuan/electron-ssr-backup/blob/master/issue.md"&gt;issue.md&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-issus" class="anchor" aria-hidden="true" href="#issus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Issus&lt;/h2&gt;
&lt;p&gt;在发issue前请先在issue中搜索是否有同类issue，如果有请跟帖。&lt;br&gt;
另外发Bug类issue请详细描述你的使用环境，包括但不限于操作系统、软件版本，操作步骤，报错日志等。&lt;br&gt;
欢迎在issue写出你们的经验，这对这个项目非常重要！包括但不限于如何解决错误，如何在不同的操作系统安装软件、依赖，设置代理，成功的使用软件的经验。对于正确的issue，我会收集到项目代码中。&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-shadowsocksr跨平台客户端" class="anchor" aria-hidden="true" href="#shadowsocksr跨平台客户端"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ShadowsocksR跨平台客户端&lt;/h2&gt;
&lt;p&gt;这是一个跨平台（支持Windows MacOS Linux系统）的&lt;code&gt;ShadowsocksR&lt;/code&gt;客户端桌面应用，它功能丰富，支持windows版大部分功能，更有更多人性化功能。它是开源的，它来源于开源，回馈以开源。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-功能特色" class="anchor" aria-hidden="true" href="#功能特色"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;功能特色&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;支持手动添加配置&lt;/li&gt;
&lt;li&gt;支持服务器订阅更新，复制该地址测试&lt;/li&gt;
&lt;li&gt;支持二维码扫描(请确保屏幕中只有一个有效的二维码)，扫描该二维码测试&lt;/li&gt;
&lt;li&gt;支持从剪贴板复制、从配置文件导入等方式添加配置&lt;/li&gt;
&lt;li&gt;支持复制二维码图片、复制SSR链接(右键应用内二维码，点击右键菜单中的复制)&lt;/li&gt;
&lt;li&gt;支持通过点击ss/ssr链接添加配置并打开应用(仅Mac和Windows)&lt;/li&gt;
&lt;li&gt;支持切换系统代理模式:PAC、全局、不代理&lt;/li&gt;
&lt;li&gt;内置http_proxy服务，可在选项中开启或关闭&lt;/li&gt;
&lt;li&gt;支持配置项变更&lt;/li&gt;
&lt;li&gt;更多功能尽在任务栏菜单中&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-配置文件位置" class="anchor" aria-hidden="true" href="#配置文件位置"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;配置文件位置&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Windows &lt;code&gt;C:\Users\{your username}\AppData\Roaming\electron-ssr\gui-config.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Mac &lt;code&gt;~/Library/Application Support/electron-ssr/gui-config.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Linux &lt;code&gt;~/.config/gui-config.json&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-快捷键" class="anchor" aria-hidden="true" href="#快捷键"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;快捷键&lt;/h2&gt;
&lt;p&gt;加入快捷键本来是为了解决部分Linux发行版无法显示图标导致功能无法使用而加入的，当然其它系统也是可以使用的，同时支持在设置中进行开启/关闭以及更换按键的操作。&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-全局快捷键" class="anchor" aria-hidden="true" href="#全局快捷键"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;全局快捷键&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CommandOrControl+Shift+W&lt;/code&gt; 切换主窗口显隐&lt;/li&gt;
&lt;li&gt;&lt;code&gt;未设置&lt;/code&gt; 切换系统代理&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-应用内快捷键" class="anchor" aria-hidden="true" href="#应用内快捷键"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;应用内快捷键&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CommandOrControl+Shift+B&lt;/code&gt; 切换是否显示操作菜单，仅Linux可用&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-已知bug" class="anchor" aria-hidden="true" href="#已知bug"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;已知Bug&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;部分Linux系统无法切换系统代理模式（本应用使用&lt;code&gt;gsetting&lt;/code&gt;设置系统代理，所以有些Linux系统无法使用该功能），如果你知道如何实现，欢迎发issue告知。&lt;/li&gt;
&lt;li&gt;Windows系统切换全局代理不生效&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-开发和构建" class="anchor" aria-hidden="true" href="#开发和构建"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;开发和构建&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; or npm install&lt;/span&gt;
yarn

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 开发时&lt;/span&gt;
npm run dev

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 打包构建&lt;/span&gt;
npm run build

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 单元测试&lt;/span&gt;
npm run mocha

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 代码风格检查&lt;/span&gt;
npm run lint
&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>qingshuisiyuan</author><guid isPermaLink="false">https://github.com/qingshuisiyuan/electron-ssr-backup</guid><pubDate>Tue, 26 Nov 2019 00:08:00 GMT</pubDate></item><item><title>Pierian-Data/Complete-Python-3-Bootcamp #9 in All Languages, Today</title><link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link><description>&lt;p&gt;&lt;i&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-complete-python-3-bootcamp" class="anchor" aria-hidden="true" href="#complete-python-3-bootcamp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete-Python-3-Bootcamp&lt;/h1&gt;
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;
&lt;p&gt;Get it now for 95% off with the link:
&lt;a href="https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB" rel="nofollow"&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pierian-Data</author><guid isPermaLink="false">https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</guid><pubDate>Tue, 26 Nov 2019 00:09:00 GMT</pubDate></item><item><title>sherlock-project/sherlock #10 in All Languages, Today</title><link>https://github.com/sherlock-project/sherlock</link><description>&lt;p&gt;&lt;i&gt;🔎 Find usernames across social networks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/27065646/53551960-ae4dff80-3b3a-11e9-9075-cef786c69364.png"&gt;&lt;img src="https://user-images.githubusercontent.com/27065646/53551960-ae4dff80-3b3a-11e9-9075-cef786c69364.png" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;span&gt;Hunt down social media accounts by username across &lt;a href="https://github.com/theyahya/sherlock/blob/master/sites.md"&gt;social networks&lt;/a&gt;&lt;/span&gt;
  &lt;br&gt;
  &lt;a href="https://www.python.org/downloads/" title="Python version" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/392b343efc8b7ac39cdb7fd56d19a8ca6792c12b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d2533453d5f332e362d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/python-%3E=_3.6-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="LICENSE" title="License: MIT"&gt;&lt;img src="https://camo.githubusercontent.com/311762166ef25238116d3cadd22fcb6091edab98/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667" data-canonical-src="https://img.shields.io/badge/License-MIT-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://travis-ci.com/TheYahya/sherlock/" title="Build Status" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a93b7b21f175cd07941d299809dbda32764d2cff/68747470733a2f2f7472617669732d63692e636f6d2f54686559616879612f736865726c6f636b2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.com/TheYahya/sherlock.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://twitter.com/intent/tweet?text=%F0%9F%94%8E%20Find%20usernames%20across%20social%20networks%20&amp;amp;url=https://github.com/TheYahya/sherlock&amp;amp;hashtags=hacking,%20osint,%20bugbounty,%20reconnaissance" title="Share on Tweeter" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/83d4084f7b71558e33b08844da5c773a8657e271/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c" data-canonical-src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="http://sherlock-project.github.io/" rel="nofollow"&gt;&lt;img alt="Website" src="https://camo.githubusercontent.com/adc37cc0993bd76eb6e4a6e661146251f8b663af/68747470733a2f2f696d672e736869656c64732e696f2f776562736974652d75702d646f776e2d677265656e2d7265642f687474702f736865726c6f636b2d70726f6a6563742e6769746875622e696f2f2e2e737667" data-canonical-src="https://img.shields.io/website-up-down-green-red/http/sherlock-project.github.io/..svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://microbadger.com/images/theyahya/sherlock" rel="nofollow"&gt;&lt;img alt="docker image" src="https://camo.githubusercontent.com/7369ca4d589232865f5ff69b001ba2794474a285/68747470733a2f2f696d616765732e6d6963726f6261646765722e636f6d2f6261646765732f76657273696f6e2f74686579616879612f736865726c6f636b2e737667" data-canonical-src="https://images.microbadger.com/badges/version/theyahya/sherlock.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="#demo"&gt;Demo&lt;/a&gt;
     |   
  &lt;a href="#installation"&gt;Installation&lt;/a&gt;
     |   
  &lt;a href="#usage"&gt;Usage&lt;/a&gt;
     |   
  &lt;a href="#docker-notes"&gt;Docker Notes&lt;/a&gt;
     |   
  &lt;a href="#adding-new-sites"&gt;Adding New Sites&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;a href="https://asciinema.org/a/223115" rel="nofollow"&gt;
&lt;img src="./images/sherlock_preview.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-demo" class="anchor" aria-hidden="true" href="#demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Demo&lt;/h2&gt;
&lt;p&gt;You can use this link to test Sherlock directly in your browser:
&lt;a href="https://elody.com/scenario/plan/16/" rel="nofollow"&gt;https://elody.com/scenario/plan/16/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Python 3.6 or higher is required.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; clone the repo&lt;/span&gt;
$ git clone https://github.com/sherlock-project/sherlock.git

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; change the working directory to sherlock&lt;/span&gt;
$ &lt;span class="pl-c1"&gt;cd&lt;/span&gt; sherlock

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install python3 and python3-pip if they are not installed&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install the requirements&lt;/span&gt;
$ python3 -m pip install -r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/sherlock-project/sherlock&amp;amp;tutorial=README.md" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f4397884ebe7c8fac1cfd4433c6c78455b24a4f0/68747470733a2f2f677374617469632e636f6d2f636c6f75647373682f696d616765732f6f70656e2d62746e2e706e67" alt="Open in Cloud Shell" data-canonical-src="https://gstatic.com/cloudssh/images/open-btn.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python3 sherlock.py --help
usage: sherlock.py [-h] [--version] [--verbose] [--rank]
                   [--folderoutput FOLDEROUTPUT] [--output OUTPUT] [--tor]
                   [--unique-tor] [--csv] [--site SITE_NAME]
                   [--proxy PROXY_URL] [--json JSON_FILE]
                   [--proxy_list PROXY_LIST] [--check_proxies CHECK_PROXY]
                   [--print-found]
                   USERNAMES [USERNAMES ...]

Sherlock: Find Usernames Across Social Networks (Version 0.9.4)

positional arguments:
  USERNAMES             One or more usernames to check with social networks.

optional arguments:
  -h, --help            show this &lt;span class="pl-c1"&gt;help&lt;/span&gt; message and &lt;span class="pl-c1"&gt;exit&lt;/span&gt;
  --version             Display version information and dependencies.
  --verbose, -v, -d, --debug
                        Display extra debugging information and metrics.
  --rank, -r            Present websites ordered by their Alexa.com global
                        rank &lt;span class="pl-k"&gt;in&lt;/span&gt; popularity.
  --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT
                        If using multiple usernames, the output of the results
                        will be saved at this folder.
  --output OUTPUT, -o OUTPUT
                        If using single username, the output of the result
                        will be saved at this file.
  --tor, -t             Make requests over Tor&lt;span class="pl-k"&gt;;&lt;/span&gt; increases runtime&lt;span class="pl-k"&gt;;&lt;/span&gt; requires
                        Tor to be installed and &lt;span class="pl-k"&gt;in&lt;/span&gt; system path.
  --unique-tor, -u      Make requests over Tor with new Tor circuit after each
                        request&lt;span class="pl-k"&gt;;&lt;/span&gt; increases runtime&lt;span class="pl-k"&gt;;&lt;/span&gt; requires Tor to be
                        installed and &lt;span class="pl-k"&gt;in&lt;/span&gt; system path.
  --csv                 Create Comma-Separated Values (CSV) File.
  --site SITE_NAME      Limit analysis to just the listed sites. Add multiple
                        options to specify more than one site.
  --proxy PROXY_URL, -p PROXY_URL
                        Make requests over a proxy. e.g.
                        socks5://127.0.0.1:1080
  --json JSON_FILE, -j JSON_FILE
                        Load data from a JSON file or an online, valid, JSON
                        file.
  --proxy_list PROXY_LIST, -pl PROXY_LIST
                        Make requests over a proxy randomly chosen from a list
                        generated from a .csv file.
  --check_proxies CHECK_PROXY, -cp CHECK_PROXY
                        To be used with the &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;--proxy_list&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; parameter. The
                        script will check &lt;span class="pl-k"&gt;if&lt;/span&gt; the proxies supplied &lt;span class="pl-k"&gt;in&lt;/span&gt; the .csv
                        file are working and anonymous.Put 0 &lt;span class="pl-k"&gt;for&lt;/span&gt; no limit on
                        successfully checked proxies, or another number to
                        institute a limit.
  --print-found         Do not output sites where the username was not found.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For example to search for only one user:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python3 sherlock.py user123
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To search for more than one user:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python3 sherlock.py user1 user2 user3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of the accounts found will be stored in an individual text file with the corresponding username (e.g &lt;code&gt;user123.txt&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-docker-notes" class="anchor" aria-hidden="true" href="#docker-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker Notes&lt;/h2&gt;
&lt;p&gt;If you have docker installed you can build an image and run this as a container.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build -t mysherlock-image .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the image is built, sherlock can be invoked by running the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -t mysherlock-image user123
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The optional &lt;code&gt;--rm&lt;/code&gt; flag removes the container filesystem on completion to prevent cruft build-up. See: &lt;a href="https://docs.docker.com/engine/reference/run/#clean-up---rm" rel="nofollow"&gt;https://docs.docker.com/engine/reference/run/#clean-up---rm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The optional &lt;code&gt;-t&lt;/code&gt; flag allocates a pseudo-TTY which allows colored output. See: &lt;a href="https://docs.docker.com/engine/reference/run/#foreground" rel="nofollow"&gt;https://docs.docker.com/engine/reference/run/#foreground&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It is possible to use the following command to access the saved results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -t -v "$PWD/results:/opt/sherlock/results" mysherlock-image -o /opt/sherlock/results/text.txt user123
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;-v "$PWD/results:/opt/sherlock/results"&lt;/code&gt; option tells docker to create (or use) the folder &lt;code&gt;results&lt;/code&gt; in the
present working directory and to mount it at &lt;code&gt;/opt/sherlock/results&lt;/code&gt; on the docker container.
The &lt;code&gt;-o /opt/sherlock/results/text.txt&lt;/code&gt; option tells &lt;code&gt;sherlock&lt;/code&gt; to output the result.&lt;/p&gt;
&lt;p&gt;Or you can simply use "Docker Hub" to run &lt;code&gt;sherlock&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run theyahya/sherlock user123
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-using-docker-compose" class="anchor" aria-hidden="true" href="#using-docker-compose"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using &lt;code&gt;docker-compose&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;You can also use the &lt;code&gt;docker-compose.yml&lt;/code&gt; file from the repository and use this command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker-compose run sherlok -o /opt/sherlock/results/text.txt user123
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-adding-new-sites" class="anchor" aria-hidden="true" href="#adding-new-sites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding New Sites&lt;/h2&gt;
&lt;p&gt;Please look at the Wiki entry on
&lt;a href="https://github.com/TheYahya/sherlock/wiki/Adding-Sites-To-Sherlock"&gt;adding new sites&lt;/a&gt;
to understand the issues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Sherlock is not accepting adult sites in the standard list.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h2&gt;
&lt;p&gt;If you are contributing to Sherlock, then Thank You!&lt;/p&gt;
&lt;p&gt;Before creating a pull request with new development, please run the tests
to ensure that everything is working great.  It would also be a good idea to run the tests
before starting development to distinguish problems between your
environment and the Sherlock software.&lt;/p&gt;
&lt;p&gt;The following is an example of the command line to run all the tests for
Sherlock.  This invocation hides the progress text that Sherlock normally
outputs, and instead shows the verbose output of the tests.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m unittest tests.all --buffer --verbose
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we do currently have 100% test coverage.  Unfortunately, some of
the sites that Sherlock checks are not always reliable, so it is common
to get response errors.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-stargazers-over-time" class="anchor" aria-hidden="true" href="#stargazers-over-time"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stargazers over time&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://starcharts.herokuapp.com/TheYahya/sherlock" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9a700fddd7ae13c41a23e7ebd1b5eaa40a1bb549/68747470733a2f2f737461726368617274732e6865726f6b756170702e636f6d2f54686559616879612f736865726c6f636b2e737667" alt="Stargazers over time" data-canonical-src="https://starcharts.herokuapp.com/TheYahya/sherlock.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;MIT © &lt;a href="https://theyahya.com" rel="nofollow"&gt;Yahya SayadArbabi&lt;/a&gt;&lt;br&gt;
Original Creator - &lt;a href="https://github.com/sdushantha"&gt;Siddharth Dushantha&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sherlock-project</author><guid isPermaLink="false">https://github.com/sherlock-project/sherlock</guid><pubDate>Tue, 26 Nov 2019 00:10:00 GMT</pubDate></item><item><title>deep-learning-with-pytorch/dlwpt-code #11 in All Languages, Today</title><link>https://github.com/deep-learning-with-pytorch/dlwpt-code</link><description>&lt;p&gt;&lt;i&gt;Code for the book Deep Learning with PyTorch by Eli Stevens and Luca Antiga.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-with-pytorch" class="anchor" aria-hidden="true" href="#deep-learning-with-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning with PyTorch&lt;/h1&gt;
&lt;p&gt;Code for the book Deep Learning with PyTorch by Eli Stevens and Luca Antiga, published by Manning Publications.&lt;/p&gt;
&lt;p&gt;In early access now!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>deep-learning-with-pytorch</author><guid isPermaLink="false">https://github.com/deep-learning-with-pytorch/dlwpt-code</guid><pubDate>Tue, 26 Nov 2019 00:11:00 GMT</pubDate></item><item><title>yanue/V2rayU #12 in All Languages, Today</title><link>https://github.com/yanue/V2rayU</link><description>&lt;p&gt;&lt;i&gt;V2rayU,基于v2ray核心的mac版客户端,用于科学上网,使用swift编写,支持vmess,shadowsocks,socks5等服务协议,支持订阅, 支持二维码,剪贴板导入,手动配置,二维码分享等&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-v2rayu" class="anchor" aria-hidden="true" href="#v2rayu"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;V2rayU&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/yanue/V2rayU/blob/master/V2rayU/Assets.xcassets/AppIcon.appiconset/128.png?raw=true"&gt;&lt;img src="https://github.com/yanue/V2rayU/raw/master/V2rayU/Assets.xcassets/AppIcon.appiconset/128.png?raw=true" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;V2rayU 是一款v2ray mac客户端,用于科学上网,使用swift4.2编写,基于v2ray项目,支持vmess,shadowsocks,socks5等服务协议(推荐搭建&lt;strong&gt;v2ray服务&lt;/strong&gt;,可伪装成正常网站,防封锁), 支持二维码,剪贴板导入,手动配置,二维码分享等, 支持订阅, 项目地址: &lt;a href="https://github.com/yanue/V2rayU"&gt;https://github.com/yanue/V2rayU&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-主要特性" class="anchor" aria-hidden="true" href="#主要特性"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;主要特性&lt;/h3&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;支持协议:&lt;/strong&gt; vmess:// 和 ss:// 和 ssr:// 协议,支持socks5协议&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;支持导入&lt;/strong&gt;: 支持二维码,粘贴板导入,本地文件及url导入&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;支持编辑&lt;/strong&gt;: 导入配置后可以手动更改配置信息&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;手动配置&lt;/strong&gt;: 支持在导入或未导入情况下手动配置主要参数&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分享二维码&lt;/strong&gt;: 支持v2ray及shadowsocks协议格式分享&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;主动更新&lt;/strong&gt;: 支持主动更新到最新版&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;支持模式&lt;/strong&gt;: 支持pac模式,手动代理模式,支持全局代理(有别于vpn,只是将代理信息更新到系统代理http,https,socks)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;支持4.0&lt;/strong&gt;: 支持手动切换到v2ray-core 4.0以上配置格式&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;支持订阅&lt;/strong&gt;: &lt;span&gt;支持v2ray和ss及ssr订阅&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-下载安装" class="anchor" aria-hidden="true" href="#下载安装"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载安装&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;方式一: 使用homebrew命令安装&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;  brew cask install v2rayu
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;方式二: 下载最新版安装&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/yanue/V2rayU/releases"&gt;https://github.com/yanue/V2rayU/releases&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-v2ray简介" class="anchor" aria-hidden="true" href="#v2ray简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;v2ray简介&lt;/h3&gt;
&lt;p&gt;V2Ray 是 Project V 下的一个工具。Project V 包含一系列工具，帮助你打造专属的定制网络体系。而 V2Ray 属于最核心的一个。
简单地说，V2Ray 是一个与 Shadowsocks 类似的代理软件，但比Shadowsocks更具优势&lt;/p&gt;
&lt;p&gt;V2Ray 用户手册：&lt;a href="https://www.v2ray.com" rel="nofollow"&gt;https://www.v2ray.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;V2Ray 项目地址：&lt;a href="https://github.com/v2ray/v2ray-core"&gt;https://github.com/v2ray/v2ray-core&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-功能预览" class="anchor" aria-hidden="true" href="#功能预览"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;功能预览&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;
	&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/yanue/V2rayU/blob/master/screenshot/menu.png?raw=true"&gt;&lt;img src="https://github.com/yanue/V2rayU/raw/master/screenshot/menu.png?raw=true" height="300" style="max-width:100%;"&gt;&lt;/a&gt; 
	&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/yanue/V2rayU/blob/master/screenshot/share.png?raw=true"&gt;&lt;img src="https://github.com/yanue/V2rayU/raw/master/screenshot/share.png?raw=true" height="300" style="max-width:100%;"&gt;&lt;/a&gt; 
    &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/yanue/V2rayU/blob/master/screenshot/about.png?raw=true"&gt;&lt;img src="https://github.com/yanue/V2rayU/raw/master/screenshot/about.png?raw=true" height="300" style="max-width:100%;"&gt;&lt;/a&gt; 
&lt;/p&gt;
&lt;p&gt;
	&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/yanue/V2rayU/blob/master/screenshot/import.png?raw=true"&gt;&lt;img src="https://github.com/yanue/V2rayU/raw/master/screenshot/import.png?raw=true" width="400" style="max-width:100%;"&gt;&lt;/a&gt; 
	&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/yanue/V2rayU/blob/master/screenshot/manual.png?raw=true"&gt;&lt;img src="https://github.com/yanue/V2rayU/raw/master/screenshot/manual.png?raw=true" width="400" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
	&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/yanue/V2rayU/blob/master/screenshot/general.png?raw=true"&gt;&lt;img src="https://github.com/yanue/V2rayU/raw/master/screenshot/general.png?raw=true" height="300" style="max-width:100%;"&gt;&lt;/a&gt; 
	&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/yanue/V2rayU/blob/master/screenshot/advance.png?raw=true"&gt;&lt;img src="https://github.com/yanue/V2rayU/raw/master/screenshot/advance.png?raw=true" height="300" style="max-width:100%;"&gt;&lt;/a&gt; 
&lt;/p&gt;
&lt;p&gt;
	&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/yanue/V2rayU/blob/master/screenshot/subscribe.png?raw=true"&gt;&lt;img src="https://github.com/yanue/V2rayU/raw/master/screenshot/subscribe.png?raw=true" height="300" style="max-width:100%;"&gt;&lt;/a&gt; 
	&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/yanue/V2rayU/blob/master/screenshot/pac.png?raw=true"&gt;&lt;img src="https://github.com/yanue/V2rayU/raw/master/screenshot/pac.png?raw=true" height="300" style="max-width:100%;"&gt;&lt;/a&gt; 
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-v2ray服务器搭建推荐" class="anchor" aria-hidden="true" href="#v2ray服务器搭建推荐"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;v2ray服务器搭建推荐&lt;/h3&gt;
&lt;p&gt;v2ray配置指南: &lt;a href="https://toutyrater.github.io/" rel="nofollow"&gt;https://toutyrater.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Caddy+h2脚本: &lt;a href="https://github.com/dylanbai8/V2Ray_h2-tls_Website_onekey.git"&gt;https://github.com/dylanbai8/V2Ray_h2-tls_Website_onekey.git&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;v2ray模板: &lt;a href="https://github.com/KiriKira/vTemplate"&gt;https://github.com/KiriKira/vTemplate&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-代理模式" class="anchor" aria-hidden="true" href="#代理模式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;代理模式&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;全局模式: 有别于vpn,只是将代理信息更新到系统代理http,https,socks,若需要真正全局模式, 推荐搭配使用Proxifier
rules模式: 浏览器推荐搭配使用Proxy SwitchyOmega
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-相关文件" class="anchor" aria-hidden="true" href="#相关文件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;相关文件&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;v2ray-core文件: /Applications/V2rayU.app/Contents/Resources/v2ray-core
v2ray-core启动: ~/Library/LaunchAgents/yanue.v2rayu.v2ray-core.plist
v2ray-core日志: ~/Library/Logs/V2rayU.log
当前启动服务配置: /Applications/V2rayU.app/Contents/Resources/config.json
其他服务配置信息: ~/Library/Preferences/net.yanue.V2rayU.plist


如果启动无反应可以尝试从命令行手动启动,查看原因
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;cd /Applications/V2rayU.app/Contents/Resources/
./v2ray-core/v2ray -config ./config.json
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-相关问题" class="anchor" aria-hidden="true" href="#相关问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;相关问题&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. 闪退&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;大多因为读取配置文件问题,删除以下文件重新配置即可&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt; ~/Library/Preferences/net.yanue.V2rayU.plist
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;另外, 可以通过 command + 空格 搜索 console.app , 打开后搜索 V2rayU 定位具体闪退错误日志&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 无法启动v2ray服务&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;多数情况为端口被占用,可以通过 show logs... 查看日志进行排查, 如端口被占用,请更改后重试&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;3. 正常启动却无法翻墙访问&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;确保配置是正确的,然后确认启动的模式,在到网络-&amp;gt;高级里面查看是否写入对应的代理信息(manual模式需要配合浏览器插件使用)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;4. 报错: open config.json: no such file or directory&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;请严格按照 dmg 文件,拖动到 Applications 里面试下&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-问题排查方法" class="anchor" aria-hidden="true" href="#问题排查方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;问题排查方法&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;不能使用&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;如果之前有用过,更新或更改配置导致不能使用, 请彻底卸载试下,包含上面的相关文件(推荐使用appcleaner)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start="2"&gt;
&lt;li&gt;无法启动或启动后无法翻墙:&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-a-检查配置是否正确主要是outbound和stream" class="anchor" aria-hidden="true" href="#a-检查配置是否正确主要是outbound和stream"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;a. 检查配置是否正确(主要是outbound和stream)&lt;/h5&gt;
&lt;h5&gt;&lt;a id="user-content-b-查看日志" class="anchor" aria-hidden="true" href="#b-查看日志"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;b. 查看日志&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;	v2ray自身日志: V2rayU -&amp;gt; Show logs...
	V2rayU日志: command + 空格 搜索 console.app , 打开后搜索 V2rayU 定位错误日志
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;&lt;a id="user-content-c-手动启动" class="anchor" aria-hidden="true" href="#c-手动启动"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;c. 手动启动&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;cd /Applications/V2rayU.app/Contents/Resources/
./v2ray-core/v2ray -config ./config.json
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;&lt;a id="user-content-d-查看网络配置-启动v2rayu后查看-网络---高级---代理-是否生效" class="anchor" aria-hidden="true" href="#d-查看网络配置-启动v2rayu后查看-网络---高级---代理-是否生效"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;d. 查看网络配置: 启动V2rayU后查看: 网络 -&amp;gt; 高级 -&amp;gt; 代理 是否生效&lt;/h5&gt;
&lt;h5&gt;&lt;a id="user-content-e-以上都解决不了提交issue" class="anchor" aria-hidden="true" href="#e-以上都解决不了提交issue"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;e. 以上都解决不了,提交issue&lt;/h5&gt;
&lt;h3&gt;&lt;a id="user-content-待实现功能" class="anchor" aria-hidden="true" href="#待实现功能"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;待实现功能:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;中文
路由规则配置
速度测试
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-欢迎贡献代码" class="anchor" aria-hidden="true" href="#欢迎贡献代码"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;欢迎贡献代码:&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;1. fork 然后 git clone
2. pod install
3. 下载最新版v2ray-core,如: https://github.com/v2ray/v2ray-core/releases/download/v4.8.0/v2ray-macos.zip,解压到Build目录,重命名为v2ray-core
4. 运行xcode即可
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-软件使用问题" class="anchor" aria-hidden="true" href="#软件使用问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;软件使用问题&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;1. 安装包显示文件已损坏的解决方案: sudo spctl --master-disable
2. 如果启动后代理无效,请查看日志,入口: 菜单 -&amp;gt; Show logs...
3. 有其他问题请提issue
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-感谢" class="anchor" aria-hidden="true" href="#感谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;感谢&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;参考: ShadowsocksX-NG V2RayX
logo: @小文
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;GPLv3
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>yanue</author><guid isPermaLink="false">https://github.com/yanue/V2rayU</guid><pubDate>Tue, 26 Nov 2019 00:12:00 GMT</pubDate></item><item><title>ArduPilot/ardupilot #13 in All Languages, Today</title><link>https://github.com/ArduPilot/ardupilot</link><description>&lt;p&gt;&lt;i&gt;ArduPlane, ArduCopter, ArduRover source&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ardupilot-project" class="anchor" aria-hidden="true" href="#ardupilot-project"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ArduPilot Project&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://gitter.im/ArduPilot/ardupilot?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" alt="Gitter" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/ArduPilot/ardupilot" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a0aec0ed7c485e2e05fba094f628a844b38b5873/68747470733a2f2f7472617669732d63692e6f72672f4172647550696c6f742f6172647570696c6f742e7376673f6272616e63683d6d6173746572" alt="Build Travis" data-canonical-src="https://travis-ci.org/ArduPilot/ardupilot.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://semaphoreci.com/ardupilot/ardupilot" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f7ae616c5135096dbab7cefadf6fa729e9283a25/68747470733a2f2f73656d6170686f726563692e636f6d2f6170692f76312f6172647570696c6f742f6172647570696c6f742f6272616e636865732f6d61737465722f62616467652e737667" alt="Build SemaphoreCI" data-canonical-src="https://semaphoreci.com/api/v1/ardupilot/ardupilot/branches/master/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/ardupilot-org/ardupilot/_build/latest?definitionId=1&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/318b8194f48fba351f39c1d77c0abcb94bd4f9ab/68747470733a2f2f6465762e617a7572652e636f6d2f6172647570696c6f742d6f72672f6172647570696c6f742f5f617069732f6275696c642f7374617475732f4172647550696c6f742e6172647570696c6f743f6272616e63684e616d653d6d6173746572" alt="Build Status" data-canonical-src="https://dev.azure.com/ardupilot-org/ardupilot/_apis/build/status/ArduPilot.ardupilot?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://scan.coverity.com/projects/ardupilot-ardupilot" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/25625649031fa627eacb843237d31798168f0592/68747470733a2f2f7363616e2e636f7665726974792e636f6d2f70726f6a656374732f353333312f62616467652e737667" alt="Coverity Scan Build Status" data-canonical-src="https://scan.coverity.com/projects/5331/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://autotest.ardupilot.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/597847da4455025947085db65acbad34ca6bf864/687474703a2f2f6175746f746573742e6172647570696c6f742e6f72672f6175746f746573742d62616467652e737667" alt="Autotest Status" data-canonical-src="http://autotest.ardupilot.org/autotest-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-ardupilot-project-is-made-up-of" class="anchor" aria-hidden="true" href="#the-ardupilot-project-is-made-up-of"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The ArduPilot project is made up of:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ArduCopter (or APM:Copter) : &lt;a href="https://github.com/ArduPilot/ardupilot/tree/master/ArduCopter"&gt;code&lt;/a&gt;, &lt;a href="http://ardupilot.org/copter/index.html" rel="nofollow"&gt;wiki&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ArduPlane (or APM:Plane) : &lt;a href="https://github.com/ArduPilot/ardupilot/tree/master/ArduPlane"&gt;code&lt;/a&gt;, &lt;a href="http://ardupilot.org/plane/index.html" rel="nofollow"&gt;wiki&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ArduRover (or APMrover2) : &lt;a href="https://github.com/ArduPilot/ardupilot/tree/master/APMrover2"&gt;code&lt;/a&gt;, &lt;a href="http://ardupilot.org/rover/index.html" rel="nofollow"&gt;wiki&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ArduSub (or APM:Sub) : &lt;a href="https://github.com/ArduPilot/ardupilot/tree/master/ArduSub"&gt;code&lt;/a&gt;, &lt;a href="http://ardusub.com/" rel="nofollow"&gt;wiki&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Antenna Tracker : &lt;a href="https://github.com/ArduPilot/ardupilot/tree/master/AntennaTracker"&gt;code&lt;/a&gt;, &lt;a href="http://ardupilot.org/antennatracker/index.html" rel="nofollow"&gt;wiki&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-user-support--discussion-forums" class="anchor" aria-hidden="true" href="#user-support--discussion-forums"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;User Support &amp;amp; Discussion Forums&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Support Forum: &lt;a href="http://discuss.ardupilot.org/" rel="nofollow"&gt;http://discuss.ardupilot.org/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Community Site: &lt;a href="http://ardupilot.org" rel="nofollow"&gt;http://ardupilot.org&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-developer-information" class="anchor" aria-hidden="true" href="#developer-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Developer Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Github repository: &lt;a href="https://github.com/ArduPilot/ardupilot"&gt;https://github.com/ArduPilot/ardupilot&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Main developer wiki: &lt;a href="http://dev.ardupilot.org" rel="nofollow"&gt;http://dev.ardupilot.org&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Developer discussion: &lt;a href="http://discuss.ardupilot.org" rel="nofollow"&gt;http://discuss.ardupilot.org&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Developer chat: &lt;a href="https://gitter.im/ArduPilot/ardupilot" rel="nofollow"&gt;https://gitter.im/ArduPilot/ardupilot&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-top-contributors" class="anchor" aria-hidden="true" href="#top-contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Top Contributors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ArduPilot/ardupilot/graphs/contributors"&gt;Flight code contributors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ArduPilot/ardupilot_wiki/graphs/contributors"&gt;Wiki contributors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://discuss.ardupilot.org/u?order=post_count&amp;amp;period=quarterly" rel="nofollow"&gt;Most active support forum users&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ardupilot.org/about/Partners" rel="nofollow"&gt;Partners who contribute financially&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-get-involved" class="anchor" aria-hidden="true" href="#how-to-get-involved"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How To Get Involved&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The ArduPilot project is open source and we encourage participation and code contributions: &lt;a href="http://ardupilot.org/dev/docs/contributing.html" rel="nofollow"&gt;guidelines for contributors to the ardupilot codebase&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We have an active group of Beta Testers especially for ArduCopter to help us find bugs: &lt;a href="http://dev.ardupilot.org/wiki/release-procedures" rel="nofollow"&gt;release procedures&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Desired Enhancements and Bugs can be posted to the &lt;a href="https://github.com/ArduPilot/ardupilot/issues"&gt;issues list&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Help other users with log analysis in the &lt;a href="http://discuss.ardupilot.org/" rel="nofollow"&gt;support forums&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Improve the wiki and chat with other &lt;a href="https://gitter.im/ArduPilot/ardupilot_wiki" rel="nofollow"&gt;wiki editors on Gitter&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Contact the developers on one of the &lt;a href="http://ardupilot.org/copter/docs/common-contact-us.html" rel="nofollow"&gt;communication channels&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;The ArduPilot project is licensed under the GNU General Public
License, version 3.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://dev.ardupilot.com/wiki/license-gplv3" rel="nofollow"&gt;Overview of license&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/ArduPilot/ardupilot/blob/master/COPYING.txt"&gt;Full Text&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-maintainers" class="anchor" aria-hidden="true" href="#maintainers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Maintainers&lt;/h2&gt;
&lt;p&gt;Ardupilot is comprised of several parts, vehicles and boards. The list below
contains the people that regularly contribute to the project and are responsible
for reviewing patches on their specific area.  See also the list of developers with &lt;a href="https://github.com/orgs/ArduPilot/teams/ardupilot-maintainers/members"&gt;merge rights&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tridge"&gt;Andrew Tridgell&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Vehicle&lt;/strong&gt;&lt;/em&gt;: Plane, AntennaTracker&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Board&lt;/strong&gt;&lt;/em&gt;: APM1, APM2, Pixhawk, Pixhawk2, PixRacer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/oxinarf"&gt;Francisco Ferreira&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Bug Master&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/gmorph"&gt;Grant Morphett&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Vehicle&lt;/strong&gt;&lt;/em&gt;: Rover&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jaxxzer"&gt;Jacob Walser&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Vehicle&lt;/strong&gt;&lt;/em&gt;: Sub&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lucasdemarchi"&gt;Lucas De Marchi&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: Linux&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WickedShell"&gt;Michael du Breuil&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: Batteries&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: GPS&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: Scripting&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/peterbarker"&gt;Peter Barker&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: DataFlash, Tools&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rmackay9"&gt;Randy Mackay&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Vehicle&lt;/strong&gt;&lt;/em&gt;: Copter, Rover, AntennaTracker&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/magicrub"&gt;Tom Pittenger&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Vehicle&lt;/strong&gt;&lt;/em&gt;: Plane&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bnsgeyer"&gt;Bill Geyer&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Vehicle&lt;/strong&gt;&lt;/em&gt;: TradHeli&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ChristopherOlson"&gt;Chris Olson&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Vehicle&lt;/strong&gt;&lt;/em&gt;: TradHeli&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/emilecastelnuovo"&gt;Emile Castelnuovo&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Board&lt;/strong&gt;&lt;/em&gt;: VRBrain&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/EShamaev"&gt;Eugene Shamaev&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: CAN bus&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: UAVCAN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/staroselskii"&gt;Georgii Staroselskii&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Board&lt;/strong&gt;&lt;/em&gt;: NavIO&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/guludo"&gt;Gustavo José de Sousa&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: Build system&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jberaud"&gt;Julien Beraud&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Board&lt;/strong&gt;&lt;/em&gt;: Bebop &amp;amp; Bebop 2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lthall"&gt;Leonard Hall&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: Copter attitude control and navigation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Pedals2Paddles"&gt;Matt Lawrence&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Vehicle&lt;/strong&gt;&lt;/em&gt;: 3DR Solo &amp;amp; Solo based vehicles&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/badzz"&gt;Matthias Badaire&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: FRSky&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mirkix"&gt;Mirko Denecke&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Board&lt;/strong&gt;&lt;/em&gt;: BBBmini, BeagleBone Blue, PocketPilot&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/priseborough"&gt;Paul Riseborough&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: AP_NavEKF2&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: AP_NavEKF3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/khancyr"&gt;Pierre Kancir&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: Copter SITL, Rover SITL&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vmayoral"&gt;Víctor Mayoral Vilches&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Board&lt;/strong&gt;&lt;/em&gt;: PXF, Erle-Brain 2, PXFmini&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/amilcarlucas"&gt;Amilcar Lucas&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Subsystem&lt;/strong&gt;&lt;/em&gt;: Marvelmind&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ArduPilot</author><guid isPermaLink="false">https://github.com/ArduPilot/ardupilot</guid><pubDate>Tue, 26 Nov 2019 00:13:00 GMT</pubDate></item><item><title>scikit-learn/scikit-learn #14 in All Languages, Today</title><link>https://github.com/scikit-learn/scikit-learn</link><description>&lt;p&gt;&lt;i&gt;scikit-learn: machine learning in Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img alt="Azure" src="https://camo.githubusercontent.com/bfe67a3604768c16e941f3331709bf55507a4b57/68747470733a2f2f6465762e617a7572652e636f6d2f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f5f617069732f6275696c642f7374617475732f7363696b69742d6c6561726e2e7363696b69742d6c6561726e3f6272616e63684e616d653d6d6173746572" data-canonical-src="https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://travis-ci.org/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="Travis" src="https://camo.githubusercontent.com/590475799489c962f111c9fc5c1432ecbc577578/68747470733a2f2f6170692e7472617669732d63692e6f72672f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://api.travis-ci.org/scikit-learn/scikit-learn.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/github/scikit-learn/scikit-learn?branch=master" rel="nofollow"&gt;&lt;img alt="Codecov" src="https://camo.githubusercontent.com/58a0b06906ca5d106ec090fe8a1ac85a092b81c2/68747470733a2f2f636f6465636f762e696f2f6769746875622f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" data-canonical-src="https://codecov.io/github/scikit-learn/scikit-learn/badge.svg?branch=master&amp;amp;service=github" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="CircleCI" src="https://camo.githubusercontent.com/d2913194913f85128f908483a265e64dcd6d31e4/68747470733a2f2f636972636c6563692e636f6d2f67682f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f747265652f6d61737465722e7376673f7374796c653d736869656c6426636972636c652d746f6b656e3d3a636972636c652d746f6b656e" data-canonical-src="https://circleci.com/gh/scikit-learn/scikit-learn/tree/master.svg?style=shield&amp;amp;circle-token=:circle-token" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://img.shields.io/pypi/pyversions/scikit-learn.svg" rel="nofollow"&gt;&lt;img alt="PythonVersion" src="https://camo.githubusercontent.com/45416807fdec5b0d83acca16b2b9f08fe7d32bf1/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7363696b69742d6c6561726e2e737667" data-canonical-src="https://img.shields.io/pypi/pyversions/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://badge.fury.io/py/scikit-learn" rel="nofollow"&gt;&lt;img alt="PyPi" src="https://camo.githubusercontent.com/9f0ed32d05350afa18a801573e4da7f4a240e181/68747470733a2f2f62616467652e667572792e696f2f70792f7363696b69742d6c6561726e2e737667" data-canonical-src="https://badge.fury.io/py/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="DOI" src="https://camo.githubusercontent.com/73c63e44b8bee62df142664048c58f83ec8ad95c/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f32313336392f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e737667" data-canonical-src="https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-scikit-learn"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-scikit-learn" class="anchor" aria-hidden="true" href="#scikit-learn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;scikit-learn&lt;/h2&gt;
&lt;p&gt;scikit-learn is a Python module for machine learning built on top of
SciPy and is distributed under the 3-Clause BSD license.&lt;/p&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the &lt;a href="http://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;It is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-installation"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;a name="user-content-dependencies"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h4&gt;
&lt;p&gt;scikit-learn requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python (&amp;gt;= 3.5)&lt;/li&gt;
&lt;li&gt;NumPy (&amp;gt;= 1.11.0)&lt;/li&gt;
&lt;li&gt;SciPy (&amp;gt;= 0.17.0)&lt;/li&gt;
&lt;li&gt;joblib (&amp;gt;= 0.11)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.&lt;/strong&gt;
scikit-learn 0.21 and later require Python 3.5 or newer.&lt;/p&gt;
&lt;p&gt;Scikit-learn plotting capabilities (i.e., functions start with &lt;code&gt;plot_&lt;/code&gt;
and classes end with "Display") require Matplotlib (&amp;gt;= 1.5.1). For running the
examples Matplotlib &amp;gt;= 1.5.1 is required. A few examples require
scikit-image &amp;gt;= 0.12.3, a few examples require pandas &amp;gt;= 0.18.0.&lt;/p&gt;
&lt;a name="user-content-user-installation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-user-installation" class="anchor" aria-hidden="true" href="#user-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;User installation&lt;/h4&gt;
&lt;p&gt;If you already have a working installation of numpy and scipy,
the easiest way to install scikit-learn is using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;pip install -U scikit-learn
&lt;/pre&gt;
&lt;p&gt;or &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;conda install scikit-learn
&lt;/pre&gt;
&lt;p&gt;The documentation includes more detailed &lt;a href="http://scikit-learn.org/stable/install.html" rel="nofollow"&gt;installation instructions&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-changelog"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h3&gt;
&lt;p&gt;See the &lt;a href="http://scikit-learn.org/dev/whats_new.html" rel="nofollow"&gt;changelog&lt;/a&gt;
for a history of notable changes to scikit-learn.&lt;/p&gt;
&lt;a name="user-content-development"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h3&gt;
&lt;p&gt;We welcome new contributors of all experience levels. The scikit-learn
community goals are to be helpful, welcoming, and effective. The
&lt;a href="http://scikit-learn.org/stable/developers/index.html" rel="nofollow"&gt;Development Guide&lt;/a&gt;
has detailed information about contributing code, documentation, tests, and
more. We've included some basic information in this README.&lt;/p&gt;
&lt;a name="user-content-important-links"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-important-links" class="anchor" aria-hidden="true" href="#important-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Important links&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Official source code repo: &lt;a href="https://github.com/scikit-learn/scikit-learn"&gt;https://github.com/scikit-learn/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Download releases: &lt;a href="https://pypi.org/project/scikit-learn/" rel="nofollow"&gt;https://pypi.org/project/scikit-learn/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Issue tracker: &lt;a href="https://github.com/scikit-learn/scikit-learn/issues"&gt;https://github.com/scikit-learn/scikit-learn/issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-source-code"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-source-code" class="anchor" aria-hidden="true" href="#source-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source code&lt;/h4&gt;
&lt;p&gt;You can check the latest sources with the command:&lt;/p&gt;
&lt;pre&gt;git clone https://github.com/scikit-learn/scikit-learn.git
&lt;/pre&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h4&gt;
&lt;p&gt;To learn more about making a contribution to scikit-learn, please see our
&lt;a href="https://scikit-learn.org/dev/developers/contributing.html" rel="nofollow"&gt;Contributing guide&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-testing"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h4&gt;
&lt;p&gt;After installation, you can launch the test suite from outside the
source directory (you will need to have &lt;code&gt;pytest&lt;/code&gt; &amp;gt;= 3.3.0 installed):&lt;/p&gt;
&lt;pre&gt;pytest sklearn
&lt;/pre&gt;
&lt;p&gt;See the web page &lt;a href="http://scikit-learn.org/dev/developers/advanced_installation.html#testing" rel="nofollow"&gt;http://scikit-learn.org/dev/developers/advanced_installation.html#testing&lt;/a&gt;
for more information.&lt;/p&gt;
&lt;blockquote&gt;
Random number generation can be controlled during testing by setting
the &lt;code&gt;SKLEARN_SEED&lt;/code&gt; environment variable.&lt;/blockquote&gt;
&lt;a name="user-content-submitting-a-pull-request"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-submitting-a-pull-request" class="anchor" aria-hidden="true" href="#submitting-a-pull-request"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Submitting a Pull Request&lt;/h4&gt;
&lt;p&gt;Before opening a Pull Request, have a look at the
full Contributing page to make sure your code complies
with our guidelines: &lt;a href="http://scikit-learn.org/stable/developers/index.html" rel="nofollow"&gt;http://scikit-learn.org/stable/developers/index.html&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-project-history"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-project-history" class="anchor" aria-hidden="true" href="#project-history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Project History&lt;/h3&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the &lt;a href="http://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;The project is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: scikit-learn was previously referred to as scikits.learn.&lt;/p&gt;
&lt;a name="user-content-help-and-support"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-help-and-support" class="anchor" aria-hidden="true" href="#help-and-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Help and Support&lt;/h3&gt;
&lt;a name="user-content-documentation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;HTML documentation (stable release): &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HTML documentation (development version): &lt;a href="http://scikit-learn.org/dev/" rel="nofollow"&gt;http://scikit-learn.org/dev/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FAQ: &lt;a href="http://scikit-learn.org/stable/faq.html" rel="nofollow"&gt;http://scikit-learn.org/stable/faq.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-communication"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-communication" class="anchor" aria-hidden="true" href="#communication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Communication&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mailing list: &lt;a href="https://mail.python.org/mailman/listinfo/scikit-learn" rel="nofollow"&gt;https://mail.python.org/mailman/listinfo/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IRC channel: &lt;code&gt;#scikit-learn&lt;/code&gt; at &lt;code&gt;webchat.freenode.net&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Stack Overflow: &lt;a href="https://stackoverflow.com/questions/tagged/scikit-learn" rel="nofollow"&gt;https://stackoverflow.com/questions/tagged/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Website: &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-citation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h4&gt;
&lt;p&gt;If you use scikit-learn in a scientific publication, we would appreciate citations: &lt;a href="http://scikit-learn.org/stable/about.html#citing-scikit-learn" rel="nofollow"&gt;http://scikit-learn.org/stable/about.html#citing-scikit-learn&lt;/a&gt;&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>scikit-learn</author><guid isPermaLink="false">https://github.com/scikit-learn/scikit-learn</guid><pubDate>Tue, 26 Nov 2019 00:14:00 GMT</pubDate></item><item><title>sveltejs/svelte #15 in All Languages, Today</title><link>https://github.com/sveltejs/svelte</link><description>&lt;p&gt;&lt;i&gt;Cybernetically enhanced web apps&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;
  &lt;a href="https://svelte.dev" rel="nofollow"&gt;
	&lt;img alt="Cybernetically enhanced web apps: Svelte" src="https://camo.githubusercontent.com/56585f80ea08a4198ab8e226daeca2cfb651bda4/68747470733a2f2f7376656c74656a732e6769746875622e696f2f6173736574732f62616e6e65722e706e67" data-canonical-src="https://sveltejs.github.io/assets/banner.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://www.npmjs.com/package/svelte" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/ea4aab84a7d36a129f9c12ce531ca6b254d29a6c/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f7376656c74652e737667" alt="npm version" data-canonical-src="https://img.shields.io/npm/v/svelte.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://packagephobia.now.sh/result?p=svelte" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/92751cfd4e7f46505900e7b4182ef302e463c890/68747470733a2f2f7061636b61676570686f6269612e6e6f772e73682f62616467653f703d7376656c7465" alt="install size" data-canonical-src="https://packagephobia.now.sh/badge?p=svelte" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://travis-ci.org/sveltejs/svelte" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/d742e0d70b1b5b88925b863a12462c7f44a0ca67/68747470733a2f2f6170692e7472617669732d63692e6f72672f7376656c74656a732f7376656c74652e7376673f6272616e63683d6d6173746572" alt="build status" data-canonical-src="https://api.travis-ci.org/sveltejs/svelte.svg?branch=master" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/sveltejs/svelte/blob/master/LICENSE"&gt;
    &lt;img src="https://camo.githubusercontent.com/300204e5f64f8788a276fd4d4baf8b006632ac8a/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f6c2f7376656c74652e737667" alt="license" data-canonical-src="https://img.shields.io/npm/l/svelte.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-svelte" class="anchor" aria-hidden="true" href="#what-is-svelte"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is Svelte?&lt;/h2&gt;
&lt;p&gt;Svelte is a new way to build web applications. It's a compiler that takes your declarative components and converts them into efficient JavaScript that surgically updates the DOM.&lt;/p&gt;
&lt;p&gt;Learn more at the &lt;a href="https://svelte.dev" rel="nofollow"&gt;Svelte website&lt;/a&gt;, or stop by the &lt;a href="https://svelte.dev/chat" rel="nofollow"&gt;Discord chatroom&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h2&gt;
&lt;p&gt;Pull requests are encouraged and always welcome. &lt;a href="https://github.com/sveltejs/svelte/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc"&gt;Pick an issue&lt;/a&gt; and help us out!&lt;/p&gt;
&lt;p&gt;To install and work on Svelte locally:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/sveltejs/svelte.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; svelte
npm install&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Many tests depend on newlines being preserved as &lt;code&gt;&amp;lt;LF&amp;gt;&lt;/code&gt;. On Windows, you can ensure this by cloning with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git -c core.autocrlf=false clone https://github.com/sveltejs/svelte.git&lt;/pre&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;p&gt;To build the compiler, and all the other modules included in the package:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;npm run build&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To watch for changes and continually rebuild the package (this is useful if you're using &lt;a href="https://docs.npmjs.com/cli/link.html" rel="nofollow"&gt;npm link&lt;/a&gt; to test out changes in a project locally):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;npm run dev&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The compiler is written in &lt;a href="https://www.typescriptlang.org/" rel="nofollow"&gt;TypeScript&lt;/a&gt;, but don't let that put you off — it's basically just JavaScript with type annotations. You'll pick it up in no time. If you're using an editor other than &lt;a href="https://code.visualstudio.com/" rel="nofollow"&gt;Visual Studio Code&lt;/a&gt; you may need to install a plugin in order to get syntax highlighting and code hints etc.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-running-tests" class="anchor" aria-hidden="true" href="#running-tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Tests&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;npm run &lt;span class="pl-c1"&gt;test&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To filter tests, use &lt;code&gt;-g&lt;/code&gt; (aka &lt;code&gt;--grep&lt;/code&gt;). For example, to only run tests involving transitions:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;npm run &lt;span class="pl-c1"&gt;test&lt;/span&gt; -- -g transition&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-sveltedev" class="anchor" aria-hidden="true" href="#sveltedev"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;svelte.dev&lt;/h2&gt;
&lt;p&gt;The source code for &lt;a href="https://svelte.dev" rel="nofollow"&gt;https://svelte.dev&lt;/a&gt;, including all the documentation, lives in the &lt;a href="site"&gt;site&lt;/a&gt; directory. The site is built with &lt;a href="https://sapper.svelte.dev" rel="nofollow"&gt;Sapper&lt;/a&gt;. To develop locally:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; site
npm install &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; npm run update
npm run dev&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;MIT&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sveltejs</author><guid isPermaLink="false">https://github.com/sveltejs/svelte</guid><pubDate>Tue, 26 Nov 2019 00:15:00 GMT</pubDate></item><item><title>brohrer/academic_advisory #16 in All Languages, Today</title><link>https://github.com/brohrer/academic_advisory</link><description>&lt;p&gt;&lt;i&gt;Collected opinions and advice for academic programs focused on data science skills.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-industry-recommendations-for-academic-data-science-programs" class="anchor" aria-hidden="true" href="#industry-recommendations-for-academic-data-science-programs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Industry recommendations for academic data science programs&lt;/h1&gt;
&lt;p&gt;&lt;a href="authors.md"&gt;List of authors&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As industry data scientists, we are grateful for the growing number of academic programs in the field. It is a challenge to find enough candidates to fill out our teams and solve the problems we face. We welcome new entrants to the field, whether it is their first career or a second, and would like to help them get the best preparation possible.&lt;/p&gt;
&lt;p&gt;Several of us have been invited to participate on the advisory boards of academic data science programs, and we want to share our collected insights as a resource to such programs anywhere.
Two of the most common questions are indirectly about how to best prepare students: &lt;a href="what_DS_do.md"&gt;&lt;strong&gt;What precisely do industry data scientists do?&lt;/strong&gt;&lt;/a&gt; and &lt;a href="strong_DS_skills.md"&gt;&lt;strong&gt;What makes someone a good data scientist?&lt;/strong&gt;&lt;/a&gt;
We've attempted to answer both of these in a way that can inform how you construct your degree program and your course material.
We address the third most common question as well: &lt;a href="partnering.md"&gt;&lt;strong&gt;How can we partner with companies?&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-a-work-in-progress" class="anchor" aria-hidden="true" href="#a-work-in-progress"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A work in progress&lt;/h2&gt;
&lt;p&gt;It is the authors' hope that this serves as a summary of an ongoing conversation.
If you are part of an academic data science program and have other questions you would like to see answered here,
reach out to Brandon Rohrer (&lt;a href="mailto:brohrer@fb.com"&gt;brohrer@fb.com&lt;/a&gt;) or one of the other authors.
If you work as a data scientist and would like to contribute, &lt;a href="author_q_and_a.md"&gt;we welcome your insights&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>brohrer</author><guid isPermaLink="false">https://github.com/brohrer/academic_advisory</guid><pubDate>Tue, 26 Nov 2019 00:16:00 GMT</pubDate></item><item><title>neoclide/coc.nvim #17 in All Languages, Today</title><link>https://github.com/neoclide/coc.nvim</link><description>&lt;p&gt;&lt;i&gt;Intellisense engine for vim8 &amp; neovim, full language server protocol support as VSCode &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="Readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a href="https://www.vim.org/scripts/script.php?script_id=5779" rel="nofollow"&gt;
    &lt;img alt="Coc Logo" src="https://user-images.githubusercontent.com/251450/55009068-f4ed2780-501c-11e9-9a3b-cf3aa6ab9272.png" height="160" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/p&gt;&lt;p align="center"&gt;Make your Vim/Neovim as smart as VSCode.&lt;/p&gt;
  &lt;p align="center"&gt;
    &lt;a href="/LICENSE.md"&gt;&lt;img alt="Software License" src="https://camo.githubusercontent.com/f8dc8b0dba0c01c69878a1b06bf2167d2bf9a5dd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://salt.bountysource.com/teams/coc-nvim" rel="nofollow"&gt;&lt;img alt="Bountysource" src="https://camo.githubusercontent.com/eed014a522777546dbe1da690220e279e63c46e9/68747470733a2f2f696d672e736869656c64732e696f2f626f756e7479736f757263652f7465616d2f636f632d6e76696d2f61637469766974792e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/bountysource/team/coc-nvim/activity.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://travis-ci.org/neoclide/coc.nvim" rel="nofollow"&gt;&lt;img alt="Travis" src="https://camo.githubusercontent.com/098afdb11bd8e3b1a8293a28c33c3a9822012029/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f6e656f636c6964652f636f632e6e76696d2f6d61737465722e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/travis/neoclide/coc.nvim/master.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://codecov.io/gh/neoclide/coc.nvim" rel="nofollow"&gt;&lt;img alt="Coverage" src="https://camo.githubusercontent.com/2e42c35ba406ee8c3afed276325deef3207252f8/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f6e656f636c6964652f636f632e6e76696d2e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/codecov/c/github/neoclide/coc.nvim.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="/doc/coc.txt"&gt;&lt;img alt="Doc" src="https://camo.githubusercontent.com/4bf63c84437d5aa9537d325f3db79a6db7fc6fc0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f632d25334168253230636f632e7478742d7265642e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/doc-%3Ah%20coc.txt-red.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://gitter.im/neoclide/coc.nvim" rel="nofollow"&gt;&lt;img alt="Gitter" src="https://camo.githubusercontent.com/4e97ce88288423bab2e902f525ba1154c70b545b/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6e656f636c6964652f636f632e6e76696d2e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/gitter/room/neoclide/coc.nvim.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Coc is an intellisense engine for Vim/Neovim.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/251450/55285193-400a9000-53b9-11e9-8cff-ffe4983c5947.gif"&gt;&lt;img alt="Gif" src="https://user-images.githubusercontent.com/251450/55285193-400a9000-53b9-11e9-8cff-ffe4983c5947.gif" width="60%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;True snippet and additional text editing support&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Check out &lt;a href="https://github.com/neoclide/coc.nvim/wiki"&gt;Wiki&lt;/a&gt;, or &lt;a href="doc/coc.txt"&gt;doc/coc.txt&lt;/a&gt; for the vim interface.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-广告" class="anchor" aria-hidden="true" href="#广告"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;广告&lt;/h2&gt;
&lt;p&gt;深圳全民点游有限公司急需小程序相关前端开发，有意者加我微信：chemzqm&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;Install &lt;a href="https://nodejs.org/en/download/" rel="nofollow"&gt;nodejs&lt;/a&gt; when necessary:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;curl -sL install-node.now.sh/lts &lt;span class="pl-k"&gt;|&lt;/span&gt; bash&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For &lt;a href="https://github.com/junegunn/vim-plug"&gt;vim-plug&lt;/a&gt; users:&lt;/p&gt;
&lt;div class="highlight highlight-source-viml"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Use release branch (Recommend)&lt;/span&gt;
Plug &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;neoclide/coc.nvim&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;branch&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;release&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;}

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Or latest tag&lt;/span&gt;
Plug &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;neoclide/coc.nvim&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tag&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;*&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;branch&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;release&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;}
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Or build from source code by use yarn: https://yarnpkg.com&lt;/span&gt;
Plug &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;neoclide/coc.nvim&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;do&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;yarn install --frozen-lockfile&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;in your &lt;code&gt;.vimrc&lt;/code&gt; or &lt;code&gt;init.vim&lt;/code&gt;, then restart vim and run &lt;code&gt;:PlugInstall&lt;/code&gt;. Checkout &lt;a href="https://github.com/neoclide/coc.nvim/wiki/Install-coc.nvim"&gt;Install coc.nvim&lt;/a&gt; wiki for more info.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The first time building from source code may be slow.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-example-vim-configuration" class="anchor" aria-hidden="true" href="#example-vim-configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example vim configuration&lt;/h2&gt;
&lt;p&gt;Configuration is required to make coc.nvim easier to work with, since it doesn't
change your key-mappings or vim options. This is done as much as possible to avoid conflict with your
other plugins.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;g-emoji class="g-emoji" alias="exclamation" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2757.png"&gt;❗️&lt;/g-emoji&gt;Important&lt;/strong&gt;: some vim plugins could change keymappings. Use a command like
&lt;code&gt;:verbose imap &amp;lt;tab&amp;gt;&lt;/code&gt; to make sure that your keymap has taken effect.&lt;/p&gt;
&lt;div class="highlight highlight-source-viml"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; if hidden is not set, TextEdit might fail.&lt;/span&gt;
&lt;span class="pl-c1"&gt;set&lt;/span&gt; &lt;span class="pl-c1"&gt;hidden&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Some servers have issues with backup files, see #649&lt;/span&gt;
&lt;span class="pl-c1"&gt;set&lt;/span&gt; &lt;span class="pl-c1"&gt;nobackup&lt;/span&gt;
&lt;span class="pl-c1"&gt;set&lt;/span&gt; &lt;span class="pl-c1"&gt;nowritebackup&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Better display for messages&lt;/span&gt;
&lt;span class="pl-c1"&gt;set&lt;/span&gt; &lt;span class="pl-c1"&gt;cmdheight&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; You will have bad experience for diagnostic messages when it's default 4000.&lt;/span&gt;
&lt;span class="pl-c1"&gt;set&lt;/span&gt; &lt;span class="pl-c1"&gt;updatetime&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;300&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; don't give |ins-completion-menu| messages.&lt;/span&gt;
&lt;span class="pl-c1"&gt;set&lt;/span&gt; &lt;span class="pl-c1"&gt;shortmess&lt;/span&gt;&lt;span class="pl-k"&gt;+=&lt;/span&gt;&lt;span class="pl-c1"&gt;c&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; always show signcolumns&lt;/span&gt;
&lt;span class="pl-c1"&gt;set&lt;/span&gt; &lt;span class="pl-c1"&gt;signcolumn&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;yes

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Use tab for trigger completion with characters ahead and navigate.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Use command ':verbose imap &amp;lt;tab&amp;gt;' to make sure tab is not mapped by other plugin.&lt;/span&gt;
&lt;span class="pl-c1"&gt;inoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&amp;lt;expr&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;TAB&amp;gt;&lt;/span&gt;
      &lt;span class="pl-cce"&gt;\ &lt;/span&gt;&lt;span class="pl-en"&gt;pumvisible&lt;/span&gt;() ? &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-cce"&gt;\&amp;lt;&lt;/span&gt;C-n&amp;gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; :
      &lt;span class="pl-cce"&gt;\ &lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;SID&amp;gt;&lt;/span&gt;&lt;span class="pl-en"&gt;check_back_space&lt;/span&gt;() ? &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-cce"&gt;\&amp;lt;&lt;/span&gt;TAB&amp;gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; :
      &lt;span class="pl-cce"&gt;\ &lt;/span&gt;&lt;span class="pl-en"&gt;coc#refresh&lt;/span&gt;()
&lt;span class="pl-c1"&gt;inoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;expr&amp;gt;&amp;lt;S-TAB&amp;gt;&lt;/span&gt; &lt;span class="pl-en"&gt;pumvisible&lt;/span&gt;() ? &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-cce"&gt;\&amp;lt;&lt;/span&gt;C-p&amp;gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; : &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-cce"&gt;\&amp;lt;&lt;/span&gt;C-h&amp;gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-k"&gt;function&lt;/span&gt;&lt;span class="pl-k"&gt;!&lt;/span&gt; &lt;span class="pl-en"&gt;&lt;span class="pl-k"&gt;s:&lt;/span&gt;check_back_space&lt;/span&gt;() &lt;span class="pl-k"&gt;abort&lt;/span&gt;
  &lt;span class="pl-k"&gt;let&lt;/span&gt; &lt;span class="pl-c1"&gt;col&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;col&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;.&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;-&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;
  &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-k"&gt;!&lt;/span&gt;&lt;span class="pl-c1"&gt;col&lt;/span&gt; &lt;span class="pl-k"&gt;||&lt;/span&gt; &lt;span class="pl-en"&gt;getline&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;.&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)[&lt;span class="pl-c1"&gt;col&lt;/span&gt; &lt;span class="pl-k"&gt;-&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;]  &lt;span class="pl-k"&gt;=&lt;/span&gt;~# &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;\s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-k"&gt;endfunction&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Use &amp;lt;c-space&amp;gt; to trigger completion.&lt;/span&gt;
&lt;span class="pl-c1"&gt;inoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&amp;lt;expr&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;c-space&amp;gt;&lt;/span&gt; &lt;span class="pl-en"&gt;coc#refresh&lt;/span&gt;()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Use &amp;lt;cr&amp;gt; to confirm completion, `&amp;lt;C-g&amp;gt;u` means break undo chain at current position.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Coc only does snippet and additional edit on confirm.&lt;/span&gt;
&lt;span class="pl-c1"&gt;inoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;expr&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;cr&amp;gt;&lt;/span&gt; &lt;span class="pl-en"&gt;pumvisible&lt;/span&gt;() ? &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-cce"&gt;\&amp;lt;&lt;/span&gt;C-y&amp;gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; : &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-cce"&gt;\&amp;lt;&lt;/span&gt;C-g&amp;gt;u&lt;span class="pl-cce"&gt;\&amp;lt;&lt;/span&gt;CR&amp;gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Or use `complete_info` if your vim support it, like:&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; inoremap &amp;lt;expr&amp;gt; &amp;lt;cr&amp;gt; complete_info()["selected"] != "-1" ? "\&amp;lt;C-y&amp;gt;" : "\&amp;lt;C-g&amp;gt;u\&amp;lt;CR&amp;gt;"&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Use `[g` and `]g` to navigate diagnostics&lt;/span&gt;
&lt;span class="pl-c1"&gt;nmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;g&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;diagnostic&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;prev&lt;/span&gt;)
&lt;span class="pl-c1"&gt;nmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; ]&lt;span class="pl-c1"&gt;g&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;diagnostic&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;next&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Remap keys for gotos&lt;/span&gt;
&lt;span class="pl-c1"&gt;nmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;gd&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;definition)
&lt;span class="pl-c1"&gt;nmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; gy &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;type&lt;/span&gt;&lt;span class="pl-k"&gt;-&lt;/span&gt;definition)
&lt;span class="pl-c1"&gt;nmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; gi &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;implementation)
&lt;span class="pl-c1"&gt;nmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;gr&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;references)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Use K to show documentation in preview window&lt;/span&gt;
&lt;span class="pl-c1"&gt;nnoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; K :&lt;span class="pl-c1"&gt;call&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;SID&amp;gt;&lt;/span&gt;&lt;span class="pl-en"&gt;show_documentation&lt;/span&gt;()&lt;span class="pl-c1"&gt;&amp;lt;CR&amp;gt;&lt;/span&gt;

&lt;span class="pl-k"&gt;function&lt;/span&gt;&lt;span class="pl-k"&gt;!&lt;/span&gt; &lt;span class="pl-en"&gt;&lt;span class="pl-k"&gt;s:&lt;/span&gt;show_documentation&lt;/span&gt;()
  &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-en"&gt;index&lt;/span&gt;([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;vim&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;help&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &amp;amp;&lt;span class="pl-c1"&gt;filetype&lt;/span&gt;) &lt;span class="pl-k"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;)
    &lt;span class="pl-c1"&gt;execute&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;h &lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.&lt;span class="pl-en"&gt;expand&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;cword&amp;gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  &lt;span class="pl-k"&gt;else&lt;/span&gt;
    &lt;span class="pl-c1"&gt;call&lt;/span&gt; &lt;span class="pl-en"&gt;CocAction&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;doHover&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  &lt;span class="pl-k"&gt;endif&lt;/span&gt;
&lt;span class="pl-k"&gt;endfunction&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Highlight symbol under cursor on CursorHold&lt;/span&gt;
&lt;span class="pl-c1"&gt;autocmd&lt;/span&gt; &lt;span class="pl-c1"&gt;CursorHold&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt; &lt;span class="pl-c1"&gt;silent&lt;/span&gt; &lt;span class="pl-c1"&gt;call&lt;/span&gt; &lt;span class="pl-en"&gt;CocActionAsync&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;highlight&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Remap for rename current word&lt;/span&gt;
&lt;span class="pl-c1"&gt;nmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;leader&amp;gt;&lt;/span&gt;rn &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;rename&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Remap for format selected region&lt;/span&gt;
&lt;span class="pl-c1"&gt;xmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;leader&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;f&lt;/span&gt;  &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;format&lt;span class="pl-k"&gt;-&lt;/span&gt;selected)
&lt;span class="pl-c1"&gt;nmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;leader&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;f&lt;/span&gt;  &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;format&lt;span class="pl-k"&gt;-&lt;/span&gt;selected)

&lt;span class="pl-k"&gt;augroup&lt;/span&gt; &lt;span class="pl-en"&gt;mygroup&lt;/span&gt;
  &lt;span class="pl-c1"&gt;autocmd&lt;/span&gt;&lt;span class="pl-k"&gt;!&lt;/span&gt;
&lt;span class="pl-c"&gt;  &lt;span class="pl-c"&gt;"&lt;/span&gt; Setup formatexpr specified filetype(s).&lt;/span&gt;
  &lt;span class="pl-c1"&gt;autocmd&lt;/span&gt; &lt;span class="pl-c1"&gt;FileType&lt;/span&gt; typescript,json &lt;span class="pl-c1"&gt;setl&lt;/span&gt; &lt;span class="pl-c1"&gt;formatexpr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-en"&gt;CocAction&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;formatSelected&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-c"&gt;  &lt;span class="pl-c"&gt;"&lt;/span&gt; Update signature help on jump placeholder&lt;/span&gt;
  &lt;span class="pl-c1"&gt;autocmd&lt;/span&gt; &lt;span class="pl-c1"&gt;User&lt;/span&gt; CocJumpPlaceholder &lt;span class="pl-c1"&gt;call&lt;/span&gt; &lt;span class="pl-en"&gt;CocActionAsync&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;showSignatureHelp&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;augroup&lt;/span&gt; &lt;span class="pl-en"&gt;end&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Remap for do codeAction of selected region,&lt;span class="pl-s"&gt; ex: `&lt;span class="pl-c1"&gt;&amp;lt;leader&amp;gt;&lt;/span&gt;aap` &lt;span class="pl-k"&gt;for&lt;/span&gt; current paragraph&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c1"&gt;xmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;leader&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;a&lt;/span&gt;  &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;codeaction&lt;span class="pl-k"&gt;-&lt;/span&gt;selected)
&lt;span class="pl-c1"&gt;nmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;leader&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;a&lt;/span&gt;  &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;codeaction&lt;span class="pl-k"&gt;-&lt;/span&gt;selected)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Remap for do codeAction of current line&lt;/span&gt;
&lt;span class="pl-c1"&gt;nmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;leader&amp;gt;&lt;/span&gt;ac  &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;codeaction)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Fix autofix problem of current line&lt;/span&gt;
&lt;span class="pl-c1"&gt;nmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;leader&amp;gt;&lt;/span&gt;qf  &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;fix&lt;/span&gt;&lt;span class="pl-k"&gt;-&lt;/span&gt;current)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Create mappings for function text object, requires document symbols feature of languageserver.&lt;/span&gt;
&lt;span class="pl-c1"&gt;xmap&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;funcobj&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;i&lt;/span&gt;)
&lt;span class="pl-c1"&gt;xmap&lt;/span&gt; af &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;funcobj&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;a&lt;/span&gt;)
&lt;span class="pl-c1"&gt;omap&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;funcobj&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;i&lt;/span&gt;)
&lt;span class="pl-c1"&gt;omap&lt;/span&gt; af &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;funcobj&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;a&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Use &amp;lt;C-d&amp;gt; for select selections ranges, needs server support, like: coc-tsserver, coc-python&lt;/span&gt;
&lt;span class="pl-c1"&gt;nmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;C-d&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;range&lt;/span&gt;&lt;span class="pl-k"&gt;-&lt;/span&gt;select)
&lt;span class="pl-c1"&gt;xmap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;C-d&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;Plug&amp;gt;&lt;/span&gt;(coc&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;range&lt;/span&gt;&lt;span class="pl-k"&gt;-&lt;/span&gt;select)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Use `:Format` to format current buffer&lt;/span&gt;
&lt;span class="pl-c1"&gt;command&lt;/span&gt;&lt;span class="pl-k"&gt;!&lt;/span&gt; -&lt;span class="pl-en"&gt;nargs&lt;/span&gt;=&lt;span class="pl-c1"&gt;0&lt;/span&gt; Format :&lt;span class="pl-c1"&gt;call&lt;/span&gt; &lt;span class="pl-en"&gt;CocAction&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;format&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Use `:Fold` to fold current buffer&lt;/span&gt;
&lt;span class="pl-c1"&gt;command&lt;/span&gt;&lt;span class="pl-k"&gt;!&lt;/span&gt; -&lt;span class="pl-en"&gt;nargs&lt;/span&gt;=? Fold :&lt;span class="pl-c1"&gt;call&lt;/span&gt;     &lt;span class="pl-en"&gt;CocAction&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;fold&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;&amp;lt;f-args&amp;gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; use `:OR` for organize import of current buffer&lt;/span&gt;
&lt;span class="pl-c1"&gt;command&lt;/span&gt;&lt;span class="pl-k"&gt;!&lt;/span&gt; -&lt;span class="pl-en"&gt;nargs&lt;/span&gt;=&lt;span class="pl-c1"&gt;0&lt;/span&gt; OR   :&lt;span class="pl-c1"&gt;call&lt;/span&gt;     &lt;span class="pl-en"&gt;CocAction&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;runCommand&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;editor.action.organizeImport&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Add status line support, for integration with other plugin, checkout `:h coc-status`&lt;/span&gt;
&lt;span class="pl-c1"&gt;set&lt;/span&gt; &lt;span class="pl-c1"&gt;statusline&lt;/span&gt;^&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-k"&gt;%&lt;/span&gt;{&lt;span class="pl-en"&gt;coc#status&lt;/span&gt;()}&lt;span class="pl-k"&gt;%&lt;/span&gt;{&lt;span class="pl-en"&gt;get&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-k"&gt;b:&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;coc_current_function&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)}

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Using CocList&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Show all diagnostics&lt;/span&gt;
&lt;span class="pl-c1"&gt;nnoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;space&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;a&lt;/span&gt;  :&lt;span class="pl-c1"&gt;&amp;lt;C-u&amp;gt;&lt;/span&gt;CocList diagnostics&lt;span class="pl-c1"&gt;&amp;lt;cr&amp;gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Manage extensions&lt;/span&gt;
&lt;span class="pl-c1"&gt;nnoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;space&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;e&lt;/span&gt;  :&lt;span class="pl-c1"&gt;&amp;lt;C-u&amp;gt;&lt;/span&gt;CocList extensions&lt;span class="pl-c1"&gt;&amp;lt;cr&amp;gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Show commands&lt;/span&gt;
&lt;span class="pl-c1"&gt;nnoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;space&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;c&lt;/span&gt;  :&lt;span class="pl-c1"&gt;&amp;lt;C-u&amp;gt;&lt;/span&gt;CocList commands&lt;span class="pl-c1"&gt;&amp;lt;cr&amp;gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Find symbol of current document&lt;/span&gt;
&lt;span class="pl-c1"&gt;nnoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;space&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;o&lt;/span&gt;  :&lt;span class="pl-c1"&gt;&amp;lt;C-u&amp;gt;&lt;/span&gt;CocList outline&lt;span class="pl-c1"&gt;&amp;lt;cr&amp;gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Search workspace symbols&lt;/span&gt;
&lt;span class="pl-c1"&gt;nnoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;space&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;s&lt;/span&gt;  :&lt;span class="pl-c1"&gt;&amp;lt;C-u&amp;gt;&lt;/span&gt;CocList &lt;span class="pl-k"&gt;-&lt;/span&gt;I symbols&lt;span class="pl-c1"&gt;&amp;lt;cr&amp;gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Do default action for next item.&lt;/span&gt;
&lt;span class="pl-c1"&gt;nnoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;space&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;j&lt;/span&gt;  :&lt;span class="pl-c1"&gt;&amp;lt;C-u&amp;gt;&lt;/span&gt;CocNext&lt;span class="pl-c1"&gt;&amp;lt;CR&amp;gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Do default action for previous item.&lt;/span&gt;
&lt;span class="pl-c1"&gt;nnoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;space&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;k&lt;/span&gt;  :&lt;span class="pl-c1"&gt;&amp;lt;C-u&amp;gt;&lt;/span&gt;CocPrev&lt;span class="pl-c1"&gt;&amp;lt;CR&amp;gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;"&lt;/span&gt; Resume latest coc list&lt;/span&gt;
&lt;span class="pl-c1"&gt;nnoremap&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;silent&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;&amp;lt;space&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;p&lt;/span&gt;  :&lt;span class="pl-c1"&gt;&amp;lt;C-u&amp;gt;&lt;/span&gt;CocListResume&lt;span class="pl-c1"&gt;&amp;lt;CR&amp;gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-articles" class="anchor" aria-hidden="true" href="#articles"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Articles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/65524706" rel="nofollow"&gt;coc.nvim 插件体系介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/71846145" rel="nofollow"&gt;CocList 入坑指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@chemzqm/create-coc-nvim-extension-to-improve-vim-experience-4461df269173" rel="nofollow"&gt;Create coc.nvim extension to improve vim experience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-trouble-shooting" class="anchor" aria-hidden="true" href="#trouble-shooting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trouble shooting&lt;/h2&gt;
&lt;p&gt;Try these steps when you have problem with coc.nvim.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make sure your vim version &amp;gt;= 8.0 by command &lt;code&gt;:version&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If service failed to start, use command &lt;code&gt;:CocInfo&lt;/code&gt; or &lt;code&gt;:checkhealth&lt;/code&gt; on neovim.&lt;/li&gt;
&lt;li&gt;Checkout the log of coc.nvim by command &lt;code&gt;:CocOpenLog&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;When you have issue with a languageserver, it's recommended to &lt;a href="https://github.com/neoclide/coc.nvim/wiki/Debug-language-server#using-output-channel"&gt;checkout the output&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-feedback" class="anchor" aria-hidden="true" href="#feedback"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Feedback&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If you think Coc is useful, consider giving it a star.&lt;/li&gt;
&lt;li&gt;If you have a question, &lt;a href="https://gitter.im/neoclide/coc.nvim" rel="nofollow"&gt;ask on gitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;中文用户请到 &lt;a href="https://gitter.im/neoclide/coc-cn" rel="nofollow"&gt;中文 gitter&lt;/a&gt; 讨论&lt;/li&gt;
&lt;li&gt;If something is not working, &lt;a href="https://github.com/neoclide/coc.nvim/issues/new"&gt;create an issue&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/251450/57566955-fb850200-7404-11e9-960f-711673f1a461.png"&gt;&lt;img src="https://user-images.githubusercontent.com/251450/57566955-fb850200-7404-11e9-960f-711673f1a461.png" width="593" height="574" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>neoclide</author><guid isPermaLink="false">https://github.com/neoclide/coc.nvim</guid><pubDate>Tue, 26 Nov 2019 00:17:00 GMT</pubDate></item><item><title>LukeSmithxyz/voidrice #18 in All Languages, Today</title><link>https://github.com/LukeSmithxyz/voidrice</link><description>&lt;p&gt;&lt;i&gt;My dotfiles (deployed by LARBS)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-the-voidrice-luke-smith-httpslukesmithxyzs-dotfiles" class="anchor" aria-hidden="true" href="#the-voidrice-luke-smith-httpslukesmithxyzs-dotfiles"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Voidrice (Luke Smith &lt;a href="https://lukesmith.xyz" rel="nofollow"&gt;https://lukesmith.xyz&lt;/a&gt;'s dotfiles)&lt;/h1&gt;
&lt;p&gt;These are the dotfiles deployed by &lt;a href="https://larbs.xyz" rel="nofollow"&gt;LARBS&lt;/a&gt; and as seen on &lt;a href="https://youtube.com/c/lukesmithxyz" rel="nofollow"&gt;my YouTube channel&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Very useful scripts are in &lt;code&gt;~/.local/bin/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Settings for:
&lt;ul&gt;
&lt;li&gt;vim/nvim (text editor)&lt;/li&gt;
&lt;li&gt;zsh (shell)&lt;/li&gt;
&lt;li&gt;i3wm/i3-gaps (window manager)&lt;/li&gt;
&lt;li&gt;i3blocks (status bar)&lt;/li&gt;
&lt;li&gt;sxhkd (general key binder)&lt;/li&gt;
&lt;li&gt;ranger (file manager)&lt;/li&gt;
&lt;li&gt;lf (file manager)&lt;/li&gt;
&lt;li&gt;mpd/ncmpcpp (music)&lt;/li&gt;
&lt;li&gt;sxiv (image/gif viewer)&lt;/li&gt;
&lt;li&gt;mpv (video player)&lt;/li&gt;
&lt;li&gt;calcurse (calendar program)&lt;/li&gt;
&lt;li&gt;tmux&lt;/li&gt;
&lt;li&gt;other stuff like xdg default programs, inputrc and more, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I try to minimize what's directly in &lt;code&gt;~&lt;/code&gt; so:
&lt;ul&gt;
&lt;li&gt;All configs that can be in &lt;code&gt;~/.config/&lt;/code&gt; are.&lt;/li&gt;
&lt;li&gt;Some environmental variables have been set in &lt;code&gt;~/.zprofile&lt;/code&gt; to move configs into &lt;code&gt;~/.config/&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bookmarks in text files used by various scripts (like &lt;code&gt;~/.local/bin/shortcuts&lt;/code&gt;)
&lt;ul&gt;
&lt;li&gt;File bookmarks in &lt;code&gt;~/.config/files&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Directory bookmarks in &lt;code&gt;~/.config/directories&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-want-even-more" class="anchor" aria-hidden="true" href="#want-even-more"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want even more?&lt;/h2&gt;
&lt;p&gt;My setup is pretty modular nowadays.
I use several suckless program that are meant to be configured and compiled by the user and I also have separate repos for some other things.
Check out their links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/lukesmithxyz/dwm"&gt;dwm&lt;/a&gt; (the window manager I usually use now which is fully compatible with this repo)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lukesmithxyz/st"&gt;st&lt;/a&gt; (the terminal emulator assumed to be used by these dotfiles)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lukesmithxyz/mutt-wizard"&gt;mutt-wizard (&lt;code&gt;mw&lt;/code&gt;)&lt;/a&gt; - (a terminal-based email system that can store your mail offline without effort)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-install-these-dotfiles" class="anchor" aria-hidden="true" href="#install-these-dotfiles"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install these dotfiles&lt;/h2&gt;
&lt;p&gt;Use &lt;a href="https://larbs.xyz" rel="nofollow"&gt;LARBS&lt;/a&gt; to autoinstall everything:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -LO larbs.xyz/larbs.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or clone the repo files directly to your home directory and install &lt;a href="https://github.com/LukeSmithxyz/LARBS/blob/master/progs.csv"&gt;the prerequisite programs&lt;/a&gt; or &lt;a href="https://github.com/LukeSmithxyz/LARBS/blob/master/legacy.csv"&gt;those required for the i3 setup&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>LukeSmithxyz</author><guid isPermaLink="false">https://github.com/LukeSmithxyz/voidrice</guid><pubDate>Tue, 26 Nov 2019 00:18:00 GMT</pubDate></item><item><title>realpython/materials #19 in All Languages, Today</title><link>https://github.com/realpython/materials</link><description>&lt;p&gt;&lt;i&gt;Bonus materials, exercises, and example projects for our Python tutorials&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-real-python-materials" class="anchor" aria-hidden="true" href="#real-python-materials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Real Python Materials&lt;/h1&gt;
&lt;p&gt;Bonus materials, exercises, and example projects for our &lt;a href="https://realpython.com" rel="nofollow"&gt;Python tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Build Status: &lt;a href="https://circleci.com/gh/realpython/materials" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/991534e54579264db6c49f9225646cb77dd8fd6a/68747470733a2f2f636972636c6563692e636f6d2f67682f7265616c707974686f6e2f6d6174657269616c732e7376673f7374796c653d737667" alt="CircleCI" data-canonical-src="https://circleci.com/gh/realpython/materials.svg?style=svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-code-style-checks" class="anchor" aria-hidden="true" href="#running-code-style-checks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Code Style Checks&lt;/h2&gt;
&lt;p&gt;We use &lt;a href="http://flake8.pycqa.org/en/latest/" rel="nofollow"&gt;flake8&lt;/a&gt; and &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; to ensure a consistent code style for all of our sample code in this repository.&lt;/p&gt;
&lt;p&gt;Run the following commands to validate your code against the linters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ flake8
$ black --check &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-running-python-code-formatter" class="anchor" aria-hidden="true" href="#running-python-code-formatter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Python Code Formatter&lt;/h2&gt;
&lt;p&gt;We're using a tool called &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; on this repo to ensure consistent formatting. On CI it runs in "check" mode to ensure any new files added to the repo are following PEP 8. If you see linter warnings that say something like "would reformat some_file.py" it means black disagrees with your formatting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The easiest way to resolve these errors is to just run Black locally on the code and then committing those changes, as explained below.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To automatically re-format your code to be consistent with our code style guidelines, run &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; in the repository root folder:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ black &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>realpython</author><guid isPermaLink="false">https://github.com/realpython/materials</guid><pubDate>Tue, 26 Nov 2019 00:19:00 GMT</pubDate></item><item><title>tevador/RandomX #20 in All Languages, Today</title><link>https://github.com/tevador/RandomX</link><description>&lt;p&gt;&lt;i&gt;Proof of work algorithm based on random code execution&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-randomx" class="anchor" aria-hidden="true" href="#randomx"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RandomX&lt;/h1&gt;
&lt;p&gt;RandomX is a proof-of-work (PoW) algorithm that is optimized for general-purpose CPUs. RandomX uses random code execution (hence the name) together with several memory-hard techniques to minimize the efficiency advantage of specialized hardware.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h2&gt;
&lt;p&gt;RandomX utilizes a virtual machine that executes programs in a special instruction set that consists of integer math, floating point math and branches. These programs can be translated into the CPU's native machine code on the fly (example: &lt;a href="doc/program.asm"&gt;program.asm&lt;/a&gt;). At the end, the outputs of the executed programs are consolidated into a 256-bit result using a cryptographic hashing function (&lt;a href="https://blake2.net/" rel="nofollow"&gt;Blake2b&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;RandomX can operate in two main modes with different memory requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fast mode&lt;/strong&gt; - requires 2080 MiB of shared memory.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Light mode&lt;/strong&gt; - requires only 256 MiB of shared memory, but runs significantly slower&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both modes are interchangeable as they give the same results. The fast mode is suitable for "mining", while the light mode is expected to be used only for proof verification.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;Full specification is available in &lt;a href="doc/specs.md"&gt;specs.md&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Design description and analysis is available in &lt;a href="doc/design.md"&gt;design.md&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-audits" class="anchor" aria-hidden="true" href="#audits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Audits&lt;/h2&gt;
&lt;p&gt;Between May and August 2019, RandomX was audited by 4 independent security research teams:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.trailofbits.com/" rel="nofollow"&gt;Trail of Bits&lt;/a&gt; (28 000 USD)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.x41-dsec.de/" rel="nofollow"&gt;X41 D-SEC&lt;/a&gt; (42 000 EUR)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kudelskisecurity.com/" rel="nofollow"&gt;Kudelski Security&lt;/a&gt; (18 250 CHF)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://quarkslab.com/en/" rel="nofollow"&gt;QuarksLab&lt;/a&gt; (52 800 USD)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first audit was generously funded by &lt;a href="https://www.arweave.org/" rel="nofollow"&gt;Arweave&lt;/a&gt;, one of the early adopters of RandomX. The remaining three audits were funded by donations from the &lt;a href="https://ccs.getmonero.org/proposals/RandomX-audit.html" rel="nofollow"&gt;Monero community&lt;/a&gt;. All four audits were coordinated by &lt;a href="https://ostif.org/" rel="nofollow"&gt;OSTIF&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Final reports from all four audits are available in the &lt;a href="audits/"&gt;audits&lt;/a&gt; directory. None of the audits found any critical vulnerabilities, but several changes in the algorithm and the code were made as a direct result of the audits. More details can be found in the &lt;a href="https://ostif.org/four-audits-of-randomx-for-monero-and-arweave-have-been-completed-results/" rel="nofollow"&gt;final report by OSTIF&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-build" class="anchor" aria-hidden="true" href="#build"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build&lt;/h2&gt;
&lt;p&gt;RandomX is written in C++11 and builds a static library with a C API provided by header file &lt;a href="src/randomx.h"&gt;randomx.h&lt;/a&gt;. Minimal API usage example is provided in &lt;a href="src/tests/api-example1.c"&gt;api-example1.c&lt;/a&gt;. The reference code includes a &lt;code&gt;randomx-benchmark&lt;/code&gt; and &lt;code&gt;randomx-tests&lt;/code&gt; executables for testing.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-linux" class="anchor" aria-hidden="true" href="#linux"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linux&lt;/h3&gt;
&lt;p&gt;Build dependencies: &lt;code&gt;cmake&lt;/code&gt; (minimum 2.8.7) and &lt;code&gt;gcc&lt;/code&gt; (minimum version 4.8, but version 7+ is recommended).&lt;/p&gt;
&lt;p&gt;To build optimized binaries for your machine, run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/tevador/RandomX.git
cd RandomX
mkdir build &amp;amp;&amp;amp; cd build
cmake -DARCH=native ..
make
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-windows" class="anchor" aria-hidden="true" href="#windows"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Windows&lt;/h3&gt;
&lt;p&gt;On Windows, it is possible to build using MinGW (same procedure as on Linux) or using Visual Studio (solution file is provided).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-precompiled-binaries" class="anchor" aria-hidden="true" href="#precompiled-binaries"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Precompiled binaries&lt;/h3&gt;
&lt;p&gt;Precompiled &lt;code&gt;randomx-benchmark&lt;/code&gt; binaries are available on the &lt;a href="https://github.com/tevador/RandomX/releases"&gt;Releases page&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-proof-of-work" class="anchor" aria-hidden="true" href="#proof-of-work"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Proof of work&lt;/h2&gt;
&lt;p&gt;RandomX was primarily designed as a PoW algorithm for &lt;a href="https://www.getmonero.org/" rel="nofollow"&gt;Monero&lt;/a&gt;. The recommended usage is following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The key &lt;code&gt;K&lt;/code&gt; is selected to be the hash of a block in the blockchain - this block is called the 'key block'. For optimal mining and verification performance, the key should change every 2048 blocks (~2.8 days) and there should be a delay of 64 blocks (~2 hours) between the key block and the change of the key &lt;code&gt;K&lt;/code&gt;. This can be achieved by changing the key when &lt;code&gt;blockHeight % 2048 == 64&lt;/code&gt; and selecting key block such that &lt;code&gt;keyBlockHeight % 2048 == 0&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The input &lt;code&gt;H&lt;/code&gt; is the standard hashing blob with a selected nonce value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you wish to use RandomX as a PoW algorithm for your cryptocurrency, please follow the &lt;a href="doc/configuration.md"&gt;configuration guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: To achieve ASIC resistance, the key &lt;code&gt;K&lt;/code&gt; must change and must not be miner-selectable. We recommend to use blockchain data as the key in a similar way to the Monero example above. If blockchain data cannot be used for some reason, use a predefined sequence of keys.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-cpu-performance" class="anchor" aria-hidden="true" href="#cpu-performance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CPU performance&lt;/h3&gt;
&lt;p&gt;The table below lists the performance of selected CPUs using the optimal number of threads (T) and large pages (if possible), in hashes per second (H/s). "CNv4" refers to the CryptoNight variant 4 (CN/R) hashrate measured using &lt;a href="https://github.com/xmrig/xmrig"&gt;XMRig&lt;/a&gt; v2.14.1. "Fast mode" and "Light mode" are the two modes of RandomX.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CPU&lt;/th&gt;
&lt;th&gt;RAM&lt;/th&gt;
&lt;th&gt;OS&lt;/th&gt;
&lt;th&gt;AES&lt;/th&gt;
&lt;th&gt;CNv4&lt;/th&gt;
&lt;th&gt;Fast mode&lt;/th&gt;
&lt;th&gt;Light mode&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Intel Core i9-9900K&lt;/td&gt;
&lt;td&gt;32G DDR4-3200&lt;/td&gt;
&lt;td&gt;Windows 10&lt;/td&gt;
&lt;td&gt;hw&lt;/td&gt;
&lt;td&gt;660 (8T)&lt;/td&gt;
&lt;td&gt;5770 (8T)&lt;/td&gt;
&lt;td&gt;1160 (16T)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AMD Ryzen 7 1700&lt;/td&gt;
&lt;td&gt;16G DDR4-2666&lt;/td&gt;
&lt;td&gt;Ubuntu 16.04&lt;/td&gt;
&lt;td&gt;hw&lt;/td&gt;
&lt;td&gt;520 (8T)&lt;/td&gt;
&lt;td&gt;4100 (8T)&lt;/td&gt;
&lt;td&gt;620 (16T)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Intel Core i7-8550U&lt;/td&gt;
&lt;td&gt;16G DDR4-2400&lt;/td&gt;
&lt;td&gt;Windows 10&lt;/td&gt;
&lt;td&gt;hw&lt;/td&gt;
&lt;td&gt;200 (4T)&lt;/td&gt;
&lt;td&gt;1700  (4T)&lt;/td&gt;
&lt;td&gt;350 (8T)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Intel Core i3-3220&lt;/td&gt;
&lt;td&gt;4G DDR3-1333&lt;/td&gt;
&lt;td&gt;Ubuntu 16.04&lt;/td&gt;
&lt;td&gt;soft&lt;/td&gt;
&lt;td&gt;42 (4T)&lt;/td&gt;
&lt;td&gt;510 (4T)&lt;/td&gt;
&lt;td&gt;150 (4T)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Raspberry Pi 3&lt;/td&gt;
&lt;td&gt;1G LPDDR2&lt;/td&gt;
&lt;td&gt;Ubuntu 16.04&lt;/td&gt;
&lt;td&gt;soft&lt;/td&gt;
&lt;td&gt;3.5 (4T)&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;20 (4T)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note that RandomX currently includes a JIT compiler for x86-64 and ARM64. Other architectures have to use the portable interpreter, which is much slower.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-gpu-performance" class="anchor" aria-hidden="true" href="#gpu-performance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GPU performance&lt;/h3&gt;
&lt;p&gt;SChernykh is developing GPU mining code for RandomX. Benchmarks are included in the following repositories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/SChernykh/RandomX_CUDA"&gt;CUDA miner&lt;/a&gt; - NVIDIA GPUs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SChernykh/RandomX_OpenCL"&gt;OpenCL miner&lt;/a&gt; - only for AMD Vega and AMD Polaris GPUs (uses GCN machine code).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The code from the above repositories is included in the open source miner &lt;a href="https://github.com/xmrig/xmrig"&gt;XMRig&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that GPUs are at a disadvantage when running RandomX since the algorithm was designed to be efficient on CPUs.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-which-cpu-is-best-for-mining-randomx" class="anchor" aria-hidden="true" href="#which-cpu-is-best-for-mining-randomx"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Which CPU is best for mining RandomX?&lt;/h3&gt;
&lt;p&gt;Most Intel and AMD CPUs made since 2011 should be fairly efficient at RandomX. More specifically, efficient mining requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;64-bit architecture&lt;/li&gt;
&lt;li&gt;IEEE 754 compliant floating point unit&lt;/li&gt;
&lt;li&gt;Hardware AES support (&lt;a href="https://en.wikipedia.org/wiki/AES_instruction_set" rel="nofollow"&gt;AES-NI&lt;/a&gt; extension for x86, Cryptography extensions for ARMv8)&lt;/li&gt;
&lt;li&gt;16 KiB of L1 cache, 256 KiB of L2 cache and 2 MiB of L3 cache per mining thread&lt;/li&gt;
&lt;li&gt;Support for large memory pages&lt;/li&gt;
&lt;li&gt;At least 2.5 GiB of free RAM per NUMA node&lt;/li&gt;
&lt;li&gt;Multiple memory channels may be required:
&lt;ul&gt;
&lt;li&gt;DDR3 memory is limited to about 1500-2000 H/s per channel (depending on frequency and timings)&lt;/li&gt;
&lt;li&gt;DDR4 memory is limited to about 4000-6000 H/s per channel  (depending on frequency and timings)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-does-randomx-facilitate-botnetsmalware-mining-or-web-mining" class="anchor" aria-hidden="true" href="#does-randomx-facilitate-botnetsmalware-mining-or-web-mining"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Does RandomX facilitate botnets/malware mining or web mining?&lt;/h3&gt;
&lt;p&gt;Efficient mining requires more than 2 GiB of memory, which is difficult to hide in an infected computer and disqualifies many low-end machines such as IoT devices. Web mining is infeasible due to the large memory requirement and the lack of directed rounding support for floating point operations in both Javascript and WebAssembly.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-since-randomx-uses-floating-point-math-does-it-give-reproducible-results-on-different-platforms" class="anchor" aria-hidden="true" href="#since-randomx-uses-floating-point-math-does-it-give-reproducible-results-on-different-platforms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Since RandomX uses floating point math, does it give reproducible results on different platforms?&lt;/h3&gt;
&lt;p&gt;RandomX uses only operations that are guaranteed to give correctly rounded results by the &lt;a href="https://en.wikipedia.org/wiki/IEEE_754" rel="nofollow"&gt;IEEE 754&lt;/a&gt; standard: addition, subtraction, multiplication, division and square root. Special care is taken to avoid corner cases such as NaN values or denormals.&lt;/p&gt;
&lt;p&gt;The reference implementation has been validated on the following platforms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;x86 (32-bit, little-endian)&lt;/li&gt;
&lt;li&gt;x86-64 (64-bit, little-endian)&lt;/li&gt;
&lt;li&gt;ARMv7+VFPv3 (32-bit, little-endian)&lt;/li&gt;
&lt;li&gt;ARMv8 (64-bit, little-endian)&lt;/li&gt;
&lt;li&gt;PPC64 (64-bit, big-endian)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-can-fpgas-mine-randomx" class="anchor" aria-hidden="true" href="#can-fpgas-mine-randomx"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Can FPGAs mine RandomX?&lt;/h3&gt;
&lt;p&gt;RandomX generates multiple unique programs for every hash, so FPGAs cannot dynamically reconfigure their circuitry because typical FPGA takes tens of seconds to load a bitstream. It is also not possible to generate bitstreams for RandomX programs in advance due to the sheer number of combinations (there are 2&lt;sup&gt;512&lt;/sup&gt; unique programs).&lt;/p&gt;
&lt;p&gt;Sufficiently large FPGAs can mine RandomX in a &lt;a href="https://en.wikipedia.org/wiki/Soft_microprocessor" rel="nofollow"&gt;soft microprocessor&lt;/a&gt; configuration by emulating a CPU. Under these circumstances, an FPGA will be much less efficient than a CPU or a specialized chip (ASIC).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tevador"&gt;tevador&lt;/a&gt; - author&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SChernykh"&gt;SChernykh&lt;/a&gt; - contributed significantly to the design of RandomX&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hyc"&gt;hyc&lt;/a&gt; - original idea of using random code execution for PoW&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tevador/RandomX/graphs/contributors"&gt;Other contributors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;RandomX uses some source code from the following 3rd party repositories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Argon2d, Blake2b hashing functions: &lt;a href="https://github.com/P-H-C/phc-winner-argon2"&gt;https://github.com/P-H-C/phc-winner-argon2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The author of RandomX declares no competing financial interest.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-donations" class="anchor" aria-hidden="true" href="#donations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Donations&lt;/h2&gt;
&lt;p&gt;If you'd like to use RandomX, please consider donating to help cover the development cost of the algorithm.&lt;/p&gt;
&lt;p&gt;Author's XMR address:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;845xHUh5GvfHwc2R8DVJCE7BT2sd4YEcmjG8GNSdmeNsP5DTEjXd1CNgxTcjHjiFuthRHAoVEJjM7GyKzQKLJtbd56xbh7V
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Total donations received: ~3.86 XMR (as of 30th August 2019). Thanks to all contributors.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tevador</author><guid isPermaLink="false">https://github.com/tevador/RandomX</guid><pubDate>Tue, 26 Nov 2019 00:20:00 GMT</pubDate></item><item><title>nothings/stb #21 in All Languages, Today</title><link>https://github.com/nothings/stb</link><description>&lt;p&gt;&lt;i&gt;stb single-file public domain libraries for C/C++&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;
&lt;h1&gt;&lt;a id="user-content-stb" class="anchor" aria-hidden="true" href="#stb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;stb&lt;/h1&gt;
&lt;p&gt;single-file public domain (or MIT licensed) libraries for C/C++&lt;/p&gt;
&lt;p&gt;Noteworthy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image loader: &lt;a href="stb_image.h"&gt;stb_image.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;image writer: &lt;a href="stb_image_write.h"&gt;stb_image_write.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;image resizer: &lt;a href="stb_image_resize.h"&gt;stb_image_resize.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;font text rasterizer: &lt;a href="stb_truetype.h"&gt;stb_truetype.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;typesafe containers: &lt;a href="stb_ds.h"&gt;stb_ds.h&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most libraries by stb, except: stb_dxt by Fabian "ryg" Giesen, stb_image_resize
by Jorge L. "VinoBS" Rodriguez, and stb_sprintf by Jeff Roberts.&lt;/p&gt;
&lt;p&gt;&lt;a name="user-content-stb_libs"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;library&lt;/th&gt;
&lt;th&gt;lastest version&lt;/th&gt;
&lt;th&gt;category&lt;/th&gt;
&lt;th&gt;LoC&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_vorbis.c"&gt;stb_vorbis.c&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.17&lt;/td&gt;
&lt;td&gt;audio&lt;/td&gt;
&lt;td&gt;5502&lt;/td&gt;
&lt;td&gt;decode ogg vorbis files from file/memory to float/16-bit signed output&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_image.h"&gt;stb_image.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;2.23&lt;/td&gt;
&lt;td&gt;graphics&lt;/td&gt;
&lt;td&gt;7559&lt;/td&gt;
&lt;td&gt;image loading/decoding from file/memory: JPG, PNG, TGA, BMP, PSD, GIF, HDR, PIC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_truetype.h"&gt;stb_truetype.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.22&lt;/td&gt;
&lt;td&gt;graphics&lt;/td&gt;
&lt;td&gt;4888&lt;/td&gt;
&lt;td&gt;parse, decode, and rasterize characters from truetype fonts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_image_write.h"&gt;stb_image_write.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.13&lt;/td&gt;
&lt;td&gt;graphics&lt;/td&gt;
&lt;td&gt;1619&lt;/td&gt;
&lt;td&gt;image writing to disk: PNG, TGA, BMP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_image_resize.h"&gt;stb_image_resize.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.96&lt;/td&gt;
&lt;td&gt;graphics&lt;/td&gt;
&lt;td&gt;2630&lt;/td&gt;
&lt;td&gt;resize images larger/smaller with good quality&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_rect_pack.h"&gt;stb_rect_pack.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;graphics&lt;/td&gt;
&lt;td&gt;628&lt;/td&gt;
&lt;td&gt;simple 2D rectangle packer with decent quality&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_ds.h"&gt;stb_ds.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.62&lt;/td&gt;
&lt;td&gt;utility&lt;/td&gt;
&lt;td&gt;1723&lt;/td&gt;
&lt;td&gt;typesafe dynamic array and hash tables for C, will compile in C++&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_sprintf.h"&gt;stb_sprintf.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.06&lt;/td&gt;
&lt;td&gt;utility&lt;/td&gt;
&lt;td&gt;1860&lt;/td&gt;
&lt;td&gt;fast sprintf, snprintf for C/C++&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stretchy_buffer.h"&gt;stretchy_buffer.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.03&lt;/td&gt;
&lt;td&gt;utility&lt;/td&gt;
&lt;td&gt;262&lt;/td&gt;
&lt;td&gt;typesafe dynamic array for C (i.e. approximation to vector&amp;lt;&amp;gt;), doesn't compile as C++&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_textedit.h"&gt;stb_textedit.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.13&lt;/td&gt;
&lt;td&gt;user interface&lt;/td&gt;
&lt;td&gt;1404&lt;/td&gt;
&lt;td&gt;guts of a text editor for games etc implementing them from scratch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_voxel_render.h"&gt;stb_voxel_render.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.88&lt;/td&gt;
&lt;td&gt;3D graphics&lt;/td&gt;
&lt;td&gt;3806&lt;/td&gt;
&lt;td&gt;Minecraft-esque voxel rendering "engine" with many more features&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_dxt.h"&gt;stb_dxt.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.08b&lt;/td&gt;
&lt;td&gt;3D graphics&lt;/td&gt;
&lt;td&gt;728&lt;/td&gt;
&lt;td&gt;Fabian "ryg" Giesen's real-time DXT compressor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_perlin.h"&gt;stb_perlin.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;td&gt;3D graphics&lt;/td&gt;
&lt;td&gt;427&lt;/td&gt;
&lt;td&gt;revised Perlin noise (3D input, 1D output)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_easy_font.h"&gt;stb_easy_font.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;3D graphics&lt;/td&gt;
&lt;td&gt;303&lt;/td&gt;
&lt;td&gt;quick-and-dirty easy-to-deploy bitmap font for printing frame rate, etc&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_tilemap_editor.h"&gt;stb_tilemap_editor.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.41&lt;/td&gt;
&lt;td&gt;game dev&lt;/td&gt;
&lt;td&gt;4161&lt;/td&gt;
&lt;td&gt;embeddable tilemap editor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_herringbone_wang_tile.h"&gt;stb_herringbone_wa...&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;td&gt;game dev&lt;/td&gt;
&lt;td&gt;1221&lt;/td&gt;
&lt;td&gt;herringbone Wang tile map generator&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_c_lexer.h"&gt;stb_c_lexer.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.11&lt;/td&gt;
&lt;td&gt;parsing&lt;/td&gt;
&lt;td&gt;966&lt;/td&gt;
&lt;td&gt;simplify writing parsers for C-like languages&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_divide.h"&gt;stb_divide.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.92&lt;/td&gt;
&lt;td&gt;math&lt;/td&gt;
&lt;td&gt;421&lt;/td&gt;
&lt;td&gt;more useful 32-bit modulus e.g. "euclidean divide"&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_connected_components.h"&gt;stb_connected_comp...&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.96&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;1049&lt;/td&gt;
&lt;td&gt;incrementally compute reachability on grids&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb.h"&gt;stb.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;2.35&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;14453&lt;/td&gt;
&lt;td&gt;helper functions for C, mostly redundant in C++; basically author's personal stuff&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_leakcheck.h"&gt;stb_leakcheck.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;190&lt;/td&gt;
&lt;td&gt;quick-and-dirty malloc/free leak-checking&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;a href="stb_include.h"&gt;stb_include.h&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.01&lt;/td&gt;
&lt;td&gt;misc&lt;/td&gt;
&lt;td&gt;288&lt;/td&gt;
&lt;td&gt;implement recursive #include support, particularly for GLSL&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Total libraries: 22&lt;br&gt;
Total lines of C code: 56088&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-whats-the-license" class="anchor" aria-hidden="true" href="#whats-the-license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's the license?&lt;/h4&gt;
&lt;p&gt;These libraries are in the public domain. You can do anything you
want with them. You have no legal obligation
to do anything else, although I appreciate attribution.&lt;/p&gt;
&lt;p&gt;They are also licensed under the MIT open source license, if you have lawyers
who are unhappy with public domain. Every source file includes an explicit
dual-license for you to choose from.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content--are-there-other-single-file-public-domainopen-source-libraries-with-minimal-dependencies-out-there" class="anchor" aria-hidden="true" href="#-are-there-other-single-file-public-domainopen-source-libraries-with-minimal-dependencies-out-there"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-other_libs"&gt;&lt;/a&gt; Are there other single-file public-domain/open source libraries with minimal dependencies out there?&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/nothings/single_file_libs"&gt;Yes.&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-if-i-wrap-an-stb-library-in-a-new-library-does-the-new-library-have-to-be-public-domainmit" class="anchor" aria-hidden="true" href="#if-i-wrap-an-stb-library-in-a-new-library-does-the-new-library-have-to-be-public-domainmit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;If I wrap an stb library in a new library, does the new library have to be public domain/MIT?&lt;/h4&gt;
&lt;p&gt;No, because it's public domain you can freely relicense it to whatever license your new
library wants to be.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-whats-the-deal-with-sse-support-in-gcc-based-compilers" class="anchor" aria-hidden="true" href="#whats-the-deal-with-sse-support-in-gcc-based-compilers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's the deal with SSE support in GCC-based compilers?&lt;/h4&gt;
&lt;p&gt;stb_image will either use SSE2 (if you compile with -msse2) or
will not use any SIMD at all, rather than trying to detect the
processor at runtime and handle it correctly. As I understand it,
the approved path in GCC for runtime-detection require
you to use multiple source files, one for each CPU configuration.
Because stb_image is a header-file library that compiles in only
one source file, there's no approved way to build both an
SSE-enabled and a non-SSE-enabled variation.&lt;/p&gt;
&lt;p&gt;While we've tried to work around it, we've had multiple issues over
the years due to specific versions of gcc breaking what we're doing,
so we've given up on it. See &lt;a href="https://github.com/nothings/stb/issues/280"&gt;https://github.com/nothings/stb/issues/280&lt;/a&gt;
and &lt;a href="https://github.com/nothings/stb/issues/410"&gt;https://github.com/nothings/stb/issues/410&lt;/a&gt; for examples.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-some-of-these-libraries-seem-redundant-to-existing-open-source-libraries-are-they-better-somehow" class="anchor" aria-hidden="true" href="#some-of-these-libraries-seem-redundant-to-existing-open-source-libraries-are-they-better-somehow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Some of these libraries seem redundant to existing open source libraries. Are they better somehow?&lt;/h4&gt;
&lt;p&gt;Generally they're only better in that they're easier to integrate,
easier to use, and easier to release (single file; good API; no
attribution requirement). They may be less featureful, slower,
and/or use more memory. If you're already using an equivalent
library, there's probably no good reason to switch.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-can-i-link-directly-to-the-table-of-stb-libraries" class="anchor" aria-hidden="true" href="#can-i-link-directly-to-the-table-of-stb-libraries"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Can I link directly to the table of stb libraries?&lt;/h4&gt;
&lt;p&gt;You can use &lt;a href="https://github.com/nothings/stb#stb_libs"&gt;this URL&lt;/a&gt; to link directly to that list.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-why-do-you-list-lines-of-code-its-a-terrible-metric" class="anchor" aria-hidden="true" href="#why-do-you-list-lines-of-code-its-a-terrible-metric"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why do you list "lines of code"? It's a terrible metric.&lt;/h4&gt;
&lt;p&gt;Just to give you some idea of the internal complexity of the library,
to help you manage your expectations, or to let you know what you're
getting into. While not all the libraries are written in the same
style, they're certainly similar styles, and so comparisons between
the libraries are probably still meaningful.&lt;/p&gt;
&lt;p&gt;Note though that the lines do include both the implementation, the
part that corresponds to a header file, and the documentation.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-why-single-file-headers" class="anchor" aria-hidden="true" href="#why-single-file-headers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why single-file headers?&lt;/h4&gt;
&lt;p&gt;Windows doesn't have standard directories where libraries
live. That makes deploying libraries in Windows a lot more
painful than open source developers on Unix-derivates generally
realize. (It also makes library dependencies a lot worse in Windows.)&lt;/p&gt;
&lt;p&gt;There's also a common problem in Windows where a library was built
against a different version of the runtime library, which causes
link conflicts and confusion. Shipping the libs as headers means
you normally just compile them straight into your project without
making libraries, thus sidestepping that problem.&lt;/p&gt;
&lt;p&gt;Making them a single file makes it very easy to just
drop them into a project that needs them. (Of course you can
still put them in a proper shared library tree if you want.)&lt;/p&gt;
&lt;p&gt;Why not two files, one a header and one an implementation?
The difference between 10 files and 9 files is not a big deal,
but the difference between 2 files and 1 file is a big deal.
You don't need to zip or tar the files up, you don't have to
remember to attach &lt;em&gt;two&lt;/em&gt; files, etc.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-why-stb-is-this-something-to-do-with-set-top-boxes" class="anchor" aria-hidden="true" href="#why-stb-is-this-something-to-do-with-set-top-boxes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why "stb"? Is this something to do with Set-Top Boxes?&lt;/h4&gt;
&lt;p&gt;No, they are just the initials for my name, Sean T. Barrett.
This was not chosen out of egomania, but as a moderately sane
way of namespacing the filenames and source function names.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-you-add-more-image-types-to-stb_imageh" class="anchor" aria-hidden="true" href="#will-you-add-more-image-types-to-stb_imageh"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will you add more image types to stb_image.h?&lt;/h4&gt;
&lt;p&gt;If people submit them, I generally add them, but the goal of stb_image
is less for applications like image viewer apps (which need to support
every type of image under the sun) and more for things like games which
can choose what images to use, so I may decline to add them if they're
too rare or if the size of implementation vs. apparent benefit is too low.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-do-you-have-any-advice-on-how-to-create-my-own-single-file-library" class="anchor" aria-hidden="true" href="#do-you-have-any-advice-on-how-to-create-my-own-single-file-library"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Do you have any advice on how to create my own single-file library?&lt;/h4&gt;
&lt;p&gt;Yes. &lt;a href="https://github.com/nothings/stb/blob/master/docs/stb_howto.txt"&gt;https://github.com/nothings/stb/blob/master/docs/stb_howto.txt&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-why-public-domain" class="anchor" aria-hidden="true" href="#why-public-domain"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why public domain?&lt;/h4&gt;
&lt;p&gt;I prefer it over GPL, LGPL, BSD, zlib, etc. for many reasons.
Some of them are listed here:
&lt;a href="https://github.com/nothings/stb/blob/master/docs/why_public_domain.md"&gt;https://github.com/nothings/stb/blob/master/docs/why_public_domain.md&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-why-c" class="anchor" aria-hidden="true" href="#why-c"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why C?&lt;/h4&gt;
&lt;p&gt;Primarily, because I use C, not C++. But it does also make it easier
for other people to use them from other languages.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-why-not-c99-stdinth-declare-anywhere-etc" class="anchor" aria-hidden="true" href="#why-not-c99-stdinth-declare-anywhere-etc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why not C99? stdint.h, declare-anywhere, etc.&lt;/h4&gt;
&lt;p&gt;I still use MSVC 6 (1998) as my IDE because it has better human factors
for me than later versions of MSVC.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>nothings</author><guid isPermaLink="false">https://github.com/nothings/stb</guid><pubDate>Tue, 26 Nov 2019 00:21:00 GMT</pubDate></item><item><title>CorentinJ/Real-Time-Voice-Cloning #22 in All Languages, Today</title><link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link><description>&lt;p&gt;&lt;i&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-real-time-voice-cloning" class="anchor" aria-hidden="true" href="#real-time-voice-cloning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Real-Time Voice Cloning&lt;/h1&gt;
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;Transfer Learning from Speaker Verification to
Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. Feel free to check &lt;a href="https://matheo.uliege.be/handle/2268.2/6801" rel="nofollow"&gt;my thesis&lt;/a&gt; if you're curious or if you're looking for info I haven't documented yet (don't hesitate to make an issue for that too). Mostly I would recommend giving a quick look to the figures beyond the introduction.&lt;/p&gt;
&lt;p&gt;SV2TTS is a three-stage deep learning framework that allows to create a numerical representation of a voice from a few seconds of audio, and to use it to condition a text-to-speech model trained to generalize to new voices.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9c33f78be8afe656503da974c478ea2ba2647db7/68747470733a2f2f692e696d6775722e636f6d2f386c46556c677a2e706e67" alt="Toolbox demo" data-canonical-src="https://i.imgur.com/8lFUlgz.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-papers-implemented" class="anchor" aria-hidden="true" href="#papers-implemented"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Papers implemented&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Designation&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Implementation source&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf" rel="nofollow"&gt;1802.08435&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;WaveRNN (vocoder)&lt;/td&gt;
&lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1712.05884.pdf" rel="nofollow"&gt;1712.05884&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tacotron 2 (synthesizer)&lt;/td&gt;
&lt;td&gt;Natural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Rayhane-mamah/Tacotron-2"&gt;Rayhane-mamah/Tacotron-2&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf" rel="nofollow"&gt;1710.10467&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GE2E (encoder)&lt;/td&gt;
&lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-news" class="anchor" aria-hidden="true" href="#news"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;News&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;13/11/19&lt;/strong&gt;: I'm sorry that I can't maintain this repo as much as I wish I could. I'm working full time on improving voice cloning techniques and I don't have the time to share my improvements here. Plus this repo relies on a lot of old tensorflow code and it's hard to work with. If you're a researcher, then this repo might be of use to you. &lt;strong&gt;If you just want to clone your voice&lt;/strong&gt;, do check our demo on &lt;a href="https://www.resemble.ai/" rel="nofollow"&gt;Resemble.AI&lt;/a&gt; - it can run for free but it will be a bit slower, and it will give much better results than this repo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;20/08/19:&lt;/strong&gt; I'm working on &lt;a href="https://github.com/resemble-ai/Resemblyzer"&gt;resemblyzer&lt;/a&gt;, an independent package for the voice encoder. You can use your trained encoder models from this repo with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;06/07/19:&lt;/strong&gt; Need to run within a docker container on a remote server? See &lt;a href="https://sean.lane.sh/posts/2019/07/Running-the-Real-Time-Voice-Cloning-project-in-Docker/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;25/06/19:&lt;/strong&gt; Experimental support for low-memory GPUs (~2gb) added for the synthesizer. Pass &lt;code&gt;--low_mem&lt;/code&gt; to &lt;code&gt;demo_cli.py&lt;/code&gt; or &lt;code&gt;demo_toolbox.py&lt;/code&gt; to enable it. It adds a big overhead, so it's not recommended if you have enough VRAM.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h3&gt;
&lt;p&gt;You will need the following whether you plan to use the toolbox only or to retrain the models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Python 3.7&lt;/strong&gt;. Python 3.6 might work too, but I wouldn't go lower because I make extensive use of pathlib.&lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to install the necessary packages. Additionally you will need &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;PyTorch&lt;/a&gt; (&amp;gt;=1.0.1).&lt;/p&gt;
&lt;p&gt;A GPU is mandatory, but you don't necessarily need a high tier GPU if you only want to use the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pretrained-models" class="anchor" aria-hidden="true" href="#pretrained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained models&lt;/h3&gt;
&lt;p&gt;Download the latest &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-preliminary" class="anchor" aria-hidden="true" href="#preliminary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preliminary&lt;/h3&gt;
&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If all tests pass, you're good to go.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h3&gt;
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="http://www.openslr.org/resources/12/train-clean-100.tar.gz" rel="nofollow"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-toolbox" class="anchor" aria-hidden="true" href="#toolbox"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Toolbox&lt;/h3&gt;
&lt;p&gt;You can then try the toolbox:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br&gt;
or&lt;br&gt;
&lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wiki" class="anchor" aria-hidden="true" href="#wiki"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wiki&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How it all works&lt;/strong&gt; (WIP - &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/How-it-all-works"&gt;stub&lt;/a&gt;, you might be better off reading my thesis until it's done)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training"&gt;&lt;strong&gt;Training models yourself&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training with other data/languages&lt;/strong&gt; (WIP - see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-507864097"&gt;here&lt;/a&gt; for now)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/TODO-&amp;amp;-planned-features"&gt;&lt;strong&gt;TODO and planned features&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributions--issues" class="anchor" aria-hidden="true" href="#contributions--issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions &amp;amp; Issues&lt;/h2&gt;
&lt;p&gt;I'm working full-time as of June 2019. I don't have time to maintain this repo nor reply to issues. Sorry.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CorentinJ</author><guid isPermaLink="false">https://github.com/CorentinJ/Real-Time-Voice-Cloning</guid><pubDate>Tue, 26 Nov 2019 00:22:00 GMT</pubDate></item><item><title>kivy/python-for-android #23 in All Languages, Today</title><link>https://github.com/kivy/python-for-android</link><description>&lt;p&gt;&lt;i&gt;Turn your Python application into an Android APK&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python-for-android" class="anchor" aria-hidden="true" href="#python-for-android"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python-for-android&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/kivy/python-for-android" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/76cc4a75f958e3fb2b2fdae6c88d69b6a168bf45/68747470733a2f2f7472617669732d63692e6f72672f6b6976792f707974686f6e2d666f722d616e64726f69642e7376673f6272616e63683d646576656c6f70" alt="Build Status" data-canonical-src="https://travis-ci.org/kivy/python-for-android.svg?branch=develop" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/kivy/python-for-android?branch=develop" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d75d2d2799ea7d31050ff58a9968fbe2e65ffabe/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6b6976792f707974686f6e2d666f722d616e64726f69642f62616467652e7376673f6272616e63683d646576656c6f70266b696c6c5f63616368653d31" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/kivy/python-for-android/badge.svg?branch=develop&amp;amp;kill_cache=1" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="#backers"&gt;&lt;img src="https://camo.githubusercontent.com/682311df432264b77af4c8ad4f54d09f8be0e4d9/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f6261636b6572732f62616467652e737667" alt="Backers on Open Collective" data-canonical-src="https://opencollective.com/kivy/backers/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="#sponsors"&gt;&lt;img src="https://camo.githubusercontent.com/cc42b96fd548bf357f9547cbc325cd900b17537d/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f73706f6e736f72732f62616467652e737667" alt="Sponsors on Open Collective" data-canonical-src="https://opencollective.com/kivy/sponsors/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;python-for-android is a packaging tool for Python apps on Android. You can
create your own Python distribution including the modules and
dependencies you want, and bundle it in an APK along with your own code.&lt;/p&gt;
&lt;p&gt;Features include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support for building with both Python 2 and Python 3.&lt;/li&gt;
&lt;li&gt;Different app backends including Kivy, PySDL2, and a WebView with
Python webserver.&lt;/li&gt;
&lt;li&gt;Automatic support for most pure Python modules, and built in support
for many others, including popular dependencies such as numpy and
sqlalchemy.&lt;/li&gt;
&lt;li&gt;Multiple architecture targets, for APKs optimised on any given
device.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For documentation and support, see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Website: &lt;a href="http://python-for-android.readthedocs.io" rel="nofollow"&gt;http://python-for-android.readthedocs.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mailing list: &lt;a href="https://groups.google.com/forum/#!forum/kivy-users" rel="nofollow"&gt;https://groups.google.com/forum/#!forum/kivy-users&lt;/a&gt; or
&lt;a href="https://groups.google.com/forum/#!forum/python-android" rel="nofollow"&gt;https://groups.google.com/forum/#!forum/python-android&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;Follow the &lt;a href="https://python-for-android.readthedocs.org/en/latest/quickstart/" rel="nofollow"&gt;quickstart
instructions&lt;/a&gt;
to install and begin creating APKs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quick instructions&lt;/strong&gt;: install python-for-android with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install python-for-android
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(for the develop branch: &lt;code&gt;pip install git+https://github.com/kivy/python-for-android.git&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;Test that the install works with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p4a --version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To build any actual apps, &lt;strong&gt;set up the Android SDK and NDK&lt;/strong&gt;
as described in the &lt;a href="https://python-for-android.readthedocs.org/en/latest/quickstart/#installing-android-sdk" rel="nofollow"&gt;quickstart&lt;/a&gt;.
&lt;strong&gt;Use the SDK/NDK API level &amp;amp; NDK version as in the quickstart,&lt;/strong&gt;
other API levels may not work.&lt;/p&gt;
&lt;p&gt;With everything installed, build an APK with SDL2 with e.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p4a apk --requirements=kivy --private /home/username/devel/planewave_frozen/ --package=net.inclem.planewavessdl2 --name="planewavessdl2" --version=0.5 --bootstrap=sdl2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;For full instructions and parameter options,&lt;/strong&gt; see &lt;a href="https://python-for-android.readthedocs.io/en/latest/quickstart/#usage" rel="nofollow"&gt;the
documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h2&gt;
&lt;p&gt;If you need assistance, you can ask for help on our mailing list:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User Group: &lt;a href="https://groups.google.com/group/kivy-users" rel="nofollow"&gt;https://groups.google.com/group/kivy-users&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Email: &lt;a href="mailto:kivy-users@googlegroups.com"&gt;kivy-users@googlegroups.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also have &lt;a href="https://chat.kivy.org/" rel="nofollow"&gt;#support Discord channel&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We love pull requests and discussing novel ideas. Check out the Kivy
project &lt;a href="http://kivy.org/docs/contribute.html" rel="nofollow"&gt;contribution guide&lt;/a&gt; and
feel free to improve python-for-android.&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://python-for-android.readthedocs.io/en/latest/contribute/" rel="nofollow"&gt;our
documentation&lt;/a&gt;
for more information about the python-for-android development and
release model, but don't worry about the details. You just need to
make a pull request, we'll take care of the rest.&lt;/p&gt;
&lt;p&gt;The following mailing list and IRC channel are used exclusively for
discussions about developing the Kivy framework and its sister projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dev Group: &lt;a href="https://groups.google.com/group/kivy-dev" rel="nofollow"&gt;https://groups.google.com/group/kivy-dev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Email: &lt;a href="mailto:kivy-dev@googlegroups.com"&gt;kivy-dev@googlegroups.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also have &lt;a href="https://chat.kivy.org/" rel="nofollow"&gt;#dev Discord channel&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;python-for-android is released under the terms of the MIT License.
Please refer to the LICENSE file.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-history" class="anchor" aria-hidden="true" href="#history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;History&lt;/h2&gt;
&lt;p&gt;In 2015 these tools were rewritten to provide a new, easier-to-use and
easier-to-extend interface. If you'd like to browse the old toolchain, its
status is recorded for posterity at at
&lt;a href="https://github.com/kivy/python-for-android/tree/old_toolchain"&gt;https://github.com/kivy/python-for-android/tree/old_toolchain&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the last quarter of 2018 the python recipes were changed. The
new recipe for python3 (3.7.1) had a new build system which was
applied to the ancient python recipe, allowing us to bump the python2
version number to 2.7.15. This change unified the build process for
both python recipes, and probably solved various issues detected over the
years. It should also be mentioned that these &lt;strong&gt;unified python recipes&lt;/strong&gt;
require a &lt;strong&gt;minimum target api level of 21&lt;/strong&gt;,
&lt;em&gt;Android 5.0 - Lollipop&lt;/em&gt;, so in the case that you need to build targeting an
api level below 21, you should use an older version of python-for-android
(&amp;lt;=0.7.1).&lt;/p&gt;
&lt;p&gt;Be aware that this project is in constant development so, as per time of writing,
you should use a minimum on Android's NDK r19, and &lt;code&gt;we recommend using NDK r19b&lt;/code&gt;.
This is because the toolchains installed by
default with the NDK can be used &lt;em&gt;in-place&lt;/em&gt; and the python-for-android project
has been adapted for that feature. Also be aware that more recent versions of the
Android's NDK may not work.&lt;/p&gt;
&lt;p&gt;Those mentioned changes has been done this way to make easier the transition
between python3 and python2. We will slowly phase out python2 support
towards 2020...so...if you are using python2 in your projects you should
consider migrating it into python3.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;This project exists thanks to all the people who contribute. [&lt;a href="CONTRIBUTING.md"&gt;Contribute&lt;/a&gt;].
&lt;a href="https://github.com/kivy/python-for-android/graphs/contributors"&gt;&lt;img src="https://camo.githubusercontent.com/dc0fdc58ccfa3e4a9388bbe0487108ebc43cf3f1/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f636f6e7472696275746f72732e7376673f77696474683d38393026627574746f6e3d66616c7365" data-canonical-src="https://opencollective.com/kivy/contributors.svg?width=890&amp;amp;button=false" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-backers" class="anchor" aria-hidden="true" href="#backers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Backers&lt;/h2&gt;
&lt;p&gt;Thank you to all our backers! &lt;g-emoji class="g-emoji" alias="pray" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64f.png"&gt;🙏&lt;/g-emoji&gt; [&lt;a href="https://opencollective.com/kivy#backer" rel="nofollow"&gt;Become a backer&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://opencollective.com/kivy#backers" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/7750c1699d32eb16c828dd4fb5d07512ebf5acc5/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f6261636b6572732e7376673f77696474683d383930" data-canonical-src="https://opencollective.com/kivy/backers.svg?width=890" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sponsors" class="anchor" aria-hidden="true" href="#sponsors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sponsors&lt;/h2&gt;
&lt;p&gt;Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [&lt;a href="https://opencollective.com/kivy#sponsor" rel="nofollow"&gt;Become a sponsor&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://opencollective.com/kivy/sponsor/0/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/669bf398c9c5ae5e91cab39e7a3f2c5a8b8a3599/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f73706f6e736f722f302f6176617461722e737667" data-canonical-src="https://opencollective.com/kivy/sponsor/0/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/kivy/sponsor/1/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/7b138c4d5b9766ccba9a6b1bf13bc79418d728c3/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f73706f6e736f722f312f6176617461722e737667" data-canonical-src="https://opencollective.com/kivy/sponsor/1/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/kivy/sponsor/2/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/20196333e29714c4536949ca29a8898d69192f9d/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f73706f6e736f722f322f6176617461722e737667" data-canonical-src="https://opencollective.com/kivy/sponsor/2/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/kivy/sponsor/3/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ecdeae83ce8de155ad556ff5ae27ff1747774f94/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f73706f6e736f722f332f6176617461722e737667" data-canonical-src="https://opencollective.com/kivy/sponsor/3/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/kivy/sponsor/4/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/59a94e83e2fa5382fc732a4ed707e3cffb24ccc3/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f73706f6e736f722f342f6176617461722e737667" data-canonical-src="https://opencollective.com/kivy/sponsor/4/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/kivy/sponsor/5/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/932bd2ec5c7aae2d9f8e80d6cc21418596aed9f5/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f73706f6e736f722f352f6176617461722e737667" data-canonical-src="https://opencollective.com/kivy/sponsor/5/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/kivy/sponsor/6/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/84920a851db0cdce26a7851b8e484008cf94f0ce/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f73706f6e736f722f362f6176617461722e737667" data-canonical-src="https://opencollective.com/kivy/sponsor/6/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/kivy/sponsor/7/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/62cba688c9a63a6016f90e03b87b19aff29d12b9/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f73706f6e736f722f372f6176617461722e737667" data-canonical-src="https://opencollective.com/kivy/sponsor/7/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/kivy/sponsor/8/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5e4d1f6776a8a20688156d5c294a8b5a905229f2/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f73706f6e736f722f382f6176617461722e737667" data-canonical-src="https://opencollective.com/kivy/sponsor/8/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/kivy/sponsor/9/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c7230ca023e3b06df8a65ac4c290fbb83d2667d0/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6b6976792f73706f6e736f722f392f6176617461722e737667" data-canonical-src="https://opencollective.com/kivy/sponsor/9/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>kivy</author><guid isPermaLink="false">https://github.com/kivy/python-for-android</guid><pubDate>Tue, 26 Nov 2019 00:23:00 GMT</pubDate></item><item><title>AnkerLeng/Cpp-0-1-Resource #24 in All Languages, Today</title><link>https://github.com/AnkerLeng/Cpp-0-1-Resource</link><description>&lt;p&gt;&lt;i&gt;C++ 匠心之作 从0到1入门资料&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-c-匠心之作-从0到1入门资料" class="anchor" aria-hidden="true" href="#c-匠心之作-从0到1入门资料"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;C++ 匠心之作 从0到1入门资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="%E7%AC%AC1%E9%98%B6%E6%AE%B5C%2B%2B%20%E5%8C%A0%E5%BF%83%E4%B9%8B%E4%BD%9C%20%E4%BB%8E0%E5%88%B01%E5%85%A5%E9%97%A8/C%2B%2B%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E8%AE%B2%E4%B9%89/C%2B%2B%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8.md"&gt;第1阶段C++ 匠心之作 从0到1入门&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="%E7%AC%AC2%E9%98%B6%E6%AE%B5%E5%AE%9E%E6%88%98-%E9%80%9A%E8%AE%AF%E5%BD%95%E7%AE%A1%E7%90%86/%E9%80%9A%E8%AE%AF%E5%BD%95%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AE%B2%E4%B9%89/%E9%80%9A%E8%AE%AF%E5%BD%95%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F.md"&gt;第2阶段实战-通讯录管理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="%E7%AC%AC3%E9%98%B6%E6%AE%B5-C%2B%2B%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B%20%E8%B5%84%E6%96%99/%E8%AE%B2%E4%B9%89/C%2B%2B%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B.md"&gt;第3阶段-C++核心编程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="%E7%AC%AC4%E9%98%B6%E6%AE%B5%E5%AE%9E%E6%88%98-%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%80%81%E7%9A%84%E4%BC%81%E4%B8%9A%E8%81%8C%E5%B7%A5%E7%B3%BB%E7%BB%9F/%E8%AE%B2%E4%B9%89/%E8%81%8C%E5%B7%A5%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F.md"&gt;第4阶段实战-基于多态的企业职工系统&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="%E7%AC%AC5%E9%98%B6%E6%AE%B5-C%2B%2B%E6%8F%90%E9%AB%98%E7%BC%96%E7%A8%8B%E8%B5%84%E6%96%99/%E6%8F%90%E9%AB%98%E7%BC%96%E7%A8%8B%E8%83%BD%E5%8A%9B%E8%B5%84%E6%96%99/%E8%AE%B2%E4%B9%89/C%2B%2B%E6%8F%90%E9%AB%98%E7%BC%96%E7%A8%8B.md"&gt;第5阶段-C++提高编程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="%E7%AC%AC6%E9%98%B6%E6%AE%B5%E5%AE%9E%E6%88%98-%E5%9F%BA%E4%BA%8ESTL%E6%B3%9B%E5%8C%96%E7%BC%96%E7%A8%8B%E7%9A%84%E6%BC%94%E8%AE%B2%E6%AF%94%E8%B5%9B%E8%B5%84%E6%96%99/%E8%B5%84%E6%96%99/%E8%AE%B2%E4%B9%89/%E5%9F%BA%E4%BA%8ESTL%E7%9A%84%E6%BC%94%E8%AE%B2%E6%AF%94%E8%B5%9B%E6%B5%81%E7%A8%8B%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F.md"&gt;第6阶段实战-基于STL泛化编程的演讲比赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="%E7%AC%AC7%E9%98%B6%E6%AE%B5-C%2B%2B%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE%E6%9C%BA%E6%88%BF%E9%A2%84%E7%BA%A6%E8%B5%84%E6%96%99/%E6%9C%BA%E6%88%BF%E9%A2%84%E7%BA%A6%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%96%99/%E8%AE%B2%E4%B9%89/%E6%9C%BA%E6%88%BF%E9%A2%84%E7%BA%A6%E7%B3%BB%E7%BB%9F.md"&gt;第7阶段-C++实战项目机房预约&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="%E9%99%84-C%2B%2B%E7%BC%96%E7%A8%8B%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/%E6%95%99%E7%A8%8B%E6%96%87%E4%BB%B6/C%2B%2B%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.md"&gt;附-C++编程环境搭建教程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-视频教程" class="anchor" aria-hidden="true" href="#视频教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;视频教程&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.bilibili.com/video/av41559729/" rel="nofollow"&gt;https://www.bilibili.com/video/av41559729/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-交流群" class="anchor" aria-hidden="true" href="#交流群"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;交流群&lt;/h2&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>AnkerLeng</author><guid isPermaLink="false">https://github.com/AnkerLeng/Cpp-0-1-Resource</guid><pubDate>Tue, 26 Nov 2019 00:24:00 GMT</pubDate></item><item><title>sprintcube/docker-compose-lamp #25 in All Languages, Today</title><link>https://github.com/sprintcube/docker-compose-lamp</link><description>&lt;p&gt;&lt;i&gt;A basic LAMP stack environment built using Docker Compose.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-lamp-stack-built-with-docker-compose" class="anchor" aria-hidden="true" href="#lamp-stack-built-with-docker-compose"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LAMP stack built with Docker Compose&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/dae623129638d0fd71f343a4757ea5f6f1fb266c/68747470733a2f2f707265766965772e6962622e636f2f674f546130792f4c414d505f535441434b2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/dae623129638d0fd71f343a4757ea5f6f1fb266c/68747470733a2f2f707265766965772e6962622e636f2f674f546130792f4c414d505f535441434b2e706e67" alt="Landing Page" data-canonical-src="https://preview.ibb.co/gOTa0y/LAMP_STACK.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a basic LAMP stack environment built using Docker Compose. It consists following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PHP&lt;/li&gt;
&lt;li&gt;Apache&lt;/li&gt;
&lt;li&gt;MySQL&lt;/li&gt;
&lt;li&gt;phpMyAdmin&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of now, we have different branches for different PHP versions. Use appropriate branch as per your php version need:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/sprintcube/docker-compose-lamp/tree/5.6.x"&gt;5.6.x&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sprintcube/docker-compose-lamp/tree/7.1.x"&gt;7.1.x&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sprintcube/docker-compose-lamp/tree/7.2.x"&gt;7.2.x&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sprintcube/docker-compose-lamp/tree/7.3.x"&gt;7.3.x&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;Clone this repository on your local computer and checkout the appropriate branch e.g. 7.3.x. Run the &lt;code&gt;docker-compose up -d&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/sprintcube/docker-compose-lamp.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; docker-compose-lamp/
git fetch --all
git checkout 7.3.x
cp sample.env .env
docker-compose up -d&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Your LAMP stack is now ready!! You can access it via &lt;code&gt;http://localhost&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-configuration-and-usage" class="anchor" aria-hidden="true" href="#configuration-and-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration and Usage&lt;/h2&gt;
&lt;p&gt;Please read from appropriate version branch.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sprintcube</author><guid isPermaLink="false">https://github.com/sprintcube/docker-compose-lamp</guid><pubDate>Tue, 26 Nov 2019 00:25:00 GMT</pubDate></item></channel></rss>