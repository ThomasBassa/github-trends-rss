<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Scala, Today</title><link>https://github.com/trending/scala?since=daily</link><description>The top repositories on GitHub for scala, measured daily</description><pubDate>Sat, 02 Nov 2019 02:38:39 GMT</pubDate><lastBuildDate>Sat, 02 Nov 2019 02:38:39 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>apache/spark #1 in Scala, Today</title><link>https://github.com/apache/spark</link><description>&lt;p&gt;&lt;i&gt;Apache Spark&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-apache-spark" class="anchor" aria-hidden="true" href="#apache-spark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Apache Spark&lt;/h1&gt;
&lt;p&gt;Spark is a unified analytics engine for large-scale data processing. It provides
high-level APIs in Scala, Java, Python, and R, and an optimized engine that
supports general computation graphs for data analysis. It also supports a
rich set of higher-level tools including Spark SQL for SQL and DataFrames,
MLlib for machine learning, GraphX for graph processing,
and Structured Streaming for stream processing.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://spark.apache.org/" rel="nofollow"&gt;https://spark.apache.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cd70373593f382473fbf1fb17a08856e918039d4/68747470733a2f2f616d706c61622e63732e6265726b656c65792e6564752f6a656e6b696e732f6a6f622f737061726b2d6d61737465722d746573742d7362742d6861646f6f702d322e372f62616467652f69636f6e" alt="Jenkins Build" data-canonical-src="https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d3336700f9c89f8132189f928e567a2c104503af/68747470733a2f2f696d672e736869656c64732e696f2f6170707665796f722f63692f417061636865536f667477617265466f756e646174696f6e2f737061726b2f6d61737465722e7376673f7374796c653d706c6173746963266c6f676f3d6170707665796f72" alt="AppVeyor Build" data-canonical-src="https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&amp;amp;logo=appveyor" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://spark-test.github.io/pyspark-coverage-site" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2aa88140c358afddfa651b98a462bd69c4af7eb8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f64796e616d69632f786d6c2e7376673f6c6162656c3d7079737061726b253230636f7665726167652675726c3d6874747073253341253246253246737061726b2d746573742e6769746875622e696f2532467079737061726b2d636f7665726167652d736974652671756572793d25324668746d6c253246626f64792532466469762535423125354425324664697625324668312532467370616e26636f6c6f72423d627269676874677265656e267374796c653d706c6173746963" alt="PySpark Coverage" data-canonical-src="https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&amp;amp;url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&amp;amp;query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&amp;amp;colorB=brightgreen&amp;amp;style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-online-documentation" class="anchor" aria-hidden="true" href="#online-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online Documentation&lt;/h2&gt;
&lt;p&gt;You can find the latest Spark documentation, including a programming
guide, on the &lt;a href="https://spark.apache.org/documentation.html" rel="nofollow"&gt;project web page&lt;/a&gt;.
This README file only contains basic setup instructions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-building-spark" class="anchor" aria-hidden="true" href="#building-spark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building Spark&lt;/h2&gt;
&lt;p&gt;Spark is built using &lt;a href="https://maven.apache.org/" rel="nofollow"&gt;Apache Maven&lt;/a&gt;.
To build Spark and its example programs, run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./build/mvn -DskipTests clean package
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(You do not need to do this if you downloaded a pre-built package.)&lt;/p&gt;
&lt;p&gt;You can build Spark using more than one thread by using the -T option with Maven, see &lt;a href="https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3" rel="nofollow"&gt;"Parallel builds in Maven 3"&lt;/a&gt;.
More detailed documentation is available from the project site, at
&lt;a href="https://spark.apache.org/docs/latest/building-spark.html" rel="nofollow"&gt;"Building Spark"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For general development tips, including info on developing Spark using an IDE, see &lt;a href="https://spark.apache.org/developer-tools.html" rel="nofollow"&gt;"Useful Developer Tools"&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-interactive-scala-shell" class="anchor" aria-hidden="true" href="#interactive-scala-shell"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Interactive Scala Shell&lt;/h2&gt;
&lt;p&gt;The easiest way to start using Spark is through the Scala shell:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./bin/spark-shell
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Try the following command, which should return 1,000,000,000:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scala&amp;gt; spark.range(1000 * 1000 * 1000).count()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-interactive-python-shell" class="anchor" aria-hidden="true" href="#interactive-python-shell"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Interactive Python Shell&lt;/h2&gt;
&lt;p&gt;Alternatively, if you prefer Python, you can use the Python shell:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./bin/pyspark
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And run the following command, which should also return 1,000,000,000:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; spark.range(1000 * 1000 * 1000).count()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-example-programs" class="anchor" aria-hidden="true" href="#example-programs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example Programs&lt;/h2&gt;
&lt;p&gt;Spark also comes with several sample programs in the &lt;code&gt;examples&lt;/code&gt; directory.
To run one of them, use &lt;code&gt;./bin/run-example &amp;lt;class&amp;gt; [params]&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./bin/run-example SparkPi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will run the Pi example locally.&lt;/p&gt;
&lt;p&gt;You can set the MASTER environment variable when running examples to submit
examples to a cluster. This can be a mesos:// or spark:// URL,
"yarn" to run on YARN, and "local" to run
locally with one thread, or "local[N]" to run locally with N threads. You
can also use an abbreviated class name if the class is in the &lt;code&gt;examples&lt;/code&gt;
package. For instance:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MASTER=spark://host:7077 ./bin/run-example SparkPi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Many of the example programs print usage help if no params are given.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-tests" class="anchor" aria-hidden="true" href="#running-tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Tests&lt;/h2&gt;
&lt;p&gt;Testing first requires &lt;a href="#building-spark"&gt;building Spark&lt;/a&gt;. Once Spark is built, tests
can be run using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./dev/run-tests
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please see the guidance on how to
&lt;a href="https://spark.apache.org/developer-tools.html#individual-tests" rel="nofollow"&gt;run tests for a module, or individual tests&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-a-note-about-hadoop-versions" class="anchor" aria-hidden="true" href="#a-note-about-hadoop-versions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A Note About Hadoop Versions&lt;/h2&gt;
&lt;p&gt;Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported
storage systems. Because the protocols have changed in different versions of
Hadoop, you must build Spark against the same version that your cluster runs.&lt;/p&gt;
&lt;p&gt;Please refer to the build documentation at
&lt;a href="https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn" rel="nofollow"&gt;"Specifying the Hadoop Version and Enabling YARN"&lt;/a&gt;
for detailed guidance on building for a particular distribution of Hadoop, including
building for particular Hive and Hive Thriftserver distributions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-configuration" class="anchor" aria-hidden="true" href="#configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
&lt;p&gt;Please refer to the &lt;a href="https://spark.apache.org/docs/latest/configuration.html" rel="nofollow"&gt;Configuration Guide&lt;/a&gt;
in the online documentation for an overview on how to configure Spark.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;Please review the &lt;a href="https://spark.apache.org/contributing.html" rel="nofollow"&gt;Contribution to Spark guide&lt;/a&gt;
for information on how to get started contributing to the project.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>apache</author><guid isPermaLink="false">https://github.com/apache/spark</guid><pubDate>Sat, 02 Nov 2019 00:01:00 GMT</pubDate></item><item><title>jdegoes/zio-intro-game #2 in Scala, Today</title><link>https://github.com/jdegoes/zio-intro-game</link><description>&lt;p&gt;&lt;i&gt;An introduction to ZIO using game programming!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-zio-intro-game" class="anchor" aria-hidden="true" href="#zio-intro-game"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;zio-intro-game&lt;/h1&gt;
&lt;p&gt;An introduction to ZIO using Game Programming!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-from-the-ui" class="anchor" aria-hidden="true" href="#from-the-ui"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;From the UI&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download the repository as a &lt;a href="https://github.com/jdegoes/zio-intro-game/archive/master.zip"&gt;zip archive&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Unzip the archive, usually by double-clicking on the file.&lt;/li&gt;
&lt;li&gt;Configure the source code files in the IDE or text editor of your choice.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-from-the-command-line" class="anchor" aria-hidden="true" href="#from-the-command-line"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;From the Command Line&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Open up a terminal window.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Clone the repository.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone git@github.com:jdegoes/zio-intro-game.git&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launch &lt;code&gt;sbt&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;./sbt&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Enter continuous compilation mode.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sbt:zio-intro-game&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;~&lt;/span&gt; test:compile&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-legal" class="anchor" aria-hidden="true" href="#legal"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Legal&lt;/h1&gt;
&lt;p&gt;Copyright© 2019 John A. De Goes. All rights reserved.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jdegoes</author><guid isPermaLink="false">https://github.com/jdegoes/zio-intro-game</guid><pubDate>Sat, 02 Nov 2019 00:02:00 GMT</pubDate></item><item><title>yahoo/kafka-manager #3 in Scala, Today</title><link>https://github.com/yahoo/kafka-manager</link><description>&lt;p&gt;&lt;i&gt;A tool for managing Apache Kafka.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-kafka-manager" class="anchor" aria-hidden="true" href="#kafka-manager"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kafka Manager&lt;/h1&gt;
&lt;p&gt;A tool for managing &lt;a href="http://kafka.apache.org" rel="nofollow"&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It supports the following :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manage multiple clusters&lt;/li&gt;
&lt;li&gt;Easy inspection of cluster state (topics, consumers, offsets, brokers, replica distribution, partition distribution)&lt;/li&gt;
&lt;li&gt;Run preferred replica election&lt;/li&gt;
&lt;li&gt;Generate partition assignments with option to select brokers to use&lt;/li&gt;
&lt;li&gt;Run reassignment of partition (based on generated assignments)&lt;/li&gt;
&lt;li&gt;Create a topic with optional topic configs (0.8.1.1 has different configs than 0.8.2+)&lt;/li&gt;
&lt;li&gt;Delete topic (only supported on 0.8.2+ and remember set delete.topic.enable=true in broker config)&lt;/li&gt;
&lt;li&gt;Topic list now indicates topics marked for deletion (only supported on 0.8.2+)&lt;/li&gt;
&lt;li&gt;Batch generate partition assignments for multiple topics with option to select brokers to use&lt;/li&gt;
&lt;li&gt;Batch run reassignment of partition for multiple topics&lt;/li&gt;
&lt;li&gt;Add partitions to existing topic&lt;/li&gt;
&lt;li&gt;Update config for existing topic&lt;/li&gt;
&lt;li&gt;Optionally enable JMX polling for broker level and topic level metrics.&lt;/li&gt;
&lt;li&gt;Optionally filter out consumers that do not have ids/ owners/ &amp;amp; offsets/ directories in zookeeper.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cluster Management&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/img/cluster.png"&gt;&lt;img src="/img/cluster.png" alt="cluster" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Topic List&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/img/topic-list.png"&gt;&lt;img src="/img/topic-list.png" alt="topic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Topic View&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/img/topic.png"&gt;&lt;img src="/img/topic.png" alt="topic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Consumer List View&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/img/consumer-list.png"&gt;&lt;img src="/img/consumer-list.png" alt="consumer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Consumed Topic View&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/img/consumed-topic.png"&gt;&lt;img src="/img/consumed-topic.png" alt="consumer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Broker List&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/img/broker-list.png"&gt;&lt;img src="/img/broker-list.png" alt="broker" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Broker View&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/img/broker.png"&gt;&lt;img src="/img/broker.png" alt="broker" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://kafka.apache.org/downloads.html" rel="nofollow"&gt;Kafka 0.8.&lt;em&gt;.&lt;/em&gt; or 0.9.&lt;em&gt;.&lt;/em&gt; or 0.10.&lt;em&gt;.&lt;/em&gt; or 0.11.&lt;em&gt;.&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Java 8+&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-configuration" class="anchor" aria-hidden="true" href="#configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
&lt;p&gt;The minimum configuration is the zookeeper hosts which are to be used for kafka manager state.
This can be found in the application.conf file in conf directory.  The same file will be packaged
in the distribution zip file; you may modify settings after unzipping the file on the desired server.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kafka-manager.zkhosts="my.zookeeper.host.com:2181"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can specify multiple zookeeper hosts by comma delimiting them, like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kafka-manager.zkhosts="my.zookeeper.host.com:2181,other.zookeeper.host.com:2181"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, use the environment variable &lt;code&gt;ZK_HOSTS&lt;/code&gt; if you don't want to hardcode any values.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ZK_HOSTS="my.zookeeper.host.com:2181"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can optionally enable/disable the following functionality by modifying the default list in application.conf :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;application.features=["KMClusterManagerFeature","KMTopicManagerFeature","KMPreferredReplicaElectionFeature","KMReassignPartitionsFeature"]
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;KMClusterManagerFeature - allows adding, updating, deleting cluster from Kafka Manager&lt;/li&gt;
&lt;li&gt;KMTopicManagerFeature - allows adding, updating, deleting topic from a Kafka cluster&lt;/li&gt;
&lt;li&gt;KMPreferredReplicaElectionFeature - allows running of preferred replica election for a Kafka cluster&lt;/li&gt;
&lt;li&gt;KMReassignPartitionsFeature - allows generating partition assignments and reassigning partitions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Consider setting these parameters for larger clusters with jmx enabled :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kafka-manager.broker-view-thread-pool-size=&amp;lt; 3 * number_of_brokers&amp;gt;&lt;/li&gt;
&lt;li&gt;kafka-manager.broker-view-max-queue-size=&amp;lt; 3 * total # of partitions across all topics&amp;gt;&lt;/li&gt;
&lt;li&gt;kafka-manager.broker-view-update-seconds=&amp;lt; kafka-manager.broker-view-max-queue-size / (10 * number_of_brokers) &amp;gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an example for a kafka cluster with 10 brokers, 100 topics, with each topic having 10 partitions giving 1000 total partitions with JMX enabled :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kafka-manager.broker-view-thread-pool-size=30&lt;/li&gt;
&lt;li&gt;kafka-manager.broker-view-max-queue-size=3000&lt;/li&gt;
&lt;li&gt;kafka-manager.broker-view-update-seconds=30&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The follow control consumer offset cache's thread pool and queue :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kafka-manager.offset-cache-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt;
&lt;li&gt;kafka-manager.offset-cache-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt;
&lt;li&gt;kafka-manager.kafka-admin-client-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt;
&lt;li&gt;kafka-manager.kafka-admin-client-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You should increase the above for large # of consumers with consumer polling enabled.  Though it mainly affects ZK based consumer polling.&lt;/p&gt;
&lt;p&gt;Kafka managed consumer offset is now consumed by KafkaManagedOffsetCache from the "__consumer_offsets" topic.  Note, this has not been tested with large number of offsets being tracked.  There is a single thread per cluster consuming this topic so it may not be able to keep up on large # of offsets being pushed to the topic.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-authenticating-a-user-with-ldap" class="anchor" aria-hidden="true" href="#authenticating-a-user-with-ldap"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authenticating a User with LDAP&lt;/h3&gt;
&lt;p&gt;Warning, you need to have SSL configured with Kafka Manager to ensure your credentials aren't passed unencrypted.
Authenticating a User with LDAP is possible by passing the user credentials with the Authorization header.
LDAP authentication is done on first visit, if successful, a cookie is set.
On next request, the cookie value is compared with credentials from Authorization header.
LDAP support is through the basic authentication filter.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Configure basic authentication&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;basicAuthentication.enabled=true&lt;/li&gt;
&lt;li&gt;basicAuthentication.realm=&amp;lt; basic authentication realm&amp;gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Encryption parameters (optional, otherwise randomly generated on startup) :&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;basicAuthentication.salt="some-hex-string-representing-byte-array"&lt;/li&gt;
&lt;li&gt;basicAuthentication.iv="some-hex-string-representing-byte-array"&lt;/li&gt;
&lt;li&gt;basicAuthentication.secret="my-secret-string"&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Configure LDAP/LDAPS authentication&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;basicAuthentication.ldap.enabled=&amp;lt; Boolean flag to enable/disable ldap authentication &amp;gt;&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.server=&amp;lt; fqdn of LDAP server&amp;gt;&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.port=&amp;lt; port of LDAP server&amp;gt;&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.username=&amp;lt; LDAP search username&amp;gt;&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.password=&amp;lt; LDAP search password&amp;gt;&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.search-base-dn=&amp;lt; LDAP search base&amp;gt;&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.search-filter=&amp;lt; LDAP search filter&amp;gt;&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.connection-pool-size=&amp;lt; number of connection to LDAP server&amp;gt;&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.ssl=&amp;lt; Boolean flag to enable/disable LDAPS&amp;gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;(Optional) Limit access to a specific LDAP Group&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;basicAuthentication.ldap.group-filter=&amp;lt; LDAP group filter&amp;gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-example-online-ldap-test-server" class="anchor" aria-hidden="true" href="#example-online-ldap-test-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example (Online LDAP Test Server):&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;basicAuthentication.ldap.enabled=true&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.server="ldap.forumsys.com"&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.port=389&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.username="cn=read-only-admin,dc=example,dc=com"&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.password="password"&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.search-base-dn="dc=example,dc=com"&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.search-filter="(uid=$capturedLogin$)"&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.group-filter="cn=allowed-group,ou=groups,dc=example,dc=com"&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.connection-pool-size=10&lt;/li&gt;
&lt;li&gt;basicAuthentication.ldap.ssl=false&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-deployment" class="anchor" aria-hidden="true" href="#deployment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deployment&lt;/h2&gt;
&lt;p&gt;The command below will create a zip file which can be used to deploy the application.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./sbt clean dist
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please refer to play framework documentation on &lt;a href="https://www.playframework.com/documentation/2.4.x/ProductionConfiguration" rel="nofollow"&gt;production deployment/configuration&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If java is not in your path, or you need to build against a specific java version,
please use the following (the example assumes oracle java8):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ PATH=/usr/local/oracle-java-8/bin:$PATH \
  JAVA_HOME=/usr/local/oracle-java-8 \
  /path/to/sbt -java-home /usr/local/oracle-java-8 clean dist
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This ensures that the 'java' and 'javac' binaries in your path are first looked up in the
oracle java8 release. Next, for all downstream tools that only listen to JAVA_HOME, it points
them to the oracle java8 location. Lastly, it tells sbt to use the oracle java8 location as
well.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-starting-the-service" class="anchor" aria-hidden="true" href="#starting-the-service"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting the service&lt;/h2&gt;
&lt;p&gt;After extracting the produced zipfile, and changing the working directory to it, you can
run the service like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ bin/kafka-manager
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, it will choose port 9000. This is overridable, as is the location of the
configuration file. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ bin/kafka-manager -Dconfig.file=/path/to/application.conf -Dhttp.port=8080
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, if java is not in your path, or you need to run against a different version of java,
add the -java-home option as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ bin/kafka-manager -java-home /usr/local/oracle-java-8
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-starting-the-service-with-security" class="anchor" aria-hidden="true" href="#starting-the-service-with-security"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting the service with Security&lt;/h2&gt;
&lt;p&gt;To add JAAS configuration for SASL, add the config file location at start:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ bin/kafka-manager -Djava.security.auth.login.config=/path/to/my-jaas.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: Make sure the user running kafka manager has read permissions on the jaas config file&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-packaging" class="anchor" aria-hidden="true" href="#packaging"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Packaging&lt;/h2&gt;
&lt;p&gt;If you'd like to create a Debian or RPM package instead, you can run one of:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sbt debian:packageBin

sbt rpm:packageBin
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-credits" class="anchor" aria-hidden="true" href="#credits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credits&lt;/h2&gt;
&lt;p&gt;Logo/favicon used is from &lt;a href="http://kafka.apache.org" rel="nofollow"&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Most of the utils code has been adapted to work with &lt;a href="http://curator.apache.org" rel="nofollow"&gt;Apache Curator&lt;/a&gt; from &lt;a href="http://kafka.apache.org" rel="nofollow"&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Apache Licensed. See accompanying LICENSE file.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>yahoo</author><guid isPermaLink="false">https://github.com/yahoo/kafka-manager</guid><pubDate>Sat, 02 Nov 2019 00:03:00 GMT</pubDate></item><item><title>zio/zio #4 in Scala, Today</title><link>https://github.com/zio/zio</link><description>&lt;p&gt;&lt;i&gt;ZIO — A type-safe, composable library for asynchronous and concurrent programming in Scala&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./ZIO.png"&gt;&lt;img src="./ZIO.png" alt="ZIO Logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CI&lt;/th&gt;
&lt;th&gt;Coverage&lt;/th&gt;
&lt;th&gt;Release&lt;/th&gt;
&lt;th&gt;Issues&lt;/th&gt;
&lt;th&gt;Scaladex&lt;/th&gt;
&lt;th&gt;Discord&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://circleci.com/gh/zio/zio" title="circleci" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b1f4a66f890c9d1895b24b5241ab093b9b50691b/68747470733a2f2f636972636c6563692e636f6d2f67682f7a696f2f7a696f2e7376673f7374796c653d737667" alt="Build Status" title="circleci" data-canonical-src="https://circleci.com/gh/zio/zio.svg?style=svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://codecov.io/gh/zio/zio?branch=master" title="Codecov" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/878e7fde2b2f507c1e11912d0052ba99ac9069e8/68747470733a2f2f636f6465636f762e696f2f67682f7a696f2f7a696f2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" title="Codecov" data-canonical-src="https://codecov.io/gh/zio/zio/coverage.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://oss.sonatype.org/content/repositories/releases/dev/zio/zio_2.12/" title="Sonatype Releases" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b382d867705c5ff62e587294ad310c1c460f6c75/68747470733a2f2f696d672e736869656c64732e696f2f6e657875732f722f68747470732f6f73732e736f6e61747970652e6f72672f6465762e7a696f2f7a696f5f322e31322e737667" alt="Release Artifacts" title="Sonatype Releases" data-canonical-src="https://img.shields.io/nexus/r/https/oss.sonatype.org/dev.zio/zio_2.12.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://isitmaintained.com/project/zio/zio" title="Average time to resolve an issue" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/dd188de6364b90695c3fbedc0d1f2ed5b875ae2d/687474703a2f2f697369746d61696e7461696e65642e636f6d2f62616467652f7265736f6c7574696f6e2f7a696f2f7a696f2e737667" alt="Average time to resolve an issue" title="Average time to resolve an issue" data-canonical-src="http://isitmaintained.com/badge/resolution/zio/zio.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://index.scala-lang.org/zio/zio/zio" title="Scaladex" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/68f9a94760c1bef87c7a4820404bdb089003c5c4/68747470733a2f2f696e6465782e7363616c612d6c616e672e6f72672f7a696f2f7a696f2f7a696f2f6c61746573742e737667" alt="Badge-Scaladex-page" title="Scaladex" data-canonical-src="https://index.scala-lang.org/zio/zio/zio/latest.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://discord.gg/2ccFBr4" title="Discord" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/961c29d1189d8f042d8c93614057a6accea86731/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3632393439313539373037303832373533303f6c6f676f3d646973636f7264" alt="Badge-Discord" title="chat on discord" data-canonical-src="https://img.shields.io/discord/629491597070827530?logo=discord" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-welcome-to-zio" class="anchor" aria-hidden="true" href="#welcome-to-zio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Welcome to ZIO&lt;/h1&gt;
&lt;p&gt;ZIO is a zero-dependency Scala library for asynchronous and concurrent programming.&lt;/p&gt;
&lt;p&gt;Powered by highly-scalable, non-blocking fibers that never waste or leak resources, ZIO lets you build scalable, resilient, and reactive applications that meet the needs of your business.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;High-performance&lt;/strong&gt;. Build scalable applications with 100x the performance of Scala's &lt;code&gt;Future&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type-safe&lt;/strong&gt;. Use the full power of the Scala compiler to catch bugs at compile time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concurrent&lt;/strong&gt;. Easily build concurrent apps without deadlocks, race conditions, or complexity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Asynchronous&lt;/strong&gt;. Write sequential code that looks the same whether it's asynchronous or synchronous.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Resource-safe&lt;/strong&gt;. Build apps that never leak resources (including threads!), even when they fail.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Testable&lt;/strong&gt;. Inject test services into your app for fast, deterministic, and type-safe testing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Resilient&lt;/strong&gt;. Build apps that never lose errors, and which respond to failure locally and flexibly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functional&lt;/strong&gt;. Rapidly compose solutions to complex problems from simple building blocks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To learn more about ZIO, see the following references:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://zio.dev/" rel="nofollow"&gt;Homepage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="./docs/about/contributing.md"&gt;Contributor's Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="LICENSE"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zio/zio/issues"&gt;Issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zio/zio/pulls"&gt;Pull Requests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-sponsors" class="anchor" aria-hidden="true" href="#sponsors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sponsors&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://7mind.io" title="Septimal Mind" rel="nofollow"&gt;&lt;img src="./website/static/img/septimal_mind.svg" alt="Septimal Mind" title="Septimal Mind" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://7mind.io" title="Septimal Mind" rel="nofollow"&gt;Septimal Mind&lt;/a&gt; sponsors work on ZIO Tracing and continuous maintenance.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://softwaremill.com" title="SoftwareMill" rel="nofollow"&gt;&lt;img src="./website/static/img/softwaremill.svg" alt="SoftwareMill" title="SoftwareMill" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://softwaremill.com" title="SoftwareMill" rel="nofollow"&gt;SoftwareMill&lt;/a&gt; generously provides ZIO with paid-for CircleCI build infrastructure.&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-learn-more-on-the-zio-homepage" class="anchor" aria-hidden="true" href="#learn-more-on-the-zio-homepage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zio.dev/" rel="nofollow"&gt;Learn More on the ZIO Homepage&lt;/a&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-code-of-conduct" class="anchor" aria-hidden="true" href="#code-of-conduct"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code of Conduct&lt;/h2&gt;
&lt;p&gt;See the &lt;a href="./docs/about/code_of_conduct.md"&gt;Code of Conduct&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h2&gt;
&lt;p&gt;Come chat with us on &lt;a href="https://discord.gg/2ccFBr4" title="Discord" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/961c29d1189d8f042d8c93614057a6accea86731/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3632393439313539373037303832373533303f6c6f676f3d646973636f7264" alt="Badge-Discord" title="chat on discord" data-canonical-src="https://img.shields.io/discord/629491597070827530?logo=discord" style="max-width:100%;"&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-legal" class="anchor" aria-hidden="true" href="#legal"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Legal&lt;/h3&gt;
&lt;p&gt;Copyright 2017 - 2019 John A. De Goes and the ZIO Contributors. All rights reserved.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zio</author><guid isPermaLink="false">https://github.com/zio/zio</guid><pubDate>Sat, 02 Nov 2019 00:04:00 GMT</pubDate></item><item><title>JohnSnowLabs/spark-nlp #5 in Scala, Today</title><link>https://github.com/JohnSnowLabs/spark-nlp</link><description>&lt;p&gt;&lt;i&gt;State of the Art Natural Language Processing&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-spark-nlp" class="anchor" aria-hidden="true" href="#spark-nlp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Spark NLP&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/JohnSnowLabs/spark-nlp" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/93a5ad94f20fe9f1cca2c6d23a1ae2fc536bdcc9/68747470733a2f2f7472617669732d63692e6f72672f4a6f686e536e6f774c6162732f737061726b2d6e6c702e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JohnSnowLabs/spark-nlp.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://search.maven.org/artifact/com.johnsnowlabs.nlp/spark-nlp_2.11" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/050bb43c04ed81495b99bf62ef12dfbb2c647ed3/68747470733a2f2f6d6176656e2d6261646765732e6865726f6b756170702e636f6d2f6d6176656e2d63656e7472616c2f636f6d2e6a6f686e736e6f776c6162732e6e6c702f737061726b2d6e6c705f322e31312f62616467652e737667" alt="Maven Central" data-canonical-src="https://maven-badges.herokuapp.com/maven-central/com.johnsnowlabs.nlp/spark-nlp_2.11/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/spark-nlp" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/73752d02b6031565b331f7b0b8276e8c59efdc9f/68747470733a2f2f62616467652e667572792e696f2f70792f737061726b2d6e6c702e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/spark-nlp.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/JohnSnowLabs/spark-nlp" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/fe902456e9b75b85cd009cc87a786d425707aa2f/68747470733a2f2f616e61636f6e64612e6f72672f6a6f686e736e6f776c6162732f737061726b2d6e6c702f6261646765732f76657273696f6e2e737667" alt="Anaconda-Cloud" data-canonical-src="https://anaconda.org/johnsnowlabs/spark-nlp/badges/version.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/dc5c93f4ddfa92aaed4ace74e89dbc075f7810c8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d627269676874677265656e2e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;John Snow Labs Spark NLP is a natural language processing library built on top of Apache Spark ML. It provides simple, performant &amp;amp; accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-projects-website" class="anchor" aria-hidden="true" href="#projects-website"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Project's website&lt;/h2&gt;
&lt;p&gt;Take a look at our official Spark NLP page: &lt;a href="http://nlp.johnsnowlabs.com/" rel="nofollow"&gt;http://nlp.johnsnowlabs.com/&lt;/a&gt; for user documentation and examples&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-slack-community-channel" class="anchor" aria-hidden="true" href="#slack-community-channel"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Slack community channel&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://join.slack.com/t/spark-nlp/shared_invite/enQtNjA4MTE2MDI1MDkxLWVjNWUzOGNlODg1Y2FkNGEzNDQ1NDJjMjc3Y2FkOGFmN2Q3ODIyZGVhMzU0NGM3NzRjNDkyZjZlZTQ0YzY1N2I" rel="nofollow"&gt;Join Slack&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#usage"&gt;Using Spark NLP&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#apache-spark-support"&gt;Apache Spark Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#spark-packages"&gt;Spark Packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#compiled-jars"&gt;Compiled JARs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#scala"&gt;Scala&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#maven"&gt;Maven&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sbt"&gt;SBT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#python"&gt;Python&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#pipconda"&gt;Pip/Conda&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#apache-zeppelin"&gt;Apache Zeppelin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#jupyter-notebook-python"&gt;Jupyter Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#google-colab-notebook"&gt;Google Colab Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#s3-cluster"&gt;S3 Cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ocr-module"&gt;OCR Module&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#eval-module"&gt;Eval Module&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#pipelines-and-models"&gt;Pipelines &amp;amp; Models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#pipelines"&gt;Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#models"&gt;Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#troubleshooting"&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#aknowledgments"&gt;Aknowledgments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-apache-spark-support" class="anchor" aria-hidden="true" href="#apache-spark-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Apache Spark Support&lt;/h2&gt;
&lt;p&gt;Spark NLP &lt;em&gt;2.3.1&lt;/em&gt; has been built on top of Apache Spark 2.4.4&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Spark NLP&lt;/th&gt;
&lt;th&gt;Spark 2.3.x&lt;/th&gt;
&lt;th&gt;Spark 2.4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2.x.x&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.8.x&lt;/td&gt;
&lt;td&gt;Partially&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.7.3&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.6.3&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.5.0&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Find out more about &lt;code&gt;Spark NLP&lt;/code&gt; versions from our &lt;a href="https://github.com/JohnSnowLabs/spark-nlp/releases"&gt;release notes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; that pre-build Spark NLP is not retrocompatible with older Spark 2.x.x, so models and environments might not work.&lt;/p&gt;
&lt;p&gt;If you are still stuck on Spark 2.x.x, you should re-build the library yourself with the desired Apache Spark version. Feel free to use &lt;a href="https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-2.3.2-nlp-assembly-1.8.0.jar" rel="nofollow"&gt;this assembly jar&lt;/a&gt; for such version.
For OCR module, &lt;a href="https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-2.3.2-nlp-ocr-assembly-1.8.0.jar" rel="nofollow"&gt;this&lt;/a&gt; is for spark &lt;code&gt;2.3.x&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-spark-packages" class="anchor" aria-hidden="true" href="#spark-packages"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Spark Packages&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-command-line-requires-internet-connection" class="anchor" aria-hidden="true" href="#command-line-requires-internet-connection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Command line (requires internet connection)&lt;/h3&gt;
&lt;p&gt;This library has been uploaded to the &lt;a href="https://spark-packages.org/package/JohnSnowLabs/spark-nlp" rel="nofollow"&gt;spark-packages repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Benefit of spark-packages is that makes it available for both Scala-Java and Python&lt;/p&gt;
&lt;p&gt;To use the most recent version just add the &lt;code&gt;--packages JohnSnowLabs:spark-nlp:2.3.1&lt;/code&gt; to you spark command&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;spark-shell --packages JohnSnowLabs:spark-nlp:2.3.1&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pyspark --packages JohnSnowLabs:spark-nlp:2.3.1&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;spark-submit --packages JohnSnowLabs:spark-nlp:2.3.1&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This can also be used to create a SparkSession manually by using the &lt;code&gt;spark.jars.packages&lt;/code&gt; option in both Python and Scala&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-compiled-jars" class="anchor" aria-hidden="true" href="#compiled-jars"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compiled JARs&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-build-from-source" class="anchor" aria-hidden="true" href="#build-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build from source&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-spark-nlp-1" class="anchor" aria-hidden="true" href="#spark-nlp-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;spark-nlp&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;FAT-JAR for CPU&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sbt assembly&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;FAT-JAR for GPU&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sbt -Dis_gpu=true assembly&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Packaging the project&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sbt package&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-spark-nlp-ocr" class="anchor" aria-hidden="true" href="#spark-nlp-ocr"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;spark-nlp-ocr&lt;/h4&gt;
&lt;p&gt;Requires native Tesseract 4.x+ for image based OCR. Does not require Spark NLP to work but highly suggested&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;FAT-JAR&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sbt ocr/assembly&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Packaging the project&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sbt ocr/package&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-spark-nlp-eval" class="anchor" aria-hidden="true" href="#spark-nlp-eval"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;spark-nlp-eval&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;FAT-JAR for Eval&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sbt evaluation/assembly&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Packaging the project&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sbt evaluation/package&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-using-the-jar-manually" class="anchor" aria-hidden="true" href="#using-the-jar-manually"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using the jar manually&lt;/h3&gt;
&lt;p&gt;If for some reason you need to use the JAR, you can either download the Fat JARs provided here or download it from &lt;a href="https://mvnrepository.com/artifact/com.johnsnowlabs.nlp" rel="nofollow"&gt;Maven Central&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To add JARs to spark programs use the &lt;code&gt;--jars&lt;/code&gt; option:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;spark-shell --jars spark-nlp.jar&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The preferred way to use the library when running spark programs is using the &lt;code&gt;--packages&lt;/code&gt; option as specified in the &lt;code&gt;spark-packages&lt;/code&gt; section.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-scala" class="anchor" aria-hidden="true" href="#scala"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Scala&lt;/h2&gt;
&lt;p&gt;Our package is deployed to maven central. In order to add this package as a dependency in your application:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-maven" class="anchor" aria-hidden="true" href="#maven"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Maven&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;spark-nlp:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-text-xml"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;&amp;lt;!--&lt;/span&gt; https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp &lt;span class="pl-c"&gt;--&amp;gt;&lt;/span&gt;&lt;/span&gt;
&amp;lt;&lt;span class="pl-ent"&gt;dependency&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;groupId&lt;/span&gt;&amp;gt;com.johnsnowlabs.nlp&amp;lt;/&lt;span class="pl-ent"&gt;groupId&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;artifactId&lt;/span&gt;&amp;gt;spark-nlp_2.11&amp;lt;/&lt;span class="pl-ent"&gt;artifactId&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;version&lt;/span&gt;&amp;gt;2.3.1&amp;lt;/&lt;span class="pl-ent"&gt;version&lt;/span&gt;&amp;gt;
&amp;lt;/&lt;span class="pl-ent"&gt;dependency&lt;/span&gt;&amp;gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-text-xml"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;&amp;lt;!--&lt;/span&gt; https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu &lt;span class="pl-c"&gt;--&amp;gt;&lt;/span&gt;&lt;/span&gt;
&amp;lt;&lt;span class="pl-ent"&gt;dependency&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;groupId&lt;/span&gt;&amp;gt;com.johnsnowlabs.nlp&amp;lt;/&lt;span class="pl-ent"&gt;groupId&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;artifactId&lt;/span&gt;&amp;gt;spark-nlp-gpu_2.11&amp;lt;/&lt;span class="pl-ent"&gt;artifactId&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;version&lt;/span&gt;&amp;gt;2.2.0&amp;lt;/&lt;span class="pl-ent"&gt;version&lt;/span&gt;&amp;gt;
&amp;lt;/&lt;span class="pl-ent"&gt;dependency&lt;/span&gt;&amp;gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;spark-nlp-ocr:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-text-xml"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;&amp;lt;!--&lt;/span&gt; https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-ocr &lt;span class="pl-c"&gt;--&amp;gt;&lt;/span&gt;&lt;/span&gt;
&amp;lt;&lt;span class="pl-ent"&gt;dependency&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;groupId&lt;/span&gt;&amp;gt;com.johnsnowlabs.nlp&amp;lt;/&lt;span class="pl-ent"&gt;groupId&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;artifactId&lt;/span&gt;&amp;gt;spark-nlp-ocr_2.11&amp;lt;/&lt;span class="pl-ent"&gt;artifactId&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;version&lt;/span&gt;&amp;gt;2.3.1&amp;lt;/&lt;span class="pl-ent"&gt;version&lt;/span&gt;&amp;gt;
&amp;lt;/&lt;span class="pl-ent"&gt;dependency&lt;/span&gt;&amp;gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;spark-nlp-eval:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-text-xml"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;&amp;lt;!--&lt;/span&gt; https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-eval &lt;span class="pl-c"&gt;--&amp;gt;&lt;/span&gt;&lt;/span&gt;
&amp;lt;&lt;span class="pl-ent"&gt;dependency&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;groupId&lt;/span&gt;&amp;gt;com.johnsnowlabs.nlp&amp;lt;/&lt;span class="pl-ent"&gt;groupId&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;artifactId&lt;/span&gt;&amp;gt;spark-nlp-eval_2.11&amp;lt;/&lt;span class="pl-ent"&gt;artifactId&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;version&lt;/span&gt;&amp;gt;2.3.1&amp;lt;/&lt;span class="pl-ent"&gt;version&lt;/span&gt;&amp;gt;
&amp;lt;/&lt;span class="pl-ent"&gt;dependency&lt;/span&gt;&amp;gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;spark-nlp-eval:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-text-xml"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;&amp;lt;!--&lt;/span&gt; https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-eval &lt;span class="pl-c"&gt;--&amp;gt;&lt;/span&gt;&lt;/span&gt;
&amp;lt;&lt;span class="pl-ent"&gt;dependency&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;groupId&lt;/span&gt;&amp;gt;com.johnsnowlabs.nlp&amp;lt;/&lt;span class="pl-ent"&gt;groupId&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;artifactId&lt;/span&gt;&amp;gt;spark-nlp-eval_2.11&amp;lt;/&lt;span class="pl-ent"&gt;artifactId&lt;/span&gt;&amp;gt;
    &amp;lt;&lt;span class="pl-ent"&gt;version&lt;/span&gt;&amp;gt;2.2.2&amp;lt;/&lt;span class="pl-ent"&gt;version&lt;/span&gt;&amp;gt;
&amp;lt;/&lt;span class="pl-ent"&gt;dependency&lt;/span&gt;&amp;gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-sbt" class="anchor" aria-hidden="true" href="#sbt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SBT&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;spark-nlp:&lt;/strong&gt;&lt;/p&gt;
&lt;pre lang="sbtshell"&gt;&lt;code&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp
libraryDependencies += "com.johnsnowlabs.nlp" %% "spark-nlp" % "2.3.1"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt;
&lt;pre lang="sbtshell"&gt;&lt;code&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu
libraryDependencies += "com.johnsnowlabs.nlp" %% "spark-nlp-gpu" % "2.2.0"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;spark-nlp-ocr:&lt;/strong&gt;&lt;/p&gt;
&lt;pre lang="sbtshell"&gt;&lt;code&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-ocr
libraryDependencies += "com.johnsnowlabs.nlp" %% "spark-nlp-ocr" % "2.3.1"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;spark-nlp-eval:&lt;/strong&gt;&lt;/p&gt;
&lt;pre lang="sbtshell"&gt;&lt;code&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-eval
libraryDependencies += "com.johnsnowlabs.nlp" %% "spark-nlp-eval" % "2.3.1"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;spark-nlp-eval:&lt;/strong&gt;&lt;/p&gt;
&lt;pre lang="sbtshell"&gt;&lt;code&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-eval
libraryDependencies += "com.johnsnowlabs.nlp" %% "spark-nlp-eval" % "2.2.2"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Maven Central: &lt;a href="https://mvnrepository.com/artifact/com.johnsnowlabs.nlp" rel="nofollow"&gt;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-python" class="anchor" aria-hidden="true" href="#python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-python-without-explicit-pyspark-installation" class="anchor" aria-hidden="true" href="#python-without-explicit-pyspark-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python without explicit Pyspark installation&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-pipconda" class="anchor" aria-hidden="true" href="#pipconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pip/Conda&lt;/h3&gt;
&lt;p&gt;If you installed pyspark through pip/conda, you can install &lt;code&gt;spark-nlp&lt;/code&gt; through the same channel.&lt;/p&gt;
&lt;p&gt;Pip:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install spark-nlp==2.3.1&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Conda:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c johnsnowlabs spark-nlp&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;PyPI &lt;a href="https://pypi.org/project/spark-nlp/" rel="nofollow"&gt;spark-nlp package&lt;/a&gt; / Anaconda &lt;a href="https://anaconda.org/JohnSnowLabs/spark-nlp" rel="nofollow"&gt;spark-nlp package&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Then you'll have to create a SparkSession either from Spark NLP:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; sparknlp

spark &lt;span class="pl-k"&gt;=&lt;/span&gt; sparknlp.start()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or manually:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;spark &lt;span class="pl-k"&gt;=&lt;/span&gt; SparkSession.builder \
    .appName(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;ner&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)\
    .master(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;local[4]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)\
    .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.driver.memory&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;8G&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)\
    .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.driver.maxResultSize&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;2G&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) \
    .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.jars.packages&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;JohnSnowLabs:spark-nlp:2.3.1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)\
    .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.kryoserializer.buffer.max&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;500m&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)\
    .getOrCreate()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If using local jars, you can use &lt;code&gt;spark.jars&lt;/code&gt; instead for a comma delimited jar files. For cluster setups, of course you'll have to put the jars in a reachable location for all driver and executor nodes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; sparknlp
&lt;span class="pl-k"&gt;from&lt;/span&gt; sparknlp.pretrained &lt;span class="pl-k"&gt;import&lt;/span&gt; PretrainedPipeline

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;create or get Spark Session&lt;/span&gt;

spark &lt;span class="pl-k"&gt;=&lt;/span&gt; sparknlp.start()

sparknlp.version()
spark.version

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;download, load, and annotate a text by pre-trained pipeline&lt;/span&gt;

pipeline &lt;span class="pl-k"&gt;=&lt;/span&gt; PretrainedPipeline(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;recognize_entities_dl&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;en&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
result &lt;span class="pl-k"&gt;=&lt;/span&gt; pipeline.annotate(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Harry Potter is a great movie&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-apache-zeppelin" class="anchor" aria-hidden="true" href="#apache-zeppelin"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Apache Zeppelin&lt;/h2&gt;
&lt;p&gt;Use either one of the following options&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add the following Maven Coordinates to the interpreter's library list&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;com.johnsnowlabs.nlp:spark-nlp_2.11:2.3.1&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Add path to pre-built jar from &lt;a href="#pre-compiled-spark-nlp-and-spark-nlp-ocr"&gt;here&lt;/a&gt; in the interpreter's library list making sure the jar is available to driver path&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-python-in-zeppelin" class="anchor" aria-hidden="true" href="#python-in-zeppelin"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python in Zeppelin&lt;/h3&gt;
&lt;p&gt;Apart from previous step, install python module through pip&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install spark-nlp==2.3.1&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or you can install &lt;code&gt;spark-nlp&lt;/code&gt; from inside Zeppelin by using Conda:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python.conda install -c johnsnowlabs spark-nlp&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Configure Zeppelin properly, use cells with %spark.pyspark or any interpreter name you chose.&lt;/p&gt;
&lt;p&gt;Finally, in Zeppelin interpreter settings, make sure you set properly zeppelin.python to the python you want to use and installed the pip library with (e.g. &lt;code&gt;python3&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;An alternative option would be to set &lt;code&gt;SPARK_SUBMIT_OPTIONS&lt;/code&gt; (zeppelin-env.sh) and make sure &lt;code&gt;--packages&lt;/code&gt; is there as shown earlier, since it includes both scala and python side installation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-jupyter-notebook-python" class="anchor" aria-hidden="true" href="#jupyter-notebook-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebook (Python)&lt;/h2&gt;
&lt;p&gt;Easiest way to get this done is by making Jupyter Notebook run using pyspark as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; SPARK_HOME=/path/to/your/spark/folder
&lt;span class="pl-k"&gt;export&lt;/span&gt; PYSPARK_PYTHON=python3
&lt;span class="pl-k"&gt;export&lt;/span&gt; PYSPARK_DRIVER_PYTHON=jupyter
&lt;span class="pl-k"&gt;export&lt;/span&gt; PYSPARK_DRIVER_PYTHON_OPTS=notebook

pyspark --packages JohnSnowLabs:spark-nlp:2.3.1&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Alternatively, you can mix in using &lt;code&gt;--jars&lt;/code&gt; option for pyspark + &lt;code&gt;pip install spark-nlp&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If not using pyspark at all, you'll have to run the instructions pointed &lt;a href="#python-without-explicit-Pyspark-installation"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-google-colab-notebook" class="anchor" aria-hidden="true" href="#google-colab-notebook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google Colab Notebook&lt;/h2&gt;
&lt;p&gt;Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or set up other than having a Google account.&lt;/p&gt;
&lt;p&gt;Run the following code in Google Colab notebook and start using spark-nlp right away.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; os

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Install java&lt;/span&gt;
! apt&lt;span class="pl-k"&gt;-&lt;/span&gt;get install &lt;span class="pl-k"&gt;-&lt;/span&gt;y openjdk&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;8&lt;/span&gt;&lt;span class="pl-k"&gt;-&lt;/span&gt;jdk&lt;span class="pl-k"&gt;-&lt;/span&gt;headless &lt;span class="pl-k"&gt;-&lt;/span&gt;qq &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;/&lt;/span&gt;dev&lt;span class="pl-k"&gt;/&lt;/span&gt;null
os.environ[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;JAVA_HOME&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/usr/lib/jvm/java-8-openjdk-amd64&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
os.environ[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PATH&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; os.environ[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;JAVA_HOME&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/bin:&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; os.environ[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;PATH&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
! java &lt;span class="pl-k"&gt;-&lt;/span&gt;version

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Install pyspark&lt;/span&gt;
! pip install &lt;span class="pl-ii"&gt;--&lt;/span&gt;ignore&lt;span class="pl-k"&gt;-&lt;/span&gt;installed pyspark&lt;span class="pl-k"&gt;==&lt;/span&gt;&lt;span class="pl-c1"&gt;2.4&lt;/span&gt;.4

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Install Spark NLP&lt;/span&gt;
! pip install &lt;span class="pl-ii"&gt;--&lt;/span&gt;ignore&lt;span class="pl-k"&gt;-&lt;/span&gt;installed spark&lt;span class="pl-k"&gt;-&lt;/span&gt;nlp&lt;span class="pl-k"&gt;==&lt;/span&gt;&lt;span class="pl-c1"&gt;2.3&lt;/span&gt;.1

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Quick SparkSession start&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; sparknlp
spark &lt;span class="pl-k"&gt;=&lt;/span&gt; sparknlp.start(&lt;span class="pl-v"&gt;include_ocr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Spark NLP version&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
sparknlp.version()
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Apache Spark version&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
spark.version&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/quick_start_google_colab.ipynb" rel="nofollow"&gt;Here&lt;/a&gt; is a live demo on Google Colab that performs sentiment analysis and NER using pretrained spark-nlp models.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-s3-cluster" class="anchor" aria-hidden="true" href="#s3-cluster"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;S3 Cluster&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-with-no-hadoop-configuration" class="anchor" aria-hidden="true" href="#with-no-hadoop-configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;With no hadoop configuration&lt;/h3&gt;
&lt;p&gt;If your distributed storage is S3 and you don't have a standard hadoop configuration (i.e. fs.defaultFS)
You need to specify where in the cluster distributed storage you want to store Spark NLP's tmp files.
First, decide where you want to put your &lt;em&gt;application.conf&lt;/em&gt; file&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;johnsnowlabs&lt;/span&gt;.&lt;span class="pl-en"&gt;util&lt;/span&gt;.&lt;span class="pl-en"&gt;ConfigLoader&lt;/span&gt;
&lt;span class="pl-en"&gt;ConfigLoader&lt;/span&gt;.setConfigPath(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/somewhere/to/put/application.conf&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then we need to put in such application.conf the following content&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sparknlp {
  settings {
    cluster_tmp_dir = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;somewhere in s3n:// path to some folder&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
  }
}&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-ocr-module" class="anchor" aria-hidden="true" href="#ocr-module"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OCR Module&lt;/h2&gt;
&lt;p&gt;To include the OCR submodule in Spark NLP, you will need to add the following to your start up commands:&lt;/p&gt;
&lt;pre lang="basg"&gt;&lt;code&gt;--repositories http://repo.spring.io/plugins-release
--packages JohnSnowLabs:spark-nlp:2.3.1,com.johnsnowlabs.nlp:spark-nlp-ocr_2.11:2.3.1,javax.media.jai:com.springsource.javax.media.jai.core:1.1.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This way you will download the extra dependencies needed by our OCR submodule. The Python SparkSession equivalent is&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;spark &lt;span class="pl-k"&gt;=&lt;/span&gt; SparkSession.builder \
    .master(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;local[*]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) \
    .appName(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Spark NLP with OCR&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) \
    .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.driver.memory&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;6g&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) \
    .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.executor.memory&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;6g&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) \
    .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.jars.repositories&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;http://repo.spring.io/plugins-release&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) \
    .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.jars.packages&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;JohnSnowLabs:spark-nlp:2.3.1,com.johnsnowlabs.nlp:spark-nlp-ocr_2.11:2.3.1,javax.media.jai:com.springsource.javax.media.jai.core:1.1.3&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) \
    .getOrCreate()&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-eval-module" class="anchor" aria-hidden="true" href="#eval-module"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eval Module&lt;/h2&gt;
&lt;p&gt;Evaluation module uses &lt;a href="https://mlflow.org/docs/latest/index.html" rel="nofollow"&gt;MLflow&lt;/a&gt; component to logging metrics.&lt;/p&gt;
&lt;p&gt;To configure &lt;a href="https://mlflow.org/docs/latest/tracking.html" rel="nofollow"&gt;MLflow tracking UI&lt;/a&gt; you just need the steps below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install &lt;a href="https://mlflow.org/docs/latest/quickstart.html" rel="nofollow"&gt;MLflow&lt;/a&gt; with Pip&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install mlflow&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Set MLFLOW_TRACKING_URI variable&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; MLFLOW_TRACKING_URI=http://localhost:5000&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now to see the results you just need the following steps before using any component from eval module:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run MLflow's Tracking UI&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;mlflow ui&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;View it at &lt;a href="http://localhost:5000" rel="nofollow"&gt;http://localhost:5000&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To include the Eval submodule in Spark NLP, you will need to add the following to your start up commands:&lt;/p&gt;
&lt;pre lang="basg"&gt;&lt;code&gt;--repositories http://repo.spring.io/plugins-release
--packages JohnSnowLabs:spark-nlp:2.3.1,com.johnsnowlabs.nlp:spark-nlp-eval_2.11:2.3.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This way you will download the extra dependencies needed by our Eval submodule. The Python SparkSession equivalent is&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;spark &lt;span class="pl-k"&gt;=&lt;/span&gt; SparkSession.builder \
    .master(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;local[*]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) \
    .appName(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Spark NLP with Eval&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) \
    .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.driver.memory&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;6g&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) \
    .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.executor.memory&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;6g&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) \
    .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.jars.repositories&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;http://repo.spring.io/plugins-release&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) \
    .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.jars.packages&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;JohnSnowLabs:spark-nlp:2.3.1,com.johnsnowlabs.nlp:spark-nlp-eval_2.11:2.3.1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) \
    .getOrCreate()&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-pipelines-and-models" class="anchor" aria-hidden="true" href="#pipelines-and-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pipelines and Models&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-pipelines" class="anchor" aria-hidden="true" href="#pipelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pipelines&lt;/h3&gt;
&lt;p&gt;Spark NLP offers more than &lt;code&gt;25 pre-trained pipelines&lt;/code&gt; in &lt;code&gt;4 languages&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;English pipelines:&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pipelines&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Explain Document ML&lt;/td&gt;
&lt;td&gt;&lt;code&gt;explain_document_ml&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Explain Document DL&lt;/td&gt;
&lt;td&gt;&lt;code&gt;explain_document_dl&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Explain Document DL Win&lt;/td&gt;
&lt;td&gt;&lt;code&gt;explain_document_dl_noncontrib&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Explain Document DL Fast&lt;/td&gt;
&lt;td&gt;&lt;code&gt;explain_document_dl_fast&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Explain Document DL Fast Win&lt;/td&gt;
&lt;td&gt;&lt;code&gt;explain_document_dl_fast_noncontrib&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recognize Entities DL&lt;/td&gt;
&lt;td&gt;&lt;code&gt;recognize_entities_dl&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recognize Entities DL Win&lt;/td&gt;
&lt;td&gt;&lt;code&gt;recognize_entities_dl_noncontrib&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OntoNotes Entities Small&lt;/td&gt;
&lt;td&gt;&lt;code&gt;onto_recognize_entities_sm&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OntoNotes Entities Large&lt;/td&gt;
&lt;td&gt;&lt;code&gt;onto_recognize_entities_lg&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Match Datetime&lt;/td&gt;
&lt;td&gt;&lt;code&gt;match_datetime&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Match Pattern&lt;/td&gt;
&lt;td&gt;&lt;code&gt;match_pattern&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Match Chunk&lt;/td&gt;
&lt;td&gt;&lt;code&gt;match_chunks&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Match Phrases&lt;/td&gt;
&lt;td&gt;&lt;code&gt;match_phrases&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Clean Stop&lt;/td&gt;
&lt;td&gt;&lt;code&gt;clean_stop&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Clean Pattern&lt;/td&gt;
&lt;td&gt;&lt;code&gt;clean_pattern&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Clean Slang&lt;/td&gt;
&lt;td&gt;&lt;code&gt;clean_slang&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Check Spelling&lt;/td&gt;
&lt;td&gt;&lt;code&gt;check_spelling&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Analyze Sentiment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;analyze_sentiment&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dependency Parse&lt;/td&gt;
&lt;td&gt;&lt;code&gt;dependency_parse&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;johnsnowlabs&lt;/span&gt;.&lt;span class="pl-en"&gt;nlp&lt;/span&gt;.&lt;span class="pl-en"&gt;pretrained&lt;/span&gt;.&lt;span class="pl-en"&gt;PretrainedPipeline&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;johnsnowlabs&lt;/span&gt;.&lt;span class="pl-en"&gt;nlp&lt;/span&gt;.&lt;span class="pl-en"&gt;SparkNLP&lt;/span&gt;

&lt;span class="pl-en"&gt;SparkNLP&lt;/span&gt;.version()

&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;testData&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; spark.createDataFrame(&lt;span class="pl-en"&gt;Seq&lt;/span&gt;(
(&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Google has announced the release of a beta version of the popular TensorFlow machine learning library&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
(&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
)).toDF(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;id&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;text&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;pipeline&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;PretrainedPipeline&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;explain_document_dl&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, lang&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;en&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;annotation&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; pipeline.transform(testData)

annotation.show()
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;/*&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline&lt;/span&gt;
&lt;span class="pl-c"&gt;import com.johnsnowlabs.nlp.SparkNLP&lt;/span&gt;
&lt;span class="pl-c"&gt;2.0.8&lt;/span&gt;
&lt;span class="pl-c"&gt;testData: org.apache.spark.sql.DataFrame = [id: int, text: string]&lt;/span&gt;
&lt;span class="pl-c"&gt;pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_dl,en,public/models)&lt;/span&gt;
&lt;span class="pl-c"&gt;annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 10 more fields]&lt;/span&gt;
&lt;span class="pl-c"&gt;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&lt;/span&gt;
&lt;span class="pl-c"&gt;| id|                text|            document|               token|            sentence|             checked|               lemma|                stem|                 pos|          embeddings|                 ner|            entities|&lt;/span&gt;
&lt;span class="pl-c"&gt;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&lt;/span&gt;
&lt;span class="pl-c"&gt;|  1|Google has announ...|[[document, 0, 10...|[[token, 0, 5, Go...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...|&lt;/span&gt;
&lt;span class="pl-c"&gt;|  2|The Paris metro w...|[[document, 0, 11...|[[token, 0, 2, Th...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 4, 8, Pa...|&lt;/span&gt;
&lt;span class="pl-c"&gt;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;*/&lt;/span&gt;&lt;/span&gt;

annotation.select(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;entities.result&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).show(&lt;span class="pl-c1"&gt;false&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;/*&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;+----------------------------------+&lt;/span&gt;
&lt;span class="pl-c"&gt;|result                            |&lt;/span&gt;
&lt;span class="pl-c"&gt;+----------------------------------+&lt;/span&gt;
&lt;span class="pl-c"&gt;|[Google, TensorFlow]              |&lt;/span&gt;
&lt;span class="pl-c"&gt;|[Donald John Trump, United States]|&lt;/span&gt;
&lt;span class="pl-c"&gt;+----------------------------------+&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;*/&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-please-check-our-documentation-for-full-list-and-example-of-pre-trained-pipelines" class="anchor" aria-hidden="true" href="#please-check-our-documentation-for-full-list-and-example-of-pre-trained-pipelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Please check our documentation for full list and example of &lt;a href="https://nlp.johnsnowlabs.com/docs/en/pipelines" rel="nofollow"&gt;pre-trained pipelines&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-models" class="anchor" aria-hidden="true" href="#models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models&lt;/h3&gt;
&lt;p&gt;Spark NLP offers more than &lt;code&gt;30 pre-trained models&lt;/code&gt; in &lt;code&gt;4 languages&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;English pipelines:&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;LemmatizerModel (Lemmatizer)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;lemma_antbnc&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PerceptronModel (POS)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;pos_anc&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NerCRFModel (NER with GloVe)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ner_crf&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NerDLModel (NER with GloVe)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ner_dl&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NerDLModel (NER with GloVe)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ner_dl_contrib&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NerDLModel (NER with BERT)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ner_dl_bert_base_cased&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NerDLModel (OntoNotes with GloVe 100d)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;onto_100&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NerDLModel (OntoNotes with GloVe 300d)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;onto_300&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WordEmbeddings (GloVe)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;glove_100d&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BertEmbeddings (base_uncased)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bert_base_uncased&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BertEmbeddings (base_cased)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bert_base_cased&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BertEmbeddings (large_uncased)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bert_large_uncased&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BertEmbeddings (large_cased)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bert_large_cased&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DeepSentenceDetector&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ner_dl_sentence&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ContextSpellCheckerModel (Spell Checker)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;spellcheck_dl&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SymmetricDeleteModel (Spell Checker)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;spellcheck_sd&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NorvigSweetingModel (Spell Checker)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;spellcheck_norvig&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ViveknSentimentModel (Sentiment)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;sentiment_vivekn&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DependencyParser (Dependency)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;dependency_conllu&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TypedDependencyParser (Dependency)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;dependency_typed_conllu&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Quick online example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load NER model trained by deep learning approach and GloVe word embeddings&lt;/span&gt;
ner_dl &lt;span class="pl-k"&gt;=&lt;/span&gt; NerDLModel.pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ner_dl&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load NER model trained by deep learning approach and BERT word embeddings&lt;/span&gt;
ner_bert &lt;span class="pl-k"&gt;=&lt;/span&gt; NerDLModel.pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ner_dl_bert&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; load French POS tagger model trained by Universal Dependencies&lt;/span&gt;
&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;french_pos&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;PerceptronModel&lt;/span&gt;.pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pos_ud_gsd&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, lang&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;fr&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; load Italain LemmatizerModel&lt;/span&gt;
&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;italian_lemma&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;LemmatizerModel&lt;/span&gt;.pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;lemma_dxc&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, lang&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;it&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Quick offline example:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Loading &lt;code&gt;PerceptronModel&lt;/code&gt; annotator model inside Spark NLP Pipeline&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;french_pos&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;PerceptronModel&lt;/span&gt;.load(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
      .setInputCols(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;document&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;token&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
      .setOutputCol(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pos&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-please-check-our-documentation-for-full-list-and-example-of-pre-trained-models" class="anchor" aria-hidden="true" href="#please-check-our-documentation-for-full-list-and-example-of-pre-trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Please check our documentation for full list and example of &lt;a href="https://nlp.johnsnowlabs.com/docs/en/models" rel="nofollow"&gt;pre-trained models&lt;/a&gt;&lt;/h4&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;p&gt;Need more examples? Check out our dedicated repository to showcase Spark NLP use cases!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/JohnSnowLabs/spark-nlp-workshop"&gt;spark-nlp-workshop&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://nlp.johnsnowlabs.com/articles.html" rel="nofollow"&gt;Check our Articles and FAQ page here&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-troubleshooting" class="anchor" aria-hidden="true" href="#troubleshooting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Troubleshooting&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-ocr" class="anchor" aria-hidden="true" href="#ocr"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OCR&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Q: I am getting a Java Core Dump when running OCR transformation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A: Add &lt;code&gt;LC_ALL=C&lt;/code&gt; environment variable&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q: Getting &lt;code&gt;org.apache.pdfbox.filter.MissingImageReaderException: Cannot read JPEG2000 image: Java Advanced Imaging (JAI) Image I/O Tools are not installed&lt;/code&gt; when running an OCR transformation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A: &lt;code&gt;--packages com.github.jai-imageio:jai-imageio-jpeg2000:1.3.0&lt;/code&gt;. This library is non-free thus we can't include it as a Spark NLP dependency by default&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgments" class="anchor" aria-hidden="true" href="#acknowledgments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgments&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-special-community-aknowledgments" class="anchor" aria-hidden="true" href="#special-community-aknowledgments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Special community aknowledgments&lt;/h3&gt;
&lt;p&gt;Thanks in general to the community who have been lately reporting important issues and pull request with bugfixes.
Community has been key in the last releases with feedback in various Spark based environments.&lt;/p&gt;
&lt;p&gt;Here a few specific mentions for recurring feedback and slack participation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/maziyarpanahi"&gt;@maziyarpanahi&lt;/a&gt; - For contributing with testing and valuable feedback&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/easimadi"&gt;@easimadi&lt;/a&gt; - For contributing with documentation and valuable feedback&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We appreciate any sort of contributions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ideas&lt;/li&gt;
&lt;li&gt;feedback&lt;/li&gt;
&lt;li&gt;documentation&lt;/li&gt;
&lt;li&gt;bug reports&lt;/li&gt;
&lt;li&gt;nlp training and testing corpora&lt;/li&gt;
&lt;li&gt;development and testing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Clone the repo and submit your pull-requests! Or directly create issues in this repo.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;&lt;a href="mailto:nlp@johnsnowlabs.com"&gt;nlp@johnsnowlabs.com&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-john-snow-labs" class="anchor" aria-hidden="true" href="#john-snow-labs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;John Snow Labs&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://johnsnowlabs.com" rel="nofollow"&gt;http://johnsnowlabs.com&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>JohnSnowLabs</author><guid isPermaLink="false">https://github.com/JohnSnowLabs/spark-nlp</guid><pubDate>Sat, 02 Nov 2019 00:05:00 GMT</pubDate></item><item><title>WeBankFinTech/Linkis #6 in Scala, Today</title><link>https://github.com/WeBankFinTech/Linkis</link><description>&lt;p&gt;&lt;i&gt;Linkis helps easily connect to various back-end computation/storage engines(Spark, Python, TiDB...), exposes various interfaces(REST, JDBC, Java ...), with multi-tenancy, high performance, and resource control.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-linkis" class="anchor" aria-hidden="true" href="#linkis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linkis&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://www.apache.org/licenses/LICENSE-2.0.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8cb994f6c4a156c623fe057fccd7fb7d7d2e8c9b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322d3445423142412e737667" alt="License" data-canonical-src="https://img.shields.io/badge/license-Apache%202-4EB1BA.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;English | &lt;a href="docs/zh_CN/README.md"&gt;中文&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Linkis helps easily connect to various back-end computation/storage engines(Spark, Python, TiDB...), exposes various interfaces(REST, JDBC, Java ...), with multi-tenancy, high performance, and resource control.&lt;/p&gt;
&lt;p&gt;Linkis connects with computation/storage engines(Spark, Hive, Python and HBase), exposes REST/WS interface, and executes multi-language jobs(SQL, Pyspark, HiveQL and Scala), as a computation middleware.&lt;/p&gt;
&lt;p&gt;Based on the microservices architecture, Linkis provides enterprise-level features of multi-tenant isolation, resource management and access control. It also offers convenient support to manage unified variables, UDFs, functions and resource files. it is also guaranteed with sophisticated task/job lifecycle management capabilities under high-concurrency, high-performance and high-availability scenarios.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/WeBankFinTech/Linkis/blob/master/images/linkis-intro-01.png?raw=true"&gt;&lt;img src="https://github.com/WeBankFinTech/Linkis/raw/master/images/linkis-intro-01.png?raw=true" alt="linkis-intro-01" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/WeBankFinTech/Linkis/blob/master/images/linkis-intro-03.png?raw=true"&gt;&lt;img src="https://github.com/WeBankFinTech/Linkis/raw/master/images/linkis-intro-03.png?raw=true" alt="linkis-intro-03" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Based on the concept of the computation middleware architecture of Linkis, we have built a large amount of applications and systems on top of it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Currently available open-source project: &lt;a href="https://github.com/WeBankFinTech/Scriptis"&gt;&lt;strong&gt;Scriptis - Data Development IDE Tool&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Upcoming open-source projects：&lt;strong&gt;Data Visualization Tool&lt;/strong&gt;, &lt;strong&gt;Graphic Workflow Tool&lt;/strong&gt; and &lt;strong&gt;Data Quality Tool&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There will be more tools released as open-source projects, please stay tuned!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Unified Job Execution Services: A distributed REST/WebSocket service for processing scripts execution requests from user.&lt;/p&gt;
&lt;p&gt;Available computation engines so far: Spark, Python, TiSpark, Hive and Shell.&lt;/p&gt;
&lt;p&gt;Available languages so far: SparkSQL, Spark Scala, PySpark, R, Python, HQL and Shell.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Resource Management Services: Available for real-time control/limit of resource usage from both perspectives of amount and load for both systems and users. With dynamic charts of resource statistics, it is convenient to monitor and manage resource usage for systems and users.&lt;/p&gt;
&lt;p&gt;Available resource types so far: Yarn queue resources, server(CPU and memory), number of concurrent instances per user.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Application Management Services: Manages global user applications, including offline batch applications, interactive query applications and real-time streaming applications. Also provides powerful reusability especially for offline and interactive applications, with complete lifecycle management which automatically releases idle applications for users.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unified Storage Services: The generic IO architecture can quickly integrate with various storage systems and provide a unified invokable entrance. It is also highly integrated with most common data formats and easy to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unified Context Services: Unite resources files of users and systems (JAR, ZIP, Properties). With unified management of arguments/variables for users, systems and engines, it is achieved that modification in random place will reflect in all the other places automatically.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Material Library: System and user-level material management, capable of sharing, transferring materials and automatic lifecycle management.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Metadata Services: Real-time display of dataset table structure and partitions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Compared with similar systems&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/en_US/images/introduction/introduction01.png"&gt;&lt;img src="docs/en_US/images/introduction/introduction01.png" alt="introduction01" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-documentations" class="anchor" aria-hidden="true" href="#documentations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentations：&lt;/h1&gt;
&lt;p&gt;&lt;a href="docs/en_US/ch3/Linkis_Introduction.md"&gt;Linkis, make big data easier&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="docs/en_US/ch1/deploy.md"&gt;Linkis Quick Deploy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="docs/en_US/ch3/Linkis_Java_SDK_doc.md"&gt;Linkis Quick Start &amp;amp; Java SDK documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="docs/en_US/ch3/Linkis_HTTP_API_Doc.md"&gt;HTTP APIs for frontend applications&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="docs/en_US/ch3/Linkis_WebSocket_API_Doc.md"&gt;WebSocket APIs for frontend applications&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="docs/en_US/ch3/How_to_adapt_Linkis_with_a_new_computation_or_storage_engine.md"&gt;How to adapt Linkis with a new computation or storage engine&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-architecture" class="anchor" aria-hidden="true" href="#architecture"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Architecture：&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./docs/en_US/images/introduction/introduction02.png"&gt;&lt;img src="./docs/en_US/images/introduction/introduction02.png" alt="introduction02" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-communication" class="anchor" aria-hidden="true" href="#communication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Communication&lt;/h2&gt;
&lt;p&gt;If you desire immediate response, please kindly raise issues to us or scan the below QR code by WeChat and QQ to join our group:
&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="docs/en_US/images/introduction/introduction05.png"&gt;&lt;img src="docs/en_US/images/introduction/introduction05.png" alt="introduction05" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Linkis is under the Apache 2.0 license. See the &lt;a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;LICENSE &lt;/a&gt;file for details.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>WeBankFinTech</author><guid isPermaLink="false">https://github.com/WeBankFinTech/Linkis</guid><pubDate>Sat, 02 Nov 2019 00:06:00 GMT</pubDate></item><item><title>Azure/mmlspark #7 in Scala, Today</title><link>https://github.com/Azure/mmlspark</link><description>&lt;p&gt;&lt;i&gt;Microsoft Machine Learning for Apache Spark&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0f31d64469b1c4f3dbe5e854f52bb3eb349e0ba8/68747470733a2f2f6d6d6c737061726b2e617a757265656467652e6e65742f69636f6e732f6d6d6c737061726b2e737667"&gt;&lt;img src="https://camo.githubusercontent.com/0f31d64469b1c4f3dbe5e854f52bb3eb349e0ba8/68747470733a2f2f6d6d6c737061726b2e617a757265656467652e6e65742f69636f6e732f6d6d6c737061726b2e737667" alt="MMLSpark" data-canonical-src="https://mmlspark.azureedge.net/icons/mmlspark.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-microsoft-machine-learning-for-apache-spark" class="anchor" aria-hidden="true" href="#microsoft-machine-learning-for-apache-spark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Microsoft Machine Learning for Apache Spark&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://msazure.visualstudio.com/Cognitive%20Services/_build/latest?definitionId=83120&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c6013b9f6e8d726998981acea8cf0c6afa87dc8d/68747470733a2f2f6d73617a7572652e76697375616c73747564696f2e636f6d2f436f676e697469766525323053657276696365732f5f617069732f6275696c642f7374617475732f417a7572652e6d6d6c737061726b3f6272616e63684e616d653d6d6173746572" alt="Build Status" data-canonical-src="https://msazure.visualstudio.com/Cognitive%20Services/_apis/build/status/Azure.mmlspark?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/Azure/mmlspark" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3d33b1f413ba945dce11c42625c751b6e13acca3/68747470733a2f2f636f6465636f762e696f2f67682f417a7572652f6d6d6c737061726b2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/Azure/mmlspark/branch/master/graph/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://gitter.im/Microsoft/MMLSpark?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2177ea7fddaac7ecb8a0d5ebda9e0550ec2f79fa/68747470733a2f2f6261646765732e6769747465722e696d2f4d6963726f736f66742f4d4d4c537061726b2e737667" alt="Gitter" data-canonical-src="https://badges.gitter.im/Microsoft/MMLSpark.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/Azure/mmlspark/releases"&gt;&lt;img src="https://camo.githubusercontent.com/2b95bde9e35735d085f2a64e334564fddb1e792b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72656c656173652d6e6f7465732d626c7565" alt="Release Notes" data-canonical-src="https://img.shields.io/badge/release-notes-blue" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://mmlspark.blob.core.windows.net/docs/1.0.0-rc1/scala/index.html#package" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9541c1e00ab17b5c0f31ab880f7da0d1dd06cff0/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d617069253230646f6373266d6573736167653d7363616c6126636f6c6f723d626c7565266c6f676f3d7363616c61" alt="Scala Docs" data-canonical-src="https://img.shields.io/static/v1?label=api%20docs&amp;amp;message=scala&amp;amp;color=blue&amp;amp;logo=scala" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://mmlspark.blob.core.windows.net/docs/1.0.0-rc1/pyspark/index.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a5ac918a682fe9c6992a7f1ab12e204964a53997/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d617069253230646f6373266d6573736167653d707974686f6e26636f6c6f723d626c7565266c6f676f3d707974686f6e" alt="PySpark Docs" data-canonical-src="https://img.shields.io/static/v1?label=api%20docs&amp;amp;message=python&amp;amp;color=blue&amp;amp;logo=python" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1810.08744" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/efa888257e8261153f5ff4f426ad4893fb84badc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61636164656d69632d70617065722d376664636637" alt="Academic Paper" data-canonical-src="https://img.shields.io/badge/academic-paper-7fdcf7" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/Azure/mmlspark/releases"&gt;&lt;img src="https://camo.githubusercontent.com/14e16b347646015d0f380e887d184c59e4640b4e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f76657273696f6e2d312e302e302d2d7263312d626c7565" alt="Version" data-canonical-src="https://img.shields.io/badge/version-1.0.0--rc1-blue" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="#sbt"&gt;&lt;img src="https://camo.githubusercontent.com/f2134790d60abaea58ea73bbeed0e0853de9e0f5/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f69636f6e732f6261646765732f6d61737465725f76657273696f6e332e737667" alt="Snapshot Version" data-canonical-src="https://mmlspark.blob.core.windows.net/icons/badges/master_version3.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;MMLSpark is an ecosystem of tools aimed towards expanding the distributed computing framework
&lt;a href="https://github.com/apache/spark"&gt;Apache Spark&lt;/a&gt; in several new directions.
MMLSpark adds many deep learning and data science tools to the Spark ecosystem,
including seamless integration of Spark Machine Learning pipelines with &lt;a href="https://github.com/Microsoft/CNTK"&gt;Microsoft Cognitive Toolkit
(CNTK)&lt;/a&gt;, &lt;a href="https://github.com/Microsoft/LightGBM"&gt;LightGBM&lt;/a&gt; and
&lt;a href="http://www.opencv.org/" rel="nofollow"&gt;OpenCV&lt;/a&gt;. These tools enable powerful and highly-scalable predictive and analytical models
for a variety of datasources.&lt;/p&gt;
&lt;p&gt;MMLSpark also brings new networking capabilities to the Spark Ecosystem. With the HTTP on Spark project, users
can embed &lt;strong&gt;any&lt;/strong&gt; web service into their SparkML models. In this vein, MMLSpark provides easy to use
SparkML transformers for a wide variety of &lt;a href="https://azure.microsoft.com/en-us/services/cognitive-services/" rel="nofollow"&gt;Microsoft Cognitive Services&lt;/a&gt;. For production grade deployment, the Spark Serving project enables high throughput,
sub-millisecond latency web services, backed by your Spark cluster.&lt;/p&gt;
&lt;p&gt;MMLSpark requires Scala 2.11, Spark 2.3+, and either Python 2.7 or Python 3.5+.
See the API documentation &lt;a href="https://mmlspark.blob.core.windows.net/docs/1.0.0-rc1/scala/index.html#package" rel="nofollow"&gt;for
Scala&lt;/a&gt; and &lt;a href="https://mmlspark.blob.core.windows.net/docs/1.0.0-rc1/pyspark/index.html" rel="nofollow"&gt;for
PySpark&lt;/a&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#notable-features"&gt;Notable features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#a-short-example"&gt;A short example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#setup-and-installation"&gt;Setup and installation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#gpu-vm-setup"&gt;GPU VM Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#spark-package"&gt;Spark package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#databricks-cloud"&gt;Databricks cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sbt"&gt;SBT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#building-from-source"&gt;Building from source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#blogs-and-publications"&gt;Blogs and Publications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contributing--feedback"&gt;Contributing &amp;amp; feedback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-relevant-projects"&gt;Other relevant projects&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;h2&gt;&lt;a id="user-content-projects" class="anchor" aria-hidden="true" href="#projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/93b3b7e4dfc1a58a5a6bdbbe67177307c9a47359/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f76772d626c75652d6461726b2d6f72616e67652e737667"&gt;&lt;img width="800" src="https://camo.githubusercontent.com/93b3b7e4dfc1a58a5a6bdbbe67177307c9a47359/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f76772d626c75652d6461726b2d6f72616e67652e737667" data-canonical-src="https://mmlspark.blob.core.windows.net/graphics/Readme/vw-blue-dark-orange.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/bfbef7ae756b550d3bd2648aed3bff7a7ceb03eb/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f636f675f73657276696365735f6f6e5f737061726b5f322e737667"&gt;&lt;img width="800" src="https://camo.githubusercontent.com/bfbef7ae756b550d3bd2648aed3bff7a7ceb03eb/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f636f675f73657276696365735f6f6e5f737061726b5f322e737667" data-canonical-src="https://mmlspark.blob.core.windows.net/graphics/Readme/cog_services_on_spark_2.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ae54b5e5042ab39028bdf52f6fe7b42c7a475f55/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f6465636973696f6e5f747265655f7265636f6c6f722e706e67"&gt;&lt;img width="800" src="https://camo.githubusercontent.com/ae54b5e5042ab39028bdf52f6fe7b42c7a475f55/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f6465636973696f6e5f747265655f7265636f6c6f722e706e67" data-canonical-src="https://mmlspark.blob.core.windows.net/graphics/Readme/decision_tree_recolor.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2a836683e4c037d1c65a09cebf680b991f3cd07e/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f6d6d6c737061726b5f73657276696e675f7265636f6c6f722e737667"&gt;&lt;img width="800" src="https://camo.githubusercontent.com/2a836683e4c037d1c65a09cebf680b991f3cd07e/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f6d6d6c737061726b5f73657276696e675f7265636f6c6f722e737667" data-canonical-src="https://mmlspark.blob.core.windows.net/graphics/Readme/mmlspark_serving_recolor.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;strong&gt;Vowpal Wabbit on Spark&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;The Cognitive Services on Spark&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;LightGBM on Spark&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;Spark Serving&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Fast, Sparse, and Effective Text Analytics&lt;/td&gt;
&lt;td align="center"&gt;Leverage the Microsoft Cognitive Services at Unprecedented Scales in your existing SparkML pipelines&lt;/td&gt;
&lt;td align="center"&gt;Train Gradient Boosted Machines with LightGBM&lt;/td&gt;
&lt;td align="center"&gt;Serve any Spark Computation as a Web Service with Sub-Millisecond Latency&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/9ea135c7b4beaee0365bf05e2f35977c4f1b148e/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f6d6963726f736572766963655f7265636f6c6f722e706e67"&gt;&lt;img width="800" src="https://camo.githubusercontent.com/9ea135c7b4beaee0365bf05e2f35977c4f1b148e/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f6d6963726f736572766963655f7265636f6c6f722e706e67" data-canonical-src="https://mmlspark.blob.core.windows.net/graphics/Readme/microservice_recolor.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e706b7bfec0b1030f979a12a938b99dad9c329ce/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f64697374726962757465645f646565705f7265636f6c6f722e706e67"&gt;&lt;img width="800" src="https://camo.githubusercontent.com/e706b7bfec0b1030f979a12a938b99dad9c329ce/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f64697374726962757465645f646565705f7265636f6c6f722e706e67" data-canonical-src="https://mmlspark.blob.core.windows.net/graphics/Readme/distributed_deep_recolor.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/a27a8de5e9aede55d8565c7109b2becd20bbd5ed/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f4c494d452e737667"&gt;&lt;img width="800" src="https://camo.githubusercontent.com/a27a8de5e9aede55d8565c7109b2becd20bbd5ed/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f4c494d452e737667" data-canonical-src="https://mmlspark.blob.core.windows.net/graphics/Readme/LIME.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1b81d8cc571ba4b0e21fb975542089374ce3a72c/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f62696e64696e67732e706e67"&gt;&lt;img width="800" src="https://camo.githubusercontent.com/1b81d8cc571ba4b0e21fb975542089374ce3a72c/68747470733a2f2f6d6d6c737061726b2e626c6f622e636f72652e77696e646f77732e6e65742f67726170686963732f526561646d652f62696e64696e67732e706e67" data-canonical-src="https://mmlspark.blob.core.windows.net/graphics/Readme/bindings.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;strong&gt;HTTP on Spark&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;CNTK on Spark&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;Lime on Spark&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;Spark Binding Autogeneration&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;An Integration Between Spark and the HTTP Protocol, enabling Distributed Microservice Orchestration&lt;/td&gt;
&lt;td align="center"&gt;Distributed Deep Learning with the Microsoft Cognitive Toolkit&lt;/td&gt;
&lt;td align="center"&gt;Distributed, Model Agnostic, Interpretations for Classifiers&lt;/td&gt;
&lt;td align="center"&gt;Automatically Generate Spark bindings for PySpark and SparklyR&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Create a deep image classifier with transfer learning (&lt;a href="notebooks/samples/DeepLearning%20-%20Flower%20Image%20Classification.ipynb" title="Deep Flower Classification"&gt;example 9&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Fit a LightGBM classification or regression model on a biochemical dataset
(&lt;a href="notebooks/samples/LightGBM%20-%20Quantile%20Regression%20for%20Drug%20Discovery.ipynb" title="Quantile Regression with LightGBM"&gt;example 3&lt;/a&gt;), to learn more check out the &lt;a href="docs/lightgbm.md"&gt;LightGBM documentation
page&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Deploy a deep network as a distributed web service with &lt;a href="docs/mmlspark-serving.md"&gt;MMLSpark
Serving&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Use web services in Spark with &lt;a href="docs/http.md"&gt;HTTP on Apache Spark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Use Bi-directional LSTMs from Keras for medical entity extraction
(&lt;a href="notebooks/samples/DeepLearning%20-%20BiLSTM%20Medical%20Entity%20Extraction.ipynb" title="Medical Entity Extraction"&gt;example 8&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Create a text analytics system on Amazon book reviews (&lt;a href="notebooks/samples/TextAnalytics%20-%20Amazon%20Book%20Reviews.ipynb" title="Amazon Book Reviews - TextFeaturizer"&gt;example 4&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Perform distributed hyperparameter tuning to identify Breast Cancer
(&lt;a href="notebooks/samples/HyperParameterTuning%20-%20Fighting%20Breast%20Cancer.ipynb" title="Hyperparameter Tuning with MMLSpark"&gt;example 5&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Easily ingest images from HDFS into Spark &lt;code&gt;DataFrame&lt;/code&gt; (&lt;a href="notebooks/samples/DeepLearning%20-%20CIFAR10%20Convolutional%20Network.ipynb" title="CIFAR10 CNTK CNN Evaluation"&gt;example 6&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Use OpenCV on Spark to manipulate images (&lt;a href="notebooks/samples/OpenCV%20-%20Pipeline%20Image%20Transformations.ipynb" title="Pipeline Image Transformations"&gt;example 7&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Train classification and regression models easily via implicit featurization
of data (&lt;a href="notebooks/samples/Classification%20-%20Adult%20Census.ipynb" title="Adult Census Income Training"&gt;example 1&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Train and evaluate a flight delay prediction system (&lt;a href="notebooks/samples/Regression%20-%20Flight%20Delays.ipynb" title="Regression Example with Flight Delay Dataset"&gt;example 2&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See our &lt;a href="notebooks/samples/"&gt;notebooks&lt;/a&gt; for all examples.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-a-short-example" class="anchor" aria-hidden="true" href="#a-short-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A short example&lt;/h2&gt;
&lt;p&gt;Below is an excerpt from a simple example of using a pre-trained CNN to
classify images in the CIFAR-10 dataset.  View the whole source code in notebook &lt;a href="notebooks/samples/DeepLearning%20-%20Flower%20Image%20Classification.ipynb" title="Deep Flower Classification"&gt;example 9&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;...&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; mmlspark
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Initialize CNTKModel and define input and output columns&lt;/span&gt;
cntkModel &lt;span class="pl-k"&gt;=&lt;/span&gt; mmlspark.cntk.CNTKModel() \
  .setInputCol(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;images&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).setOutputCol(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;output&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) \
  .setModelLocation(modelFile)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train on dataset with internal spark pipeline&lt;/span&gt;
scoredImages &lt;span class="pl-k"&gt;=&lt;/span&gt; cntkModel.transform(imagesWithLabels)
&lt;span class="pl-c1"&gt;...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See &lt;a href="notebooks/samples/"&gt;other sample notebooks&lt;/a&gt; as well as the MMLSpark
documentation for &lt;a href="http://mmlspark.azureedge.net/docs/scala/" rel="nofollow"&gt;Scala&lt;/a&gt; and
&lt;a href="http://mmlspark.azureedge.net/docs/pyspark/" rel="nofollow"&gt;PySpark&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-setup-and-installation" class="anchor" aria-hidden="true" href="#setup-and-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup and installation&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-spark-package" class="anchor" aria-hidden="true" href="#spark-package"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Spark package&lt;/h3&gt;
&lt;p&gt;MMLSpark can be conveniently installed on existing Spark clusters via the
&lt;code&gt;--packages&lt;/code&gt; option, examples:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;spark-shell --packages com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1
pyspark --packages com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1
spark-submit --packages com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1 MyApp.jar&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This can be used in other Spark contexts too. For example, you can use MMLSpark
in &lt;a href="https://github.com/Azure/aztk/"&gt;AZTK&lt;/a&gt; by &lt;a href="https://github.com/Azure/aztk/wiki/PySpark-on-Azure-with-AZTK#optional-set-up-mmlspark"&gt;adding it to the
&lt;code&gt;.aztk/spark-defaults.conf&lt;/code&gt;
file&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-databricks" class="anchor" aria-hidden="true" href="#databricks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Databricks&lt;/h3&gt;
&lt;p&gt;To install MMLSpark on the &lt;a href="http://community.cloud.databricks.com" rel="nofollow"&gt;Databricks
cloud&lt;/a&gt;, create a new &lt;a href="https://docs.databricks.com/user-guide/libraries.html#libraries-from-maven-pypi-or-spark-packages" rel="nofollow"&gt;library from Maven
coordinates&lt;/a&gt;
in your workspace.&lt;/p&gt;
&lt;p&gt;For the coordinates use: &lt;code&gt;com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1&lt;/code&gt;.  Ensure this library is
attached to all clusters you create.&lt;/p&gt;
&lt;p&gt;Finally, ensure that your Spark cluster has at least Spark 2.1 and Scala 2.11.&lt;/p&gt;
&lt;p&gt;You can use MMLSpark in both your Scala and PySpark notebooks. To get started with our example notebooks import the following databricks archive:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;https://mmlspark.blob.core.windows.net/dbcs/MMLSpark%20Examples%20v1.0.0-rc1.dbc&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-docker" class="anchor" aria-hidden="true" href="#docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker&lt;/h3&gt;
&lt;p&gt;The easiest way to evaluate MMLSpark is via our pre-built Docker container.  To
do so, run the following command:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker run -it -p 8888:8888 -e ACCEPT_EULA=yes mcr.microsoft.com/mmlspark/release&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Navigate to &lt;a href="http://localhost:8888/" rel="nofollow"&gt;http://localhost:8888/&lt;/a&gt; in your web browser to run the sample
notebooks.  See the &lt;a href="docs/docker.md"&gt;documentation&lt;/a&gt; for more on Docker use.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To read the EULA for using the docker image, run \
&lt;code&gt;docker run -it -p 8888:8888 mcr.microsoft.com/mmlspark/release eula&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-gpu-vm-setup" class="anchor" aria-hidden="true" href="#gpu-vm-setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GPU VM Setup&lt;/h3&gt;
&lt;p&gt;MMLSpark can be used to train deep learning models on GPU nodes from a Spark
application.  See the instructions for &lt;a href="docs/gpu-setup.md"&gt;setting up an Azure GPU
VM&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python" class="anchor" aria-hidden="true" href="#python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python&lt;/h3&gt;
&lt;p&gt;To try out MMLSpark on a Python (or Conda) installation you can get Spark
installed via pip with &lt;code&gt;pip install pyspark&lt;/code&gt;.  You can then use &lt;code&gt;pyspark&lt;/code&gt; as in
the above example, or from python:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; pyspark
spark &lt;span class="pl-k"&gt;=&lt;/span&gt; pyspark.sql.SparkSession.builder.appName(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;MyApp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) \
            .config(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;spark.jars.packages&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) \
            .getOrCreate()
&lt;span class="pl-k"&gt;import&lt;/span&gt; mmlspark&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-sbt" class="anchor" aria-hidden="true" href="#sbt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SBT&lt;/h3&gt;
&lt;p&gt;If you are building a Spark application in Scala, add the following lines to
your &lt;code&gt;build.sbt&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;libraryDependencies &lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;com.microsoft.ml.spark&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%%&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;mmlspark&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;1.0.0-rc1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-building-from-source" class="anchor" aria-hidden="true" href="#building-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building from source&lt;/h3&gt;
&lt;p&gt;MMLSpark has recently transitioned to a new build infrastructure.
For detailed developer docs please see the &lt;a href="docs/developer-readme.md"&gt;Developer Readme&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you are an existing mmlspark developer, you will need to reconfigure your
development setup. We now support platform independent development and
better integrate with intellij and SBT.
If you encounter issues please reach out to our support email!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-r-beta" class="anchor" aria-hidden="true" href="#r-beta"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;R (Beta)&lt;/h3&gt;
&lt;p&gt;To try out MMLSpark using the R autogenerated wrappers &lt;a href="docs/R-setup.md"&gt;see our
instructions&lt;/a&gt;.  Note: This feature is still under development
and some necessary custom wrappers may be missing.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-learn-more" class="anchor" aria-hidden="true" href="#learn-more"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learn More&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Visit our &lt;a href="https://mmlspark.blob.core.windows.net/website/index.html" title="aka.ms/spark" rel="nofollow"&gt;new website&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Watch our keynote demos at &lt;a href="https://youtu.be/T_fs4C0aqD0?t=425" rel="nofollow"&gt;the Spark+AI Summit 2019&lt;/a&gt;, &lt;a href="https://youtu.be/N3ozCZXeOeU?t=472" rel="nofollow"&gt;the Spark+AI European Summit 2018&lt;/a&gt;, and &lt;a href="https://databricks.com/sparkaisummit/north-america/spark-summit-2018-keynotes#Intelligent-cloud" title="Developing for the Intelligent Cloud and Intelligent Edge" rel="nofollow"&gt;the Spark+AI Summit 2018&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Read &lt;a href="https://arxiv.org/abs/1804.04031" title="Flexible and Scalable Deep Learning with MMLSpark" rel="nofollow"&gt;our paper&lt;/a&gt; for a deep dive on MMLSpark.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;See how MMLSpark is used to &lt;a href="https://www.microsoft.com/en-us/ai/ai-lab-stories?activetab=pivot1:primaryr3" title="Identifying snow leopards with AI" rel="nofollow"&gt;help endangered species&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Explore generative adversarial artwork in &lt;a href="https://www.microsoft.com/en-us/ai/ai-lab-stories?activetab=pivot1:primaryr4" title="Generative art at the MET" rel="nofollow"&gt;our collaboration with The MET and MIT&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Explore &lt;a href="https://blogs.technet.microsoft.com/machinelearning/2018/03/05/image-data-support-in-apache-spark/" title="Image Data Support in Apache Spark" rel="nofollow"&gt;our collaboration with Apache Spark&lt;/a&gt; on image analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use &lt;a href="https://docs.microsoft.com/en-us/azure/machine-learning/preview/how-to-use-mmlspark" title="How to Use Microsoft Machine Learning Library for Apache Spark" rel="nofollow"&gt;MMLSpark in Azure Machine Learning&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributing--feedback" class="anchor" aria-hidden="true" href="#contributing--feedback"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing &amp;amp; feedback&lt;/h2&gt;
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;.  For more
information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow"&gt;Code of Conduct FAQ&lt;/a&gt; or contact
&lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional
questions or comments.&lt;/p&gt;
&lt;p&gt;See &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for contribution guidelines.&lt;/p&gt;
&lt;p&gt;To give feedback and/or report an issue, open a &lt;a href="https://help.github.com/articles/creating-an-issue/"&gt;GitHub
Issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-other-relevant-projects" class="anchor" aria-hidden="true" href="#other-relevant-projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other relevant projects&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/VowpalWabbit/vowpal_wabbit"&gt;Vowpal Wabbit&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/Microsoft/LightGBM"&gt;LightGBM&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/Microsoft/DMTK"&gt;DMTK: Microsoft Distributed Machine Learning Toolkit&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/Microsoft/Recommenders"&gt;Recommenders&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/alipay/jpmml-sparkml-lightgbm"&gt;JPMML-SparkML plugin for converting MMLSpark LightGBM models to
PMML&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/Microsoft/CNTK"&gt;Microsoft Cognitive Toolkit&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/machine-learning/preview" rel="nofollow"&gt;Azure Machine Learning
preview features&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Apache®, Apache Spark, and Spark® are either registered trademarks or
trademarks of the Apache Software Foundation in the United States and/or other
countries.&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Azure</author><guid isPermaLink="false">https://github.com/Azure/mmlspark</guid><pubDate>Sat, 02 Nov 2019 00:07:00 GMT</pubDate></item><item><title>lampepfl/dotty #8 in Scala, Today</title><link>https://github.com/lampepfl/dotty</link><description>&lt;p&gt;&lt;i&gt;Research compiler that will become Scala 3&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-dotty" class="anchor" aria-hidden="true" href="#dotty"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dotty&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://dotty-ci.epfl.ch/lampepfl/dotty" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3d1502dcc47d117fc84e52127fd5ded7d83ea5e3/687474703a2f2f646f7474792d63692e6570666c2e63682f6170692f6261646765732f6c616d706570666c2f646f7474792f7374617475732e737667" alt="Build Status" data-canonical-src="http://dotty-ci.epfl.ch/api/badges/lampepfl/dotty/status.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/lampepfl/dotty?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" alt="Join the chat at https://gitter.im/lampepfl/dotty" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/lampepfl/dotty-knowledge/issues/new/choose"&gt;&lt;img src="https://camo.githubusercontent.com/8dc0fde05ec65645ffa79fbae4b5e717923bdee4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6f672d6b6e6f776c656467652d626c756576696f6c65742e737667" alt="Log Knowledge" data-canonical-src="https://img.shields.io/badge/log-knowledge-blueviolet.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://dotty.epfl.ch" rel="nofollow"&gt;Homepage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://dotty.epfl.ch/docs" rel="nofollow"&gt;Documentation&lt;/a&gt; &lt;a href="https://travis-ci.org/nicolasstucki/dotty-website-linkcheck" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8416ff9b78df9058e6e6dbe7209b1501686b1883/68747470733a2f2f7472617669732d63692e6f72672f6e69636f6c6173737475636b692f646f7474792d776562736974652d6c696e6b636865636b2e7376673f6272616e63683d6d6173746572" alt="Deadlink Status" data-canonical-src="https://travis-ci.org/nicolasstucki/dotty-website-linkcheck.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-try-it-out" class="anchor" aria-hidden="true" href="#try-it-out"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Try it out&lt;/h1&gt;
&lt;p&gt;To try it in your project see also the &lt;a href="https://dotty.epfl.ch/#getting-started" rel="nofollow"&gt;Getting Started User Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-code-of-conduct" class="anchor" aria-hidden="true" href="#code-of-conduct"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code of Conduct&lt;/h1&gt;
&lt;p&gt;Dotty uses the &lt;a href="https://www.scala-lang.org/conduct.html" rel="nofollow"&gt;Scala Code of Conduct&lt;/a&gt;
for all communication and discussion. This includes both GitHub, Gitter chat and
other more direct lines of communication such as email.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-how-to-contribute" class="anchor" aria-hidden="true" href="#how-to-contribute"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Contribute&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://dotty.epfl.ch/docs/contributing/getting-started.html" rel="nofollow"&gt;Getting Started as Contributor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scala-lang.org/blog/2016/10/14/dotty-errors.html" rel="nofollow"&gt;Awesome Error Messages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lampepfl/dotty/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22"&gt;Issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contribute-internals-related-knowledge" class="anchor" aria-hidden="true" href="#contribute-internals-related-knowledge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribute Internals-related Knowledge&lt;/h2&gt;
&lt;p&gt;If you know anything useful at all about Dotty, feel free to log this knowledge:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/lampepfl/dotty-knowledge/issues/new/choose"&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt;Log the Knowledge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lampepfl/dotty-knowledge/blob/master/README.md"&gt;&lt;g-emoji class="g-emoji" alias="mortar_board" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png"&gt;🎓&lt;/g-emoji&gt;More about Logging the Knowledge&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In short, no need to make it pretty, particularly human-readable or give it a particular structure. Just dump the knowledge you have and we'll take it from there.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;Dotty is licensed under the &lt;a href="https://github.com/lampepfl/dotty/blob/master/LICENSE.md"&gt;3-Clause BSD License&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>lampepfl</author><guid isPermaLink="false">https://github.com/lampepfl/dotty</guid><pubDate>Sat, 02 Nov 2019 00:08:00 GMT</pubDate></item><item><title>scalanlp/breeze #9 in Scala, Today</title><link>https://github.com/scalanlp/breeze</link><description>&lt;p&gt;&lt;i&gt;Breeze is a numerical processing library for Scala.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-breeze-" class="anchor" aria-hidden="true" href="#breeze-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Breeze &lt;a href="https://travis-ci.org/scalanlp/breeze" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/365d69f871bc31a825bb8532a6ccaff3929e89cc/68747470733a2f2f7472617669732d63692e6f72672f7363616c616e6c702f627265657a652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/scalanlp/breeze.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Breeze is a library for numerical processing. It aims to be generic, clean, and powerful without sacrificing (much) efficiency.&lt;/p&gt;
&lt;p&gt;The latest release is 1.0, which is cross-built against Scala 2.11, 2.12, and 2.13.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/scalanlp/breeze/wiki/Quickstart"&gt;https://github.com/scalanlp/breeze/wiki/Quickstart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/scalanlp/breeze/wiki/Linear-Algebra-Cheat-Sheet"&gt;https://github.com/scalanlp/breeze/wiki/Linear-Algebra-Cheat-Sheet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.scalanlp.org/api/breeze/" rel="nofollow"&gt;Scaladoc&lt;/a&gt; (Scaladoc is typically horribly out of date, and not a good way to learn Breeze.)&lt;/li&gt;
&lt;li&gt;There is also the &lt;a href="https://groups.google.com/forum/#!forum/scala-breeze" rel="nofollow"&gt;scala-breeze google group&lt;/a&gt; for general questions and discussion.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-using-breeze" class="anchor" aria-hidden="true" href="#using-breeze"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Breeze&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-building-it-yourself" class="anchor" aria-hidden="true" href="#building-it-yourself"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building it yourself&lt;/h3&gt;
&lt;p&gt;This project can be built with SBT 1.2+&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-sbt" class="anchor" aria-hidden="true" href="#sbt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SBT&lt;/h3&gt;
&lt;p&gt;For SBT, add these lines to your SBT project definition:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;libraryDependencies  &lt;span class="pl-k"&gt;++&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;Seq&lt;/span&gt;(
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; Last stable release&lt;/span&gt;
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;org.scalanlp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%%&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;breeze&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;1.0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
  
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; Native libraries are not included by default. add this if you want them&lt;/span&gt;
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; Native libraries greatly improve performance, but increase jar sizes. &lt;/span&gt;
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; It also packages various blas implementations, which have licenses that may or may not&lt;/span&gt;
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; be compatible with the Apache License. No GPL code, as best I know.&lt;/span&gt;
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;org.scalanlp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%%&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;breeze-natives&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;1.0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
  
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; The visualization library is distributed separately as well.&lt;/span&gt;
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; It depends on LGPL code&lt;/span&gt;
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;org.scalanlp&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%%&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;breeze-viz&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;1.0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
)

&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more details on the optional &lt;code&gt;breeze-natives&lt;/code&gt; module, please watch Sam Halliday's talk at Scala eXchange 2014 &lt;a href="https://skillsmatter.com/skillscasts/5849-high-performance-linear-algebra-in-scala" rel="nofollow"&gt;High Performance Linear Algebra in Scala&lt;/a&gt; (&lt;a href="http://fommil.github.io/scalax14/#/" rel="nofollow"&gt;follow along with high-res slides&lt;/a&gt;).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-maven" class="anchor" aria-hidden="true" href="#maven"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Maven&lt;/h3&gt;
&lt;p&gt;Maven looks like this:&lt;/p&gt;
&lt;div class="highlight highlight-text-xml"&gt;&lt;pre&gt;&amp;lt;&lt;span class="pl-ent"&gt;dependency&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class="pl-ent"&gt;groupId&lt;/span&gt;&amp;gt;org.scalanlp&amp;lt;/&lt;span class="pl-ent"&gt;groupId&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class="pl-ent"&gt;artifactId&lt;/span&gt;&amp;gt;breeze_2.13&amp;lt;/&lt;span class="pl-ent"&gt;artifactId&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span class="pl-ent"&gt;version&lt;/span&gt;&amp;gt;1.0&amp;lt;/&lt;span class="pl-ent"&gt;version&lt;/span&gt;&amp;gt;
&amp;lt;/&lt;span class="pl-ent"&gt;dependency&lt;/span&gt;&amp;gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-other-build-tools" class="anchor" aria-hidden="true" href="#other-build-tools"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other build tools&lt;/h3&gt;
&lt;p&gt;[&lt;a href="http://mvnrepository.com/artifact/org.scalanlp/breeze_2.12/1.0" rel="nofollow"&gt;http://mvnrepository.com/artifact/org.scalanlp/breeze_2.12/1.0&lt;/a&gt;] (as an example) is a great resource for finding other configuration examples for other build tools.&lt;/p&gt;
&lt;p&gt;See documentation (linked above!) for more information on using Breeze.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-history" class="anchor" aria-hidden="true" href="#history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;History&lt;/h2&gt;
&lt;p&gt;Breeze is the merger of the ScalaNLP and Scalala projects, because one of the original maintainers is unable to continue development. The Scalala parts are largely rewritten.&lt;/p&gt;
&lt;p&gt;(c) David Hall, 2009 -&lt;/p&gt;
&lt;p&gt;Portions (c) Daniel Ramage, 2009 - 2011&lt;/p&gt;
&lt;p&gt;Contributions from:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jason Zaugg (@retronym)&lt;/li&gt;
&lt;li&gt;Alexander Lehmann (@afwlehmann)&lt;/li&gt;
&lt;li&gt;Jonathan Merritt (@lancelet)&lt;/li&gt;
&lt;li&gt;Keith Stevens (@fozziethebeat)&lt;/li&gt;
&lt;li&gt;Jason Baldridge (@jasonbaldridge)&lt;/li&gt;
&lt;li&gt;Timothy Hunter (@tjhunter)&lt;/li&gt;
&lt;li&gt;Dave DeCaprio (@DaveDeCaprio)&lt;/li&gt;
&lt;li&gt;Daniel Duckworth (@duckworthd)&lt;/li&gt;
&lt;li&gt;Eric Christiansen (@emchristiansen)&lt;/li&gt;
&lt;li&gt;Marc Millstone (@splittingfield)&lt;/li&gt;
&lt;li&gt;Mérő László (@laci37)&lt;/li&gt;
&lt;li&gt;Alexey Noskov (@alno)&lt;/li&gt;
&lt;li&gt;Devon Bryant (@devonbryant)&lt;/li&gt;
&lt;li&gt;Kentaroh Takagaki (@ktakagaki)&lt;/li&gt;
&lt;li&gt;Sam Halliday (@fommil)&lt;/li&gt;
&lt;li&gt;Chris Stucchio (@stucchio)&lt;/li&gt;
&lt;li&gt;Xiangrui Meng (@mengxr)&lt;/li&gt;
&lt;li&gt;Gabriel Schubiner (@gabeos)&lt;/li&gt;
&lt;li&gt;Debasish Das (@debasish83)&lt;/li&gt;
&lt;li&gt;Julien Dumazert (@DumazertJulien)&lt;/li&gt;
&lt;li&gt;Matthias Langer (@bashimao)&lt;/li&gt;
&lt;li&gt;Mohamed Kafsi (@mou7)&lt;/li&gt;
&lt;li&gt;Max Thomas (@maxthomas)&lt;/li&gt;
&lt;li&gt;@qilab&lt;/li&gt;
&lt;li&gt;Weichen Xu (@WeichenXu123)&lt;/li&gt;
&lt;li&gt;Sergei Lebedev (@superbobry)&lt;/li&gt;
&lt;li&gt;Zac Blanco (@ZacBlanco)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Corporate (Code) Contributors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.semanticmachines.com/" rel="nofollow"&gt;Semantic Machines&lt;/a&gt; (@semanticmachines)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.contentsquare.com/en/" rel="nofollow"&gt;ContentSquare&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Big Data Analytics, Verizon Lab, Palo Alto&lt;/li&gt;
&lt;li&gt;&lt;a href="https://crealytics.com/" rel="nofollow"&gt;crealytics GmbH, Berlin/Passau, Germany&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And others (contact David Hall if you've contributed and aren't listed).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>scalanlp</author><guid isPermaLink="false">https://github.com/scalanlp/breeze</guid><pubDate>Sat, 02 Nov 2019 00:09:00 GMT</pubDate></item><item><title>http4s/http4s #10 in Scala, Today</title><link>https://github.com/http4s/http4s</link><description>&lt;p&gt;&lt;i&gt;A minimal, idiomatic Scala interface for HTTP&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-http4s-----" class="anchor" aria-hidden="true" href="#http4s-----"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Http4s &lt;a href="https://travis-ci.org/http4s/http4s" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a9022c01e1ad1eb4cbfaa85468c1c1587173c19e/68747470733a2f2f7472617669732d63692e6f72672f6874747034732f6874747034732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/http4s/http4s.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://gitter.im/http4s/http4s" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/be1902f93402d993f93bfc2e666ddb0cb5b4dd6a/68747470733a2f2f6261646765732e6769747465722e696d2f6874747034732f6874747034732e706e67" alt="Gitter chat" data-canonical-src="https://badges.gitter.im/http4s/http4s.png" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://maven-badges.herokuapp.com/maven-central/org.http4s/http4s-core_2.12" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/01334c24d98856a8c00dbc8d82e549d2cdc31f29/68747470733a2f2f6d6176656e2d6261646765732e6865726f6b756170702e636f6d2f6d6176656e2d63656e7472616c2f6f72672e6874747034732f6874747034732d636f72655f322e31322f62616467652e737667" alt="Maven Central" data-canonical-src="https://maven-badges.herokuapp.com/maven-central/org.http4s/http4s-core_2.12/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://typelevel.org/projects/#http4s" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3b0e74dc481efe47f57b9f293fc57cf49637119a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f747970656c6576656c2d6c6962726172792d677265656e2e737667" alt="Typelevel library" data-canonical-src="https://img.shields.io/badge/typelevel-library-green.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://typelevel.org/cats/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2940747bf90ab8763b220669379d3a5190bde2fa/68747470733a2f2f747970656c6576656c2e6f72672f636174732f696d672f636174732d62616467652e737667" height="40px" align="right" alt="Cats friendly" data-canonical-src="https://typelevel.org/cats/img/cats-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Http4s is a minimal, idiomatic Scala interface for HTTP services.  Http4s is
Scala's answer to Ruby's Rack, Python's WSGI, Haskell's WAI, and Java's
Servlets.&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;http&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;HttpRoutes&lt;/span&gt;.of {
  &lt;span class="pl-k"&gt;case&lt;/span&gt; &lt;span class="pl-en"&gt;GET&lt;/span&gt; &lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-en"&gt;Root&lt;/span&gt; &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;hello&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt;
    &lt;span class="pl-en"&gt;Ok&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Hello, better world.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Learn more at &lt;a href="https://http4s.org/" rel="nofollow"&gt;http4s.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you run into any difficulties please enable partial unification in your &lt;code&gt;build.sbt&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;scalacOptions &lt;span class="pl-k"&gt;++&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;Seq&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;-Ypartial-unification&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-code-of-conduct" class="anchor" aria-hidden="true" href="#code-of-conduct"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code of Conduct&lt;/h2&gt;
&lt;p&gt;http4s is proud to be a &lt;a href="https://typelevel.org/" rel="nofollow"&gt;Typelevel&lt;/a&gt; incubator
project.  We are committed to providing a friendly, safe and welcoming
environment for all, and ask that the community adhere to the &lt;a href="https://http4s.org/code-of-conduct/" rel="nofollow"&gt;Scala
Code of Conduct&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This software is licensed under the Apache 2 license, quoted below.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Copyright 2013-2019 http4s [&lt;a href="https://http4s.org/" rel="nofollow"&gt;https://http4s.org&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[&lt;a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgments" class="anchor" aria-hidden="true" href="#acknowledgments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.yourkit.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/97fa03cac759a772255b93c64ab1c9f76a103681/68747470733a2f2f7777772e796f75726b69742e636f6d2f696d616765732f796b6c6f676f2e706e67" alt="YourKit" data-canonical-src="https://www.yourkit.com/images/yklogo.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Special thanks to &lt;a href="https://www.yourkit.com/" rel="nofollow"&gt;YourKit&lt;/a&gt; for supporting this project's ongoing performance tuning efforts with licenses to their excellent product.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>http4s</author><guid isPermaLink="false">https://github.com/http4s/http4s</guid><pubDate>Sat, 02 Nov 2019 00:10:00 GMT</pubDate></item><item><title>playframework/playframework #11 in Scala, Today</title><link>https://github.com/playframework/playframework</link><description>&lt;p&gt;&lt;i&gt;Play Framework&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-play-framework---the-high-velocity-web-framework" class="anchor" aria-hidden="true" href="#play-framework---the-high-velocity-web-framework"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Play Framework - The High Velocity Web Framework&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://gitter.im/playframework/playframework?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e9c60a0e4c534f0f6e4b18c0d71e337e683104c3/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f67697474657248512f6769747465722e737667" alt="Gitter" data-canonical-src="https://img.shields.io/gitter/room/gitterHQ/gitter.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://travis-ci.org/playframework/playframework" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2494b1b0c8a9027b30f41fe465e0333d3b2db61f/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f706c61796672616d65776f726b2f706c61796672616d65776f726b2e737667" data-canonical-src="https://img.shields.io/travis/playframework/playframework.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="http://mvnrepository.com/artifact/com.typesafe.play/play_2.13" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ab77956dd4d9def7cc89402b168f2d8ca2e49883/68747470733a2f2f696d672e736869656c64732e696f2f6d6176656e2d63656e7472616c2f762f636f6d2e74797065736166652e706c61792f706c61795f322e31332e737667" alt="Maven" data-canonical-src="https://img.shields.io/maven-central/v/com.typesafe.play/play_2.13.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Play Framework combines productivity and performance making it easy to build scalable web applications with Java and Scala.  Play is developer friendly with a "just hit refresh" workflow and built-in testing support.  With Play, applications scale predictably due to a stateless and non-blocking architecture.  By being RESTful by default, including assets compilers, JSON &amp;amp; WebSocket support, Play is a perfect fit for modern web &amp;amp; mobile applications.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-learn-more" class="anchor" aria-hidden="true" href="#learn-more"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learn More&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.playframework.com" rel="nofollow"&gt;www.playframework.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.playframework.com/download" rel="nofollow"&gt;Download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.playframework.com/documentation/latest/Installing" rel="nofollow"&gt;Install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.playframework.com/documentation/latest/NewApplication" rel="nofollow"&gt;Create a new application&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.playframework.com/documentation/latest/ScalaHome" rel="nofollow"&gt;Play for Scala developers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.playframework.com/documentation/latest/JavaHome" rel="nofollow"&gt;Play for Java developers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.playframework.com/documentation/latest/BuildingFromSource" rel="nofollow"&gt;Build from source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/playframework/playframework/issues"&gt;Search or create issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/tagged/playframework" rel="nofollow"&gt;Get help&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.playframework.com/contributing" rel="nofollow"&gt;Contribute&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright (C) 2009-2019 Lightbend Inc. (&lt;a href="https://www.lightbend.com" rel="nofollow"&gt;https://www.lightbend.com&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Licensed under the Apache License, Version 2.0 (the "License"); you may not use this project except in compliance with the License. You may obtain a copy of the License at &lt;a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>playframework</author><guid isPermaLink="false">https://github.com/playframework/playframework</guid><pubDate>Sat, 02 Nov 2019 00:11:00 GMT</pubDate></item><item><title>intel-analytics/BigDL #12 in Scala, Today</title><link>https://github.com/intel-analytics/BigDL</link><description>&lt;p&gt;&lt;i&gt;BigDL: Distributed Deep Learning Library for Apache Spark&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/bigdl-project/bigdl-project.github.io/blob/master/img/bigdl-logo-bw.jpg"&gt;&lt;img src="https://github.com/bigdl-project/bigdl-project.github.io/raw/master/img/bigdl-logo-bw.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-bigdl-distributed-deep-learning-on-apache-spark" class="anchor" aria-hidden="true" href="#bigdl-distributed-deep-learning-on-apache-spark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BigDL: Distributed Deep Learning on Apache Spark&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-bigdl" class="anchor" aria-hidden="true" href="#what-is-bigdl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is BigDL?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://bigdl-project.github.io/master/#whitepaper/" rel="nofollow"&gt;BigDL&lt;/a&gt; is a distributed deep learning library for Apache Spark; with BigDL, users can write their deep learning applications as standard Spark programs, which can directly run on top of existing Spark or Hadoop clusters. To makes it easy to build Spark and BigDL applications, a high level &lt;a href="https://github.com/intel-analytics/analytics-zoo"&gt;Analytics Zoo&lt;/a&gt; is provided for end-to-end analytics + AI pipelines.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rich deep learning support.&lt;/strong&gt; Modeled after &lt;a href="http://torch.ch/" rel="nofollow"&gt;Torch&lt;/a&gt;, BigDL provides comprehensive support for deep learning, including numeric computing (via &lt;a href="https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/tensor"&gt;Tensor&lt;/a&gt;) and high level &lt;a href="https://github.com/intel-analytics/BigDL/tree/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/nn"&gt;neural networks&lt;/a&gt;; in addition, users can load pre-trained &lt;a href="http://caffe.berkeleyvision.org/" rel="nofollow"&gt;Caffe&lt;/a&gt; or &lt;a href="http://torch.ch/" rel="nofollow"&gt;Torch&lt;/a&gt; models into Spark programs using BigDL.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Extremely high performance.&lt;/strong&gt; To achieve high performance, BigDL uses &lt;a href="https://software.intel.com/en-us/intel-mkl" rel="nofollow"&gt;Intel MKL&lt;/a&gt; / &lt;a href="https://software.intel.com/en-us/articles/intel-mkl-dnn-part-1-library-overview-and-installation" rel="nofollow"&gt;Intel MKL-DNN&lt;/a&gt; and multi-threaded programming in each Spark task. Consequently, it is orders of magnitude faster than out-of-box open source &lt;a href="http://caffe.berkeleyvision.org/" rel="nofollow"&gt;Caffe&lt;/a&gt;, &lt;a href="http://torch.ch/" rel="nofollow"&gt;Torch&lt;/a&gt; or &lt;a href="https://www.tensorflow.org/" rel="nofollow"&gt;TensorFlow&lt;/a&gt; on a single-node Xeon (i.e., comparable with mainstream GPU). With adoption of &lt;a href="https://www.intel.ai/intel-deep-learning-boost/" rel="nofollow"&gt;Intel DL Boost&lt;/a&gt;, BigDL improves inference latency and throughput significantly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Efficiently scale-out.&lt;/strong&gt; BigDL can efficiently scale out to perform data analytics at "Big Data scale", by leveraging &lt;a href="http://spark.apache.org/" rel="nofollow"&gt;Apache Spark&lt;/a&gt; (a lightning fast distributed data processing framework), as well as efficient implementations of synchronous SGD and all-reduce communications on Spark.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-why-bigdl" class="anchor" aria-hidden="true" href="#why-bigdl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why BigDL?&lt;/h2&gt;
&lt;p&gt;You may want to write your deep learning programs using BigDL if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You want to analyze a large amount of data on the same Big Data (Hadoop/Spark) cluster where the data are stored (in, say, HDFS, HBase, Hive, etc.).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You want to add deep learning functionalities (either training or prediction) to your Big Data (Spark) programs and/or workflow.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You want to leverage existing Hadoop/Spark clusters to run your deep learning applications, which can be then dynamically shared with other workloads (e.g., ETL, data warehouse, feature engineering, classical machine learning, graph analytics, etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use-bigdl" class="anchor" aria-hidden="true" href="#how-to-use-bigdl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use BigDL?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For the technical overview of BigDL, please refer to the &lt;a href="https://bigdl-project.github.io/master/#whitepaper/" rel="nofollow"&gt;BigDL white paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;More information can be found at the BigDL project website:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bigdl-project.github.io/" rel="nofollow"&gt;https://bigdl-project.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In particular, you can check out the &lt;a href="https://bigdl-project.github.io/master/#getting-started/" rel="nofollow"&gt;Getting Started page&lt;/a&gt; for a quick overview of how to use BigDL&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For step-by-step deep leaning tutorials on BigDL (using Python), you can check out the &lt;a href="https://github.com/intel-analytics/BigDL-tutorials"&gt;BigDL Tutorials project&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can join the &lt;a href="https://groups.google.com/forum/#!forum/bigdl-user-group" rel="nofollow"&gt;BigDL Google Group&lt;/a&gt; (or subscribe to the &lt;a href="mailto:bigdl-user-group+subscribe@googlegroups.com"&gt;Mail List&lt;/a&gt;) for more questions and discussions on BigDL&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can post bug reports and feature requests at the &lt;a href="https://github.com/intel-analytics/BigDL/issues"&gt;Issue Page&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You may refer to &lt;a href="https://github.com/intel-analytics/analytics-zoo"&gt;Analytics Zoo&lt;/a&gt; for high level pipeline APIs, built-in deep learning models, reference use cases, etc. on Spark and BigDL&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>intel-analytics</author><guid isPermaLink="false">https://github.com/intel-analytics/BigDL</guid><pubDate>Sat, 02 Nov 2019 00:12:00 GMT</pubDate></item><item><title>spark-jobserver/spark-jobserver #13 in Scala, Today</title><link>https://github.com/spark-jobserver/spark-jobserver</link><description>&lt;p&gt;&lt;i&gt;REST job server for Apache Spark&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://travis-ci.org/spark-jobserver/spark-jobserver" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c2f25b2f01af437fafa169b6a3fd7a59fe963ef0/68747470733a2f2f7472617669732d63692e6f72672f737061726b2d6a6f627365727665722f737061726b2d6a6f627365727665722e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/spark-jobserver/spark-jobserver.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/spark-jobserver/spark-jobserver/branch/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5465af4128b7d50cf4dd4a7e92996d4a7e741706/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f737061726b2d6a6f627365727665722f737061726b2d6a6f627365727665722f6d61737465722e737667" alt="Coverage" data-canonical-src="https://img.shields.io/codecov/c/github/spark-jobserver/spark-jobserver/master.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gitter.im/spark-jobserver/spark-jobserver?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" alt="Join the chat at https://gitter.im/spark-jobserver/spark-jobserver" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;spark-jobserver provides a RESTful interface for submitting and managing &lt;a href="http://spark-project.org" rel="nofollow"&gt;Apache Spark&lt;/a&gt; jobs, jars, and job contexts.
This repo contains the complete Spark job server project, including unit tests and deploy scripts.
It was originally started at &lt;a href="http://www.ooyala.com" rel="nofollow"&gt;Ooyala&lt;/a&gt;, but this is now the main development repo.&lt;/p&gt;
&lt;p&gt;Other useful links: &lt;a href="doc/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt;, &lt;a href="doc/cluster.md"&gt;cluster&lt;/a&gt;, &lt;a href="doc/yarn.md"&gt;YARN client&lt;/a&gt;, &lt;a href="doc/EMR.md"&gt;YARN on EMR&lt;/a&gt;, &lt;a href="doc/mesos.md"&gt;Mesos&lt;/a&gt;, &lt;a href="doc/jmx.md"&gt;JMX tips&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also see &lt;a href="doc/chinese/job-server.md"&gt;Chinese docs / 中文&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;  &lt;em&gt;generated with &lt;a href="https://github.com/thlorenz/doctoc"&gt;DocToc&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#users"&gt;Users&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#features"&gt;Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#version-information"&gt;Version Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-started-with-spark-job-server"&gt;Getting Started with Spark Job Server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#development-mode"&gt;Development mode&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#wordcountexample-walk-through"&gt;WordCountExample walk-through&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#package-jar---send-to-server"&gt;Package Jar - Send to Server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ad-hoc-mode---single-unrelated-jobs-transient-context"&gt;Ad-hoc Mode - Single, Unrelated Jobs (Transient Context)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#persistent-context-mode---faster--required-for-related-jobs"&gt;Persistent Context Mode - Faster &amp;amp; Required for Related Jobs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#debug-mode"&gt;Debug mode&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#create-a-job-server-project"&gt;Create a Job Server Project&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#creating-a-project-from-scratch-using-giter8-template"&gt;Creating a project from scratch using giter8 template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#creating-a-project-manually-assuming-that-you-already-have-sbt-project-structure"&gt;Creating a project manually assuming that you already have sbt project structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#new-sparkjob-api"&gt;NEW SparkJob API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#new-sparkjob-api-with-spark-v21"&gt;NEW SparkJob API with Spark v2.1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dependency-jars"&gt;Dependency jars&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#named-objects"&gt;Named Objects&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#using-named-rdds"&gt;Using Named RDDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-named-objects"&gt;Using Named Objects&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#https--ssl-configuration"&gt;HTTPS / SSL Configuration&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#server-authentication"&gt;Server authentication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#client-authentication"&gt;Client authentication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#basic-authentication"&gt;Basic authentication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#deployment"&gt;Deployment&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#manual-steps"&gt;Manual steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#context-per-jvm"&gt;Context per JVM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#configuring-spark-jobserver-backend"&gt;Configuring Spark Jobserver backend&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#configuring-spark-jobserver-h2-database-backend"&gt;Configuring Spark Jobserver H2 Database backend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#configuring-spark-jobserver-postgresql-database-backend"&gt;Configuring Spark Jobserver PostgreSQL Database backend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#configuring-spark-jobserver-mysql-database-backend"&gt;Configuring Spark Jobserver MySQL Database backend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#configuring-spark-jobserver-zookeeper--hdfs-database-backend"&gt;Configuring Spark Jobserver Zookeeper + HDFS Database backend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#ha-deployment-beta"&gt;HA Deployment (beta)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#chef"&gt;Chef&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#architecture"&gt;Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#api"&gt;API&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#binaries"&gt;Binaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contexts"&gt;Contexts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#jobs"&gt;Jobs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data"&gt;Data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#data-api-example"&gt;Data API Example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#context-configuration"&gt;Context configuration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-configuration-settings"&gt;Other configuration settings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#job-result-serialization"&gt;Job Result Serialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#http-override"&gt;HTTP Override&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#clients"&gt;Clients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contribution-and-development"&gt;Contribution and Development&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#publishing-packages"&gt;Publishing packages&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#contact"&gt;Contact&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#todo"&gt;TODO&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;&lt;a id="user-content-users" class="anchor" aria-hidden="true" href="#users"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Users&lt;/h2&gt;
&lt;p&gt;(Please add yourself to this list!)&lt;/p&gt;
&lt;p&gt;Spark Job Server is included in Datastax Enterprise!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.ooyala.com" rel="nofollow"&gt;Ooyala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.netflix.com" rel="nofollow"&gt;Netflix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.avenida.com" rel="nofollow"&gt;Avenida.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GumGum&lt;/li&gt;
&lt;li&gt;Fuse Elements&lt;/li&gt;
&lt;li&gt;Frontline Solvers&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.arubanetworks.com/" rel="nofollow"&gt;Aruba Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.zed.com" rel="nofollow"&gt;Zed Worldwide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.knime.org/" rel="nofollow"&gt;KNIME&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://azavea.com" rel="nofollow"&gt;Azavea&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://maana.io/" rel="nofollow"&gt;Maana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.newsweaver.com" rel="nofollow"&gt;Newsweaver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.instaclustr.com" rel="nofollow"&gt;Instaclustr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.snappydata.io" rel="nofollow"&gt;SnappyData&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.linkfluence.com" rel="nofollow"&gt;Linkfluence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.smartsct.com" rel="nofollow"&gt;Smartsct&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.datadoghq.com/" rel="nofollow"&gt;Datadog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.planalytics.com" rel="nofollow"&gt;Planalytics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.target.com/" rel="nofollow"&gt;Target&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://branch.io" rel="nofollow"&gt;Branch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;"Spark as a Service"&lt;/em&gt;: Simple REST interface (including HTTPS) for all aspects of job, context management&lt;/li&gt;
&lt;li&gt;Support for Spark SQL, Hive, Streaming Contexts/jobs and custom job contexts!  See &lt;a href="doc/contexts.md"&gt;Contexts&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="doc/python.md"&gt;Python&lt;/a&gt;, Scala, and &lt;a href="doc/javaapi.md"&gt;Java&lt;/a&gt; (see &lt;a href="https://github.com/spark-jobserver/spark-jobserver/blob/master/job-server-api/src/main/java/spark/jobserver/api/TestJob.java"&gt;TestJob.java&lt;/a&gt;) support&lt;/li&gt;
&lt;li&gt;LDAP Auth support via Apache Shiro integration&lt;/li&gt;
&lt;li&gt;Separate JVM per SparkContext for isolation (EXPERIMENTAL)&lt;/li&gt;
&lt;li&gt;Supports sub-second low-latency jobs via long-running job contexts&lt;/li&gt;
&lt;li&gt;Start and stop job contexts for RDD sharing and low-latency jobs; change resources on restart&lt;/li&gt;
&lt;li&gt;Kill running jobs via stop context and delete job&lt;/li&gt;
&lt;li&gt;Separate jar uploading step for faster job startup&lt;/li&gt;
&lt;li&gt;Asynchronous and synchronous job API.  Synchronous API is great for low latency jobs!&lt;/li&gt;
&lt;li&gt;Works with Standalone Spark as well on &lt;a href="doc/cluster.md"&gt;cluster&lt;/a&gt;, &lt;a href="doc/mesos.md"&gt;Mesos&lt;/a&gt;, YARN &lt;a href="doc/yarn.md"&gt;client&lt;/a&gt; and &lt;a href="doc/EMR.md"&gt;on EMR&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Job and jar info is persisted via a pluggable DAO interface&lt;/li&gt;
&lt;li&gt;Named Objects (such as RDDs or DataFrames) to cache and retrieve RDDs or DataFrames by name, improving object sharing and reuse among jobs.&lt;/li&gt;
&lt;li&gt;Supports Scala 2.10 and 2.11&lt;/li&gt;
&lt;li&gt;Support for supervise mode of Spark (EXPERIMENTAL)&lt;/li&gt;
&lt;li&gt;Possible to be deployed in an &lt;a href="#ha-deployment-beta"&gt;HA setup&lt;/a&gt; of multiple jobservers (beta)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-version-information" class="anchor" aria-hidden="true" href="#version-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Version Information&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Spark Version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.3.1&lt;/td&gt;
&lt;td&gt;0.9.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.4.0&lt;/td&gt;
&lt;td&gt;1.0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.4.1&lt;/td&gt;
&lt;td&gt;1.1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.5.0&lt;/td&gt;
&lt;td&gt;1.2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.5.1&lt;/td&gt;
&lt;td&gt;1.3.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.5.2&lt;/td&gt;
&lt;td&gt;1.3.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.6.0&lt;/td&gt;
&lt;td&gt;1.4.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.6.1&lt;/td&gt;
&lt;td&gt;1.5.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.6.2&lt;/td&gt;
&lt;td&gt;1.6.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.7.0&lt;/td&gt;
&lt;td&gt;1.6.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.8.0&lt;/td&gt;
&lt;td&gt;2.2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.9.0-SNAPSHOT&lt;/td&gt;
&lt;td&gt;2.3.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For release notes, look in the &lt;code&gt;notes/&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;If you need non-released jars, please visit &lt;a href="https://jitpack.io" rel="nofollow"&gt;Jitpack&lt;/a&gt; - they provide non-release jar builds for any Git repo.  :)&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-with-spark-job-server" class="anchor" aria-hidden="true" href="#getting-started-with-spark-job-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started with Spark Job Server&lt;/h2&gt;
&lt;p&gt;The easiest way to get started is to try the &lt;a href="doc/docker.md"&gt;Docker container&lt;/a&gt; which prepackages a Spark distribution with the job server and lets you start and deploy it.&lt;/p&gt;
&lt;p&gt;Alternatives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Build and run Job Server in local &lt;a href="#development-mode"&gt;development mode&lt;/a&gt; within SBT.  NOTE:  This does NOT work for YARN, and in fact is only recommended with &lt;code&gt;spark.master&lt;/code&gt; set to &lt;code&gt;local[*]&lt;/code&gt;.  Please deploy if you want to try with YARN or other real cluster.&lt;/li&gt;
&lt;li&gt;Deploy job server to a cluster.  There are two alternatives (see the &lt;a href="#deployment"&gt;deployment section&lt;/a&gt;):
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;server_deploy.sh&lt;/code&gt;  deploys job server to a directory on a remote host.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;server_package.sh&lt;/code&gt; deploys job server to a local directory, from which you can deploy the directory, or create a .tar.gz for Mesos or YARN deployment.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;EC2 Deploy scripts - follow the instructions in &lt;a href="doc/EC2.md"&gt;EC2&lt;/a&gt; to spin up a Spark cluster with job server and an example application.&lt;/li&gt;
&lt;li&gt;EMR Deploy instruction - follow the instruction in &lt;a href="doc/EMR.md"&gt;EMR&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NOTE: Spark Job Server can optionally run &lt;code&gt;SparkContext&lt;/code&gt;s in their own, forked JVM process when the config option &lt;code&gt;spark.jobserver.context-per-jvm&lt;/code&gt; is set to &lt;code&gt;true&lt;/code&gt;.  This option does not currently work for SBT/local dev mode. See &lt;a href="#deployment"&gt;Deployment&lt;/a&gt; section for more info.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-development-mode" class="anchor" aria-hidden="true" href="#development-mode"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development mode&lt;/h2&gt;
&lt;p&gt;The example walk-through below shows you how to use the job server with an included example job, by running the job server in local development mode in SBT.  This is not an example of usage in production.&lt;/p&gt;
&lt;p&gt;You need to have &lt;a href="http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html" rel="nofollow"&gt;SBT&lt;/a&gt; installed.&lt;/p&gt;
&lt;p&gt;To set the current version, do something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export VER=`sbt version | tail -1 | cut -f2`
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From SBT shell, simply type "reStart".  This uses a default configuration file.  An optional argument is a
path to an alternative config file.  You can also specify JVM parameters after "---".  Including all the
options looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;job-server-extras/reStart /path/to/my.conf --- -Xmx8g
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that reStart (SBT Revolver) forks the job server in a separate process.  If you make a code change, simply
type reStart again at the SBT shell prompt, it will compile your changes and restart the jobserver.  It enables
very fast turnaround cycles.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE2&lt;/strong&gt;: You cannot do &lt;code&gt;sbt reStart&lt;/code&gt; from the OS shell.  SBT will start job server and immediately kill it.&lt;/p&gt;
&lt;p&gt;For example jobs see the job-server-tests/ project / folder.&lt;/p&gt;
&lt;p&gt;When you use &lt;code&gt;reStart&lt;/code&gt;, the log file goes to &lt;code&gt;job-server/job-server-local.log&lt;/code&gt;.  There is also an environment variable
EXTRA_JAR for adding a jar to the classpath.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-wordcountexample-walk-through" class="anchor" aria-hidden="true" href="#wordcountexample-walk-through"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WordCountExample walk-through&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-package-jar---send-to-server" class="anchor" aria-hidden="true" href="#package-jar---send-to-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Package Jar - Send to Server&lt;/h4&gt;
&lt;p&gt;First, to package the test jar containing the WordCountExample: &lt;code&gt;sbt job-server-tests/package&lt;/code&gt;.
Then go ahead and start the job server using the instructions above.&lt;/p&gt;
&lt;p&gt;Let's upload the jar:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -X POST localhost:8090/binaries/test -H "Content-Type: application/java-archive" --data-binary @job-server-tests/target/scala-2.10/job-server-tests-$VER.jar
OK⏎
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-ad-hoc-mode---single-unrelated-jobs-transient-context" class="anchor" aria-hidden="true" href="#ad-hoc-mode---single-unrelated-jobs-transient-context"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ad-hoc Mode - Single, Unrelated Jobs (Transient Context)&lt;/h4&gt;
&lt;p&gt;The above jar is uploaded as app &lt;code&gt;test&lt;/code&gt;.  Next, let's start an ad-hoc word count job, meaning that the job
server will create its own SparkContext, and return a job ID for subsequent querying:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -d "input.string = a b c a b see" "localhost:8090/jobs?appName=test&amp;amp;classPath=spark.jobserver.WordCountExample"
{
  "duration": "Job not done yet",
  "classPath": "spark.jobserver.WordCountExample",
  "startTime": "2016-06-19T16:27:12.196+05:30",
  "context": "b7ea0eb5-spark.jobserver.WordCountExample",
  "status": "STARTED",
  "jobId": "5453779a-f004-45fc-a11d-a39dae0f9bf4"
}⏎
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: If you want to feed in a text file config and POST using curl, you want the &lt;code&gt;--data-binary&lt;/code&gt; option, otherwise
curl will munge your line separator chars.  Like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl --data-binary @my-job-config.json "localhost:8090/jobs?appNam=..."
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE2: If you want to send in UTF-8 chars, make sure you pass in a proper header to CURL for the encoding, otherwise it may assume an encoding which is not what you expect.&lt;/p&gt;
&lt;p&gt;From this point, you could asynchronously query the status and results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl localhost:8090/jobs/5453779a-f004-45fc-a11d-a39dae0f9bf4
{
  "duration": "6.341 secs",
  "classPath": "spark.jobserver.WordCountExample",
  "startTime": "2015-10-16T03:17:03.127Z",
  "context": "b7ea0eb5-spark.jobserver.WordCountExample",
  "result": {
    "a": 2,
    "b": 2,
    "c": 1,
    "see": 1
  },
  "status": "FINISHED",
  "jobId": "5453779a-f004-45fc-a11d-a39dae0f9bf4"
}⏎
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that you could append &lt;code&gt;&amp;amp;sync=true&lt;/code&gt; when you POST to /jobs to get the results back in one request, but for
real clusters and most jobs this may be too slow.&lt;/p&gt;
&lt;p&gt;You can also append &lt;code&gt;&amp;amp;timeout=XX&lt;/code&gt; to extend the request timeout for &lt;code&gt;sync=true&lt;/code&gt; requests.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-persistent-context-mode---faster--required-for-related-jobs" class="anchor" aria-hidden="true" href="#persistent-context-mode---faster--required-for-related-jobs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Persistent Context Mode - Faster &amp;amp; Required for Related Jobs&lt;/h4&gt;
&lt;p&gt;Another way of running this job is in a pre-created context.  Start a new context:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -d "" "localhost:8090/contexts/test-context?num-cpu-cores=4&amp;amp;memory-per-node=512m"
OK⏎
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can verify that the context has been created:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl localhost:8090/contexts
["test-context"]⏎
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let's run the job in the context and get the results back right away:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -d "input.string = a b c a b see" "localhost:8090/jobs?appName=test&amp;amp;classPath=spark.jobserver.WordCountExample&amp;amp;context=test-context&amp;amp;sync=true"
{
  "result": {
    "a": 2,
    "b": 2,
    "c": 1,
    "see": 1
  }
}⏎
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the addition of &lt;code&gt;context=&lt;/code&gt; and &lt;code&gt;sync=true&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-debug-mode" class="anchor" aria-hidden="true" href="#debug-mode"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Debug mode&lt;/h3&gt;
&lt;p&gt;Spark job server is started using SBT Revolver (which forks a new JVM), so debugging directly in an IDE is not feasible.
To enable debugging, the Spark job server should be started from the SBT shell with the following Java options :&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;job-server-extras/reStart /absolute/path/to/your/dev.conf --- -Xdebug -Xrunjdwp:transport=dt_socket,address=15000,server=y,suspend=y&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The above command starts a remote debugging server on port 15000. The Spark job server is not started until a debugging client
(Intellij, Eclipse, telnet, ...) connects to the exposed port.&lt;/p&gt;
&lt;p&gt;In your IDE you just have to start a Remote debugging debug job and use the above defined port. Once the client connects to the debugging server the Spark job server is started and you can start adding breakpoints and debugging requests.&lt;/p&gt;
&lt;p&gt;Note that you might need to adjust some server parameters to avoid short Spary/Akka/Spark timeouts, in your &lt;code&gt;dev.conf&lt;/code&gt; add the following values :&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;spark {
  jobserver {
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Dev debug timeouts&lt;/span&gt;
    context-creation-timeout = 1000000 s
    yarn-context-creation-timeout = 1000000 s
    default-sync-timeout = 1000000 s
  }

  context-settings {
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Dev debug timeout&lt;/span&gt;
    context-init-timeout = 1000000 s
  }
}
spray.can.server {
      &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Debug timeouts&lt;/span&gt;
      idle-timeout = infinite
      request-timeout = infinite
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Additionally, you might have to increase the Akka Timeouts by adding the following query parameter &lt;code&gt;timeout=1000000&lt;/code&gt; in your HTTP requests :&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;curl -d &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;input.string = a b c a b see&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;localhost:8090/jobs?appName=test&amp;amp;classPath=spark.jobserver.WordCountExample&amp;amp;sync=true&amp;amp;timeout=100000&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-create-a-job-server-project" class="anchor" aria-hidden="true" href="#create-a-job-server-project"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Create a Job Server Project&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-creating-a-project-from-scratch-using-giter8-template" class="anchor" aria-hidden="true" href="#creating-a-project-from-scratch-using-giter8-template"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Creating a project from scratch using giter8 template&lt;/h3&gt;
&lt;p&gt;There is a giter8 template available at &lt;a href="https://github.com/spark-jobserver/spark-jobserver.g8"&gt;https://github.com/spark-jobserver/spark-jobserver.g8&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sbt new spark-jobserver/spark-jobserver.g8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Answer the questions to generate a project structure for you. This contains Word Count example spark job using both old API and new one.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd /path/to/project/directory
$ sbt package
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you could remove example application and start adding your one.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-creating-a-project-manually-assuming-that-you-already-have-sbt-project-structure" class="anchor" aria-hidden="true" href="#creating-a-project-manually-assuming-that-you-already-have-sbt-project-structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Creating a project manually assuming that you already have sbt project structure&lt;/h3&gt;
&lt;p&gt;In your &lt;code&gt;build.sbt&lt;/code&gt;, add this to use the job server jar:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    resolvers += "Job Server Bintray" at "https://dl.bintray.com/spark-jobserver/maven"

    libraryDependencies += "spark.jobserver" %% "job-server-api" % "0.8.0" % "provided"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If a SQL or Hive job/context is desired, you also want to pull in &lt;code&gt;job-server-extras&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;libraryDependencies += "spark.jobserver" %% "job-server-extras" % "0.8.0" % "provided"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For most use cases it's better to have the dependencies be "provided" because you don't want SBT assembly to include the whole job server jar.&lt;/p&gt;
&lt;p&gt;To create a job that can be submitted through the job server, the job must implement the &lt;code&gt;SparkJob&lt;/code&gt; trait.
Your job will look like:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;object&lt;/span&gt; &lt;span class="pl-en"&gt;SampleJob&lt;/span&gt; &lt;span class="pl-k"&gt;extends&lt;/span&gt; &lt;span class="pl-e"&gt;SparkJob&lt;/span&gt; {
    &lt;span class="pl-k"&gt;override&lt;/span&gt; &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;runJob&lt;/span&gt;(&lt;span class="pl-v"&gt;sc&lt;/span&gt;: &lt;span class="pl-en"&gt;SparkContext&lt;/span&gt;, &lt;span class="pl-v"&gt;jobConfig&lt;/span&gt;: &lt;span class="pl-en"&gt;Config&lt;/span&gt;)&lt;span class="pl-k"&gt;:&lt;/span&gt; &lt;span class="pl-en"&gt;Any&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;???&lt;/span&gt;
    &lt;span class="pl-k"&gt;override&lt;/span&gt; &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;validate&lt;/span&gt;(&lt;span class="pl-v"&gt;sc&lt;/span&gt;: &lt;span class="pl-en"&gt;SparkContext&lt;/span&gt;, &lt;span class="pl-v"&gt;config&lt;/span&gt;: &lt;span class="pl-en"&gt;Config&lt;/span&gt;)&lt;span class="pl-k"&gt;:&lt;/span&gt; &lt;span class="pl-en"&gt;SparkJobValidation&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;???&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;runJob&lt;/code&gt; contains the implementation of the Job. The SparkContext is managed by the JobServer and will be provided to the job through this method.
This relieves the developer from the boiler-plate configuration management that comes with the creation of a Spark job and allows the Job Server to
manage and re-use contexts.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;validate&lt;/code&gt; allows for an initial validation of the context and any provided configuration. If the context and configuration are OK to run the job, returning &lt;code&gt;spark.jobserver.SparkJobValid&lt;/code&gt; will let the job execute, otherwise returning &lt;code&gt;spark.jobserver.SparkJobInvalid(reason)&lt;/code&gt; prevents the job from running and provides means to convey the reason of failure. In this case, the call immediately returns an &lt;code&gt;HTTP/1.1 400 Bad Request&lt;/code&gt; status code.
&lt;code&gt;validate&lt;/code&gt; helps you preventing running jobs that will eventually fail due to missing or wrong configuration and save both time and resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-new-sparkjob-api" class="anchor" aria-hidden="true" href="#new-sparkjob-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NEW SparkJob API&lt;/h3&gt;
&lt;p&gt;Note: As of version 0.7.0, a new SparkJob API that is significantly better than the old SparkJob API will take over.  Existing jobs should continue to compile against the old &lt;code&gt;spark.jobserver.SparkJob&lt;/code&gt; API, but this will be deprecated in the future.  Note that jobs before 0.7.0 will need to be recompiled, older jobs may not work with the current SJS example.  The new API looks like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;object&lt;/span&gt; &lt;span class="pl-en"&gt;WordCountExampleNewApi&lt;/span&gt; &lt;span class="pl-k"&gt;extends&lt;/span&gt; &lt;span class="pl-e"&gt;NewSparkJob&lt;/span&gt; {
  &lt;span class="pl-k"&gt;type&lt;/span&gt; &lt;span class="pl-en"&gt;JobData&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;Seq&lt;/span&gt;[&lt;span class="pl-k"&gt;String&lt;/span&gt;]
  &lt;span class="pl-k"&gt;type&lt;/span&gt; &lt;span class="pl-en"&gt;JobOutput&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; collection.&lt;span class="pl-en"&gt;Map&lt;/span&gt;[&lt;span class="pl-k"&gt;String&lt;/span&gt;, &lt;span class="pl-k"&gt;Long&lt;/span&gt;]

  &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;runJob&lt;/span&gt;(&lt;span class="pl-v"&gt;sc&lt;/span&gt;: &lt;span class="pl-en"&gt;SparkContext&lt;/span&gt;, &lt;span class="pl-v"&gt;runtime&lt;/span&gt;: &lt;span class="pl-en"&gt;JobEnvironment&lt;/span&gt;, &lt;span class="pl-v"&gt;data&lt;/span&gt;: &lt;span class="pl-en"&gt;JobData&lt;/span&gt;)&lt;span class="pl-k"&gt;:&lt;/span&gt; &lt;span class="pl-en"&gt;JobOutput&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt;
    sc.parallelize(data).countByValue

  &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;validate&lt;/span&gt;(&lt;span class="pl-v"&gt;sc&lt;/span&gt;: &lt;span class="pl-en"&gt;SparkContext&lt;/span&gt;, &lt;span class="pl-v"&gt;runtime&lt;/span&gt;: &lt;span class="pl-en"&gt;JobEnvironment&lt;/span&gt;, &lt;span class="pl-v"&gt;config&lt;/span&gt;: &lt;span class="pl-en"&gt;Config&lt;/span&gt;)&lt;span class="pl-k"&gt;:&lt;/span&gt;
    &lt;span class="pl-en"&gt;JobData&lt;/span&gt; &lt;span class="pl-en"&gt;Or&lt;/span&gt; &lt;span class="pl-en"&gt;Every&lt;/span&gt;[&lt;span class="pl-en"&gt;ValidationProblem&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; {
    &lt;span class="pl-en"&gt;Try&lt;/span&gt;(config.getString(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;input.string&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).split(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt; &lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).toSeq)
      .map(words &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-en"&gt;Good&lt;/span&gt;(words))
      .getOrElse(&lt;span class="pl-en"&gt;Bad&lt;/span&gt;(&lt;span class="pl-en"&gt;One&lt;/span&gt;(&lt;span class="pl-en"&gt;SingleProblem&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;No input.string param&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;))))
  }
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It is much more type safe, separates context configuration, job ID, named objects, and other environment variables into a separate JobEnvironment input, and allows the validation method to return specific data for the runJob method.  See the &lt;a href="job-server-tests/src/main/scala/spark/jobserver/WordCountExample.scala"&gt;WordCountExample&lt;/a&gt; and &lt;a href="job-server-tests/src/main/scala/spark/jobserver/LongPiJob.scala"&gt;LongPiJob&lt;/a&gt; for examples.&lt;/p&gt;
&lt;p&gt;Let's try running our sample job with an invalid configuration:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -i -d "bad.input=abc" "localhost:8090/jobs?appName=test&amp;amp;classPath=spark.jobserver.WordCountExample"
HTTP/1.1 400 Bad Request
Server: spray-can/1.3.4
Date: Thu, 14 Sep 2017 12:01:37 GMT
Access-Control-Allow-Origin: *
Content-Type: application/json; charset=UTF-8
Content-Length: 738

{
  "status": "VALIDATION FAILED",
  "result": {
    "message": "One(SparkJobInvalid(No input.string config param))",
    "errorClass": "java.lang.Throwable",
    "stack": "java.lang.Throwable: One(SparkJobInvalid(No input.string config param))\n\tat spark.jobserver.JobManagerActor$$anonfun$getJobFuture$4.apply(JobManagerActor.scala:327)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n"
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-new-sparkjob-api-with-spark-v21" class="anchor" aria-hidden="true" href="#new-sparkjob-api-with-spark-v21"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NEW SparkJob API with Spark v2.1&lt;/h3&gt;
&lt;p&gt;Deploying Spark JobServer with Spark v2.x cluster, you can create a SparkSession context which enables Spark-SQL and Hive support&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;curl &lt;span class="pl-k"&gt;-&lt;/span&gt;i &lt;span class="pl-k"&gt;-&lt;/span&gt;d &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;'http&lt;/span&gt;&lt;span class="pl-k"&gt;:&lt;/span&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt;localhost:8090/contexts/sql-context-1?num-cpu-cores=2&amp;amp;memory-per-node=512M&amp;amp;context-factory=spark.jobserver.context.SessionContextFactory'&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Spark JobServer application shall extend from the SparkSessionJob to use the spark.jobserver.context.SessionContextFactory, here is an example&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;typesafe&lt;/span&gt;.&lt;span class="pl-en"&gt;config&lt;/span&gt;.&lt;span class="pl-en"&gt;Config&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;org&lt;/span&gt;.&lt;span class="pl-en"&gt;apache&lt;/span&gt;.&lt;span class="pl-en"&gt;spark&lt;/span&gt;.&lt;span class="pl-en"&gt;sql&lt;/span&gt;.&lt;span class="pl-en"&gt;SparkSession&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;org&lt;/span&gt;.&lt;span class="pl-en"&gt;scalactic&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;spark&lt;/span&gt;.&lt;span class="pl-en"&gt;jobserver&lt;/span&gt;.&lt;span class="pl-en"&gt;SparkSessionJob&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;spark&lt;/span&gt;.&lt;span class="pl-en"&gt;jobserver&lt;/span&gt;.&lt;span class="pl-en"&gt;api&lt;/span&gt;.{&lt;span class="pl-en"&gt;JobEnvironment&lt;/span&gt;, &lt;span class="pl-en"&gt;SingleProblem&lt;/span&gt;, &lt;span class="pl-en"&gt;ValidationProblem&lt;/span&gt;}

&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;scala&lt;/span&gt;.&lt;span class="pl-en"&gt;util&lt;/span&gt;.&lt;span class="pl-en"&gt;Try&lt;/span&gt;

&lt;span class="pl-k"&gt;object&lt;/span&gt; &lt;span class="pl-en"&gt;WordCountExampleSparkSession&lt;/span&gt; &lt;span class="pl-k"&gt;extends&lt;/span&gt; &lt;span class="pl-e"&gt;SparkSessionJob&lt;/span&gt; {
  &lt;span class="pl-k"&gt;type&lt;/span&gt; &lt;span class="pl-en"&gt;JobData&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;Seq&lt;/span&gt;[&lt;span class="pl-k"&gt;String&lt;/span&gt;]
  &lt;span class="pl-k"&gt;type&lt;/span&gt; &lt;span class="pl-en"&gt;JobOutput&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; collection.&lt;span class="pl-en"&gt;Map&lt;/span&gt;[&lt;span class="pl-k"&gt;String&lt;/span&gt;, &lt;span class="pl-k"&gt;Long&lt;/span&gt;]

  &lt;span class="pl-k"&gt;override&lt;/span&gt; &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;runJob&lt;/span&gt;(&lt;span class="pl-v"&gt;sparkSession&lt;/span&gt;: &lt;span class="pl-en"&gt;SparkSession&lt;/span&gt;, &lt;span class="pl-v"&gt;runtime&lt;/span&gt;: &lt;span class="pl-en"&gt;JobEnvironment&lt;/span&gt;, &lt;span class="pl-v"&gt;data&lt;/span&gt;: &lt;span class="pl-en"&gt;JobData&lt;/span&gt;)&lt;span class="pl-k"&gt;:&lt;/span&gt; &lt;span class="pl-en"&gt;JobOutput&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt;
    sparkSession.sparkContext.parallelize(data).countByValue

  &lt;span class="pl-k"&gt;override&lt;/span&gt; &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;validate&lt;/span&gt;(&lt;span class="pl-v"&gt;sparkSession&lt;/span&gt;: &lt;span class="pl-en"&gt;SparkSession&lt;/span&gt;, &lt;span class="pl-v"&gt;runtime&lt;/span&gt;: &lt;span class="pl-en"&gt;JobEnvironment&lt;/span&gt;, &lt;span class="pl-v"&gt;config&lt;/span&gt;: &lt;span class="pl-en"&gt;Config&lt;/span&gt;)&lt;span class="pl-k"&gt;:&lt;/span&gt;
  &lt;span class="pl-en"&gt;JobData&lt;/span&gt; &lt;span class="pl-en"&gt;Or&lt;/span&gt; &lt;span class="pl-en"&gt;Every&lt;/span&gt;[&lt;span class="pl-en"&gt;ValidationProblem&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; {
    &lt;span class="pl-en"&gt;Try&lt;/span&gt;(config.getString(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;input.string&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).split(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt; &lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).toSeq)
      .map(words &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-en"&gt;Good&lt;/span&gt;(words))
      .getOrElse(&lt;span class="pl-en"&gt;Bad&lt;/span&gt;(&lt;span class="pl-en"&gt;One&lt;/span&gt;(&lt;span class="pl-en"&gt;SingleProblem&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;No input.string param&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;))))
  }
}&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-dependency-jars" class="anchor" aria-hidden="true" href="#dependency-jars"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependency jars&lt;/h3&gt;
&lt;p&gt;You have a couple options to package and upload dependency jars.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The easiest is to use something like &lt;a href="https://github.com/sbt/sbt-assembly"&gt;sbt-assembly&lt;/a&gt; to produce a fat jar.  Be sure to mark the Spark and job-server dependencies as "provided" so it won't blow up the jar size.  This works well if the number of dependencies is not large.&lt;/li&gt;
&lt;li&gt;When the dependencies are sizeable and/or you don't want to load them with every different job, you can package the dependencies separately and use one of several options:
&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;dependent-jar-uris&lt;/code&gt; context configuration param. Then the jar gets loaded for every job.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;dependent-jar-uris&lt;/code&gt; can also be used in job configuration param when submitting a job. On an ad-hoc context this has the same effect as &lt;code&gt;dependent-jar-uris&lt;/code&gt; context configuration param. On a persistent context the jars will be loaded for the current job and then for every job that will be executed on the persistent context.
&lt;pre&gt;&lt;code&gt;curl -d "" "localhost:8090/contexts/test-context?num-cpu-cores=4&amp;amp;memory-per-node=512m"
OK⏎
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;curl "localhost:8090/jobs?appName=test&amp;amp;classPath=spark.jobserver.WordCountExample&amp;amp;context=test-context&amp;amp;sync=true" -d '{
    dependent-jar-uris = ["file:///myjars/deps01.jar", "file:///myjars/deps02.jar"],
    input.string = "a b c a b see"
}'
&lt;/code&gt;&lt;/pre&gt;
The jars /myjars/deps01.jar &amp;amp; /myjars/deps02.jar (present only on the SJS node) will be loaded and made available for the Spark driver &amp;amp; executors.&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;--package&lt;/code&gt; option with Maven coordinates with &lt;code&gt;server_start.sh&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Put the extra jars in the SPARK_CLASSPATH&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-named-objects" class="anchor" aria-hidden="true" href="#named-objects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Named Objects&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-using-named-rdds" class="anchor" aria-hidden="true" href="#using-named-rdds"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Named RDDs&lt;/h4&gt;
&lt;p&gt;Initially, the job server only supported Named RDDs. For backwards compatibility and convenience, the following is still supported even though it is now possible to use the more generic Named Object support described in the next section.&lt;/p&gt;
&lt;p&gt;Named RDDs are a way to easily share RDDs among jobs. Using this facility, computed RDDs can be cached with a given name and later on retrieved.
To use this feature, the SparkJob needs to mixin &lt;code&gt;NamedRddSupport&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;object&lt;/span&gt; &lt;span class="pl-en"&gt;SampleNamedRDDJob&lt;/span&gt;  &lt;span class="pl-k"&gt;extends&lt;/span&gt; &lt;span class="pl-e"&gt;SparkJob&lt;/span&gt; &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-e"&gt;NamedRddSupport&lt;/span&gt; {
    &lt;span class="pl-k"&gt;override&lt;/span&gt; &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;runJob&lt;/span&gt;(sc&lt;span class="pl-k"&gt;:&lt;/span&gt;&lt;span class="pl-en"&gt;SparkContext&lt;/span&gt;, &lt;span class="pl-v"&gt;jobConfig&lt;/span&gt;: &lt;span class="pl-en"&gt;Config&lt;/span&gt;)&lt;span class="pl-k"&gt;:&lt;/span&gt; &lt;span class="pl-en"&gt;Any&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;???&lt;/span&gt;
    &lt;span class="pl-k"&gt;override&lt;/span&gt; &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;validate&lt;/span&gt;(sc&lt;span class="pl-k"&gt;:&lt;/span&gt;&lt;span class="pl-en"&gt;SparkContext&lt;/span&gt;, &lt;span class="pl-v"&gt;config&lt;/span&gt;: &lt;span class="pl-en"&gt;Config&lt;/span&gt;)&lt;span class="pl-k"&gt;:&lt;/span&gt; &lt;span class="pl-en"&gt;SparkJobValidation&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;???&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then in the implementation of the job, RDDs can be stored with a given name:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;this&lt;/span&gt;.namedRdds.update(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;french_dictionary&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, frenchDictionaryRDD)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Other job running in the same context can retrieve and use this RDD later on:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;rdd&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;this&lt;/span&gt;.namedRdds.get[(&lt;span class="pl-k"&gt;String&lt;/span&gt;, &lt;span class="pl-k"&gt;String&lt;/span&gt;)](&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;french_dictionary&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).get&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(note the explicit type provided to get. This will allow to cast the retrieved RDD that otherwise is of type RDD[_])&lt;/p&gt;
&lt;p&gt;For jobs that depends on a named RDDs it's a good practice to check for the existence of the NamedRDD in the &lt;code&gt;validate&lt;/code&gt; method as explained earlier:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;validate&lt;/span&gt;(sc&lt;span class="pl-k"&gt;:&lt;/span&gt;&lt;span class="pl-en"&gt;SparkContext&lt;/span&gt;, &lt;span class="pl-v"&gt;config&lt;/span&gt;: &lt;span class="pl-en"&gt;Config&lt;/span&gt;)&lt;span class="pl-k"&gt;:&lt;/span&gt; &lt;span class="pl-en"&gt;SparkJobValidation&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; {
  ...
  &lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;rdd&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;this&lt;/span&gt;.namedRdds.get[(&lt;span class="pl-k"&gt;Long&lt;/span&gt;, scala.&lt;span class="pl-en"&gt;Seq&lt;/span&gt;[&lt;span class="pl-k"&gt;String&lt;/span&gt;])](&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;dictionary&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
  &lt;span class="pl-k"&gt;if&lt;/span&gt; (rdd.isDefined) &lt;span class="pl-en"&gt;SparkJobValid&lt;/span&gt; &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-en"&gt;SparkJobInvalid&lt;/span&gt;(&lt;span class="pl-k"&gt;s&lt;/span&gt;&lt;span class="pl-s"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;Missing named RDD [dictionary]&lt;/span&gt;&lt;span class="pl-s"&gt;"&lt;/span&gt;)
}&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-using-named-objects" class="anchor" aria-hidden="true" href="#using-named-objects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Named Objects&lt;/h4&gt;
&lt;p&gt;Named Objects are a way to easily share RDDs, DataFrames or other objects among jobs. Using this facility, computed objects can be cached with a given name and later on retrieved.
To use this feature, the SparkJob needs to mixin &lt;code&gt;NamedObjectSupport&lt;/code&gt;. It is also necessary to define implicit persisters for each desired type of named objects. For convencience, we have provided implementations for RDD persistence and for DataFrame persistence (defined in &lt;code&gt;job-server-extras&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;object&lt;/span&gt; &lt;span class="pl-en"&gt;SampleNamedObjectJob&lt;/span&gt;  &lt;span class="pl-k"&gt;extends&lt;/span&gt; &lt;span class="pl-e"&gt;SparkJob&lt;/span&gt; &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-e"&gt;NamedObjectSupport&lt;/span&gt; {

  &lt;span class="pl-k"&gt;implicit&lt;/span&gt; &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;rddPersister&lt;/span&gt;[&lt;span class="pl-en"&gt;T&lt;/span&gt;] &lt;span class="pl-k"&gt;:&lt;/span&gt; &lt;span class="pl-en"&gt;NamedObjectPersister&lt;/span&gt;[&lt;span class="pl-en"&gt;NamedRDD&lt;/span&gt;[&lt;span class="pl-en"&gt;T&lt;/span&gt;]] &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;RDDPersister&lt;/span&gt;[&lt;span class="pl-en"&gt;T&lt;/span&gt;]
  &lt;span class="pl-k"&gt;implicit&lt;/span&gt; &lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;dataFramePersister&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;DataFramePersister&lt;/span&gt;

    &lt;span class="pl-k"&gt;override&lt;/span&gt; &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;runJob&lt;/span&gt;(sc&lt;span class="pl-k"&gt;:&lt;/span&gt;&lt;span class="pl-en"&gt;SparkContext&lt;/span&gt;, &lt;span class="pl-v"&gt;jobConfig&lt;/span&gt;: &lt;span class="pl-en"&gt;Config&lt;/span&gt;)&lt;span class="pl-k"&gt;:&lt;/span&gt; &lt;span class="pl-en"&gt;Any&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;???&lt;/span&gt;
    &lt;span class="pl-k"&gt;override&lt;/span&gt; &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;validate&lt;/span&gt;(sc&lt;span class="pl-k"&gt;:&lt;/span&gt;&lt;span class="pl-en"&gt;SparkContext&lt;/span&gt;, &lt;span class="pl-v"&gt;config&lt;/span&gt;: &lt;span class="pl-en"&gt;Config&lt;/span&gt;)&lt;span class="pl-k"&gt;:&lt;/span&gt; &lt;span class="pl-en"&gt;SparkJobValidation&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;???&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then in the implementation of the job, RDDs can be stored with a given name:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;this&lt;/span&gt;.namedObjects.update(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;rdd:french_dictionary&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-en"&gt;NamedRDD&lt;/span&gt;(frenchDictionaryRDD, forceComputation &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;, storageLevel &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;StorageLevel&lt;/span&gt;.&lt;span class="pl-en"&gt;NONE&lt;/span&gt;))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;DataFrames can be stored like so:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;this&lt;/span&gt;.namedObjects.update(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;df:some df&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-en"&gt;NamedDataFrame&lt;/span&gt;(frenchDictionaryDF, forceComputation &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;, storageLevel &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;StorageLevel&lt;/span&gt;.&lt;span class="pl-en"&gt;NONE&lt;/span&gt;))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It is advisable to use different name prefixes for different types of objects to avoid confusion.&lt;/p&gt;
&lt;p&gt;Another job running in the same context can retrieve and use these objects later on:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-c1"&gt;NamedRDD&lt;/span&gt;(frenchDictionaryRDD, _ ,_) &lt;span class="pl-k"&gt;=&lt;/span&gt; namedObjects.get[&lt;span class="pl-en"&gt;NamedRDD&lt;/span&gt;[(&lt;span class="pl-k"&gt;String&lt;/span&gt;, &lt;span class="pl-k"&gt;String&lt;/span&gt;)]](&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;rdd:french_dictionary&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).get

&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-c1"&gt;NamedDataFrame&lt;/span&gt;(frenchDictionaryDF, _, _) &lt;span class="pl-k"&gt;=&lt;/span&gt; namedObjects.get[&lt;span class="pl-en"&gt;NamedDataFrame&lt;/span&gt;](&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;df:some df&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).get
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(Note the explicit type provided to get. This will allow to cast the retrieved RDD/DataFrame object to the proper result type.)&lt;/p&gt;
&lt;p&gt;For jobs that depends on a named objects it's a good practice to check for the existence of the NamedObject in the &lt;code&gt;validate&lt;/code&gt; method as explained earlier:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;validate&lt;/span&gt;(sc&lt;span class="pl-k"&gt;:&lt;/span&gt;&lt;span class="pl-en"&gt;SparkContext&lt;/span&gt;, &lt;span class="pl-v"&gt;config&lt;/span&gt;: &lt;span class="pl-en"&gt;Config&lt;/span&gt;)&lt;span class="pl-k"&gt;:&lt;/span&gt; &lt;span class="pl-en"&gt;SparkJobValidation&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; {
  ...
  &lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;obj&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;this&lt;/span&gt;.namedObjects.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;dictionary&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
  &lt;span class="pl-k"&gt;if&lt;/span&gt; (obj.isDefined) &lt;span class="pl-en"&gt;SparkJobValid&lt;/span&gt; &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-en"&gt;SparkJobInvalid&lt;/span&gt;(&lt;span class="pl-k"&gt;s&lt;/span&gt;&lt;span class="pl-s"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;Missing named object [dictionary]&lt;/span&gt;&lt;span class="pl-s"&gt;"&lt;/span&gt;)
}&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-https--ssl-configuration" class="anchor" aria-hidden="true" href="#https--ssl-configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTTPS / SSL Configuration&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-server-authentication" class="anchor" aria-hidden="true" href="#server-authentication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Server authentication&lt;/h4&gt;
&lt;p&gt;To activate server authentication and ssl communication, set these flags in your application.conf file (Section 'spray.can.server'):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  ssl-encryption = on
  # absolute path to keystore file
  keystore = "/some/path/sjs.jks"
  keystorePW = "changeit"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will need a keystore that contains the server certificate. The bare minimum is achieved with this command which creates a self-signed certificate:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; keytool -genkey -keyalg RSA -alias jobserver -keystore ~/sjs.jks -storepass changeit -validity 360 -keysize 2048
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may place the keystore anywhere.
Here is an example of a simple curl command that utilizes ssl:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -k https://localhost:8090/contexts
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;-k&lt;/code&gt; flag tells curl to "Allow connections to SSL sites without certs". Export your server certificate and import it into the client's truststore to fully utilize ssl security.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-client-authentication" class="anchor" aria-hidden="true" href="#client-authentication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Client authentication&lt;/h4&gt;
&lt;p&gt;Client authentication can be enabled by simply pointing Job Server to a valid Trust Store.
As for server authentication, this is done by setting appropriate values in the application.conf.
The minimum set of parameters to enable client authentication consists of:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  # truststore = "/some/path/server-truststore.jks"
  # truststorePW = "changeit"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, client authentication implies server authentication, therefore client authentication will only be enabled once server authentication is activated.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-basic-authentication" class="anchor" aria-hidden="true" href="#basic-authentication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic authentication&lt;/h3&gt;
&lt;p&gt;Basic authentication (username and password) in Job Server relies on the &lt;a href="http://shiro.apache.org/index.html" rel="nofollow"&gt;Apache Shiro&lt;/a&gt; framework.
Basic authentication is activated by setting this flag (Section 'shiro'):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;authentication = on
# absolute path to shiro config file, including file name
config.path = "/some/path/shiro.ini"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Shiro-specific configuration options should be placed into a file named 'shiro.ini' in the directory as specified by the config option 'config.path'.
Here is an example that configures LDAP with user group verification:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# use this for basic ldap authorization, without group checking
# activeDirectoryRealm = org.apache.shiro.realm.ldap.JndiLdapRealm
# use this for checking group membership of users based on the 'member' attribute of the groups:
activeDirectoryRealm = spark.jobserver.auth.LdapGroupRealm
# search base for ldap groups (only relevant for LdapGroupRealm):
activeDirectoryRealm.contextFactory.environment[ldap.searchBase] = dc=xxx,dc=org
# allowed groups (only relevant for LdapGroupRealm):
activeDirectoryRealm.contextFactory.environment[ldap.allowedGroups] = "cn=group1,ou=groups", "cn=group2,ou=groups"
activeDirectoryRealm.contextFactory.environment[java.naming.security.credentials] = password
activeDirectoryRealm.contextFactory.url = ldap://localhost:389
activeDirectoryRealm.userDnTemplate = cn={0},ou=people,dc=xxx,dc=org

cacheManager = org.apache.shiro.cache.MemoryConstrainedCacheManager

securityManager.cacheManager = $cacheManager
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make sure to edit the url, credentials, userDnTemplate, ldap.allowedGroups and ldap.searchBase settings in accordance with your local setup.&lt;/p&gt;
&lt;p&gt;Here is an example of a simple curl command that authenticates a user and uses ssl (you may want to use -H to hide the
credentials, this is just a simple example to get you started):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -k --basic --user 'user:pw' https://localhost:8090/contexts
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-deployment" class="anchor" aria-hidden="true" href="#deployment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deployment&lt;/h2&gt;
&lt;p&gt;See also running on &lt;a href="doc/cluster.md"&gt;cluster&lt;/a&gt;, &lt;a href="doc/yarn.md"&gt;YARN client&lt;/a&gt;, on &lt;a href="doc/EMR.md"&gt;EMR&lt;/a&gt; and running on &lt;a href="doc/mesos.md"&gt;Mesos&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-manual-steps" class="anchor" aria-hidden="true" href="#manual-steps"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Manual steps&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Copy &lt;code&gt;config/local.sh.template&lt;/code&gt; to &lt;code&gt;&amp;lt;environment&amp;gt;.sh&lt;/code&gt; and edit as appropriate.  NOTE: be sure to set SPARK_VERSION if you need to compile against a different version.&lt;/li&gt;
&lt;li&gt;Copy &lt;code&gt;config/shiro.ini.template&lt;/code&gt; to &lt;code&gt;shiro.ini&lt;/code&gt; and edit as appropriate. NOTE: only required when &lt;code&gt;authentication = on&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Copy &lt;code&gt;config/local.conf.template&lt;/code&gt; to &lt;code&gt;&amp;lt;environment&amp;gt;.conf&lt;/code&gt; and edit as appropriate.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bin/server_deploy.sh &amp;lt;environment&amp;gt;&lt;/code&gt; -- this packages the job server along with config files and pushes
it to the remotes you have configured in &lt;code&gt;&amp;lt;environment&amp;gt;.sh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;On the remote server, start it in the deployed directory with &lt;code&gt;server_start.sh&lt;/code&gt; and stop it with &lt;code&gt;server_stop.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;code&gt;server_start.sh&lt;/code&gt; script uses &lt;code&gt;spark-submit&lt;/code&gt; under the hood and may be passed any of the standard extra arguments from &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;NOTE: Under the hood, the deploy scripts generate an assembly jar from the &lt;code&gt;job-server-extras&lt;/code&gt; project.  Generating assemblies from other projects may not include all the necessary components for job execution.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-context-per-jvm" class="anchor" aria-hidden="true" href="#context-per-jvm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Context per JVM&lt;/h3&gt;
&lt;p&gt;Each context can be a separate process launched using SparkLauncher, if &lt;code&gt;context-per-jvm&lt;/code&gt; is set to true.
This can be especially desirable when you want to run many contexts at once, or for certain types of contexts such as StreamingContexts which really need their own processes.&lt;/p&gt;
&lt;p&gt;Also, the extra processes talk to the master HTTP process via random ports using the Akka Cluster gossip protocol.  If for some reason the separate processes causes issues, set &lt;code&gt;spark.jobserver.context-per-jvm&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;, which will cause the job server to use a single JVM for all contexts.&lt;/p&gt;
&lt;p&gt;Among the known issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Launched contexts do not shut down by themselves.  You need to manually kill each separate process, or do &lt;code&gt;-X DELETE /contexts/&amp;lt;context-name&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Log files are separated out for each context (assuming &lt;code&gt;context-per-jvm&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt;) in their own subdirs under the &lt;code&gt;LOG_DIR&lt;/code&gt; configured in &lt;code&gt;settings.sh&lt;/code&gt; in the deployed directory.&lt;/p&gt;
&lt;p&gt;Note: to test out the deploy to a local staging dir, or package the job server for Mesos,
use &lt;code&gt;bin/server_package.sh &amp;lt;environment&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-configuring-spark-jobserver-backend" class="anchor" aria-hidden="true" href="#configuring-spark-jobserver-backend"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuring Spark Jobserver backend&lt;/h3&gt;
&lt;p&gt;Spark Jobserver offers a variety of options for backend storage such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;H2/PostreSQL or other SQL Databases&lt;/li&gt;
&lt;li&gt;Cassandra&lt;/li&gt;
&lt;li&gt;Combination of SQL DB or Zookeeper with HDFS&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-configuring-spark-jobserver-h2-database-backend" class="anchor" aria-hidden="true" href="#configuring-spark-jobserver-h2-database-backend"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuring Spark Jobserver H2 Database backend&lt;/h4&gt;
&lt;p&gt;By default, H2 database is used for storing Spark Jobserver related meta data.
This can be overridden if you prefer to use PostgreSQL or MySQL.
It is also important that any dependent jars are to be added to Job Server class path.&lt;/p&gt;
&lt;p&gt;To use the embedded H2 db as a backend, add the following configuration to local.conf.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spark {
  jobserver {
    ...
    sqldao {
      # Slick database driver, full classpath
      slick-driver = slick.driver.H2Driver

      # JDBC driver, full classpath
      jdbc-driver = org.h2.Driver

      # Directory where default H2 driver stores its data. Only needed for H2.
      rootdir = "/var/spark-jobserver/sqldao/data"

      jdbc {
        url = "jdbc:h2:file:/var/spark-jobserver/sqldao/data/h2-db"
        user = "secret"
        password = "secret"
      }

      dbcp {
        maxactive = 20
        maxidle = 10
        initialsize = 10
      }
    }
  }
}
# also add the following line at the root level.
flyway.locations="db/h2/migration"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you are using &lt;code&gt;context-per-jvm = true&lt;/code&gt;, be sure to add &lt;a href="http://h2database.com/html/features.html#auto_mixed_mode" rel="nofollow"&gt;AUTO_MIXED_MODE&lt;/a&gt; to your
H2 JDBC URL; this allows multiple processes to share the same H2 database using a lock file.&lt;/p&gt;
&lt;p&gt;In yarn-client mode, use H2 in server mode as described below instead of embedded mode.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the full H2 jar from &lt;a href="http://www.h2database.com/html/download.html" rel="nofollow"&gt;http://www.h2database.com/html/download.html&lt;/a&gt; and follow docs.&lt;/li&gt;
&lt;li&gt;Note that the version of H2 should match the H2 client version bundled with spark-jobserver, currently 1.3.176.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A sample JDBC configuration is below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jdbc {
        url = "jdbc:h2:tcp://localhost//ROOT/PARENT/DIRECTORIES/spark_jobserver"
        user = "secret"
        password = "secret"
      }

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: /ROOT/PARENT/DIRECTORIES/spark_jobserver is the absolute path to a directory to which H2 has write access.&lt;/p&gt;
&lt;p&gt;Example command line to launch H2 Server:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;java -cp h2-1.3.176.jar org.h2.tools.Server -tcp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use -? on command line to see other options.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-configuring-spark-jobserver-postgresql-database-backend" class="anchor" aria-hidden="true" href="#configuring-spark-jobserver-postgresql-database-backend"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuring Spark Jobserver PostgreSQL Database backend&lt;/h4&gt;
&lt;p&gt;Ensure that you have spark_jobserver database created with necessary rights
granted to user.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# create database user jobserver and database spark_jobserver:
$ createuser --username=&amp;lt;superuser&amp;gt; -RDIElPS jobserver
$ createdb -Ojobserver -Eutf8 spark_jobserver
CTRL-D -&amp;gt; logout from psql

# logon as superuser and enable the large object extension:
$ psql -U &amp;lt;superuser&amp;gt; spark_jobserver
spark_jobserver=# CREATE EXTENSION lo;
CTRL-D -&amp;gt; logout from psql

# you can connect to the database using the psql command line client:
$ psql -U jobserver spark_jobserver
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To use PostgreSQL as backend add the following configuration to local.conf.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spark {
  jobserver {
    ...
    sqldao {
      # Slick database driver, full classpath
      slick-driver = slick.driver.PostgresDriver

      # JDBC driver, full classpath
      jdbc-driver = org.postgresql.Driver

      # Directory where default H2 driver stores its data. Only needed for H2.
      rootdir = "/var/spark-jobserver/sqldao/data"

      jdbc {
        url = "jdbc:postgresql://db_host/spark_jobserver"
        user = "jobserver"
        password = "secret"
      }

      dbcp {
        maxactive = 20
        maxidle = 10
        initialsize = 10
      }
    }
  }
}
# also add the following line at the root level.
flyway.locations="db/postgresql/migration"
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-configuring-spark-jobserver-mysql-database-backend" class="anchor" aria-hidden="true" href="#configuring-spark-jobserver-mysql-database-backend"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuring Spark Jobserver MySQL Database backend&lt;/h4&gt;
&lt;p&gt;Ensure that you have spark_jobserver database created with necessary rights
granted to user.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# secure your mysql installation and define password for mysql root user
$ mysql_secure_installation

# logon as database root
$ mysql -u root -p

# create a database user and a database for spark jobserver:
mysql&amp;gt; CREATE USER 'jobserver'@'localhost' IDENTIFIED BY 'secret';
mysql&amp;gt; CREATE DATABASE spark_jobserver;
mysql&amp;gt; GRANT ALL ON spark_jobserver.* TO 'jobserver'@'localhost';
mysql&amp;gt; FLUSH PRIVILEGES;
CTRL-D -&amp;gt; logout from mysql

# you can connect to the database using the mysql command line client:
$ mysql -u jobserver -p
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To use MySQL as backend add the following configuration to local.conf.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spark {
  jobserver {
    ...
    sqldao {
      # Slick database driver, full classpath
      slick-driver = slick.driver.MySQLDriver

      # JDBC driver, full classpath
      jdbc-driver = com.mysql.jdbc.Driver

      jdbc {
        url = "jdbc:mysql://db_host/spark_jobserver"
        user = "jobserver"
        password = "secret"
      }

      dbcp {
        maxactive = 20
        maxidle = 10
        initialsize = 10
      }
    }
  }
}
# also add the following line at the root level.
flyway.locations="db/mysql/migration"
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-configuring-spark-jobserver-zookeeper--hdfs-database-backend" class="anchor" aria-hidden="true" href="#configuring-spark-jobserver-zookeeper--hdfs-database-backend"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuring Spark Jobserver Zookeeper + HDFS Database backend&lt;/h4&gt;
&lt;p&gt;To use Zookeeper (for metadata) and HDFS (for binaries) as backend add the following
configuration to local.conf.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    combineddao {
      rootdir = "/tmp/combineddao"
      binarydao {
        class = spark.jobserver.io.HdfsBinaryDAO
        dir = "hdfs:///spark-jobserver/binaries"
      }
      metadatadao {
        class = spark.jobserver.io.zookeeper.MetaDataZookeeperDAO
      }
    }

    zookeeperdao {
      dir = "jobserver/db"
      connection-string = "localhost:2181"
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More information on setting up different backends for binaries and jobserver meta data: &lt;a href="doc/dao-setup.md"&gt;setting up dao&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-ha-deployment-beta" class="anchor" aria-hidden="true" href="#ha-deployment-beta"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HA Deployment (beta)&lt;/h3&gt;
&lt;p&gt;It is possible to run multiple Spark Jobservers in a highly available setup. For a documentation of a Jobserver HA setup, refer to the &lt;a href="doc/HA.md"&gt;Jobserver HA documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-chef" class="anchor" aria-hidden="true" href="#chef"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Chef&lt;/h3&gt;
&lt;p&gt;There is also a &lt;a href="https://github.com/spark-jobserver/chef-spark-jobserver"&gt;Chef cookbook&lt;/a&gt; which can be used to deploy Spark Jobserver.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-architecture" class="anchor" aria-hidden="true" href="#architecture"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Architecture&lt;/h2&gt;
&lt;p&gt;The job server is intended to be run as one or more independent processes, separate from the Spark cluster
(though it very well may be collocated with say the Master).&lt;/p&gt;
&lt;p&gt;At first glance, it seems many of these functions (eg job management) could be integrated into the Spark standalone master.  While this is true, we believe there are many significant reasons to keep it separate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We want the job server to work for Mesos and YARN as well&lt;/li&gt;
&lt;li&gt;Spark and Mesos masters are organized around "applications" or contexts, but the job server supports running many discrete "jobs" inside a single context&lt;/li&gt;
&lt;li&gt;We want it to support Shark functionality in the future&lt;/li&gt;
&lt;li&gt;Loose coupling allows for flexible HA arrangements (multiple job servers targeting same standalone master, or possibly multiple Spark clusters per job server)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Flow diagrams are checked in in the doc/ subdirectory.  .diagram files are for websequencediagrams.com... check them out, they really will help you understand the flow of messages between actors.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-api" class="anchor" aria-hidden="true" href="#api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;API&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-binaries" class="anchor" aria-hidden="true" href="#binaries"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Binaries&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;GET /binaries               - lists all current binaries
POST /binaries/&amp;lt;appName&amp;gt;    - upload a new binary file
DELETE /binaries/&amp;lt;appName&amp;gt;  - delete defined binary
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When POSTing new binaries, the content-type header must be set to one of the types supported by the subclasses of the &lt;code&gt;BinaryType&lt;/code&gt; trait. e.g. "application/java-archive" or application/python-archive". If you are using curl command, then you must pass "-H 'Content-Type: application/python-archive'" or "-H 'Content-Type: application/java-archive'".&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contexts" class="anchor" aria-hidden="true" href="#contexts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contexts&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;GET /contexts               - lists all current contexts
GET /contexts/&amp;lt;name&amp;gt;        - gets info about a context, such as the spark UI url
POST /contexts/&amp;lt;name&amp;gt;       - creates a new context
DELETE /contexts/&amp;lt;name&amp;gt;     - stops a context and all jobs running in it. Additionally, you can pass ?force=true to stop a context forcefully. This is equivalent to killing the application from SparkUI (works for spark standalone only).
PUT /contexts?reset=reboot  - shuts down all contexts and re-loads only the contexts from config. Use ?sync=false to execute asynchronously.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spark context configuration params can follow &lt;code&gt;POST /contexts/&amp;lt;name&amp;gt;&lt;/code&gt; as query params. See section below for more details.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-jobs" class="anchor" aria-hidden="true" href="#jobs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jobs&lt;/h3&gt;
&lt;p&gt;Jobs submitted to the job server must implement a &lt;code&gt;SparkJob&lt;/code&gt; trait.  It has a main &lt;code&gt;runJob&lt;/code&gt; method which is
passed a SparkContext and a typesafe Config object.  Results returned by the method are made available through
the REST API.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;GET /jobs                - Lists the last N jobs
POST /jobs               - Starts a new job, use ?sync=true to wait for results
GET /jobs/&amp;lt;jobId&amp;gt;        - Gets the result or status of a specific job
DELETE /jobs/&amp;lt;jobId&amp;gt;     - Kills the specified job
GET /jobs/&amp;lt;jobId&amp;gt;/config - Gets the job configuration
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For details on the Typesafe config format used for input (JSON also works), see the &lt;a href="https://github.com/typesafehub/config"&gt;Typesafe Config docs&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-data" class="anchor" aria-hidden="true" href="#data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data&lt;/h3&gt;
&lt;p&gt;It is sometime necessary to programmatically upload files to the server. Use these paths to manage such files:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;GET /data                - Lists previously uploaded files that were not yet deleted
POST /data/&amp;lt;prefix&amp;gt;      - Uploads a new file, the full path of the file on the server is returned, the
                           prefix is the prefix of the actual filename used on the server (a timestamp is
                           added to ensure uniqueness)
DELETE /data/&amp;lt;filename&amp;gt;  - Deletes the specified file (only if under control of the JobServer)
PUT /data?reset=reboot   - Deletes all uploaded files. Use ?sync=false to execute asynchronously.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These files are uploaded to the server and are stored in a local temporary
directory where the JobServer runs. The POST command returns the full
pathname and filename of the uploaded file so that later jobs can work with this
just the same as with any other server-local file. A job could therefore add this file to HDFS or distribute
it to worker nodes via the SparkContext.addFile command.
For files that are larger than a few hundred MB, it is recommended to manually upload these files to the server or
to directly add them to your HDFS.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-data-api-example" class="anchor" aria-hidden="true" href="#data-api-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data API Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ curl -d "Test data file api" http://localhost:8090/data/test_data_file_upload.txt
{
  "result": {
    "filename": "/tmp/spark-jobserver/upload/test_data_file_upload.txt-2016-07-04T09_09_57.928+05_30.dat"
  }
}

$ curl http://localhost:8090/data
["/tmp/spark-jobserver/upload/test_data_file_upload.txt-2016-07-04T09_09_57.928+05_30.dat"]

$ curl -X DELETE http://localhost:8090/data/%2Ftmp%2Fspark-jobserver%2Fupload%2Ftest_data_file_upload.txt-2016-07-04T09_09_57.928%2B05_30.dat
OK

$ curl http://localhost:8090/data
[]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: Both POST and DELETE requests takes URI encoded file names.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-context-configuration" class="anchor" aria-hidden="true" href="#context-configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Context configuration&lt;/h3&gt;
&lt;p&gt;A number of context-specific settings can be controlled when creating a context (POST /contexts) or running an
ad-hoc job (which creates a context on the spot).  For example, add urls of dependent jars for a context.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;POST '/contexts/my-new-context?dependent-jar-uris=file:///some/path/of/my-foo-lib.jar'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: Only the latest &lt;code&gt;dependent-jar-uris&lt;/code&gt; (btw it’s jar-uris, not jars-uri) takes effect.  You can specify multiple URIs by comma-separating them.  So like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;amp;dependent-jar-uris=file:///path/a.jar,file:///path/b.jar
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When creating a context via POST /contexts, the query params are used to override the default configuration in
spark.context-settings.  For example,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;POST /contexts/my-new-context?num-cpu-cores=10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;would override the default spark.context-settings.num-cpu-cores setting.&lt;/p&gt;
&lt;p&gt;When starting a job, and the &lt;code&gt;context=&lt;/code&gt; query param is not specified, then an ad-hoc context is created.  Any
settings specified in spark.context-settings will override the defaults in the job server config when it is
started up.&lt;/p&gt;
&lt;p&gt;Any spark configuration param can be overridden either in POST /contexts query params, or through &lt;code&gt;spark .context-settings&lt;/code&gt; job configuration.  In addition, &lt;code&gt;num-cpu-cores&lt;/code&gt; maps to &lt;code&gt;spark.cores.max&lt;/code&gt;, and &lt;code&gt;mem-per- node&lt;/code&gt; maps to &lt;code&gt;spark.executor.memory&lt;/code&gt;.  Therefore the following are all equivalent:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;POST /contexts/my-new-context?num-cpu-cores=10

POST /contexts/my-new-context?spark.cores.max=10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or in the job config when using POST /jobs,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spark.context-settings {
    spark.cores.max = 10
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;User impersonation for an already Kerberos authenticated user is supported via &lt;code&gt;spark.proxy.user&lt;/code&gt; query param:&lt;/p&gt;
&lt;p&gt;POST /contexts/my-new-context?spark.proxy.user=&lt;/p&gt;
&lt;p&gt;However, whenever the flag &lt;code&gt;shiro.use-as-proxy-user&lt;/code&gt; is set to &lt;code&gt;on&lt;/code&gt; (and authentication is &lt;code&gt;on&lt;/code&gt;) then this parameter
is ignored and the name of the authenticated user is &lt;em&gt;always&lt;/em&gt; used as the value of the &lt;code&gt;spark.proxy.user&lt;/code&gt;
parameter when creating contexts.&lt;/p&gt;
&lt;p&gt;To pass settings directly to the sparkConf that do not use the "spark." prefix "as-is", use the "passthrough" section.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spark.context-settings {
    spark.cores.max = 10
    passthrough {
      some.custom.hadoop.config = "192.168.1.1"
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To add to the underlying Hadoop configuration in a Spark context, add the hadoop section to the context settings&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spark.context-settings {
    hadoop {
        mapreduce.framework.name = "Foo"
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the exact context configuration parameters, see JobManagerActor docs as well as application.conf.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-other-configuration-settings" class="anchor" aria-hidden="true" href="#other-configuration-settings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other configuration settings&lt;/h3&gt;
&lt;p&gt;For all of the Spark Job Server configuration settings, see &lt;code&gt;job-server/src/main/resources/application.conf&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-job-result-serialization" class="anchor" aria-hidden="true" href="#job-result-serialization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Job Result Serialization&lt;/h3&gt;
&lt;p&gt;The result returned by the &lt;code&gt;SparkJob&lt;/code&gt; &lt;code&gt;runJob&lt;/code&gt; method is serialized by the job server into JSON for routes
that return the result (GET /jobs with sync=true, GET /jobs/).  Currently the following types can be
serialized properly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;String, Int, Long, Double, Float, Boolean&lt;/li&gt;
&lt;li&gt;Scala Map's with string key values (non-string keys may be converted to strings)&lt;/li&gt;
&lt;li&gt;Scala Seq's&lt;/li&gt;
&lt;li&gt;Array's&lt;/li&gt;
&lt;li&gt;Anything that implements Product (Option, case classes) -- they will be serialized as lists&lt;/li&gt;
&lt;li&gt;Subclasses of java.util.List&lt;/li&gt;
&lt;li&gt;Subclasses of java.util.Map with string key values (non-string keys may be converted to strings)&lt;/li&gt;
&lt;li&gt;Maps, Seqs, Java Maps and Java Lists may contain nested values of any of the above&lt;/li&gt;
&lt;li&gt;If a job result is of scala's Stream[Byte] type it will be serialised directly as a chunk encoded stream.
This is useful if your job result payload is large and may cause a timeout serialising as objects. Beware, this
will not currently work as desired with context-per-jvm=true configuration, since it would require serialising
Stream[_] blob between processes. For now use Stream[_] job results in context-per-jvm=false configuration, pending
potential future enhancements to support this in context-per-jvm=true mode.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we encounter a data type that is not supported, then the entire result will be serialized to a string.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-http-override" class="anchor" aria-hidden="true" href="#http-override"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTTP Override&lt;/h3&gt;
&lt;p&gt;Spark Job Server offers HTTP override functionality.
Often reverse proxies and firewall implement access limitations to, for example, DELETE and PUT requests.
HTTP override allows overcoming these limitations by wrapping, for example, a DELETE request into a POST request.&lt;/p&gt;
&lt;p&gt;Requesting the destruction of a context can be accomplished through HTTP override using the following syntax:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ curl -X POST "localhost:8090/contexts/test_context?_method=DELETE"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, a DELETE request is passed to Spark Job Server "through" a POST request.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-clients" class="anchor" aria-hidden="true" href="#clients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Clients&lt;/h2&gt;
&lt;p&gt;Spark Jobserver project has a
&lt;a href="https://github.com/spark-jobserver/python-sjsclient"&gt;python binding&lt;/a&gt; package.
This can be used to quickly develop python applications that can interact with
Spark Jobserver programmatically.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-and-development" class="anchor" aria-hidden="true" href="#contribution-and-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution and Development&lt;/h2&gt;
&lt;p&gt;Contributions via Github Pull Request are welcome. Please start by taking a look at the &lt;a href="doc/contribution-guidelines.md"&gt;contribution guidelines&lt;/a&gt; and check the TODO for some contribution ideas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you need to build with a specific scala version use ++x.xx.x followed by the regular command,
for instance: &lt;code&gt;sbt ++2.11.6 job-server/compile&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;From the "master" project, please run "test" to ensure nothing is broken.
&lt;ul&gt;
&lt;li&gt;You may need to set &lt;code&gt;SPARK_LOCAL_IP&lt;/code&gt; to &lt;code&gt;localhost&lt;/code&gt; to ensure Akka port can bind successfully&lt;/li&gt;
&lt;li&gt;Note for Windows users: very few tests fail on Windows. Thus, run &lt;code&gt;testOnly -- -l WindowsIgnore&lt;/code&gt; from SBT shell to ignore them.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Logging for tests goes to "job-server-test.log"&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;scoverage:test&lt;/code&gt; to check the code coverage and improve it.
&lt;ul&gt;
&lt;li&gt;Windows users: run &lt;code&gt;; coverage ; testOnly -- -l WindowsIgnore ; coverageReport&lt;/code&gt; from SBT shell.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Please run scalastyle to ensure your code changes don't break the style guide.&lt;/li&gt;
&lt;li&gt;Do "reStart" from SBT for quick restarts of the job server process&lt;/li&gt;
&lt;li&gt;Please update the g8 template if you change the SparkJob API&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Profiling software generously provided by &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/97fa03cac759a772255b93c64ab1c9f76a103681/68747470733a2f2f7777772e796f75726b69742e636f6d2f696d616765732f796b6c6f676f2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/97fa03cac759a772255b93c64ab1c9f76a103681/68747470733a2f2f7777772e796f75726b69742e636f6d2f696d616765732f796b6c6f676f2e706e67" alt="" data-canonical-src="https://www.yourkit.com/images/yklogo.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;YourKit supports open source projects with its full-featured &lt;a href="https://www.yourkit.com/java/profiler/index.jsp" rel="nofollow"&gt;Java Profiler&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-publishing-packages" class="anchor" aria-hidden="true" href="#publishing-packages"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Publishing packages&lt;/h3&gt;
&lt;p&gt;In the root project, do &lt;code&gt;release cross&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To announce the release on &lt;a href="http://ls.implicit.ly/" rel="nofollow"&gt;ls.implicit.ly&lt;/a&gt;, use
&lt;a href="https://github.com/n8han/herald#install"&gt;Herald&lt;/a&gt; after adding release notes in
the &lt;code&gt;notes/&lt;/code&gt; dir.  Also regenerate the catalog with &lt;code&gt;lsWriteVersion&lt;/code&gt; SBT task
and &lt;code&gt;lsync&lt;/code&gt;, in project job-server.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;For user/dev questions, we are using google group for discussions:
&lt;a href="https://groups.google.com/forum/#!forum/spark-jobserver" rel="nofollow"&gt;https://groups.google.com/forum/#!forum/spark-jobserver&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Please report bugs/problems to:
&lt;a href="https://github.com/spark-jobserver/spark-jobserver/issues"&gt;https://github.com/spark-jobserver/spark-jobserver/issues&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Apache 2.0, see LICENSE.md&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;More debugging for classpath issues&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add Swagger support.  See the spray-swagger project.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Implement an interactive SQL window.  See: &lt;a href="https://github.com/adatao/spark-admin"&gt;spark-admin&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stream the current job progress via a Listener&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add routes to return stage info for a job.  Persist it via DAO so that we can always retrieve stage / performance info
even for historical jobs.  This would be pretty kickass.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>spark-jobserver</author><guid isPermaLink="false">https://github.com/spark-jobserver/spark-jobserver</guid><pubDate>Sat, 02 Nov 2019 00:13:00 GMT</pubDate></item><item><title>gatling/gatling #14 in Scala, Today</title><link>https://github.com/gatling/gatling</link><description>&lt;p&gt;&lt;i&gt;Async Scala-Akka-Netty based Load Test Tool&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-gatling--" class="anchor" aria-hidden="true" href="#gatling--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gatling &lt;a href="https://travis-ci.org/gatling/gatling" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e87d40ad3c3901b02cc99091df3484450b0c9f2d/68747470733a2f2f7472617669732d63692e6f72672f6761746c696e672f6761746c696e672e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/gatling/gatling.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://maven-badges.herokuapp.com/maven-central/io.gatling/gatling-core/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c8415d087772ac68ab22a404ed7ac2dcea6169c4/68747470733a2f2f6d6176656e2d6261646765732e6865726f6b756170702e636f6d2f6d6176656e2d63656e7472616c2f696f2e6761746c696e672f6761746c696e672d636f72652f62616467652e737667" alt="Maven Central" data-canonical-src="https://maven-badges.herokuapp.com/maven-central/io.gatling/gatling-core/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-gatling-" class="anchor" aria-hidden="true" href="#what-is-gatling-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is Gatling ?&lt;/h2&gt;
&lt;p&gt;Gatling is a stress tool.
Development is currently focusing on HTTP support.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-motivation" class="anchor" aria-hidden="true" href="#motivation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Motivation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Finding fancy GUIs not that convenient for describing stress tests, what you want is a friendly expressive DSL?&lt;/li&gt;
&lt;li&gt;Wanting something more convenient than huge XML dumps to store in your source version control system?&lt;/li&gt;
&lt;li&gt;Fed up with having to host a farm of injecting servers because your tool uses blocking IO and one-thread-per-user architecture?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Gatling is for you!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-underlying-technologies" class="anchor" aria-hidden="true" href="#underlying-technologies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Underlying technologies&lt;/h2&gt;
&lt;p&gt;Gatling is developed in Scala and built upon :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://netty.io" rel="nofollow"&gt;Netty&lt;/a&gt; for non blocking HTTP&lt;/li&gt;
&lt;li&gt;&lt;a href="https://akka.io" rel="nofollow"&gt;Akka&lt;/a&gt; for actions (requests, pauses, assertions, etc...) modeling and orchestration
...&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-snapshots" class="anchor" aria-hidden="true" href="#snapshots"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Snapshots&lt;/h2&gt;
&lt;p&gt;For people wanting to use the latest evolutions, the SNAPSHOT versions are available from the Sonatype OSS &lt;a href="https://oss.sonatype.org/content/repositories/snapshots/io/gatling/highcharts/gatling-charts-highcharts/" rel="nofollow"&gt;repository&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-questions-help" class="anchor" aria-hidden="true" href="#questions-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Questions, help?&lt;/h2&gt;
&lt;p&gt;Read the &lt;a href="https://gatling.io/docs/current/" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Join the &lt;a href="https://groups.google.com/group/gatling" rel="nofollow"&gt;Gatling User Group&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Found a real bug? Raise an &lt;a href="https://github.com/gatling/gatling/issues?sort=created&amp;amp;direction=desc&amp;amp;state=open"&gt;issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-partners" class="anchor" aria-hidden="true" href="#partners"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Partners&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/gatling/gatling/master/src/sphinx/project/img/logo-takima-1-nom-bas.png"&gt;&lt;img alt="Takima" src="https://raw.githubusercontent.com/gatling/gatling/master/src/sphinx/project/img/logo-takima-1-nom-bas.png" width="80" style="max-width:100%;"&gt;&lt;/a&gt;    
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/gatling/gatling/master/src/sphinx/project/img/highsoft_logo.png"&gt;&lt;img src="https://raw.githubusercontent.com/gatling/gatling/master/src/sphinx/project/img/highsoft_logo.png" alt="Highsoft AS" style="max-width:100%;"&gt;&lt;/a&gt;    &lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>gatling</author><guid isPermaLink="false">https://github.com/gatling/gatling</guid><pubDate>Sat, 02 Nov 2019 00:14:00 GMT</pubDate></item><item><title>salesforce/TransmogrifAI #15 in Scala, Today</title><link>https://github.com/salesforce/TransmogrifAI</link><description>&lt;p&gt;&lt;i&gt;TransmogrifAI (pronounced trăns-mŏgˈrə-fī) is an AutoML library for building modular, reusable, strongly typed machine learning workflows on Apache Spark with minimal hand-tuning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-transmogrifai" class="anchor" aria-hidden="true" href="#transmogrifai"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TransmogrifAI&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://search.maven.org/search?q=g:com.salesforce.transmogrifai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b90684a7b523597c5bb23fc244e620a61596f49f/68747470733a2f2f696d672e736869656c64732e696f2f6d6176656e2d63656e7472616c2f762f636f6d2e73616c6573666f7263652e7472616e736d6f6772696661692f7472616e736d6f6772696661692d636f72655f322e31312e7376673f636f6c6f72423d626c7565" alt="Maven Central" data-canonical-src="https://img.shields.io/maven-central/v/com.salesforce.transmogrifai/transmogrifai-core_2.11.svg?colorB=blue" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://bintray.com/salesforce/maven/TransmogrifAI/_latestVersion" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/47a99ee284dc7fd42d48a542218e463ea3d781bb/68747470733a2f2f6170692e62696e747261792e636f6d2f7061636b616765732f73616c6573666f7263652f6d6176656e2f5472616e736d6f6772696641492f696d616765732f646f776e6c6f61642e737667" alt="Download" data-canonical-src="https://api.bintray.com/packages/salesforce/maven/TransmogrifAI/images/download.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://www.javadoc.io/doc/com.salesforce.transmogrifai/transmogrifai-core_2.11/0.6.1" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c292531f70d1fbdb09f5930964d05c32544fce03/68747470733a2f2f7777772e6a617661646f632e696f2f62616467652f636f6d2e73616c6573666f7263652e7472616e736d6f6772696661692f7472616e736d6f6772696661692d636f72655f322e31312f302e362e312e7376673f636f6c6f723d626c7565" alt="Javadocs" data-canonical-src="https://www.javadoc.io/badge/com.salesforce.transmogrifai/transmogrifai-core_2.11/0.6.1.svg?color=blue" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://spark.apache.org/downloads.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/af5a0294a9cde2e48fcb2362cc6a01f238be2ded/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f737061726b2d322e332d627269676874677265656e2e737667" alt="Spark version" data-canonical-src="https://img.shields.io/badge/spark-2.3-brightgreen.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://www.scala-lang.org/download/2.11.12.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6274e3e748c37e0d38b015430e783ee80a56b706/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7363616c612d322e31312d627269676874677265656e2e737667" alt="Scala version" data-canonical-src="https://img.shields.io/badge/scala-2.11-brightgreen.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="./LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/71c681eae8e353cf67a52a723f17a6173f114366/687474703a2f2f696d672e736869656c64732e696f2f3a6c6963656e73652d4253442d2d332d626c75652e737667" alt="License" data-canonical-src="http://img.shields.io/:license-BSD--3-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://gitter.im/salesforce/TransmogrifAI?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/256fd481d388adab18da870527536c884dd68889/68747470733a2f2f6261646765732e6769747465722e696d2f73616c6573666f7263652f5472616e736d6f6772696641492e737667" alt="Chat" data-canonical-src="https://badges.gitter.im/salesforce/TransmogrifAI.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.com/salesforce/TransmogrifAI" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/dc8bfdfb9abe10db2f5df7fafeef897d49d1d457/68747470733a2f2f7472617669732d63692e636f6d2f73616c6573666f7263652f5472616e736d6f6772696641492e7376673f746f6b656e3d457839637a5645554437417a50546d5668366958266272616e63683d6d6173746572" alt="TravisCI Build Status" data-canonical-src="https://travis-ci.com/salesforce/TransmogrifAI.svg?token=Ex9czVEUD7AzPTmVh6iX&amp;amp;branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/salesforce/TransmogrifAI" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/fc68118308525a770db5410e0442222d0dcb472a/68747470733a2f2f636972636c6563692e636f6d2f67682f73616c6573666f7263652f5472616e736d6f6772696641492e7376673f267374796c653d736869656c6426636972636c652d746f6b656e3d65383463313033376165333636353264333862343932303737323831383165653835333337653062" alt="CircleCI Build Status" data-canonical-src="https://circleci.com/gh/salesforce/TransmogrifAI.svg?&amp;amp;style=shield&amp;amp;circle-token=e84c1037ae36652d38b49207728181ee85337e0b" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://docs.transmogrif.ai/en/stable/?badge=stable" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/936463254be27afc7cf41025a34d91a73eaac884/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7472616e736d6f6772696661692f62616467652f3f76657273696f6e3d737461626c65" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/transmogrifai/badge/?version=stable" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://bestpractices.coreinfrastructure.org/projects/2557" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/341700f3e7461a8f24a3f14bd1048f39c34b9f5d/68747470733a2f2f626573747072616374696365732e636f7265696e6672617374727563747572652e6f72672f70726f6a656374732f323535372f6261646765" alt="CII Best Practices" data-canonical-src="https://bestpractices.coreinfrastructure.org/projects/2557/badge" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/salesforce/TransmogrifAI" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b2ae0814f49ac0913c3c11ca5c790709edf6a812/68747470733a2f2f636f6465636f762e696f2f67682f73616c6573666f7263652f5472616e736d6f6772696641492f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/salesforce/TransmogrifAI/branch/master/graph/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://www.codefactor.io/repository/github/salesforce/transmogrifai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/715e572a89c5ad8e17917890a83458f80c4fd14e/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f73616c6573666f7263652f7472616e736d6f6772696661692f6261646765" alt="CodeFactor" data-canonical-src="https://www.codefactor.io/repository/github/salesforce/transmogrifai/badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TransmogrifAI (pronounced trăns-mŏgˈrə-fī) is an AutoML library written in Scala that runs on top of Apache Spark. It was developed with a focus on accelerating machine learning developer productivity through machine learning automation, and an API that enforces compile-time type-safety, modularity, and reuse.
&lt;em&gt;Through automation, it achieves accuracies close to hand-tuned models with almost 100x reduction in time.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Use TransmogrifAI if you need a machine learning library to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Build production ready machine learning applications in hours, not months&lt;/li&gt;
&lt;li&gt;Build machine learning models without getting a Ph.D. in machine learning&lt;/li&gt;
&lt;li&gt;Build modular, reusable, strongly typed machine learning workflows&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To understand the motivation behind TransmogrifAI check out these:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://engineering.salesforce.com/open-sourcing-transmogrifai-4e5d0e098da2" rel="nofollow"&gt;Open Sourcing TransmogrifAI: Automated Machine Learning for Structured Data&lt;/a&gt;, a blog post by &lt;a href="https://github.com/snabar"&gt;@snabar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=93vsqjfGPCw&amp;amp;feature=youtu.be&amp;amp;t=2800" rel="nofollow"&gt;Meet TransmogrifAI, Open Source AutoML That Powers Einstein Predictions&lt;/a&gt;, a talk by &lt;a href="https://github.com/tovbinm"&gt;@tovbinm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=PKTvo9X9Sjg" rel="nofollow"&gt;Low Touch Machine Learning&lt;/a&gt;, a talk by &lt;a href="https://github.com/leahmcguire"&gt;@leahmcguire&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Skip to &lt;a href="#quick-start-and-documentation"&gt;Quick Start and Documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-predicting-titanic-survivors-with-transmogrifai" class="anchor" aria-hidden="true" href="#predicting-titanic-survivors-with-transmogrifai"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Predicting Titanic Survivors with TransmogrifAI&lt;/h2&gt;
&lt;p&gt;The Titanic dataset is an often-cited dataset in the machine learning community. The goal is to build a machine learnt model that will predict survivors from the Titanic passenger manifest. Here is how you would build the model using TransmogrifAI:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;readers&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;features&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;features&lt;/span&gt;.&lt;span class="pl-en"&gt;types&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;stages&lt;/span&gt;.&lt;span class="pl-en"&gt;impl&lt;/span&gt;.&lt;span class="pl-en"&gt;classification&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;org&lt;/span&gt;.&lt;span class="pl-en"&gt;apache&lt;/span&gt;.&lt;span class="pl-en"&gt;spark&lt;/span&gt;.&lt;span class="pl-en"&gt;SparkConf&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;org&lt;/span&gt;.&lt;span class="pl-en"&gt;apache&lt;/span&gt;.&lt;span class="pl-en"&gt;spark&lt;/span&gt;.&lt;span class="pl-en"&gt;sql&lt;/span&gt;.&lt;span class="pl-en"&gt;SparkSession&lt;/span&gt;

&lt;span class="pl-k"&gt;implicit&lt;/span&gt; &lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;spark&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;SparkSession&lt;/span&gt;.builder.config(&lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;SparkConf&lt;/span&gt;()).getOrCreate()
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;spark&lt;/span&gt;.&lt;span class="pl-en"&gt;implicits&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; Read Titanic data as a DataFrame&lt;/span&gt;
&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;passengersData&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;DataReaders&lt;/span&gt;.&lt;span class="pl-en"&gt;Simple&lt;/span&gt;.csvCase[&lt;span class="pl-en"&gt;Passenger&lt;/span&gt;](path &lt;span class="pl-k"&gt;=&lt;/span&gt; pathToData).readDataset().toDF()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; Extract response and predictor Features&lt;/span&gt;
&lt;span class="pl-k"&gt;val&lt;/span&gt; (survived, predictors) &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;FeatureBuilder&lt;/span&gt;.fromDataFrame[&lt;span class="pl-en"&gt;RealNN&lt;/span&gt;](passengersData, response &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;survived&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; Automated feature engineering&lt;/span&gt;
&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;featureVector&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; predictors.transmogrify()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; Automated feature validation and selection&lt;/span&gt;
&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;checkedFeatures&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; survived.sanityCheck(featureVector, removeBadFeatures &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;true&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; Automated model selection&lt;/span&gt;
&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;pred&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;BinaryClassificationModelSelector&lt;/span&gt;().setInput(survived, checkedFeatures).getOutput()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; Setting up a TransmogrifAI workflow and training the model&lt;/span&gt;
&lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;model&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;OpWorkflow&lt;/span&gt;().setInputDataset(passengersData).setResultFeatures(pred).train()

println(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Model summary:&lt;span class="pl-cce"&gt;\n&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; model.summaryPretty())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Model summary:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Evaluated Logistic Regression, Random Forest models with 3 folds and AuPR metric.
Evaluated 3 Logistic Regression models with AuPR between [0.6751930383321765, 0.7768725281794376]
Evaluated 16 Random Forest models with AuPR between [0.7781671467343991, 0.8104798040316159]

Selected model Random Forest classifier with parameters:
|-----------------------|--------------|
| Model Param           |     Value    |
|-----------------------|--------------|
| modelType             | RandomForest |
| featureSubsetStrategy |         auto |
| impurity              |         gini |
| maxBins               |           32 |
| maxDepth              |           12 |
| minInfoGain           |        0.001 |
| minInstancesPerNode   |           10 |
| numTrees              |           50 |
| subsamplingRate       |          1.0 |
|-----------------------|--------------|

Model evaluation metrics:
|-------------|--------------------|---------------------|
| Metric Name | Hold Out Set Value |  Training Set Value |
|-------------|--------------------|---------------------|
| Precision   |               0.85 |   0.773851590106007 |
| Recall      | 0.6538461538461539 |  0.6930379746835443 |
| F1          | 0.7391304347826088 |  0.7312186978297163 |
| AuROC       | 0.8821603927986905 |  0.8766642291593114 |
| AuPR        | 0.8225075757571668 |   0.850331080886535 |
| Error       | 0.1643835616438356 | 0.19682151589242053 |
| TP          |               17.0 |               219.0 |
| TN          |               44.0 |               438.0 |
| FP          |                3.0 |                64.0 |
| FN          |                9.0 |                97.0 |
|-------------|--------------------|---------------------|

Top model insights computed using correlation:
|-----------------------|----------------------|
| Top Positive Insights |      Correlation     |
|-----------------------|----------------------|
| sex = "female"        |   0.5177801026737666 |
| cabin = "OTHER"       |   0.3331391338844782 |
| pClass = 1            |   0.3059642953159715 |
|-----------------------|----------------------|
| Top Negative Insights |      Correlation     |
|-----------------------|----------------------|
| sex = "male"          |  -0.5100301587292186 |
| pClass = 3            |  -0.5075774968534326 |
| cabin = null          | -0.31463114463832633 |
|-----------------------|----------------------|

Top model insights computed using CramersV:
|-----------------------|----------------------|
|      Top Insights     |       CramersV       |
|-----------------------|----------------------|
| sex                   |    0.525557139885501 |
| embarked              |  0.31582347194683386 |
| age                   |  0.21582347194683386 |
|-----------------------|----------------------|
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While this may seem a bit too magical, for those who want more control, TransmogrifAI also provides the flexibility to completely specify all the features being extracted and all the algorithms being applied in your ML pipeline. Visit our &lt;a href="https://docs.transmogrif.ai" rel="nofollow"&gt;docs site&lt;/a&gt; for full documentation, getting started, examples, faq and other information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-adding-transmogrifai-into-your-project" class="anchor" aria-hidden="true" href="#adding-transmogrifai-into-your-project"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding TransmogrifAI into your project&lt;/h2&gt;
&lt;p&gt;You can simply add TransmogrifAI as a regular dependency to an existing project.
Start by picking TransmogrifAI version to match your project dependencies from the version matrix below (if not sure - take the &lt;strong&gt;stable&lt;/strong&gt; version):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;TransmogrifAI Version&lt;/th&gt;
&lt;th align="center"&gt;Spark Version&lt;/th&gt;
&lt;th align="center"&gt;Scala Version&lt;/th&gt;
&lt;th align="center"&gt;Java Version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.6.2 (unreleased, master)&lt;/td&gt;
&lt;td align="center"&gt;2.3&lt;/td&gt;
&lt;td align="center"&gt;2.11&lt;/td&gt;
&lt;td align="center"&gt;1.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;0.6.1 (stable)&lt;/strong&gt;, 0.6.0, 0.5.3, 0.5.2, 0.5.1, 0.5.0&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2.3&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2.11&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;1.8&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.4.0, 0.3.4&lt;/td&gt;
&lt;td align="center"&gt;2.2&lt;/td&gt;
&lt;td align="center"&gt;2.11&lt;/td&gt;
&lt;td align="center"&gt;1.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For Gradle in &lt;code&gt;build.gradle&lt;/code&gt; add:&lt;/p&gt;
&lt;div class="highlight highlight-source-groovy-gradle"&gt;&lt;pre&gt;&lt;span class="pl-en"&gt;repositories&lt;/span&gt; {
    jcenter()
    mavenCentral()
}
&lt;span class="pl-en"&gt;dependencies&lt;/span&gt; {
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; TransmogrifAI core dependency&lt;/span&gt;
    compile &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;com.salesforce.transmogrifai:transmogrifai-core_2.11:0.6.1&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; TransmogrifAI pretrained models, e.g. OpenNLP POS/NER models etc. (optional)&lt;/span&gt;
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; compile 'com.salesforce.transmogrifai:transmogrifai-models_2.11:0.6.1'&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For SBT in &lt;code&gt;build.sbt&lt;/code&gt; add:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;scalaVersion &lt;span class="pl-k"&gt;:&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;2.11.12&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;

resolvers &lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;Resolver&lt;/span&gt;.jcenterRepo

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; TransmogrifAI core dependency&lt;/span&gt;
libraryDependencies &lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;com.salesforce.transmogrifai&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%%&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;transmogrifai-core&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;0.6.1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; TransmogrifAI pretrained models, e.g. OpenNLP POS/NER models etc. (optional)&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; libraryDependencies += "com.salesforce.transmogrifai" %% "transmogrifai-models" % "0.6.1"&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then import TransmogrifAI into your code:&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; TransmogrifAI functionality: feature types, feature builders, feature dsl, readers, aggregators etc.&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;aggregators&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;features&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;features&lt;/span&gt;.&lt;span class="pl-en"&gt;types&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;readers&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; Spark enrichments (optional)&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;utils&lt;/span&gt;.&lt;span class="pl-en"&gt;spark&lt;/span&gt;.&lt;span class="pl-en"&gt;RichDataset&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;utils&lt;/span&gt;.&lt;span class="pl-en"&gt;spark&lt;/span&gt;.&lt;span class="pl-en"&gt;RichRDD&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;utils&lt;/span&gt;.&lt;span class="pl-en"&gt;spark&lt;/span&gt;.&lt;span class="pl-en"&gt;RichRow&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;utils&lt;/span&gt;.&lt;span class="pl-en"&gt;spark&lt;/span&gt;.&lt;span class="pl-en"&gt;RichMetadata&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;salesforce&lt;/span&gt;.&lt;span class="pl-en"&gt;op&lt;/span&gt;.&lt;span class="pl-en"&gt;utils&lt;/span&gt;.&lt;span class="pl-en"&gt;spark&lt;/span&gt;.&lt;span class="pl-en"&gt;RichStructType&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start-and-documentation" class="anchor" aria-hidden="true" href="#quick-start-and-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start and Documentation&lt;/h2&gt;
&lt;p&gt;Visit our &lt;a href="https://docs.transmogrif.ai" rel="nofollow"&gt;docs site&lt;/a&gt; for full documentation, getting started, examples, faq and other information.&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://scaladoc.transmogrif.ai" rel="nofollow"&gt;scaladoc&lt;/a&gt; for the programming API.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Kevin Moore	&lt;a href="https://github.com/jauntbox"&gt;@jauntbox&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kin Fai Kan	&lt;a href="https://github.com/kinfaikan"&gt;@kinfaikan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Leah McGuire &lt;a href="https://github.com/leahmcguire"&gt;@leahmcguire&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Matthew Tovbin &lt;a href="https://github.com/tovbinm"&gt;@tovbinm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Max Ovsiankin	&lt;a href="https://github.com/maxov"&gt;@maxov&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Michael Loh	&lt;a href="https://github.com/mikeloh77"&gt;@mikeloh77&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Michael Weil	&lt;a href="https://github.com/michaelweilsalesforce"&gt;@michaelweilsalesforce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Shubha Nabar	&lt;a href="https://github.com/snabar"&gt;@snabar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vitaly Gordon	&lt;a href="https://github.com/vitalyg"&gt;@vitalyg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vlad Patryshev	&lt;a href="https://github.com/vpatryshev"&gt;@vpatryshev&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-internal-contributors-prior-to-release" class="anchor" aria-hidden="true" href="#internal-contributors-prior-to-release"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Internal Contributors (prior to release)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Chris Rupley	&lt;a href="https://github.com/crupley"&gt;@crupley&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chris Wu	&lt;a href="https://github.com/cjwooo"&gt;@cjwooo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Eric Wayman	&lt;a href="https://github.com/ericwayman"&gt;@ericwayman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Felipe Oliveira	&lt;a href="https://github.com/feliperazeek"&gt;@feliperazeek&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gera Shegalov	&lt;a href="https://github.com/gerashegalov"&gt;@gerashegalov&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jean-Marc Soumet	&lt;a href="https://github.com/ajmssc"&gt;@ajmssc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Marco Vivero	&lt;a href="https://github.com/marcovivero"&gt;@marcovivero&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mario Rodriguez	&lt;a href="https://github.com/mrodriguezsfiq"&gt;@mrodriguezsfiq&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mayukh Bhaowal	&lt;a href="https://github.com/mayukhb"&gt;@mayukhb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Minh-An Quinn	&lt;a href="https://github.com/minhanquinn"&gt;@minhanquinn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Nicolas Drizard	&lt;a href="https://github.com/nicodri"&gt;@nicodri&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Oleg Gusak	&lt;a href="https://github.com/ogusak"&gt;@ogusak&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Patrick Framption	&lt;a href="https://github.com/tricktrap"&gt;@tricktrap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryle Goehausen	&lt;a href="https://github.com/ryleg"&gt;@ryleg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sanmitra Ijeri	&lt;a href="https://github.com/sanmitra"&gt;@sanmitra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sky Chen	&lt;a href="https://github.com/almandsky"&gt;@almandsky&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sophie Xiaodan Sun	&lt;a href="https://github.com/sxd929"&gt;@sxd929&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Till Bergmann	&lt;a href="https://github.com/tillbe"&gt;@tillbe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Xiaoqian Liu	&lt;a href="https://github.com/wingsrc"&gt;@wingsrc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;BSD 3-Clause&lt;/a&gt; © Salesforce.com, Inc.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>salesforce</author><guid isPermaLink="false">https://github.com/salesforce/TransmogrifAI</guid><pubDate>Sat, 02 Nov 2019 00:15:00 GMT</pubDate></item><item><title>ghostdogpr/caliban #16 in Scala, Today</title><link>https://github.com/ghostdogpr/caliban</link><description>&lt;p&gt;&lt;i&gt;Functional GraphQL backend in Scala&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-caliban" class="anchor" aria-hidden="true" href="#caliban"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Caliban&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://oss.sonatype.org/content/repositories/releases/com/github/ghostdogpr/caliban_2.12/" title="Sonatype Releases" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d0e10ce353faf85f54b7d8753c80bf6ba771ca6c/68747470733a2f2f696d672e736869656c64732e696f2f6e657875732f722f68747470732f6f73732e736f6e61747970652e6f72672f636f6d2e6769746875622e67686f7374646f6770722f63616c6962616e5f322e31322e737667" alt="Release Artifacts" title="Sonatype Releases" data-canonical-src="https://img.shields.io/nexus/r/https/oss.sonatype.org/com.github.ghostdogpr/caliban_2.12.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://circleci.com/gh/ghostdogpr/caliban" title="circleci" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f1de596c70482b3587d7055b577ab15dbd03de41/68747470733a2f2f636972636c6563692e636f6d2f67682f67686f7374646f6770722f63616c6962616e2e7376673f7374796c653d737667" alt="Build Status" title="circleci" data-canonical-src="https://circleci.com/gh/ghostdogpr/caliban.svg?style=svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Caliban is a purely functional library for creating GraphQL backends in Scala.
It relies on &lt;a href="https://github.com/propensive/magnolia"&gt;Magnolia&lt;/a&gt; to automatically derives GraphQL schemas from your data types, &lt;a href="https://github.com/lihaoyi/fastparse"&gt;Fastparse&lt;/a&gt; to parse queries and &lt;a href="https://github.com/zio/zio"&gt;ZIO&lt;/a&gt; to handle various effects.&lt;/p&gt;
&lt;p&gt;The design principles behind the library are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pure interface: errors and effects are returned explicitly (no exceptions thrown), all returned types are referentially transparent (no &lt;code&gt;Future&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;clean separation between schema definition and implementation: schema is defined and validated at compile time using Scala standard types, resolver is a simple value provided at runtime.&lt;/li&gt;
&lt;li&gt;minimal amount of boilerplate: no need to manually define a schema for every type in your API.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-consult-the-documentation-to-learn-how-to-use-caliban" class="anchor" aria-hidden="true" href="#consult-the-documentation-to-learn-how-to-use-caliban"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Consult the &lt;a href="https://ghostdogpr.github.io/caliban/docs/" rel="nofollow"&gt;Documentation&lt;/a&gt; to learn how to use Caliban.&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-any-questions-head-up-to-the-caliban-channel-on-zio-discord" class="anchor" aria-hidden="true" href="#any-questions-head-up-to-the-caliban-channel-on-zio-discord"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Any questions? Head up to the &lt;a href="https://discordapp.com/channels/629491597070827530/633200096393166868" rel="nofollow"&gt;#caliban&lt;/a&gt; channel on &lt;a href="https://discord.gg/EYpumuv" rel="nofollow"&gt;ZIO Discord&lt;/a&gt;.&lt;/h3&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ghostdogpr</author><guid isPermaLink="false">https://github.com/ghostdogpr/caliban</guid><pubDate>Sat, 02 Nov 2019 00:16:00 GMT</pubDate></item><item><title>gitbucket/gitbucket #17 in Scala, Today</title><link>https://github.com/gitbucket/gitbucket</link><description>&lt;p&gt;&lt;i&gt;A Git platform powered by Scala with easy installation, high extensibility &amp; GitHub API compatibility&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-gitbucket----" class="anchor" aria-hidden="true" href="#gitbucket----"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GitBucket &lt;a href="https://gitter.im/gitbucket/gitbucket" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/80991daab83c7504f938fe81ce11f8c241db9e53/68747470733a2f2f6261646765732e6769747465722e696d2f6769746275636b65742f6769746275636b65742e737667" alt="Gitter chat" data-canonical-src="https://badges.gitter.im/gitbucket/gitbucket.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://travis-ci.org/gitbucket/gitbucket" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c42fc7615b51af1d0b59be6233fb5c76d255d291/68747470733a2f2f7472617669732d63692e6f72672f6769746275636b65742f6769746275636b65742e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/gitbucket/gitbucket.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://maven-badges.herokuapp.com/maven-central/io.github.gitbucket/gitbucket_2.13" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/463aa101cad2714a9c400ff16a91fcc9948cb648/68747470733a2f2f6d6176656e2d6261646765732e6865726f6b756170702e636f6d2f6d6176656e2d63656e7472616c2f696f2e6769746875622e6769746275636b65742f6769746275636b65745f322e31332f62616467652e737667" alt="Maven Central" data-canonical-src="https://maven-badges.herokuapp.com/maven-central/io.github.gitbucket/gitbucket_2.13/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://github.com/gitbucket/gitbucket/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/8051e9938a1ab39cf002818dfceb6b6092f34d68/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;GitBucket is a Git web platform powered by Scala offering:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easy installation&lt;/li&gt;
&lt;li&gt;Intuitive UI&lt;/li&gt;
&lt;li&gt;High extensibility by plugins&lt;/li&gt;
&lt;li&gt;API compatibility with GitHub&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can try an &lt;a href="https://gitbucket.herokuapp.com/" rel="nofollow"&gt;online demo&lt;/a&gt; &lt;em&gt;(ID: root / Pass: root)&lt;/em&gt; of GitBucket, and also get the latest information at &lt;a href="https://gitbucket.github.io/gitbucket-news/" rel="nofollow"&gt;GitBucket News&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;p&gt;The current version of GitBucket provides many features such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Public / Private Git repositories (with http/https and ssh access)&lt;/li&gt;
&lt;li&gt;GitLFS support&lt;/li&gt;
&lt;li&gt;Repository viewer including an online file editor&lt;/li&gt;
&lt;li&gt;Issues, Pull Requests and Wiki for repositories&lt;/li&gt;
&lt;li&gt;Activity timeline and email notifications&lt;/li&gt;
&lt;li&gt;Account and group management with LDAP integration&lt;/li&gt;
&lt;li&gt;a Plug-in system&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to try the development version of GitBucket, see the &lt;a href="https://github.com/gitbucket/gitbucket/blob/master/doc/readme.md"&gt;Developer's Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;GitBucket requires &lt;strong&gt;Java8&lt;/strong&gt;. You have to install it, if it is not already installed.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download the latest &lt;strong&gt;gitbucket.war&lt;/strong&gt; from &lt;a href="https://github.com/gitbucket/gitbucket/releases"&gt;the releases page&lt;/a&gt; and run it by &lt;code&gt;java -jar gitbucket.war&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Go to &lt;code&gt;http://[hostname]:8080/&lt;/code&gt; and log in with ID: &lt;strong&gt;root&lt;/strong&gt; / Pass: &lt;strong&gt;root&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can specify following options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--port=[NUMBER]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--prefix=[CONTEXTPATH]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--host=[HOSTNAME]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--gitbucket.home=[DATA_DIR]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--temp_dir=[TEMP_DIR]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--max_file_size=[MAX_FILE_SIZE]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--upload_timeout=[MAX_UPLOAD_TIMEOUT]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;TEMP_DIR&lt;/code&gt; is used as the &lt;a href="https://www.eclipse.org/jetty/documentation/9.3.x/ref-temporary-directories.html" rel="nofollow"&gt;temporary directory for the jetty application context&lt;/a&gt;. This is the directory into which the &lt;code&gt;gitbucket.war&lt;/code&gt; file is unpacked, the source files are compiled, etc. If given this parameter &lt;strong&gt;must&lt;/strong&gt; match the path of an existing directory or the application will quit reporting an error; if not given the path used will be a &lt;code&gt;tmp&lt;/code&gt; directory inside the gitbucket home.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;MAX_FILE_SIZE&lt;/code&gt; is the max file size in bytes for upload files &lt;em&gt;( default is 3 MB -&amp;gt; 3 x 1024 x 1024 )&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;You can also deploy &lt;code&gt;gitbucket.war&lt;/code&gt; to a servlet container which supports Servlet 3.0 (like Jetty, Tomcat, JBoss, etc)&lt;/p&gt;
&lt;p&gt;For more information about installation on Mac or Windows Server (with IIS), or configuration of Apache or Nginx and also integration with other tools or services such as Jenkins or Slack, see &lt;a href="https://github.com/gitbucket/gitbucket/wiki"&gt;Wiki&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To upgrade GitBucket, replace &lt;code&gt;gitbucket.war&lt;/code&gt; with the new version, after stopping GitBucket. All GitBucket data is stored in &lt;code&gt;HOME/.gitbucket&lt;/code&gt; by default. So if you want to back up GitBucket's data, copy this directory to the backup location.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-plugins" class="anchor" aria-hidden="true" href="#plugins"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Plugins&lt;/h2&gt;
&lt;p&gt;GitBucket has a plug-in system that allows extra functionality. Officially the following plug-ins are provided:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/gitbucket/gitbucket-gist-plugin"&gt;gitbucket-gist-plugin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/gitbucket/gitbucket-emoji-plugin"&gt;gitbucket-emoji-plugin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/gitbucket/gitbucket-pages-plugin"&gt;gitbucket-pages-plugin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/gitbucket/gitbucket-notifications-plugin"&gt;gitbucket-notifications-plugin&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can find more plugins made by the community at &lt;a href="https://gitbucket-plugins.github.io/" rel="nofollow"&gt;GitBucket community plugins&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If you have any questions about GitBucket, see &lt;a href="https://github.com/gitbucket/gitbucket/wiki"&gt;Wiki&lt;/a&gt; and check issues whether there is a same question or request in the past.&lt;/li&gt;
&lt;li&gt;If you can't find same question and report, send it to &lt;a href="https://gitter.im/gitbucket/gitbucket" rel="nofollow"&gt;gitter room&lt;/a&gt; before raising an issue.&lt;/li&gt;
&lt;li&gt;The highest priority of GitBucket is the ease of installation and API compatibility with GitHub, so your feature request might be rejected if they go against those principles.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-whats-new-in-432x" class="anchor" aria-hidden="true" href="#whats-new-in-432x"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's New in 4.32.x&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-4320---7-aug-2019" class="anchor" aria-hidden="true" href="#4320---7-aug-2019"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.32.0 - 7 Aug 2019&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bump to Scala 2.13.0 and Scalatra 2.7.0&lt;/li&gt;
&lt;li&gt;Draft pull request&lt;/li&gt;
&lt;li&gt;Drop network installation of plugins&lt;/li&gt;
&lt;li&gt;Compare view works for commit id&lt;/li&gt;
&lt;li&gt;Apply default priority to pull requests&lt;/li&gt;
&lt;li&gt;Focus title after clicking issue / pull request edit button&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See the &lt;a href="CHANGELOG.md"&gt;change log&lt;/a&gt; for all of the updates.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>gitbucket</author><guid isPermaLink="false">https://github.com/gitbucket/gitbucket</guid><pubDate>Sat, 02 Nov 2019 00:17:00 GMT</pubDate></item><item><title>scalameta/metals #18 in Scala, Today</title><link>https://github.com/scalameta/metals</link><description>&lt;p&gt;&lt;i&gt;Scala language server with rich IDE features 🚀 &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-metals" class="anchor" aria-hidden="true" href="#metals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Metals&lt;/h1&gt;
&lt;a href="https://travis-ci.org/scalameta/metals" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/a2524f52634adb07fd59d018693f072f552a90d0/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f7363616c616d6574612f6d6574616c732f6d61737465722e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/travis/scalameta/metals/master.svg?style=flat-square" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://gitter.im/scalameta/metals" rel="nofollow"&gt;
&lt;img alt="Join the chat on Gitter" src="https://camo.githubusercontent.com/f7f01ae811427cbf536fb8047f67b2578a4cbe67/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f7363616c616d6574612f6d6574616c732e7376673f6c6f676f3d676974746572267374796c653d666c61742d73717561726526636f6c6f723d463731323633" data-canonical-src="https://img.shields.io/gitter/room/scalameta/metals.svg?logo=gitter&amp;amp;style=flat-square&amp;amp;color=F71263" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://twitter.com/scalameta" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/1079a3a4332d6c0fc38218766326dd730d3b2db4/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f7363616c616d6574612e7376673f6c6f676f3d74776974746572267374796c653d666c61742d73717561726526636f6c6f723d626c7565" data-canonical-src="https://img.shields.io/twitter/follow/scalameta.svg?logo=twitter&amp;amp;style=flat-square&amp;amp;color=blue" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://index.scala-lang.org/scalameta/metals/metals" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/809e4a056728ca7f7ab406a3308a2e35e708a4fb/68747470733a2f2f696e6465782e7363616c612d6c616e672e6f72672f7363616c616d6574612f6d6574616c732f6d6574616c732f6c61746573742e737667" data-canonical-src="https://index.scala-lang.org/scalameta/metals/metals/latest.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;See the website: &lt;a href="https://scalameta.org/metals/" rel="nofollow"&gt;https://scalameta.org/metals/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;See the contributing guide:
&lt;a href="https://scalameta.org/metals/docs/contributors/getting-started.html" rel="nofollow"&gt;https://scalameta.org/metals/docs/contributors/getting-started.html&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-team" class="anchor" aria-hidden="true" href="#team"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Team&lt;/h3&gt;
&lt;p&gt;The current maintainers (people who can merge pull requests) are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alexey Alekhin - &lt;a href="https://github.com/laughedelic"&gt;&lt;code&gt;@laughedelic&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gabriele Petronella - &lt;a href="https://github.com/gabro"&gt;&lt;code&gt;@gabro&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Johan Mudsam - &lt;a href="https://github.com/mudsam"&gt;&lt;code&gt;@mudsam&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jorge Vicente Cantero - &lt;a href="https://github.com/jvican"&gt;&lt;code&gt;@jvican&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Marek Żarnowski - &lt;a href="https://github.com/marek1840"&gt;&lt;code&gt;@marek1840&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ólafur Páll Geirsson - &lt;a href="https://github.com/olafurpg"&gt;&lt;code&gt;@olafurpg&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Shane Delmore - &lt;a href="https://github.com/ShaneDelmore"&gt;&lt;code&gt;@ShaneDelmore&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tomasz Godzik - &lt;a href="https://github.com/tgodzik"&gt;&lt;code&gt;@tgodzik&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;Huge thanks to &lt;a href="https://github.com/dragos"&gt;&lt;code&gt;@dragos&lt;/code&gt;&lt;/a&gt; for his work on a Scala
implementation of the LSP (see: &lt;a href="https://github.com/dragos/dragos-vscode-scala"&gt;https://github.com/dragos/dragos-vscode-scala&lt;/a&gt;).
This project helped us get quickly started with LSP. Since then, we have
refactored the project's original sources to the point where only a few simple
case classes remain.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-alternatives" class="anchor" aria-hidden="true" href="#alternatives"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Alternatives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.jetbrains.com/help/idea/discover-intellij-idea-for-scala.html" rel="nofollow"&gt;IntelliJ IDEA&lt;/a&gt;:
the most widely used IDE for Scala using a re-implementation of the Scala
typechecker.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scala-ide.org/" rel="nofollow"&gt;Scala IDE&lt;/a&gt;: Eclipse-based IDE using the Scala
Presentation Compiler.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-why-metals" class="anchor" aria-hidden="true" href="#why-metals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why Metals?&lt;/h2&gt;
&lt;p&gt;Metals = Meta (from Scalameta) + LS (from Language Server)&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>scalameta</author><guid isPermaLink="false">https://github.com/scalameta/metals</guid><pubDate>Sat, 02 Nov 2019 00:18:00 GMT</pubDate></item><item><title>ornicar/lila #19 in Scala, Today</title><link>https://github.com/ornicar/lila</link><description>&lt;p&gt;&lt;i&gt;♞ lichess.org: the forever free, adless and open source chess server ♞&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-lichessorg" class="anchor" aria-hidden="true" href="#lichessorg"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://lichess.org" rel="nofollow"&gt;lichess.org&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/ornicar/lila" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6df19d7ec694d0aec86100cad5931cbe26043117/68747470733a2f2f7472617669732d63692e6f72672f6f726e696361722f6c696c612e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/ornicar/lila.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://crowdin.com/project/lichess" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/23588c80b3a8c9a04dd2e12956326ac0f3d2ee5d/68747470733a2f2f64333232637174353834626f346f2e636c6f756466726f6e742e6e65742f6c6963686573732f6c6f63616c697a65642e737667" alt="Crowdin" data-canonical-src="https://d322cqt584bo4o.cloudfront.net/lichess/localized.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/lichess" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c9726f1341a098dd47e78700566287dc967e767c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547769747465722d2534306c6963686573732d626c75652e737667" alt="Twitter" data-canonical-src="https://img.shields.io/badge/Twitter-%40lichess-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/ornicar/lila/master/public/images/home-bicolor.png"&gt;&lt;img src="https://raw.githubusercontent.com/ornicar/lila/master/public/images/home-bicolor.png" alt="lichess.org" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lila (li[chess in sca]la) is a free online chess game server focused on &lt;a href="https://lichess.org/games" rel="nofollow"&gt;realtime&lt;/a&gt; gameplay and ease of use.&lt;/p&gt;
&lt;p&gt;It features a &lt;a href="https://lichess.org/games/search" rel="nofollow"&gt;search engine&lt;/a&gt;,
&lt;a href="https://lichess.org/ief49lif" rel="nofollow"&gt;computer analysis&lt;/a&gt; distributed with &lt;a href="https://github.com/niklasf/fishnet"&gt;fishnet&lt;/a&gt;,
&lt;a href="https://lichess.org/tournament" rel="nofollow"&gt;tournaments&lt;/a&gt;,
&lt;a href="https://lichess.org/simul" rel="nofollow"&gt;simuls&lt;/a&gt;,
&lt;a href="https://lichess.org/forum" rel="nofollow"&gt;forums&lt;/a&gt;,
&lt;a href="https://lichess.org/team" rel="nofollow"&gt;teams&lt;/a&gt;,
&lt;a href="https://lichess.org/training" rel="nofollow"&gt;tactic trainer&lt;/a&gt;,
a &lt;a href="https://lichess.org/mobile" rel="nofollow"&gt;mobile app&lt;/a&gt;,
and a &lt;a href="https://lichess.org/study" rel="nofollow"&gt;shared analysis board&lt;/a&gt;.
The UI is available in more than &lt;a href="https://crowdin.com/project/lichess" rel="nofollow"&gt;130 languages&lt;/a&gt; thanks to the community.&lt;/p&gt;
&lt;p&gt;Lichess is written in &lt;a href="https://www.scala-lang.org/" rel="nofollow"&gt;Scala 2.11&lt;/a&gt;,
and relies on the &lt;a href="https://www.playframework.com/" rel="nofollow"&gt;Play 2.4&lt;/a&gt; framework.
&lt;a href="http://www.lihaoyi.com/scalatags/" rel="nofollow"&gt;scalatags&lt;/a&gt; is used for templating.
Pure chess logic is contained in the &lt;a href="https://github.com/ornicar/scalachess"&gt;scalachess&lt;/a&gt; submodule.
The server is fully asynchronous, making heavy use of Scala Futures and &lt;a href="http://akka.io" rel="nofollow"&gt;Akka 2 actors&lt;/a&gt;.
Some WebSocket connections are handled by a &lt;a href="https://github.com/ornicar/lila-ws"&gt;seperate server&lt;/a&gt; that communicates using &lt;a href="https://redis.io/" rel="nofollow"&gt;redis&lt;/a&gt;.
Lichess talks to &lt;a href="http://stockfishchess.org/" rel="nofollow"&gt;Stockfish&lt;/a&gt; deployed in an &lt;a href="https://github.com/niklasf/fishnet"&gt;AI cluster&lt;/a&gt; of donated servers.
It uses &lt;a href="https://mongodb.org" rel="nofollow"&gt;MongoDB&lt;/a&gt; to store more than 1 billion games, which are indexed by &lt;a href="http://elasticsearch.org" rel="nofollow"&gt;elasticsearch&lt;/a&gt;.
HTTP requests and WebSocket connections are proxied by &lt;a href="http://nginx.org" rel="nofollow"&gt;nginx&lt;/a&gt;.
The web client is written in &lt;a href="https://typescriptlang.org" rel="nofollow"&gt;TypeScript&lt;/a&gt; and &lt;a href="https://github.com/snabbdom/snabbdom"&gt;snabbdom&lt;/a&gt;, using &lt;a href="https://sass-lang.com/" rel="nofollow"&gt;Sass&lt;/a&gt; to generate CSS.
The &lt;a href="https://lichess.org/blog" rel="nofollow"&gt;blog&lt;/a&gt; uses a free open content plan from &lt;a href="https://prismic.io" rel="nofollow"&gt;prismic.io&lt;/a&gt;.
All rated games are published in a &lt;a href="https://database.lichess.org" rel="nofollow"&gt;free PGN database&lt;/a&gt;.
Browser testing done with &lt;a href="https://www.browserstack.com" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/ornicar/lila/master/public/images/browserstack.png" alt="Browserstack" style="max-width:100%;"&gt;&lt;/a&gt;.
Please help us &lt;a href="https://crowdin.com/project/lichess" rel="nofollow"&gt;translate lichess with Crowdin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://lichess.org/source" rel="nofollow"&gt;lichess.org/source&lt;/a&gt; for a list of repositories.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discord.gg/hy5jqSs" rel="nofollow"&gt;Join us on discord&lt;/a&gt; or in the &lt;code&gt;#lichess&lt;/code&gt; freenode IRC channel for more info.
Use &lt;a href="https://github.com/ornicar/lila/issues"&gt;GitHub issues&lt;/a&gt; for bug reports and feature requests.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;The Wiki describes &lt;a href="https://github.com/ornicar/lila/wiki/Lichess-Development-Onboarding"&gt;how to setup a development environment&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The source code is available for learning and contribution, but please don't just setup a public Lichess clone. Don't expect developers to help you run your own instance. Questions about the installation and runtime issues will probably be ignored.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-http-api" class="anchor" aria-hidden="true" href="#http-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTTP API&lt;/h2&gt;
&lt;p&gt;Feel free to use the &lt;a href="https://lichess.org/api" rel="nofollow"&gt;Lichess API&lt;/a&gt; in your applications and websites.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-credits" class="anchor" aria-hidden="true" href="#credits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credits&lt;/h2&gt;
&lt;p&gt;See the &lt;a href="https://github.com/ornicar/lila/graphs/contributors"&gt;contributors&lt;/a&gt; on this repository and &lt;a href="https://lichess.org/thanks" rel="nofollow"&gt;lichess.org/thanks&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-supported-browsers" class="anchor" aria-hidden="true" href="#supported-browsers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supported browsers&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Notes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Chromium / Chrome&lt;/td&gt;
&lt;td&gt;last 10&lt;/td&gt;
&lt;td&gt;Full support, fastest local analysis&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Firefox&lt;/td&gt;
&lt;td&gt;55+&lt;/td&gt;
&lt;td&gt;Full support, second fastest local analysis&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Safari&lt;/td&gt;
&lt;td&gt;10.1+&lt;/td&gt;
&lt;td&gt;Reasonable support&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Opera&lt;/td&gt;
&lt;td&gt;55+&lt;/td&gt;
&lt;td&gt;Reasonable support&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Edge&lt;/td&gt;
&lt;td&gt;17+&lt;/td&gt;
&lt;td&gt;Reasonable support&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Older browsers (including any version of Internet Explorer) will not work.
For your own sake, please upgrade. Security and performance, think about
it!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Lila is licensed under the GNU Affero General Public License 3 or any later
version at your choice with an exception for Highcharts. See COPYING for
details.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ornicar</author><guid isPermaLink="false">https://github.com/ornicar/lila</guid><pubDate>Sat, 02 Nov 2019 00:19:00 GMT</pubDate></item><item><title>bkirwi/decline #20 in Scala, Today</title><link>https://github.com/bkirwi/decline</link><description>&lt;p&gt;&lt;i&gt;A composable command-line parser for Scala.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-decline" class="anchor" aria-hidden="true" href="#decline"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;decline&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/bkirwi/decline" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4ce789476dde87729026dfbe1a8d946fef4fd5fe/68747470733a2f2f7472617669732d63692e6f72672f626b697277692f6465636c696e652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/bkirwi/decline.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A composable command-line parser, inspired by &lt;a href="https://github.com/pcapriotti/optparse-applicative"&gt;&lt;code&gt;optparse-applicative&lt;/code&gt;&lt;/a&gt;
and built on &lt;a href="https://github.com/typelevel/cats"&gt;&lt;code&gt;cats&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-scala"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;cats&lt;/span&gt;.&lt;span class="pl-en"&gt;implicits&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;
&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-en"&gt;com&lt;/span&gt;.&lt;span class="pl-en"&gt;monovore&lt;/span&gt;.&lt;span class="pl-en"&gt;decline&lt;/span&gt;.&lt;span class="pl-en"&gt;_&lt;/span&gt;

&lt;span class="pl-k"&gt;object&lt;/span&gt; &lt;span class="pl-en"&gt;HelloWorld&lt;/span&gt; &lt;span class="pl-k"&gt;extends&lt;/span&gt; &lt;span class="pl-e"&gt;CommandApp&lt;/span&gt;(
  name &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;hello-world&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
  header &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Says hello!&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
  main &lt;span class="pl-k"&gt;=&lt;/span&gt; {
    &lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;userOpt&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt;
      &lt;span class="pl-en"&gt;Opts&lt;/span&gt;.option[&lt;span class="pl-k"&gt;String&lt;/span&gt;](&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;target&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, help &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Person to greet.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
        .withDefault(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;world&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

    &lt;span class="pl-k"&gt;val&lt;/span&gt; &lt;span class="pl-smi"&gt;quietOpt&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;Opts&lt;/span&gt;.flag(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;quiet&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, help &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Whether to be quiet.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).orFalse

    (userOpt, quietOpt).mapN { (user, quiet) &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; 

      &lt;span class="pl-k"&gt;if&lt;/span&gt; (quiet) println(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;...&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
      &lt;span class="pl-k"&gt;else&lt;/span&gt; println(&lt;span class="pl-k"&gt;s&lt;/span&gt;&lt;span class="pl-s"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;Hello &lt;/span&gt;$user&lt;span class="pl-s"&gt;!&lt;/span&gt;&lt;span class="pl-s"&gt;"&lt;/span&gt;)
    }
  }
)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;To get started, please visit &lt;a href="http://monovore.com/decline/" rel="nofollow"&gt;monovore.com/decline&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-about-the-project" class="anchor" aria-hidden="true" href="#about-the-project"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About the Project&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;decline&lt;/code&gt; is a &lt;a href="https://typelevel.org/projects/" rel="nofollow"&gt;Typelevel Incubator&lt;/a&gt; project,
and follows the &lt;a href="https://typelevel.org/code_of_conduct.html" rel="nofollow"&gt;Scala Code of Conduct&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This project is released under the &lt;a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bkirwi</author><guid isPermaLink="false">https://github.com/bkirwi/decline</guid><pubDate>Sat, 02 Nov 2019 00:20:00 GMT</pubDate></item><item><title>wavesplatform/Waves #21 in Scala, Today</title><link>https://github.com/wavesplatform/Waves</link><description>&lt;p&gt;&lt;i&gt;Waves Node application&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-waves-" class="anchor" aria-hidden="true" href="#waves-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Waves &lt;a href="https://travis-ci.org/wavesplatform/Waves" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/72b6866e22012a45d6af1cda42c381a9e87adaa7/68747470733a2f2f7472617669732d63692e6f72672f7761766573706c6174666f726d2f57617665732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/wavesplatform/Waves.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Waves is an open source &lt;a href="https://wavesplatform.com/" rel="nofollow"&gt;blockchain platform&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can use it to build your own decentralised applications. Waves provides full blockchain ecosystem including smart contracts language called RIDE.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-does-the-blockchain-network-work" class="anchor" aria-hidden="true" href="#how-does-the-blockchain-network-work"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How does the blockchain network work?&lt;/h3&gt;
&lt;p&gt;There is a huge collection of nodes deployed by miners that store all of the network information in the chain of blocks (blockchain), process requests and can add new transactions to the network after checking their compliance with the rules. The miners are rewarded with the network coins called MRT.&lt;/p&gt;
&lt;p&gt;The main advantage of this technology is that each node is a synchronized copy of the main blockchain: it means that the information is stored decentralized and won't be overwritten globally if one of the users changes it at one of the node storages. This can guarantee that the user's information will stay fair and unchangeable.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-what-is-node" class="anchor" aria-hidden="true" href="#what-is-node"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is node?&lt;/h1&gt;
&lt;p&gt;A &lt;strong&gt;node&lt;/strong&gt; is a &lt;a href="https://en.wikipedia.org/wiki/Host_(network)" rel="nofollow"&gt;host&lt;/a&gt; connected to the &lt;a href="https://docs.wavesplatform.com/en/blockchain/blockchain-network.html" rel="nofollow"&gt;blockchain network&lt;/a&gt; using the &lt;a href="https://github.com/wavesplatform/Waves"&gt;Waves Node&lt;/a&gt; application.&lt;/p&gt;
&lt;p&gt;Node functions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.wavesplatform.com/en/blockchain/block.html" rel="nofollow"&gt;blocks&lt;/a&gt; storage&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.wavesplatform.com/en/blockchain/transaction/transaction-validation.html" rel="nofollow"&gt;transaction validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sending &lt;a href="https://docs.wavesplatform.com/en/blockchain/transaction.html" rel="nofollow"&gt;transactions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learn more about Waves Full Node in the &lt;a href="https://docs.wavesplatform.com/en/waves-node/what-is-a-full-node.html" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-how-to-build-and-test-a-node" class="anchor" aria-hidden="true" href="#how-to-build-and-test-a-node"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Build and Test a Node&lt;/h1&gt;
&lt;p&gt;The node can be built and installed wherever java can run. For &lt;em&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;&lt;/em&gt;,sbt packageAll ‌produces only deb package but for other operating systems, ZIP archive or a fat JAR can be used as well. To build and test your Waves Node, you will need to follow these steps:&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-1-setup-the-environment" class="anchor" aria-hidden="true" href="#1-setup-the-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Setup the environment&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-installing-java" class="anchor" aria-hidden="true" href="#installing-java"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing Java&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install default-jre default-jdk
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-installing-sbt" class="anchor" aria-hidden="true" href="#installing-sbt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing SBT&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please follow the SBT installation instructions depending on your operating system (&lt;a href="https://www.scala-sbt.org/1.0/docs/Installing-sbt-on-Mac.html" rel="nofollow"&gt;Mac&lt;/a&gt;, &lt;a href="https://www.scala-sbt.org/1.0/docs/Installing-sbt-on-Windows.html" rel="nofollow"&gt;Windows&lt;/a&gt;, &lt;a href="https://www.scala-sbt.org/1.0/docs/Installing-sbt-on-Linux.html" rel="nofollow"&gt;Linux&lt;/a&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-2-obtaining-source-codes" class="anchor" aria-hidden="true" href="#2-obtaining-source-codes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Obtaining Source Codes&lt;/h2&gt;
&lt;p&gt;Cloning with HTTPS URLs (recommended)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/wavesplatform/Waves.git
cd Waves
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or cloning with SSH URLs&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone git@github.com:wavesplatform/Waves.git
cd Waves
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-3-compilation-and-unit-tests" class="anchor" aria-hidden="true" href="#3-compilation-and-unit-tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. Compilation and unit tests&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;sbt checkPR
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-4-running-node-integration-tests-optional" class="anchor" aria-hidden="true" href="#4-running-node-integration-tests-optional"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4. Running NODE integration tests (optional)&lt;/h2&gt;
&lt;p&gt;Create a Docker image before you run any test: &lt;code&gt;sbt node-it/docker&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run all tests: &lt;code&gt;SBT_THREAD_NUMBER=4 sbt node-it/test&lt;/code&gt; . You can increase or decrease number of parallel running tests by changing &lt;code&gt;SBT_THREAD_NUMBER&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Run one test: &lt;code&gt;sbt node-it/testOnly *.TestClassName&lt;/code&gt; or &lt;code&gt;node-it/testOnly full.package.TestClassName&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-5-building-packages" class="anchor" aria-hidden="true" href="#5-building-packages"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5. Building packages&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-mainnet" class="anchor" aria-hidden="true" href="#mainnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mainnet&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sbt packageAll
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-testnet" class="anchor" aria-hidden="true" href="#testnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testnet&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sbt -Dnetwork=testnet packageAll
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-6-installing-deb-package" class="anchor" aria-hidden="true" href="#6-installing-deb-package"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;6. Installing DEB package&lt;/h2&gt;
&lt;p&gt;DEB package located in target folder. You can replace '*' with actual package name:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo dpkg -i node/target/*.deb
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-7-running-fat-jar" class="anchor" aria-hidden="true" href="#7-running-fat-jar"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;7. Running fat jar&lt;/h2&gt;
&lt;p&gt;You can replace waves-all*.jar with actual jar name (it should have "all"-word):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;java -jar node/target/waves-all*.jar path/to/config/file
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note.&lt;/strong&gt; For OSX - homebrew is preferable choice. You can install java with brew cask install java and sbt with brew install sbt@1. Build/Test steps are common for any OS (but you should use ‘' instead of '/' in windows). {% endprettyhint %}&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-8-running-an-extension-project-locally-during-development" class="anchor" aria-hidden="true" href="#8-running-an-extension-project-locally-during-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;8. Running an extension project locally during development&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-sbt" class="anchor" aria-hidden="true" href="#sbt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SBT&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;sbt "extension-module/run /path/to/configuration"

&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-intellij-idea" class="anchor" aria-hidden="true" href="#intellij-idea"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;IntelliJ IDEA&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Click on &lt;code&gt;Add configuration&lt;/code&gt; (or &lt;code&gt;Edit configurations...&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Click on &lt;code&gt;+&lt;/code&gt; to add a new configuration, choose &lt;code&gt;Application&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Specify:
&lt;ul&gt;
&lt;li&gt;Main class: &lt;code&gt;com.wavesplatform.Application&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Program arguments: &lt;code&gt;/path/to/configuration&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use classpath of module: &lt;code&gt;extension-module&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Click on &lt;code&gt;OK&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Run this configuration&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;There is another way to run a Waves Node: using &lt;strong&gt;Waves Docker container&lt;/strong&gt;. Learn more about it in the &lt;a href="https://docs.wavesplatform.com/en/waves-node/waves-node-in-docker.html" rel="nofollow"&gt;docs&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-useful-links" class="anchor" aria-hidden="true" href="#useful-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Useful links&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.wavesplatform.com/" rel="nofollow"&gt;Official Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://client.wavesplatform.com/" rel="nofollow"&gt;Client Mainnet&lt;/a&gt; – Waves Platform client&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wavesexplorer.com/" rel="nofollow"&gt;Explorer Mainnet&lt;/a&gt; – Waves Platform transactions explorer&lt;/li&gt;
&lt;li&gt;&lt;a href="https://testnet.wavesplatform.com/" rel="nofollow"&gt;Testnet&lt;/a&gt; – the alternative Waves blockchain being used for testing&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wavesexplorer.com/testnet" rel="nofollow"&gt;Testnet Explorer&lt;/a&gt; – Test Net transactions explorer&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wavesplatform/waves-documentation/blob/master/en/ride/ride-script.md"&gt;RIDE&lt;/a&gt; – Waves smart contract coding language&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ide.wavesplatform.com/" rel="nofollow"&gt;Waves Ride IDE&lt;/a&gt; – software for RIDE coding&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h1&gt;
&lt;p&gt;Keep up with the latest news and articles, and find out all about events happening on the &lt;a href="https://wavesplatform.com/" rel="nofollow"&gt;Waves Platform&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://t.me/waves_ride_dapps_dev" rel="nofollow"&gt;Telegram Dev Chat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://forum.wavesplatform.com/" rel="nofollow"&gt;Community Forum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wavescommunity.com/" rel="nofollow"&gt;Community Portal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.wavesplatform.com/" rel="nofollow"&gt;Waves Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://support.wavesplatform.com/" rel="nofollow"&gt;Support&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/97fa03cac759a772255b93c64ab1c9f76a103681/68747470733a2f2f7777772e796f75726b69742e636f6d2f696d616765732f796b6c6f676f2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/97fa03cac759a772255b93c64ab1c9f76a103681/68747470733a2f2f7777772e796f75726b69742e636f6d2f696d616765732f796b6c6f676f2e706e67" alt="img" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We use YourKit full-featured Java Profiler to make Waves node faster. YourKit, LLC is the creator of innovative and intelligent tools for profiling Java and .NET applications.&lt;/p&gt;
&lt;p&gt;Take a look at YourKit's leading software products: YourKit Java Profiler and YourKit .NET Profiler.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>wavesplatform</author><guid isPermaLink="false">https://github.com/wavesplatform/Waves</guid><pubDate>Sat, 02 Nov 2019 00:21:00 GMT</pubDate></item><item><title>lw-lin/CoolplaySpark #22 in Scala, Today</title><link>https://github.com/lw-lin/CoolplaySpark</link><description>&lt;p&gt;&lt;i&gt;酷玩 Spark: Spark 源代码解析、Spark 类库等&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a target="_blank" rel="noopener noreferrer" href="coolplay_spark_logo_cn_small.png"&gt;&lt;img src="coolplay_spark_logo_cn_small.png" alt="coolplay_spark_logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;欢迎来到 Coolplay Spark（中文名：酷玩 Spark）！&lt;/p&gt;
&lt;p&gt;Coolplay Spark 将包含 Spark 源代码解析、Spark 类库、Spark 代码等。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-已发布的内容" class="anchor" aria-hidden="true" href="#已发布的内容"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;已发布的内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/lw-lin/CoolplaySpark/tree/master/Structured%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97"&gt;Structured Streaming 源码解析系列&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Structured Streaming 是 Spark 2.x 的流数据处理系统&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97"&gt;Spark Streaming 源码解析系列&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Spark Streaming 是 Spark 1.x 的流数据处理系统&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20%E8%B5%84%E6%BA%90%E9%9B%86%E5%90%88"&gt;Spark 资源集合&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;包括 Spark 技术交流（中文社群）&lt;br&gt;&lt;a target="_blank" rel="noopener noreferrer" href="Spark%20%E8%B5%84%E6%BA%90%E9%9B%86%E5%90%88/resources/wechat_spark_streaming_small_.PNG"&gt;&lt;img src="Spark%20%E8%B5%84%E6%BA%90%E9%9B%86%E5%90%88/resources/wechat_spark_streaming_small_.PNG" alt="wechat_spark_streaming_small" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Spark Summit 视频等资源集合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>lw-lin</author><guid isPermaLink="false">https://github.com/lw-lin/CoolplaySpark</guid><pubDate>Sat, 02 Nov 2019 00:22:00 GMT</pubDate></item><item><title>adobe-fonts/source-han-sans #23 in Scala, Today</title><link>https://github.com/adobe-fonts/source-han-sans</link><description>&lt;p&gt;&lt;i&gt;Source Han Sans | 思源黑体 | 思源黑體 | 思源黑體 香港 | 源ノ角ゴシック | 본고딕&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://github.com/adobe-fonts/source-han-sans/"&gt;English&lt;/a&gt; &lt;a href="README-CN.md"&gt;简体中文&lt;/a&gt; &lt;a href="README-TW.md"&gt;繁體中文&lt;/a&gt; &lt;a href="README-JP.md"&gt;日本語&lt;/a&gt; &lt;a href="README-KR.md"&gt;한국어&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-source-han-sans" class="anchor" aria-hidden="true" href="#source-han-sans"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source Han Sans&lt;/h1&gt;
&lt;p&gt;Source Han Sans is a set of OpenType/CFF Pan-CJK fonts. In addition to functional OpenType fonts, this open source project provides all of the source files that were used to build these OpenType fonts by using the AFDKO &lt;em&gt;makeotf&lt;/em&gt; and &lt;em&gt;otf2otc&lt;/em&gt; tools.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-download-the-fonts-otf-otc-super-otc-subset-otf" class="anchor" aria-hidden="true" href="#download-the-fonts-otf-otc-super-otc-subset-otf"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download the fonts (OTF, OTC, Super OTC, Subset OTF)&lt;/h2&gt;
&lt;p&gt;Individual font resources or ZIP files for various deployment configurations are available for download:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/adobe-fonts/source-han-sans/tree/release"&gt;Latest release&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To help decide which fonts to download, please refer to the Configurations section of the &lt;a href="https://github.com/adobe-fonts/source-han-sans/raw/release/SourceHanSansReadMe.pdf"&gt;official font readme file&lt;/a&gt;. Those who are unfamiliar with GitHub are encouraged to reference the &lt;a href="https://github.com/adobe-fonts/source-han-serif/raw/release/download-guide-source-han.pdf"&gt;official Source Han download guide&lt;/a&gt;, which is provided in English, Japanese, Korean, Simplified Chinese, and Traditional Chinese.&lt;/p&gt;
&lt;p&gt;You can also download entire &lt;a href="../../releases"&gt;releases&lt;/a&gt; that include all available configurations in a single ZIP file. The ZIP file for the &lt;a href="../../releases/latest"&gt;latest release&lt;/a&gt; is approximately 2GB.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-building-the-fonts-from-source" class="anchor" aria-hidden="true" href="#building-the-fonts-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building the fonts from source&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h3&gt;
&lt;p&gt;To build the binary font files from source, you need to have installed the &lt;a href="https://github.com/adobe-type-tools/afdko/"&gt;Adobe Font Development Kit for OpenType&lt;/a&gt; (AFDKO). The AFDKO tools are widely used for font development today, and are part of most font editor applications.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-build-all-fonts" class="anchor" aria-hidden="true" href="#build-all-fonts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build all fonts&lt;/h3&gt;
&lt;p&gt;In this repository, all necessary files are in place for building the OpenType/CFF and OpenType/CFF Collection fonts. The &lt;a href="COMMANDS.txt"&gt;COMMANDS.txt&lt;/a&gt; file provides the command lines that are used to build the OTFs and OTCs.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-involved" class="anchor" aria-hidden="true" href="#getting-involved"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;Send suggestions for changes to the Source Han Sans project maintainer, &lt;a href="mailto:lunde@adobe.com?subject=%5BGitHub%5D%20Source%20Han%20Sans"&gt;Dr. Ken Lunde&lt;/a&gt;, for consideration.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-further-information" class="anchor" aria-hidden="true" href="#further-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Further information&lt;/h2&gt;
&lt;p&gt;For information about the design and background of Source Han Sans, please refer to the &lt;a href="https://github.com/adobe-fonts/source-han-sans/raw/release/SourceHanSansDesignGuide.pdf"&gt;design guide&lt;/a&gt; and &lt;a href="https://github.com/adobe-fonts/source-han-sans/raw/release/SourceHanSansReadMe.pdf"&gt;official font readme file&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>adobe-fonts</author><guid isPermaLink="false">https://github.com/adobe-fonts/source-han-sans</guid><pubDate>Sat, 02 Nov 2019 00:23:00 GMT</pubDate></item><item><title>spotify/scio #24 in Scala, Today</title><link>https://github.com/spotify/scio</link><description>&lt;p&gt;&lt;i&gt;A Scala API for Apache Beam and Google Cloud Dataflow.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-scio" class="anchor" aria-hidden="true" href="#scio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Scio&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://circleci.com/gh/spotify/scio" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f80c6c40430a1001959ddc970c2a59ee4200b7b6/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f70726f6a6563742f6769746875622f73706f746966792f7363696f2f6d61737465722e737667" alt="Build Status" data-canonical-src="https://img.shields.io/circleci/project/github/spotify/scio/master.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://codecov.io/github/spotify/scio?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/1e5d591e1d3cbd2a96cdfea9e259f53f66f50fb0/68747470733a2f2f636f6465636f762e696f2f6769746875622f73706f746966792f7363696f2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="https://codecov.io/github/spotify/scio/coverage.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="./LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/76c80e90d7fc32651eac47c09837c16696c3e3fa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f73706f746966792f7363696f2e737667" alt="GitHub license" data-canonical-src="https://img.shields.io/github/license/spotify/scio.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://maven-badges.herokuapp.com/maven-central/com.spotify/scio-core_2.12" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8be9a46a63a61bc2b38f0b7b80d800caf3198ec5/68747470733a2f2f696d672e736869656c64732e696f2f6d6176656e2d63656e7472616c2f762f636f6d2e73706f746966792f7363696f2d636f72655f322e31322e737667" alt="Maven Central" data-canonical-src="https://img.shields.io/maven-central/v/com.spotify/scio-core_2.12.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://spotify.github.io/scio/api/com/spotify/scio/index.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c0eaae219904b315f0dd76c72c9106a089d4a615/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7363616c61646f632d6c61746573742d626c75652e737667" alt="Scaladoc" data-canonical-src="https://img.shields.io/badge/scaladoc-latest-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/spotify/scio" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/07667aadc106a44dcf77c5cdbbef82fbc4430e6f/68747470733a2f2f6261646765732e6769747465722e696d2f73706f746966792f7363696f2e737667" alt="Join the chat at https://gitter.im/spotify/scio" data-canonical-src="https://badges.gitter.im/spotify/scio.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://scala-steward.org" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3bc29c0da5030c382bf252762c73f4bd2064188c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5363616c615f537465776172642d68656c70696e672d627269676874677265656e2e7376673f7374796c653d666c6174266c6f676f3d646174613a696d6167652f706e673b6261736536342c6956424f5277304b47676f414141414e5355684555674141414134414141415143414d414141415253723449414141415646424d56455541414143486a6f6a6c4f79354e576c724b7a6359524b6a47466a496270323933597963754c6133705959324c5371716c346633704355465467536a4e6f6459526d6358557350442f4e5454626a52532b326a6f6d68676e7a4e633232336347765a53304861534430584c6a6261536a456c6849722b4141414141585253546c4d41514f62595a67414141486c4a52454655434e644e796f734f77794149685748415153315674376137372f3366637878646d763078776d636b75744152316e6b6d34676762794563672f77576d6c474c444141336f4c3530786936666b3566665a334532453351665a444363434e3259746245575a742b447263367536726c717637556b304c644b71717235726b32554352584f6b30766d514b47666339346e4f4a79516a6f754639482f774363396745434559664f4e6f41414141415355564f524b35435949493d" alt="Scala Steward badge" data-canonical-src="https://img.shields.io/badge/Scala_Steward-helping-brightgreen.svg?style=flat&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAQCAMAAAARSr4IAAAAVFBMVEUAAACHjojlOy5NWlrKzcYRKjGFjIbp293YycuLa3pYY2LSqql4f3pCUFTgSjNodYRmcXUsPD/NTTbjRS+2jomhgnzNc223cGvZS0HaSD0XLjbaSjElhIr+AAAAAXRSTlMAQObYZgAAAHlJREFUCNdNyosOwyAIhWHAQS1Vt7a77/3fcxxdmv0xwmckutAR1nkm4ggbyEcg/wWmlGLDAA3oL50xi6fk5ffZ3E2E3QfZDCcCN2YtbEWZt+Drc6u6rlqv7Uk0LdKqqr5rk2UCRXOk0vmQKGfc94nOJyQjouF9H/wCc9gECEYfONoAAAAASUVORK5CYII=" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/904d54a9c48e79b7f6c6eae7b6ff0f28ef8793f9/68747470733a2f2f7261772e6769746875622e636f6d2f73706f746966792f7363696f2f6d61737465722f736974652f7372632f70617261646f782f696d616765732f7363696f2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/904d54a9c48e79b7f6c6eae7b6ff0f28ef8793f9/68747470733a2f2f7261772e6769746875622e636f6d2f73706f746966792f7363696f2f6d61737465722f736974652f7372632f70617261646f782f696d616765732f7363696f2e706e67" alt="Scio Logo" width="250" data-canonical-src="https://raw.github.com/spotify/scio/master/site/src/paradox/images/scio.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ecclesiastical Latin IPA: /ˈʃi.o/, [ˈʃiː.o], [ˈʃi.i̯o]
Verb: I can, know, understand, have knowledge.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Scio is a Scala API for &lt;a href="http://beam.incubator.apache.org/" rel="nofollow"&gt;Apache Beam&lt;/a&gt; and &lt;a href="https://github.com/GoogleCloudPlatform/DataflowJavaSDK"&gt;Google Cloud Dataflow&lt;/a&gt; inspired by &lt;a href="http://spark.apache.org/" rel="nofollow"&gt;Apache Spark&lt;/a&gt; and &lt;a href="https://github.com/twitter/scalding"&gt;Scalding&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Scio 0.3.0 and future versions depend on Apache Beam (&lt;code&gt;org.apache.beam&lt;/code&gt;) while earlier versions depend on Google Cloud Dataflow SDK (&lt;code&gt;com.google.cloud.dataflow&lt;/code&gt;). See this &lt;a href="https://spotify.github.io/scio/Apache-Beam.html" rel="nofollow"&gt;page&lt;/a&gt; for a list of breaking changes.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Scala API close to that of Spark and Scalding core APIs&lt;/li&gt;
&lt;li&gt;Unified batch and streaming programming model&lt;/li&gt;
&lt;li&gt;Fully managed service&lt;sup&gt;*&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Integration with Google Cloud products: Cloud Storage, BigQuery, Pub/Sub, Datastore, Bigtable&lt;/li&gt;
&lt;li&gt;JDBC, &lt;a href="http://tensorflow.org/" rel="nofollow"&gt;TensorFlow&lt;/a&gt; TFRecords, Cassandra, Elasticsearch and Parquet I/O&lt;/li&gt;
&lt;li&gt;Interactive mode with Scio REPL&lt;/li&gt;
&lt;li&gt;Type safe BigQuery&lt;/li&gt;
&lt;li&gt;Integration with &lt;a href="https://github.com/twitter/algebird"&gt;Algebird&lt;/a&gt; and &lt;a href="https://github.com/scalanlp/breeze"&gt;Breeze&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pipeline orchestration with &lt;a href="http://docs.scala-lang.org/overviews/core/futures.html" rel="nofollow"&gt;Scala Futures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Distributed cache&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; provided by Google Cloud Dataflow&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h1&gt;
&lt;p&gt;Use our &lt;a href="https://github.com/spotify/scio.g8"&gt;giter8 template&lt;/a&gt; to quickly create a new Scio job repository:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sbt new spotify/scio.g8&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Switch to the new repo (default &lt;code&gt;scio-job&lt;/code&gt;) and build it:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd scio-job
sbt pack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the included word count example:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;target/pack/bin/word-count --output=wc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;List result files and inspect content:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ls -l wc
cat wc/part-00000-of-00001.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://spotify.github.io/scio/Getting-Started.html" rel="nofollow"&gt;Getting Started&lt;/a&gt; is the best place to start with Scio. If you are new to Apache Beam and distributed data processing, check out the &lt;a href="https://beam.apache.org/documentation/programming-guide/" rel="nofollow"&gt;Beam Programming Guide&lt;/a&gt; first for a detailed explanation of the Beam programming model and concepts. If you have experience with other Scala data processing libraries, check out this comparison between &lt;a href="https://spotify.github.io/scio/Scio,-Scalding-and-Spark.html" rel="nofollow"&gt;Scio, Scalding and Spark&lt;/a&gt;. Finally check out this document about the relationship between &lt;a href="https://spotify.github.io/scio/Scio,-Beam-and-Dataflow.html" rel="nofollow"&gt;Scio, Beam and Dataflow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Example Scio pipelines and tests can be found under &lt;a href="https://github.com/spotify/scio/tree/master/scio-examples/src"&gt;scio-examples&lt;/a&gt;. A lot of them are direct ports from Beam's Java &lt;a href="https://github.com/apache/beam/tree/master/examples"&gt;examples&lt;/a&gt;. See this &lt;a href="http://spotify.github.io/scio/examples/" rel="nofollow"&gt;page&lt;/a&gt; for some of them with side-by-side explanation. Also see &lt;a href="https://github.com/spotify/big-data-rosetta-code"&gt;Big Data Rosetta Code&lt;/a&gt; for common data processing code snippets in Scio, Scalding and Spark.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://spotify.github.io/scio/" rel="nofollow"&gt;Scio Wiki&lt;/a&gt; - wiki page&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spotify.github.io/scio/api/" rel="nofollow"&gt;Scio Scaladocs&lt;/a&gt; - current API documentation&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spotify.github.io/scio/examples/" rel="nofollow"&gt;Scio Examples&lt;/a&gt; - examples with side-by-side explanation&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-artifacts" class="anchor" aria-hidden="true" href="#artifacts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Artifacts&lt;/h1&gt;
&lt;p&gt;Scio includes the following artifacts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;scio-core&lt;/code&gt;: core library&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scio-test&lt;/code&gt;: test utilities, add to your project as a "test" dependency&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scio-avro&lt;/code&gt;: add-on for Avro, can also be used standalone&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scio-bigquery&lt;/code&gt;: add-on for BigQuery, can also be used standalone&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scio-bigtable&lt;/code&gt;: add-on for Bigtable&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scio-cassandra*&lt;/code&gt;: add-ons for Cassandra&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scio-elasticsearch*&lt;/code&gt;: add-ons for Elasticsearch&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scio-extra&lt;/code&gt;: extra utilities for working with collections, Breeze, etc.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scio-jdbc&lt;/code&gt;: add-on for JDBC IO&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scio-parquet&lt;/code&gt;: add-on for Parquet&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scio-tensorflow&lt;/code&gt;: add-on for TensorFlow TFRecords IO and prediction&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;Copyright 2016 Spotify AB.&lt;/p&gt;
&lt;p&gt;Licensed under the Apache License, Version 2.0: &lt;a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>spotify</author><guid isPermaLink="false">https://github.com/spotify/scio</guid><pubDate>Sat, 02 Nov 2019 00:24:00 GMT</pubDate></item><item><title>snowplow/snowplow #25 in Scala, Today</title><link>https://github.com/snowplow/snowplow</link><description>&lt;p&gt;&lt;i&gt;Cloud-native web, mobile and event analytics, running on AWS and GCP&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-snowplow" class="anchor" aria-hidden="true" href="#snowplow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Snowplow&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://travis-ci.org/snowplow/snowplow" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c7425b82c4bf0fe5d0c20d5d41db78de0179dee0/68747470733a2f2f7472617669732d63692e6f72672f736e6f77706c6f772f736e6f77706c6f772e706e673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/snowplow/snowplow.png?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/snowplow/snowplow/releases"&gt;&lt;img src="https://camo.githubusercontent.com/dcd06e4c7869d6780ddca154743d431b09c68b70/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72656c656173652d3131365f4d61646172615f52696465722d6f72616e67652e7376673f7374796c653d666c6174" alt="Release" data-canonical-src="https://img.shields.io/badge/release-116_Madara_Rider-orange.svg?style=flat" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0257a158db7f15a3a2b76dfd75be916fda130867/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4170616368652d2d322d626c75652e7376673f7374796c653d666c6174" alt="License" data-canonical-src="http://img.shields.io/badge/license-Apache--2-blue.svg?style=flat" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/snowplow/snowplow?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/37363c9bf8448c0fb1ea1f042ad0c54639643548/68747470733a2f2f6261646765732e6769747465722e696d2f736e6f77706c6f772f736e6f77706c6f772e737667" alt="Join the chat at https://gitter.im/snowplow/snowplow" data-canonical-src="https://badges.gitter.im/snowplow/snowplow.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c29e7e91cefe59d30deb3f6317f090abb733940e/68747470733a2f2f64336936666d7331636d316a30692e636c6f756466726f6e742e6e65742f6769746875622d77696b692f696d616765732f736e6f77706c6f772d6e65772d6c6f676f2d6c617267652e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/c29e7e91cefe59d30deb3f6317f090abb733940e/68747470733a2f2f64336936666d7331636d316a30692e636c6f756466726f6e742e6e65742f6769746875622d77696b692f696d616765732f736e6f77706c6f772d6e65772d6c6f676f2d6c617267652e706e67" alt="Snowplow logo" title="Snowplow" align="right" data-canonical-src="https://d3i6fms1cm1j0i.cloudfront.net/github-wiki/images/snowplow-new-logo-large.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Snowplow is an enterprise-strength marketing and product analytics platform. It does three things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identifies your users, and tracks the way they engage with your website or application&lt;/li&gt;
&lt;li&gt;Stores your users' behavioural data in a scalable "event data warehouse" you control: in Amazon S3 and (optionally) Amazon Redshift or Postgres&lt;/li&gt;
&lt;li&gt;Lets you leverage the biggest range of tools to analyze that data, including big data tools (e.g. Spark) via EMR or more traditional tools e.g. Looker, Mode, Superset, Re:dash to analyze that behavioural data&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;To find out more, please check out the &lt;a href="http://snowplowanalytics.com" rel="nofollow"&gt;Snowplow website&lt;/a&gt; and the &lt;a href="https://github.com/snowplow/snowplow/wiki"&gt;Snowplow wiki&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-snowplow-technology-101" class="anchor" aria-hidden="true" href="#snowplow-technology-101"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Snowplow technology 101&lt;/h2&gt;
&lt;p&gt;The repository structure follows the conceptual architecture of Snowplow, which consists of six loosely-coupled sub-systems connected by five standardized data protocols/formats:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/05914f02874cfc540e98af29bd68bf0d6818f54e/68747470733a2f2f64336936666d7331636d316a30692e636c6f756466726f6e742e6e65742f6769746875622d77696b692f696d616765732f736e6f77706c6f772d6172636869746563747572652e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/05914f02874cfc540e98af29bd68bf0d6818f54e/68747470733a2f2f64336936666d7331636d316a30692e636c6f756466726f6e742e6e65742f6769746875622d77696b692f696d616765732f736e6f77706c6f772d6172636869746563747572652e706e67" alt="architecture" data-canonical-src="https://d3i6fms1cm1j0i.cloudfront.net/github-wiki/images/snowplow-architecture.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To briefly explain these six sub-systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Trackers&lt;/strong&gt; fire Snowplow events. Currently we have 12 trackers, covering web, mobile, desktop, server and IoT&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Collectors&lt;/strong&gt; receive Snowplow events from trackers. Currently we have three different event collectors, sinking events either to Amazon S3, Apache Kafka or Amazon Kinesis&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enrich&lt;/strong&gt; cleans up the raw Snowplow events, enriches them and puts them into storage. Currently we have a Hadoop-based enrichment process, and a Kinesis- or Kafka-based process&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt; is where the Snowplow events live. Currently we store the Snowplow events in a flatfile structure on S3, and in the Redshift and Postgres databases&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data modeling&lt;/strong&gt; is where event-level data is joined with other data sets and aggregated into smaller data sets, and business logic is applied. This produces a clean set of tables which make it easier to perform analysis on the data. We have data models for Redshift and &lt;strong&gt;&lt;a href="http://www.looker.com/" rel="nofollow"&gt;Looker&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Analytics&lt;/strong&gt; are performed on the Snowplow events or on the aggregate tables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;For more information on the current Snowplow architecture, please see the &lt;a href="https://github.com/snowplow/snowplow/wiki/Technical-architecture"&gt;Technical architecture&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quickstart" class="anchor" aria-hidden="true" href="#quickstart"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quickstart&lt;/h2&gt;
&lt;p&gt;Assuming git and &lt;a href="https://www.scala-sbt.org/" rel="nofollow"&gt;SBT&lt;/a&gt; installed:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/snowplow/snowplow.git
$ &lt;span class="pl-c1"&gt;cd&lt;/span&gt; snowplow/3-enrich/scala-common-enrich
$ sbt &lt;span class="pl-c1"&gt;test&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-find-out-more" class="anchor" aria-hidden="true" href="#find-out-more"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Find out more&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;&lt;a href="https://github.com/snowplow/snowplow/wiki/SnowPlow-technical-documentation"&gt;Technical Docs&lt;/a&gt;&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;&lt;a href="https://github.com/snowplow/snowplow/wiki/Setting-up-SnowPlow"&gt;Setup Guide&lt;/a&gt;&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;&lt;a href="https://github.com/snowplow/snowplow/wiki/Product-roadmap"&gt;Roadmap&lt;/a&gt;&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;&lt;a href="./CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/snowplow/snowplow/wiki/SnowPlow-technical-documentation"&gt;&lt;img src="https://camo.githubusercontent.com/8c6159b25596bb5a534a95784f8c7bd38fefe95a/68747470733a2f2f64336936666d7331636d316a30692e636c6f756466726f6e742e6e65742f6769746875622f696d616765732f74656368646f63732e706e67" alt="i1" data-canonical-src="https://d3i6fms1cm1j0i.cloudfront.net/github/images/techdocs.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/snowplow/snowplow/wiki/Setting-up-SnowPlow"&gt;&lt;img src="https://camo.githubusercontent.com/0390397513935043e56a7829a27810eee31458d9/68747470733a2f2f64336936666d7331636d316a30692e636c6f756466726f6e742e6e65742f6769746875622f696d616765732f73657475702e706e67" alt="i2" data-canonical-src="https://d3i6fms1cm1j0i.cloudfront.net/github/images/setup.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/snowplow/snowplow/wiki/Product-roadmap"&gt;&lt;img src="https://camo.githubusercontent.com/80cb14d5c90978bd9ac999eaaeedb559a7621eb7/68747470733a2f2f64336936666d7331636d316a30692e636c6f756466726f6e742e6e65742f6769746875622f696d616765732f726f61646d61702e706e67" alt="i3" data-canonical-src="https://d3i6fms1cm1j0i.cloudfront.net/github/images/roadmap.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="./CONTRIBUTING.md"&gt;&lt;img src="https://camo.githubusercontent.com/b0be97a372fd4a9c0b0478066e70526d869da250/68747470733a2f2f64336936666d7331636d316a30692e636c6f756466726f6e742e6e65742f6769746875622f696d616765732f636f6e747269627574696e672e706e67" alt="i4" data-canonical-src="https://d3i6fms1cm1j0i.cloudfront.net/github/images/contributing.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We're committed to a loosely-coupled architecture for Snowplow and would love to get your contributions within each of the six sub-systems.&lt;/p&gt;
&lt;p&gt;If you would like help implementing a new tracker, adding an additional enrichment or loading Snowplow events into an alternative database, check out our &lt;strong&gt;&lt;a href="./CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/strong&gt; page on the wiki!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-questions-or-need-help" class="anchor" aria-hidden="true" href="#questions-or-need-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Questions or need help?&lt;/h2&gt;
&lt;p&gt;Check out the &lt;strong&gt;&lt;a href="https://github.com/snowplow/snowplow/wiki/Talk-to-us"&gt;Talk to us&lt;/a&gt;&lt;/strong&gt; page on our wiki.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-copyright-and-license" class="anchor" aria-hidden="true" href="#copyright-and-license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Copyright and license&lt;/h2&gt;
&lt;p&gt;Snowplow is copyright 2012-2019 Snowplow Analytics Ltd.&lt;/p&gt;
&lt;p&gt;Licensed under the &lt;strong&gt;&lt;a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;Apache License, Version 2.0&lt;/a&gt;&lt;/strong&gt; (the "License");
you may not use this software except in compliance with the License.&lt;/p&gt;
&lt;p&gt;Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>snowplow</author><guid isPermaLink="false">https://github.com/snowplow/snowplow</guid><pubDate>Sat, 02 Nov 2019 00:25:00 GMT</pubDate></item></channel></rss>