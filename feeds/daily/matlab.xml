<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: MATLAB, Today</title><link>https://github.com/trending/matlab?since=daily</link><description>The top repositories on GitHub for matlab, measured daily</description><pubDate>Sun, 12 Jan 2020 01:10:17 GMT</pubDate><lastBuildDate>Sun, 12 Jan 2020 01:10:17 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>BIMK/PlatEMO #1 in MATLAB, Today</title><link>https://github.com/BIMK/PlatEMO</link><description>&lt;p&gt;&lt;i&gt;Evolutionary multi-objective optimization platform&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./Doc/logo.png"&gt;&lt;img src="./Doc/logo.png" width="256" height="256" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;  
&lt;h1&gt;&lt;a id="user-content-platemo" class="anchor" aria-hidden="true" href="#platemo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PlatEMO&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/BIMK/PlatEMO/archive/master.zip"&gt;&lt;img src="https://camo.githubusercontent.com/60b2e104217818ea0706e204aaeb52889e4277e1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f776e6c6f61642d4c61746573742d79656c6c6f772e737667" alt="" data-canonical-src="https://img.shields.io/badge/Download-Latest-yellow.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/BIMK/PlatEMO/releases/"&gt;&lt;img src="https://camo.githubusercontent.com/a937e90f46f8f309d041125cc9e76dd9f855a60d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f42494d4b2f506c6174454d4f2e737667" alt="" data-canonical-src="https://img.shields.io/github/release/BIMK/PlatEMO.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="#PlatEMO"&gt;&lt;img src="https://camo.githubusercontent.com/29182b460d257897b808b436a175dd3ac5e09b2e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d61746c61622d25334525334425323032303134612532302d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/Matlab-%3E%3D%202014a%20-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;
&lt;a href="#PlatEMO"&gt;&lt;img src="https://camo.githubusercontent.com/c2089b82da22e530e36e7f825c55464b3cc9b18e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f57696e646f77732d506173732d627269676874677265656e2e737667" alt="" data-canonical-src="https://img.shields.io/badge/Windows-Pass-brightgreen.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="#PlatEMO"&gt;&lt;img src="https://camo.githubusercontent.com/1638e630880206a9e39ffca48c5208c4933cc8b9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c696e75782d506173732d627269676874677265656e2e737667" alt="" data-canonical-src="https://img.shields.io/badge/Linux-Pass-brightgreen.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="#PlatEMO"&gt;&lt;img src="https://camo.githubusercontent.com/2e16b98fc4082773919cced9fd255ed247d80015/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d61634f532d56616c69646174696e672d7265642e737667" alt="" data-canonical-src="https://img.shields.io/badge/MacOS-Validating-red.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;
Evolutionary multi-objective optimization platform&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;100+ open source evolutionary algorithms&lt;/li&gt;
&lt;li&gt;200+ open source multi-objective test problems&lt;/li&gt;
&lt;li&gt;Powerful GUI for performing experiments in parallel&lt;/li&gt;
&lt;li&gt;Generating results in the format of Excel or LaTeX table by one-click operation&lt;/li&gt;
&lt;li&gt;State-of-the-art algorithms will be included continuously&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you very much for using PlatEMO. The copyright of PlatEMO belongs to the BIMK Group. This
tool is mainly for research and educational purposes. The codes were implemented based on our
understanding of the algorithms published in the papers. You should not rely upon the material or
information on the website as a basis for making any business, legal or any other decisions. We
assume no responsibilities for any consequences of your using any algorithms in the tool. All
publications using the platform should acknowledge the use of “PlatEMO” and reference the
following literature:&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-copyright" class="anchor" aria-hidden="true" href="#copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Copyright&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The Copyright of the PlatEMO belongs to the BIMK group. You are free to &lt;a href="https://github.com/BIMK/PlatEMO/releases"&gt;use the PlatEMO&lt;/a&gt; for &lt;strong&gt;research purposes&lt;/strong&gt;. All publications which use this platform or any code in the platform should &lt;strong&gt;acknowledge the use of "PlatEMO" and reference&lt;/strong&gt; &lt;em&gt;"Ye Tian, Ran Cheng, Xingyi Zhang, and Yaochu Jin, PlatEMO: A MATLAB Platform for Evolutionary Multi-Objective Optimization [Educational Forum], IEEE Computational Intelligence Magazine, 2017, 12(4): 73-87".&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;@article{PlatEMO,
  title={{PlatEMO}: A {MATLAB} platform for evolutionary multi-objective optimization},
  author={Tian, Ye and Cheng, Ran and Zhang, Xingyi and Jin, Yaochu},
  journal={IEEE Computational Intelligence Magazine},
  volume={12},
  number={4},
  pages={73--87},
  year={2017},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-release-highlights-of-platemo-24" class="anchor" aria-hidden="true" href="#release-highlights-of-platemo-24"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Release Highlights of PlatEMO 2.4&lt;/h1&gt;
&lt;p&gt;&lt;a href="./Doc/releasenote.md"&gt;Release Note can be found here&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add two algorithms: MSEA and OSP-NSDE. There are currently 110 algorithms in the platform.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-features-of-platemo" class="anchor" aria-hidden="true" href="#features-of-platemo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features of PlatEMO&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Totally Developed in MATLAB&lt;br&gt;
PlatEMO consists of a number of MATLAB functions without using any other libraries. Any machines able to run MATLAB can use PlatEMO regardless of the operating system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Includes Many Popular Algorithms&lt;br&gt;
PlatEMO includes more than ninety existing popular MOEAs, including genetic algorithm, differential evolution, particle swarm optimization, memetic algorithm, estimation of distribution algorithm, and surrogate model based algorithm. Most of them are representative algorithms published in top journals after 2010.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Various Figure Demonstrations&lt;br&gt;
Users can select various figures to be displayed, including the Pareto front of the result, the Pareto set of the result, the true Pareto front, and the evolutionary trajectories of any performance indicator values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Powerful and Friendly GUI&lt;br&gt;
PlatEMO provides a powerful and friendly GUI, where users can configure all the settings and perform experiments in parallel via the GUI without writing any code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generates Data in the Format of Excel or LaTeX&lt;br&gt;
Users can save the statistical experimental results generated by PlatEMO as an Excel table or LaTeX table, which can be directly used in academic writings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;strong&gt;recommend&lt;/strong&gt;] You can ask any question in &lt;a href="https://github.com/BIMK/PlatEMO/issues"&gt;issues block&lt;/a&gt; and upload your contribution by pulling request(PR).&lt;/li&gt;
&lt;li&gt;If you want to add your MOEA, MOP, operator or performance indicator to PlatEMO, please send the MATLAB code (able to be used in PlatEMO) and the relevant literature to &lt;a href="mailto:field910921@gmail.com"&gt;field910921@gmail.com&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If you have any question, comment or suggestion to PlatEMO or the algorithms in PlatEMO, please contact Ye Tian (&lt;a href="mailto:field910921@gmail.com"&gt;field910921@gmail.com&lt;/a&gt;) or join the group of QQ(Group number: 100065008).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./Doc/QQgroupNumber.jpg"&gt;&lt;img src="./Doc/QQgroupNumber.jpg" width="180" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;  
&lt;h1&gt;&lt;a id="user-content-acknowledge" class="anchor" aria-hidden="true" href="#acknowledge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledge&lt;/h1&gt;
&lt;p&gt;This repo belongs to BIMK group and has been transferred project from &lt;a href="http://bimk.ahu.edu.cn/" rel="nofollow"&gt;BIMK&lt;/a&gt; to github by Ye Tian and Shichen Peng&lt;a href="https://github.com/anonymone"&gt;@anonymone&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>BIMK</author><guid isPermaLink="false">https://github.com/BIMK/PlatEMO</guid><pubDate>Sun, 12 Jan 2020 00:01:00 GMT</pubDate></item><item><title>rasmusbergpalm/DeepLearnToolbox #2 in MATLAB, Today</title><link>https://github.com/rasmusbergpalm/DeepLearnToolbox</link><description>&lt;p&gt;&lt;i&gt;Matlab/Octave toolbox for deep learning. Includes Deep Belief Nets, Stacked Autoencoders, Convolutional Neural Nets, Convolutional Autoencoders and vanilla Neural Nets. Each method has examples to get you started.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-deprecation-notice" class="anchor" aria-hidden="true" href="#deprecation-notice"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deprecation notice.&lt;/h2&gt;
&lt;p&gt;This toolbox is outdated and no longer maintained.&lt;/p&gt;
&lt;p&gt;There are much better tools available for deep learning than this toolbox, e.g. &lt;a href="http://deeplearning.net/software/theano/" rel="nofollow"&gt;Theano&lt;/a&gt;, &lt;a href="http://torch.ch/" rel="nofollow"&gt;torch&lt;/a&gt; or &lt;a href="http://www.tensorflow.org/" rel="nofollow"&gt;tensorflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I would suggest you use one of the tools mentioned above rather than use this toolbox.&lt;/p&gt;
&lt;p&gt;Best, Rasmus.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-deeplearntoolbox" class="anchor" aria-hidden="true" href="#deeplearntoolbox"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepLearnToolbox&lt;/h1&gt;
&lt;p&gt;A Matlab toolbox for Deep Learning.&lt;/p&gt;
&lt;p&gt;Deep Learning is a new subfield of machine learning that focuses on learning deep hierarchical models of data.
It is inspired by the human brain's apparent deep (layered, hierarchical) architecture.
A good overview of the theory of Deep Learning theory is
&lt;a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf" rel="nofollow"&gt;Learning Deep Architectures for AI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For a more informal introduction, see the following videos by Geoffrey Hinton and Andrew Ng.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.youtube.com/watch?v=AyzOUbkUf3M" rel="nofollow"&gt;The Next Generation of Neural Networks&lt;/a&gt; (Hinton, 2007)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.youtube.com/watch?v=VdIURAu1-aU" rel="nofollow"&gt;Recent Developments in Deep Learning&lt;/a&gt; (Hinton, 2010)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.youtube.com/watch?v=ZmNOAtZIgIk" rel="nofollow"&gt;Unsupervised Feature Learning and Deep Learning&lt;/a&gt; (Ng, 2011)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you use this toolbox in your research please cite &lt;a href="http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6284" rel="nofollow"&gt;Prediction as a candidate for learning deep hierarchical models of data&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@MASTERSTHESIS\{IMM2012-06284,
    author       = "R. B. Palm",
    title        = "Prediction as a candidate for learning deep hierarchical models of data",
    year         = "2012",
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Contact: rasmusbergpalm at gmail dot com&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-directories-included-in-the-toolbox" class="anchor" aria-hidden="true" href="#directories-included-in-the-toolbox"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Directories included in the toolbox&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;NN/&lt;/code&gt;   - A library for Feedforward Backpropagation Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;code&gt;CNN/&lt;/code&gt;  - A library for Convolutional Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;code&gt;DBN/&lt;/code&gt;  - A library for Deep Belief Networks&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SAE/&lt;/code&gt;  - A library for Stacked Auto-Encoders&lt;/p&gt;
&lt;p&gt;&lt;code&gt;CAE/&lt;/code&gt; - A library for Convolutional Auto-Encoders&lt;/p&gt;
&lt;p&gt;&lt;code&gt;util/&lt;/code&gt; - Utility functions used by the libraries&lt;/p&gt;
&lt;p&gt;&lt;code&gt;data/&lt;/code&gt; - Data used by the examples&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tests/&lt;/code&gt; - unit tests to verify toolbox is working&lt;/p&gt;
&lt;p&gt;For references on each library check REFS.md&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download.&lt;/li&gt;
&lt;li&gt;addpath(genpath('DeepLearnToolbox'));&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-example-deep-belief-network" class="anchor" aria-hidden="true" href="#example-deep-belief-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example: Deep Belief Network&lt;/h2&gt;
&lt;div class="highlight highlight-source-matlab"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;function&lt;/span&gt; &lt;span class="pl-en"&gt;test_example_DBN&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-smi"&gt;load&lt;/span&gt; mnist_uint8;&lt;/span&gt;

train_x = double(train_x) &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;255&lt;/span&gt;;
test_x  = double(test_x)  &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;255&lt;/span&gt;;
train_y = double(train_y);
test_y  = double(test_y);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt;  ex1 train a 100 hidden unit RBM and visualize its weights&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
dbn.sizes = [&lt;span class="pl-c1"&gt;100&lt;/span&gt;];
opts.numepochs =   &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;
opts.momentum  =   &lt;span class="pl-c1"&gt;0&lt;/span&gt;;
opts.alpha     =   &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
dbn = dbnsetup(dbn, train_x, opts);
dbn = dbntrain(dbn, train_x, opts);
figure; visualize(dbn.rbm{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}.&lt;span class="pl-k"&gt;W'&lt;/span&gt;);   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Visualize the RBM weights&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt;  ex2 train a 100-100 hidden unit DBN and use its weights to initialize a NN&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;train dbn&lt;/span&gt;
dbn.sizes = [&lt;span class="pl-c1"&gt;100&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt;];
opts.numepochs =   &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;
opts.momentum  =   &lt;span class="pl-c1"&gt;0&lt;/span&gt;;
opts.alpha     =   &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
dbn = dbnsetup(dbn, train_x, opts);
dbn = dbntrain(dbn, train_x, opts);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;unfold dbn to nn&lt;/span&gt;
nn = dbnunfoldtonn(dbn, &lt;span class="pl-c1"&gt;10&lt;/span&gt;);
nn.activation_function = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;sigm&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;train nn&lt;/span&gt;
opts.numepochs =  &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;
nn = nntrain(nn, train_x, train_y, opts);
[er, bad] = nntest(nn, test_x, test_y);

assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.10&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-example-stacked-auto-encoders" class="anchor" aria-hidden="true" href="#example-stacked-auto-encoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example: Stacked Auto-Encoders&lt;/h2&gt;
&lt;div class="highlight highlight-source-matlab"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;function&lt;/span&gt; &lt;span class="pl-en"&gt;test_example_SAE&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-smi"&gt;load&lt;/span&gt; mnist_uint8;&lt;/span&gt;

train_x = double(train_x)/&lt;span class="pl-c1"&gt;255&lt;/span&gt;;
test_x  = double(test_x)/&lt;span class="pl-c1"&gt;255&lt;/span&gt;;
train_y = double(train_y);
test_y  = double(test_y);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt;  ex1 train a 100 hidden unit SDAE and use it to initialize a FFNN&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Setup and train a stacked denoising autoencoder (SDAE)&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
sae = saesetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt;]);
sae.ae{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}.activation_function       = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;sigm&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;
sae.ae{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}.learningRate              = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
sae.ae{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}.inputZeroMaskedFraction   = &lt;span class="pl-c1"&gt;0.5&lt;/span&gt;;
opts.numepochs =   &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;
sae = saetrain(sae, train_x, opts);
visualize(sae.ae{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}.W{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}(:,&lt;span class="pl-c1"&gt;2&lt;/span&gt;:&lt;span class="pl-k"&gt;end&lt;/span&gt;)&lt;span class="pl-k"&gt;'&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt; Use the SDAE to initialize a FFNN&lt;/span&gt;
nn = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);
nn.activation_function              = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;sigm&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;
nn.learningRate                     = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
nn.W{&lt;span class="pl-c1"&gt;1&lt;/span&gt;} = sae.ae{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}.W{&lt;span class="pl-c1"&gt;1&lt;/span&gt;};

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt; Train the FFNN&lt;/span&gt;
opts.numepochs =   &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;
nn = nntrain(nn, train_x, train_y, opts);
[er, bad] = nntest(nn, test_x, test_y);
assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.16&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-example-convolutional-neural-nets" class="anchor" aria-hidden="true" href="#example-convolutional-neural-nets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example: Convolutional Neural Nets&lt;/h2&gt;
&lt;div class="highlight highlight-source-matlab"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;function&lt;/span&gt; &lt;span class="pl-en"&gt;test_example_CNN&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-smi"&gt;load&lt;/span&gt; mnist_uint8;&lt;/span&gt;

train_x = double(reshape(&lt;span class="pl-k"&gt;train_x'&lt;/span&gt;,&lt;span class="pl-c1"&gt;28&lt;/span&gt;,&lt;span class="pl-c1"&gt;28&lt;/span&gt;,&lt;span class="pl-c1"&gt;60000&lt;/span&gt;))/&lt;span class="pl-c1"&gt;255&lt;/span&gt;;
test_x = double(reshape(&lt;span class="pl-k"&gt;test_x'&lt;/span&gt;,&lt;span class="pl-c1"&gt;28&lt;/span&gt;,&lt;span class="pl-c1"&gt;28&lt;/span&gt;,&lt;span class="pl-c1"&gt;10000&lt;/span&gt;))/&lt;span class="pl-c1"&gt;255&lt;/span&gt;;
train_y = double(&lt;span class="pl-k"&gt;train_y'&lt;/span&gt;);
test_y = double(&lt;span class="pl-k"&gt;test_y'&lt;/span&gt;);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex1 Train a 6c-2s-12c-2s Convolutional neural network &lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;will run 1 epoch in about 200 second and get around 11% error. &lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;With 100 epochs you'll get around 1.2% error&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
cnn.layers = {
    struct(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;type&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;i&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;input layer&lt;/span&gt;
    struct(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;type&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;c&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;outputmaps&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;kernelsize&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;convolution layer&lt;/span&gt;
    struct(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;type&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;scale&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;sub sampling layer&lt;/span&gt;
    struct(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;type&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;c&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;outputmaps&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;12&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;kernelsize&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;convolution layer&lt;/span&gt;
    struct(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;type&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;scale&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;subsampling layer&lt;/span&gt;
};
cnn = cnnsetup(cnn, train_x, train_y);

opts.alpha = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
opts.batchsize = &lt;span class="pl-c1"&gt;50&lt;/span&gt;;
opts.numepochs = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;

cnn = cnntrain(cnn, train_x, train_y, opts);

[er, bad] = cnntest(cnn, test_x, test_y);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;plot mean squared error&lt;/span&gt;
figure; plot(cnn.rL);

assert(er&amp;lt;0.12, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-example-neural-networks" class="anchor" aria-hidden="true" href="#example-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example: Neural Networks&lt;/h2&gt;
&lt;div class="highlight highlight-source-matlab"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;function&lt;/span&gt; &lt;span class="pl-en"&gt;test_example_NN&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-smi"&gt;load&lt;/span&gt; mnist_uint8;&lt;/span&gt;

train_x = double(train_x) &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;255&lt;/span&gt;;
test_x  = double(test_x)  &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;255&lt;/span&gt;;
train_y = double(train_y);
test_y  = double(test_y);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt; normalize&lt;/span&gt;
[train_x, mu, sigma] = zscore(train_x);
test_x = normalize(test_x, mu, sigma);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex1 vanilla neural net&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
nn = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);
opts.numepochs =  &lt;span class="pl-c1"&gt;1&lt;/span&gt;;   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Number of full sweeps through data&lt;/span&gt;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Take a mean gradient step over this many samples&lt;/span&gt;
[nn, L] = nntrain(nn, train_x, train_y, opts);

[er, bad] = nntest(nn, test_x, test_y);

assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.08&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex2 neural net with L2 weight decay&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
nn = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);

nn.weightPenaltyL2 = &lt;span class="pl-c1"&gt;1e-4&lt;/span&gt;;  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  L2 weight decay&lt;/span&gt;
opts.numepochs =  &lt;span class="pl-c1"&gt;1&lt;/span&gt;;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Number of full sweeps through data&lt;/span&gt;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;       &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Take a mean gradient step over this many samples&lt;/span&gt;

nn = nntrain(nn, train_x, train_y, opts);

[er, bad] = nntest(nn, test_x, test_y);
assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.1&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);


&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex3 neural net with dropout&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
nn = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);

nn.dropoutFraction = &lt;span class="pl-c1"&gt;0.5&lt;/span&gt;;   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Dropout fraction &lt;/span&gt;
opts.numepochs =  &lt;span class="pl-c1"&gt;1&lt;/span&gt;;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Number of full sweeps through data&lt;/span&gt;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;       &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Take a mean gradient step over this many samples&lt;/span&gt;

nn = nntrain(nn, train_x, train_y, opts);

[er, bad] = nntest(nn, test_x, test_y);
assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.1&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex4 neural net with sigmoid activation function&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
nn = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);

nn.activation_function = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;sigm&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Sigmoid activation function&lt;/span&gt;
nn.learningRate = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;                &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Sigm require a lower learning rate&lt;/span&gt;
opts.numepochs =  &lt;span class="pl-c1"&gt;1&lt;/span&gt;;                &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Number of full sweeps through data&lt;/span&gt;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;               &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Take a mean gradient step over this many samples&lt;/span&gt;

nn = nntrain(nn, train_x, train_y, opts);

[er, bad] = nntest(nn, test_x, test_y);
assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.1&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex5 plotting functionality&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
nn = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;20&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);
opts.numepochs         = &lt;span class="pl-c1"&gt;5&lt;/span&gt;;            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Number of full sweeps through data&lt;/span&gt;
nn.output              = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;softmax&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  use softmax output&lt;/span&gt;
opts.batchsize         = &lt;span class="pl-c1"&gt;1000&lt;/span&gt;;         &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Take a mean gradient step over this many samples&lt;/span&gt;
opts.plot              = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  enable plotting&lt;/span&gt;

nn = nntrain(nn, train_x, train_y, opts);

[er, bad] = nntest(nn, test_x, test_y);
assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.1&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex6 neural net with sigmoid activation and plotting of validation and training error&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt; split training data into training and validation data&lt;/span&gt;
vx   = train_x(&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;10000&lt;/span&gt;,:);
tx = train_x(&lt;span class="pl-c1"&gt;10001&lt;/span&gt;:&lt;span class="pl-k"&gt;end&lt;/span&gt;,:);
vy   = train_y(&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;10000&lt;/span&gt;,:);
ty = train_y(&lt;span class="pl-c1"&gt;10001&lt;/span&gt;:&lt;span class="pl-k"&gt;end&lt;/span&gt;,:);

rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
nn                      = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;20&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);     
nn.output               = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;softmax&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  use softmax output&lt;/span&gt;
opts.numepochs          = &lt;span class="pl-c1"&gt;5&lt;/span&gt;;                           &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Number of full sweeps through data&lt;/span&gt;
opts.batchsize          = &lt;span class="pl-c1"&gt;1000&lt;/span&gt;;                        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Take a mean gradient step over this many samples&lt;/span&gt;
opts.plot               = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;                           &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  enable plotting&lt;/span&gt;
nn = nntrain(nn, tx, ty, opts, vx, vy);                &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  nntrain takes validation set as last two arguments (optionally)&lt;/span&gt;

[er, bad] = nntest(nn, test_x, test_y);
assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.1&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="https://bitdeli.com/free" title="Bitdeli Badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/00e5bbbe78ce5418547b4672181b673cccc1c89c/68747470733a2f2f64327765637a68766c38323376302e636c6f756466726f6e742e6e65742f7261736d75736265726770616c6d2f646565706c6561726e746f6f6c626f782f7472656e642e706e67" alt="Bitdeli Badge" data-canonical-src="https://d2weczhvl823v0.cloudfront.net/rasmusbergpalm/deeplearntoolbox/trend.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rasmusbergpalm</author><guid isPermaLink="false">https://github.com/rasmusbergpalm/DeepLearnToolbox</guid><pubDate>Sun, 12 Jan 2020 00:02:00 GMT</pubDate></item><item><title>tjssmy/4700Code #3 in MATLAB, Today</title><link>https://github.com/tjssmy/4700Code</link><description>&lt;p&gt;&lt;i&gt;Code for 4700 examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-elec-4700-matlab-demonstrations-test-of-windows2" class="anchor" aria-hidden="true" href="#elec-4700-matlab-demonstrations-test-of-windows2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ELEC 4700 MATLAB Demonstrations (test of windows2)&lt;/h1&gt;
&lt;p&gt;The following repo includes a number of demonstrations for &lt;a href="http://www.doe.carleton.ca/~tjs/4700.html" rel="nofollow"&gt;ELEC 4700&lt;/a&gt;
and &lt;a href="http://www.doe.carleton.ca/~tjs/475.html" rel="nofollow"&gt;ELEC 4705&lt;/a&gt; written in MATLAB.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;Install MATLAB (or use a Carleton lab computer with MATLAB preinstalled). This
is the only prerequisite software needed to run the demonstration code in this
repo.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;MATLAB files are plain-text &lt;code&gt;.m&lt;/code&gt; files and should be opened directly in MATLAB
if you wish to run them. If you would just like to edit them, any plain text
editor will suffice.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;If you wish to contribute to this code, please send a pull request.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;MIT.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tjssmy</author><guid isPermaLink="false">https://github.com/tjssmy/4700Code</guid><pubDate>Sun, 12 Jan 2020 00:03:00 GMT</pubDate></item><item><title>atinesh-s/Coursera-Machine-Learning-Stanford #4 in MATLAB, Today</title><link>https://github.com/atinesh-s/Coursera-Machine-Learning-Stanford</link><description>&lt;p&gt;&lt;i&gt;Machine learning-Stanford University&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-coursera" class="anchor" aria-hidden="true" href="#machine-learning-coursera"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning (Coursera)&lt;/h1&gt;
&lt;p&gt;This is my solution to all the programming assignments and quizzes of Machine-Learning (Coursera) taught by Andrew Ng. After completing this course you will get a broad idea of Machine learning algorithms. Try to solve all the assignments by yourself first, but if you get stuck somewhere then feel free to browse the code.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Lectures Slides&lt;/li&gt;
&lt;li&gt;Solution to programming assignment&lt;/li&gt;
&lt;li&gt;Solution to Quizzes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-certificate" class="anchor" aria-hidden="true" href="#certificate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Certificate&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/account/accomplishments/certificate/GDDBFB572MUQ" rel="nofollow"&gt;Verified Certificate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.coursera.org/learn/machine-learning" rel="nofollow"&gt;[1] Machine Learning - Stanford University&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>atinesh-s</author><guid isPermaLink="false">https://github.com/atinesh-s/Coursera-Machine-Learning-Stanford</guid><pubDate>Sun, 12 Jan 2020 00:04:00 GMT</pubDate></item><item><title>jlblancoc/2020-ual-factor-graphs-course #5 in MATLAB, Today</title><link>https://github.com/jlblancoc/2020-ual-factor-graphs-course</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-2020-ual-factor-graphs-course" class="anchor" aria-hidden="true" href="#2020-ual-factor-graphs-course"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2020-ual-factor-graphs-course&lt;/h1&gt;
&lt;p&gt;Practice material for a short set of lectures on Factor Graphs taught at University of Almeria (Jan 2020).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slides: &lt;a href="http://ingmec.ual.es/~jlblanco/papers/2020-introduction-factor-graphs_JLBlanco.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Download binaries for MATLAB 64bit as &lt;a href="https://github.com/jlblancoc/2020-ual-factor-graphs-course/archive/master.zip"&gt;ZIP&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Lectures were recorded and will be made available online in &lt;a href="https://www.youtube.com/playlist?list=PLOJ3GF0x2_eWtGXfZ5Ne1Jul5L-6Q76Sz" rel="nofollow"&gt;this YouTube playlist&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/6887feb0136db5156c4f4146e3dd2681d06d9c75/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d6e632d73612f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;This work is licensed under a &lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jlblancoc</author><guid isPermaLink="false">https://github.com/jlblancoc/2020-ual-factor-graphs-course</guid><pubDate>Sun, 12 Jan 2020 00:05:00 GMT</pubDate></item><item><title>hhping/LDPC_en-decoder #6 in MATLAB, Today</title><link>https://github.com/hhping/LDPC_en-decoder</link><description>&lt;p&gt;&lt;i&gt;LDPC编码解码matlab代码和Verilog代码及资料&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;This repo does not have a README.&lt;/i&gt;&lt;/p&gt;</description><author>hhping</author><guid isPermaLink="false">https://github.com/hhping/LDPC_en-decoder</guid><pubDate>Sun, 12 Jan 2020 00:06:00 GMT</pubDate></item></channel></rss>