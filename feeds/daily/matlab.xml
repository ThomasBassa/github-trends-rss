<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: MATLAB, Today</title><link>https://github.com/trending/matlab?since=daily</link><description>The top repositories on GitHub for matlab, measured daily</description><pubDate>Thu, 26 Dec 2019 01:12:41 GMT</pubDate><lastBuildDate>Thu, 26 Dec 2019 01:12:41 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>AvaisP/machine-learning-programming-assignments-coursera-andrew-ng #1 in MATLAB, Today</title><link>https://github.com/AvaisP/machine-learning-programming-assignments-coursera-andrew-ng</link><description>&lt;p&gt;&lt;i&gt;Solutions to Andrew NG's machine learning course on Coursera&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-programming-assignments-coursera-andrew-ng" class="anchor" aria-hidden="true" href="#machine-learning-programming-assignments-coursera-andrew-ng"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;machine-learning-programming-assignments-coursera-andrew-ng&lt;/h1&gt;
&lt;p&gt;Solutions to Andrew NG's machine learning course on Coursera&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>AvaisP</author><guid isPermaLink="false">https://github.com/AvaisP/machine-learning-programming-assignments-coursera-andrew-ng</guid><pubDate>Thu, 26 Dec 2019 00:01:00 GMT</pubDate></item><item><title>Borye/machine-learning-coursera-1 #2 in MATLAB, Today</title><link>https://github.com/Borye/machine-learning-coursera-1</link><description>&lt;p&gt;&lt;i&gt;This repo is specially created for all the work done my me as a part of Coursera's Machine Learning Course.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-coursera" class="anchor" aria-hidden="true" href="#machine-learning-coursera"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;machine-learning-coursera&lt;/h1&gt;
&lt;p&gt;This repo is specially created for all the work done my me as a part of Coursera's Machine Learning Course.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Borye</author><guid isPermaLink="false">https://github.com/Borye/machine-learning-coursera-1</guid><pubDate>Thu, 26 Dec 2019 00:02:00 GMT</pubDate></item><item><title>rbgirshick/rcnn #3 in MATLAB, Today</title><link>https://github.com/rbgirshick/rcnn</link><description>&lt;p&gt;&lt;i&gt;R-CNN: Regions with Convolutional Neural Network Features&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h3&gt;&lt;a id="user-content-this-code-base-is-no-longer-maintained-and-exists-as-a-historical-artifact-to-supplement-our-cvpr-and-pami-papers-on-region-based-convolutional-neural-netwoks-for-more-recent-work-thats-faster-and-more-accurrate-please-see-fast-and-faster-r-cnn" class="anchor" aria-hidden="true" href="#this-code-base-is-no-longer-maintained-and-exists-as-a-historical-artifact-to-supplement-our-cvpr-and-pami-papers-on-region-based-convolutional-neural-netwoks-for-more-recent-work-thats-faster-and-more-accurrate-please-see-fast-and-faster-r-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;This code base is no longer maintained and exists as a historical artifact to supplement our CVPR and PAMI papers on Region-based Convolutional Neural Netwoks. For more recent work that's faster and more accurrate, please see &lt;a href="https://github.com/rbgirshick/py-faster-rcnn"&gt;Fast and Faster R-CNN&lt;/a&gt;.&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-r-cnn-region-based-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#r-cnn-region-based-convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;R-CNN: &lt;em&gt;Region-based Convolutional Neural Networks&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Created by Ross Girshick, Jeff Donahue, Trevor Darrell and Jitendra Malik at UC Berkeley EECS.&lt;/p&gt;
&lt;p&gt;Acknowledgements: a huge thanks to Yangqing Jia for creating Caffe and the BVLC team, with a special shoutout to Evan Shelhamer, for maintaining Caffe and helping to merge the R-CNN fine-tuning code into Caffe.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h3&gt;
&lt;p&gt;R-CNN is a state-of-the-art visual object detection system that combines bottom-up region proposals with rich features computed by a convolutional neural network. At the time of its release, R-CNN improved the previous best detection performance on PASCAL VOC 2012 by 30% relative, going from 40.9% to 53.3% mean average precision. Unlike the previous best results, R-CNN achieves this performance without using contextual rescoring or an ensemble of feature types.&lt;/p&gt;
&lt;p&gt;R-CNN was initially described in an &lt;a href="http://arxiv.org/abs/1311.2524" rel="nofollow"&gt;arXiv tech report&lt;/a&gt; and will appear in a forthcoming CVPR 2014 paper.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-citing-r-cnn" class="anchor" aria-hidden="true" href="#citing-r-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing R-CNN&lt;/h3&gt;
&lt;p&gt;If you find R-CNN useful in your research, please consider citing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{girshick14CVPR,
    Author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
    Title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
    Booktitle = {Computer Vision and Pattern Recognition},
    Year = {2014}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;R-CNN is released under the Simplified BSD License (refer to the
LICENSE file for details).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pascal-voc-detection-results" class="anchor" aria-hidden="true" href="#pascal-voc-detection-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PASCAL VOC detection results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th align="center"&gt;VOC 2007 mAP&lt;/th&gt;
&lt;th align="center"&gt;VOC 2010 mAP&lt;/th&gt;
&lt;th align="center"&gt;VOC 2012 mAP&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;R-CNN&lt;/td&gt;
&lt;td align="center"&gt;54.2%&lt;/td&gt;
&lt;td align="center"&gt;50.2%&lt;/td&gt;
&lt;td align="center"&gt;49.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;R-CNN bbox reg&lt;/td&gt;
&lt;td align="center"&gt;58.5%&lt;/td&gt;
&lt;td align="center"&gt;53.7%&lt;/td&gt;
&lt;td align="center"&gt;53.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;VOC 2007 per-class results are available in our &lt;a href="http://www.rossgirshick.info/#girshick2014rcnn" rel="nofollow"&gt;CVPR14 paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VOC 2010 per-class results are available on the &lt;a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb_dt.php?challengeid=6&amp;amp;compid=4" rel="nofollow"&gt;VOC 2010 leaderboard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VOC 2012 per-class results are available on the &lt;a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb_dt.php?challengeid=11&amp;amp;compid=4" rel="nofollow"&gt;VOC 2012 leaderboard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;These models are available in the model package (see below)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-imagenet-200-class-detection-results" class="anchor" aria-hidden="true" href="#imagenet-200-class-detection-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ImageNet 200-class detection results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th align="center"&gt;ILSVRC2013 test mAP&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;R-CNN bbox reg&lt;/td&gt;
&lt;td align="center"&gt;31.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;For more details see the updated &lt;a href="http://arxiv.org/abs/1311.2524v3" rel="nofollow"&gt;R-CNN tech report&lt;/a&gt; (Sections 2.5 and 4, in particular)&lt;/li&gt;
&lt;li&gt;This model is available in the model package (see below)&lt;/li&gt;
&lt;li&gt;The code that was used for training is in the &lt;code&gt;ilsvrc&lt;/code&gt; branch (still needs some cleanup before merging into &lt;code&gt;master&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-installing-r-cnn" class="anchor" aria-hidden="true" href="#installing-r-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing R-CNN&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;MATLAB (tested with 2012b on 64-bit Linux)&lt;/li&gt;
&lt;li&gt;Caffe's &lt;a href="http://caffe.berkeleyvision.org/installation.html#prequequisites" rel="nofollow"&gt;prerequisites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Install Caffe&lt;/strong&gt; (this is the most complicated part)&lt;/li&gt;
&lt;li&gt;R-CNN has been checked for compatability against Caffe release v0.999. &lt;em&gt;It has not been updated to work with the current Caffe master.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Download &lt;a href="https://github.com/BVLC/caffe/archive/v0.999.tar.gz"&gt;Caffe v0.999&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow the &lt;a href="http://caffe.berkeleyvision.org/installation.html" rel="nofollow"&gt;Caffe installation instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Let's call the place where you installed caffe &lt;code&gt;$CAFFE_ROOT&lt;/code&gt; (you can run &lt;code&gt;export CAFFE_ROOT=$(pwd)&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Important:&lt;/strong&gt; Make sure to compile the Caffe MATLAB wrapper, which is not built by default: &lt;code&gt;make matcaffe&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Important:&lt;/strong&gt; Make sure to run &lt;code&gt;cd $CAFFE_ROOT/data/ilsvrc12 &amp;amp;&amp;amp; ./get_ilsvrc_aux.sh&lt;/code&gt; to download the ImageNet image mean&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Install R-CNN&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Get the R-CNN source code by cloning the repository: &lt;code&gt;git clone https://github.com/rbgirshick/rcnn.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Now change into the R-CNN source code directory: &lt;code&gt;cd rcnn&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;R-CNN expects to find Caffe in &lt;code&gt;external/caffe&lt;/code&gt;, so create a symlink: &lt;code&gt;ln -sf $CAFFE_ROOT external/caffe&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start MATLAB (make sure you're still in the &lt;code&gt;rcnn&lt;/code&gt; directory): &lt;code&gt;matlab&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;You'll be prompted to download the &lt;a href="http://disi.unitn.it/~uijlings/MyHomepage/index.php#page=projects1" rel="nofollow"&gt;Selective Search&lt;/a&gt; code, which we cannot redistribute. Afterwards, you should see the message &lt;code&gt;R-CNN startup done&lt;/code&gt; followed by the MATLAB prompt &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Run the build script: &lt;code&gt;&amp;gt;&amp;gt; rcnn_build()&lt;/code&gt; (builds &lt;a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/" rel="nofollow"&gt;liblinear&lt;/a&gt; and &lt;a href="http://www.science.uva.nl/research/publications/2013/UijlingsIJCV2013/" rel="nofollow"&gt;Selective Search&lt;/a&gt;). Don't worry if you see compiler warnings while building liblinear, this is normal on my system.&lt;/li&gt;
&lt;li&gt;Check that Caffe and MATLAB wrapper are set up correctly (this code should run without error): &lt;code&gt;&amp;gt;&amp;gt; key = caffe('get_init_key');&lt;/code&gt; (expected output is key = -2)&lt;/li&gt;
&lt;li&gt;Download the model package, which includes precompute models (see below).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Common issues:&lt;/strong&gt; You may need to set an &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; before you start MATLAB. If you see a message like "Invalid MEX-file '/path/to/rcnn/external/caffe/matlab/caffe/caffe.mexa64': libmkl_rt.so: cannot open shared object file: No such file or directory" then make sure that CUDA and MKL are in your &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt;. On my system, I use:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export LD_LIBRARY_PATH=/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-downloading-pre-computed-models-the-model-package" class="anchor" aria-hidden="true" href="#downloading-pre-computed-models-the-model-package"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading pre-computed models (the model package)&lt;/h3&gt;
&lt;p&gt;The quickest way to get started is to download pre-computed R-CNN detectors. Currently we have detectors trained on PASCAL VOC 2007 train+val, 2012 train, and ILSVRC13 train+val. Unfortunately the download is large (1.5GB), so brew some coffee or take a walk while waiting.&lt;/p&gt;
&lt;p&gt;From the &lt;code&gt;rcnn&lt;/code&gt; folder, run the model fetch script: &lt;code&gt;./data/fetch_models.sh&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This will populate the &lt;code&gt;rcnn/data&lt;/code&gt; folder with &lt;code&gt;caffe_nets&lt;/code&gt; and &lt;code&gt;rcnn_models&lt;/code&gt;. See &lt;code&gt;rcnn/data/README.md&lt;/code&gt; for details.&lt;/p&gt;
&lt;p&gt;Pre-computed selective search boxes can also be downloaded for VOC2007, VOC2012, and ILSVRC13.
From the &lt;code&gt;rcnn&lt;/code&gt; folder, run the selective search data fetch script: &lt;code&gt;./data/fetch_selective_search_data.sh&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This will populate the &lt;code&gt;rcnn/data&lt;/code&gt; folder with &lt;code&gt;selective_selective_data&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Caffe compatibility note:&lt;/strong&gt; R-CNN has been updated to use the new Caffe proto messages that were rolled out in Caffe v0.999. The model package contains models in the up-to-date proto format. If, for some reason, you need to get the old (Caffe proto v0) models, they can still be downloaded: &lt;a href="https://dl.dropboxusercontent.com/s/ttw041hqgw64ymx/r-cnn-release1-data-caffe-proto-v0.tgz" rel="nofollow"&gt;VOC models&lt;/a&gt;
&lt;a href="https://dl.dropboxusercontent.com/s/c6aqns2bvoqi86q/r-cnn-release1-data-ilsvrc2013-caffe-proto-v0.tgz" rel="nofollow"&gt;ILSVRC13 model&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-running-an-r-cnn-detector-on-an-image" class="anchor" aria-hidden="true" href="#running-an-r-cnn-detector-on-an-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running an R-CNN detector on an image&lt;/h3&gt;
&lt;p&gt;Let's assume that you've downloaded the precomputed detectors. Now:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Change to where you installed R-CNN: &lt;code&gt;cd rcnn&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Start MATLAB &lt;code&gt;matlab&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Important:&lt;/strong&gt; if you don't see the message &lt;code&gt;R-CNN startup done&lt;/code&gt; when MATLAB starts, then you probably didn't start MATLAB in &lt;code&gt;rcnn&lt;/code&gt; directory.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Run the demo: &lt;code&gt;&amp;gt;&amp;gt; rcnn_demo&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Enjoy the detected bicycle and person&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-training-your-own-r-cnn-detector-on-pascal-voc" class="anchor" aria-hidden="true" href="#training-your-own-r-cnn-detector-on-pascal-voc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training your own R-CNN detector on PASCAL VOC&lt;/h3&gt;
&lt;p&gt;Let's use PASCAL VOC 2007 as an example. The basic pipeline is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;extract features to disk -&amp;gt; train SVMs -&amp;gt; test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You'll need about 200GB of disk space free for the feature cache (which is stored in &lt;code&gt;rcnn/feat_cache&lt;/code&gt; by default; symlink &lt;code&gt;rcnn/feat_cache&lt;/code&gt; elsewhere if needed). &lt;strong&gt;It's best if the feature cache is on a fast, local disk.&lt;/strong&gt; Before running the pipeline, we first need to install the PASCAL VOC 2007 dataset.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-installing-pascal-voc-2007" class="anchor" aria-hidden="true" href="#installing-pascal-voc-2007"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing PASCAL VOC 2007&lt;/h4&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Download the training, validation, test data and VOCdevkit:&lt;/li&gt;
&lt;/ol&gt;
  &lt;pre&gt;  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtrainval_06-Nov-2007.tar
  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtest_06-Nov-2007.tar
  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCdevkit_08-Jun-2007.tar
  &lt;/pre&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Extract all of these tars into one directory, it's called &lt;code&gt;VOCdevkit&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
  &lt;pre&gt;  tar xvf VOCtrainval_06-Nov-2007.tar
  tar xvf VOCtest_06-Nov-2007.tar
  tar xvf VOCdevkit_08-Jun-2007.tar
  &lt;/pre&gt;
&lt;ol start="0"&gt;
&lt;li&gt;It should have this basic structure:&lt;/li&gt;
&lt;/ol&gt;
  &lt;pre&gt;  VOCdevkit/                           % development kit
  VOCdevkit/VOCcode/                   % VOC utility code
  VOCdevkit/VOC2007                    % image sets, annotations, etc.
  ... and several other directories ...
  &lt;/pre&gt;
&lt;ol start="0"&gt;
&lt;li&gt;I use a symlink to hook the R-CNN codebase to the PASCAL VOC dataset:&lt;/li&gt;
&lt;/ol&gt;
  &lt;pre&gt;  ln -sf /your/path/to/voc2007/VOCdevkit /path/to/rcnn/datasets/VOCdevkit2007
  &lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-extracting-features" class="anchor" aria-hidden="true" href="#extracting-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Extracting features&lt;/h4&gt;
&lt;pre&gt;&amp;gt;&amp;gt; rcnn_exp_cache_features('train');   % chunk1
&amp;gt;&amp;gt; rcnn_exp_cache_features('val');     % chunk2
&amp;gt;&amp;gt; rcnn_exp_cache_features('test_1');  % chunk3
&amp;gt;&amp;gt; rcnn_exp_cache_features('test_2');  % chunk4
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Pro tip:&lt;/strong&gt; on a machine with one hefty GPU (e.g., k20, k40, titan) and a six-core processor, I run start two MATLAB sessions each with a three worker matlabpool. I then run chunk1 and chunk2 in parallel on that machine. In this setup, completing chunk1 and chunk2 takes about 8-9 hours (depending on your CPU/GPU combo and disk) on a single machine. Obviously, if you have more machines you can hack this function to split the workload.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-training-r-cnn-models-and-testing" class="anchor" aria-hidden="true" href="#training-r-cnn-models-and-testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training R-CNN models and testing&lt;/h4&gt;
&lt;p&gt;Now to run the training and testing code, use the following experiments script:&lt;/p&gt;
&lt;pre&gt;&amp;gt;&amp;gt; test_results = rcnn_exp_train_and_test()
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The training and testing procedures save models and results under &lt;code&gt;rcnn/cachedir&lt;/code&gt; by default. You can customize this by creating a local config file named &lt;code&gt;rcnn_config_local.m&lt;/code&gt; and defining the experiment directory variable &lt;code&gt;EXP_DIR&lt;/code&gt;. Look at &lt;code&gt;rcnn_config_local.example.m&lt;/code&gt; for an example.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-training-an-r-cnn-detector-on-another-dataset" class="anchor" aria-hidden="true" href="#training-an-r-cnn-detector-on-another-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training an R-CNN detector on another dataset&lt;/h3&gt;
&lt;p&gt;It should be easy to train an R-CNN detector using another detection dataset as long as that dataset has &lt;em&gt;complete&lt;/em&gt; bounding box annotations (i.e., all instances of all classes are labeled).&lt;/p&gt;
&lt;p&gt;To support a new dataset, you define three functions: (1) one that returns a structure that describes the class labels and list of images; (2) one that returns a region of interest (roi) structure that describes the bounding box annotations; and (3) one that provides an test evaluation function.&lt;/p&gt;
&lt;p&gt;You can follow the PASCAL VOC implementation as your guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;imdb/imdb_from_voc.m   (list of images and classes)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;imdb/roidb_from_voc.m (region of interest database)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;imdb/imdb_eval_voc.m   (evalutation)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-fine-tuning-a-cnn-for-detection-with-caffe" class="anchor" aria-hidden="true" href="#fine-tuning-a-cnn-for-detection-with-caffe"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning a CNN for detection with Caffe&lt;/h3&gt;
&lt;p&gt;As an example, let's see how you would fine-tune a CNN for detection on PASCAL VOC 2012.&lt;/p&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Create window files for VOC 2012 train and VOC 2012 val.&lt;/li&gt;
&lt;li&gt;Start MATLAB in the &lt;code&gt;rcnn&lt;/code&gt; directory&lt;/li&gt;
&lt;li&gt;Get the imdb for VOC 2012 train: &lt;code&gt;&amp;gt;&amp;gt; imdb_train = imdb_from_voc('datasets/VOCdevkit2012', 'train', '2012');&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Get the imdb for VOC 2012 val: &lt;code&gt;&amp;gt;&amp;gt; imdb_val = imdb_from_voc('datasets/VOCdevkit2012', 'val', '2012');&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create the window file for VOC 2012 train: &lt;code&gt;&amp;gt;&amp;gt; rcnn_make_window_file(imdb_train, 'external/caffe/examples/pascal-finetuning');&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create the window file for VOC 2012 val: &lt;code&gt;&amp;gt;&amp;gt; rcnn_make_window_file(imdb_val, 'external/caffe/examples/pascal-finetuning');&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Exit MATLAB&lt;/li&gt;
&lt;li&gt;Run fine-tuning with Caffe&lt;/li&gt;
&lt;li&gt;Copy the fine-tuning prototxt files: &lt;code&gt;cp finetuning/voc_2012_prototxt/pascal_finetune_* external/caffe/examples/pascal-finetuning/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Change directories to &lt;code&gt;external/caffe/examples/pascal-finetuning&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Execute the fine-tuning code (make sure to replace &lt;code&gt;/path/to/rcnn&lt;/code&gt; with the actual path to where R-CNN is installed):&lt;/li&gt;
&lt;/ol&gt;
  &lt;pre&gt;  GLOG_logtostderr=1 ../../build/tools/finetune_net.bin \
  pascal_finetune_solver.prototxt \
  /path/to/rcnn/data/caffe_nets/ilsvrc_2012_train_iter_310k 2&amp;gt;&amp;amp;1 | tee log.txt
  &lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In my experiments, I've let fine-tuning run for 70k iterations, although with hindsight it appears that improvement in mAP saturates at around 40k iterations.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rbgirshick</author><guid isPermaLink="false">https://github.com/rbgirshick/rcnn</guid><pubDate>Thu, 26 Dec 2019 00:03:00 GMT</pubDate></item><item><title>TadasBaltrusaitis/OpenFace #4 in MATLAB, Today</title><link>https://github.com/TadasBaltrusaitis/OpenFace</link><description>&lt;p&gt;&lt;i&gt;OpenFace – a state-of-the art tool intended for facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-openface-220-a-facial-behavior-analysis-toolkit" class="anchor" aria-hidden="true" href="#openface-220-a-facial-behavior-analysis-toolkit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenFace 2.2.0: a facial behavior analysis toolkit&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/TadasBaltrusaitis/OpenFace" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4243d567083c3a651ecbadc77c77a00ba697fb40/68747470733a2f2f7472617669732d63692e6f72672f546164617342616c7472757361697469732f4f70656e466163652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/TadasBaltrusaitis/OpenFace.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/TadasBaltrusaitis/openface/branch/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8c6fb6db38385292d352f1b17205f42469d06fda/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f386d73696b6c786662686c6e736d78702f6272616e63682f6d61737465723f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/8msiklxfbhlnsmxp/branch/master?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Over the past few years, there has been an increased interest in automatic facial behavior analysis
and understanding. We present OpenFace – a tool intended for computer vision and machine learning
researchers, affective computing community and people interested in building interactive
applications based on facial behavior analysis. OpenFace is the ﬁrst toolkit capable of facial
landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation
with available source code for both running and training the models. The computer vision algorithms
which represent the core of OpenFace demonstrate state-of-the-art results in all of the above
mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a
simple webcam without any specialist hardware.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/muticomp_logo_black.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/muticomp_logo_black.png" alt="Multicomp logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OpenFace was originally developed by Tadas Baltrušaitis in collaboration with CMU MultiComp Lab led by Prof. Louis-Philippe Morency. Some of the original algorithms were created while at Rainbow Group, Cambridge University. The OpenFace library is still actively developed at the CMU MultiComp Lab in collaboration with Tadas Baltršaitis. Special thanks to researcher who helped developing, implementing and testing the algorithms present in OpenFace: Amir Zadeh and Yao Chong Lim on work on the CE-CLM model and Erroll Wood for the gaze estimation work.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wiki" class="anchor" aria-hidden="true" href="#wiki"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WIKI&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;For instructions of how to install/compile/use the project please see &lt;a href="https://github.com/TadasBaltrusaitis/OpenFace/wiki"&gt;WIKI&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-functionality" class="anchor" aria-hidden="true" href="#functionality"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Functionality&lt;/h2&gt;
&lt;p&gt;The system is capable of performing a number of facial analysis tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facial Landmark Detection&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/multi_face_img.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/multi_face_img.png" alt="Sample facial landmark detection image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facial Landmark and head pose tracking (links to YouTube videos)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=V7rV0uy7heQ" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/861e54570c553cb53c797dd8bc57a24f29152655/687474703a2f2f696d672e796f75747562652e636f6d2f76692f56377256307579376865512f302e6a7067" alt="Multiple Face Tracking" width="240" height="180" border="10" data-canonical-src="http://img.youtube.com/vi/V7rV0uy7heQ/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.youtube.com/watch?v=vYOa8Pif5lY" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a9137826a1740e8df2125ef9b76541445e32957e/687474703a2f2f696d672e796f75747562652e636f6d2f76692f76594f6138506966356c592f302e6a7067" alt="Multiple Face Tracking" width="240" height="180" border="10" data-canonical-src="http://img.youtube.com/vi/vYOa8Pif5lY/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facial Action Unit Recognition&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/au_sample.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/au_sample.png" height="280" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaze tracking (image of it in action)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/gaze_ex.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/gaze_ex.png" height="182" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facial Feature Extraction (aligned faces and HOG features)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/appearance.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/appearance.png" alt="Sample aligned face and HOG image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use any of the resources provided on this page in any of your publications we ask you to cite the following work and the work for a relevant submodule you used.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-overall-system" class="anchor" aria-hidden="true" href="#overall-system"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overall system&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;OpenFace 2.0: Facial Behavior Analysis Toolkit&lt;/strong&gt;
Tadas Baltrušaitis, Amir Zadeh, Yao Chong Lim, and Louis-Philippe Morency,
&lt;em&gt;IEEE International Conference on Automatic Face and Gesture Recognition&lt;/em&gt;, 2018&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-facial-landmark-detection-and-tracking" class="anchor" aria-hidden="true" href="#facial-landmark-detection-and-tracking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Facial landmark detection and tracking&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Convolutional experts constrained local model for facial landmark detection&lt;/strong&gt;
A. Zadeh, T. Baltrušaitis, and Louis-Philippe Morency.
&lt;em&gt;Computer Vision and Pattern Recognition Workshops&lt;/em&gt;, 2017&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Constrained Local Neural Fields for robust facial landmark detection in the wild&lt;/strong&gt;
Tadas Baltrušaitis, Peter Robinson, and Louis-Philippe Morency.
in IEEE Int. &lt;em&gt;Conference on Computer Vision Workshops, 300 Faces in-the-Wild Challenge&lt;/em&gt;, 2013.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-eye-gaze-tracking" class="anchor" aria-hidden="true" href="#eye-gaze-tracking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eye gaze tracking&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Rendering of Eyes for Eye-Shape Registration and Gaze Estimation&lt;/strong&gt;
Erroll Wood, Tadas Baltrušaitis, Xucong Zhang, Yusuke Sugano, Peter Robinson, and Andreas Bulling
in &lt;em&gt;IEEE International Conference on Computer Vision (ICCV)&lt;/em&gt;, 2015&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-facial-action-unit-detection" class="anchor" aria-hidden="true" href="#facial-action-unit-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Facial Action Unit detection&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Cross-dataset learning and person-specific normalisation for automatic Action Unit detection&lt;/strong&gt;
Tadas Baltrušaitis, Marwa Mahmoud, and Peter Robinson
in &lt;em&gt;Facial Expression Recognition and Analysis Challenge&lt;/em&gt;,
&lt;em&gt;IEEE International Conference on Automatic Face and Gesture Recognition&lt;/em&gt;, 2015&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-commercial-license" class="anchor" aria-hidden="true" href="#commercial-license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Commercial license&lt;/h1&gt;
&lt;p&gt;For inquiries about the commercial licensing of the OpenFace toolkit please visit &lt;a href="https://www.flintbox.com/public/project/50632/" rel="nofollow"&gt;https://www.flintbox.com/public/project/50632/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-final-remarks" class="anchor" aria-hidden="true" href="#final-remarks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Final remarks&lt;/h1&gt;
&lt;p&gt;I did my best to make sure that the code runs out of the box but there are always issues and I would be grateful for your understanding that this is research code and a research project. If you encounter any problems/bugs/issues please contact me on github or by emailing me at &lt;a href="mailto:tadyla@gmail.com"&gt;tadyla@gmail.com&lt;/a&gt; for any bug reports/questions/suggestions. I prefer questions and bug reports on github as that provides visibility to others who might be encountering same issues or who have the same questions.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-copyright" class="anchor" aria-hidden="true" href="#copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Copyright&lt;/h1&gt;
&lt;p&gt;Copyright can be found in the Copyright.txt&lt;/p&gt;
&lt;p&gt;You have to respect dlib, OpenBLAS, and OpenCV licenses.&lt;/p&gt;
&lt;p&gt;Furthermore you have to respect the licenses of the datasets used for model training - &lt;a href="https://github.com/TadasBaltrusaitis/OpenFace/wiki/Datasets"&gt;https://github.com/TadasBaltrusaitis/OpenFace/wiki/Datasets&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>TadasBaltrusaitis</author><guid isPermaLink="false">https://github.com/TadasBaltrusaitis/OpenFace</guid><pubDate>Thu, 26 Dec 2019 00:04:00 GMT</pubDate></item></channel></rss>