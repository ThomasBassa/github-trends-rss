<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: MATLAB, Today</title><link>https://github.com/trending/matlab?since=daily</link><description>The top repositories on GitHub for matlab, measured daily</description><pubDate>Mon, 06 Jan 2020 01:08:17 GMT</pubDate><lastBuildDate>Mon, 06 Jan 2020 01:08:17 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>BIMK/PlatEMO #1 in MATLAB, Today</title><link>https://github.com/BIMK/PlatEMO</link><description>&lt;p&gt;&lt;i&gt;Evolutionary multi-objective optimization platform&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./Doc/logo.png"&gt;&lt;img src="./Doc/logo.png" width="256" height="256" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;  
&lt;h1&gt;&lt;a id="user-content-platemo" class="anchor" aria-hidden="true" href="#platemo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PlatEMO&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/BIMK/PlatEMO/archive/master.zip"&gt;&lt;img src="https://camo.githubusercontent.com/60b2e104217818ea0706e204aaeb52889e4277e1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f776e6c6f61642d4c61746573742d79656c6c6f772e737667" alt="" data-canonical-src="https://img.shields.io/badge/Download-Latest-yellow.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/BIMK/PlatEMO/releases/"&gt;&lt;img src="https://camo.githubusercontent.com/a937e90f46f8f309d041125cc9e76dd9f855a60d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f42494d4b2f506c6174454d4f2e737667" alt="" data-canonical-src="https://img.shields.io/github/release/BIMK/PlatEMO.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="#PlatEMO"&gt;&lt;img src="https://camo.githubusercontent.com/29182b460d257897b808b436a175dd3ac5e09b2e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d61746c61622d25334525334425323032303134612532302d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/Matlab-%3E%3D%202014a%20-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;
&lt;a href="#PlatEMO"&gt;&lt;img src="https://camo.githubusercontent.com/c2089b82da22e530e36e7f825c55464b3cc9b18e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f57696e646f77732d506173732d627269676874677265656e2e737667" alt="" data-canonical-src="https://img.shields.io/badge/Windows-Pass-brightgreen.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="#PlatEMO"&gt;&lt;img src="https://camo.githubusercontent.com/1638e630880206a9e39ffca48c5208c4933cc8b9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c696e75782d506173732d627269676874677265656e2e737667" alt="" data-canonical-src="https://img.shields.io/badge/Linux-Pass-brightgreen.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="#PlatEMO"&gt;&lt;img src="https://camo.githubusercontent.com/2e16b98fc4082773919cced9fd255ed247d80015/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d61634f532d56616c69646174696e672d7265642e737667" alt="" data-canonical-src="https://img.shields.io/badge/MacOS-Validating-red.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;
Evolutionary multi-objective optimization platform&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;100+ open source evolutionary algorithms&lt;/li&gt;
&lt;li&gt;200+ open source multi-objective test problems&lt;/li&gt;
&lt;li&gt;Powerful GUI for performing experiments in parallel&lt;/li&gt;
&lt;li&gt;Generating results in the format of Excel or LaTeX table by one-click operation&lt;/li&gt;
&lt;li&gt;State-of-the-art algorithms will be included continuously&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you very much for using PlatEMO. The copyright of PlatEMO belongs to the BIMK Group. This
tool is mainly for research and educational purposes. The codes were implemented based on our
understanding of the algorithms published in the papers. You should not rely upon the material or
information on the website as a basis for making any business, legal or any other decisions. We
assume no responsibilities for any consequences of your using any algorithms in the tool. All
publications using the platform should acknowledge the use of “PlatEMO” and reference the
following literature:&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-copyright" class="anchor" aria-hidden="true" href="#copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Copyright&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The Copyright of the PlatEMO belongs to the BIMK group. You are free to &lt;a href="https://github.com/BIMK/PlatEMO/releases"&gt;use the PlatEMO&lt;/a&gt; for &lt;strong&gt;research purposes&lt;/strong&gt;. All publications which use this platform or any code in the platform should &lt;strong&gt;acknowledge the use of "PlatEMO" and reference&lt;/strong&gt; &lt;em&gt;"Ye Tian, Ran Cheng, Xingyi Zhang, and Yaochu Jin, PlatEMO: A MATLAB Platform for Evolutionary Multi-Objective Optimization [Educational Forum], IEEE Computational Intelligence Magazine, 2017, 12(4): 73-87".&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;@article{PlatEMO,
  title={{PlatEMO}: A {MATLAB} platform for evolutionary multi-objective optimization},
  author={Tian, Ye and Cheng, Ran and Zhang, Xingyi and Jin, Yaochu},
  journal={IEEE Computational Intelligence Magazine},
  volume={12},
  number={4},
  pages={73--87},
  year={2017},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-release-highlights-of-platemo-24" class="anchor" aria-hidden="true" href="#release-highlights-of-platemo-24"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Release Highlights of PlatEMO 2.4&lt;/h1&gt;
&lt;p&gt;&lt;a href="./Doc/releasenote.md"&gt;Release Note can be found here&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add two algorithms: MSEA and OSP-NSDE. There are currently 110 algorithms in the platform.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-features-of-platemo" class="anchor" aria-hidden="true" href="#features-of-platemo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features of PlatEMO&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Totally Developed in MATLAB&lt;br&gt;
PlatEMO consists of a number of MATLAB functions without using any other libraries. Any machines able to run MATLAB can use PlatEMO regardless of the operating system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Includes Many Popular Algorithms&lt;br&gt;
PlatEMO includes more than ninety existing popular MOEAs, including genetic algorithm, differential evolution, particle swarm optimization, memetic algorithm, estimation of distribution algorithm, and surrogate model based algorithm. Most of them are representative algorithms published in top journals after 2010.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Various Figure Demonstrations&lt;br&gt;
Users can select various figures to be displayed, including the Pareto front of the result, the Pareto set of the result, the true Pareto front, and the evolutionary trajectories of any performance indicator values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Powerful and Friendly GUI&lt;br&gt;
PlatEMO provides a powerful and friendly GUI, where users can configure all the settings and perform experiments in parallel via the GUI without writing any code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generates Data in the Format of Excel or LaTeX&lt;br&gt;
Users can save the statistical experimental results generated by PlatEMO as an Excel table or LaTeX table, which can be directly used in academic writings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;strong&gt;recommend&lt;/strong&gt;] You can ask any question in &lt;a href="https://github.com/BIMK/PlatEMO/issues"&gt;issues block&lt;/a&gt; and upload your contribution by pulling request(PR).&lt;/li&gt;
&lt;li&gt;If you want to add your MOEA, MOP, operator or performance indicator to PlatEMO, please send the MATLAB code (able to be used in PlatEMO) and the relevant literature to &lt;a href="mailto:field910921@gmail.com"&gt;field910921@gmail.com&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If you have any question, comment or suggestion to PlatEMO or the algorithms in PlatEMO, please contact Ye Tian (&lt;a href="mailto:field910921@gmail.com"&gt;field910921@gmail.com&lt;/a&gt;) or join the group of QQ(Group number: 100065008).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./Doc/QQgroupNumber.jpg"&gt;&lt;img src="./Doc/QQgroupNumber.jpg" width="180" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;  
&lt;h1&gt;&lt;a id="user-content-acknowledge" class="anchor" aria-hidden="true" href="#acknowledge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledge&lt;/h1&gt;
&lt;p&gt;This repo belongs to BIMK group and has been transferred project from &lt;a href="http://bimk.ahu.edu.cn/" rel="nofollow"&gt;BIMK&lt;/a&gt; to github by Ye Tian and Shichen Peng&lt;a href="https://github.com/anonymone"&gt;@anonymone&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>BIMK</author><guid isPermaLink="false">https://github.com/BIMK/PlatEMO</guid><pubDate>Mon, 06 Jan 2020 00:01:00 GMT</pubDate></item><item><title>albanie/convnet-burden #2 in MATLAB, Today</title><link>https://github.com/albanie/convnet-burden</link><description>&lt;p&gt;&lt;i&gt;Memory consumption and FLOP count estimates for convnets&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-convnet-burden" class="anchor" aria-hidden="true" href="#convnet-burden"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;convnet-burden&lt;/h2&gt;
&lt;p&gt;Estimates of memory consumption and FLOP counts for various convolutional neural networks.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-image-classification-architectures" class="anchor" aria-hidden="true" href="#image-classification-architectures"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Classification Architectures&lt;/h3&gt;
&lt;p&gt;The numbers below are given for single element batches.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;model&lt;/th&gt;
&lt;th&gt;input size&lt;/th&gt;
&lt;th&gt;param mem&lt;/th&gt;
&lt;th&gt;feat. mem&lt;/th&gt;
&lt;th&gt;flops&lt;/th&gt;
&lt;th&gt;src&lt;/th&gt;
&lt;th&gt;performance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/alexnet.md"&gt;alexnet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;227 x 227&lt;/td&gt;
&lt;td&gt;233 MB&lt;/td&gt;
&lt;td&gt;3 MB&lt;/td&gt;
&lt;td&gt;727 MFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;41.80 / 19.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/caffenet.md"&gt;caffenet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;233 MB&lt;/td&gt;
&lt;td&gt;3 MB&lt;/td&gt;
&lt;td&gt;724 MFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;42.60 / 19.70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/squeezenet1-0.md"&gt;squeezenet1-0&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;5 MB&lt;/td&gt;
&lt;td&gt;30 MB&lt;/td&gt;
&lt;td&gt;837 MFLOPs&lt;/td&gt;
&lt;td&gt;PT&lt;/td&gt;
&lt;td&gt;41.90 / 19.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/squeezenet1-1.md"&gt;squeezenet1-1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;5 MB&lt;/td&gt;
&lt;td&gt;17 MB&lt;/td&gt;
&lt;td&gt;360 MFLOPs&lt;/td&gt;
&lt;td&gt;PT&lt;/td&gt;
&lt;td&gt;41.81 / 19.38&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/vgg-f.md"&gt;vgg-f&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;232 MB&lt;/td&gt;
&lt;td&gt;4 MB&lt;/td&gt;
&lt;td&gt;727 MFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;41.40 / 19.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/vgg-m.md"&gt;vgg-m&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;393 MB&lt;/td&gt;
&lt;td&gt;12 MB&lt;/td&gt;
&lt;td&gt;2 GFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;36.90 / 15.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/vgg-s.md"&gt;vgg-s&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;393 MB&lt;/td&gt;
&lt;td&gt;12 MB&lt;/td&gt;
&lt;td&gt;3 GFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;37.00 / 15.80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/vgg-m-2048.md"&gt;vgg-m-2048&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;353 MB&lt;/td&gt;
&lt;td&gt;12 MB&lt;/td&gt;
&lt;td&gt;2 GFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;37.10 / 15.80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/vgg-m-1024.md"&gt;vgg-m-1024&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;333 MB&lt;/td&gt;
&lt;td&gt;12 MB&lt;/td&gt;
&lt;td&gt;2 GFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;37.80 / 16.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/vgg-m-128.md"&gt;vgg-m-128&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;315 MB&lt;/td&gt;
&lt;td&gt;12 MB&lt;/td&gt;
&lt;td&gt;2 GFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;40.80 / 18.40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/vgg-vd-16-atrous.md"&gt;vgg-vd-16-atrous&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;82 MB&lt;/td&gt;
&lt;td&gt;58 MB&lt;/td&gt;
&lt;td&gt;16 GFLOPs&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;- / -&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/vgg-vd-16.md"&gt;vgg-vd-16&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;528 MB&lt;/td&gt;
&lt;td&gt;58 MB&lt;/td&gt;
&lt;td&gt;16 GFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;28.50 / 9.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/vgg-vd-19.md"&gt;vgg-vd-19&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;548 MB&lt;/td&gt;
&lt;td&gt;63 MB&lt;/td&gt;
&lt;td&gt;20 GFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;28.70 / 9.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/googlenet.md"&gt;googlenet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;51 MB&lt;/td&gt;
&lt;td&gt;26 MB&lt;/td&gt;
&lt;td&gt;2 GFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;34.20 / 12.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/resnet18.md"&gt;resnet18&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;45 MB&lt;/td&gt;
&lt;td&gt;23 MB&lt;/td&gt;
&lt;td&gt;2 GFLOPs&lt;/td&gt;
&lt;td&gt;PT&lt;/td&gt;
&lt;td&gt;30.24 / 10.92&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/resnet34.md"&gt;resnet34&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;83 MB&lt;/td&gt;
&lt;td&gt;35 MB&lt;/td&gt;
&lt;td&gt;4 GFLOPs&lt;/td&gt;
&lt;td&gt;PT&lt;/td&gt;
&lt;td&gt;26.70 / 8.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/resnet-50.md"&gt;resnet-50&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;98 MB&lt;/td&gt;
&lt;td&gt;103 MB&lt;/td&gt;
&lt;td&gt;4 GFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;24.60 / 7.70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/resnet-101.md"&gt;resnet-101&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;170 MB&lt;/td&gt;
&lt;td&gt;155 MB&lt;/td&gt;
&lt;td&gt;8 GFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;23.40 / 7.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/resnet-152.md"&gt;resnet-152&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;230 MB&lt;/td&gt;
&lt;td&gt;219 MB&lt;/td&gt;
&lt;td&gt;11 GFLOPs&lt;/td&gt;
&lt;td&gt;MCN&lt;/td&gt;
&lt;td&gt;23.00 / 6.70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/resnext-50-32x4d.md"&gt;resnext-50-32x4d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;96 MB&lt;/td&gt;
&lt;td&gt;132 MB&lt;/td&gt;
&lt;td&gt;4 GFLOPs&lt;/td&gt;
&lt;td&gt;L1&lt;/td&gt;
&lt;td&gt;22.60 / 6.49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/resnext-101-32x4d.md"&gt;resnext-101-32x4d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;169 MB&lt;/td&gt;
&lt;td&gt;197 MB&lt;/td&gt;
&lt;td&gt;8 GFLOPs&lt;/td&gt;
&lt;td&gt;L1&lt;/td&gt;
&lt;td&gt;21.55 / 5.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/resnext-101-64x4d.md"&gt;resnext-101-64x4d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;319 MB&lt;/td&gt;
&lt;td&gt;273 MB&lt;/td&gt;
&lt;td&gt;16 GFLOPs&lt;/td&gt;
&lt;td&gt;PT&lt;/td&gt;
&lt;td&gt;20.81 / 5.66&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/inception-v3.md"&gt;inception-v3&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;299 x 299&lt;/td&gt;
&lt;td&gt;91 MB&lt;/td&gt;
&lt;td&gt;89 MB&lt;/td&gt;
&lt;td&gt;6 GFLOPs&lt;/td&gt;
&lt;td&gt;PT&lt;/td&gt;
&lt;td&gt;22.55 / 6.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/SE-ResNet-50.md"&gt;SE-ResNet-50&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;107 MB&lt;/td&gt;
&lt;td&gt;103 MB&lt;/td&gt;
&lt;td&gt;4 GFLOPs&lt;/td&gt;
&lt;td&gt;SE&lt;/td&gt;
&lt;td&gt;22.37 / 6.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/SE-ResNet-101.md"&gt;SE-ResNet-101&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;189 MB&lt;/td&gt;
&lt;td&gt;155 MB&lt;/td&gt;
&lt;td&gt;8 GFLOPs&lt;/td&gt;
&lt;td&gt;SE&lt;/td&gt;
&lt;td&gt;21.75 / 5.72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/SE-ResNet-152.md"&gt;SE-ResNet-152&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;255 MB&lt;/td&gt;
&lt;td&gt;220 MB&lt;/td&gt;
&lt;td&gt;11 GFLOPs&lt;/td&gt;
&lt;td&gt;SE&lt;/td&gt;
&lt;td&gt;21.34 / 5.54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/SE-ResNeXt-50-32x4d.md"&gt;SE-ResNeXt-50-32x4d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;105 MB&lt;/td&gt;
&lt;td&gt;132 MB&lt;/td&gt;
&lt;td&gt;4 GFLOPs&lt;/td&gt;
&lt;td&gt;SE&lt;/td&gt;
&lt;td&gt;20.97 / 5.54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/SE-ResNeXt-101-32x4d.md"&gt;SE-ResNeXt-101-32x4d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;187 MB&lt;/td&gt;
&lt;td&gt;197 MB&lt;/td&gt;
&lt;td&gt;8 GFLOPs&lt;/td&gt;
&lt;td&gt;SE&lt;/td&gt;
&lt;td&gt;19.81 / 4.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/SENet.md"&gt;SENet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;440 MB&lt;/td&gt;
&lt;td&gt;347 MB&lt;/td&gt;
&lt;td&gt;21 GFLOPs&lt;/td&gt;
&lt;td&gt;SE&lt;/td&gt;
&lt;td&gt;18.68 / 4.47&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/SE-BN-Inception.md"&gt;SE-BN-Inception&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;46 MB&lt;/td&gt;
&lt;td&gt;43 MB&lt;/td&gt;
&lt;td&gt;2 GFLOPs&lt;/td&gt;
&lt;td&gt;SE&lt;/td&gt;
&lt;td&gt;23.62 / 7.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/densenet121.md"&gt;densenet121&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;31 MB&lt;/td&gt;
&lt;td&gt;126 MB&lt;/td&gt;
&lt;td&gt;3 GFLOPs&lt;/td&gt;
&lt;td&gt;PT&lt;/td&gt;
&lt;td&gt;25.35 / 7.83&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/densenet161.md"&gt;densenet161&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;110 MB&lt;/td&gt;
&lt;td&gt;235 MB&lt;/td&gt;
&lt;td&gt;8 GFLOPs&lt;/td&gt;
&lt;td&gt;PT&lt;/td&gt;
&lt;td&gt;22.35 / 6.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/densenet169.md"&gt;densenet169&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;55 MB&lt;/td&gt;
&lt;td&gt;152 MB&lt;/td&gt;
&lt;td&gt;3 GFLOPs&lt;/td&gt;
&lt;td&gt;PT&lt;/td&gt;
&lt;td&gt;24.00 / 7.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/densenet201.md"&gt;densenet201&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;77 MB&lt;/td&gt;
&lt;td&gt;196 MB&lt;/td&gt;
&lt;td&gt;4 GFLOPs&lt;/td&gt;
&lt;td&gt;PT&lt;/td&gt;
&lt;td&gt;22.80 / 6.43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/mcn-mobilenet.md"&gt;mcn-mobilenet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;224 x 224&lt;/td&gt;
&lt;td&gt;16 MB&lt;/td&gt;
&lt;td&gt;38 MB&lt;/td&gt;
&lt;td&gt;579 MFLOPs&lt;/td&gt;
&lt;td&gt;AU&lt;/td&gt;
&lt;td&gt;29.40 / -&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Click on the model name for a more detailed breakdown of feature extraction costs at different input image/batch sizes if needed.  The performance numbers are reported as &lt;code&gt;top-1 error/top-5 error&lt;/code&gt; on the 2012 ILSVRC validation data.  The &lt;code&gt;src&lt;/code&gt; column indicates the source of the benchmark scores using the following abberviations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.vlfeat.org/matconvnet/pretrained/" rel="nofollow"&gt;MCN&lt;/a&gt; - scores obtained from the matconvnet website.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pytorch.org/docs/master/torchvision/models.html" rel="nofollow"&gt;PT&lt;/a&gt; - scores obtained from the PyTorch torchvision module.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/albanie/mcnPyTorch/blob/master/benchmarks/cnn_imagenet_pt_mcn.m"&gt;L1&lt;/a&gt; - evaluated locally (follow link to view benchmark code).&lt;/li&gt;
&lt;li&gt;AU - numbers reported by the paper authors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These numbers provide an estimate of performance, but note that there may be small differences between the evaluation scripts from different sources.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" rel="nofollow"&gt;alexnet&lt;/a&gt; - &lt;em&gt;Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems. 2012.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1602.07360" rel="nofollow"&gt;squeezenet&lt;/a&gt; - &lt;em&gt;Iandola, Forrest N., et al. "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&amp;lt; 0.5 MB model size." arXiv preprint arXiv:1602.07360 (2016).&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1405.3531" rel="nofollow"&gt;vgg-m&lt;/a&gt; -  &lt;em&gt;Chatfield, Ken, et al. "Return of the devil in the details: Delving deep into convolutional nets." arXiv preprint arXiv:1405.3531 (2014).&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1409.1556" rel="nofollow"&gt;vgg-vd-16/vgg-vd-19&lt;/a&gt; -  &lt;em&gt;Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014).&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1506.04579" rel="nofollow"&gt;vgg-vd-16-reduced&lt;/a&gt; - &lt;em&gt;Liu, Wei, Andrew Rabinovich, and Alexander C. Berg. "Parsenet: Looking wider to see better." arXiv preprint arXiv:1506.04579 (2015)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html" rel="nofollow"&gt;googlenet&lt;/a&gt; - &lt;em&gt;Szegedy, Christian, et al. "Going deeper with convolutions." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1512.00567" rel="nofollow"&gt;inception&lt;/a&gt; - &lt;em&gt;Szegedy, Christian, et al. "Rethinking the inception architecture for computer vision." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1512.03385" rel="nofollow"&gt;resnet&lt;/a&gt; - &lt;em&gt;He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1611.05431" rel="nofollow"&gt;resnext&lt;/a&gt; - &lt;em&gt;Xie, Saining, et al. "Aggregated residual transformations for deep neural networks." arXiv preprint arXiv:1611.05431 (2016).&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1709.01507" rel="nofollow"&gt;SENets&lt;/a&gt; - &lt;em&gt;Jie Hu, Li Shen and Gang Sun. "Squeeze-and-Excitation Networks." arXiv preprint arXiv:1709.01507 (2017).&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1608.06993" rel="nofollow"&gt;Densenet&lt;/a&gt; - &lt;em&gt;Huang, Gao, et al. "Densely connected convolutional networks." CVPR, (2017).&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-object-detection-architectures" class="anchor" aria-hidden="true" href="#object-detection-architectures"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Object Detection Architectures&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;model&lt;/th&gt;
&lt;th&gt;input size&lt;/th&gt;
&lt;th&gt;param memory&lt;/th&gt;
&lt;th&gt;feature memory&lt;/th&gt;
&lt;th&gt;flops&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/rfcn-res50-pascal.md"&gt;rfcn-res50-pascal&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;600 x 850&lt;/td&gt;
&lt;td&gt;122 MB&lt;/td&gt;
&lt;td&gt;1 GB&lt;/td&gt;
&lt;td&gt;79 GFLOPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/rfcn-res101-pascal.md"&gt;rfcn-res101-pascal&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;600 x 850&lt;/td&gt;
&lt;td&gt;194 MB&lt;/td&gt;
&lt;td&gt;2 GB&lt;/td&gt;
&lt;td&gt;117 GFLOPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/ssd-pascal-vggvd-300.md"&gt;ssd-pascal-vggvd-300&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;300 x 300&lt;/td&gt;
&lt;td&gt;100 MB&lt;/td&gt;
&lt;td&gt;116 MB&lt;/td&gt;
&lt;td&gt;31 GFLOPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/ssd-pascal-vggvd-512.md"&gt;ssd-pascal-vggvd-512&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;512 x 512&lt;/td&gt;
&lt;td&gt;104 MB&lt;/td&gt;
&lt;td&gt;337 MB&lt;/td&gt;
&lt;td&gt;91 GFLOPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/ssd-pascal-mobilenet-ft.md"&gt;ssd-pascal-mobilenet-ft&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;300 x 300&lt;/td&gt;
&lt;td&gt;22 MB&lt;/td&gt;
&lt;td&gt;37 MB&lt;/td&gt;
&lt;td&gt;1 GFLOPs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/faster-rcnn-vggvd-pascal.md"&gt;faster-rcnn-vggvd-pascal&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;600 x 850&lt;/td&gt;
&lt;td&gt;523 MB&lt;/td&gt;
&lt;td&gt;600 MB&lt;/td&gt;
&lt;td&gt;172 GFLOPS&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The input sizes used are "typical" for each of the architectures listed, but can be varied.  &lt;em&gt;Anchor/priorbox&lt;/em&gt; generation and &lt;em&gt;roi/psroi&lt;/em&gt;-pooling are not included in flop estimates.  The &lt;em&gt;ssd-pascal-mobilenet-ft&lt;/em&gt; detector uses the MobileNet feature extractor (the model used here was imported from the architecture made available by &lt;a href="https://github.com/chuanqi305/MobileNet-SSD"&gt;chuanqi305&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks" rel="nofollow"&gt;faster-rcnn&lt;/a&gt; - &lt;em&gt;Ren, Shaoqing, et al. "Faster R-CNN: Towards real-time object detection with region proposal networks." Advances in neural information processing systems. 2015..&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1605.06409" rel="nofollow"&gt;r-fcn&lt;/a&gt; - &lt;em&gt;Li, Yi, Kaiming He, and Jian Sun. "R-fcn: Object detection via region-based fully convolutional networks." Advances in Neural Information Processing Systems. 2016.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://link.springer.com/chapter/10.1007%2F978-3-319-46448-0_2" rel="nofollow"&gt;ssd&lt;/a&gt; - &lt;em&gt;Liu, Wei, et al. "Ssd: Single shot multibox detector." European conference on computer vision. Springer, Cham, 2016.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1704.04861" rel="nofollow"&gt;mobilenets&lt;/a&gt; - &lt;em&gt;Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. "Mobilenets: Efficient convolutional neural networks for mobile vision applications." arXiv preprint arXiv:1704.04861 (2017).&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-semantic-segmentation-architectures" class="anchor" aria-hidden="true" href="#semantic-segmentation-architectures"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Semantic Segmentation Architectures&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;model&lt;/th&gt;
&lt;th&gt;input size&lt;/th&gt;
&lt;th&gt;param memory&lt;/th&gt;
&lt;th&gt;feature memory&lt;/th&gt;
&lt;th&gt;flops&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/pascal-fcn32s.md"&gt;pascal-fcn32s&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;384 x 384&lt;/td&gt;
&lt;td&gt;519 MB&lt;/td&gt;
&lt;td&gt;423 MB&lt;/td&gt;
&lt;td&gt;125 GFLOPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/pascal-fcn16s.md"&gt;pascal-fcn16s&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;384 x 384&lt;/td&gt;
&lt;td&gt;514 MB&lt;/td&gt;
&lt;td&gt;424 MB&lt;/td&gt;
&lt;td&gt;125 GFLOPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/pascal-fcn8s.md"&gt;pascal-fcn8s&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;384 x 384&lt;/td&gt;
&lt;td&gt;513 MB&lt;/td&gt;
&lt;td&gt;426 MB&lt;/td&gt;
&lt;td&gt;125 GFLOPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/deeplab-vggvd-v2.md"&gt;deeplab-vggvd-v2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;513 x 513&lt;/td&gt;
&lt;td&gt;144 MB&lt;/td&gt;
&lt;td&gt;755 MB&lt;/td&gt;
&lt;td&gt;202 GFLOPs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/deeplab-res101-v2.md"&gt;deeplab-res101-v2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;513 x 513&lt;/td&gt;
&lt;td&gt;505 MB&lt;/td&gt;
&lt;td&gt;4 GB&lt;/td&gt;
&lt;td&gt;346 GFLOPs&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In this case, the input sizes are those which are typically taken as input crops during training.  The &lt;em&gt;deeplab-res101-v2&lt;/em&gt; model uses multi-scale input, with scales &lt;code&gt;x1, x0.75, x0.5&lt;/code&gt; (computed relative to the given input size).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html" rel="nofollow"&gt;pascal-fcn&lt;/a&gt; - &lt;em&gt;Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks for semantic segmentation." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015..&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1606.00915" rel="nofollow"&gt;deeplab&lt;/a&gt; - &lt;em&gt;DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs
Liang-Chieh Chen^, George Papandreou^, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille (^equal contribution)
Transactions on Pattern Analysis and Machine Intelligence (TPAMI)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-keypoint-detection-architectures" class="anchor" aria-hidden="true" href="#keypoint-detection-architectures"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Keypoint Detection Architectures&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;model&lt;/th&gt;
&lt;th&gt;input size&lt;/th&gt;
&lt;th&gt;param memory&lt;/th&gt;
&lt;th&gt;feature memory&lt;/th&gt;
&lt;th&gt;flops&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/multipose-mpi.md"&gt;multipose-mpi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;368 x 368&lt;/td&gt;
&lt;td&gt;196 MB&lt;/td&gt;
&lt;td&gt;245 MB&lt;/td&gt;
&lt;td&gt;134 GFLOPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="reports/multipose-coco.md"&gt;multipose-coco&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;368 x 368&lt;/td&gt;
&lt;td&gt;200 MB&lt;/td&gt;
&lt;td&gt;246 MB&lt;/td&gt;
&lt;td&gt;136 GFLOPS&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1611.08050" rel="nofollow"&gt;multipose&lt;/a&gt; - &lt;em&gt;Cao, Zhe, et al. "Realtime multi-person 2d pose estimation using part affinity fields." arXiv preprint arXiv:1611.08050 (2016)..&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-notes-and-assumptions" class="anchor" aria-hidden="true" href="#notes-and-assumptions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Notes and Assumptions&lt;/h3&gt;
&lt;p&gt;The numbers for each architecture should be reasonably framework agnostic. It is assumed that all weights and activations are stored as floats (with 4 bytes per datum) and that all relus are performed in-place.  Feature memory therefore represents an estimate of the total memory consumption of the features computed via a forward pass of the network for a given input, assuming that memory is not re-used (the exception to this is that, as noted above, relus are performed in-place and do not add to the feature memory total).  In practice, many frameworks will clear features from memory when they are no-longer required by the execution path and will therefore require less memory than is noted here.  The feature memory statistic is simply a rough guide as to "how big" the activations of the network look.&lt;/p&gt;
&lt;p&gt;Fused multiply-adds are counted as single operations.  The numbers should be considered to be rough approximations -  modern hardware makes it very difficult to accurately count operations (and even if you could, pipelining etc. means that it is not necessarily a good estimate of inference time).&lt;/p&gt;
&lt;p&gt;The tool for computing the estimates is implemented as a module for the &lt;a href="https://github.com/vlfeat/autonn"&gt;autonn&lt;/a&gt; wrapper of matconvnet and is included in this &lt;a href="core/burden.m"&gt;repo&lt;/a&gt;, so feel free to take a look for extra details.  This module can be installed with the &lt;code&gt;vl_contrib&lt;/code&gt; package manager (it has two dependencies which can be installed in a similar manner: &lt;a href="https://github.com/vlfeat/autonn"&gt;autonn&lt;/a&gt; and &lt;a href="https://github.com/albanie/mcnExtraLayers"&gt;mcnExtraLayers&lt;/a&gt;). Matconvnet versions of all of the models can be obtained from either &lt;a href="http://www.vlfeat.org/matconvnet/pretrained/" rel="nofollow"&gt;here&lt;/a&gt; or &lt;a href="http://www.robots.ox.ac.uk/~albanie/mcn-models.html" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For further reading on the topic, the 2017 ICLR submission &lt;a href="https://openreview.net/pdf?id=Bygq-H9eg" rel="nofollow"&gt;An analysis of deep neural network models for practical applications&lt;/a&gt; is interesting.  If you find any issues, or would like to add additional models, add an issue/PR.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>albanie</author><guid isPermaLink="false">https://github.com/albanie/convnet-burden</guid><pubDate>Mon, 06 Jan 2020 00:02:00 GMT</pubDate></item><item><title>atinesh-s/Coursera-Machine-Learning-Stanford #3 in MATLAB, Today</title><link>https://github.com/atinesh-s/Coursera-Machine-Learning-Stanford</link><description>&lt;p&gt;&lt;i&gt;Machine learning-Stanford University&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-coursera" class="anchor" aria-hidden="true" href="#machine-learning-coursera"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning (Coursera)&lt;/h1&gt;
&lt;p&gt;This is my solution to all the programming assignments and quizzes of Machine-Learning (Coursera) taught by Andrew Ng. After completing this course you will get a broad idea of Machine learning algorithms. Try to solve all the assignments by yourself first, but if you get stuck somewhere then feel free to browse the code.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Lectures Slides&lt;/li&gt;
&lt;li&gt;Solution to programming assignment&lt;/li&gt;
&lt;li&gt;Solution to Quizzes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-certificate" class="anchor" aria-hidden="true" href="#certificate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Certificate&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/account/accomplishments/certificate/GDDBFB572MUQ" rel="nofollow"&gt;Verified Certificate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.coursera.org/learn/machine-learning" rel="nofollow"&gt;[1] Machine Learning - Stanford University&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>atinesh-s</author><guid isPermaLink="false">https://github.com/atinesh-s/Coursera-Machine-Learning-Stanford</guid><pubDate>Mon, 06 Jan 2020 00:03:00 GMT</pubDate></item><item><title>AvaisP/machine-learning-programming-assignments-coursera-andrew-ng #4 in MATLAB, Today</title><link>https://github.com/AvaisP/machine-learning-programming-assignments-coursera-andrew-ng</link><description>&lt;p&gt;&lt;i&gt;Solutions to Andrew NG's machine learning course on Coursera&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-programming-assignments-coursera-andrew-ng" class="anchor" aria-hidden="true" href="#machine-learning-programming-assignments-coursera-andrew-ng"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;machine-learning-programming-assignments-coursera-andrew-ng&lt;/h1&gt;
&lt;p&gt;Solutions to Andrew NG's machine learning course on Coursera&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>AvaisP</author><guid isPermaLink="false">https://github.com/AvaisP/machine-learning-programming-assignments-coursera-andrew-ng</guid><pubDate>Mon, 06 Jan 2020 00:04:00 GMT</pubDate></item><item><title>Borye/machine-learning-coursera-1 #5 in MATLAB, Today</title><link>https://github.com/Borye/machine-learning-coursera-1</link><description>&lt;p&gt;&lt;i&gt;This repo is specially created for all the work done my me as a part of Coursera's Machine Learning Course.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-coursera" class="anchor" aria-hidden="true" href="#machine-learning-coursera"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;machine-learning-coursera&lt;/h1&gt;
&lt;p&gt;This repo is specially created for all the work done my me as a part of Coursera's Machine Learning Course.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Borye</author><guid isPermaLink="false">https://github.com/Borye/machine-learning-coursera-1</guid><pubDate>Mon, 06 Jan 2020 00:05:00 GMT</pubDate></item></channel></rss>