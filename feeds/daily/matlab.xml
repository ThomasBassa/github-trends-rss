<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: MATLAB, Today</title><link>https://github.com/trending/matlab?since=daily</link><description>The top repositories on GitHub for matlab, measured daily</description><pubDate>Sun, 19 Jan 2020 04:36:42 GMT</pubDate><lastBuildDate>Sun, 19 Jan 2020 04:36:42 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>vkosuri/CourseraMachineLearning #1 in MATLAB, Today</title><link>https://github.com/vkosuri/CourseraMachineLearning</link><description>&lt;p&gt;&lt;i&gt;Coursera Machine Learning By Prof. Andrew Ng&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-by-prof-andrew-ng-star2star2star2star2star" class="anchor" aria-hidden="true" href="#machine-learning-by-prof-andrew-ng-star2star2star2star2star"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning By Prof. Andrew Ng &lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;üåü&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;üåü&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;üåü&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;üåü&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;‚≠êÔ∏è&lt;/g-emoji&gt;&lt;/h1&gt;
&lt;p&gt;This page continas all my coursera machine learning courses and resources &lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;üìñ&lt;/g-emoji&gt; by &lt;a href="http://www.andrewng.org/" rel="nofollow"&gt;Prof. Andrew Ng&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="man" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f468.png"&gt;üë®&lt;/g-emoji&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#breif-intro"&gt;Breif Intro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#video-lectures-index"&gt;Video lectures Index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#programming-exercise-tutorials"&gt;Programming Exercise Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#programming-exercise-test-cases"&gt;Programming Exercise Test Cases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#useful-resources"&gt;Useful Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#schedule"&gt;Schedule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#extra-information"&gt;Extra Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#online-e-books"&gt;Online E-Books&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#aditional-information"&gt;Aditional Information&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-breif-intro" class="anchor" aria-hidden="true" href="#breif-intro"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Breif Intro&lt;/h2&gt;
&lt;p&gt;The most of the course talking about &lt;strong&gt;hypothesis function&lt;/strong&gt; and minimising &lt;strong&gt;cost funtions&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-hypothesis" class="anchor" aria-hidden="true" href="#hypothesis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hypothesis&lt;/h3&gt;
&lt;p&gt;A hypothesis is a certain function that we believe (or hope) is similar to the true function, the target function that we want to model. In context of email spam classification, it would be the rule we came up with that allows us to separate spam from non-spam emails.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-cost-function" class="anchor" aria-hidden="true" href="#cost-function"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cost Function&lt;/h3&gt;
&lt;p&gt;The cost function or &lt;strong&gt;Sum of Squeared Errors(SSE)&lt;/strong&gt; is a measure of how far away our hypothesis is from the optimal hypothesis. The closer our hypothesis matches the training examples, the smaller the value of the cost function. Theoretically, we would like J(Œ∏)=0&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-gradient-descent" class="anchor" aria-hidden="true" href="#gradient-descent"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gradient Descent&lt;/h3&gt;
&lt;p&gt;Gradient descent is an iterative minimization method. The gradient of the error function always shows in the direction of the steepest ascent of the error function. Thus, we can start with a random weight vector and subsequently follow the
negative gradient (using a learning rate alpha)&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-differnce-between-cost-function-and-gradient-descent-functions" class="anchor" aria-hidden="true" href="#differnce-between-cost-function-and-gradient-descent-functions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Differnce between cost function and gradient descent functions&lt;/h4&gt;
&lt;table&gt;
    
        
        
    
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt; Cost Function &lt;/th&gt;
            &lt;th&gt; Gradient Descent &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr valign="top"&gt;
            &lt;td&gt;
            &lt;pre&gt;&lt;code&gt;
            function J = computeCostMulti(X, y, theta)
                m = length(y); % number of training examples
                J = 0;
                predictions =  X*theta;
                sqerrors = (predictions - y).^2;
                J = 1/(2*m)* sum(sqerrors);
            end
            &lt;/code&gt;&lt;/pre&gt;
            &lt;/td&gt;
            &lt;td&gt;
            &lt;pre&gt;&lt;code&gt;
            function [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters)    
                m = length(y); % number of training examples
                J_history = zeros(num_iters, 1);
                for iter = 1:num_iters
                    predictions =  X * theta;
                    updates = X' * (predictions - y);
                    theta = theta - alpha * (1/m) * updates;
                    J_history(iter) = computeCostMulti(X, y, theta);
                end
            end
            &lt;/code&gt;&lt;/pre&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-bias-and-variance" class="anchor" aria-hidden="true" href="#bias-and-variance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bias and Variance&lt;/h3&gt;
&lt;p&gt;When we discuss prediction models, prediction errors can be decomposed into two main subcomponents we care about: error due to "bias" and error due to "variance". There is a tradeoff between a model's ability to minimize bias and variance. Understanding these two types of error can help us diagnose model results and avoid the mistake of over- or under-fitting.&lt;/p&gt;
&lt;p&gt;Source: &lt;a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="nofollow"&gt;http://scott.fortmann-roe.com/docs/BiasVariance.html&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-hypotheis-and-cost-function-table" class="anchor" aria-hidden="true" href="#hypotheis-and-cost-function-table"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hypotheis and Cost Function Table&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Algorithem&lt;/th&gt;
&lt;th&gt;Hypothesis Function&lt;/th&gt;
&lt;th&gt;Cost Function&lt;/th&gt;
&lt;th&gt;Gradient Descent&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Linear Regression&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/extra/img/linear_hypothesis.gif"&gt;&lt;img src="/extra/img/linear_hypothesis.gif" alt="linear_regression_hypothesis" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/extra/img/linear_cost.gif"&gt;&lt;img src="/extra/img/linear_cost.gif" alt="linear_regression_cost" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Linear Regression with Multiple variables&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/extra/img/linear_hypothesis.gif"&gt;&lt;img src="/extra/img/linear_hypothesis.gif" alt="linear_regression_hypothesis" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/extra/img/linear_cost.gif"&gt;&lt;img src="/extra/img/linear_cost.gif" alt="linear_regression_cost" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/extra/img/linear_multi_var_gradient_descent.gif"&gt;&lt;img src="/extra/img/linear_multi_var_gradient_descent.gif" alt="linear_regression_multi_var_gradient" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Logistic Regression&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/extra/img/logistic_hypothesis.gif"&gt;&lt;img src="/extra/img/logistic_hypothesis.gif" alt="logistic_regression_hypothesis" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/extra/img/logistic_cost.gif"&gt;&lt;img src="/extra/img/logistic_cost.gif" alt="logistic_regression_cost" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/extra/img/logistic_gradient.gif"&gt;&lt;img src="/extra/img/logistic_gradient.gif" alt="logistic_regression_gradient" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Logistic Regression with Multiple Variable&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/extra/img/logistic_multi_var_cost.gif"&gt;&lt;img src="/extra/img/logistic_multi_var_cost.gif" alt="logistic_regression_multi_var_cost" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/extra/img/logistic_multi_var_gradient.gif"&gt;&lt;img src="/extra/img/logistic_multi_var_gradient.gif" alt="logistic_regression_multi_var_gradient" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nural Networks&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/extra/img/nural_cost.gif"&gt;&lt;img src="/extra/img/nural_cost.gif" alt="nural_cost" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-regression-with-pictures" class="anchor" aria-hidden="true" href="#regression-with-pictures"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Regression with Pictures&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://adit.io/posts/2016-02-20-Linear-Regression-in-Pictures.html" rel="nofollow"&gt;Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://adit.io/posts/2016-03-13-Logistic-Regression.html#non-linear-classification" rel="nofollow"&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-video-lectures-index" class="anchor" aria-hidden="true" href="#video-lectures-index"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video lectures Index&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://class.coursera.org/ml/lecture/preview" rel="nofollow"&gt;https://class.coursera.org/ml/lecture/preview&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-programming-exercise-tutorials" class="anchor" aria-hidden="true" href="#programming-exercise-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Programming Exercise Tutorials&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.coursera.org/learn/machine-learning/discussions/all/threads/m0ZdvjSrEeWddiIAC9pDDA" rel="nofollow"&gt;https://www.coursera.org/learn/machine-learning/discussions/all/threads/m0ZdvjSrEeWddiIAC9pDDA&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-programming-exercise-test-cases" class="anchor" aria-hidden="true" href="#programming-exercise-test-cases"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Programming Exercise Test Cases&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.coursera.org/learn/machine-learning/discussions/all/threads/0SxufTSrEeWPACIACw4G5w" rel="nofollow"&gt;https://www.coursera.org/learn/machine-learning/discussions/all/threads/0SxufTSrEeWPACIACw4G5w&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-useful-resources" class="anchor" aria-hidden="true" href="#useful-resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Useful Resources&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.coursera.org/learn/machine-learning/resources/NrY2G" rel="nofollow"&gt;https://www.coursera.org/learn/machine-learning/resources/NrY2G&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-schedule" class="anchor" aria-hidden="true" href="#schedule"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Schedule:&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-week-1---due-071617" class="anchor" aria-hidden="true" href="#week-1---due-071617"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Week 1 - Due 07/16/17:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Welcome - &lt;a href="/home/week-1/lectures/pdf/Lecture1.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home//week-1/lectures/ppt/Lecture1.pptx"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linear regression with one variable - &lt;a href="/home/week-1/lectures/pdf/Lecture2.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-1/lectures/ppt/Lecture2.pptx"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linear Algebra review (Optional) - &lt;a href="/home/week-1/lectures/pdf/Lecture3.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-1/lectures/ppt/Lecture3.pptx"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-1/lectures/notes.pdf"&gt;Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-1/errata.pdf"&gt;Errata&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-week-2---due-072317" class="anchor" aria-hidden="true" href="#week-2---due-072317"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Week 2 - Due 07/23/17:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression with multiple variables - &lt;a href="/home/week-2/lectures/pdf/Lecture4.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-2/lectures/ppt/Lecture4.pptx"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Octave tutorial &lt;a href="/home/week-2/lectures/pdf/Lecture5.pdf"&gt;pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Exercise 1: Linear Regression - &lt;a href="/home/week-2/exercises/machine-learning-ex1/ex1.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-2/exercises/machine-learning-ex1.zip"&gt;Problem&lt;/a&gt; - &lt;a href="/home/week-2/exercises/machine-learning-ex1/ex1/"&gt;Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-2/lectures/notes.pdf"&gt;Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-2/errata.pdf"&gt;Errata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-2/exercises/Programming%20Ex.1.pdf"&gt;Program Exercise Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-week-3---due-073017" class="anchor" aria-hidden="true" href="#week-3---due-073017"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Week 3 - Due 07/30/17:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Logistic regression - &lt;a href="/home/week-3/lectures/pdf/Lecture6.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-3/lectures/ppt/Lecture6.pptx"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Regularization - &lt;a href="/home/week-3/lectures/pdf/Lecture7.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-3/lectures/ppt/Lecture7.pptx"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Exercise 2: Logistic Regression - &lt;a href="/home/week-3/exercises/machine-learning-ex2/ex2.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="home/week-3/exercises/machine-learning-ex2.zip"&gt;Problem&lt;/a&gt; - &lt;a href="/home/week-3/exercises/machine-learning-ex2/ex2"&gt;Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-3/lectures/notes.pdf"&gt;Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-3/errata.pdf"&gt;Errata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-3/exercises/Programming%20Ex.2.pdf"&gt;Program Exercise Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-week-4---due-080617" class="anchor" aria-hidden="true" href="#week-4---due-080617"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Week 4 - Due 08/06/17:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Neural Networks: Representation - &lt;a href="/home/week-4/lectures/pdf/Lecture8.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-4/lectures/ppt/Lecture8.pptx"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Exercise 3: Multi-class Classification and Neural Networks - &lt;a href="/home/week-4/exercises/machine-learning-ex3/ex3.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-4/exercises/machine-learning-ex3.zip"&gt;Problem&lt;/a&gt; - &lt;a href="/home/week-4/exercises/machine-learning-ex3/ex3"&gt;Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-4/lectures/notes.pdf"&gt;Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-4/errata.pdf"&gt;Errata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-4/exercises/Programming%20Ex.3.pdf"&gt;Program Exercise Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-week-5---due-081317" class="anchor" aria-hidden="true" href="#week-5---due-081317"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Week 5 - Due 08/13/17:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Neural Networks: Learning - &lt;a href="/home/week-5/lectures/pdf/Lecture9.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-5/lectures/ppt/Lecture9.pptx"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Exercise 4: Neural Networks Learning - &lt;a href="/home/week-5/exercises/machine-learning-ex4/ex4.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-5/exercises/machine-learning-ex4.zip"&gt;Problem&lt;/a&gt; - &lt;a href="/home/week-5/exercises/machine-learning-ex4/ex4"&gt;Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-5/lectures/notes.pdf"&gt;Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-5/errata.pdf"&gt;Errata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-4/exercises/Programming%20Ex.4.pdf"&gt;Program Exercise Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-week-6---due-082017" class="anchor" aria-hidden="true" href="#week-6---due-082017"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Week 6 - Due 08/20/17:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Advice for applying machine learning - &lt;a href="/home/week-6/lectures/pdf/Lecture10.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-6/lectures/ppt/Lecture10.pptx"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Machine learning system design - &lt;a href="/home/week-6/lectures/pdf/Lecture11.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-6/lectures/ppt/Lecture11.pptx"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Exercise 5: Regularized Linear Regression and Bias v.s. Variance - &lt;a href="/home/week-6/exercises/machine-learning-ex5/ex5.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-6/exercises/machine-learning-ex5.zip"&gt;Problem&lt;/a&gt; - &lt;a href="/home/week-6/exercises/machine-learning-ex5/ex5"&gt;Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-6/lectures/notes.pdf"&gt;Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-6/errata.pdf"&gt;Errata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-6/exercises/Programming%20Ex.5.pdf"&gt;Program Exercise Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-week-7---due-082717" class="anchor" aria-hidden="true" href="#week-7---due-082717"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Week 7 - Due 08/27/17:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Support vector machines - &lt;a href="/home/week-7/lectures/pdf/Lecture12.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-7/lectures/ppt/Lecture12.pptx"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Exercise 6: Support Vector Machines - &lt;a href="/home/week-7/exercises/machine-learning-ex6/ex6.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-7/exercises/machine-learning-ex6.zip"&gt;Problem&lt;/a&gt; - &lt;a href="/home/week-7/exercises/machine-learning-ex6/ex6"&gt;Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-7/lectures/notes.pdf"&gt;Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-7/errata.pdf"&gt;Errata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-7/exercises/Programming%20Ex.6.pdf"&gt;Program Exercise Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-week-8---due-090317" class="anchor" aria-hidden="true" href="#week-8---due-090317"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Week 8 - Due 09/03/17:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Clustering - &lt;a href="/home/week-8/lectures/pdf/Lecture13.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-8/lectures/ppt/Lecture13.ppt"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dimensionality reduction - &lt;a href="/home/week-8/lectures/pdf/Lecture14.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-8/lectures/ppt/Lecture14.ppt"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Exercise 7: K-means Clustering and Principal Component Analysis - &lt;a href="/home/week-8/exercises/machine-learning-ex7/ex7.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-8/exercises/machine-learning-ex7.zip"&gt;Problems&lt;/a&gt; - &lt;a href="/home/week-8/exercises/machine-learning-ex7/ex7"&gt;Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-8/lectures/notes.pdf"&gt;Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-8/errata.pdf"&gt;Errata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-8/exercises/Programming%20Ex.7.pdf"&gt;Program Exercise Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-week-9---due-091017" class="anchor" aria-hidden="true" href="#week-9---due-091017"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Week 9 - Due 09/10/17:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Anomaly Detection - &lt;a href="/home/week-9/lectures/pdf/Lecture15.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-9/lectures/ppt/Lecture15.ppt"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recommender Systems  - &lt;a href="/home/week-9/lectures/pdf/Lecture16.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-9/lectures/ppt/Lecture16.ppt"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Programming Exercise 8: Anomaly Detection and Recommender Systems - &lt;a href="/home/week-9/exercises/machine-learning-ex8/ex8.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-9/exercises/machine-learning-ex8.zip"&gt;Problems&lt;/a&gt; - &lt;a href="/home/week-9/exercises/machine-learning-ex8/ex8"&gt;Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-9/lectures/notes.pdf"&gt;Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-9/errata.pdf"&gt;Errata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-9/exercises/Programming%20Ex.8.pdf"&gt;Program Exercise Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-week-10---due-091717" class="anchor" aria-hidden="true" href="#week-10---due-091717"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Week 10 - Due 09/17/17:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Large scale machine learning - &lt;a href="/home/week-10/lectures/pdf/Lecture17.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-10/lectures/ppt/Lecture17.ppt"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/home/week-10/lectures/notes.pdf"&gt;Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-week-11---due-092417" class="anchor" aria-hidden="true" href="#week-11---due-092417"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Week 11 - Due 09/24/17:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Application example: Photo OCR - &lt;a href="/home/week-11/lectures/pdf/Lecture18.pdf"&gt;pdf&lt;/a&gt; - &lt;a href="/home/week-11/lectures/ppt/Lecture18.ppt"&gt;ppt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-extra-information" class="anchor" aria-hidden="true" href="#extra-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Extra Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/extra/cs229-linalg.pdf"&gt;Linear Algebra Review and Reference Zico Kolter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/extra/cs229-notes1.pdf"&gt;CS229 Lecture notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/extra/cs229-prob.pdf"&gt;CS229 Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/extra/machine%20learning%20stocks.pdf"&gt;Financial time series forecasting with machine learning techniques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/extra/octave_session.m"&gt;Octave Examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-online-e-books" class="anchor" aria-hidden="true" href="#online-e-books"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online E Books&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="robotics.stanford.edu/~nilsson/MLBOOK.pdf"&gt;Introduction to Machine Learning by Nils J. Nilsson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://alex.smola.org/drafts/thebook.pdf" rel="nofollow"&gt;Introduction to Machine Learning by Alex Smola and S.V.N. Vishwanathan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://surface.syr.edu/cgi/viewcontent.cgi?article=1165&amp;amp;context=istpub" rel="nofollow"&gt;Introduction to Data Science by Jeffrey Stanton&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online" rel="nofollow"&gt;Bayesian Reasoning and Machine Learning by David Barber&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/copy.html" rel="nofollow"&gt;Understanding Machine Learning, ¬© 2014 by Shai Shalev-Shwartz and Shai Ben-David&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/" rel="nofollow"&gt;Elements of Statistical Learning, by Hastie, Tibshirani, and Friedman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf" rel="nofollow"&gt;Pattern Recognition and Machine Learning, by Christopher M. Bishop&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-aditional-information" class="anchor" aria-hidden="true" href="#aditional-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Aditional Information&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-boom-course-status-point_down" class="anchor" aria-hidden="true" href="#boom-course-status-point_down"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="boom" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a5.png"&gt;üí•&lt;/g-emoji&gt; Course Status &lt;g-emoji class="g-emoji" alias="point_down" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f447.png"&gt;üëá&lt;/g-emoji&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/extra/img/coursera_course_completion.png"&gt;&lt;img src="/extra/img/coursera_course_completion.png" alt="coursera_course_completion" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/What-are-the-top-10-problems-in-deep-learning-for-2017" rel="nofollow"&gt;What are the top 10 problems in deep learning for 2017?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/When-will-the-deep-learning-bubble-burst" rel="nofollow"&gt;When will the deep learning bubble burst?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-statistics-models" class="anchor" aria-hidden="true" href="#statistics-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Statistics Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;HMM - &lt;a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" rel="nofollow"&gt;Hidden Markov Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CRFs - &lt;a href="https://en.wikipedia.org/wiki/Conditional_random_field" rel="nofollow"&gt;Conditional Random Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LSI - &lt;a href="https://www.searchenginejournal.com/what-is-latent-semantic-indexing-seo-defined/21642/" rel="nofollow"&gt;Latent Semantic Indexing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MRF - &lt;a href="https://en.wikipedia.org/wiki/Markov_random_field" rel="nofollow"&gt;Markov Random Fields&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-nlp-forums" class="anchor" aria-hidden="true" href="#nlp-forums"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP forums&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SIGIR - &lt;a href="http://sigir.org/" rel="nofollow"&gt;Special Interest Group on Information Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ACL - &lt;a href="https://www.aclweb.org/portal/" rel="nofollow"&gt;Association for Computational Linguistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NAACL - &lt;a href="http://naacl.org/" rel="nofollow"&gt;The North American Chapter of the Association for Computational Linguistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EMNLP - &lt;a href="http://emnlp2017.net/" rel="nofollow"&gt;Empirical Methods in Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NIPS - &lt;a href="https://nips.cc/" rel="nofollow"&gt;Neural Information Processing Systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>vkosuri</author><guid isPermaLink="false">https://github.com/vkosuri/CourseraMachineLearning</guid><pubDate>Sun, 19 Jan 2020 00:01:00 GMT</pubDate></item></channel></rss>