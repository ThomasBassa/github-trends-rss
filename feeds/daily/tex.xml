<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: TeX, Today</title><link>https://github.com/trending/tex?since=daily</link><description>The top repositories on GitHub for tex, measured daily</description><pubDate>Thu, 19 Dec 2019 04:56:06 GMT</pubDate><lastBuildDate>Thu, 19 Dec 2019 04:56:06 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>lib-pku/libpku #1 in TeX, Today</title><link>https://github.com/lib-pku/libpku</link><description>&lt;p&gt;&lt;i&gt;贵校课程资料民间整理&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-libpku---贵校课程资料民间整理" class="anchor" aria-hidden="true" href="#libpku---贵校课程资料民间整理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;libpku - 贵校课程资料民间整理&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-preface" class="anchor" aria-hidden="true" href="#preface"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preface&lt;/h2&gt;
&lt;p&gt;（引用自 &lt;a href="https://github.com/QSCTech/zju-icicles"&gt;QSCTech/zju-icicles&lt;/a&gt; ）&lt;/p&gt;
&lt;p&gt;来到一所大学，从第一次接触许多课，直到一门一门完成，这个过程中我们时常收集起许多资料和情报。&lt;/p&gt;
&lt;p&gt;有些是需要在网上搜索的电子书，每次见到一门新课程，Google 一下教材名称，有的可以立即找到，有的却是要花费许多眼力；有些是历年试卷或者 A4 纸，前人精心收集制作，抱着能对他人有用的想法公开，却需要在各个群或者私下中摸索以至于从学长手中代代相传；有些是上完一门课才恍然领悟的技巧，原来这门课重点如此，当初本可以更轻松地完成得更好……&lt;/p&gt;
&lt;p&gt;我也曾很努力地收集各种课程资料，但到最后，某些重要信息的得到却往往依然是纯属偶然。这种状态时常令我感到后怕与不安。我也曾在课程结束后终于有了些许方法与总结，但这些想法无处诉说，最终只能把花费时间与精力才换来的经验耗散在了漫漫的遗忘之中。&lt;/p&gt;
&lt;p&gt;我为这一年一年，这么多人孤军奋战的重复劳动感到不平。&lt;/p&gt;
&lt;p&gt;我希望能够将这些隐晦的、不确定的、口口相传的资料和经验，变为公开的、易于获取的和大家能够共同完善、积累的共享资料。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我希望只要是前人走过的弯路，后人就不必再走。&lt;/strong&gt; 这是我的信念，也是我建立这个项目的原因。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;使用方法：访问 &lt;a href="https://lib-pku.github.io/" rel="nofollow"&gt;https://lib-pku.github.io/&lt;/a&gt; ，点击资料链接即可下载。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://minhaskamal.github.io/DownGit/#/home" rel="nofollow"&gt;https://minhaskamal.github.io/DownGit/#/home&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;欢迎贡献！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;欢迎贡献！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;欢迎贡献！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;——因为很重要所以说了三遍&lt;/p&gt;
&lt;p&gt;Issue、PR、纠错、资料、选课/考试攻略，完全欢迎！&lt;/p&gt;
&lt;p&gt;来自大家的关注、维护和贡献，才是让这个攻略继续存在的动力~&lt;/p&gt;
&lt;p&gt;对于课程的评价可写在对应课程文件夹的 &lt;code&gt;README.md&lt;/code&gt; 中。如果想上传课件（请确保无版权问题），推荐使用 PDF 格式，避免系统差。&lt;/p&gt;
&lt;p&gt;由于本项目体积很大，故推荐采用在 &lt;strong&gt;GitHub Web 端直接上传&lt;/strong&gt; 的方式，具体操作如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;首先 Fork 本项目&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;上传文件到已有文件夹：打开对应文件夹，点击绿色 Download 按钮旁的 upload，上传你的文件。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传文件到新文件夹：打开任意文件夹，点击绿色 Download 按钮旁的 upload，&lt;strong&gt;把浏览器地址栏中文件夹名称改为你想要新建的文件夹名称，然后回车&lt;/strong&gt;，上传你的文件。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;提交 PR：Fork 本项目，然后在 GitHub 网页端点击 Upload File 上传文件，发起 PR 即可。留意一下项目的文件组织喔。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;或者也可以直接附加在 &lt;strong&gt;Issue&lt;/strong&gt; 中，由维护者进行添加。&lt;/p&gt;
&lt;p&gt;或者也可以发送邮件至 &lt;strong&gt;&lt;a href="mailto:libpku@protonmail.com"&gt;libpku@protonmail.com&lt;/a&gt;&lt;/strong&gt; ，由维护者进行添加。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;这不是北京大学图书馆。
我们也不对项目中信息的准确性或真实性做任何承诺。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果有侵权情况，麻烦您发送必要的信息至 &lt;a href="mailto:libpku@protonmail.com"&gt;libpku@protonmail.com&lt;/a&gt; ，带来不便还请您谅解。&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;资料来自网络，相关权利由原作者所有，这个 repo 仅用于收集现有资料。&lt;/p&gt;
&lt;p&gt;当然，我们不会为收集到的资料收费，或是尝试收取捐赠。&lt;/p&gt;
&lt;p&gt;我们只是尝试为后来的同学节省一些时间。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-related-works" class="anchor" aria-hidden="true" href="#related-works"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related Works&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/QSCTech/zju-icicles"&gt;浙江大学课程攻略共享计划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/martinwu42/project-hover"&gt;气垫船计划——免费、去中心化的北京大学往年题资料库&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/EECS-PKU-XSB/Shared-learning-materials"&gt;北京大学信科学生会学术部资料库&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tongtzeho/PKUCourse"&gt;北大计算机课程大作业&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PKUanonym/REKCARC-TSC-UHT"&gt;清华大学计算机系课程攻略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zjdx1998/seucourseshare"&gt;东南大学课程共享计划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/USTC-Resource/USTC-Course"&gt;中国科学技术大学计算机学院课程资源&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CoolPhilChen/SJTU-Courses/"&gt;上海交通大学课程资料分享&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sysuexam/SYSU-Exam"&gt;中山大学课程资料分享&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/idealclover/NJU-Review-Materials"&gt;南京大学课程复习资料&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CooperNiu/ZZU-Courses-Resource"&gt;郑州大学课程复习资料&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(more to be added....)&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>lib-pku</author><guid isPermaLink="false">https://github.com/lib-pku/libpku</guid><pubDate>Thu, 19 Dec 2019 00:01:00 GMT</pubDate></item><item><title>pingcap/docs-cn #2 in TeX, Today</title><link>https://github.com/pingcap/docs-cn</link><description>&lt;p&gt;&lt;i&gt;TiDB/TiKV/PD documents in Chinese.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;table data-table-type="yaml-metadata"&gt;
  &lt;thead&gt;
  &lt;tr&gt;
  &lt;th&gt;draft&lt;/th&gt;
  &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
  &lt;tr&gt;
  &lt;td&gt;&lt;div&gt;true&lt;/div&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1&gt;&lt;a id="user-content-tidb-简介" class="anchor" aria-hidden="true" href="#tidb-简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TiDB 简介&lt;/h1&gt;
&lt;p&gt;TiDB 是 PingCAP 公司设计的开源分布式 HTAP (Hybrid Transactional and Analytical Processing) 数据库，结合了传统的 RDBMS 和 NoSQL 的最佳特性。TiDB 兼容 MySQL，支持无限的水平扩展，具备强一致性和高可用性。TiDB 的目标是为 OLTP (Online Transactional Processing) 和 OLAP (Online Analytical Processing) 场景提供一站式的解决方案。&lt;/p&gt;
&lt;p&gt;TiDB 具备如下特性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;高度兼容 MySQL&lt;/p&gt;
&lt;p&gt;&lt;a href="/v3.0/reference/mysql-compatibility.md"&gt;大多数情况下&lt;/a&gt;，无需修改代码即可从 MySQL 轻松迁移至 TiDB，分库分表后的 MySQL 集群亦可通过 TiDB 工具进行实时迁移。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;水平弹性扩展&lt;/p&gt;
&lt;p&gt;通过简单地增加新节点即可实现 TiDB 的水平扩展，按需扩展吞吐或存储，轻松应对高并发、海量数据场景。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分布式事务&lt;/p&gt;
&lt;p&gt;TiDB 100% 支持标准的 ACID 事务。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;真正金融级高可用&lt;/p&gt;
&lt;p&gt;相比于传统主从 (M-S) 复制方案，基于 Raft 的多数派选举协议可以提供金融级的 100% 数据强一致性保证，且在不丢失大多数副本的前提下，可以实现故障的自动恢复 (auto-failover)，无需人工介入。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一站式 HTAP 解决方案&lt;/p&gt;
&lt;p&gt;TiDB 作为典型的 OLTP 行存数据库，同时兼具强大的 OLAP 性能，配合 TiSpark，可提供一站式 HTAP 解决方案，一份存储同时处理 OLTP &amp;amp; OLAP，无需传统繁琐的 ETL 过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;云原生 SQL 数据库&lt;/p&gt;
&lt;p&gt;TiDB 是为云而设计的数据库，支持公有云、私有云和混合云，使部署、配置和维护变得十分简单。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TiDB 的设计目标是 100% 的 OLTP 场景和 80% 的 OLAP 场景，更复杂的 OLAP 分析可以通过 &lt;a href="/v3.0/reference/tispark.md"&gt;TiSpark 项目&lt;/a&gt;来完成。&lt;/p&gt;
&lt;p&gt;TiDB 对业务没有任何侵入性，能优雅的替换传统的数据库中间件、数据库分库分表等 Sharding 方案。同时它也让开发运维人员不用关注数据库 Scale 的细节问题，专注于业务开发，极大的提升研发的生产力。&lt;/p&gt;
&lt;p&gt;三篇文章了解 TiDB 技术内幕：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-1/" rel="nofollow"&gt;说存储&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-2/" rel="nofollow"&gt;说计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-3/" rel="nofollow"&gt;谈调度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pingcap</author><guid isPermaLink="false">https://github.com/pingcap/docs-cn</guid><pubDate>Thu, 19 Dec 2019 00:02:00 GMT</pubDate></item><item><title>exacity/deeplearningbook-chinese #3 in TeX, Today</title><link>https://github.com/exacity/deeplearningbook-chinese</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Book Chinese Translation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-中文翻译" class="anchor" aria-hidden="true" href="#deep-learning-中文翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning 中文翻译&lt;/h1&gt;
&lt;p&gt;在众多网友的帮助和校对下，中文版终于出版了。尽管还有很多问题，但至少90%的内容是可读的，并且是准确的。
我们尽可能地保留了原书&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;Deep Learning&lt;/a&gt;中的意思并保留原书的语句。&lt;/p&gt;
&lt;p&gt;然而我们水平有限，我们无法消除众多读者的方差。我们仍需要大家的建议和帮助，一起减小翻译的偏差。&lt;/p&gt;
&lt;p&gt;大家所要做的就是阅读，然后汇总你的建议，提issue（最好不要一个一个地提）。如果你确定你的建议不需要商量，可以直接发起PR。&lt;/p&gt;
&lt;p&gt;对应的翻译者：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第1、4、7、10、14、20章及第12.4、12.5节由 @swordyork 负责&lt;/li&gt;
&lt;li&gt;第2、5、8、11、15、18章由 @liber145 负责&lt;/li&gt;
&lt;li&gt;第3、6、9章由 @KevinLee1110 负责&lt;/li&gt;
&lt;li&gt;第13、16、17、19章及第12.1至12.3节由 @futianfan 负责&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-面向的读者" class="anchor" aria-hidden="true" href="#面向的读者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;面向的读者&lt;/h2&gt;
&lt;p&gt;请直接下载&lt;a href="https://github.com/exacity/deeplearningbook-chinese/releases/download/v0.5-beta/dlbook_cn_v0.5-beta.pdf"&gt;PDF&lt;/a&gt;阅读。
不打算提供EPUB等格式，如有需要请自行修改。&lt;/p&gt;
&lt;p&gt;这一版准确性已经有所提高，读者可以以中文版为主、英文版为辅来阅读学习，但我们仍建议研究者阅读&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;原版&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-出版及开源原因" class="anchor" aria-hidden="true" href="#出版及开源原因"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;出版及开源原因&lt;/h2&gt;
&lt;p&gt;本书由人民邮电出版社出版，如果你觉得中文版PDF对你有所帮助，希望你能支持下纸质正版书籍。
如果你觉得中文版不行，希望你能多提建议。非常感谢各位！
纸质版也会进一步更新，需要大家更多的建议和意见，一起完善中文版。&lt;/p&gt;
&lt;p&gt;纸质版目前在人民邮电出版社的异步社区出售，见&lt;a href="http://www.epubit.com.cn/book/details/4278" rel="nofollow"&gt;地址&lt;/a&gt;。
价格不低，但看了样本之后，我们认为物有所值。
注意，我们不会通过媒体进行宣传，希望大家先看电子版内容，再判断是否购买纸质版。&lt;/p&gt;
&lt;p&gt;以下是开源的具体原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们不是文学工作者，不专职翻译。单靠我们，无法给出今天的翻译，众多网友都给我们提出了宝贵的建议，因此开源帮了很大的忙。出版社会给我们稿费（我们也不知道多少，可能2万左右），我们也不好意思自己用，商量之后觉得捐出是最合适的，以所有贡献过的网友的名义（我们把稿费捐给了杉树公益，用于4名贵州高中生三年的生活费，见&lt;a href="https://github.com/exacity/deeplearningbook-chinese/blob/master/donation.pdf"&gt;捐赠情况&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;PDF电子版对于技术类书籍来说是很重要的，随时需要查询，拿着纸质版到处走显然不合适。国外很多技术书籍都有对应的电子版（虽然不一定是正版），而国内的几乎没有。个人认为这是出版社或者作者认为国民素质还没有高到主动为知识付费的境界，所以不愿意"泄露"电子版。时代在进步，我们也需要改变。特别是翻译作品普遍质量不高的情况下，要敢为天下先。&lt;/li&gt;
&lt;li&gt;深度学习发展太快，日新月异，所以我们希望大家更早地学到相关的知识。我觉得原作者开放PDF电子版也有类似的考虑，也就是先阅读后付费。我们认为中国人口素质已经足够高，懂得为知识付费。当然这不是付给我们的，是付给出版社的，出版社再付给原作者。我们不希望中文版的销量因PDF电子版的存在而下滑。出版社只有值回了版权才能在以后引进更多的优秀书籍。我们这个开源翻译先例也不会成为一个反面案例，以后才会有更多的PDF电子版。&lt;/li&gt;
&lt;li&gt;开源也涉及版权问题，出于版权原因，我们不再更新此初版PDF文件，请大家以最终的纸质版为准。（但源码会一直更新）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-致谢" class="anchor" aria-hidden="true" href="#致谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;致谢&lt;/h2&gt;
&lt;p&gt;我们有3个类别的校对人员。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;负责人也就是对应的翻译者。&lt;/li&gt;
&lt;li&gt;简单阅读，对语句不通顺或难以理解的地方提出修改意见。&lt;/li&gt;
&lt;li&gt;中英对比，进行中英对应阅读，排除少翻错翻的情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有校对建议都保存在各章的&lt;code&gt;annotations.txt&lt;/code&gt;文件中。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;章节&lt;/th&gt;
&lt;th&gt;负责人&lt;/th&gt;
&lt;th&gt;简单阅读&lt;/th&gt;
&lt;th&gt;中英对比&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter1_introduction/" rel="nofollow"&gt;第一章 前言&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc, @SiriusXDJ, @corenel, @NeutronT&lt;/td&gt;
&lt;td&gt;@linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter2_linear_algebra/" rel="nofollow"&gt;第二章 线性代数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@SiriusXDJ, @angrymidiao&lt;/td&gt;
&lt;td&gt;@badpoem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter3_probability_and_information_theory/" rel="nofollow"&gt;第三章 概率与信息论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@SiriusXDJ&lt;/td&gt;
&lt;td&gt;@kkpoker, @Peiyan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter4_numerical_computation/" rel="nofollow"&gt;第四章 数值计算&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;@zhangyafeikimi&lt;/td&gt;
&lt;td&gt;@hengqujushi&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter5_machine_learning_basics/" rel="nofollow"&gt;第五章 机器学习基础&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@wheaio, @huangpingchun&lt;/td&gt;
&lt;td&gt;@fairmiracle, @linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter6_deep_feedforward_networks/" rel="nofollow"&gt;第六章 深度前馈网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;David_Chow, @linzhp, @sailordiary&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter7_regularization/" rel="nofollow"&gt;第七章 深度学习中的正则化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@NBZCC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/" rel="nofollow"&gt;第八章 深度模型中的优化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@happynoom, @codeVerySlow&lt;/td&gt;
&lt;td&gt;@huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter9_convolutional_networks/" rel="nofollow"&gt;第九章 卷积网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @corenel&lt;/td&gt;
&lt;td&gt;@zhiding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter10_sequence_modeling_rnn/" rel="nofollow"&gt;第十章 序列建模：循环和递归网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @yinruiqing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter11_practical_methodology/" rel="nofollow"&gt;第十一章 实践方法论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter12_applications/" rel="nofollow"&gt;第十二章 应用&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork, @futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@corenel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter13_linear_factor_models/" rel="nofollow"&gt;第十三章 线性因子模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;@cloudygoose&lt;/td&gt;
&lt;td&gt;@ZhiweiYang&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter14_autoencoders/" rel="nofollow"&gt;第十四章 自编码器&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@Seaball, @huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter15_representation_learning/" rel="nofollow"&gt;第十五章 表示学习&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@cnscottzheng&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter16_structured_probabilistic_modelling/" rel="nofollow"&gt;第十六章 深度学习中的结构化概率模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter17_monte_carlo_methods/" rel="nofollow"&gt;第十七章 蒙特卡罗方法&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter18_confronting_the_partition_function/" rel="nofollow"&gt;第十八章 面对配分函数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@tankeco&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter19_approximate_inference/" rel="nofollow"&gt;第十九章 近似推断&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary, @hengqujushi, huanghaojun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter20_deep_generative_models/" rel="nofollow"&gt;第二十章 深度生成模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;参考文献&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@pkuwwt&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我们会在纸质版正式出版的时候，在书中致谢，正式感谢各位作出贡献的同学！&lt;/p&gt;
&lt;p&gt;还有很多同学提出了不少建议，我们都列在此处。&lt;/p&gt;
&lt;p&gt;@tttwwy @tankeco @fairmiracle @GageGao @huangpingchun @MaHongP @acgtyrant @yanhuibin315 @Buttonwood @titicacafz
@weijy026a @RuiZhang1993 @zymiboxpay @xingkongliang @oisc @tielei @yuduowu @Qingmu @HC-2016 @xiaomingabc
@bengordai @Bojian @JoyFYan @minoriwww @khty2000 @gump88 @zdx3578 @PassStory @imwebson @wlbksy @roachsinai @Elvinczp
@endymecy name:YUE-DaJiong @9578577 @linzhp @cnscottzheng @germany-zhu  @zhangyafeikimi @showgood163 @gump88
@kangqf @NeutronT @badpoem @kkpoker @Seaball @wheaio @angrymidiao @ZhiweiYang @corenel @zhaoyu611 @SiriusXDJ @dfcv24 EmisXXY
FlyingFire vsooda @friskit-china @poerin @ninesunqian @JiaqiYao @Sofring @wenlei @wizyoung @imageslr @@indam @XuLYC
@zhouqingping @freedomRen @runPenguin @pkuwwt @wuqi @tjliupeng @neo0801 @jt827859032 @demolpc @fishInAPool
@xiaolangyuxin @jzj1993 @whatbeg LongXiaJun jzd&lt;/p&gt;
&lt;p&gt;如有遗漏，请务必通知我们，可以发邮件至&lt;code&gt;echo c3dvcmQueW9ya0BnbWFpbC5jb20K | base64 --decode&lt;/code&gt;。
这是我们必须要感谢的，所以不要不好意思。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;排版&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-注意" class="anchor" aria-hidden="true" href="#注意"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;注意&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;各种问题或者建议可以提issue，建议使用中文。&lt;/li&gt;
&lt;li&gt;由于版权问题，我们不能将图片和bib上传，请见谅。&lt;/li&gt;
&lt;li&gt;Due to copyright issues, we would not upload figures and the bib file.&lt;/li&gt;
&lt;li&gt;可用于学习研究目的，不得用于任何商业行为。谢谢！&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-markdown格式" class="anchor" aria-hidden="true" href="#markdown格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Markdown格式&lt;/h2&gt;
&lt;p&gt;这种格式确实比较重要，方便查阅，也方便索引。初步转换后，生成网页，具体见&lt;a href="https://exacity.github.io/deeplearningbook-chinese" rel="nofollow"&gt;deeplearningbook-chinese&lt;/a&gt;。
注意，这种转换没有把图放进去，也不会放图。目前使用单个&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;，基于latex文件转换，以后可能会更改但原则是不直接修改&lt;a href="docs/_posts"&gt;md文件&lt;/a&gt;。
需要的同学可以自行修改&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-html格式" class="anchor" aria-hidden="true" href="#html格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTML格式&lt;/h2&gt;
&lt;p&gt;读者可以使用&lt;a href="https://github.com/coolwanglu/pdf2htmlEX"&gt;pdf2htmlEX&lt;/a&gt;进行转换，直接将PDF转换为HTML。&lt;/p&gt;
&lt;p&gt;Updating.....&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>exacity</author><guid isPermaLink="false">https://github.com/exacity/deeplearningbook-chinese</guid><pubDate>Thu, 19 Dec 2019 00:03:00 GMT</pubDate></item><item><title>tuhdo/os01 #4 in TeX, Today</title><link>https://github.com/tuhdo/os01</link><description>&lt;p&gt;&lt;i&gt;Bootstrap yourself to write an OS from scratch. A book for self-learner.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://www.paypal.com/cgi-bin/webscr?cmd=_donations&amp;amp;business=tuhdo1710%40gmail%2ecom&amp;amp;lc=VN&amp;amp;item_number=tuhdo&amp;amp;currency_code=USD&amp;amp;bn=PP%2dDonationsBF%3aDonate%2dPayPal%2dgreen%2esvg%3aNonHosted" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d5d24e33e2f4b6fe53987419a21b203c03789a8f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d677265656e2e737667" alt="Donate" data-canonical-src="https://img.shields.io/badge/Donate-PayPal-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-operating-systems-from-0-to-1" class="anchor" aria-hidden="true" href="#operating-systems-from-0-to-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://tuhdo.github.io/os01/" rel="nofollow"&gt;Operating Systems: From 0 to 1&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;This book helps you gain the foundational knowledge required to write an
operating system from scratch. Hence the title, 0 to 1.&lt;/p&gt;
&lt;p&gt;After completing this book, at the very least you will learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;How to write an operating system from scratch by reading hardware datasheets.
In the real world, it works like that. You won't be able to consult Google for
a quick answer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A big picture of how each layer of a computer is related to the other, from hardware to software.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Write code independently. It's pointless to copy and paste code. Real learning
happens when you solve problems on your own. Some examples are given to kick
start, but most problems are yours to conquer. However, the solutions are
available online for you to examine after giving it a good try.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Linux as a development environment and how to use common tools for low-level
programming.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;x86 assembly in-depth.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How a program is structured so that an operating system can run.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to debug a program running directly on hardware with gdb and QEMU.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Linking and loading on bare metal x86_64, with pure C. No standard library. No
runtime overhead.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/tuhdo/os01/blob/master/Operating_Systems_From_0_to_1.pdf"&gt;Download the book&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-the-pedagogy-of-the-book" class="anchor" aria-hidden="true" href="#the-pedagogy-of-the-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The pedagogy of the book&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;You give a poor man a fish and you feed him for a day. You teach him to fish
and you give him an occupation that will feed him for a lifetime.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This has been the guiding principle of the book when I was writing it. The book does
not try to teach you everything, but enough to enable you to learn by yourself.
The book itself, at this point, is quite "complete": once you master part 1 and
part 2 (which consist of 8 chapters), you can drop the book and learn by
yourself. At this point, smart readers should be able to continue on their own.
For example, they can continue their journeys
on &lt;a href="http://wiki.osdev.org/Main_Page" rel="nofollow"&gt;OSDev wiki&lt;/a&gt;; in fact, after you study
everything in part 1 and part 2, you only meet
the &lt;a href="http://wiki.osdev.org/Required_Knowledge" rel="nofollow"&gt;minimum requirement&lt;/a&gt; by OSDev
Wiki (well, not quite, the book actually goes deeper for the suggested topics).
Or, if you consider developing an OS for fun is impractical, you can continue
with a Linux-specific book, such as this free
book &lt;a href="https://0xax.gitbooks.io/linux-insides/content/" rel="nofollow"&gt;Linux Insides&lt;/a&gt;, or other
popular Linux kernel books. The book tries hard to provide you a strong
foundation, and that's why part 1 and part 2 were released first.&lt;/p&gt;
&lt;p&gt;The book teaches you core concepts, such as x86 Assembly, ELF, linking and
debugging on bare metal, etc., but more importantly, where such information
come from. For example, instead of just teaching x86 Assembly, it also teaches
how to use reference manuals from Intel. Learning to read the official
manuals is important because only the hardware manufacturers themselves
understand how their hardware work. If you only learn from the secondary
resources because it is easier, you will never gain a complete understanding of
the hardware you are programming for. Have you ever read a book on Assembly, and
wondered where all the information came from? How does the author know
everything he says is correct? And how one seems to magically know so much about
hardware programming? This book gives pointers to such questions.&lt;/p&gt;
&lt;p&gt;As an example, you should skim through chapter 4, "x86 Assembly and C", to see
how it makes use of the Intel manual, Volume 2. And in
the process, it guides you how to use the official manuals.&lt;/p&gt;
&lt;p&gt;Part 3 is planned as a series of specifications that a reader will implement to
complete each operating system component. It does not contain code aside from a
few examples. Part 3 is just there to shorten the reader's time when reading the
official manuals by giving hints where to read, explaining difficult concepts
and how to use the manuals to debug. In short, the implementation is up to the
reader to work on his or her own; the chapters are just like university assignments.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h1&gt;
&lt;p&gt;Know some circuit concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basic Concepts of Electricity: atoms, electrons, protons, neutrons, current flow.&lt;/li&gt;
&lt;li&gt;Ohm's law&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, if you know absolutely nothing about electricity, you can quickly learn it here:
&lt;a href="http://www.allaboutcircuits.com/textbook/" rel="nofollow"&gt;http://www.allaboutcircuits.com/textbook/&lt;/a&gt;, by reading chapter 1 and chapter 2.&lt;/p&gt;
&lt;p&gt;C programming. In particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Variable and function declarations/definitions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;While and for loops&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pointers and function pointers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fundamental algorithms and data structures in C&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Linux basics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Know how to navigate directory with the command line&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Know how to invoke a command with options&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Know how to pipe output to another program&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Touch typing. Since we are going to use Linux, touch typing helps. I know typing
speed does not relate to problem-solving, but at least your typing speed should
be fast enough not to let it get it the way and degrade the learning experience.&lt;/p&gt;
&lt;p&gt;In general, I assume that the reader has basic C programming knowledge, and can
use an IDE to build and run a program.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-status" class="anchor" aria-hidden="true" href="#status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Status:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Part 1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 1: Complete&lt;/li&gt;
&lt;li&gt;Chapter 2: Complete&lt;/li&gt;
&lt;li&gt;Chapter 3: Almost. Currently, the book relies on the Intel Manual for fully explaining x86 execution environment.&lt;/li&gt;
&lt;li&gt;Chapter 4: Complete&lt;/li&gt;
&lt;li&gt;Chapter 5: Complete&lt;/li&gt;
&lt;li&gt;Chapter 6: Complete&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Part 2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 7: Complete&lt;/li&gt;
&lt;li&gt;Chapter 8: Complete&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Part 3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 9: Incomplete&lt;/li&gt;
&lt;li&gt;Chapter 10: Incomplete&lt;/li&gt;
&lt;li&gt;Chapter 11: Incomplete&lt;/li&gt;
&lt;li&gt;Chapter 12: Incomplete&lt;/li&gt;
&lt;li&gt;Chapter 13: Incomplete&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;... and future chapters not included yet ...&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the future, I hope to expand part 3 to cover more than the first 2 parts. But
for the time being, I will try to finish the above chapters first.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-sample-os" class="anchor" aria-hidden="true" href="#sample-os"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample OS&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/tuhdo/sample-os"&gt;This repository&lt;/a&gt; is the sample OS of the
book that is intended as a reference material for part 3. It covers 10 chapters
of the "System Programming Guide" (Intel Manual Volume 3), along with a simple
keyboard and video driver for input and output. However, at the moment, only the
following features are implemented:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Protected mode.&lt;/li&gt;
&lt;li&gt;Creating and managing processes with TSS (Task State Structure).&lt;/li&gt;
&lt;li&gt;Interrupts&lt;/li&gt;
&lt;li&gt;LAPIC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Paging and I/O are not yet implemented. I will try to implement it as the book progresses.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h1&gt;
&lt;p&gt;If you find any grammatical issues, please report it using Github Issues. Or, if
some sentence or paragraph is difficult to understand, feel free to open an
issue with the following title format: &lt;code&gt;[page number][type] Descriptive Title&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example: &lt;code&gt;[pg.9][grammar] Incorrect verb usage&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;type&lt;/code&gt; can be one of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Typo&lt;/code&gt;: indicates typing mistake.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Grammar&lt;/code&gt;: indicates incorrect grammar usage.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Style&lt;/code&gt;: indicates a style improvement.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Content&lt;/code&gt;: indicates problems with the content.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even better, you can make a pull request with the provided book source. The main
content of the book is in the file "Operating Systems: From 0 to 1.lyx". You can
edit the .txt file, then I will integrate the changes manually. It is a
workaround for now since Lyx can cause a huge diff which makes it impossible to
review changes.&lt;/p&gt;
&lt;p&gt;The book is in development, so please bear with me if the English irritates you.
I really appreciate it.&lt;/p&gt;
&lt;p&gt;Finally, if you like the project and if it is possible, please donate to help
this project and keep it going.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-got-questions" class="anchor" aria-hidden="true" href="#got-questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Got questions?&lt;/h1&gt;
&lt;p&gt;If you have any question related to the material or the development of the book,
feel free to &lt;a href="https://github.com/tuhdo/os01/issues/new"&gt;open a Github issue&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tuhdo</author><guid isPermaLink="false">https://github.com/tuhdo/os01</guid><pubDate>Thu, 19 Dec 2019 00:04:00 GMT</pubDate></item><item><title>popsim-consortium/manuscript #5 in TeX, Today</title><link>https://github.com/popsim-consortium/manuscript</link><description>&lt;p&gt;&lt;i&gt;PopSim consortium manuscript&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-manuscript" class="anchor" aria-hidden="true" href="#manuscript"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;manuscript&lt;/h1&gt;
&lt;p&gt;PopSim consortium manuscript&lt;/p&gt;
&lt;p&gt;To compile the manuscript, inside the &lt;code&gt;manuscript&lt;/code&gt; directory,
first clean the directory with &lt;code&gt;make clean&lt;/code&gt;,
then compile with &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>popsim-consortium</author><guid isPermaLink="false">https://github.com/popsim-consortium/manuscript</guid><pubDate>Thu, 19 Dec 2019 00:05:00 GMT</pubDate></item><item><title>hak7a3/raytracing-tex #6 in TeX, Today</title><link>https://github.com/hak7a3/raytracing-tex</link><description>&lt;p&gt;&lt;i&gt;simple ray tracing by TeX&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;This repo does not have a README.&lt;/i&gt;&lt;/p&gt;</description><author>hak7a3</author><guid isPermaLink="false">https://github.com/hak7a3/raytracing-tex</guid><pubDate>Thu, 19 Dec 2019 00:06:00 GMT</pubDate></item><item><title>posquit0/Awesome-CV #7 in TeX, Today</title><link>https://github.com/posquit0/Awesome-CV</link><description>&lt;p&gt;&lt;i&gt;:page_facing_up: Awesome CV is LaTeX template for your outstanding job application&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1 align="center"&gt;&lt;a id="user-content-------------awesome-cv" class="anchor" aria-hidden="true" href="#------------awesome-cv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
  &lt;a href="https://github.com/posquit0/Awesome-CV" title="AwesomeCV Documentation"&gt;
    &lt;img alt="AwesomeCV" src="https://github.com/posquit0/Awesome-CV/raw/master/icon.png" width="200px" height="200px" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;br&gt;
  Awesome CV
&lt;/h1&gt;
&lt;p align="center"&gt;
  LaTeX template for your outstanding job application
&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a href="https://www.paypal.me/posquit0" rel="nofollow"&gt;
    &lt;img alt="Donate" src="https://camo.githubusercontent.com/abbdd7bf97ae7919db5962b255f40aded5189c4f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d626c75652e737667" data-canonical-src="https://img.shields.io/badge/Donate-PayPal-blue.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://circleci.com/gh/posquit0/Awesome-CV" rel="nofollow"&gt;
    &lt;img alt="CircleCI" src="https://camo.githubusercontent.com/d42593802854990d35ca42943e478dd35d6c64c9/68747470733a2f2f636972636c6563692e636f6d2f67682f706f7371756974302f417765736f6d652d43562e7376673f7374796c653d736869656c64" data-canonical-src="https://circleci.com/gh/posquit0/Awesome-CV.svg?style=shield" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;
    &lt;img alt="Example Resume" src="https://camo.githubusercontent.com/836d3a9f44da3462e5c47b6c58bf066bffbaf739/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f726573756d652d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/resume-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/cv.pdf" rel="nofollow"&gt;
    &lt;img alt="Example CV" src="https://camo.githubusercontent.com/8afab53a91bc30d0da18a9ea0cc70f2d0a1571df/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f63762d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/cv-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;
    &lt;img alt="Example Coverletter" src="https://camo.githubusercontent.com/ce88ed0c1af9e5611df67818460447b69572ae9d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f7665726c65747465722d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/coverletter-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-awesome-cv" class="anchor" aria-hidden="true" href="#what-is-awesome-cv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is Awesome CV?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Awesome CV&lt;/strong&gt; is LaTeX template for a &lt;strong&gt;CV(Curriculum Vitae)&lt;/strong&gt;, &lt;strong&gt;Résumé&lt;/strong&gt; or &lt;strong&gt;Cover Letter&lt;/strong&gt; inspired by &lt;a href="https://www.sharelatex.com/templates/cv-or-resume/fancy-cv" rel="nofollow"&gt;Fancy CV&lt;/a&gt;. It is easy to customize your own template, especially since it is really written by a clean, semantic markup.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-donate" class="anchor" aria-hidden="true" href="#donate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Donate&lt;/h2&gt;
&lt;p&gt;Please help keep this project alive! Donations are welcome and will go towards further development of this project.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PayPal: paypal.me/posquit0
BTC: 1Je3DxJVM2a9nTVPNo55SfQwpmxA6N2KKb
BCH: 1Mg1wG7PwHGrHYSWS67TsGSjo5GHEVbF16
ETH: 0x77ED9B4659F80205E9B9C9FB1E26EDB9904AFCC7
QTUM: QZT7D6m3QtTTqp7s4ZWAwLtGDsoHMMaM8E
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Thank you for your support!&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-résumé" class="anchor" aria-hidden="true" href="#résumé"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Résumé&lt;/h4&gt;
&lt;p&gt;You can see &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Page. 1&lt;/th&gt;
&lt;th align="center"&gt;Page. 2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-0.png" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-1.png" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-cover-letter" class="anchor" aria-hidden="true" href="#cover-letter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cover Letter&lt;/h4&gt;
&lt;p&gt;You can see &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Without Sections&lt;/th&gt;
&lt;th align="center"&gt;With Sections&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-0.png" alt="Cover Letter(Traditional)" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-1.png" alt="Cover Letter(Awesome)" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.overleaf.com/latex/templates/awesome-cv/tvmzpvdjfqxp" rel="nofollow"&gt;&lt;strong&gt;Edit Résumé on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.overleaf.com/latex/templates/awesome-cv-cover-letter/pfzzjspkthbk" rel="nofollow"&gt;&lt;strong&gt;Edit Cover Letter on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note:&lt;/em&gt; Above services do not guarantee up-to-date source code of Awesome CV&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Use&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h4&gt;
&lt;p&gt;A full TeX distribution is assumed.  &lt;a href="http://tex.stackexchange.com/q/55437" rel="nofollow"&gt;Various distributions for different operating systems (Windows, Mac, *nix) are available&lt;/a&gt; but TeX Live is recommended.
You can &lt;a href="http://tex.stackexchange.com/q/1092" rel="nofollow"&gt;install TeX from upstream&lt;/a&gt; (recommended; most up-to-date) or use &lt;code&gt;sudo apt-get install texlive-full&lt;/code&gt; if you really want that.  (It's generally a few years behind.)&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h4&gt;
&lt;p&gt;At a command prompt, run&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ xelatex {your-cv}.tex&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This should result in the creation of &lt;code&gt;{your-cv}.pdf&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-credit" class="anchor" aria-hidden="true" href="#credit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credit&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.latex-project.org" rel="nofollow"&gt;&lt;strong&gt;LaTeX&lt;/strong&gt;&lt;/a&gt; is a fantastic typesetting program that a lot of people use these days, especially the math and computer science people in academia.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/furl/latex-fontawesome"&gt;&lt;strong&gt;LaTeX FontAwesome&lt;/strong&gt;&lt;/a&gt; is bindings for FontAwesome icons to be used in XeLaTeX.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/google/roboto"&gt;&lt;strong&gt;Roboto&lt;/strong&gt;&lt;/a&gt; is the default font on Android and ChromeOS, and the recommended font for Google’s visual language, Material Design.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/adobe-fonts/source-sans-pro"&gt;&lt;strong&gt;Source Sans Pro&lt;/strong&gt;&lt;/a&gt; is a set of OpenType fonts that have been designed to work well in user interface (UI) environments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;You are free to take my &lt;code&gt;.tex&lt;/code&gt; file and modify it to create your own resume. Please don't use my resume for anything else without my permission, though!&lt;/p&gt;
&lt;p&gt;If you have any questions, feel free to join me at &lt;code&gt;#posquit0&lt;/code&gt; on Freenode and ask away. Click &lt;a href="https://kiwiirc.com/client/irc.freenode.net/posquit0" rel="nofollow"&gt;here&lt;/a&gt; to connect.&lt;/p&gt;
&lt;p&gt;Good luck!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-see-also" class="anchor" aria-hidden="true" href="#see-also"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;See Also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/posquit0/hugo-awesome-identity"&gt;Awesome Identity&lt;/a&gt; - A single-page Hugo theme to introduce yourself.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>posquit0</author><guid isPermaLink="false">https://github.com/posquit0/Awesome-CV</guid><pubDate>Thu, 19 Dec 2019 00:07:00 GMT</pubDate></item><item><title>albarqouni/Deep-Learning-for-Medical-Applications #8 in TeX, Today</title><link>https://github.com/albarqouni/Deep-Learning-for-Medical-Applications</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Papers on Medical Image Analysis&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-papers-on-medical-image-analysis" class="anchor" aria-hidden="true" href="#deep-learning-papers-on-medical-image-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning Papers on Medical Image Analysis&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-background" class="anchor" aria-hidden="true" href="#background"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Background&lt;/h2&gt;
&lt;p&gt;To the best of our knowledge, this is the first list of deep learning papers on medical applications. There are couple of lists for deep learning papers in general, or computer vision, for example &lt;a href="https://github.com/terryum/awesome-deep-learning-papers.git"&gt;Awesome Deep Learning Papers&lt;/a&gt;. In this list, I try to classify the papers based on their deep learning techniques and learning methodology. I believe this list could be a good starting point for DL researchers on Medical Applications.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-criteria" class="anchor" aria-hidden="true" href="#criteria"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Criteria&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A list of &lt;strong&gt;top deep learning papers&lt;/strong&gt; published since 2015.&lt;/li&gt;
&lt;li&gt;Papers are collected from peer-reviewed journals and high reputed conferences. However, it may have recent papers on arXiv.&lt;/li&gt;
&lt;li&gt;A meta-data is required along with the paper, i.e. Deep Learning technique, Imaging Modality, Area of Interest, Clinical Database (DB).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;List of Journals / Conferences (J/C):&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.journals.elsevier.com/medical-image-analysis/" rel="nofollow"&gt;Medical Image Analysis (MedIA)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://ieee-tmi.org/" rel="nofollow"&gt;IEEE Transaction on Medical Imaging (IEEE-TMI)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="http://tbme.embs.org/" rel="nofollow"&gt;IEEE Transaction on Biomedical Engineering (IEEE-TBME)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="http://jbhi.embs.org/" rel="nofollow"&gt;IEEE Journal of Biomedical and Health Informatics (IEEE-JBHI)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="http://www.springer.com/medicine/radiology/journal/11548" rel="nofollow"&gt;International Journal on Computer Assisted Radiology and Surgery (IJCARS)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;International Conference on Information Processing in Medical Imaging (IPMI)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;International Conference on Information Processing in Computer-Assisted Interventions (IPCAI)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IEEE International Symposium on Biomedical Imaging (ISBI)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-shortcuts" class="anchor" aria-hidden="true" href="#shortcuts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Shortcuts&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Deep Learning Techniques:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NN: Neural Networks&lt;/li&gt;
&lt;li&gt;MLP: Multilayer Perceptron&lt;/li&gt;
&lt;li&gt;RBM: Restricted Boltzmann Machine&lt;/li&gt;
&lt;li&gt;SAE: Stacked Auto-Encoders&lt;/li&gt;
&lt;li&gt;CAE: Convolutional Auto-Encoders&lt;/li&gt;
&lt;li&gt;CNN: Convolutional Neural Networks&lt;/li&gt;
&lt;li&gt;RNN: Recurrent Neural Networks&lt;/li&gt;
&lt;li&gt;LSTM: Long Short Term Memory&lt;/li&gt;
&lt;li&gt;M-CNN: Multi-Scale/View/Stream CNN&lt;/li&gt;
&lt;li&gt;MIL-CNN: Multi-instance Learning CNN&lt;/li&gt;
&lt;li&gt;FCN: Fully Convolutional Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Imaging Modality:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;US: Ultrasound&lt;/li&gt;
&lt;li&gt;MR/MRI: Magnetic Resonance Imaging&lt;/li&gt;
&lt;li&gt;PET: Positron Emission Tomography&lt;/li&gt;
&lt;li&gt;MG: Mammography&lt;/li&gt;
&lt;li&gt;CT: Computed Tompgraphy&lt;/li&gt;
&lt;li&gt;H&amp;amp;E: Hematoxylin &amp;amp; Eosin Histology Images&lt;/li&gt;
&lt;li&gt;RGB: Optical Images&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-deep-learning-techniques" class="anchor" aria-hidden="true" href="#deep-learning-techniques"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning Techniques&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#autoencoders--stacked-autoencoders"&gt;AutoEncoders/ Stacked AutoEncoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#convolutional-neural-networks"&gt;Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#recurrent-neural-networks"&gt;Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#generative-adversarial-networks"&gt;Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-medical-applications" class="anchor" aria-hidden="true" href="#medical-applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Medical Applications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#annotation"&gt;Annotation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#classification"&gt;Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#detection--localization"&gt;Detection/ Localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#segmentation"&gt;Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#registration"&gt;Registration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#regression"&gt;Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Image Reconstruction and Post-Processing&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-tasks"&gt;Other tasks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-deep-learning-techniques-1" class="anchor" aria-hidden="true" href="#deep-learning-techniques-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning Techniques&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-auto-encoders-stacked-auto-encoders" class="anchor" aria-hidden="true" href="#auto-encoders-stacked-auto-encoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Auto-Encoders/ Stacked Auto-Encoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ieeexplore.ieee.org/document/7405343/" rel="nofollow"&gt;AggNet: Deep Learning From Crowds for Mitosis Detection in Breast Cancer Histology Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ieeexplore.ieee.org/document/7401052/#full-text-section" rel="nofollow"&gt;Fast Convolutional Neural Network Training Using Selective Data Sampling: Application to Hemorrhage Detection in Color Fundus Images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-recurrent-neural-networks" class="anchor" aria-hidden="true" href="#recurrent-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-generative-adversarial-networks" class="anchor" aria-hidden="true" href="#generative-adversarial-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1710.09288" rel="nofollow"&gt;Adversarial Deep Structured Nets for Mass Segmentation from Mammograms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-medical-applications-1" class="anchor" aria-hidden="true" href="#medical-applications-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Medical Applications&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-annotation" class="anchor" aria-hidden="true" href="#annotation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Annotation&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technique&lt;/th&gt;
&lt;th&gt;Modality&lt;/th&gt;
&lt;th&gt;Area&lt;/th&gt;
&lt;th&gt;Paper Title&lt;/th&gt;
&lt;th&gt;DB&lt;/th&gt;
&lt;th&gt;J/C&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NN&lt;/td&gt;
&lt;td&gt;H&amp;amp;E&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;Deep learning of feature representation with multiple instance learning for medical image analysis &lt;a href=""&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;ICASSP&lt;/td&gt;
&lt;td&gt;2014&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;M-CNN&lt;/td&gt;
&lt;td&gt;H&amp;amp;E&lt;/td&gt;
&lt;td&gt;Breast&lt;/td&gt;
&lt;td&gt;AggNet: Deep Learning From Crowds for Mitosis Detection in Breast Cancer Histology Images &lt;a href="http://ieeexplore.ieee.org/document/7405343/" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="amida13.isi.uu.nl"&gt;AMIDA&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IEEE-TMI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FCN&lt;/td&gt;
&lt;td&gt;H&amp;amp;E&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;Suggestive Annotation: A Deep Active Learning Framework for Biomedical Image Segmentation &lt;a href="https://arxiv.org/pdf/1706.04737.pdf" rel="nofollow"&gt;pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-classification" class="anchor" aria-hidden="true" href="#classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Classification&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technique&lt;/th&gt;
&lt;th&gt;Modality&lt;/th&gt;
&lt;th&gt;Area&lt;/th&gt;
&lt;th&gt;Paper Title&lt;/th&gt;
&lt;th&gt;DB&lt;/th&gt;
&lt;th&gt;J/C&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;M-CNN&lt;/td&gt;
&lt;td&gt;CT&lt;/td&gt;
&lt;td&gt;Lung&lt;/td&gt;
&lt;td&gt;Multi-scale Convolutional Neural Networks for Lung Nodule Classification &lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-19992-4_46" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI" rel="nofollow"&gt;LIDC-IDRI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IPMI&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Brain&lt;/td&gt;
&lt;td&gt;Predicting Alzheimer's disease: a neuroimaging study with 3D convolutional neural networks &lt;a href="https://arxiv.org/abs/1502.02506" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="adni.loni.usc.edu"&gt;ADNI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN+RNN&lt;/td&gt;
&lt;td&gt;RGB&lt;/td&gt;
&lt;td&gt;Eye&lt;/td&gt;
&lt;td&gt;Automatic Feature Learning to Grade Nuclear Cataracts Based on Deep Learning &lt;a href="https://pdfs.semanticscholar.org/2650/44769c0a35228d8512570f7ec4cc38e1c511.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;IEEE-TBME&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;X-ray&lt;/td&gt;
&lt;td&gt;Knee&lt;/td&gt;
&lt;td&gt;Quantifying Radiographic Knee Osteoarthritis Severity using Deep Convolutional Neural Networks &lt;a href="https://arxiv.org/pdf/1609.02469" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://oai.epi-ucsf.org/datarelease/" rel="nofollow"&gt;O.E.1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;H&amp;amp;E&lt;/td&gt;
&lt;td&gt;Thyroid&lt;/td&gt;
&lt;td&gt;A Deep Semantic Mobile Application for Thyroid Cytopathology &lt;a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2513164" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;SPIE&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN, 3D-CAE&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Brain&lt;/td&gt;
&lt;td&gt;Alzheimer's Disease Diagnostics by a Deeply Supervised Adaptable 3D Convolutional Network &lt;a href="https://arxiv.org/pdf/1607.00556.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="adni.loni.usc.edu"&gt;ADNI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;M-CNN&lt;/td&gt;
&lt;td&gt;RGB&lt;/td&gt;
&lt;td&gt;Skin&lt;/td&gt;
&lt;td&gt;Multi-resolution-tract CNN with hybrid pretrained and skin-lesion trained layers &lt;a href="http://www.cs.sfu.ca/~hamarneh/ecopy/miccai_mlmi2016a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://licensing.eri.ed.ac.uk/i/software/dermofit-image-library.html" rel="nofollow"&gt;Dermofit&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;MLMI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;RGB&lt;/td&gt;
&lt;td&gt;Skin, Eye&lt;/td&gt;
&lt;td&gt;Towards Automated Melanoma Screening: Exploring Transfer Learning Schemes &lt;a href="https://arxiv.org/pdf/1609.01228.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://dermoscopy.org/" rel="nofollow"&gt;EDRA&lt;/a&gt;, &lt;a href="https://www.kaggle.com/c/diabetic-retinopathy-detection" rel="nofollow"&gt;DRD&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;M-CNN&lt;/td&gt;
&lt;td&gt;CT&lt;/td&gt;
&lt;td&gt;Lung&lt;/td&gt;
&lt;td&gt;Pulmonary Nodule Detection in CT Images: False Positive Reduction Using Multi-View Convolutional Networks &lt;a href="https://www.researchgate.net/profile/Geert_Litjens/publication/296624579_Pulmonary_Nodule_Detection_in_CT_Images_False_Positive_Reduction_Using_Multi-View_Convolutional_Networks/links/57f254cf08ae8da3ce517202.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI" rel="nofollow"&gt;LIDC-IDRI&lt;/a&gt;, &lt;a href="https://anode09.grand-challenge.org/" rel="nofollow"&gt;ANODE09&lt;/a&gt;, &lt;a href="https://clinicaltrials.gov/ct2/show/study/NCT00496977" rel="nofollow"&gt;DLCST&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IEEE-TMI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN&lt;/td&gt;
&lt;td&gt;CT&lt;/td&gt;
&lt;td&gt;Lung&lt;/td&gt;
&lt;td&gt;DeepLung: Deep 3D Dual Path Nets for Automated Pulmonary Nodule Detection and Classification &lt;a href="https://arxiv.org/abs/1801.09555" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI" rel="nofollow"&gt;LIDC-IDRI&lt;/a&gt;, &lt;a href="https://luna16.grand-challenge.org/" rel="nofollow"&gt;LUNA16&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IEEE-WACV&lt;/td&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Brain&lt;/td&gt;
&lt;td&gt;3D Deep Learning for Multi-modal Imaging-Guided Survival Time Prediction of Brain Tumor Patients &lt;a href="http://www.unc.edu/~eadeli/publications/Dong_MICCAI2016.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SAE&lt;/td&gt;
&lt;td&gt;US, CT&lt;/td&gt;
&lt;td&gt;Breast, Lung&lt;/td&gt;
&lt;td&gt;Computer-Aided Diagnosis with Deep Learning Architecture: Applications to Breast Lesions in US Images and Pulmonary Nodules in CT Scans &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4832199/pdf/srep24454.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI" rel="nofollow"&gt;LIDC-IDRI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Nature&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CAE&lt;/td&gt;
&lt;td&gt;MG&lt;/td&gt;
&lt;td&gt;Breast&lt;/td&gt;
&lt;td&gt;Unsupervised deep learning applied to breast density segmentation and mammographic risk scoring &lt;a href="https://ieeexplore.ieee.org/document/7412749/" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;IEEE-TMI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MIL-CNN&lt;/td&gt;
&lt;td&gt;MG&lt;/td&gt;
&lt;td&gt;Breast&lt;/td&gt;
&lt;td&gt;Deep multi-instance networks with sparse label assignment for whole mammogram classification &lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-66179-7_69" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://medicalresearch.inescporto.pt/breastresearch/index.php/Get_INbreast_Database" rel="nofollow"&gt;INbreast&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GCN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Brain&lt;/td&gt;
&lt;td&gt;Spectral Graph Convolutions for Population-based Disease Prediction &lt;a href="http://arxiv.org/abs/1703.03020" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://adni.loni.usc.edu" rel="nofollow"&gt;ADNI&lt;/a&gt;, &lt;a href="http://preprocessed-connectomes-project.org/abide/" rel="nofollow"&gt;ABIDE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;RGB&lt;/td&gt;
&lt;td&gt;Skin&lt;/td&gt;
&lt;td&gt;Dermatologist-level classification of skin cancer with deep neural networks&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Nature&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FCN + CNN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Liver-Liver Tumor&lt;/td&gt;
&lt;td&gt;SurvivalNet: Predicting patient survival from diffusion weighted magnetic resonance images using cascaded fully convolutional and 3D convolutional neural networks &lt;a href="https://arxiv.org/abs/1702.05941" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;ISBI&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-detection--localization" class="anchor" aria-hidden="true" href="#detection--localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Detection / Localization&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technique&lt;/th&gt;
&lt;th&gt;Modality&lt;/th&gt;
&lt;th&gt;Area&lt;/th&gt;
&lt;th&gt;Paper Title&lt;/th&gt;
&lt;th&gt;DB&lt;/th&gt;
&lt;th&gt;J/C&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;MLP&lt;/td&gt;
&lt;td&gt;CT&lt;/td&gt;
&lt;td&gt;Head-Neck&lt;/td&gt;
&lt;td&gt;3D Deep Learning for Efficient and Robust Landmark Detection in Volumetric Data &lt;a href="https://pdfs.semanticscholar.org/6cd3/ea4361e035969e6cf819422d0262f7c0a186.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;US&lt;/td&gt;
&lt;td&gt;Fetal&lt;/td&gt;
&lt;td&gt;Standard Plane Localization in Fetal Ultrasound via Domain Transferred Deep Neural Networks &lt;a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7090943" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;IEEE-JBHI&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.5D-CNN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Femur&lt;/td&gt;
&lt;td&gt;Automated anatomical landmark detection ondistal femur surface using convolutional neural network &lt;a href="http://webpages.uncc.edu/~szhang16/paper/ISBI15_knee.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://oai.epi-ucsf.org/datarelease/" rel="nofollow"&gt;OAI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ISBI&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LSTM&lt;/td&gt;
&lt;td&gt;US&lt;/td&gt;
&lt;td&gt;Fetal&lt;/td&gt;
&lt;td&gt;Automatic Fetal Ultrasound Standard Plane Detection Using Knowledge Transferred Recurrent Neural Networks &lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-24553-9_62" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;X-ray, MRI&lt;/td&gt;
&lt;td&gt;Hand&lt;/td&gt;
&lt;td&gt;Regressing Heatmaps for Multiple Landmark Localization using CNNs &lt;a href="https://www.tugraz.at/fileadmin/user_upload/Institute/ICG/Images/team_bischof/mib/paper_pdfs/MICCAI2016_CNNHeatmaps.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://www.ipilab.org/BAAweb/" rel="nofollow"&gt;DHADS&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;MRI, US, CT&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;An artificial agent for anatomical landmark detection in medical images &lt;a href="https://www5.informatik.uni-erlangen.de/Forschung/Publikationen/2016/Ghesu16-AAA.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://stacom.cardiacatlas.org/lv-landmark-detection-challenge/" rel="nofollow"&gt;SATCOM&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FCN&lt;/td&gt;
&lt;td&gt;US&lt;/td&gt;
&lt;td&gt;Fetal&lt;/td&gt;
&lt;td&gt;Real-time Standard Scan Plane Detection and Localisation in Fetal Ultrasound using Fully Convolutional Neural Networks &lt;a href="https://www.doc.ic.ac.uk/~bkainz/publications/Kainz_MICCAI2016b.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN+LSTM&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Heart&lt;/td&gt;
&lt;td&gt;Recognizing end-diastole and end-systole frames via deep temporal regression network &lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-46726-9_31" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;M-CNN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Heart&lt;/td&gt;
&lt;td&gt;Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation Neural Networks &lt;a href="http://www.cs.jhu.edu/~lelu/publication/07279156.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;IEEE-TMI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;PET/CT&lt;/td&gt;
&lt;td&gt;Heart&lt;/td&gt;
&lt;td&gt;Automated detection of pulmonary nodules in PET/CT images: Ensemble false-positive reduction using a convolutional neural network technique Neural Networks &lt;a href="http://onlinelibrary.wiley.com/doi/10.1118/1.4948498/epdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;MP&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Brain&lt;/td&gt;
&lt;td&gt;Automatic Detection of Cerebral Microbleeds From MR Images via 3D Convolutional Neural Networks &lt;a href="http://ieeexplore.ieee.org/document/7403984/#full-text-section" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;IEEE-TMI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;X-ray, MG&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;Self-Transfer Learning for Fully Weakly Supervised Lesion Localization &lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-46723-8_28" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6663723" rel="nofollow"&gt;NIH,China&lt;/a&gt;, &lt;a href="http://www.mammoimage.org/databases/" rel="nofollow"&gt;DDSM,MIAS&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;RGB&lt;/td&gt;
&lt;td&gt;Eye&lt;/td&gt;
&lt;td&gt;Fast Convolutional Neural Network Training Using Selective Data Sampling: Application to Hemorrhage Detection in Color Fundus Images &lt;a href="http://ieeexplore.ieee.org/document/7401052/#full-text-section" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.kaggle.com/c/diabetic-retinopathy-detection/data" rel="nofollow"&gt;DRD&lt;/a&gt;, &lt;a href="http://www.adcis.net/en/Download-Third-Party/Messidor.html" rel="nofollow"&gt;MESSIDOR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GAN&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;IPMI&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FCN&lt;/td&gt;
&lt;td&gt;X-ray&lt;/td&gt;
&lt;td&gt;Cardiac&lt;/td&gt;
&lt;td&gt;CathNets: Detection and Single-View Depth Prediction of Catheter Electrodes&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;MIAR&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN&lt;/td&gt;
&lt;td&gt;CT&lt;/td&gt;
&lt;td&gt;Lung&lt;/td&gt;
&lt;td&gt;DeepLung: Deep 3D Dual Path Nets for Automated Pulmonary Nodule Detection and Classification &lt;a href="https://arxiv.org/abs/1801.09555" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI" rel="nofollow"&gt;LIDC-IDRI&lt;/a&gt;, &lt;a href="https://luna16.grand-challenge.org/" rel="nofollow"&gt;LUNA16&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;IEEE-WACV&lt;/td&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN&lt;/td&gt;
&lt;td&gt;CT&lt;/td&gt;
&lt;td&gt;Lung&lt;/td&gt;
&lt;td&gt;DeepEM: Deep 3D ConvNets with EM for weakly supervised pulmonary nodule detection &lt;a href="https://arxiv.org/abs/1805.05373v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI" rel="nofollow"&gt;LIDC-IDRI&lt;/a&gt;, &lt;a href="https://luna16.grand-challenge.org/" rel="nofollow"&gt;LUNA16&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-segmentation" class="anchor" aria-hidden="true" href="#segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Segmentation&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technique&lt;/th&gt;
&lt;th&gt;Modality&lt;/th&gt;
&lt;th&gt;Area&lt;/th&gt;
&lt;th&gt;Paper Title&lt;/th&gt;
&lt;th&gt;DB&lt;/th&gt;
&lt;th&gt;J/C&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;U-Net&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;U-net: Convolutional networks for biomedical image segmentation&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FCN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Head-Neck&lt;/td&gt;
&lt;td&gt;Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation &lt;a href="https://arxiv.org/pdf/1603.05959" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;U-Net&lt;/td&gt;
&lt;td&gt;CT&lt;/td&gt;
&lt;td&gt;Head-Neck&lt;/td&gt;
&lt;td&gt;AnatomyNet: Deep learning for fast and fully automated whole‐volume segmentation of head and neck anatomy &lt;a href="https://www.researchgate.net/profile/Wentao_Zhu4/publication/329224429_AnatomyNet_Deep_Learning_for_Fast_and_Fully_Automated_Whole-volume_Segmentation_of_Head_and_Neck_Anatomy/links/5c075ae4458515ae5447b0eb/AnatomyNet-Deep-Learning-for-Fast-and-Fully-Automated-Whole-volume-Segmentation-of-Head-and-Neck-Anatomy.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Medical Physics&lt;/td&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FCN&lt;/td&gt;
&lt;td&gt;CT&lt;/td&gt;
&lt;td&gt;Liver-Liver Tumor&lt;/td&gt;
&lt;td&gt;Automatic Liver and Lesion Segmentation in CT Using Cascaded Fully Convolutional Neural Networks and 3D Conditional Random Fields  &lt;a href="https://arxiv.org/abs/1610.02177" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Spine&lt;/td&gt;
&lt;td&gt;Model-Based Segmentation of Vertebral Bodies from MR Images with 3D CNNs&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FCN&lt;/td&gt;
&lt;td&gt;CT&lt;/td&gt;
&lt;td&gt;Liver-Liver Tumor&lt;/td&gt;
&lt;td&gt;Automatic Liver and Tumor Segmentation of CT and MRI Volumes using Cascaded Fully Convolutional Neural Networks &lt;a href="https://arxiv.org/abs/1702.05970" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FCN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Liver-Liver Tumor&lt;/td&gt;
&lt;td&gt;SurvivalNet: Predicting patient survival from diffusion weighted magnetic resonance images using cascaded fully convolutional and 3D convolutional neural networks &lt;a href="https://arxiv.org/abs/1702.05941" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;ISBI&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN&lt;/td&gt;
&lt;td&gt;Diffusion MRI&lt;/td&gt;
&lt;td&gt;Brain&lt;/td&gt;
&lt;td&gt;q-Space Deep Learning: Twelve-Fold Shorter and Model-Free Diffusion MRI &lt;a href="http://ieeexplore.ieee.org/document/7448418/" rel="nofollow"&gt;[pdf]&lt;/a&gt; (Section II.B.2)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;IEEE-TMI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GAN&lt;/td&gt;
&lt;td&gt;MG&lt;/td&gt;
&lt;td&gt;Breast Mass&lt;/td&gt;
&lt;td&gt;Adversarial Deep Structured Nets for Mass Segmentation from Mammograms &lt;a href="https://arxiv.org/abs/1710.09288" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://medicalresearch.inescporto.pt/breastresearch/index.php/Get_INbreast_Database" rel="nofollow"&gt;INbreast&lt;/a&gt;, &lt;a href="http://marathon.csee.usf.edu/Mammography/DDSM/BCRP/" rel="nofollow"&gt;DDSM-BCRP&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ISBI&lt;/td&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN&lt;/td&gt;
&lt;td&gt;CT&lt;/td&gt;
&lt;td&gt;Liver&lt;/td&gt;
&lt;td&gt;3D Deeply Supervised Network for Automatic Liver Segmentation from CT Volumes &lt;a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-46723-8_18.pdf" rel="nofollow"&gt;pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;MICCAI&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Brain&lt;/td&gt;
&lt;td&gt;Unsupervised domain adaptation in brain lesion segmentation with adversarial networks &lt;a href="https://arxiv.org/pdf/1612.08894.pdf" rel="nofollow"&gt;pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;IPMI&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FCN&lt;/td&gt;
&lt;td&gt;FUNDUS&lt;/td&gt;
&lt;td&gt;Retina&lt;/td&gt;
&lt;td&gt;A Fully Convolutional Neural Network based Structured Prediction Approach Towards the Retinal Vessel Segmentation &lt;a href="https://arxiv.org/pdf/1611.02064" rel="nofollow"&gt;pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;ISBI&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-registration" class="anchor" aria-hidden="true" href="#registration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Registration&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technique&lt;/th&gt;
&lt;th&gt;Modality&lt;/th&gt;
&lt;th&gt;Area&lt;/th&gt;
&lt;th&gt;Paper Title&lt;/th&gt;
&lt;th&gt;DB&lt;/th&gt;
&lt;th&gt;J/C&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN&lt;/td&gt;
&lt;td&gt;CT&lt;/td&gt;
&lt;td&gt;Spine&lt;/td&gt;
&lt;td&gt;An Artificial Agent for Robust Image Registration &lt;a href="https://arxiv.org/pdf/1611.10336.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-regression" class="anchor" aria-hidden="true" href="#regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Regression&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technique&lt;/th&gt;
&lt;th&gt;Modality&lt;/th&gt;
&lt;th&gt;Area&lt;/th&gt;
&lt;th&gt;Paper Title&lt;/th&gt;
&lt;th&gt;DB&lt;/th&gt;
&lt;th&gt;J/C&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2.5D-CNN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Automated anatomical landmark detection ondistal femur surface using convolutional neural network &lt;a href="http://webpages.uncc.edu/~szhang16/paper/ISBI15_knee.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://oai.epi-ucsf.org/datarelease/" rel="nofollow"&gt;OAI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ISBI&lt;/td&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3D-CNN&lt;/td&gt;
&lt;td&gt;Diffusion MRI&lt;/td&gt;
&lt;td&gt;Brain&lt;/td&gt;
&lt;td&gt;q-Space Deep Learning: Twelve-Fold Shorter and Model-Free Diffusion MRI &lt;a href="http://ieeexplore.ieee.org/document/7448418/" rel="nofollow"&gt;[pdf]&lt;/a&gt; (Section II.B.1)&lt;/td&gt;
&lt;td&gt;&lt;a href="http://www.humanconnectome.org/" rel="nofollow"&gt;[HCP]&lt;/a&gt; and other&lt;/td&gt;
&lt;td&gt;IEEE-TMI&lt;/td&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-image-reconstruction-and-post-processing" class="anchor" aria-hidden="true" href="#image-reconstruction-and-post-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Reconstruction and Post Processing&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technique&lt;/th&gt;
&lt;th&gt;Modality&lt;/th&gt;
&lt;th&gt;Area&lt;/th&gt;
&lt;th&gt;Paper Title&lt;/th&gt;
&lt;th&gt;DB&lt;/th&gt;
&lt;th&gt;J/C&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;CS-MRI&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;A Deep Cascade of Convolutional Neural Networks for Dynamic MR Image Reconstruction &lt;a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=8067520" rel="nofollow"&gt;pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;IEEE-TMI&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GAN&lt;/td&gt;
&lt;td&gt;CS-MRI&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Deep Generative Adversarial Networks for Compressed Sensing Automates MRI &lt;a href="https://www.doc.ic.ac.uk/~bglocker/public/mednips2017/med-nips_2017_paper_7.pdf" rel="nofollow"&gt;pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;NIPS&lt;/td&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-image-synthesis-for-data-augmentation" class="anchor" aria-hidden="true" href="#image-synthesis-for-data-augmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image synthesis for data augmentation&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technique&lt;/th&gt;
&lt;th&gt;Modality&lt;/th&gt;
&lt;th&gt;Area&lt;/th&gt;
&lt;th&gt;Paper Title&lt;/th&gt;
&lt;th&gt;DB&lt;/th&gt;
&lt;th&gt;J/C&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;GAN&lt;/td&gt;
&lt;td&gt;RGB (Microscopy)&lt;/td&gt;
&lt;td&gt;Red Blood Cells&lt;/td&gt;
&lt;td&gt;Red blood cell image generation for data augmentation using Conditional Generative Adversarial Networks &lt;a href="https://arxiv.org/abs/1901.06219" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GAN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Brain&lt;/td&gt;
&lt;td&gt;Learning Data Augmentation for Brain Tumor Segmentation with Coarse-to-Fine Generative Adversarial Networks &lt;a href="https://arxiv.org/pdf/1805.11291.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GAN&lt;/td&gt;
&lt;td&gt;MRI&lt;/td&gt;
&lt;td&gt;Brain&lt;/td&gt;
&lt;td&gt;Medical Image Synthesis for Data Augmentation and Anonymization using Generative Adversarial Networks &lt;a href="https://arxiv.org/pdf/1807.10225.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GAN&lt;/td&gt;
&lt;td&gt;CT, MRI&lt;/td&gt;
&lt;td&gt;Brain&lt;/td&gt;
&lt;td&gt;GAN Augmentation: Augmenting Training Data using Generative Adversarial Networks &lt;a href="https://arxiv.org/pdf/1810.10863.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GAN&lt;/td&gt;
&lt;td&gt;CT&lt;/td&gt;
&lt;td&gt;Liver&lt;/td&gt;
&lt;td&gt;GAN-based Synthetic Medical Image Augmentation for increased CNN Performance in Liver Lesion Classification &lt;a href="https://arxiv.org/pdf/1803.01229.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;arXiv&lt;/td&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-other-tasks" class="anchor" aria-hidden="true" href="#other-tasks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other tasks&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>albarqouni</author><guid isPermaLink="false">https://github.com/albarqouni/Deep-Learning-for-Medical-Applications</guid><pubDate>Thu, 19 Dec 2019 00:08:00 GMT</pubDate></item><item><title>openbmc/docs #9 in TeX, Today</title><link>https://github.com/openbmc/docs</link><description>&lt;p&gt;&lt;i&gt;OpenBMC Documentation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-openbmc-documentation" class="anchor" aria-hidden="true" href="#openbmc-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenBMC documentation&lt;/h1&gt;
&lt;p&gt;This repository contains documentation for OpenBMC as a whole. There may
be component-specific documentation in the repository for each component.&lt;/p&gt;
&lt;p&gt;The &lt;a href="features.md"&gt;features&lt;/a&gt; document lists the project's major features
with links to more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-openbmc-usage" class="anchor" aria-hidden="true" href="#openbmc-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenBMC Usage&lt;/h2&gt;
&lt;p&gt;These documents describe how to use OpenBMC, including using the programmatic
interfaces to an OpenBMC system.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="rest-api.md"&gt;rest-api.md&lt;/a&gt;: Introduction to using the OpenBMC REST API&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="console.md"&gt;console.md&lt;/a&gt;: Using the host console&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="host-management.md"&gt;host-management.md&lt;/a&gt;: Performing host management tasks
with OpenBMC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="code-update"&gt;code-update&lt;/a&gt;: Updating OpenBMC and host platform firmware&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-openbmc-development" class="anchor" aria-hidden="true" href="#openbmc-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenBMC Development&lt;/h2&gt;
&lt;p&gt;These documents contain details on developing OpenBMC code itself&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="cheatsheet.md"&gt;cheatsheet.md&lt;/a&gt;: Quick reference for some common
development tasks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;: Guidelines for contributing to
OpenBMC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="kernel-development.md"&gt;kernel-development.md&lt;/a&gt;: Reference for common
kernel development tasks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="REST-cheatsheet.md"&gt;REST-cheatsheet.md&lt;/a&gt;: Quick reference for some common
curl commands usage.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-openbmc-goals" class="anchor" aria-hidden="true" href="#openbmc-goals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenBMC Goals&lt;/h2&gt;
&lt;p&gt;The OpenBMC project's aim is to create a highly extensible framework for BMC
software and implement for data-center computer systems.&lt;/p&gt;
&lt;p&gt;We have a few high-level objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The OpenBMC framework must be extensible, easy to learn, and usable in a
variety of programming languages.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a REST API for external management, and allow for "pluggable"
interfaces for other types of management interactions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a remote host console, accessible over the network&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Persist network configuration settable from REST interface and host&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a robust solution for RTC management, exposed to the host.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compatible with host firmware implementations for basic IPMI communication
between host and BMC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a flexible and hierarchical inventory tracking component&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maintain a sensor database and track thresholds&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-technical-steering-committee" class="anchor" aria-hidden="true" href="#technical-steering-committee"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Technical Steering Committee&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Brad Bishop (chair), IBM&lt;/li&gt;
&lt;li&gt;Nancy Yuen, Google&lt;/li&gt;
&lt;li&gt;Sai Dasari, Facebook&lt;/li&gt;
&lt;li&gt;James Mihm, Intel&lt;/li&gt;
&lt;li&gt;Sagar Dharia, Microsoft&lt;/li&gt;
&lt;li&gt;Supreeth Venkatesh, Arm&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>openbmc</author><guid isPermaLink="false">https://github.com/openbmc/docs</guid><pubDate>Thu, 19 Dec 2019 00:09:00 GMT</pubDate></item><item><title>swq123459/GZHU-Report-Latex-Version #10 in TeX, Today</title><link>https://github.com/swq123459/GZHU-Report-Latex-Version</link><description>&lt;p&gt;&lt;i&gt;我自己制作的广州大学Latex报告模板，有毕业设计，课程设计，毕业论文，等等🎈&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1 align="center"&gt;&lt;a id="user-content-------------gzhu-报告的-latex-模板" class="anchor" aria-hidden="true" href="#------------gzhu-报告的-latex-模板"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
  &lt;a href="https://github.com/swq123459/GZHU-Report-Latex-Version"&gt;
    &lt;img alt="AwesomeCV" src="https://github.com/swq123459/swq123456-readmePicture/raw/master/commom/latexhalf.png?raw=true" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;br&gt;
  GZHU 报告的 Latex 模板
&lt;/h1&gt;
&lt;p align="center"&gt;
  Latex Template for Guangzhou University Report
&lt;/p&gt;
&lt;br&gt;
&lt;div align="center"&gt;
  &lt;a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;amp;email=LxsZFhsbGRsWH29eXgFMQEI" rel="nofollow"&gt;
    &lt;img alt="Email" src="https://camo.githubusercontent.com/fe68d369e1f85db026aa4e7da972ba84c6186cc7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d61696c2d51512d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/Mail-QQ-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;h1&gt;&lt;a id="user-content-简单介绍" class="anchor" aria-hidden="true" href="#简单介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简单介绍&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;临近毕业之际(可能你看到这个我已毕业)，应许多人还有老师的关照，将自己制作的这些模板贡献出来，即使没有写过很标准的文档的同学或者对Tex没有基础的同学也可以用我的模板编译出美丽而且符合标准的文档，还有很多陆续会更新 （如果有时间的话）&lt;/li&gt;
&lt;li&gt;有好几种样式，可以点进去文件夹中，查看&lt;code&gt;样例欣赏.pdf&lt;/code&gt;就可以看到大概的效果啦&lt;/li&gt;
&lt;li&gt;&lt;code&gt;一些我自己的例子&lt;/code&gt;文件夹中有一些我写的报告，可以看看&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-使用方法" class="anchor" aria-hidden="true" href="#使用方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;使用方法&lt;/h2&gt;
&lt;div&gt;
  前奏：可以通过我写的教程安装 CTex 和 Texstudio 开发环境，点击右边标签&lt;g-emoji class="g-emoji" alias="ok_hand" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44c.png"&gt;👌&lt;/g-emoji&gt;
  &lt;a href="https://blog.csdn.net/qq_33826564/article/details/81490478" rel="nofollow"&gt;
    &lt;img alt="CSDN BLOG" src="https://camo.githubusercontent.com/dd7639ad946172ceed340c3652721883aff5582b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4353444e2d7377713132333435392d7265642e737667" data-canonical-src="https://img.shields.io/badge/CSDN-swq123459-red.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;  
&lt;/div&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;安装完成之后我们就可以使用&lt;code&gt;Texstudio&lt;/code&gt;来写&lt;code&gt;Latex&lt;/code&gt;文档了，这里很简单，我模板都为大家做好了，要做的就是在&lt;code&gt;Texstudio&lt;/code&gt;里面点开&lt;code&gt;.tex&lt;/code&gt;文件，然后点击“编译”按钮，就可以生成好开的&lt;code&gt;.pdf&lt;/code&gt;格式的报告啦(生成的&lt;code&gt;.pdf&lt;/code&gt;文件在根目录哦)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为了方便小白写&lt;code&gt;Latex&lt;/code&gt;,我里面注释都有很多比方说公式，图片插入（插入图片最后用&lt;code&gt;.pdf&lt;/code&gt;的图片或者&lt;code&gt;.eps&lt;/code&gt;的图片，这样放大之后不会有丢失，也就是矢量图），表格插入，代码插入，之类的示范，大家点进去看就知道了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;a href="https://github.com/swq123459/GZHU-Report-Latex-Version/tree/master/%E8%87%AA%E5%88%B6%E7%89%88-%E7%BB%BC%E5%90%88%E6%80%A7%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A-%E7%89%88%E6%9C%AC1"&gt;自制版-综合性实验报告模板-版本1&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a href="https://github.com/swq123459/GZHU-Report-Latex-Version/tree/master/%E8%87%AA%E5%88%B6%E7%89%88-%E7%BB%BC%E5%90%88%E6%80%A7%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A-%E7%89%88%E6%9C%AC2"&gt;自制版-综合性实验报告模板-版本2&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/swq123459/swq123456-readmePicture/blob/master/report/zzv1.png?raw=true"&gt;&lt;img src="https://github.com/swq123459/swq123456-readmePicture/raw/master/report/zzv1.png?raw=true" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/swq123459/swq123456-readmePicture/blob/master/report/xy1.png?raw=true"&gt;&lt;img src="https://github.com/swq123459/swq123456-readmePicture/raw/master/report/xy1.png?raw=true" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;a href="https://github.com/swq123459/GZHU-Report-Latex-Version/tree/master/%E5%AE%98%E6%96%B9%E7%89%88-%E7%BB%BC%E5%90%88%E6%80%A7%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A%E6%A8%A1%E6%9D%BF-%E7%89%88%E6%9C%AC3"&gt;官方版-综合性实验报告模板-版本3&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a href="https://github.com/swq123459/GZHU-Report-Latex-Version/tree/master/%E5%AD%A6%E9%99%A2%E7%89%88-%E6%AF%95%E4%B8%9A%E5%AE%9E%E4%B9%A0%E6%8A%A5%E5%91%8A"&gt;广大官方版仿照-毕业实习报告模板&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;&lt;img src="https://github.com/swq123459/swq123456-readmePicture/raw/master/report/xy1.png?raw=true" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;&lt;img src="https://github.com/swq123459/swq123456-readmePicture/raw/master/report/by1.png?raw=true" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;a href="https://github.com/swq123459/GZHU-Report-Latex-Version/tree/master/%E5%AD%A6%E9%99%A2%E7%89%88-%E6%AF%95%E4%B8%9A%E5%AE%9E%E4%B9%A0%E6%8A%A5%E5%91%8A"&gt;广大官方版-课程设计报告模板-通信电子系&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a href="https://github.com/swq123459/GZHU-Report-Latex-Version/tree/master/%E5%AE%98%E6%96%B9%E7%89%88-%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A%E6%A8%A1%E6%9D%BF"&gt;广大官方版-实验报告&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/swq123459/swq123456-readmePicture/blob/master/report/kcsj1.png?raw=true"&gt;&lt;img src="https://github.com/swq123459/swq123456-readmePicture/raw/master/report/kcsj1.png?raw=true" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/swq123459/swq123456-readmePicture/blob/master/report/bg1.png?raw=true"&gt;&lt;img src="https://github.com/swq123459/swq123456-readmePicture/raw/master/report/bg1.png?raw=true" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;a href="https://github.com/swq123459/GZHU-Report-Latex-Version/tree/master/%E5%AD%A6%E9%99%A2%E7%89%88-%E6%AF%95%E4%B8%9A%E5%AE%9E%E4%B9%A0%E6%8A%A5%E5%91%8A"&gt;广大官方版-毕业设计模板-通信电子系&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;a href="https://github.com/swq123459/GZHU-Report-Latex-Version/tree/master/%E5%AE%98%E6%96%B9%E7%89%88-%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A%E6%A8%A1%E6%9D%BF"&gt;广大官方版-实验报告&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/swq123459/swq123456-readmePicture/blob/master/report/bysj.png?raw=true"&gt;&lt;img src="https://github.com/swq123459/swq123456-readmePicture/raw/master/report/bysj.png?raw=true" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/swq123459/swq123456-readmePicture/blob/master/report/bg1.png?raw=true"&gt;&lt;img src="https://github.com/swq123459/swq123456-readmePicture/raw/master/report/bg1.png?raw=true" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>swq123459</author><guid isPermaLink="false">https://github.com/swq123459/GZHU-Report-Latex-Version</guid><pubDate>Thu, 19 Dec 2019 00:10:00 GMT</pubDate></item></channel></rss>