<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: TeX, Today</title><link>https://github.com/trending/tex?since=daily</link><description>The top repositories on GitHub for tex, measured daily</description><pubDate>Mon, 20 Jan 2020 01:06:16 GMT</pubDate><lastBuildDate>Mon, 20 Jan 2020 01:06:16 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>deedy/Deedy-Resume #1 in TeX, Today</title><link>https://github.com/deedy/Deedy-Resume</link><description>&lt;p&gt;&lt;i&gt;A one page , two asymmetric column resume template in XeTeX that caters to an undergraduate Computer Science student&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deedy-resume" class="anchor" aria-hidden="true" href="#deedy-resume"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deedy-Resume&lt;/h1&gt;
&lt;p&gt;A &lt;strong&gt;one-page&lt;/strong&gt;, &lt;strong&gt;two asymmetric column&lt;/strong&gt; resume template in &lt;strong&gt;XeTeX&lt;/strong&gt; that caters particularly to an &lt;strong&gt;undergraduate Computer Science&lt;/strong&gt; student.
As of &lt;strong&gt;v1.2&lt;/strong&gt;, there is an option to choose from two templates:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;MacFonts&lt;/strong&gt; - uses fonts native to OSX - &lt;em&gt;Helvetica&lt;/em&gt;, &lt;em&gt;Helvetica Neue&lt;/em&gt; (and it's Light and Ultralight versions) and the CJK fonts &lt;em&gt;Heiti SC&lt;/em&gt;, and &lt;em&gt;Heiti TC&lt;/em&gt;. The EULA of these fonts prevents distribution on Open Source.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenFonts&lt;/strong&gt; - uses free, open-source fonts that resemble the above - &lt;em&gt;Lato&lt;/em&gt; (and its various variants) and &lt;em&gt;Raleway&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is licensed under the Apache License 2.0.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-motivation" class="anchor" aria-hidden="true" href="#motivation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Common LaTeX resume-builders such as &lt;a href="http://www.latextemplates.com/template/moderncv-cv-and-cover-letter" rel="nofollow"&gt;&lt;strong&gt;moderncv&lt;/strong&gt;&lt;/a&gt;  and the &lt;a href="https://github.com/afriggeri/cv"&gt;&lt;strong&gt;friggeri-cv&lt;/strong&gt;&lt;/a&gt; look great if you're looking for a multi-page resume with numerous citations, but usually imperfect for making a thorough, single-page one. A lot of companies today search resumes based on &lt;a href="http://www.businessinsider.com/most-big-companies-have-a-tracking-system-that-scans-your-resume-for-keywords-2012-1" rel="nofollow"&gt;keywords&lt;/a&gt; but at the same time require/prefer a one-page resume, especially for undergraduates.&lt;/p&gt;
&lt;p&gt;This template attempts to &lt;strong&gt;look clean&lt;/strong&gt;, highlight &lt;strong&gt;details&lt;/strong&gt;, be a &lt;strong&gt;single page&lt;/strong&gt;, and allow useful &lt;strong&gt;LaTeX templating&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-openfonts" class="anchor" aria-hidden="true" href="#openfonts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenFonts&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/OpenFonts/sample-image.png"&gt;&lt;img src="https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/OpenFonts/sample-image.png" alt="alt tag" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-macfonts" class="anchor" aria-hidden="true" href="#macfonts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MacFonts&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/MacFonts/sample-image.png"&gt;&lt;img src="https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/MacFonts/sample-image.png" alt="alt tag" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Compiles only with &lt;strong&gt;XeTeX&lt;/strong&gt; and required &lt;strong&gt;BibTex&lt;/strong&gt; for compiling publications and the .bib filetype.&lt;/li&gt;
&lt;li&gt;Uses fonts that are usually only available to &lt;strong&gt;Mac&lt;/strong&gt; users such as Helvetica Neue Light.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-availability" class="anchor" aria-hidden="true" href="#availability"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Availability&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;MacFonts version - &lt;a href="http://debarghyadas.com/resume/debarghya-das-resume.pdf" rel="nofollow"&gt;as an online preview&lt;/a&gt; and &lt;a href="https://github.com/deedydas/Deedy-Resume/raw/master/MacFonts/deedy_resume.pdf"&gt;as a direct download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenFonts version - &lt;a href="https://github.com/deedydas/Deedy-Resume/raw/master/OpenFonts/deedy_resume-openfont.pdf"&gt;as a direct download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overleaf&lt;/strong&gt;.com (formerly &lt;strong&gt;WriteLatex&lt;/strong&gt;.com) (v1 fonts/colors changed) - &lt;a href="https://www.writelatex.com/templates/deedy-resume/sqdbztjjghvz#.U2H9Kq1dV18" rel="nofollow"&gt;compilable online&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ShareLatex&lt;/strong&gt;.com (v1 fonts changes) - &lt;a href="https://www.sharelatex.com/templates/cv-or-resume/deedy-resume" rel="nofollow"&gt;compilable online&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-v12" class="anchor" aria-hidden="true" href="#v12"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;v1.2&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Added publications in place of societies.&lt;/li&gt;
&lt;li&gt;Collapsed a portion of education.&lt;/li&gt;
&lt;li&gt;Fixed a bug with alignment of overflowing long last updated dates on the top right.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-v11" class="anchor" aria-hidden="true" href="#v11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;v1.1&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Fixed several compilation bugs with \renewcommand&lt;/li&gt;
&lt;li&gt;Got Open-source fonts (Windows/Linux support)&lt;/li&gt;
&lt;li&gt;Added Last Updated&lt;/li&gt;
&lt;li&gt;Moved Title styling into .sty&lt;/li&gt;
&lt;li&gt;Commented .sty file.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Merge OpenFont and MacFonts as a single sty with options.&lt;/li&gt;
&lt;li&gt;Figure out a smoother way for the document to flow onto the next page.&lt;/li&gt;
&lt;li&gt;Add styling information for a "Projects/Hacks" section.&lt;/li&gt;
&lt;li&gt;Add location/address information&lt;/li&gt;
&lt;li&gt;Fix the hacky 'References' omission outside the .cls file in the MacFonts version.&lt;/li&gt;
&lt;li&gt;Add various styling and section options and allow for multiple pages smoothly.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-known-issues" class="anchor" aria-hidden="true" href="#known-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Known Issues:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Overflows onto second page if any column's contents are more than the vertical limit&lt;/li&gt;
&lt;li&gt;Hacky space on the first bullet point on the second column.&lt;/li&gt;
&lt;li&gt;Hacky redefinition of \refname to omit 'References' text for publications in the MacFonts version.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Copyright 2014 Debarghya Das

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>deedy</author><guid isPermaLink="false">https://github.com/deedy/Deedy-Resume</guid><pubDate>Mon, 20 Jan 2020 00:01:00 GMT</pubDate></item><item><title>terryum/awesome-deep-learning-papers #2 in TeX, Today</title><link>https://github.com/terryum/awesome-deep-learning-papers</link><description>&lt;p&gt;&lt;i&gt;The most cited deep learning papers&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-awesome---most-cited-deep-learning-papers" class="anchor" aria-hidden="true" href="#awesome---most-cited-deep-learning-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome - Most Cited Deep Learning Papers&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://camo.githubusercontent.com/13c4e50d88df7178ae1882a203ed57b641674f94/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667" alt="Awesome" data-canonical-src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[Notice] This list is not being maintained anymore because of the overwhelming amount of deep learning papers published every day since 2017.&lt;/p&gt;
&lt;p&gt;A curated list of the most cited deep learning papers (2012-2016)&lt;/p&gt;
&lt;p&gt;We believe that there exist &lt;em&gt;classic&lt;/em&gt; deep learning papers which are worth reading regardless of their application domain. Rather than providing overwhelming amount of papers, We would like to provide a &lt;em&gt;curated list&lt;/em&gt; of the awesome deep learning papers which are considered as &lt;em&gt;must-reads&lt;/em&gt; in certain research domains.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-background" class="anchor" aria-hidden="true" href="#background"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Background&lt;/h2&gt;
&lt;p&gt;Before this list, there exist other &lt;em&gt;awesome deep learning lists&lt;/em&gt;, for example, &lt;a href="https://github.com/kjw0612/awesome-deep-vision"&gt;Deep Vision&lt;/a&gt; and &lt;a href="https://github.com/kjw0612/awesome-rnn"&gt;Awesome Recurrent Neural Networks&lt;/a&gt;. Also, after this list comes out, another awesome list for deep learning beginners, called &lt;a href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap"&gt;Deep Learning Papers Reading Roadmap&lt;/a&gt;, has been created and loved by many deep learning researchers.&lt;/p&gt;
&lt;p&gt;Although the &lt;em&gt;Roadmap List&lt;/em&gt; includes lots of important deep learning papers, it feels overwhelming for me to read them all. As I mentioned in the introduction, I believe that seminal works can give us lessons regardless of their application domain. Thus, I would like to introduce &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; here as a good starting point of overviewing deep learning researches.&lt;/p&gt;
&lt;p&gt;To get the news for newly released papers everyday, follow my &lt;a href="https://twitter.com/TerryUm_ML" rel="nofollow"&gt;twitter&lt;/a&gt; or &lt;a href="https://www.facebook.com/terryum.io/" rel="nofollow"&gt;facebook page&lt;/a&gt;!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-awesome-list-criteria" class="anchor" aria-hidden="true" href="#awesome-list-criteria"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome list criteria&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A list of &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; published from 2012 to 2016 is suggested.&lt;/li&gt;
&lt;li&gt;If a paper is added to the list, another paper (usually from *More Papers from 2016" section) should be removed to keep top 100 papers. (Thus, removing papers is also important contributions as well as adding papers)&lt;/li&gt;
&lt;li&gt;Papers that are important, but failed to be included in the list, will be listed in &lt;em&gt;More than Top 100&lt;/em&gt; section.&lt;/li&gt;
&lt;li&gt;Please refer to &lt;em&gt;New Papers&lt;/em&gt; and &lt;em&gt;Old Papers&lt;/em&gt; sections for the papers published in recent 6 months or before 2012.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;(Citation criteria)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;lt; 6 months&lt;/strong&gt; : &lt;em&gt;New Papers&lt;/em&gt; (by discussion)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2016&lt;/strong&gt; :  +60 citations or "More Papers from 2016"&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2015&lt;/strong&gt; :  +200 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2014&lt;/strong&gt; :  +400 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2013&lt;/strong&gt; :  +600 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2012&lt;/strong&gt; :  +800 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;~2012&lt;/strong&gt; : &lt;em&gt;Old Papers&lt;/em&gt; (by discussion)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please note that we prefer seminal deep learning papers that can be applied to various researches rather than application papers. For that reason, some papers that meet the criteria may not be accepted while others can be. It depends on the impact of the paper, applicability to other researches scarcity of the research domain, and so on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We need your contributions!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you have any suggestions (missing papers, new papers, key researchers or typos), please feel free to edit and pull a request.
(Please read the &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md"&gt;contributing guide&lt;/a&gt; for further instructions, though just letting me know the title of papers can also be a big contribution to us.)&lt;/p&gt;
&lt;p&gt;(Update) You can download all top-100 papers with &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/fetch_papers.py"&gt;this&lt;/a&gt; and collect all authors' names with &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/get_authors.py"&gt;this&lt;/a&gt;. Also, &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/top100papers.bib"&gt;bib file&lt;/a&gt; for all top-100 papers are available. Thanks, doodhwala, &lt;a href="https://github.com/sunshinemyson"&gt;Sven&lt;/a&gt; and &lt;a href="https://github.com/grepinsight"&gt;grepinsight&lt;/a&gt;!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can anyone contribute the code for obtaining the statistics of the authors of Top-100 papers?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#understanding--generalization--transfer"&gt;Understanding / Generalization / Transfer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimization--training-techniques"&gt;Optimization / Training Techniques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unsupervised--generative-models"&gt;Unsupervised / Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#convolutional-neural-network-models"&gt;Convolutional Network Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image-segmentation--object-detection"&gt;Image Segmentation / Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image--video--etc"&gt;Image / Video / Etc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#natural-language-processing--rnns"&gt;Natural Language Processing / RNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#speech--other-domain"&gt;Speech / Other Domain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement-learning--robotics"&gt;Reinforcement Learning / Robotics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#more-papers-from-2016"&gt;More Papers from 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(More than Top 100)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#new-papers"&gt;New Papers&lt;/a&gt; : Less than 6 months&lt;/li&gt;
&lt;li&gt;&lt;a href="#old-papers"&gt;Old Papers&lt;/a&gt; : Before 2012&lt;/li&gt;
&lt;li&gt;&lt;a href="#hw--sw--dataset"&gt;HW / SW / Dataset&lt;/a&gt; : Technical reports&lt;/li&gt;
&lt;li&gt;&lt;a href="#book--survey--review"&gt;Book / Survey / Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#video-lectures--tutorials--blogs"&gt;Video Lectures / Tutorials / Blogs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#appendix-more-than-top-100"&gt;Appendix: More than Top 100&lt;/a&gt; : More papers not in the list&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-understanding--generalization--transfer" class="anchor" aria-hidden="true" href="#understanding--generalization--transfer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Understanding / Generalization / Transfer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Distilling the knowledge in a neural network&lt;/strong&gt; (2015), G. Hinton et al. &lt;a href="http://arxiv.org/pdf/1503.02531" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep neural networks are easily fooled: High confidence predictions for unrecognizable images&lt;/strong&gt; (2015), A. Nguyen et al. &lt;a href="http://arxiv.org/pdf/1412.1897" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How transferable are features in deep neural networks?&lt;/strong&gt; (2014), J. Yosinski et al. &lt;a href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CNN features off-the-Shelf: An astounding baseline for recognition&lt;/strong&gt; (2014), A. Razavian et al. &lt;a href="http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning and transferring mid-Level image representations using convolutional neural networks&lt;/strong&gt; (2014), M. Oquab et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Visualizing and understanding convolutional networks&lt;/strong&gt; (2014), M. Zeiler and R. Fergus &lt;a href="http://arxiv.org/pdf/1311.2901" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decaf: A deep convolutional activation feature for generic visual recognition&lt;/strong&gt; (2014), J. Donahue et al. &lt;a href="http://arxiv.org/pdf/1310.1531" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-optimization--training-techniques" class="anchor" aria-hidden="true" href="#optimization--training-techniques"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimization / Training Techniques&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Training very deep networks&lt;/strong&gt; (2015), R. Srivastava et al. &lt;a href="http://papers.nips.cc/paper/5850-training-very-deep-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Batch normalization: Accelerating deep network training by reducing internal covariate shift&lt;/strong&gt; (2015), S. Loffe and C. Szegedy &lt;a href="http://arxiv.org/pdf/1502.03167" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification&lt;/strong&gt; (2015), K. He et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dropout: A simple way to prevent neural networks from overfitting&lt;/strong&gt; (2014), N. Srivastava et al. &lt;a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adam: A method for stochastic optimization&lt;/strong&gt; (2014), D. Kingma and J. Ba &lt;a href="http://arxiv.org/pdf/1412.6980" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href="http://arxiv.org/pdf/1207.0580.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random search for hyper-parameter optimization&lt;/strong&gt; (2012) J. Bergstra and Y. Bengio &lt;a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-unsupervised--generative-models" class="anchor" aria-hidden="true" href="#unsupervised--generative-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised / Generative Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pixel recurrent neural networks&lt;/strong&gt; (2016), A. Oord et al. &lt;a href="http://arxiv.org/pdf/1601.06759v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improved techniques for training GANs&lt;/strong&gt; (2016), T. Salimans et al. &lt;a href="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unsupervised representation learning with deep convolutional generative adversarial networks&lt;/strong&gt; (2015), A. Radford et al. &lt;a href="https://arxiv.org/pdf/1511.06434v2" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DRAW: A recurrent neural network for image generation&lt;/strong&gt; (2015), K. Gregor et al. &lt;a href="http://arxiv.org/pdf/1502.04623" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generative adversarial nets&lt;/strong&gt; (2014), I. Goodfellow et al. &lt;a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Auto-encoding variational Bayes&lt;/strong&gt; (2013), D. Kingma and M. Welling &lt;a href="http://arxiv.org/pdf/1312.6114" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Building high-level features using large scale unsupervised learning&lt;/strong&gt; (2013), Q. Le et al. &lt;a href="http://arxiv.org/pdf/1112.6209" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-convolutional-neural-network-models" class="anchor" aria-hidden="true" href="#convolutional-neural-network-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Network Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Rethinking the inception architecture for computer vision&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inception-v4, inception-resnet and the impact of residual connections on learning&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href="http://arxiv.org/pdf/1602.07261" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Identity Mappings in Deep Residual Networks&lt;/strong&gt; (2016), K. He et al. &lt;a href="https://arxiv.org/pdf/1603.05027v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep residual learning for image recognition&lt;/strong&gt; (2016), K. He et al. &lt;a href="http://arxiv.org/pdf/1512.03385" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spatial transformer network&lt;/strong&gt; (2015), M. Jaderberg et al., &lt;a href="http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Going deeper with convolutions&lt;/strong&gt; (2015), C. Szegedy et al.  &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Very deep convolutional networks for large-scale image recognition&lt;/strong&gt; (2014), K. Simonyan and A. Zisserman &lt;a href="http://arxiv.org/pdf/1409.1556" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Return of the devil in the details: delving deep into convolutional nets&lt;/strong&gt; (2014), K. Chatfield et al. &lt;a href="http://arxiv.org/pdf/1405.3531" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OverFeat: Integrated recognition, localization and detection using convolutional networks&lt;/strong&gt; (2013), P. Sermanet et al. &lt;a href="http://arxiv.org/pdf/1312.6229" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maxout networks&lt;/strong&gt; (2013), I. Goodfellow et al. &lt;a href="http://arxiv.org/pdf/1302.4389v4" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Network in network&lt;/strong&gt; (2013), M. Lin et al. &lt;a href="http://arxiv.org/pdf/1312.4400" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ImageNet classification with deep convolutional neural networks&lt;/strong&gt; (2012), A. Krizhevsky et al. &lt;a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-image-segmentation--object-detection" class="anchor" aria-hidden="true" href="#image-segmentation--object-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image: Segmentation / Object Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You only look once: Unified, real-time object detection&lt;/strong&gt; (2016), J. Redmon et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fully convolutional networks for semantic segmentation&lt;/strong&gt; (2015), J. Long et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/strong&gt; (2015), S. Ren et al. &lt;a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fast R-CNN&lt;/strong&gt; (2015), R. Girshick &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/strong&gt; (2014), R. Girshick et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spatial pyramid pooling in deep convolutional networks for visual recognition&lt;/strong&gt; (2014), K. He et al. &lt;a href="http://arxiv.org/pdf/1406.4729" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Semantic image segmentation with deep convolutional nets and fully connected CRFs&lt;/strong&gt;, L. Chen et al. &lt;a href="https://arxiv.org/pdf/1412.7062" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning hierarchical features for scene labeling&lt;/strong&gt; (2013), C. Farabet et al. &lt;a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-image--video--etc" class="anchor" aria-hidden="true" href="#image--video--etc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image / Video / Etc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/strong&gt; (2016), C. Dong et al. &lt;a href="https://arxiv.org/pdf/1501.00092v3.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A neural algorithm of artistic style&lt;/strong&gt; (2015), L. Gatys et al. &lt;a href="https://arxiv.org/pdf/1508.06576" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep visual-semantic alignments for generating image descriptions&lt;/strong&gt; (2015), A. Karpathy and L. Fei-Fei &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Show, attend and tell: Neural image caption generation with visual attention&lt;/strong&gt; (2015), K. Xu et al. &lt;a href="http://arxiv.org/pdf/1502.03044" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Show and tell: A neural image caption generator&lt;/strong&gt; (2015), O. Vinyals et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Long-term recurrent convolutional networks for visual recognition and description&lt;/strong&gt; (2015), J. Donahue et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VQA: Visual question answering&lt;/strong&gt; (2015), S. Antol et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DeepFace: Closing the gap to human-level performance in face verification&lt;/strong&gt; (2014), Y. Taigman et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Large-scale video classification with convolutional neural networks&lt;/strong&gt; (2014), A. Karpathy et al. &lt;a href="http://vision.stanford.edu/pdf/karpathy14.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Two-stream convolutional networks for action recognition in videos&lt;/strong&gt; (2014), K. Simonyan et al. &lt;a href="http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3D convolutional neural networks for human action recognition&lt;/strong&gt; (2013), S. Ji et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;&lt;a id="user-content-natural-language-processing--rnns" class="anchor" aria-hidden="true" href="#natural-language-processing--rnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natural Language Processing / RNNs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Neural Architectures for Named Entity Recognition&lt;/strong&gt; (2016), G. Lample et al. &lt;a href="http://aclweb.org/anthology/N/N16/N16-1030.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exploring the limits of language modeling&lt;/strong&gt; (2016), R. Jozefowicz et al. &lt;a href="http://arxiv.org/pdf/1602.02410" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Teaching machines to read and comprehend&lt;/strong&gt; (2015), K. Hermann et al. &lt;a href="http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Effective approaches to attention-based neural machine translation&lt;/strong&gt; (2015), M. Luong et al. &lt;a href="https://arxiv.org/pdf/1508.04025" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conditional random fields as recurrent neural networks&lt;/strong&gt; (2015), S. Zheng and S. Jayasumana. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory networks&lt;/strong&gt; (2014), J. Weston et al. &lt;a href="https://arxiv.org/pdf/1410.3916" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural turing machines&lt;/strong&gt; (2014), A. Graves et al. &lt;a href="https://arxiv.org/pdf/1410.5401" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural machine translation by jointly learning to align and translate&lt;/strong&gt; (2014), D. Bahdanau et al. &lt;a href="http://arxiv.org/pdf/1409.0473" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sequence to sequence learning with neural networks&lt;/strong&gt; (2014), I. Sutskever et al. &lt;a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning phrase representations using RNN encoder-decoder for statistical machine translation&lt;/strong&gt; (2014), K. Cho et al. &lt;a href="http://arxiv.org/pdf/1406.1078" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A convolutional neural network for modeling sentences&lt;/strong&gt; (2014), N. Kalchbrenner et al. &lt;a href="http://arxiv.org/pdf/1404.2188v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convolutional neural networks for sentence classification&lt;/strong&gt; (2014), Y. Kim &lt;a href="http://arxiv.org/pdf/1408.5882" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Glove: Global vectors for word representation&lt;/strong&gt; (2014), J. Pennington et al. &lt;a href="http://anthology.aclweb.org/D/D14/D14-1162.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed representations of sentences and documents&lt;/strong&gt; (2014), Q. Le and T. Mikolov &lt;a href="http://arxiv.org/pdf/1405.4053" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed representations of words and phrases and their compositionality&lt;/strong&gt; (2013), T. Mikolov et al. &lt;a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficient estimation of word representations in vector space&lt;/strong&gt; (2013), T. Mikolov et al.  &lt;a href="http://arxiv.org/pdf/1301.3781" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recursive deep models for semantic compositionality over a sentiment treebank&lt;/strong&gt; (2013), R. Socher et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generating sequences with recurrent neural networks&lt;/strong&gt; (2013), A. Graves. &lt;a href="https://arxiv.org/pdf/1308.0850" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-speech--other-domain" class="anchor" aria-hidden="true" href="#speech--other-domain"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speech / Other Domain&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end attention-based large vocabulary speech recognition&lt;/strong&gt; (2016), D. Bahdanau et al. &lt;a href="https://arxiv.org/pdf/1508.04395" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep speech 2: End-to-end speech recognition in English and Mandarin&lt;/strong&gt; (2015), D. Amodei et al. &lt;a href="https://arxiv.org/pdf/1512.02595" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speech recognition with deep recurrent neural networks&lt;/strong&gt; (2013), A. Graves &lt;a href="http://arxiv.org/pdf/1303.5778.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href="http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition&lt;/strong&gt; (2012) G. Dahl et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Acoustic modeling using deep belief networks&lt;/strong&gt; (2012), A. Mohamed et al. &lt;a href="http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-reinforcement-learning--robotics" class="anchor" aria-hidden="true" href="#reinforcement-learning--robotics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reinforcement Learning / Robotics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end training of deep visuomotor policies&lt;/strong&gt; (2016), S. Levine et al. &lt;a href="http://www.jmlr.org/papers/volume17/15-522/source/15-522.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection&lt;/strong&gt; (2016), S. Levine et al. &lt;a href="https://arxiv.org/pdf/1603.02199" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Asynchronous methods for deep reinforcement learning&lt;/strong&gt; (2016), V. Mnih et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep Reinforcement Learning with Double Q-Learning&lt;/strong&gt; (2016), H. Hasselt et al. &lt;a href="https://arxiv.org/pdf/1509.06461.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mastering the game of Go with deep neural networks and tree search&lt;/strong&gt; (2016), D. Silver et al. &lt;a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Continuous control with deep reinforcement learning&lt;/strong&gt; (2015), T. Lillicrap et al. &lt;a href="https://arxiv.org/pdf/1509.02971" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Human-level control through deep reinforcement learning&lt;/strong&gt; (2015), V. Mnih et al. &lt;a href="http://www.davidqiu.com:8888/research/nature14236.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep learning for detecting robotic grasps&lt;/strong&gt; (2015), I. Lenz et al. &lt;a href="http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Playing atari with deep reinforcement learning&lt;/strong&gt; (2013), V. Mnih et al. &lt;a href="http://arxiv.org/pdf/1312.5602.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-more-papers-from-2016" class="anchor" aria-hidden="true" href="#more-papers-from-2016"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More Papers from 2016&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Layer Normalization&lt;/strong&gt; (2016), J. Ba et al. &lt;a href="https://arxiv.org/pdf/1607.06450v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning to learn by gradient descent by gradient descent&lt;/strong&gt; (2016), M. Andrychowicz et al. &lt;a href="http://arxiv.org/pdf/1606.04474v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Domain-adversarial training of neural networks&lt;/strong&gt; (2016), Y. Ganin et al. &lt;a href="http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WaveNet: A Generative Model for Raw Audio&lt;/strong&gt; (2016), A. Oord et al. &lt;a href="https://arxiv.org/pdf/1609.03499v2" rel="nofollow"&gt;[pdf]&lt;/a&gt; &lt;a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Colorful image colorization&lt;/strong&gt; (2016), R. Zhang et al. &lt;a href="https://arxiv.org/pdf/1603.08511" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generative visual manipulation on the natural image manifold&lt;/strong&gt; (2016), J. Zhu et al. &lt;a href="https://arxiv.org/pdf/1609.03552" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Texture networks: Feed-forward synthesis of textures and stylized images&lt;/strong&gt; (2016), D Ulyanov et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/ulyanov16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSD: Single shot multibox detector&lt;/strong&gt; (2016), W. Liu et al. &lt;a href="https://arxiv.org/pdf/1512.02325" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&amp;lt; 1MB model size&lt;/strong&gt; (2016), F. Iandola et al. &lt;a href="http://arxiv.org/pdf/1602.07360" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eie: Efficient inference engine on compressed deep neural network&lt;/strong&gt; (2016), S. Han et al. &lt;a href="http://arxiv.org/pdf/1602.01528" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1&lt;/strong&gt; (2016), M. Courbariaux et al. &lt;a href="https://arxiv.org/pdf/1602.02830" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic memory networks for visual and textual question answering&lt;/strong&gt; (2016), C. Xiong et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stacked attention networks for image question answering&lt;/strong&gt; (2016), Z. Yang et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid computing using a neural network with dynamic external memory&lt;/strong&gt; (2016), A. Graves et al. &lt;a href="https://www.gwern.net/docs/2016-graves.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google's neural machine translation system: Bridging the gap between human and machine translation&lt;/strong&gt; (2016), Y. Wu et al. &lt;a href="https://arxiv.org/pdf/1609.08144" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-new-papers" class="anchor" aria-hidden="true" href="#new-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New papers&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Newly published papers (&amp;lt; 6 months) which are worth reading&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (2017), Andrew G. Howard et al. &lt;a href="https://arxiv.org/pdf/1704.04861.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convolutional Sequence to Sequence Learning (2017), Jonas Gehring et al. &lt;a href="https://arxiv.org/pdf/1705.03122" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A Knowledge-Grounded Neural Conversation Model (2017), Marjan Ghazvininejad et al. &lt;a href="https://arxiv.org/pdf/1702.01932" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour (2017), Priya Goyal et al. &lt;a href="https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h3.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TACOTRON: Towards end-to-end speech synthesis (2017), Y. Wang et al. &lt;a href="https://arxiv.org/pdf/1703.10135.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Photo Style Transfer (2017), F. Luan et al. &lt;a href="http://arxiv.org/pdf/1703.07511v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al. &lt;a href="http://arxiv.org/pdf/1703.03864v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deformable Convolutional Networks (2017), J. Dai et al. &lt;a href="http://arxiv.org/pdf/1703.06211v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mask R-CNN (2017), K. He et al. &lt;a href="https://128.84.21.199/pdf/1703.06870" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al. &lt;a href="http://arxiv.org/pdf/1703.05192v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep voice: Real-time neural text-to-speech (2017), S. Arik et al., &lt;a href="http://arxiv.org/pdf/1702.07825v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PixelNet: Representation of the pixels, by the pixels, and for the pixels (2017), A. Bansal et al. &lt;a href="http://arxiv.org/pdf/1702.06506v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Batch renormalization: Towards reducing minibatch dependence in batch-normalized models (2017), S. Ioffe. &lt;a href="https://arxiv.org/abs/1702.03275" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wasserstein GAN (2017), M. Arjovsky et al. &lt;a href="https://arxiv.org/pdf/1701.07875v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding deep learning requires rethinking generalization (2017), C. Zhang et al. &lt;a href="https://arxiv.org/pdf/1611.03530" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Least squares generative adversarial networks (2016), X. Mao et al. &lt;a href="https://arxiv.org/abs/1611.04076v2" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-old-papers" class="anchor" aria-hidden="true" href="#old-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Old Papers&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Classic papers published before 2012&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep sparse rectifier neural networks (2011), X. Glorot et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Natural language processing (almost) from scratch (2011), R. Collobert et al. &lt;a href="http://arxiv.org/pdf/1103.0398" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recurrent neural network based language model (2010), T. Mikolov et al. &lt;a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning mid-level features for recognition (2010), Y. Boureau &lt;a href="http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A practical guide to training restricted boltzmann machines (2010), G. Hinton &lt;a href="http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Why does unsupervised pre-training help deep learning (2010), D. Erhan et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning deep architectures for AI (2009), Y. Bengio. &lt;a href="http://sanghv.com/download/soft/machine%20learning,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%20(2009).pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009), H. Lee et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Greedy layer-wise training of deep networks (2007), Y. Bengio et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reducing the dimensionality of data with neural networks, G. Hinton and R. Salakhutdinov. &lt;a href="http://homes.mpimf-heidelberg.mpg.de/~mhelmsta/pdf/2006%20Hinton%20Salakhudtkinov%20Science.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A fast learning algorithm for deep belief nets (2006), G. Hinton et al. &lt;a href="http://nuyoo.utm.mx/~jjf/rna/A8%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gradient-based learning applied to document recognition (1998), Y. LeCun et al. &lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Long short-term memory (1997), S. Hochreiter and J. Schmidhuber. &lt;a href="http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-hw--sw--dataset" class="anchor" aria-hidden="true" href="#hw--sw--dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HW / SW / Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SQuAD: 100,000+ Questions for Machine Comprehension of Text (2016), Rajpurkar et al. &lt;a href="https://arxiv.org/pdf/1606.05250.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenAI gym (2016), G. Brockman et al. &lt;a href="https://arxiv.org/pdf/1606.01540" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TensorFlow: Large-scale machine learning on heterogeneous distributed systems (2016), M. Abadi et al. &lt;a href="http://arxiv.org/pdf/1603.04467" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Theano: A Python framework for fast computation of mathematical expressions, R. Al-Rfou et al.&lt;/li&gt;
&lt;li&gt;Torch7: A matlab-like environment for machine learning, R. Collobert et al. &lt;a href="https://ronan.collobert.com/pub/matos/2011_torch7_nipsw.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MatConvNet: Convolutional neural networks for matlab (2015), A. Vedaldi and K. Lenc &lt;a href="http://arxiv.org/pdf/1412.4564" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Imagenet large scale visual recognition challenge (2015), O. Russakovsky et al. &lt;a href="http://arxiv.org/pdf/1409.0575" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Caffe: Convolutional architecture for fast feature embedding (2014), Y. Jia et al. &lt;a href="http://arxiv.org/pdf/1408.5093" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-book--survey--review" class="anchor" aria-hidden="true" href="#book--survey--review"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Book / Survey / Review&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;On the Origin of Deep Learning (2017), H. Wang and Bhiksha Raj. &lt;a href="https://arxiv.org/pdf/1702.07800" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Reinforcement Learning: An Overview (2017), Y. Li, &lt;a href="http://arxiv.org/pdf/1701.07274v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Machine Translation and Sequence-to-sequence Models(2017): A Tutorial, G. Neubig. &lt;a href="http://arxiv.org/pdf/1703.01619v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Network and Deep Learning (Book, Jan 2017), Michael Nielsen. &lt;a href="http://neuralnetworksanddeeplearning.com/index.html" rel="nofollow"&gt;[html]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep learning (Book, 2016), Goodfellow et al. &lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;[html]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LSTM: A search space odyssey (2016), K. Greff et al. &lt;a href="https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&amp;amp;utm_medium=social&amp;amp;utm_source=plus.google.com&amp;amp;utm_campaign=buffer" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tutorial on Variational Autoencoders (2016), C. Doersch. &lt;a href="https://arxiv.org/pdf/1606.05908" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton &lt;a href="https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep learning in neural networks: An overview (2015), J. Schmidhuber &lt;a href="http://arxiv.org/pdf/1404.7828" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Representation learning: A review and new perspectives (2013), Y. Bengio et al. &lt;a href="http://arxiv.org/pdf/1206.5538" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-video-lectures--tutorials--blogs" class="anchor" aria-hidden="true" href="#video-lectures--tutorials--blogs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video Lectures / Tutorials / Blogs&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(Lectures)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CS231n, Convolutional Neural Networks for Visual Recognition, Stanford University &lt;a href="http://cs231n.stanford.edu/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CS224d, Deep Learning for Natural Language Processing, Stanford University &lt;a href="http://cs224d.stanford.edu/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Oxford Deep NLP 2017, Deep Learning for Natural Language Processing, University of Oxford &lt;a href="https://github.com/oxford-cs-deepnlp-2017/lectures"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(Tutorials)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NIPS 2016 Tutorials, Long Beach &lt;a href="https://nips.cc/Conferences/2016/Schedule?type=Tutorial" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ICML 2016 Tutorials, New York City &lt;a href="http://techtalks.tv/icml/2016/tutorials/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ICLR 2016 Videos, San Juan &lt;a href="http://videolectures.net/iclr2016_san_juan/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Learning Summer School 2016, Montreal &lt;a href="http://videolectures.net/deeplearning2016_montreal/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bay Area Deep Learning School 2016, Stanford &lt;a href="https://www.bayareadlschool.org/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(Blogs)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenAI &lt;a href="https://www.openai.com/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Distill &lt;a href="http://distill.pub/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Andrej Karpathy Blog &lt;a href="http://karpathy.github.io/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Colah's Blog &lt;a href="http://colah.github.io/" rel="nofollow"&gt;[Web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WildML &lt;a href="http://www.wildml.com/" rel="nofollow"&gt;[Web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FastML &lt;a href="http://www.fastml.com/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TheMorningPaper &lt;a href="https://blog.acolyer.org" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-appendix-more-than-top-100" class="anchor" aria-hidden="true" href="#appendix-more-than-top-100"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Appendix: More than Top 100&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(2016)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A character-level decoder without explicit segmentation for neural machine translation (2016), J. Chung et al. &lt;a href="https://arxiv.org/pdf/1603.06147" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dermatologist-level classification of skin cancer with deep neural networks (2017), A. Esteva et al. &lt;a href="http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html" rel="nofollow"&gt;[html]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Weakly supervised object localization with multi-fold multiple instance learning (2017), R. Gokberk et al. &lt;a href="https://arxiv.org/pdf/1503.00949" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Brain tumor segmentation with deep neural networks (2017), M. Havaei et al. &lt;a href="https://arxiv.org/pdf/1505.03540" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Professor Forcing: A New Algorithm for Training Recurrent Networks (2016), A. Lamb et al. &lt;a href="https://arxiv.org/pdf/1610.09038" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adversarially learned inference (2016), V. Dumoulin et al. &lt;a href="https://ishmaelbelghazi.github.io/ALI/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1606.00704v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding convolutional neural networks (2016), J. Koushik &lt;a href="https://arxiv.org/pdf/1605.09081v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Taking the human out of the loop: A review of bayesian optimization (2016), B. Shahriari et al. &lt;a href="https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adaptive computation time for recurrent neural networks (2016), A. Graves &lt;a href="http://arxiv.org/pdf/1603.08983" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Densely connected convolutional networks (2016), G. Huang et al. &lt;a href="https://arxiv.org/pdf/1608.06993v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Region-based convolutional networks for accurate object detection and segmentation (2016), R. Girshick et al.&lt;/li&gt;
&lt;li&gt;Continuous deep q-learning with model-based acceleration (2016), S. Gu et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/gu16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A thorough examination of the cnn/daily mail reading comprehension task (2016), D. Chen et al. &lt;a href="https://arxiv.org/pdf/1606.02858" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Achieving open vocabulary neural machine translation with hybrid word-character models, M. Luong and C. Manning. &lt;a href="https://arxiv.org/pdf/1604.00788" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Very Deep Convolutional Networks for Natural Language Processing (2016), A. Conneau et al. &lt;a href="https://arxiv.org/pdf/1606.01781" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bag of tricks for efficient text classification (2016), A. Joulin et al. &lt;a href="https://arxiv.org/pdf/1607.01759" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Efficient piecewise training of deep structured models for semantic segmentation (2016), G. Lin et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning to compose neural networks for question answering (2016), J. Andreas et al. &lt;a href="https://arxiv.org/pdf/1601.01705" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perceptual losses for real-time style transfer and super-resolution (2016), J. Johnson et al. &lt;a href="https://arxiv.org/pdf/1603.08155" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reading text in the wild with convolutional neural networks (2016), M. Jaderberg et al. &lt;a href="http://arxiv.org/pdf/1412.1842" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;What makes for effective detection proposals? (2016), J. Hosang et al. &lt;a href="https://arxiv.org/pdf/1502.05082" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks (2016), S. Bell et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Instance-aware semantic segmentation via multi-task network cascades (2016), J. Dai et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conditional image generation with pixelcnn decoders (2016), A. van den Oord et al. &lt;a href="http://papers.nips.cc/paper/6527-tree-structured-reinforcement-learning-for-sequential-object-localization.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep networks with stochastic depth (2016), G. Huang et al., &lt;a href="https://arxiv.org/pdf/1603.09382" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics (2016), Yee Whye Teh et al. &lt;a href="http://www.jmlr.org/papers/volume17/teh16a/teh16a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(2015)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ask your neurons: A neural-based approach to answering questions about images (2015), M. Malinowski et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Exploring models and data for image question answering (2015), M. Ren et al. &lt;a href="http://papers.nips.cc/paper/5640-stochastic-variational-inference-for-hidden-markov-models.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Are you talking to a machine? dataset and methods for multilingual image question (2015), H. Gao et al. &lt;a href="http://papers.nips.cc/paper/5641-are-you-talking-to-a-machine-dataset-and-methods-for-multilingual-image-question.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mind's eye: A recurrent visual representation for image caption generation (2015), X. Chen and C. Zitnick. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Chen_Minds_Eye_A_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;From captions to visual concepts and back (2015), H. Fang et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Towards AI-complete question answering: A set of prerequisite toy tasks (2015), J. Weston et al. &lt;a href="http://arxiv.org/pdf/1502.05698" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ask me anything: Dynamic memory networks for natural language processing (2015), A. Kumar et al. &lt;a href="http://arxiv.org/pdf/1506.07285" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Unsupervised learning of video representations using LSTMs (2015), N. Srivastava et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v37/srivastava15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding (2015), S. Han et al. &lt;a href="https://arxiv.org/pdf/1510.00149" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved semantic representations from tree-structured long short-term memory networks (2015), K. Tai et al. &lt;a href="https://arxiv.org/pdf/1503.00075" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Character-aware neural language models (2015), Y. Kim et al. &lt;a href="https://arxiv.org/pdf/1508.06615" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Grammar as a foreign language (2015), O. Vinyals et al. &lt;a href="http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Trust Region Policy Optimization (2015), J. Schulman et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Beyond short snippents: Deep networks for video classification (2015) &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning Deconvolution Network for Semantic Segmentation (2015), H. Noh et al. &lt;a href="https://arxiv.org/pdf/1505.04366v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning spatiotemporal features with 3d convolutional networks (2015), D. Tran et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding neural networks through deep visualization (2015), J. Yosinski et al. &lt;a href="https://arxiv.org/pdf/1506.06579" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An Empirical Exploration of Recurrent Network Architectures (2015), R. Jozefowicz et al.  &lt;a href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep generative image models using a￼ laplacian pyramid of adversarial networks (2015), E.Denton et al. &lt;a href="http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gated Feedback Recurrent Neural Networks (2015), J. Chung et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v37/chung15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fast and accurate deep network learning by exponential linear units (ELUS) (2015), D. Clevert et al. &lt;a href="https://arxiv.org/pdf/1511.07289.pdf%5Cnhttp://arxiv.org/abs/1511.07289%5Cnhttp://arxiv.org/abs/1511.07289" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pointer networks (2015), O. Vinyals et al. &lt;a href="http://papers.nips.cc/paper/5866-pointer-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Visualizing and Understanding Recurrent Networks (2015), A. Karpathy et al. &lt;a href="https://arxiv.org/pdf/1506.02078" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Attention-based models for speech recognition (2015), J. Chorowski et al. &lt;a href="http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;End-to-end memory networks (2015), S. Sukbaatar et al. &lt;a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Describing videos by exploiting temporal structure (2015), L. Yao et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Describing_Videos_by_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A neural conversational model (2015), O. Vinyals and Q. Le. &lt;a href="https://arxiv.org/pdf/1506.05869.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improving distributional similarity with lessons learned from word embeddings, O. Levy et al. [[pdf]] (&lt;a href="https://www.transacl.org/ojs/index.php/tacl/article/download/570/124" rel="nofollow"&gt;https://www.transacl.org/ojs/index.php/tacl/article/download/570/124&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Transition-Based Dependency Parsing with Stack Long Short-Term Memory (2015), C. Dyer et al. &lt;a href="http://aclweb.org/anthology/P/P15/P15-1033.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs (2015), M. Ballesteros et al. &lt;a href="http://aclweb.org/anthology/D/D15/D15-1041.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Finding function in form: Compositional character models for open vocabulary word representation (2015), W. Ling et al. &lt;a href="http://aclweb.org/anthology/D/D15/D15-1176.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(~2014)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DeepPose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning a Deep Convolutional Network for Image Super-Resolution (2014, C. Dong et al. &lt;a href="https://www.researchgate.net/profile/Chen_Change_Loy/publication/264552416_Lecture_Notes_in_Computer_Science/links/53e583e50cf25d674e9c280e.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recurrent models of visual attention (2014), V. Mnih et al. &lt;a href="http://arxiv.org/pdf/1406.6247.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Empirical evaluation of gated recurrent neural networks on sequence modeling (2014), J. Chung et al. &lt;a href="https://arxiv.org/pdf/1412.3555" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Addressing the rare word problem in neural machine translation (2014), M. Luong et al. &lt;a href="https://arxiv.org/pdf/1410.8206" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;On the properties of neural machine translation: Encoder-decoder approaches (2014), K. Cho et. al.&lt;/li&gt;
&lt;li&gt;Recurrent neural network regularization (2014), W. Zaremba et al. &lt;a href="http://arxiv.org/pdf/1409.2329" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Intriguing properties of neural networks (2014), C. Szegedy et al. &lt;a href="https://arxiv.org/pdf/1312.6199.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Towards end-to-end speech recognition with recurrent neural networks (2014), A. Graves and N. Jaitly. &lt;a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Scalable object detection using deep neural networks (2014), D. Erhan et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;On the importance of initialization and momentum in deep learning (2013), I. Sutskever et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_sutskever13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Regularization of neural networks using dropconnect (2013), L. Wan et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning Hierarchical Features for Scene Labeling (2013), C. Farabet et al. &lt;a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linguistic Regularities in Continuous Space Word Representations (2013), T. Mikolov et al. &lt;a href="http://www.aclweb.org/anthology/N13-1#page=784" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Large scale distributed deep networks (2012), J. Dean et al. &lt;a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A Fast and Accurate Dependency Parser using Neural Networks. Chen and Manning. &lt;a href="http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;Thank you for all your contributions. Please make sure to read the &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md"&gt;contributing guide&lt;/a&gt; before you make a pull request.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://creativecommons.org/publicdomain/zero/1.0/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/60561947585c982aee67ed3e3b25388184cc0aa3/687474703a2f2f6d6972726f72732e6372656174697665636f6d6d6f6e732e6f72672f70726573736b69742f627574746f6e732f38387833312f7376672f63632d7a65726f2e737667" alt="CC0" data-canonical-src="http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To the extent possible under law, &lt;a href="https://www.facebook.com/terryum.io/" rel="nofollow"&gt;Terry T. Um&lt;/a&gt; has waived all copyright and related or neighboring rights to this work.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>terryum</author><guid isPermaLink="false">https://github.com/terryum/awesome-deep-learning-papers</guid><pubDate>Mon, 20 Jan 2020 00:02:00 GMT</pubDate></item><item><title>karthik/ddd #3 in TeX, Today</title><link>https://github.com/karthik/ddd</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;p&gt;No README was found for this project.&lt;/p&gt;</description><author>karthik</author><guid isPermaLink="false">https://github.com/karthik/ddd</guid><pubDate>Mon, 20 Jan 2020 00:03:00 GMT</pubDate></item><item><title>tuhdo/os01 #4 in TeX, Today</title><link>https://github.com/tuhdo/os01</link><description>&lt;p&gt;&lt;i&gt;Bootstrap yourself to write an OS from scratch. A book for self-learner.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://www.paypal.com/cgi-bin/webscr?cmd=_donations&amp;amp;business=tuhdo1710%40gmail%2ecom&amp;amp;lc=VN&amp;amp;item_number=tuhdo&amp;amp;currency_code=USD&amp;amp;bn=PP%2dDonationsBF%3aDonate%2dPayPal%2dgreen%2esvg%3aNonHosted" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d5d24e33e2f4b6fe53987419a21b203c03789a8f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d677265656e2e737667" alt="Donate" data-canonical-src="https://img.shields.io/badge/Donate-PayPal-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-operating-systems-from-0-to-1" class="anchor" aria-hidden="true" href="#operating-systems-from-0-to-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://tuhdo.github.io/os01/" rel="nofollow"&gt;Operating Systems: From 0 to 1&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;This book helps you gain the foundational knowledge required to write an
operating system from scratch. Hence the title, 0 to 1.&lt;/p&gt;
&lt;p&gt;After completing this book, at the very least you will learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;How to write an operating system from scratch by reading hardware datasheets.
In the real world, it works like that. You won't be able to consult Google for
a quick answer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A big picture of how each layer of a computer is related to the other, from hardware to software.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Write code independently. It's pointless to copy and paste code. Real learning
happens when you solve problems on your own. Some examples are given to kick
start, but most problems are yours to conquer. However, the solutions are
available online for you to examine after giving it a good try.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Linux as a development environment and how to use common tools for low-level
programming.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;x86 assembly in-depth.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How a program is structured so that an operating system can run.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to debug a program running directly on hardware with gdb and QEMU.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Linking and loading on bare metal x86_64, with pure C. No standard library. No
runtime overhead.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/tuhdo/os01/blob/master/Operating_Systems_From_0_to_1.pdf"&gt;Download the book&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-the-pedagogy-of-the-book" class="anchor" aria-hidden="true" href="#the-pedagogy-of-the-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The pedagogy of the book&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;You give a poor man a fish and you feed him for a day. You teach him to fish
and you give him an occupation that will feed him for a lifetime.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This has been the guiding principle of the book when I was writing it. The book does
not try to teach you everything, but enough to enable you to learn by yourself.
The book itself, at this point, is quite "complete": once you master part 1 and
part 2 (which consist of 8 chapters), you can drop the book and learn by
yourself. At this point, smart readers should be able to continue on their own.
For example, they can continue their journeys
on &lt;a href="http://wiki.osdev.org/Main_Page" rel="nofollow"&gt;OSDev wiki&lt;/a&gt;; in fact, after you study
everything in part 1 and part 2, you only meet
the &lt;a href="http://wiki.osdev.org/Required_Knowledge" rel="nofollow"&gt;minimum requirement&lt;/a&gt; by OSDev
Wiki (well, not quite, the book actually goes deeper for the suggested topics).
Or, if you consider developing an OS for fun is impractical, you can continue
with a Linux-specific book, such as this free
book &lt;a href="https://0xax.gitbooks.io/linux-insides/content/" rel="nofollow"&gt;Linux Insides&lt;/a&gt;, or other
popular Linux kernel books. The book tries hard to provide you a strong
foundation, and that's why part 1 and part 2 were released first.&lt;/p&gt;
&lt;p&gt;The book teaches you core concepts, such as x86 Assembly, ELF, linking and
debugging on bare metal, etc., but more importantly, where such information
come from. For example, instead of just teaching x86 Assembly, it also teaches
how to use reference manuals from Intel. Learning to read the official
manuals is important because only the hardware manufacturers themselves
understand how their hardware work. If you only learn from the secondary
resources because it is easier, you will never gain a complete understanding of
the hardware you are programming for. Have you ever read a book on Assembly, and
wondered where all the information came from? How does the author know
everything he says is correct? And how one seems to magically know so much about
hardware programming? This book gives pointers to such questions.&lt;/p&gt;
&lt;p&gt;As an example, you should skim through chapter 4, "x86 Assembly and C", to see
how it makes use of the Intel manual, Volume 2. And in
the process, it guides you how to use the official manuals.&lt;/p&gt;
&lt;p&gt;Part 3 is planned as a series of specifications that a reader will implement to
complete each operating system component. It does not contain code aside from a
few examples. Part 3 is just there to shorten the reader's time when reading the
official manuals by giving hints where to read, explaining difficult concepts
and how to use the manuals to debug. In short, the implementation is up to the
reader to work on his or her own; the chapters are just like university assignments.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h1&gt;
&lt;p&gt;Know some circuit concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basic Concepts of Electricity: atoms, electrons, protons, neutrons, current flow.&lt;/li&gt;
&lt;li&gt;Ohm's law&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, if you know absolutely nothing about electricity, you can quickly learn it here:
&lt;a href="http://www.allaboutcircuits.com/textbook/" rel="nofollow"&gt;http://www.allaboutcircuits.com/textbook/&lt;/a&gt;, by reading chapter 1 and chapter 2.&lt;/p&gt;
&lt;p&gt;C programming. In particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Variable and function declarations/definitions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;While and for loops&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pointers and function pointers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fundamental algorithms and data structures in C&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Linux basics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Know how to navigate directory with the command line&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Know how to invoke a command with options&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Know how to pipe output to another program&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Touch typing. Since we are going to use Linux, touch typing helps. I know typing
speed does not relate to problem-solving, but at least your typing speed should
be fast enough not to let it get it the way and degrade the learning experience.&lt;/p&gt;
&lt;p&gt;In general, I assume that the reader has basic C programming knowledge, and can
use an IDE to build and run a program.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-status" class="anchor" aria-hidden="true" href="#status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Status:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Part 1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 1: Complete&lt;/li&gt;
&lt;li&gt;Chapter 2: Complete&lt;/li&gt;
&lt;li&gt;Chapter 3: Almost. Currently, the book relies on the Intel Manual for fully explaining x86 execution environment.&lt;/li&gt;
&lt;li&gt;Chapter 4: Complete&lt;/li&gt;
&lt;li&gt;Chapter 5: Complete&lt;/li&gt;
&lt;li&gt;Chapter 6: Complete&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Part 2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 7: Complete&lt;/li&gt;
&lt;li&gt;Chapter 8: Complete&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Part 3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 9: Incomplete&lt;/li&gt;
&lt;li&gt;Chapter 10: Incomplete&lt;/li&gt;
&lt;li&gt;Chapter 11: Incomplete&lt;/li&gt;
&lt;li&gt;Chapter 12: Incomplete&lt;/li&gt;
&lt;li&gt;Chapter 13: Incomplete&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;... and future chapters not included yet ...&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the future, I hope to expand part 3 to cover more than the first 2 parts. But
for the time being, I will try to finish the above chapters first.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-sample-os" class="anchor" aria-hidden="true" href="#sample-os"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample OS&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/tuhdo/sample-os"&gt;This repository&lt;/a&gt; is the sample OS of the
book that is intended as a reference material for part 3. It covers 10 chapters
of the "System Programming Guide" (Intel Manual Volume 3), along with a simple
keyboard and video driver for input and output. However, at the moment, only the
following features are implemented:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Protected mode.&lt;/li&gt;
&lt;li&gt;Creating and managing processes with TSS (Task State Structure).&lt;/li&gt;
&lt;li&gt;Interrupts&lt;/li&gt;
&lt;li&gt;LAPIC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Paging and I/O are not yet implemented. I will try to implement it as the book progresses.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h1&gt;
&lt;p&gt;If you find any grammatical issues, please report it using Github Issues. Or, if
some sentence or paragraph is difficult to understand, feel free to open an
issue with the following title format: &lt;code&gt;[page number][type] Descriptive Title&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example: &lt;code&gt;[pg.9][grammar] Incorrect verb usage&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;type&lt;/code&gt; can be one of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Typo&lt;/code&gt;: indicates typing mistake.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Grammar&lt;/code&gt;: indicates incorrect grammar usage.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Style&lt;/code&gt;: indicates a style improvement.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Content&lt;/code&gt;: indicates problems with the content.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even better, you can make a pull request with the provided book source. The main
content of the book is in the file "Operating Systems: From 0 to 1.lyx". You can
edit the .txt file, then I will integrate the changes manually. It is a
workaround for now since Lyx can cause a huge diff which makes it impossible to
review changes.&lt;/p&gt;
&lt;p&gt;The book is in development, so please bear with me if the English irritates you.
I really appreciate it.&lt;/p&gt;
&lt;p&gt;Finally, if you like the project and if it is possible, please donate to help
this project and keep it going.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-got-questions" class="anchor" aria-hidden="true" href="#got-questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Got questions?&lt;/h1&gt;
&lt;p&gt;If you have any question related to the material or the development of the book,
feel free to &lt;a href="https://github.com/tuhdo/os01/issues/new"&gt;open a Github issue&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tuhdo</author><guid isPermaLink="false">https://github.com/tuhdo/os01</guid><pubDate>Mon, 20 Jan 2020 00:04:00 GMT</pubDate></item><item><title>jacobeisenstein/gt-nlp-class #5 in TeX, Today</title><link>https://github.com/jacobeisenstein/gt-nlp-class</link><description>&lt;p&gt;&lt;i&gt;Course materials for Georgia Tech CS 4650 and 7650, "Natural Language"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cs-4650-and-7650" class="anchor" aria-hidden="true" href="#cs-4650-and-7650"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CS 4650 and 7650&lt;/h1&gt;
&lt;p&gt;(&lt;strong&gt;Note about registration&lt;/strong&gt;: registration is currently restricted to students pursuing CS degrees for which this course is an essential requirement. Unfortunately, the enrollment is already at the limit of the classroom space, so this restriction is unlikely to be lifted.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Course&lt;/strong&gt;: Natural Language Understanding&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instructor&lt;/strong&gt;: Jacob Eisenstein&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Semester&lt;/strong&gt;: Spring 2018&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time&lt;/strong&gt;: Mondays and Wednesdays, 3:00-4:15pm&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TAs&lt;/strong&gt;: Murali Raghu Babu, James Mullenbach, Yuval Pinter, Zhewei Sun&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1BuvRjPhfHmy7XAfpc5KoygdfqI3Cue3bbmiO6yYuX_E/edit?usp=sharing" rel="nofollow"&gt;Schedule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/document/d/1loefqZhmOaF2mP8yQPEx91jZ7BHylWixVtYlFhpIlGM/edit?usp=sharing" rel="nofollow"&gt;Recaps&lt;/a&gt; from previous classes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This course gives an overview of modern data-driven techniques for natural language processing. The course moves from shallow bag-of-words models to richer structural representations of how words interact to create meaning. At each level, we will discuss the salient linguistic phemonena and most successful computational models. Along the way we will cover machine learning techniques which
are especially relevant to natural language processing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#readings"&gt;Readings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grading"&gt;Grading&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#help"&gt;Help&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#policies"&gt;Policies&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-learning-goals" class="anchor" aria-hidden="true" href="#learning-goals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learning goals&lt;/h1&gt;
&lt;a name="user-content-learning"&gt;
&lt;ul&gt;
&lt;li&gt;Acquire the fundamental linguistic concepts that are relevant to language technology. This goal will be assessed in the short homework assignments and the exams.&lt;/li&gt;
&lt;li&gt;Analyze and understand state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the exams and the assigned projects.&lt;/li&gt;
&lt;li&gt;Implement state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the assigned projects.&lt;/li&gt;
&lt;li&gt;Adapt and apply state-of-the-art language technology to new problems and settings. This goal will be assessed in assigned projects.&lt;/li&gt;
&lt;li&gt;(7650 only) Read and understand current research on natural language processing. This goal will be assessed in assigned projects.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-readings" class="anchor" aria-hidden="true" href="#readings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Readings&lt;/h1&gt;
&lt;/a&gt;&lt;a name="user-content-readings"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-readings"&gt;Readings will be drawn mainly from my &lt;/a&gt;&lt;a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes"&gt;notes&lt;/a&gt;. Additional readings may be assigned from published papers, blogposts, and tutorials.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-supplemental-textbooks" class="anchor" aria-hidden="true" href="#supplemental-textbooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supplemental textbooks&lt;/h2&gt;
&lt;p&gt;These are completely optional, but might deepen your understanding of the material.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210/" rel="nofollow"&gt;Speech and Language Processing&lt;/a&gt; is the textbook most often used in NLP courses. It's a great reference for both the linguistics and algorithms we'll encounter in this course. Several chapters from the upcoming &lt;a href="https://web.stanford.edu/~jurafsky/slp3/" rel="nofollow"&gt;third edition&lt;/a&gt; are free online.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.amazon.com/Natural-Language-Processing-Python-Steven/dp/0596516495" rel="nofollow"&gt;Natural Language Processing with Python&lt;/a&gt;
shows how to do hands-on work with Python's Natural Language Toolkit (NLTK), and also brings a strong linguistic perspective.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.amazon.com/Schaums-Outline-Probability-Statistics-Edition/dp/007179557X/ref=pd_sim_b_1?ie=UTF8&amp;amp;refRID=1R57HWNCW6EEWD1ZRH4C" rel="nofollow"&gt;Schaum's Outline of Probability and Statistics&lt;/a&gt; can help you review the probability and statistics that we use in this course.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-grading" class="anchor" aria-hidden="true" href="#grading"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grading&lt;/h1&gt;
&lt;a name="user-content-grading"&gt;
&lt;p&gt;The graded material for the course will consist of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Seven short homework assignments, of which you must do six. Most of these involve performing linguistic annotation on some text of your choice. The purpose is to get a basic understanding of key linguistic concepts. Each assignment should take less than an hour. Each homework is worth 2 points (12 total). (Many of these homeworks are implemented at &lt;strong&gt;quizzes&lt;/strong&gt; on Canvas.)&lt;/li&gt;
&lt;li&gt;Four assigned problem sets. These involve building and using NLP techniques which are at or near the state-of-the-art. The purpose is to learn how to implement natural language processing software, and to have fun. These assignments must be done individually. Each problem set is worth ten points (48 total). Students enrolled in CS 7650 will have an additional, research-oriented component to the problem sets.&lt;/li&gt;
&lt;li&gt;An in-class midterm exam, worth 20 points, and a final exam, worth 20 points. The purpose of these exams is to assess understanding of the core theoretical concepts, and to encourage you to review and synthesize your understanding of these concepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-grading"&gt;Barring a personal emergency or an institute-approved absence, you must take each exam on the day indicated in the schedule. Job interviews and travel plans are generally not a reason for an institute-approved absence. See &lt;/a&gt;&lt;a href="https://registrar.gatech.edu/info/institute-approved-absence-form-for-students" rel="nofollow"&gt;here&lt;/a&gt; for more information on GT policy about absences.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-late-policy" class="anchor" aria-hidden="true" href="#late-policy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Late policy&lt;/h2&gt;
&lt;p&gt;Problem sets will be accepted up to 72 hours late, at a penalty of 2 points per 24 hours. (Maximum score after missing the deadline: 10/12; maximum score 24 hours after the deadline: 8/12, etc.)  It is usually best just to turn in what you have at the due date. Late homeworks will not be accepted. This late policy is intended to ensure fair and timely evaluation.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-getting-help" class="anchor" aria-hidden="true" href="#getting-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting help&lt;/h1&gt;
&lt;a name="user-content-help"&gt;
&lt;h2&gt;&lt;a id="user-content-office-hours" class="anchor" aria-hidden="true" href="#office-hours"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Office hours&lt;/h2&gt;
&lt;p&gt;My office hours follow Wednesday classes (4:15-5:15PM) and take place in class when available.&lt;/p&gt;
&lt;p&gt;TA office hours are in CCB commons (1st floor) unless otherwise announced on Piazza.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Murali: Friday   10AM-11AM&lt;/li&gt;
&lt;li&gt;James:  Thursday 11AM-12PM&lt;/li&gt;
&lt;li&gt;Yuval:  Tuesday  3PM-4PM&lt;/li&gt;
&lt;li&gt;Zhewei: Monday   1PM-2PM&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-online-help" class="anchor" aria-hidden="true" href="#online-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online help&lt;/h2&gt;
&lt;p&gt;Please use Piazza rather than personal email to ask questions. This helps other students, who may have the same question. Personal emails may not be answered. If you cannot make it to office hours, please use Piazza to make an appointment. It is unlikely that I will be able to chat if you make an unscheduled visit to my office. The same is true for the TAs.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-class-policies" class="anchor" aria-hidden="true" href="#class-policies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Class policies&lt;/h1&gt;
&lt;/a&gt;&lt;a name="user-content-policies"&gt;
&lt;p&gt;Attendance will not be taken, but &lt;strong&gt;you are responsible for knowing what happens in every class&lt;/strong&gt;. If you cannot attend class, make sure you check up with someone who was there.&lt;/p&gt;
&lt;p&gt;Respect your classmates and your instructor by preventing distractions. This means be on time, turn off your cellphone, and save side conversations for after class. If you can't read something I wrote on the board, or if you think I made a mistake in a derivation, please raise your hand and tell me!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using a laptop in class is likely to reduce your education attainment&lt;/strong&gt;. This has been documented by multiple studies, which are nicely summarized in the following article:&lt;/p&gt;
&lt;/a&gt;&lt;ul&gt;&lt;a name="user-content-policies"&gt;
&lt;/a&gt;&lt;li&gt;&lt;a name="user-content-policies"&gt;&lt;/a&gt;&lt;a href="https://www.nytimes.com/2017/11/22/business/laptops-not-during-lecture-or-meeting.html" rel="nofollow"&gt;https://www.nytimes.com/2017/11/22/business/laptops-not-during-lecture-or-meeting.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I am not going to ban laptops, as long as they are not a distraction to anyone but the user. But I suggest you try pen and paper for a few weeks, and see if it helps.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h2&gt;
&lt;a name="user-content-prerequisites"&gt;
&lt;p&gt;The official prerequisite for CS 4650 is CS 3510/3511, "Design and Analysis of Algorithms." This prerequisite is essential because understanding natural language processing algorithms requires familiarity with dynamic programming, as well as automata and formal language theory: finite-state and context-free languages, NP-completeness, etc. While course prerequisites are not enforced for graduate students, prior exposure to analysis of algorithms is very strongly recommended.&lt;/p&gt;
&lt;p&gt;Furthermore, this course assumes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Good coding ability, corresponding to at least a third or fourth-year undergraduate CS major. Assignments will be in Python.&lt;/li&gt;
&lt;li&gt;Background in basic probability, linear algebra, and calculus.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;People sometimes want to take the course without having all of these
prerequisites. Frequent cases are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Junior CS students with strong programming skills but limited theoretical and mathematical background,&lt;/li&gt;
&lt;li&gt;Non-CS students with strong mathematical background but limited programming experience.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Students in the first group suffer in the exam and don't understand the lectures, and students in the second group suffer in the problem sets. My advice is to get the background material first, and
then take this course.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-collaboration-policy" class="anchor" aria-hidden="true" href="#collaboration-policy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Collaboration policy&lt;/h2&gt;
&lt;p&gt;One of the goals of the assigned work is to assess your individual progress in meeting the learning objectives of the course. You may discuss the homework and projects with other students, but your work must be your own -- particularly all coding and writing. For example:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examples-of-acceptable-collaboration" class="anchor" aria-hidden="true" href="#examples-of-acceptable-collaboration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples of acceptable collaboration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Alice and Bob discuss alternatives for storing large, sparse vectors of feature counts, as required by a problem set.&lt;/li&gt;
&lt;li&gt;Bob is confused about how to implement the Viterbi algorithm, and asks Alice for a conceptual description of her strategy.&lt;/li&gt;
&lt;li&gt;Alice asks Bob if he encountered a failure condition at a "sanity check" in a coding assignment, and Bob explains at a conceptual level how he overcame that failure condition.&lt;/li&gt;
&lt;li&gt;Alice is having trouble getting adequate performance from her part-of-speech tagger. She finds a blog page or research paper that gives her some new ideas, which she implements.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-examples-of-unacceptable-collaboration" class="anchor" aria-hidden="true" href="#examples-of-unacceptable-collaboration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples of unacceptable collaboration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Alice and Bob work together to write code for storing feature counts.&lt;/li&gt;
&lt;li&gt;Alice and Bob divide the assignment into parts, and each write the code for their part, and then share their solutions with each other to complete the assignment.&lt;/li&gt;
&lt;li&gt;Alice or Bob obtain a solution to a previous year's assignment or to a related assignment in another class, and use it as the starting point for their own solutions.&lt;/li&gt;
&lt;li&gt;Bob is having trouble getting adequate performance from his part-of-speech tagger. He finds source code online, and copies it into his own submission.&lt;/li&gt;
&lt;li&gt;Alice wants to win the Kaggle competition for a problem set. She finds the test set online, and customizes her submission to do well on it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some assignments will involve written responses. Using other people’s text or figures without attribution is plagiarism, and is never acceptable.&lt;/p&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-prerequisites"&gt;Suspected cases of academic misconduct will be (and have been!) referred to the Honor Advisory Council. For any questions involving these or any other Academic Honor Code issues, please consult me, my teaching assistants, or &lt;/a&gt;&lt;a href="http://www.honor.gatech.edu" rel="nofollow"&gt;http://www.honor.gatech.edu&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jacobeisenstein</author><guid isPermaLink="false">https://github.com/jacobeisenstein/gt-nlp-class</guid><pubDate>Mon, 20 Jan 2020 00:05:00 GMT</pubDate></item><item><title>lervag/vimtex #6 in TeX, Today</title><link>https://github.com/lervag/vimtex</link><description>&lt;p&gt;&lt;i&gt;A modern vim plugin for editing LaTeX files.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-vimtex" class="anchor" aria-hidden="true" href="#vimtex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;vimtex&lt;/h1&gt;
&lt;p&gt;vimtex is a &lt;a href="http://www.vim.org/" rel="nofollow"&gt;Vim&lt;/a&gt; plugin that provides support for writing
LaTeX documents. It is based on
&lt;a href="https://github.com/LaTeX-Box-Team/LaTeX-Box"&gt;LaTeX-Box&lt;/a&gt; and it shares a
similar goal: to provide a simple and lightweight LaTeX plugin. It has been
rewritten from scratch to provide a more modern and modular code base. See
&lt;a href="#alternatives"&gt;here&lt;/a&gt; for some more comments on the difference between vimtex
and other LaTeX plugins for Vim.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/lervag/vimtex/workflows/main.yml/badge.svg"&gt;&lt;img src="https://github.com/lervag/vimtex/workflows/main.yml/badge.svg" alt="Build Status" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;amp;hosted_button_id=5N4MFVXN7U8NW" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d5d24e33e2f4b6fe53987419a21b203c03789a8f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d677265656e2e737667" alt="Donate" data-canonical-src="https://img.shields.io/badge/Donate-PayPal-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#features"&gt;Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-relevant-plugins"&gt;Other relevant plugins&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#alternatives"&gt;Alternatives&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;If you use &lt;a href="https://github.com/junegunn/vim-plug"&gt;vim-plug&lt;/a&gt;, then add the
following line to your &lt;code&gt;vimrc&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight highlight-source-viml"&gt;&lt;pre&gt;Plug &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;lervag/vimtex&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or use some other plugin manager:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/gmarik/vundle"&gt;vundle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Shougo/neobundle.vim"&gt;neobundle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tpope/vim-pathogen"&gt;pathogen&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you use the new package feature in Vim, please note the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make sure to read and understand the package feature: &lt;code&gt;:help package&lt;/code&gt;!&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;/pack/foo/start&lt;/code&gt; subdirectory to make sure the filetype plugin is automatically loaded for the &lt;code&gt;tex&lt;/code&gt; filetypes.&lt;/li&gt;
&lt;li&gt;Helptags are not generated automatically. Run &lt;code&gt;:helptags&lt;/code&gt; to generate them.&lt;/li&gt;
&lt;li&gt;Please note that by default Vim puts custom &lt;code&gt;/start/&lt;/code&gt; plugin directories at the end of the &lt;code&gt;&amp;amp;runtimepath&lt;/code&gt;. This means the built in filetype plugin is loaded, which prevents Vimtex from loading. See &lt;a href="https://github.com/lervag/vimtex/issues/1413"&gt;#1413&lt;/a&gt; for two suggested solutions to this. To see which scripts are loaded and in which order, use &lt;code&gt;:scriptnames&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;For more information on how to use the Vim native package solution, see &lt;a href="https://vi.stackexchange.com/questions/9522/what-is-the-vim8-package-feature-and-how-should-i-use-it" rel="nofollow"&gt;here&lt;/a&gt; and &lt;a href="https://shapeshed.com/vim-packages/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;The following is a simple guide for how to use vimtex. It only displays the
most basic features. Users are &lt;em&gt;strongly&lt;/em&gt; encouraged to read or at least skim
through the documentation to learn about the different features and
possibilities provided by vimtex (see &lt;a href="doc/vimtex.txt"&gt;&lt;code&gt;:h vimtex&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Note: Vimtex supports neovim; see the &lt;a href="https://github.com/lervag/vimtex/wiki/introduction#neovim"&gt;related wiki
section&lt;/a&gt; or &lt;code&gt;:h vimtex-faq-neovim&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="media/quick_start.gif?raw=true"&gt;&lt;img src="media/quick_start.gif?raw=true" alt="Quick start gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;p&gt;Below is a list of features offered by vimtex.  The features are accessible as
both commands and mappings.  The mappings generally start with
&lt;code&gt;&amp;lt;localleader&amp;gt;l&lt;/code&gt;, but if desired one can disable default mappings to define
custom mappings.  All features are enabled by default, but each feature may be
disabled if desired.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Document compilation with
&lt;a href="http://users.phys.psu.edu/~collins/software/latexmk-jcc/" rel="nofollow"&gt;latexmk&lt;/a&gt;,
&lt;a href="https://github.com/aclements/latexrun"&gt;latexrun&lt;/a&gt;,
&lt;a href="https://tectonic-typesetting.github.io" rel="nofollow"&gt;tectonic&lt;/a&gt;, or
&lt;a href="https://github.com/cereda/arara"&gt;arara&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LaTeX log parsing for quickfix entries using
&lt;ul&gt;
&lt;li&gt;internal method&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/stefanhepp/pplatex"&gt;pplatex&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compilation of selected part of document&lt;/li&gt;
&lt;li&gt;Support for several PDF viewers with forward search
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.mupdf.com/" rel="nofollow"&gt;MuPDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://okular.kde.org/" rel="nofollow"&gt;Okular&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://launchpad.net/qpdfview" rel="nofollow"&gt;qpdfview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://skim-app.sourceforge.net/" rel="nofollow"&gt;Skim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.sumatrapdfreader.org/free-pdf-reader.html" rel="nofollow"&gt;SumatraPDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pwmt.org/projects/zathura/" rel="nofollow"&gt;Zathura&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Other viewers are supported through a general interface&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Completion of
&lt;ul&gt;
&lt;li&gt;citations&lt;/li&gt;
&lt;li&gt;labels&lt;/li&gt;
&lt;li&gt;commands&lt;/li&gt;
&lt;li&gt;file names for figures, input/include, includepdf, includestandalone&lt;/li&gt;
&lt;li&gt;glossary entries&lt;/li&gt;
&lt;li&gt;package and documentclass names based on available &lt;code&gt;.sty&lt;/code&gt; and &lt;code&gt;.cls&lt;/code&gt; files&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Document navigation through
&lt;ul&gt;
&lt;li&gt;table of content&lt;/li&gt;
&lt;li&gt;table of labels&lt;/li&gt;
&lt;li&gt;proper settings for &lt;code&gt;'include'&lt;/code&gt;, &lt;code&gt;'includexpr'&lt;/code&gt;, &lt;code&gt;'suffixesadd'&lt;/code&gt; and
&lt;code&gt;'define'&lt;/code&gt;, which among other things
&lt;ul&gt;
&lt;li&gt;allow &lt;code&gt;:h include-search&lt;/code&gt; and &lt;code&gt;:h definition-search&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;give enhanced &lt;code&gt;gf&lt;/code&gt; command&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Easy access to (online) documentation of packages&lt;/li&gt;
&lt;li&gt;Word count (through &lt;code&gt;texcount&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Motions
&lt;ul&gt;
&lt;li&gt;Move between section boundaries with &lt;code&gt;[[&lt;/code&gt;, &lt;code&gt;[]&lt;/code&gt;, &lt;code&gt;][&lt;/code&gt;, and &lt;code&gt;]]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Move between environment boundaries with &lt;code&gt;[m&lt;/code&gt;, &lt;code&gt;[M&lt;/code&gt;, &lt;code&gt;]m&lt;/code&gt;, and &lt;code&gt;]M&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Move between comment boundaries with &lt;code&gt;[*&lt;/code&gt; and &lt;code&gt;]*&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Move between matching delimiters with &lt;code&gt;%&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Text objects
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ic ac&lt;/code&gt; Commands&lt;/li&gt;
&lt;li&gt;&lt;code&gt;id ad&lt;/code&gt; Delimiters&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ie ae&lt;/code&gt; LaTeX environments&lt;/li&gt;
&lt;li&gt;&lt;code&gt;i$ a$&lt;/code&gt; Inline math structures&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iP aP&lt;/code&gt; Sections&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Other mappings
&lt;ul&gt;
&lt;li&gt;Delete the surrounding command, environment or delimiter with
&lt;code&gt;dsc&lt;/code&gt;/&lt;code&gt;dse&lt;/code&gt;/&lt;code&gt;ds$&lt;/code&gt;/&lt;code&gt;dsd&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Change the surrounding command, environment or delimiter with
&lt;code&gt;csc&lt;/code&gt;/&lt;code&gt;cse&lt;/code&gt;/&lt;code&gt;cs$&lt;/code&gt;/&lt;code&gt;csd&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Toggle starred command or environment with &lt;code&gt;tsc&lt;/code&gt;/&lt;code&gt;tse&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Toggle between e.g. &lt;code&gt;()&lt;/code&gt; and &lt;code&gt;\left(\right)&lt;/code&gt; with &lt;code&gt;tsd&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Close the current environment/delimiter in insert mode with &lt;code&gt;]]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Insert new command with &lt;code&gt;&amp;lt;F7&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Convenient insert mode mappings for faster typing of e.g. maths&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Improved folding (&lt;code&gt;:h 'foldexpr'&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Improved indentation (&lt;code&gt;:h 'indentexpr'&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Improved syntax highlighting
&lt;ul&gt;
&lt;li&gt;Highlight matching delimiters&lt;/li&gt;
&lt;li&gt;Support for &lt;code&gt;biblatex&lt;/code&gt;/&lt;code&gt;natbib&lt;/code&gt; package&lt;/li&gt;
&lt;li&gt;Support for &lt;code&gt;cleveref&lt;/code&gt; package&lt;/li&gt;
&lt;li&gt;Support for &lt;code&gt;listings&lt;/code&gt; package&lt;/li&gt;
&lt;li&gt;Nested syntax highlighting (&lt;code&gt;minted&lt;/code&gt;, &lt;code&gt;dot2tex&lt;/code&gt;, &lt;code&gt;lualatex&lt;/code&gt;,
&lt;code&gt;gnuplottex&lt;/code&gt;, &lt;code&gt;asymptote&lt;/code&gt;, &lt;code&gt;pythontex&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Support for multi-file project packages
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ctan.uib.no/macros/latex/contrib/import/import.pdf" rel="nofollow"&gt;import&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ctan.uib.no/macros/latex/contrib/subfiles/subfiles.pdf" rel="nofollow"&gt;subfiles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See the documentation for a thorough introduction to vimtex (e.g. &lt;code&gt;:h vimtex&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-other-relevant-plugins" class="anchor" aria-hidden="true" href="#other-relevant-plugins"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other relevant plugins&lt;/h2&gt;
&lt;p&gt;Even though vimtex provides a lot of nice features for working with LaTeX
documents, there are several features that are better served by other,
dedicated plugins. For a more detailed listing of these, please see &lt;a href="doc/vimtex.txt#L156"&gt;&lt;code&gt;:help vimtex-non-features&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-linting-and-syntax-checking" class="anchor" aria-hidden="true" href="#linting-and-syntax-checking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linting and syntax checking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/w0rp/ale"&gt;ale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/neomake/neomake"&gt;neomake&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vim-syntastic/syntastic"&gt;syntastic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-snippets-and-templates" class="anchor" aria-hidden="true" href="#snippets-and-templates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Snippets and templates&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/SirVer/ultisnips"&gt;UltiSnips&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Shougo/neosnippet.vim"&gt;neosnippet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-tag-navigation" class="anchor" aria-hidden="true" href="#tag-navigation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tag navigation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ludovicchabant/vim-gutentags"&gt;vim-gutentags&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-alternatives" class="anchor" aria-hidden="true" href="#alternatives"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Alternatives&lt;/h2&gt;
&lt;p&gt;The following are some alternative LaTeX plugins for Vim:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/LaTeX-Box-Team/LaTeX-Box"&gt;LaTeX-Box&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;vimtex currently has most of the features of LaTeX-Box, as well as
some additional ones. See &lt;a href="#features"&gt;here&lt;/a&gt; for a relatively complete list
of features.&lt;/p&gt;
&lt;p&gt;One particular feature that LaTeX-Box has but vimtex misses, is the ability
to do single-shot compilation &lt;em&gt;with callback&lt;/em&gt;. This functionality was
removed because it adds a lot of complexity for relatively little gain
(IMHO).&lt;/p&gt;
&lt;p&gt;Note: LaTeX-Box is included with
&lt;a href="https://github.com/sheerun/vim-polyglot"&gt;vim-polyglot&lt;/a&gt;. Some users are not
quite aware of this and end up trying vimtex with LaTeX-Box enabled. This
will not work --- please disable LaTeX-Box first!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://vim-latex.sourceforge.net" rel="nofollow"&gt;LaTeX-Suite&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The main difference between vimtex and LaTeX-Suite (aka vim-latex) is
probably that vimtex does not try to implement a full fledged IDE for LaTeX
inside Vim. E.g.:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;vimtex does not provide a full snippet feature, because this is better
handled by &lt;a href="https://github.com/SirVer/ultisnips"&gt;UltiSnips&lt;/a&gt; or
&lt;a href="https://github.com/Shougo/neosnippet.vim"&gt;neosnippet&lt;/a&gt; or similar snippet
engines.&lt;/li&gt;
&lt;li&gt;vimtex builds upon Vim principles: It provides text objects for
environments, inline math, it provides motions for sections and
paragraphs&lt;/li&gt;
&lt;li&gt;vimtex uses &lt;code&gt;latexmk&lt;/code&gt;, &lt;code&gt;latexrun&lt;/code&gt;, &lt;code&gt;tectonic&lt;/code&gt; or &lt;code&gt;arara&lt;/code&gt; for compilation
with a callback feature to get instant feedback on compilation errors&lt;/li&gt;
&lt;li&gt;vimtex is very modular: if you don't like a feature, you can turn it off.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://atp-vim.sourceforge.net" rel="nofollow"&gt;AutomaticTexPlugin&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/xuhdev/vim-latex-live-preview"&gt;vim-latex-live-preview&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more alternatives and more information and discussions regarding LaTeX
plugins for Vim, see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://vi.stackexchange.com/questions/2047/what-are-the-differences-between-latex-plugins" rel="nofollow"&gt;What are the differences between LaTeX
plugins&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tex.stackexchange.com/questions/339/latex-editors-ides" rel="nofollow"&gt;List of LaTeX editors (not only
Vim)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>lervag</author><guid isPermaLink="false">https://github.com/lervag/vimtex</guid><pubDate>Mon, 20 Jan 2020 00:06:00 GMT</pubDate></item><item><title>Wandmalfarbe/pandoc-latex-template #7 in TeX, Today</title><link>https://github.com/Wandmalfarbe/pandoc-latex-template</link><description>&lt;p&gt;&lt;i&gt;A pandoc LaTeX template to convert markdown files to PDF or LaTeX.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="icon.png"&gt;&lt;img src="icon.png" align="right" height="110" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-eisvogel" class="anchor" aria-hidden="true" href="#eisvogel"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eisvogel&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/Wandmalfarbe/pandoc-latex-template" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b87755780c096b231f6f2e8a6192d30318d187a3/68747470733a2f2f7472617669732d63692e6f72672f57616e646d616c66617262652f70616e646f632d6c617465782d74656d706c6174652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/Wandmalfarbe/pandoc-latex-template.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A clean &lt;strong&gt;pandoc LaTeX template&lt;/strong&gt; to convert your markdown files to PDF or LaTeX. It is designed for lecture notes and exercises with a focus on computer science. The template is compatible with pandoc 2.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A custom title page&lt;/th&gt;
&lt;th align="center"&gt;A basic example page&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/custom-titlepage/custom-titlepage.pdf"&gt;&lt;img src="examples/custom-titlepage/custom-titlepage.png" alt="A custom title page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/basic-example/basic-example.pdf"&gt;&lt;img src="examples/basic-example/basic-example.png" alt="A basic example page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install pandoc from &lt;a href="http://pandoc.org/" rel="nofollow"&gt;http://pandoc.org/&lt;/a&gt;. You also need to install &lt;a href="https://en.wikibooks.org/wiki/LaTeX/Installation#Distributions" rel="nofollow"&gt;LaTeX&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download the latest version of the Eisvogel template from &lt;a href="https://github.com/Wandmalfarbe/pandoc-latex-template/releases/latest"&gt;the release page&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extract the downloaded ZIP archive and open the folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Move the template &lt;code&gt;eisvogel.tex&lt;/code&gt; to your pandoc templates folder and rename the file to &lt;code&gt;eisvogel.latex&lt;/code&gt;. The location of the templates folder depends on your operating system:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unix, Linux, macOS: &lt;code&gt;/Users/USERNAME/.local/share/pandoc/templates/&lt;/code&gt; or &lt;code&gt;/Users/USERNAME/.pandoc/templates/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Windows Vista or later: &lt;code&gt;C:\Users\USERNAME\AppData\Roaming\pandoc\templates\&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If there are no folders called &lt;code&gt;templates&lt;/code&gt; or &lt;code&gt;pandoc&lt;/code&gt; you need to create them and put the template &lt;code&gt;eisvogel.latex&lt;/code&gt; inside. You can find the default user data directory on your system by looking at the output of &lt;code&gt;pandoc --version&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Open the terminal and navigate to the folder where your markdown file is located.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Execute the following command&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --from markdown --template eisvogel --listings&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where &lt;code&gt;example.md&lt;/code&gt; is the markdown file you want to convert to PDF.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to have nice headers and footers you need to supply metadata to your document. You can do that with a &lt;a href="http://pandoc.org/MANUAL.html#extension-yaml_metadata_block" rel="nofollow"&gt;YAML metadata block&lt;/a&gt; at the top of your markdown document (see the &lt;a href="examples/basic-example/basic-example.md"&gt;example markdown file&lt;/a&gt;). Your markdown document may look like the following:&lt;/p&gt;
&lt;div class="highlight highlight-source-gfm"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;---&lt;/span&gt;
&lt;span class="pl-ent"&gt;title&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;The Document Title&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-ent"&gt;author&lt;/span&gt;: &lt;span class="pl-s"&gt;[Example Author, Another Author]&lt;/span&gt;
&lt;span class="pl-ent"&gt;date&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;2017-02-20&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-ent"&gt;keywords&lt;/span&gt;: &lt;span class="pl-s"&gt;[Markdown, Example]&lt;/span&gt;
&lt;span class="pl-c"&gt;...&lt;/span&gt;

Here is the actual document text...&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-custom-template-variables" class="anchor" aria-hidden="true" href="#custom-template-variables"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Template Variables&lt;/h3&gt;
&lt;p&gt;This template defines some new variables to control the appearance of the resulting PDF document. The existing template variables from pandoc are all supported and their documentation can be found in &lt;a href="https://pandoc.org/MANUAL.html#variables-for-latex" rel="nofollow"&gt;the pandoc manual&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;turns on the title page when &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-color&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the background color of the title page. The color value must be given as an HTML hex color like &lt;code&gt;D8DE2C&lt;/code&gt; without the leading number sign (&lt;code&gt;#&lt;/code&gt;). When specifying the color in YAML, it is advisable to enclose it in quotes like so &lt;code&gt;titlepage-color: "D8DE2C"&lt;/code&gt; to avoid the truncation of the color (e.g. &lt;code&gt;000000&lt;/code&gt; becoming &lt;code&gt;0&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-text-color&lt;/code&gt; (defaults to &lt;code&gt;5F5F5F&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the text color of the title page&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-rule-color&lt;/code&gt; (defaults to &lt;code&gt;435488&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the color of the rule on the top of the title page&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-rule-height&lt;/code&gt; (defaults to &lt;code&gt;4&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the height of the rule on the top of the title page (in points)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-background&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the path to a background image for the title page. The background image is scaled to cover the entire page. In the examples folder under &lt;code&gt;titlepage-background&lt;/code&gt; are a few example background images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;page-background&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the path to a background image for any page. The background image is scaled to cover the entire page. In the examples folder under &lt;code&gt;page-background&lt;/code&gt; are a few example background images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;page-background-opacity&lt;/code&gt; (defaults to &lt;code&gt;0.2&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the background image opacity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;caption-justification&lt;/code&gt; (defaults to &lt;code&gt;raggedright&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;justification setting for captions (uses the &lt;code&gt;justification&lt;/code&gt; parameter of the &lt;a href="https://ctan.org/pkg/caption?lang=en" rel="nofollow"&gt;caption&lt;/a&gt; package)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;toc-own-page&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;begin new page after table of contents, when &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;listings-disable-line-numbers&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;disables line numbers for all listings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;listings-no-page-break&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;avoid page break inside listings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;disable-header-and-footer&lt;/code&gt; (default to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;disables the header and footer completely on all pages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;header-left&lt;/code&gt; (defaults to the title)&lt;/p&gt;
&lt;p&gt;the text on the left side of the header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;header-center&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the text in the center of the header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;header-right&lt;/code&gt; (defaults to the date)&lt;/p&gt;
&lt;p&gt;the text on the right side of the header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footer-left&lt;/code&gt; (defaults to the author)&lt;/p&gt;
&lt;p&gt;the text on the left side of the footer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footer-center&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the text in the center of the footer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footer-right&lt;/code&gt; (defaults to the page number)&lt;/p&gt;
&lt;p&gt;the text on the right side of the footer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footnotes-pretty&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;prettifies formatting of footnotes (requires package &lt;code&gt;footmisc&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footnotes-disable-backlinks&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;disables making the reference from the footnote at the bottom of the page into a link back to the occurence of the footnote in the main text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;book&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;typeset as book&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;logo&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;path to an image that will be displayed on the title page. The path is always relative to where pandoc is executed. The option &lt;code&gt;--resource-path&lt;/code&gt; has no effect.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;logo-width&lt;/code&gt; (defaults to &lt;code&gt;100&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the width of the logo (in points)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;first-chapter&lt;/code&gt; (defaults to &lt;code&gt;1&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;if typesetting a book with chapter numbers, specifies the number that will be assigned to the first chapter&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;float-placement-figure&lt;/code&gt; (defaults to &lt;code&gt;H&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;Reset the default placement specifier for figure environments to the supplied value e.g. &lt;code&gt;htbp&lt;/code&gt;. The available specifiers are listed below. The first four placement specifiers can be combined.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;h&lt;/code&gt;: Place the float &lt;em&gt;here&lt;/em&gt;, i.e., approximately at the same point it occurs in the source text.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;t&lt;/code&gt;: Place the float at the &lt;em&gt;top&lt;/em&gt; of the page.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt;: Place the float at the &lt;em&gt;bottom&lt;/em&gt; of the page.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;p&lt;/code&gt;: Place the float on the next &lt;em&gt;page&lt;/em&gt; that will contain only floats like figures and tables.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;H&lt;/code&gt;: Place the float &lt;em&gt;HERE&lt;/em&gt; (exactly where it occurs in the source text). The &lt;code&gt;H&lt;/code&gt; specifier is provided by the &lt;a href="https://ctan.org/pkg/float" rel="nofollow"&gt;float package&lt;/a&gt; and may not be used in conjunction with any other placement specifiers.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;table-use-row-colors&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;enables row colors for tables. The default value is &lt;code&gt;false&lt;/code&gt; because the coloring extends beyond the edge of the table and there is currently no way to change that.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;code-block-font-size&lt;/code&gt; (defaults to &lt;code&gt;\small&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;LaTeX command to change the font size for code blocks. The available values are &lt;code&gt;\tiny&lt;/code&gt;, &lt;code&gt;\scriptsize&lt;/code&gt;, &lt;code&gt;\footnotesize&lt;/code&gt;, &lt;code&gt;\small&lt;/code&gt;, &lt;code&gt;\normalsize&lt;/code&gt;, &lt;code&gt;\large&lt;/code&gt;, &lt;code&gt;\Large&lt;/code&gt;, &lt;code&gt;\LARGE&lt;/code&gt;, &lt;code&gt;\huge&lt;/code&gt; and &lt;code&gt;\Huge&lt;/code&gt;. This option will change the font size for default code blocks using the verbatim environment and for code blocks generated with listings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-required-latex-packages" class="anchor" aria-hidden="true" href="#required-latex-packages"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Required LaTeX Packages&lt;/h2&gt;
&lt;p&gt;LaTeX manages addons and additional functionality in so called packages. You
might get the following error when compiling a document with the Eisvogel
template:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;!&lt;/span&gt; LaTeX Error: File &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;`&lt;/span&gt;footnotebackref.sty&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt; not found.&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;Type X to quit or &amp;lt;RETURN&amp;gt; to proceed,&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;or enter new name. (Default extension: sty)&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;Enter file name:&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;! Emergency stop.&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;&amp;lt;read *&amp;gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;LaTeX informs you that the additional package &lt;code&gt;footnotebackref&lt;/code&gt; is required to
render the document.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-texlive" class="anchor" aria-hidden="true" href="#texlive"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Texlive&lt;/h3&gt;
&lt;p&gt;Eisvogel requires a full texlive distribution that can be installed by running
&lt;code&gt;apt-get install texlive-full&lt;/code&gt; in the terminal. Because &lt;code&gt;texlive-full&lt;/code&gt; is very
large (about 5 Gigabytes) you can also install the smaller texlive bundles and
add any missing packages manually.&lt;/p&gt;
&lt;p&gt;A smaller texlive bundle is &lt;code&gt;texlive-latex-extra&lt;/code&gt;. With &lt;code&gt;texlive-latex-extra&lt;/code&gt;
you also need to install these packages manually:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;adjustbox babel-german background bidi collectbox csquotes everypage filehook
footmisc footnotebackref framed fvextra letltxmacro ly1 mdframed mweights
needspace pagecolor sourcecodepro sourcesanspro titling ucharcat ulem
unicode-math upquote xecjk xurl zref
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install them with the following command:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;tlmgt install adjustbox babel-german background bidi collectbox csquotes everypage filehook footmisc footnotebackref framed fvextra letltxmacro ly1 mdframed mweights needspace pagecolor sourcecodepro sourcesanspro titling ucharcat ulem unicode-math upquote xecjk xurl zref&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Additional information about the different texlive packages can be found at
this TeX-StackExchange answer: &lt;a href="https://tex.stackexchange.com/a/504566" rel="nofollow"&gt;https://tex.stackexchange.com/a/504566&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-miktex" class="anchor" aria-hidden="true" href="#miktex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MiKTeX&lt;/h3&gt;
&lt;p&gt;If you don't want to install all missing packages manually, &lt;a href="https://miktex.org/howto/miktex-console" rel="nofollow"&gt;MiKTeX might be
an alternative&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;MiKTeX has the ability to automatically install missing packages.
You can turn this feature on or off. And you can let MiKTeX ask you each time a package has to be installed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Click &lt;code&gt;Settings&lt;/code&gt; to navigate to the settings page.&lt;/li&gt;
&lt;li&gt;Click the &lt;code&gt;General&lt;/code&gt; tab.&lt;/li&gt;
&lt;li&gt;Click one of the radio buttons:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Ask me&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Always install missing packages on-the-fly&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Never install missing packages on-the-fly&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-numbered-sections" class="anchor" aria-hidden="true" href="#numbered-sections"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Numbered Sections&lt;/h3&gt;
&lt;p&gt;For PDFs with &lt;a href="http://pandoc.org/MANUAL.html#options-affecting-specific-writers" rel="nofollow"&gt;numbered sections&lt;/a&gt; use the &lt;code&gt;--number-sections&lt;/code&gt; or &lt;code&gt;-N&lt;/code&gt; option.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --number-sections&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-syntax-highlighting-with-listings" class="anchor" aria-hidden="true" href="#syntax-highlighting-with-listings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syntax Highlighting with Listings&lt;/h3&gt;
&lt;p&gt;You can get syntax highlighting of delimited code blocks by using the LaTeX package listings with the option &lt;code&gt;--listings&lt;/code&gt;. This example will produce the same syntax highlighting as in the example PDF.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --listings&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-syntax-highlighting-without-listings" class="anchor" aria-hidden="true" href="#syntax-highlighting-without-listings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syntax Highlighting Without Listings&lt;/h3&gt;
&lt;p&gt;The following examples show &lt;a href="http://pandoc.org/MANUAL.html#syntax-highlighting" rel="nofollow"&gt;syntax highlighting of delimited code blocks&lt;/a&gt; without using listings. To see a list of all the supported highlight styles, type &lt;code&gt;pandoc --list-highlight-styles&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style pygments&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style kate&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style espresso&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style tango&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-standalone-latex-document" class="anchor" aria-hidden="true" href="#standalone-latex-document"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Standalone LaTeX Document&lt;/h3&gt;
&lt;p&gt;To produce a standalone LaTeX document for compiling with any LaTeX editor use &lt;code&gt;.tex&lt;/code&gt; as an output file extension.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.tex --template eisvogel&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-changing-the-document-language" class="anchor" aria-hidden="true" href="#changing-the-document-language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changing the Document Language&lt;/h3&gt;
&lt;p&gt;The default language of this template is American English. The &lt;code&gt;lang&lt;/code&gt; variable identifies the main language of the document, using a code according to &lt;a href="https://tools.ietf.org/html/bcp47" rel="nofollow"&gt;BCP 47&lt;/a&gt; (e.g. &lt;code&gt;en&lt;/code&gt; or &lt;code&gt;en-GB&lt;/code&gt;). For an incomplete list of the supported language codes see &lt;a href="http://mirrors.ctan.org/language/hyph-utf8/doc/generic/hyph-utf8/hyph-utf8.pdf" rel="nofollow"&gt;the documentation for the hyph-utf8 package (Section 2)&lt;/a&gt;. The following example changes the language to British English:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel -V lang=en-GB&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The following example changes the language to German:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel -V lang=de&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-typesetting-a-book" class="anchor" aria-hidden="true" href="#typesetting-a-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Typesetting a Book&lt;/h3&gt;
&lt;p&gt;To typeset a book supply the template variable &lt;code&gt;-V book&lt;/code&gt; from the command line or via &lt;code&gt;book: true&lt;/code&gt; in the metadata.&lt;/p&gt;
&lt;p&gt;To get the correct chapter headings you need to tell pandoc that it should convert first level headings (indicated by one &lt;code&gt;#&lt;/code&gt; in markdown) to chapters with the command line option &lt;code&gt;--top-level-division=chapter&lt;/code&gt;. Chapter numbers start at 1. If you need to change that, specify &lt;code&gt;first-chapter&lt;/code&gt; in the template variables.&lt;/p&gt;
&lt;p&gt;There will be one blank page before each chapter because the template is two-sided per default. So if you plan to publish your book as a PDF and don’t need a blank page you should add the class option &lt;code&gt;onesided&lt;/code&gt; which can be done by supplying a template variable &lt;code&gt;-V classoption=oneside&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-example-images" class="anchor" aria-hidden="true" href="#example-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example Images&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A green title page&lt;/th&gt;
&lt;th align="center"&gt;A background image on the title page&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/green-titlepage/green-titlepage.pdf"&gt;&lt;img src="examples/green-titlepage/green-titlepage.png" alt="A green title page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/titlepage-background/titlepage-background.pdf"&gt;&lt;img src="examples/titlepage-background/titlepage-background.png" alt="A background image on the title page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;images and tables&lt;/th&gt;
&lt;th align="center"&gt;Code blocks styled without listings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/images-and-tables/images-and-tables.pdf"&gt;&lt;img src="examples/images-and-tables/images-and-tables.png" alt="images and tables" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/without-listings/without-listings.pdf"&gt;&lt;img src="examples/without-listings/without-listings.png" alt="Code blocks styled without listings" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A book&lt;/th&gt;
&lt;th align="center"&gt;Code blocks styled with listings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/book/book.pdf"&gt;&lt;img src="examples/book/book.png" alt="A book" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/listings/listings.pdf"&gt;&lt;img src="examples/listings/listings.png" alt="Code blocks styled with listings" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A background images on all pages&lt;/th&gt;
&lt;th align="center"&gt;CJK Support (when using XeLaTeX)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/page-background/page-background.pdf"&gt;&lt;img src="examples/page-background/page-background.png" alt="A background images on all pages" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/japanese/japanese.pdf"&gt;&lt;img src="examples/japanese/japanese.png" alt="CJK Support (when using XeLaTeX)" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-credits" class="anchor" aria-hidden="true" href="#credits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credits&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This template includes code for styling block quotations from &lt;a href="https://github.com/aaronwolen/pandoc-letter"&gt;pandoc-letter&lt;/a&gt; by &lt;a href="https://github.com/aaronwolen"&gt;Aaron Wolen&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This project is open source licensed under the BSD 3-Clause License. Please see the &lt;a href="LICENSE"&gt;LICENSE file&lt;/a&gt; for more information.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Wandmalfarbe</author><guid isPermaLink="false">https://github.com/Wandmalfarbe/pandoc-latex-template</guid><pubDate>Mon, 20 Jan 2020 00:07:00 GMT</pubDate></item><item><title>posquit0/Awesome-CV #8 in TeX, Today</title><link>https://github.com/posquit0/Awesome-CV</link><description>&lt;p&gt;&lt;i&gt;:page_facing_up: Awesome CV is LaTeX template for your outstanding job application&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1 align="center"&gt;&lt;a id="user-content-------------awesome-cv" class="anchor" aria-hidden="true" href="#------------awesome-cv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
  &lt;a href="https://github.com/posquit0/Awesome-CV" title="AwesomeCV Documentation"&gt;
    &lt;img alt="AwesomeCV" src="https://github.com/posquit0/Awesome-CV/raw/master/icon.png" width="200px" height="200px" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;br&gt;
  Awesome CV
&lt;/h1&gt;
&lt;p align="center"&gt;
  LaTeX template for your outstanding job application
&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a href="https://www.paypal.me/posquit0" rel="nofollow"&gt;
    &lt;img alt="Donate" src="https://camo.githubusercontent.com/abbdd7bf97ae7919db5962b255f40aded5189c4f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d626c75652e737667" data-canonical-src="https://img.shields.io/badge/Donate-PayPal-blue.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://circleci.com/gh/posquit0/Awesome-CV" rel="nofollow"&gt;
    &lt;img alt="CircleCI" src="https://camo.githubusercontent.com/d42593802854990d35ca42943e478dd35d6c64c9/68747470733a2f2f636972636c6563692e636f6d2f67682f706f7371756974302f417765736f6d652d43562e7376673f7374796c653d736869656c64" data-canonical-src="https://circleci.com/gh/posquit0/Awesome-CV.svg?style=shield" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;
    &lt;img alt="Example Resume" src="https://camo.githubusercontent.com/836d3a9f44da3462e5c47b6c58bf066bffbaf739/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f726573756d652d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/resume-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/cv.pdf" rel="nofollow"&gt;
    &lt;img alt="Example CV" src="https://camo.githubusercontent.com/8afab53a91bc30d0da18a9ea0cc70f2d0a1571df/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f63762d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/cv-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;
    &lt;img alt="Example Coverletter" src="https://camo.githubusercontent.com/ce88ed0c1af9e5611df67818460447b69572ae9d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f7665726c65747465722d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/coverletter-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-awesome-cv" class="anchor" aria-hidden="true" href="#what-is-awesome-cv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is Awesome CV?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Awesome CV&lt;/strong&gt; is LaTeX template for a &lt;strong&gt;CV(Curriculum Vitae)&lt;/strong&gt;, &lt;strong&gt;Résumé&lt;/strong&gt; or &lt;strong&gt;Cover Letter&lt;/strong&gt; inspired by &lt;a href="https://www.sharelatex.com/templates/cv-or-resume/fancy-cv" rel="nofollow"&gt;Fancy CV&lt;/a&gt;. It is easy to customize your own template, especially since it is really written by a clean, semantic markup.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-donate" class="anchor" aria-hidden="true" href="#donate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Donate&lt;/h2&gt;
&lt;p&gt;Please help keep this project alive! Donations are welcome and will go towards further development of this project.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PayPal: paypal.me/posquit0
BTC: 1Je3DxJVM2a9nTVPNo55SfQwpmxA6N2KKb
BCH: 1Mg1wG7PwHGrHYSWS67TsGSjo5GHEVbF16
ETH: 0x77ED9B4659F80205E9B9C9FB1E26EDB9904AFCC7
QTUM: QZT7D6m3QtTTqp7s4ZWAwLtGDsoHMMaM8E
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Thank you for your support!&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-résumé" class="anchor" aria-hidden="true" href="#résumé"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Résumé&lt;/h4&gt;
&lt;p&gt;You can see &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Page. 1&lt;/th&gt;
&lt;th align="center"&gt;Page. 2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-0.png" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-1.png" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-cover-letter" class="anchor" aria-hidden="true" href="#cover-letter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cover Letter&lt;/h4&gt;
&lt;p&gt;You can see &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Without Sections&lt;/th&gt;
&lt;th align="center"&gt;With Sections&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-0.png" alt="Cover Letter(Traditional)" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-1.png" alt="Cover Letter(Awesome)" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.overleaf.com/latex/templates/awesome-cv/tvmzpvdjfqxp" rel="nofollow"&gt;&lt;strong&gt;Edit Résumé on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.overleaf.com/latex/templates/awesome-cv-cover-letter/pfzzjspkthbk" rel="nofollow"&gt;&lt;strong&gt;Edit Cover Letter on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note:&lt;/em&gt; Above services do not guarantee up-to-date source code of Awesome CV&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Use&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h4&gt;
&lt;p&gt;A full TeX distribution is assumed.  &lt;a href="http://tex.stackexchange.com/q/55437" rel="nofollow"&gt;Various distributions for different operating systems (Windows, Mac, *nix) are available&lt;/a&gt; but TeX Live is recommended.
You can &lt;a href="http://tex.stackexchange.com/q/1092" rel="nofollow"&gt;install TeX from upstream&lt;/a&gt; (recommended; most up-to-date) or use &lt;code&gt;sudo apt-get install texlive-full&lt;/code&gt; if you really want that.  (It's generally a few years behind.)&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h4&gt;
&lt;p&gt;At a command prompt, run&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ xelatex {your-cv}.tex&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This should result in the creation of &lt;code&gt;{your-cv}.pdf&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-credit" class="anchor" aria-hidden="true" href="#credit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credit&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.latex-project.org" rel="nofollow"&gt;&lt;strong&gt;LaTeX&lt;/strong&gt;&lt;/a&gt; is a fantastic typesetting program that a lot of people use these days, especially the math and computer science people in academia.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/furl/latex-fontawesome"&gt;&lt;strong&gt;LaTeX FontAwesome&lt;/strong&gt;&lt;/a&gt; is bindings for FontAwesome icons to be used in XeLaTeX.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/google/roboto"&gt;&lt;strong&gt;Roboto&lt;/strong&gt;&lt;/a&gt; is the default font on Android and ChromeOS, and the recommended font for Google’s visual language, Material Design.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/adobe-fonts/source-sans-pro"&gt;&lt;strong&gt;Source Sans Pro&lt;/strong&gt;&lt;/a&gt; is a set of OpenType fonts that have been designed to work well in user interface (UI) environments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;You are free to take my &lt;code&gt;.tex&lt;/code&gt; file and modify it to create your own resume. Please don't use my resume for anything else without my permission, though!&lt;/p&gt;
&lt;p&gt;If you have any questions, feel free to join me at &lt;code&gt;#posquit0&lt;/code&gt; on Freenode and ask away. Click &lt;a href="https://kiwiirc.com/client/irc.freenode.net/posquit0" rel="nofollow"&gt;here&lt;/a&gt; to connect.&lt;/p&gt;
&lt;p&gt;Good luck!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-see-also" class="anchor" aria-hidden="true" href="#see-also"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;See Also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/posquit0/hugo-awesome-identity"&gt;Awesome Identity&lt;/a&gt; - A single-page Hugo theme to introduce yourself.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>posquit0</author><guid isPermaLink="false">https://github.com/posquit0/Awesome-CV</guid><pubDate>Mon, 20 Jan 2020 00:08:00 GMT</pubDate></item><item><title>AllenDowney/ThinkBayes #9 in TeX, Today</title><link>https://github.com/AllenDowney/ThinkBayes</link><description>&lt;p&gt;&lt;i&gt;Code repository for Think Bayes.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-thinkbayes" class="anchor" aria-hidden="true" href="#thinkbayes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ThinkBayes&lt;/h1&gt;
&lt;p&gt;Code repository for Think Bayes: Bayesian Statistics Made Simple
by Allen B. Downey&lt;/p&gt;
&lt;p&gt;Available from Green Tea Press at &lt;a href="http://thinkbayes.com" rel="nofollow"&gt;http://thinkbayes.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Published by O'Reilly Media, October 2013.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>AllenDowney</author><guid isPermaLink="false">https://github.com/AllenDowney/ThinkBayes</guid><pubDate>Mon, 20 Jan 2020 00:09:00 GMT</pubDate></item><item><title>mohuangrui/ucasthesis #10 in TeX, Today</title><link>https://github.com/mohuangrui/ucasthesis</link><description>&lt;p&gt;&lt;i&gt; [最新样式] 中国科学院大学学位论文 LaTeX 模板  LaTeX Thesis Template for the University of Chinese Academy of Sciences &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ucasthesis-国科大学位论文-latex-模板-最新样式" class="anchor" aria-hidden="true" href="#ucasthesis-国科大学位论文-latex-模板-最新样式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;ucasthesis&lt;/code&gt; 国科大学位论文 LaTeX 模板 [最新样式]&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-模板下载" class="anchor" aria-hidden="true" href="#模板下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模板下载&lt;/h2&gt;
&lt;p&gt;请在页面右边点击：&lt;strong&gt;Clone or download -&amp;gt; Download Zip&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-重要建议" class="anchor" aria-hidden="true" href="#重要建议"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;重要建议&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;关于 ucasthesis 编译和设计的问题，请先读 &lt;strong&gt;模板使用说明.pdf&lt;/strong&gt;，如发问需遵从&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"&gt;提问流程&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;使用邮件传播 ucasthesis 时，请先删除 &lt;code&gt;artratex.bat&lt;/code&gt; 以防范 Dos 脚本的潜在风险。&lt;/li&gt;
&lt;li&gt;开题报告请见：&lt;a href="https://github.com/mohuangrui/ucasproposal"&gt;ucasproposal: 中国科学院大学开题报告 LaTeX 模板&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;书脊制作请见：&lt;a href="https://github.com/mohuangrui/latexspine"&gt;latexspine: LaTeX 书脊模板&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-模板简介" class="anchor" aria-hidden="true" href="#模板简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模板简介&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ucasthesis 为撰写中国科学院大学&lt;strong&gt;本&lt;/strong&gt;、&lt;strong&gt;硕&lt;/strong&gt;、&lt;strong&gt;博&lt;/strong&gt;学位论文和&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-"&gt;&lt;strong&gt;博后&lt;/strong&gt;&lt;/a&gt;报告的 LaTeX 模版。ucasthesis 提供了简单明了的&lt;strong&gt;模板使用说明.pdf&lt;/strong&gt;。无论你是否具有 LaTeX 使用经验，都可较为轻松地使用以完成学位论文的撰写和排版。谢谢大家的测试、反馈和支持，我们一起的努力让 ucasthesis 非常荣幸地得到了国科大本科部陆晴老师、本科部学位办丁云云老师和中科院数学与系统科学研究院吴凌云研究员的支持，并得到吴凌云学长在 &lt;a href="http://www.ctex.org/HomePage" rel="nofollow"&gt;CTEX&lt;/a&gt; 的发布。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;考虑到许多同学可能缺乏 LaTeX 使用经验，ucasthesis 将 LaTeX 的复杂性高度封装，开放出简单的接口，以便轻易使用。同时，对用 LaTeX 撰写论文的一些主要难题，如制图、制表、文献索引等，进行了详细说明，并提供了相应的代码样本，理解了上述问题后，对于初学者而言，使用此模板撰写学位论文将不存在实质性的困难。所以，如果你是初学者，请不要直接放弃，因为同样为初学者的我，十分明白让 LaTeX 简单易用的重要性，而这正是 ucasthesis 所追求和体现的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;此中国科学院大学学位论文模板 ucasthesis 基于中科院数学与系统科学研究院吴凌云研究员的 CASthesis 模板发展而来。当前 ucasthesis 模板满足最新的中国科学院大学学位论文撰写要求和封面设定。兼顾操作系统：Windows，Linux，MacOS 和 LaTeX 编译引擎：pdflatex，xelatex，lualatex。支持中文书签、中文渲染、中文粗体显示、拷贝 PDF 中的文本到其他文本编辑器等特性（&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE"&gt;Windows 系统 PDF 拷贝乱码的解决方案需见：字体配置&lt;/a&gt;）。此外，对模板的文档结构进行了精心设计，撰写了编译脚本提高模板的易用性和使用效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ucasthesis 的目标在于简化学位论文的撰写，利用 LaTeX 格式与内容分离的特征，模板将格式设计好后，作者可只需关注论文内容。 同时，ucasthesis 有着整洁一致的代码结构和扼要的注解，对文档的仔细阅读可为初学者提供一个学习 LaTeX 的窗口。此外，模板的架构十分注重通用性，事实上，ucasthesis 不仅是国科大学位论文模板，同时，通过少量修改即可成为使用 LaTeX 撰写中英文文章或书籍的通用模板，并为使用者的个性化设定提供了接口。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-重要通知" class="anchor" aria-hidden="true" href="#重要通知"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;重要通知&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;2020-01-09&lt;/code&gt; 模板样式进行了修改，请查看下面的修改描述，以决定是否需要更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-更新记录" class="anchor" aria-hidden="true" href="#更新记录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新记录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2020-01-09&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/223"&gt;NineSH, issue #223&lt;/a&gt; 修复&lt;code&gt;bicaption&lt;/code&gt;错误。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-12-06&lt;/code&gt; 移除 commit 中的二进制文件，以极大减少 Fork 后的文件大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/198"&gt;huiwenzhang, issue #198&lt;/a&gt; 修复&lt;code&gt;mainmatter&lt;/code&gt;下&lt;code&gt;\chapter*&lt;/code&gt;的页眉错误。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/195"&gt;Fancy0609, muzimuzhi, issue #195&lt;/a&gt; 调整由&lt;code&gt;AutoFakeBold&lt;/code&gt;控制的伪粗体加粗程度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-11&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/190"&gt;Pantrick, issue #190&lt;/a&gt; 采用 &lt;a href="https://github.com/muzimuzhi"&gt;muzimuzhi&lt;/a&gt; 提供的方法实现&lt;code&gt;\advisor{}&lt;/code&gt;和&lt;code&gt;\institute{}&lt;/code&gt;的自动换行功能。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-08-01&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/183"&gt;vectorliu, issue #183&lt;/a&gt; 修改英文模式下的&lt;code&gt;plain&lt;/code&gt;选项为&lt;code&gt;scheme=plain&lt;/code&gt;以消除对&lt;code&gt;Algorithm&lt;/code&gt;样式的修改。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-06-15&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/177"&gt;HaorenWang, issue #177&lt;/a&gt; 调整矢量、矩阵、张量字体样式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-06-09&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/170"&gt;DRjy, issue #170&lt;/a&gt; 轻微缩减目录中编号与标题的间距；根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/174"&gt;e71828, issue #174&lt;/a&gt; 轻微增加页眉中编号与标题的间距。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-05-25&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/169"&gt;CDMA2019, issue #169&lt;/a&gt; 提供横排图表环境下页眉页脚的横排，具体使用见 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%A8%AA%E6%8E%92%E5%9B%BE%E8%A1%A8"&gt;横排图表&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-24&lt;/code&gt; 拓展模版兼容 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-"&gt;博后报告&lt;/a&gt;。修复 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/156"&gt;gsp2014, issue #156&lt;/a&gt; 文献引用中的连字符的间断显示和上标引用中逗号下沉。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-19&lt;/code&gt; 修复 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/117"&gt;nihaomiao, issue #117&lt;/a&gt;&lt;code&gt;\mathbf&lt;/code&gt;失效问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-16&lt;/code&gt; 修复国际生需要的&lt;code&gt;plain&lt;/code&gt;模式下无法改变英文章标题字体大小的问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-09&lt;/code&gt; 对部分宏命令进行调整，无功能及样式上的修改。若需更新，建议参考 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%9B%B4%E6%96%B0%E6%8C%87%E5%8D%97"&gt;更新指南&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-04&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/134"&gt;liuy334, songchunlin, issue #134&lt;/a&gt; ，调整行距使&lt;code&gt;LaTeX&lt;/code&gt;版与&lt;code&gt;Word&lt;/code&gt;版的行数和每行字数相一致。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-28&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/49"&gt;zssasa, allenwoods, issue #49&lt;/a&gt; ，修复&lt;code&gt;bicaption&lt;/code&gt;对&lt;code&gt;longtable&lt;/code&gt;的兼容性。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/133"&gt;BowenHou, issue #133&lt;/a&gt; ，使下划线能对长标题自动换行。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-25&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/127"&gt;DRjy, muzimuzhi, issue #127&lt;/a&gt; ，为&lt;code&gt;摘要&lt;/code&gt;等无需在目录中显示的结构元素建立书签。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/130"&gt;muzimuzhi, issue #130&lt;/a&gt; ，修正对&lt;code&gt;\voffset&lt;/code&gt;的使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-14&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/121"&gt;opt-gaobin, issue #121&lt;/a&gt; ，修正中文标点使下划线断掉的问题。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/120"&gt;Guoqiang Zhang, email; weili-ict, issue #120&lt;/a&gt; ，修复&lt;code&gt;\proofname&lt;/code&gt;命令对2015年及更早&lt;code&gt;LaTeX&lt;/code&gt;编译器的兼容性问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-02-20&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/100"&gt;opt-gaobin, issue #100&lt;/a&gt; ，增加定理、定义、证明等数学环境。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/102"&gt;DRjy, issue #102&lt;/a&gt; ，调整&lt;code&gt;\mathcal&lt;/code&gt;字体样式。根据 [zike Liu, email] ，适当缩减目录列表的缩进。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/105"&gt;xiaoyaoE, issue #105&lt;/a&gt; ，使数字字体和英文字体一致。完善中文版和国际版之间的中英格式切换。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-01-10&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/57"&gt;mnpengjk, issue #57&lt;/a&gt; ， 将公式编号前加点纳入模版默认，更多讨论可见：&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82"&gt;琐屑细节&lt;/a&gt; 。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/95"&gt;yunyun2019, issue #95&lt;/a&gt; ，采用 &lt;a href="https://github.com/zepinglee"&gt;zepinglee&lt;/a&gt; 基于国标样式为&lt;code&gt;ucas&lt;/code&gt;所定制文献样式：&lt;a href="https://github.com/CTeX-org/gbt7714-bibtex-style/tree/ucas"&gt;ucas 样式分支&lt;/a&gt; ，文献样式更多讨论可见：&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%96%87%E7%8C%AE%E6%A0%B7%E5%BC%8F"&gt;文献样式&lt;/a&gt;。根据 [邵岳林, email] ，将附录复原为常规的排版设置，若需将附录置于参考文献后，请见：&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82"&gt;琐屑细节&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-04-03&lt;/code&gt; 根据国科大本科部陆晴老师和本科部学位办丁云云老师的复审审核建议再次修复一些样式细节问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-04-02&lt;/code&gt; 模板进行了重大更新，修复了样式、字体、格式等许多问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据国科大本科部陆晴老师的建议对模版样式进行了诸多拓展和修正，并完善对本科生论文元素的兼容性。&lt;/li&gt;
&lt;li&gt;在 &lt;a href="https://github.com/CTeX-org/ctex-kit"&gt;ctex&lt;/a&gt; 开发者的帮助下解决了如何多次调用&lt;code&gt;Times New Roman&lt;/code&gt;而不导致黑体调用错误的问题。根据 [twn1993, email]，修复默认黑体为微软雅黑而不是&lt;code&gt;SimHei&lt;/code&gt;的问题。&lt;/li&gt;
&lt;li&gt;繁复折腾测试后终于找出一个在&lt;code&gt;ctex&lt;/code&gt;默认黑体替换粗宋体设定环境内全局&lt;code&gt;AutoFakeBold&lt;/code&gt;失效状态下折衷特定字体库不全条件下生僻字显示和系统默认字重不全条件下粗宋体显示以及不同操作系统下如何平衡上述字库自重矛盾还有根据操作系统自动调用所带有的&lt;code&gt;Times&lt;/code&gt;字体的方案。&lt;/li&gt;
&lt;li&gt;设定论文封面据英文学位名如自动切换。密级据是否填写自动显示。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-03-22&lt;/code&gt; 演示表标题居表上，加粗图表标注，设置长图表标题悬挂缩进（由于&lt;code&gt;bicaption&lt;/code&gt;宏包无法正确接受&lt;code&gt;caption&lt;/code&gt;宏包的&lt;code&gt;margin&lt;/code&gt;选项，图表中英标题第一行无法正确同步缩进，从而放弃第一行的缩进），强调多图中子图标题的规范使用，通过摘要和符号列表演示标题不在目录中显示却仍在页眉中显示。根据 [赵永明, email]，设置双语图表标题和&lt;code&gt;bicaption&lt;/code&gt;不在图形列表和表格列表中显示英文标题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-03-21&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/42"&gt;zhanglinbo, issue #42&lt;/a&gt; ，使用 &lt;a href="https://github.com/xiaoyao9933/UCASthesis"&gt;xiaoyao9933&lt;/a&gt; 制作的&lt;code&gt;ucas_logo.pdf&lt;/code&gt;使学校&lt;code&gt;logo&lt;/code&gt;放大不失真。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/41"&gt;Starsky Wong, issue #41&lt;/a&gt; ，设置标题英文设为&lt;code&gt;Times New Roman&lt;/code&gt;。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/29"&gt;will0n, issue #29&lt;/a&gt; ，&lt;a href="https://github.com/mohuangrui/ucasthesis/issues/26"&gt;Man-Ting-Fang, issue #26&lt;/a&gt; ，&lt;a href="https://github.com/mohuangrui/ucasthesis/issues/12"&gt;diyiliaoya, issue #12&lt;/a&gt; ，和 [赵永明, email] ，矫正一些格式细节问题。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/30"&gt;tangjie1992, issue #30&lt;/a&gt; ，配置算法环境。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-02-04&lt;/code&gt; 在 &lt;a href="https://github.com/CTeX-org/ctex-kit"&gt;ctex&lt;/a&gt; 开发者的帮助下修复误用字体命令导致的粗宋体异常。然后，将模板兼容性进一步扩展为兼容操作系统&lt;code&gt;Windows&lt;/code&gt;，&lt;code&gt;Linux&lt;/code&gt;，&lt;code&gt;MacOS&lt;/code&gt;和&lt;code&gt;LaTeX &lt;/code&gt;编译引擎&lt;code&gt;pdflatex&lt;/code&gt;，&lt;code&gt;xelatex&lt;/code&gt;，&lt;code&gt;lualatex&lt;/code&gt;。移除&lt;code&gt;microtype&lt;/code&gt;宏包以提高编译效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-01-28&lt;/code&gt; 基于国科大&lt;code&gt;2018&lt;/code&gt;新版论文规范进行了重大修改，采用新的封面、声明、页眉页脚样式。展示标题中使用数学公式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2017-05-14&lt;/code&gt; 根据 [赵永明, email] ，增加&lt;code&gt;\citepns{}&lt;/code&gt;和&lt;code&gt;\citetns{}&lt;/code&gt;命令提供上标引用下混合非上标引用的需求。根据 [臧光明, email] ，添加设定论文为&lt;code&gt;thesis&lt;/code&gt;或&lt;code&gt;dissertation&lt;/code&gt;的命令。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>mohuangrui</author><guid isPermaLink="false">https://github.com/mohuangrui/ucasthesis</guid><pubDate>Mon, 20 Jan 2020 00:10:00 GMT</pubDate></item><item><title>exacity/deeplearningbook-chinese #11 in TeX, Today</title><link>https://github.com/exacity/deeplearningbook-chinese</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Book Chinese Translation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-中文翻译" class="anchor" aria-hidden="true" href="#deep-learning-中文翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning 中文翻译&lt;/h1&gt;
&lt;p&gt;在众多网友的帮助和校对下，中文版终于出版了。尽管还有很多问题，但至少90%的内容是可读的，并且是准确的。
我们尽可能地保留了原书&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;Deep Learning&lt;/a&gt;中的意思并保留原书的语句。&lt;/p&gt;
&lt;p&gt;然而我们水平有限，我们无法消除众多读者的方差。我们仍需要大家的建议和帮助，一起减小翻译的偏差。&lt;/p&gt;
&lt;p&gt;大家所要做的就是阅读，然后汇总你的建议，提issue（最好不要一个一个地提）。如果你确定你的建议不需要商量，可以直接发起PR。&lt;/p&gt;
&lt;p&gt;对应的翻译者：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第1、4、7、10、14、20章及第12.4、12.5节由 @swordyork 负责&lt;/li&gt;
&lt;li&gt;第2、5、8、11、15、18章由 @liber145 负责&lt;/li&gt;
&lt;li&gt;第3、6、9章由 @KevinLee1110 负责&lt;/li&gt;
&lt;li&gt;第13、16、17、19章及第12.1至12.3节由 @futianfan 负责&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-面向的读者" class="anchor" aria-hidden="true" href="#面向的读者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;面向的读者&lt;/h2&gt;
&lt;p&gt;请直接下载&lt;a href="https://github.com/exacity/deeplearningbook-chinese/releases/download/v0.5-beta/dlbook_cn_v0.5-beta.pdf"&gt;PDF&lt;/a&gt;阅读。
不打算提供EPUB等格式，如有需要请自行修改。&lt;/p&gt;
&lt;p&gt;这一版准确性已经有所提高，读者可以以中文版为主、英文版为辅来阅读学习，但我们仍建议研究者阅读&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;原版&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-出版及开源原因" class="anchor" aria-hidden="true" href="#出版及开源原因"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;出版及开源原因&lt;/h2&gt;
&lt;p&gt;本书由人民邮电出版社出版，如果你觉得中文版PDF对你有所帮助，希望你能支持下纸质正版书籍。
如果你觉得中文版不行，希望你能多提建议。非常感谢各位！
纸质版也会进一步更新，需要大家更多的建议和意见，一起完善中文版。&lt;/p&gt;
&lt;p&gt;纸质版目前在人民邮电出版社的异步社区出售，见&lt;a href="http://www.epubit.com.cn/book/details/4278" rel="nofollow"&gt;地址&lt;/a&gt;。
价格不低，但看了样本之后，我们认为物有所值。
注意，我们不会通过媒体进行宣传，希望大家先看电子版内容，再判断是否购买纸质版。&lt;/p&gt;
&lt;p&gt;以下是开源的具体原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们不是文学工作者，不专职翻译。单靠我们，无法给出今天的翻译，众多网友都给我们提出了宝贵的建议，因此开源帮了很大的忙。出版社会给我们稿费（我们也不知道多少，可能2万左右），我们也不好意思自己用，商量之后觉得捐出是最合适的，以所有贡献过的网友的名义（我们把稿费捐给了杉树公益，用于4名贵州高中生三年的生活费，见&lt;a href="https://github.com/exacity/deeplearningbook-chinese/blob/master/donation.pdf"&gt;捐赠情况&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;PDF电子版对于技术类书籍来说是很重要的，随时需要查询，拿着纸质版到处走显然不合适。国外很多技术书籍都有对应的电子版（虽然不一定是正版），而国内的几乎没有。个人认为这是出版社或者作者认为国民素质还没有高到主动为知识付费的境界，所以不愿意"泄露"电子版。时代在进步，我们也需要改变。特别是翻译作品普遍质量不高的情况下，要敢为天下先。&lt;/li&gt;
&lt;li&gt;深度学习发展太快，日新月异，所以我们希望大家更早地学到相关的知识。我觉得原作者开放PDF电子版也有类似的考虑，也就是先阅读后付费。我们认为中国人口素质已经足够高，懂得为知识付费。当然这不是付给我们的，是付给出版社的，出版社再付给原作者。我们不希望中文版的销量因PDF电子版的存在而下滑。出版社只有值回了版权才能在以后引进更多的优秀书籍。我们这个开源翻译先例也不会成为一个反面案例，以后才会有更多的PDF电子版。&lt;/li&gt;
&lt;li&gt;开源也涉及版权问题，出于版权原因，我们不再更新此初版PDF文件，请大家以最终的纸质版为准。（但源码会一直更新）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-致谢" class="anchor" aria-hidden="true" href="#致谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;致谢&lt;/h2&gt;
&lt;p&gt;我们有3个类别的校对人员。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;负责人也就是对应的翻译者。&lt;/li&gt;
&lt;li&gt;简单阅读，对语句不通顺或难以理解的地方提出修改意见。&lt;/li&gt;
&lt;li&gt;中英对比，进行中英对应阅读，排除少翻错翻的情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有校对建议都保存在各章的&lt;code&gt;annotations.txt&lt;/code&gt;文件中。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;章节&lt;/th&gt;
&lt;th&gt;负责人&lt;/th&gt;
&lt;th&gt;简单阅读&lt;/th&gt;
&lt;th&gt;中英对比&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter1_introduction/" rel="nofollow"&gt;第一章 前言&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc, @SiriusXDJ, @corenel, @NeutronT&lt;/td&gt;
&lt;td&gt;@linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter2_linear_algebra/" rel="nofollow"&gt;第二章 线性代数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@SiriusXDJ, @angrymidiao&lt;/td&gt;
&lt;td&gt;@badpoem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter3_probability_and_information_theory/" rel="nofollow"&gt;第三章 概率与信息论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@SiriusXDJ&lt;/td&gt;
&lt;td&gt;@kkpoker, @Peiyan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter4_numerical_computation/" rel="nofollow"&gt;第四章 数值计算&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;@zhangyafeikimi&lt;/td&gt;
&lt;td&gt;@hengqujushi&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter5_machine_learning_basics/" rel="nofollow"&gt;第五章 机器学习基础&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@wheaio, @huangpingchun&lt;/td&gt;
&lt;td&gt;@fairmiracle, @linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter6_deep_feedforward_networks/" rel="nofollow"&gt;第六章 深度前馈网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;David_Chow, @linzhp, @sailordiary&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter7_regularization/" rel="nofollow"&gt;第七章 深度学习中的正则化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@NBZCC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/" rel="nofollow"&gt;第八章 深度模型中的优化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@happynoom, @codeVerySlow&lt;/td&gt;
&lt;td&gt;@huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter9_convolutional_networks/" rel="nofollow"&gt;第九章 卷积网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @corenel&lt;/td&gt;
&lt;td&gt;@zhiding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter10_sequence_modeling_rnn/" rel="nofollow"&gt;第十章 序列建模：循环和递归网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @yinruiqing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter11_practical_methodology/" rel="nofollow"&gt;第十一章 实践方法论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter12_applications/" rel="nofollow"&gt;第十二章 应用&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork, @futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@corenel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter13_linear_factor_models/" rel="nofollow"&gt;第十三章 线性因子模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;@cloudygoose&lt;/td&gt;
&lt;td&gt;@ZhiweiYang&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter14_autoencoders/" rel="nofollow"&gt;第十四章 自编码器&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@Seaball, @huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter15_representation_learning/" rel="nofollow"&gt;第十五章 表示学习&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@cnscottzheng&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter16_structured_probabilistic_modelling/" rel="nofollow"&gt;第十六章 深度学习中的结构化概率模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter17_monte_carlo_methods/" rel="nofollow"&gt;第十七章 蒙特卡罗方法&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter18_confronting_the_partition_function/" rel="nofollow"&gt;第十八章 面对配分函数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@tankeco&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter19_approximate_inference/" rel="nofollow"&gt;第十九章 近似推断&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary, @hengqujushi, huanghaojun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter20_deep_generative_models/" rel="nofollow"&gt;第二十章 深度生成模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;参考文献&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@pkuwwt&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我们会在纸质版正式出版的时候，在书中致谢，正式感谢各位作出贡献的同学！&lt;/p&gt;
&lt;p&gt;还有很多同学提出了不少建议，我们都列在此处。&lt;/p&gt;
&lt;p&gt;@tttwwy @tankeco @fairmiracle @GageGao @huangpingchun @MaHongP @acgtyrant @yanhuibin315 @Buttonwood @titicacafz
@weijy026a @RuiZhang1993 @zymiboxpay @xingkongliang @oisc @tielei @yuduowu @Qingmu @HC-2016 @xiaomingabc
@bengordai @Bojian @JoyFYan @minoriwww @khty2000 @gump88 @zdx3578 @PassStory @imwebson @wlbksy @roachsinai @Elvinczp
@endymecy name:YUE-DaJiong @9578577 @linzhp @cnscottzheng @germany-zhu  @zhangyafeikimi @showgood163 @gump88
@kangqf @NeutronT @badpoem @kkpoker @Seaball @wheaio @angrymidiao @ZhiweiYang @corenel @zhaoyu611 @SiriusXDJ @dfcv24 EmisXXY
FlyingFire vsooda @friskit-china @poerin @ninesunqian @JiaqiYao @Sofring @wenlei @wizyoung @imageslr @@indam @XuLYC
@zhouqingping @freedomRen @runPenguin @pkuwwt @wuqi @tjliupeng @neo0801 @jt827859032 @demolpc @fishInAPool
@xiaolangyuxin @jzj1993 @whatbeg LongXiaJun jzd&lt;/p&gt;
&lt;p&gt;如有遗漏，请务必通知我们，可以发邮件至&lt;code&gt;echo c3dvcmQueW9ya0BnbWFpbC5jb20K | base64 --decode&lt;/code&gt;。
这是我们必须要感谢的，所以不要不好意思。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;排版&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-注意" class="anchor" aria-hidden="true" href="#注意"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;注意&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;各种问题或者建议可以提issue，建议使用中文。&lt;/li&gt;
&lt;li&gt;由于版权问题，我们不能将图片和bib上传，请见谅。&lt;/li&gt;
&lt;li&gt;Due to copyright issues, we would not upload figures and the bib file.&lt;/li&gt;
&lt;li&gt;可用于学习研究目的，不得用于任何商业行为。谢谢！&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-markdown格式" class="anchor" aria-hidden="true" href="#markdown格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Markdown格式&lt;/h2&gt;
&lt;p&gt;这种格式确实比较重要，方便查阅，也方便索引。初步转换后，生成网页，具体见&lt;a href="https://exacity.github.io/deeplearningbook-chinese" rel="nofollow"&gt;deeplearningbook-chinese&lt;/a&gt;。
注意，这种转换没有把图放进去，也不会放图。目前使用单个&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;，基于latex文件转换，以后可能会更改但原则是不直接修改&lt;a href="docs/_posts"&gt;md文件&lt;/a&gt;。
需要的同学可以自行修改&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-html格式" class="anchor" aria-hidden="true" href="#html格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTML格式&lt;/h2&gt;
&lt;p&gt;读者可以使用&lt;a href="https://github.com/coolwanglu/pdf2htmlEX"&gt;pdf2htmlEX&lt;/a&gt;进行转换，直接将PDF转换为HTML。&lt;/p&gt;
&lt;p&gt;Updating.....&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>exacity</author><guid isPermaLink="false">https://github.com/exacity/deeplearningbook-chinese</guid><pubDate>Mon, 20 Jan 2020 00:11:00 GMT</pubDate></item><item><title>xueruini/thuthesis #12 in TeX, Today</title><link>https://github.com/xueruini/thuthesis</link><description>&lt;p&gt;&lt;i&gt;LaTeX Thesis Template for Tsinghua University&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://github.com/xueruini/thuthesis/actions"&gt;&lt;img src="https://github.com/xueruini/thuthesis/workflows/build/badge.svg" alt="Actions Status" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/thuthesis/Lobby" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f93c05a42da86d653b4d5ad075031b2f4a9c60a1/68747470733a2f2f6261646765732e6769747465722e696d2f7468757468657369732f4c6f6262792e737667" alt="Join the chat at https://gitter.im/thuthesis/Lobby" data-canonical-src="https://badges.gitter.im/thuthesis/Lobby.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/xueruini/thuthesis/releases"&gt;&lt;img src="https://camo.githubusercontent.com/1d3087711a6511f9a6cd0c32e39c0753bc7b9a82/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f7875657275696e692f7468757468657369732f746f74616c" alt="GitHub downloads" data-canonical-src="https://img.shields.io/github/downloads/xueruini/thuthesis/total" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/xueruini/thuthesis/commits/master"&gt;&lt;img src="https://camo.githubusercontent.com/e0f6144648ee34760e0c7d2c52679ba66a19f384/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d6974732d73696e63652f7875657275696e692f7468757468657369732f6c6174657374" alt="GitHub commits" data-canonical-src="https://img.shields.io/github/commits-since/xueruini/thuthesis/latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/xueruini/thuthesis/releases/latest"&gt;&lt;img src="https://camo.githubusercontent.com/2763596b27f215fdbea8b7d72448c4e7d820ad21/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f7875657275696e692f746875746865736973" alt="GitHub release" data-canonical-src="https://img.shields.io/github/v/release/xueruini/thuthesis" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9f69d9cb1cdc67fa9f89a7978705d969e5371d18/68747470733a2f2f696d672e736869656c64732e696f2f6374616e2f762f746875746865736973" alt="CTAN" data-canonical-src="https://img.shields.io/ctan/v/thuthesis" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-whats-thuthesis" class="anchor" aria-hidden="true" href="#whats-thuthesis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's ThuThesis?&lt;/h1&gt;
&lt;p&gt;ThuThesis is an abbreviation of &lt;b&gt;T&lt;/b&gt;sing&lt;b&gt;h&lt;/b&gt;ua &lt;b&gt;U&lt;/b&gt;niversity &lt;b&gt;Thesis&lt;/b&gt; LaTeX Template.&lt;/p&gt;
&lt;p&gt;This package establishes a simple and easy-to-use LaTeX template for Tsinghua dissertations, including general undergraduate research papers, masters theses, doctoral dissertations, and postdoctoral reports. An English translation of this README follows the Chinese below.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-thuthesis是什么" class="anchor" aria-hidden="true" href="#thuthesis是什么"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ThuThesis是什么？&lt;/h1&gt;
&lt;p&gt;ThuThesis为 &lt;b&gt;T&lt;/b&gt;sing&lt;b&gt;h&lt;/b&gt;ua &lt;b&gt;U&lt;/b&gt;niversity &lt;b&gt;Thesis&lt;/b&gt; LaTeX Template之缩写。&lt;/p&gt;
&lt;p&gt;此宏包旨在建立一个简单易用的清华大学学位论文LaTeX模板，包括本科综合论文训练、硕士论文、博士论文以及博士后出站报告。&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-文档" class="anchor" aria-hidden="true" href="#文档"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;文档&lt;/h1&gt;
&lt;p&gt;请&lt;a href="https://github.com/xueruini/thuthesis/releases"&gt;下载&lt;/a&gt;模板，里面包括具体使用说明以及示例文档：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模板使用说明 (thuthesis.pdf)&lt;/li&gt;
&lt;li&gt;示例文档 (main.pdf)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-下载" class="anchor" aria-hidden="true" href="#下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;发行版：&lt;a href="https://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;CTAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;开发版：&lt;a href="https://github.com/xueruini/thuthesis"&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-升级" class="anchor" aria-hidden="true" href="#升级"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;升级&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-自动更新" class="anchor" aria-hidden="true" href="#自动更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;自动更新&lt;/h2&gt;
&lt;p&gt;通过 TeX 发行版工具自动从 &lt;a href="https://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;CTAN&lt;/a&gt; 更新。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-手动更新" class="anchor" aria-hidden="true" href="#手动更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;手动更新&lt;/h2&gt;
&lt;p&gt;从 &lt;a href="https://github.com/xueruini/thuthesis"&gt;GitHub&lt;/a&gt; 下载放入论文目录，执行命令（Windows 用户在文件夹空白处按&lt;code&gt;Shift+鼠标右键&lt;/code&gt;，点击“在此处打开命令行窗口”）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xetex thuthesis.ins
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;即可得到 &lt;code&gt;thuthesis.cls&lt;/code&gt; 等模板文件。&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-提问" class="anchor" aria-hidden="true" href="#提问"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;提问&lt;/h1&gt;
&lt;p&gt;按推荐顺序排序：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先到 &lt;a href="https://github.com/xueruini/thuthesis/wiki/FAQ"&gt;FAQ&lt;/a&gt; 看看常见问题&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/xueruini/thuthesis/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-makefile的用法" class="anchor" aria-hidden="true" href="#makefile的用法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Makefile的用法&lt;/h1&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;make [{all&lt;span class="pl-k"&gt;|&lt;/span&gt;thesis&lt;span class="pl-k"&gt;|&lt;/span&gt;spine&lt;span class="pl-k"&gt;|&lt;/span&gt;doc&lt;span class="pl-k"&gt;|&lt;/span&gt;clean&lt;span class="pl-k"&gt;|&lt;/span&gt;cleanall&lt;span class="pl-k"&gt;|&lt;/span&gt;distclean}]&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-目标" class="anchor" aria-hidden="true" href="#目标"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目标&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;make thesis&lt;/code&gt;    生成论文 main.pdf；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make spine&lt;/code&gt;     生成书脊 spine.pdf；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make doc&lt;/code&gt;       生成模板使用说明书 thuthesis.pdf；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make all&lt;/code&gt;       生成论文和书籍，相当于 &lt;code&gt;make thesis &amp;amp;&amp;amp; make spine&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;     删除示例文件的中间文件（不含 main.pdf）；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make cleanall&lt;/code&gt;  删除示例文件的中间文件和 main.pdf；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt; 删除示例文件和模板的所有中间文件和 PDF。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h1&gt;
&lt;p&gt;Download and unzip the template. Specific usage documentation and examples can be found in the files below. At present, these documents are &lt;b&gt;only available in Chinese&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Template usage (thuthesis.pdf)&lt;/li&gt;
&lt;li&gt;Template example (main.pdf)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-downloads" class="anchor" aria-hidden="true" href="#downloads"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloads&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Published version: &lt;a href="https://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;CTAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Developer version: &lt;a href="https://github.com/xueruini/thuthesis"&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-automatic" class="anchor" aria-hidden="true" href="#automatic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Automatic&lt;/h2&gt;
&lt;p&gt;Get the most up-to-date published version of the TeX tools from &lt;a href="https://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;CTAN&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-manual" class="anchor" aria-hidden="true" href="#manual"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Manual&lt;/h2&gt;
&lt;p&gt;Download the package from &lt;a href="https://github.com/xueruini/thuthesis"&gt;GitHub&lt;/a&gt; to the root directory of your thesis, then execute the command (Windows users &lt;code&gt;Shift + right click&lt;/code&gt; white area in the file window and click "Open command line window here from the popup menu"):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xetex thuthesis.ins
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You'll get &lt;code&gt;thuthesis.cls&lt;/code&gt; along with other template files.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-reporting-issues" class="anchor" aria-hidden="true" href="#reporting-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reporting Issues&lt;/h1&gt;
&lt;p&gt;Please follow the procedure below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Check the &lt;a href="https://github.com/xueruini/thuthesis/wiki/FAQ"&gt;FAQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/xueruini/thuthesis/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-makefile-usage" class="anchor" aria-hidden="true" href="#makefile-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Makefile Usage&lt;/h1&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;make [{all&lt;span class="pl-k"&gt;|&lt;/span&gt;thesis&lt;span class="pl-k"&gt;|&lt;/span&gt;spine&lt;span class="pl-k"&gt;|&lt;/span&gt;doc&lt;span class="pl-k"&gt;|&lt;/span&gt;clean&lt;span class="pl-k"&gt;|&lt;/span&gt;cleanall&lt;span class="pl-k"&gt;|&lt;/span&gt;distclean}]&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-targets" class="anchor" aria-hidden="true" href="#targets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Targets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;make thesis&lt;/code&gt;    generate thesis main.pdf;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make spine&lt;/code&gt;     generate book spine for printing spine.pdf;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make doc&lt;/code&gt;       generate template documentation thuthesis.pdf;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make all&lt;/code&gt;       generate thesis and spine, same as &lt;code&gt;make thesis &amp;amp;&amp;amp; make spine&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;     delete all examples' files (excluding main.pdf);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make cleanall&lt;/code&gt;  delete all examples' files and main.pdf;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt; delete all examples' and templates' files and PDFs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>xueruini</author><guid isPermaLink="false">https://github.com/xueruini/thuthesis</guid><pubDate>Mon, 20 Jan 2020 00:12:00 GMT</pubDate></item></channel></rss>