<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: TeX, Today</title><link>https://github.com/trending/tex?since=daily</link><description>The top repositories on GitHub for tex, measured daily</description><pubDate>Fri, 15 Nov 2019 01:05:56 GMT</pubDate><lastBuildDate>Fri, 15 Nov 2019 01:05:56 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>deedy/Deedy-Resume #1 in TeX, Today</title><link>https://github.com/deedy/Deedy-Resume</link><description>&lt;p&gt;&lt;i&gt;A one page , two asymmetric column resume template in XeTeX that caters to an undergraduate Computer Science student&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deedy-resume" class="anchor" aria-hidden="true" href="#deedy-resume"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deedy-Resume&lt;/h1&gt;
&lt;p&gt;A &lt;strong&gt;one-page&lt;/strong&gt;, &lt;strong&gt;two asymmetric column&lt;/strong&gt; resume template in &lt;strong&gt;XeTeX&lt;/strong&gt; that caters particularly to an &lt;strong&gt;undergraduate Computer Science&lt;/strong&gt; student.
As of &lt;strong&gt;v1.2&lt;/strong&gt;, there is an option to choose from two templates:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;MacFonts&lt;/strong&gt; - uses fonts native to OSX - &lt;em&gt;Helvetica&lt;/em&gt;, &lt;em&gt;Helvetica Neue&lt;/em&gt; (and it's Light and Ultralight versions) and the CJK fonts &lt;em&gt;Heiti SC&lt;/em&gt;, and &lt;em&gt;Heiti TC&lt;/em&gt;. The EULA of these fonts prevents distribution on Open Source.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenFonts&lt;/strong&gt; - uses free, open-source fonts that resemble the above - &lt;em&gt;Lato&lt;/em&gt; (and its various variants) and &lt;em&gt;Raleway&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is licensed under the Apache License 2.0.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-motivation" class="anchor" aria-hidden="true" href="#motivation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Common LaTeX resume-builders such as &lt;a href="http://www.latextemplates.com/template/moderncv-cv-and-cover-letter" rel="nofollow"&gt;&lt;strong&gt;moderncv&lt;/strong&gt;&lt;/a&gt;  and the &lt;a href="https://github.com/afriggeri/cv"&gt;&lt;strong&gt;friggeri-cv&lt;/strong&gt;&lt;/a&gt; look great if you're looking for a multi-page resume with numerous citations, but usually imperfect for making a thorough, single-page one. A lot of companies today search resumes based on &lt;a href="http://www.businessinsider.com/most-big-companies-have-a-tracking-system-that-scans-your-resume-for-keywords-2012-1" rel="nofollow"&gt;keywords&lt;/a&gt; but at the same time require/prefer a one-page resume, especially for undergraduates.&lt;/p&gt;
&lt;p&gt;This template attempts to &lt;strong&gt;look clean&lt;/strong&gt;, highlight &lt;strong&gt;details&lt;/strong&gt;, be a &lt;strong&gt;single page&lt;/strong&gt;, and allow useful &lt;strong&gt;LaTeX templating&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-openfonts" class="anchor" aria-hidden="true" href="#openfonts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenFonts&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/OpenFonts/sample-image.png"&gt;&lt;img src="https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/OpenFonts/sample-image.png" alt="alt tag" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-macfonts" class="anchor" aria-hidden="true" href="#macfonts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MacFonts&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/MacFonts/sample-image.png"&gt;&lt;img src="https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/MacFonts/sample-image.png" alt="alt tag" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Compiles only with &lt;strong&gt;XeTeX&lt;/strong&gt; and required &lt;strong&gt;BibTex&lt;/strong&gt; for compiling publications and the .bib filetype.&lt;/li&gt;
&lt;li&gt;Uses fonts that are usually only available to &lt;strong&gt;Mac&lt;/strong&gt; users such as Helvetica Neue Light.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-availability" class="anchor" aria-hidden="true" href="#availability"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Availability&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;MacFonts version - &lt;a href="http://debarghyadas.com/resume/debarghya-das-resume.pdf" rel="nofollow"&gt;as an online preview&lt;/a&gt; and &lt;a href="https://github.com/deedydas/Deedy-Resume/raw/master/MacFonts/deedy_resume.pdf"&gt;as a direct download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenFonts version - &lt;a href="https://github.com/deedydas/Deedy-Resume/raw/master/OpenFonts/deedy_resume-openfont.pdf"&gt;as a direct download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overleaf&lt;/strong&gt;.com (formerly &lt;strong&gt;WriteLatex&lt;/strong&gt;.com) (v1 fonts/colors changed) - &lt;a href="https://www.writelatex.com/templates/deedy-resume/sqdbztjjghvz#.U2H9Kq1dV18" rel="nofollow"&gt;compilable online&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ShareLatex&lt;/strong&gt;.com (v1 fonts changes) - &lt;a href="https://www.sharelatex.com/templates/cv-or-resume/deedy-resume" rel="nofollow"&gt;compilable online&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-v12" class="anchor" aria-hidden="true" href="#v12"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;v1.2&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Added publications in place of societies.&lt;/li&gt;
&lt;li&gt;Collapsed a portion of education.&lt;/li&gt;
&lt;li&gt;Fixed a bug with alignment of overflowing long last updated dates on the top right.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-v11" class="anchor" aria-hidden="true" href="#v11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;v1.1&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Fixed several compilation bugs with \renewcommand&lt;/li&gt;
&lt;li&gt;Got Open-source fonts (Windows/Linux support)&lt;/li&gt;
&lt;li&gt;Added Last Updated&lt;/li&gt;
&lt;li&gt;Moved Title styling into .sty&lt;/li&gt;
&lt;li&gt;Commented .sty file.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Merge OpenFont and MacFonts as a single sty with options.&lt;/li&gt;
&lt;li&gt;Figure out a smoother way for the document to flow onto the next page.&lt;/li&gt;
&lt;li&gt;Add styling information for a "Projects/Hacks" section.&lt;/li&gt;
&lt;li&gt;Add location/address information&lt;/li&gt;
&lt;li&gt;Fix the hacky 'References' omission outside the .cls file in the MacFonts version.&lt;/li&gt;
&lt;li&gt;Add various styling and section options and allow for multiple pages smoothly.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-known-issues" class="anchor" aria-hidden="true" href="#known-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Known Issues:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Overflows onto second page if any column's contents are more than the vertical limit&lt;/li&gt;
&lt;li&gt;Hacky space on the first bullet point on the second column.&lt;/li&gt;
&lt;li&gt;Hacky redefinition of \refname to omit 'References' text for publications in the MacFonts version.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Copyright 2014 Debarghya Das

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>deedy</author><guid isPermaLink="false">https://github.com/deedy/Deedy-Resume</guid><pubDate>Fri, 15 Nov 2019 00:01:00 GMT</pubDate></item><item><title>posquit0/Awesome-CV #2 in TeX, Today</title><link>https://github.com/posquit0/Awesome-CV</link><description>&lt;p&gt;&lt;i&gt;:page_facing_up: Awesome CV is LaTeX template for your outstanding job application&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1 align="center"&gt;&lt;a id="user-content-------------awesome-cv" class="anchor" aria-hidden="true" href="#------------awesome-cv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
  &lt;a href="https://github.com/posquit0/Awesome-CV" title="AwesomeCV Documentation"&gt;
    &lt;img alt="AwesomeCV" src="https://github.com/posquit0/Awesome-CV/raw/master/icon.png" width="200px" height="200px" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;br&gt;
  Awesome CV
&lt;/h1&gt;
&lt;p align="center"&gt;
  LaTeX template for your outstanding job application
&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a href="https://www.paypal.me/posquit0" rel="nofollow"&gt;
    &lt;img alt="Donate" src="https://camo.githubusercontent.com/abbdd7bf97ae7919db5962b255f40aded5189c4f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d626c75652e737667" data-canonical-src="https://img.shields.io/badge/Donate-PayPal-blue.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://circleci.com/gh/posquit0/Awesome-CV" rel="nofollow"&gt;
    &lt;img alt="CircleCI" src="https://camo.githubusercontent.com/d42593802854990d35ca42943e478dd35d6c64c9/68747470733a2f2f636972636c6563692e636f6d2f67682f706f7371756974302f417765736f6d652d43562e7376673f7374796c653d736869656c64" data-canonical-src="https://circleci.com/gh/posquit0/Awesome-CV.svg?style=shield" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;
    &lt;img alt="Example Resume" src="https://camo.githubusercontent.com/836d3a9f44da3462e5c47b6c58bf066bffbaf739/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f726573756d652d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/resume-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/cv.pdf" rel="nofollow"&gt;
    &lt;img alt="Example CV" src="https://camo.githubusercontent.com/8afab53a91bc30d0da18a9ea0cc70f2d0a1571df/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f63762d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/cv-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;
    &lt;img alt="Example Coverletter" src="https://camo.githubusercontent.com/ce88ed0c1af9e5611df67818460447b69572ae9d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f7665726c65747465722d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/coverletter-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-awesome-cv" class="anchor" aria-hidden="true" href="#what-is-awesome-cv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is Awesome CV?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Awesome CV&lt;/strong&gt; is LaTeX template for a &lt;strong&gt;CV(Curriculum Vitae)&lt;/strong&gt;, &lt;strong&gt;Résumé&lt;/strong&gt; or &lt;strong&gt;Cover Letter&lt;/strong&gt; inspired by &lt;a href="https://www.sharelatex.com/templates/cv-or-resume/fancy-cv" rel="nofollow"&gt;Fancy CV&lt;/a&gt;. It is easy to customize your own template, especially since it is really written by a clean, semantic markup.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-donate" class="anchor" aria-hidden="true" href="#donate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Donate&lt;/h2&gt;
&lt;p&gt;Please help keep this project alive! Donations are welcome and will go towards further development of this project.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PayPal: paypal.me/posquit0
BTC: 1Je3DxJVM2a9nTVPNo55SfQwpmxA6N2KKb
BCH: 1Mg1wG7PwHGrHYSWS67TsGSjo5GHEVbF16
ETH: 0x77ED9B4659F80205E9B9C9FB1E26EDB9904AFCC7
QTUM: QZT7D6m3QtTTqp7s4ZWAwLtGDsoHMMaM8E
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Thank you for your support!&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-résumé" class="anchor" aria-hidden="true" href="#résumé"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Résumé&lt;/h4&gt;
&lt;p&gt;You can see &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Page. 1&lt;/th&gt;
&lt;th align="center"&gt;Page. 2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-0.png" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-1.png" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-cover-letter" class="anchor" aria-hidden="true" href="#cover-letter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cover Letter&lt;/h4&gt;
&lt;p&gt;You can see &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Without Sections&lt;/th&gt;
&lt;th align="center"&gt;With Sections&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-0.png" alt="Cover Letter(Traditional)" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-1.png" alt="Cover Letter(Awesome)" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.overleaf.com/latex/templates/awesome-cv/tvmzpvdjfqxp" rel="nofollow"&gt;&lt;strong&gt;Edit Résumé on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.overleaf.com/latex/templates/awesome-cv-cover-letter/pfzzjspkthbk" rel="nofollow"&gt;&lt;strong&gt;Edit Cover Letter on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note:&lt;/em&gt; Above services do not guarantee up-to-date source code of Awesome CV&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Use&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h4&gt;
&lt;p&gt;A full TeX distribution is assumed.  &lt;a href="http://tex.stackexchange.com/q/55437" rel="nofollow"&gt;Various distributions for different operating systems (Windows, Mac, *nix) are available&lt;/a&gt; but TeX Live is recommended.
You can &lt;a href="http://tex.stackexchange.com/q/1092" rel="nofollow"&gt;install TeX from upstream&lt;/a&gt; (recommended; most up-to-date) or use &lt;code&gt;sudo apt-get install texlive-full&lt;/code&gt; if you really want that.  (It's generally a few years behind.)&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h4&gt;
&lt;p&gt;At a command prompt, run&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ xelatex {your-cv}.tex&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This should result in the creation of &lt;code&gt;{your-cv}.pdf&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-credit" class="anchor" aria-hidden="true" href="#credit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credit&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.latex-project.org" rel="nofollow"&gt;&lt;strong&gt;LaTeX&lt;/strong&gt;&lt;/a&gt; is a fantastic typesetting program that a lot of people use these days, especially the math and computer science people in academia.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/furl/latex-fontawesome"&gt;&lt;strong&gt;LaTeX FontAwesome&lt;/strong&gt;&lt;/a&gt; is bindings for FontAwesome icons to be used in XeLaTeX.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/google/roboto"&gt;&lt;strong&gt;Roboto&lt;/strong&gt;&lt;/a&gt; is the default font on Android and ChromeOS, and the recommended font for Google’s visual language, Material Design.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/adobe-fonts/source-sans-pro"&gt;&lt;strong&gt;Source Sans Pro&lt;/strong&gt;&lt;/a&gt; is a set of OpenType fonts that have been designed to work well in user interface (UI) environments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;You are free to take my &lt;code&gt;.tex&lt;/code&gt; file and modify it to create your own resume. Please don't use my resume for anything else without my permission, though!&lt;/p&gt;
&lt;p&gt;If you have any questions, feel free to join me at &lt;code&gt;#posquit0&lt;/code&gt; on Freenode and ask away. Click &lt;a href="https://kiwiirc.com/client/irc.freenode.net/posquit0" rel="nofollow"&gt;here&lt;/a&gt; to connect.&lt;/p&gt;
&lt;p&gt;Good luck!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-see-also" class="anchor" aria-hidden="true" href="#see-also"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;See Also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/posquit0/hugo-awesome-identity"&gt;Awesome Identity&lt;/a&gt; - A single-page Hugo theme to introduce yourself.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>posquit0</author><guid isPermaLink="false">https://github.com/posquit0/Awesome-CV</guid><pubDate>Fri, 15 Nov 2019 00:02:00 GMT</pubDate></item><item><title>jikexueyuanwiki/tensorflow-zh #3 in TeX, Today</title><link>https://github.com/jikexueyuanwiki/tensorflow-zh</link><description>&lt;p&gt;&lt;i&gt;谷歌全新开源人工智能系统TensorFlow官方文档中文版&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-官方文档中文版" class="anchor" aria-hidden="true" href="#tensorflow-官方文档中文版"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 官方文档中文版&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="SOURCE/images/TensorFlow.jpg"&gt;&lt;img src="SOURCE/images/TensorFlow.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-你正在阅读的项目可能会比-android-系统更加深远地影响着世界" class="anchor" aria-hidden="true" href="#你正在阅读的项目可能会比-android-系统更加深远地影响着世界"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;你正在阅读的项目可能会比 Android 系统更加深远地影响着世界！&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-缘起" class="anchor" aria-hidden="true" href="#缘起"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;缘起&lt;/h2&gt;
&lt;p&gt;2015年11月9日，Google发布人工智能系统TensorFlow并宣布开源，同日，极客学院组织在线TensorFlow中文文档翻译。&lt;/p&gt;
&lt;p&gt;机器学习作为人工智能的一种类型，可以让软件根据大量的数据来对未来的情况进行阐述或预判。如今，领先的科技巨头无不在机器学习下予以极大投入。Facebook、苹果、微软，甚至国内的百度。Google 自然也在其中。「TensorFlow」是 Google 多年以来内部的机器学习系统。如今，Google 正在将此系统成为开源系统，并将此系统的参数公布给业界工程师、学者和拥有大量编程能力的技术人员，这意味着什么呢？&lt;/p&gt;
&lt;p&gt;打个不太恰当的比喻，如今 Google 对待 TensorFlow 系统，有点类似于该公司对待旗下移动操作系统 Android。如果更多的数据科学家开始使用 Google 的系统来从事机器学习方面的研究，那么这将有利于 Google 对日益发展的机器学习行业拥有更多的主导权。&lt;/p&gt;
&lt;p&gt;为了让国内的技术人员在最短的时间内迅速掌握这一世界领先的 AI 系统，极客学院 Wiki 团队发起对 TensorFlow 官方文档的中文协同翻译，一周之内，全部翻译认领完成，一个月后，全部30章节翻译校对完成，上线极客学院Wiki平台并提供下载。&lt;/p&gt;
&lt;p&gt;Google TensorFlow项目负责人Jeff Dean为该中文翻译项目回信称："&lt;em&gt;看到能够将TensorFlow翻译成中文我非常激动，我们将TensorFlow开源的主要原因之一是为了让全世界的人们能够从机器学习与人工智能中获益，类似这样的协作翻译能够让更多的人更容易地接触到TensorFlow项目，很期待接下来该项目在全球范围内的应用!&lt;/em&gt;"&lt;/p&gt;
&lt;p&gt;Jeff回信原文：&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="SOURCE/images/jeff.png"&gt;&lt;img src="SOURCE/images/jeff.png" alt="jeff" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;再次衷心感谢每一位为该翻译项目做出贡献的同学，我们会持续关注TensorFlow、AI领域以及其它最新技术的发展、持续维护该协作翻译、持续提供更多更优质的内容，为广大IT学习者们服务！&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-内容来源" class="anchor" aria-hidden="true" href="#内容来源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;内容来源&lt;/h2&gt;
&lt;p&gt;英文官方网站：&lt;br&gt;
&lt;a href="http://tensorflow.org/" rel="nofollow"&gt;http://tensorflow.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官方GitHub仓库：&lt;br&gt;
&lt;a href="https://github.com/tensorflow/tensorflow"&gt;https://github.com/tensorflow/tensorflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;中文版 GitHub 仓库：&lt;br&gt;
&lt;a href="https://github.com/jikexueyuanwiki/tensorflow-zh"&gt;https://github.com/jikexueyuanwiki/tensorflow-zh&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-参与者按认领章节排序" class="anchor" aria-hidden="true" href="#参与者按认领章节排序"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;参与者（按认领章节排序）&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-翻译" class="anchor" aria-hidden="true" href="#翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;翻译&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/PFZheng"&gt;@PFZheng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/linbojin"&gt;@Tony Jin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chenweican"&gt;@chenweican&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bingjin"&gt;@bingjin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/oskycar"&gt;@oskycar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/btpeter"&gt;@btpeter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Warln"&gt;@Warln&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ericxk"&gt;@ericxk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangaicc"&gt;@wangaicc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/TerenceCooper"&gt;@Terence Cooper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhyhooo"&gt;@zhyhooo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/thylaco1eo"&gt;@thylaco1eo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/volvet"&gt;@volvet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhangkom"&gt;@zhangkom&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/derekshang"&gt;@derekshang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lianghyv"&gt;@lianghyv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nb312"&gt;@nb312&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Jim-Zenn"&gt;@Jim-Zenn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/andyiac"&gt;@andyiac&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/TerenceCooper"&gt;@Terence Cooper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/leege100"&gt;@leege100&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-校对" class="anchor" aria-hidden="true" href="#校对"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;校对&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/sstruct"&gt;@yangtze&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ericxk"&gt;@ericxk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WangHong-yang"&gt;@HongyangWang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/LichAmnesia"&gt;@LichAmnesia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhyhooo"&gt;@zhyhooo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/waiwaizheng"&gt;@waiwaizheng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WangHong-yang"&gt;@HongyangWang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorfly"&gt;@tensorfly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lonlonago"&gt;@lonlonago&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jishaoming"&gt;@jishaoming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lucky521"&gt;@lucky521&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/allensummer"&gt;@allensummer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/volvet"&gt;@volvet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ZHNathanielLee"&gt;@ZHNathanielLee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PengFoo"&gt;@pengfoo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/qiaohaijun"&gt;@qiaohaijun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SeikaScarlet"&gt;@Seika&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-进度记录" class="anchor" aria-hidden="true" href="#进度记录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;进度记录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2015-11-10, 谷歌发布全新人工智能系统TensorFlow并宣布开源, 极客学院Wiki启动协同翻译，创建 GitHub 仓库，制定协同规范&lt;/li&gt;
&lt;li&gt;2015-11-18, 所有章节认领完毕，翻译完成18章，校对认领7章，Star数361，fork数100，协同翻译QQ群及技术交流群的TF爱好者将近300人，GitHub搜索TensorFlow排名第二&lt;/li&gt;
&lt;li&gt;2015-12-10, Star数超过500&lt;/li&gt;
&lt;li&gt;2015-12-15, 项目正式上线&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-花絮" class="anchor" aria-hidden="true" href="#花絮"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;花絮&lt;/h2&gt;
&lt;p&gt;在组织翻译的过程中，有些事情令人印象深刻，记录下来，希望以后来学习文档的同学能够明了到手中这份文档的由来：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参加翻译的有学生，也有老师；有专门研究AI/ML的，也有对此感兴趣的；有国内的，也有远在纽约的；有工程技术人员也有博士、专家&lt;/li&gt;
&lt;li&gt;其中一位，&lt;a href="http://www.longmotto.com" rel="nofollow"&gt;恩泽&lt;/a&gt;同学，为了翻译一篇文档，在前一天没有睡觉的情况下坚持翻完，20个小时没有合眼&lt;/li&gt;
&lt;li&gt;还有一位老师，刚从讲台上讲完课，就立即给我们的翻译提修改意见&lt;/li&gt;
&lt;li&gt;很多同学自发的将搭建环境中遇到的问题总结到FAQ里帮助他人&lt;/li&gt;
&lt;li&gt;为了一个翻译细节，经常是来回几次，和其他人讨论完善&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-持续改进" class="anchor" aria-hidden="true" href="#持续改进"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;持续改进&lt;/h2&gt;
&lt;p&gt;这样的一个高技术领域的文档，我们在翻译的过程中，难免会有不完善的地方，希望请大家一起帮助我们持续改进文档的翻译质量，帮助更多的人，方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在GitHub上提Issue或Pull Request，地址为: &lt;a href="https://github.com/jikexueyuanwiki/tensorflow-zh"&gt;https://github.com/jikexueyuanwiki/tensorflow-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;加入TensorFlow技术交流群，与TensorFlower们一起研究交流技术干货--TensorFlow技术交流群：782484288&lt;/li&gt;
&lt;li&gt;对翻译感兴趣？加入协同翻译群：248320884，与翻译大神一道研究TensorFlow的本地化&lt;/li&gt;
&lt;li&gt;给我们写邮件： &lt;a href="mailto:wiki@jikexueyuan.com"&gt;wiki@jikexueyuan.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-感谢支持" class="anchor" aria-hidden="true" href="#感谢支持"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;感谢支持&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://wiki.jikexueyuan.com" rel="nofollow"&gt;极客学院 Wiki&lt;/a&gt; 提供图文教程托管服务&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-离线版本" class="anchor" aria-hidden="true" href="#离线版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;离线版本&lt;/h2&gt;
&lt;p&gt;目前，离线版本(PDF、ePub)可正常下载、使用&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tex-pdf-修订版" class="anchor" aria-hidden="true" href="#tex-pdf-修订版"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tex-PDF 修订版&lt;/h2&gt;
&lt;p&gt;&lt;a href="tex_pdf"&gt;Tex-PDF 修订版&lt;/a&gt; 目前正在编订中，欢迎加入进来一起修订。您可以在此查看&lt;a href="tex_pdf/tensorflow_manual_cn.pdf"&gt;预览版&lt;/a&gt;目前最新状态。&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jikexueyuanwiki</author><guid isPermaLink="false">https://github.com/jikexueyuanwiki/tensorflow-zh</guid><pubDate>Fri, 15 Nov 2019 00:03:00 GMT</pubDate></item><item><title>thunlp/NRLPapers #4 in TeX, Today</title><link>https://github.com/thunlp/NRLPapers</link><description>&lt;p&gt;&lt;i&gt;Must-read papers on network representation learning (NRL) / network embedding (NE)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-must-read-papers-on-nrlne" class="anchor" aria-hidden="true" href="#must-read-papers-on-nrlne"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Must-read papers on NRL/NE.&lt;/h2&gt;
&lt;p&gt;NRL: network representation learning. NE: network embedding.&lt;/p&gt;
&lt;p&gt;Contributed by &lt;a href="http://thunlp.org/~tcc/" rel="nofollow"&gt;Cunchao Tu&lt;/a&gt;, Yuan Yao, Zhengyan Zhang, GanquCui, Hao Wang (BUPT), Changxin Tian (BUPT), Jie Zhou and Cheng Yang (BUPT).&lt;/p&gt;
&lt;p&gt;We release &lt;a href="https://github.com/thunlp/openne"&gt;OpenNE&lt;/a&gt;, an open source toolkit for NE/NRL. This repository provides a standard NE/NRL(Network Representation Learning）training and testing framework. Currently, the implemented models in OpenNE include DeepWalk, LINE, node2vec, GraRep, TADW and GCN.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-content" class="anchor" aria-hidden="true" href="#content"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Content&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#survey-papers"&gt;Survey Papers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#models"&gt;Models&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#bacis-models"&gt;Bacis Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#attributed-network"&gt;Attributed Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dynamic-network"&gt;Dynamic Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#heterogeneous-information-network"&gt;Heterogeneous Information Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bipartite-network"&gt;Bipartite Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#directed-network"&gt;Directed Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-models"&gt;Other Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#applications"&gt;Applications&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#natural-language-processing"&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#knowledge-graph"&gt;Knowledge Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#social-network"&gt;Social Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#graph-clustering"&gt;Graph Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#community-detection"&gt;Community Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#recommendation"&gt;Recommendation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-applications"&gt;Other Applications&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-survey-papers" class="anchor" aria-hidden="true" href="#survey-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Survey Papers&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Representation Learning on Graphs: Methods and Applications.&lt;/strong&gt;
&lt;em&gt;William L. Hamilton, Rex Ying, Jure Leskovec.&lt;/em&gt; IEEE Data(base) Engineering Bulletin 2017. &lt;a href="https://arxiv.org/pdf/1709.05584.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Embedding Techniques, Applications, and Performance: A Survey.&lt;/strong&gt;
&lt;em&gt;Palash Goyal, Emilio Ferrara.&lt;/em&gt; Knowledge Based Systems 2017. &lt;a href="https://arxiv.org/pdf/1705.02801.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications.&lt;/strong&gt;
&lt;em&gt;Hongyun Cai, Vincent W. Zheng, Kevin Chen-Chuan Chang.&lt;/em&gt; TKDE 2017. &lt;a href="https://arxiv.org/pdf/1709.07604.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Representation Learning: A Survey.&lt;/strong&gt;
&lt;em&gt;Daokun Zhang, Jie Yin, Xingquan Zhu, Chengqi Zhang.&lt;/em&gt; IEEE Transactions on Big Data 2018. &lt;a href="https://arxiv.org/pdf/1801.05852.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Tutorial on Network Embeddings.&lt;/strong&gt;
&lt;em&gt;Haochen Chen, Bryan Perozzi, Rami Al-Rfou, Steven Skiena.&lt;/em&gt; arxiv 2018. &lt;a href="https://arxiv.org/pdf/1808.02590.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Representation Learning: An Overview.(In Chinese)&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Cheng Yang, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; 2017. &lt;a href="http://engine.scichina.com/publisher/scp/journal/SSI/47/8/10.1360/N112017-00145" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational inductive biases, deep learning, and graph networks.&lt;/strong&gt;
&lt;em&gt;Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu.&lt;/em&gt; arxiv 2018. &lt;a href="https://arxiv.org/pdf/1806.01261.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-models" class="anchor" aria-hidden="true" href="#models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Models&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-bacis-models" class="anchor" aria-hidden="true" href="#bacis-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Bacis Models&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SepNE: Bringing Separability to Network Embedding.&lt;/strong&gt;
&lt;em&gt;Ziyao Li, Liang Zhang, Guojie Song.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4333" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robust Negative Sampling for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Mohammadreza Armandpour, Patrick Ding, Jianhua Huang, Xia Hu.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.stat.tamu.edu/~armand/R-NS.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Structure and Transfer Behaviors Embedding via Deep Prediction Model.&lt;/strong&gt;
&lt;em&gt;Xin Sun, Zenghui Song, Junyu Dong, Yongbo Yu, Claudia Plant, Christian Böhm.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4436" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Simplifying Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Weinberger.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/wu19e/wu19e.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GMNN: Graph Markov Neural Networks.&lt;/strong&gt;
&lt;em&gt;Meng Qu, Yoshua Bengio, Jian Tang.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/qu19a/qu19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stochastic Blockmodels meet Graph Neural Networks.&lt;/strong&gt;
&lt;em&gt;Nikhil Mehta, Lawrence Carin Duke, Piyush Rai.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/mehta19a/mehta19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Disentangled Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, Wenwu Zhu.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/ma19a/ma19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Position-aware Graph Neural Networks.&lt;/strong&gt;
&lt;em&gt;Jiaxuan You, Rex Ying, Jure Leskovec.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/you19b/you19b.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing.&lt;/strong&gt;
&lt;em&gt;Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Harutyunyan, Greg Ver Steeg, Aram Galstyan.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/abu-el-haija19a/abu-el-haija19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph U-Nets.&lt;/strong&gt;
&lt;em&gt;Hongyang Gao, Shuiwang Ji.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/gao19a/gao19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Attention Graph Pooling.&lt;/strong&gt;
&lt;em&gt;Junhyun Lee, Inyeop Lee, Jaewoo Kang.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/lee19c/lee19c.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking.&lt;/strong&gt;
&lt;em&gt;Aleksandar Bojchevski, Stephan Günnemann.&lt;/em&gt; ICLR 2018. &lt;a href="https://arxiv.org/pdf/1707.03815.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling.&lt;/strong&gt;
&lt;em&gt;Jie Chen, Tengfei Ma, Cao Xiao.&lt;/em&gt; ICLR 2018. &lt;a href="https://arxiv.org/pdf/1801.10247.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Attention Networks.&lt;/strong&gt;
&lt;em&gt;Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio.&lt;/em&gt; ICLR 2018. &lt;a href="https://arxiv.org/pdf/1710.10903.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stochastic Training of Graph Convolutional Networks with Variance Reduction.&lt;/strong&gt;
&lt;em&gt;Jianfei Chen, Jun Zhu, Le Song.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1710.10568.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarially Regularized Graph Autoencoder for Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, Chengqi Zhang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0362.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discrete Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiaobo Shen, Shirui Pan, Weiwei Liu, Yew-Soon Ong, Quan-Sen Sun.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0493.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Hashing for Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Qixiang Wang, Shanfeng Wang, Maoguo Gong, Yue Wu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0390.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Inductive Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Ryan A. Rossi, Rong Zhou, Nesreen K. Ahmed.&lt;/em&gt; WWW 2018. &lt;a href="http://ryanrossi.com/pubs/rossi-et-al-WWW18-BigNet.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Active Discriminative Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Li Gao, Hong Yang, Chuan Zhou, Jia Wu, Shirui Pan, Yue Hu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0296.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MILE: A Multi-Level Framework for Scalable Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Jiongqian Liang, Saket Gurukar, Srinivasan Parthasarathy.&lt;/em&gt; arxiv 2018. &lt;a href="https://arxiv.org/pdf/1802.09612.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Out-of-sample extension of graph adjacency spectral embedding.&lt;/strong&gt;
&lt;em&gt;Keith Levin, Farbod Roosta-Khorasani, Michael W. Mahoney, Carey E. Priebe.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1802.06307.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DeepWalk: Online Learning of Social Representations.&lt;/strong&gt;
&lt;em&gt;Bryan Perozzi, Rami Al-Rfou, Steven Skiena.&lt;/em&gt; KDD 2014. &lt;a href="https://arxiv.org/pdf/1403.6652" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/phanein/deepwalk"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non-transitive Hashing with Latent Similarity Componets.&lt;/strong&gt;
&lt;em&gt;Mingdong Ou, Peng Cui, Fei Wang, Jun Wang, Wenwu Zhu.&lt;/em&gt; KDD 2015. &lt;a href="http://cuip.thumedialab.com/papers/KDD-NonTransitiveHashing.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GraRep: Learning Graph Representations with Global Structural Information.&lt;/strong&gt;
&lt;em&gt;Shaosheng Cao, Wei Lu, Qiongkai Xu.&lt;/em&gt; CIKM 2015. &lt;a href="https://www.researchgate.net/profile/Qiongkai_Xu/publication/301417811_GraRep/links/5847ecdb08ae8e63e633b5f2/GraRep.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/ShelsonCao/GraRep"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LINE: Large-scale Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Me.&lt;/em&gt; WWW 2015. &lt;a href="https://arxiv.org/pdf/1503.03578.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/tangjianpku/LINE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Neural Networks for Learning Graph Representations.&lt;/strong&gt;
&lt;em&gt;Shaosheng Cao, Wei Lu, Xiongkai Xu.&lt;/em&gt; AAAI 2016. &lt;a href="https://pdfs.semanticscholar.org/1a37/f07606d60df365d74752857e8ce909f700b3.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/ShelsonCao/DNGR"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Revisiting Semi-supervised Learning with Graph Embeddings.&lt;/strong&gt;
&lt;em&gt;Zhilin Yang, William W. Cohen, Ruslan Salakhutdinov.&lt;/em&gt; ICML 2016. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/yanga16.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Max-Margin DeepWalk: Discriminative Learning of Network Representation.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; IJCAI 2016. &lt;a href="http://thunlp.org/~tcc/publications/ijcai2016_mmdw.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/mmdw"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discriminative Deep RandomWalk for Network Classification.&lt;/strong&gt;
&lt;em&gt;Juzheng Li, Jun Zhu, Bo Zhang.&lt;/em&gt; ACL 2016. &lt;a href="http://www.aclweb.org/anthology/P16-1095" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structural Deep Network Embedding.&lt;/strong&gt;
&lt;em&gt;Daixin Wang, Peng Cui, Wenwu Zhu.&lt;/em&gt; KDD 2016. &lt;a href="http://cuip.thumedialab.com/papers/SDNE.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structural Neighborhood Based Classification of Nodes in a Network.&lt;/strong&gt;
&lt;em&gt;Sharad Nandanwar, M. N. Murty.&lt;/em&gt; KDD 2016. &lt;a href="http://www.kdd.org/kdd2016/papers/files/Paper_679.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Community Preserving Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, Shiqiang Yang.&lt;/em&gt; AAAI 2017. &lt;a href="http://cuip.thumedialab.com/papers/NE-Community.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised Classification with Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Thomas N. Kipf, Max Welling.&lt;/em&gt; ICLR 2017. &lt;a href="https://arxiv.org/pdf/1609.02907.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/tkipf/gcn"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fast Network Embedding Enhancement via High Order Proximity Approximation.&lt;/strong&gt;
&lt;em&gt;Cheng Yang, Maosong Sun, Zhiyuan Liu, Cunchao Tu.&lt;/em&gt; IJCAI 2017. &lt;a href="http://thunlp.org/~tcc/publications/ijcai2017_neu.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/neu"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CANE: Context-Aware Network Embedding for Relation Modeling.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Han Liu, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; ACL 2017. &lt;a href="http://thunlp.org/~tcc/publications/acl2017_cane.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/cane"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A General View for Network Embedding as Matrix Factorization.&lt;/strong&gt;
&lt;em&gt;Xin Liu, Tsuyoshi Murata, Kyoung-Sook Kim, Chatchawan Kotarasu, Chenyi Zhuang.&lt;/em&gt; WSDM 2019. &lt;a href="https://dl.acm.org/citation.cfm?doid=3289600.3291029" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Co-Embedding Attributed Networks.&lt;/strong&gt;
&lt;em&gt;Zaiqiao Meng, Shangsong Liang, Xiangliang Zhang, Hongyan Bao.&lt;/em&gt; WSDM 2019. &lt;a href="https://mine.kaust.edu.sa/Documents/papers/WSDM19attribute.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enhanced Network Embeddings via Exploiting Edge Labels.&lt;/strong&gt;
&lt;em&gt;Haochen Chen, Xiaofei Sun, Yingtao Tian, Bryan Perozzi, Muhao Chen, Steven Skiena.&lt;/em&gt; CIKM 2018. &lt;a href="https://arxiv.org/pdf/1809.05124.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Improve Network Embeddings with Regularization.&lt;/strong&gt;
&lt;em&gt;Yi Zhang, Jianguo Lu, Ofer Shai.&lt;/em&gt; CIKM 2018. &lt;a href="https://jlu.myweb.cs.uwindsor.ca/n2v.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Modeling Multi-way Relations with Hypergraph Embedding.&lt;/strong&gt;
&lt;em&gt;Chia-An Yu, Ching-Lun Tai, Tak-Shing Chan, Yi-Hsuan Yang.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3269274" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/chia-an/HGE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;REGAL: Representation Learning-based Graph Alignment.&lt;/strong&gt;
&lt;em&gt;Mark Heimann, Haoming Shen, Tara Safavi, Danai Koutra.&lt;/em&gt; CIKM 2018. &lt;a href="https://arxiv.org/pdf/1802.06257.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Network Embedding.&lt;/strong&gt;
&lt;em&gt;Quanyu Dai, Qiang Li, Jian Tang, Dan Wang.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.07838.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/sachinbiradar9/Adverserial-Inductive-Deep-Walk"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bernoulli Embeddings for Graphs.&lt;/strong&gt;
&lt;em&gt;Vinith Misra, Sumit Bhatia.&lt;/em&gt; AAAI 2018. &lt;a href="http://sumitbhatia.net/papers/aaai18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GraphGAN: Graph Representation Learning with Generative Adversarial Nets.&lt;/strong&gt;
&lt;em&gt;Hongwei Wang, jia Wang, jialin Wang, MIAO ZHAO, Weinan Zhang, Fuzheng Zhang, Xie Xing, Minyi Guo.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.08267.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HARP: Hierarchical Representation Learning for Networks.&lt;/strong&gt;
&lt;em&gt;Haochen Chen, Bryan Perozzi, Yifan Hu, Steven Skiena.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1706.07845.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/GTmac/HARP"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Social Rank Regulated Large-scale Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yupeng Gu, Yizhou Sun, Yanen Li, Yang Yang.&lt;/em&gt; WWW 2018. &lt;a href="http://yangy.org/works/ge/rare.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Latent Network Summarization: Bridging Network Embedding and Summarization.&lt;/strong&gt;
&lt;em&gt;Di Jin,Ryan Rossi,Danai Koutra,Eunyee Koh,Sungchul Kim,Anup Rao&lt;/em&gt; KDD 2019. &lt;a href="https://arxiv.org/pdf/1811.04461.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching.&lt;/strong&gt;
&lt;em&gt;Dingqi Yang,Paolo Rosso,Bin Li,Philippe Cudre-Mauroux.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330951/p1162-yang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ProGAN: Network Embedding via Proximity Generative Adversarial Network.&lt;/strong&gt;
&lt;em&gt;Hongchang Gao,Jian Pei,Heng Huang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330866/p1308-gao.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Global Alignment Graph Kernel Using Random Features: From Node Embedding to Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Lingfei Wu,Ian En-Hsu Yen,Zhen Zhang,Kun Xu,Liang Zhao,Xi Peng,Yinglong Xia,Charu Aggarwal.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330918/p1418-wu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Graph Embeddings via Sparse Transpose Proximities.&lt;/strong&gt;
&lt;em&gt;Yuan Yin,Zhewei Wei.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330860/p1429-yin.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AutoNRL: Hyperparameter Optimization for Massive Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Ke Tu,Jianxin Ma,Peng Cui,Jian Pei,Wenwu Zhu.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330848/p216-tu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Representation Learning via Hard and Channel-Wise Attention Networks.&lt;/strong&gt;
&lt;em&gt;Hongyang Gao,Shuiwang Ji.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330897/p741-gao.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Training Methods for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Quanyu Dai,Xiao Shen,Liang Zhang,Qiang Li,Dan Wang.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1908.11514.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-relational Network Embeddings with Relational Proximity and Node Attributes.&lt;/strong&gt;
&lt;em&gt;Ming-Han Feng,Chin-Chi Hsu,Cheng-Te Li,Mi-Yen Yeh,Shou-De Lin.&lt;/em&gt; WWW 2019. &lt;a href="https://pdfs.semanticscholar.org/6274/3cbebc142897c6c005f3c12c00b9202ca43f.pdf?_ga=2.108748866.1527570260.1569422306-1231101604.1568798295" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sampled in Pairs and Driven by Text: A New Graph Embedding Framework.&lt;/strong&gt;
&lt;em&gt;Liheng Chen,Yanru Qu,Zhenghui Wang,Weinan Zhang,Ken Chen,Shaodian Zhang,Yong Yu.&lt;/em&gt; WWW 2019. &lt;a href="https://sci-hub.tw/10.1145/3308558.3313520#" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DDGK: Learning Graph Representations via Deep Divergence Graph Kernels.&lt;/strong&gt;
&lt;em&gt;Rami Al-Rfou,Dustin Zelle,Bryan Perozzi.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1904.09671.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tag2Vec: Learning Tag Representations in Tag Networks.&lt;/strong&gt;
&lt;em&gt;Junshan Wang,Zhicong Lu,Guojia Song,Yue Fan,Lun Du,Wei Lin.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1905.03041.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;struc2vec: Learning Node Representations from Structural Identity.&lt;/strong&gt;
&lt;em&gt;Leonardo F. R. Ribeiro, Pedro H. P. Saverese, Daniel R. Figueiredo.&lt;/em&gt; KDD 2017. &lt;a href="https://arxiv.org/pdf/1704.03165.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inductive Representation Learning on Large Graphs.&lt;/strong&gt;
&lt;em&gt;William L. Hamilton, Rex Ying, Jure Leskovec.&lt;/em&gt; NIPS 2017. &lt;a href="https://arxiv.org/pdf/1706.02216.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Graph Embeddings with Embedding Propagation.&lt;/strong&gt;
&lt;em&gt;Alberto Garcia Duran, Mathias Niepert.&lt;/em&gt; NIPS 2017. &lt;a href="https://arxiv.org/pdf/1710.03059.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enhancing the Network Embedding Quality with Structural Similarity.&lt;/strong&gt;
&lt;em&gt;Tianshu Lyu, Yuan Zhang, Yan Zhang.&lt;/em&gt; CIKM 2017. &lt;a href="https://pdfs.semanticscholar.org/e54a/374d7e24260450e2081b93005a491d1b9116.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;An Attention-based Collaboration Framework for Multi-View Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Meng Qu, Jian Tang, Jingbo Shang, Xiang Ren, Ming Zhang, Jiawei Han.&lt;/em&gt; CIKM 2017. &lt;a href="https://arxiv.org/pdf/1709.06636.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;On Embedding Uncertain Graphs.&lt;/strong&gt;
&lt;em&gt;Jiafeng Hu, Reynold Cheng, Zhipeng Huang, Yixang Fang, Siqiang Luo.&lt;/em&gt; CIKM 2017. &lt;a href="https://i.cs.hku.hk/~zphuang/pub/CIKM17.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec.&lt;/strong&gt;
&lt;em&gt;Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang.&lt;/em&gt; WSDM 2018. &lt;a href="https://arxiv.org/pdf/1710.02971.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Conditional Network Embeddings.&lt;/strong&gt;
&lt;em&gt;Bo Kang, Jefrey Lijffijt, Tijl De Bie.&lt;/em&gt; ICLR 2019. &lt;a href="https://arxiv.org/abs/1805.07544" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Graph Infomax.&lt;/strong&gt;
&lt;em&gt;Petar Veličković, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, R Devon Hjelm.&lt;/em&gt; ICLR 2019. &lt;a href="https://arxiv.org/abs/1809.10341" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Anonymous Walk Embeddings.&lt;/strong&gt;
&lt;em&gt;Sergey Ivanov, Evgeny Burnaev.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1805.11921.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fairwalk: Towards Fair Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Tahleen Rahman, Bartlomiej Surma, Michael Backes, Yang Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://yangzhangalmo.github.io/papers/IJCAI19.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph and Autoencoder Based Feature Extraction for Zero-shot Learning.&lt;/strong&gt;
&lt;em&gt;Yang Liu, Deyan Xie, Quanxue Gao, Jungong Han, Shujian Wang, Xinbo Gao.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0421.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Space Embedding.&lt;/strong&gt;
&lt;em&gt;João Pereira, Evgeni Levin, Erik Stroes, Albert Groen.&lt;/em&gt; IJCAI 2019. &lt;a href="https://arxiv.org/abs/1907.13443" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Arbitrary-Order Proximity Preserved Network Embedding.&lt;/strong&gt;
&lt;em&gt;Ziwei Zhang, Peng Cui, Xiao Wang, Jian Pei, Xuanrong Yao, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="http://cuip.thumedialab.com/papers/NE-ArbitraryProximity.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Variational Network Embedding in Wasserstein Space.&lt;/strong&gt;
&lt;em&gt;Dingyuan Zhu, Peng Cui, Daixin Wang, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="http://cuip.thumedialab.com/papers/NE-DeepVariational.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MEGAN: A Generative Adversarial Network for Multi-View Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yiwei Sun, Suhang Wang, Tsung-Yu Hsieh, Xianfeng Tang, Vasant Honavar.&lt;/em&gt; IJCAI 2019. &lt;a href="https://arxiv.org/abs/1909.01084" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Embedding under Partial Monitoring for Evolving Networks&lt;/strong&gt;
&lt;em&gt;Yu Han, Jie Tang, Qian Chen.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0342.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Embedding with Dual Generation Tasks.&lt;/strong&gt;
&lt;em&gt;Jie Liu, Na Li, Zhicheng He.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0709.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Triplet Enhanced AutoEncoder: Model-free Discriminative Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yao Yang, Haoran Chen, Junming Shao.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0745.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Recursive Network Embedding with Regular Equivalence.&lt;/strong&gt;
&lt;em&gt;Ke Tu, Peng Cui, Xiao Wang, Philip S. Yu, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="http://cuip.thumedialab.com/papers/NE-RegularEquivalence.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Structural Node Embeddings via Diffusion Wavelets.&lt;/strong&gt;
&lt;em&gt;Claire Donnat, Marinka Zitnik, David Hallac, Jure Leskovec.&lt;/em&gt; KDD 2018. &lt;a href="https://arxiv.org/pdf/1710.10321.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Paced Network Embedding.&lt;/strong&gt;
&lt;em&gt;Hongchang Gao, Heng Huang.&lt;/em&gt; KDD 2018. &lt;a href="https://par.nsf.gov/servlets/purl/10074506" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Deep Network Representations with Adversarially Regularized Autoencoders.&lt;/strong&gt;
&lt;em&gt;Wenchao Yu, Cheng Zheng, Wei Cheng, Charu Aggarwal, Dongjin Song, Bo Zong, Haifeng Chen, Wei Wang.&lt;/em&gt; KDD 2018. &lt;a href="https://sites.cs.ucsb.edu/~bzong/doc/kdd-18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Large-Scale Learnable Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Hongyang Gao, Zhengyang Wang, Shuiwang Ji.&lt;/em&gt; KDD 2018. &lt;a href="https://arxiv.org/pdf/1808.03965" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-attributed-network" class="anchor" aria-hidden="true" href="#attributed-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Attributed Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Outlier Aware Network Embedding for Attributed Networks.&lt;/strong&gt;
&lt;em&gt;Sambaran Bandyopadhyay, N. Lokesh, M. N. Murty.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/3763" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Large-Scale Heterogeneous Feature Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Qingquan Song, Fan Yang, Xia Hu.&lt;/em&gt; AAAI 2019. &lt;a href="https://aaai.org/ojs/index.php/AAAI/article/view/4276" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Bayesian Optimization on Attributed Graphs.&lt;/strong&gt;
&lt;em&gt;Jiaxu Cui, Bo Yang, Xia Hu.&lt;/em&gt; AAAI 2019. &lt;a href="https://arxiv.org/pdf/1905.13403.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Efficient Attributed Network Embedding via Recursive Randomized Hashing.&lt;/strong&gt;
&lt;em&gt;Wei Wu, Bin Li, Ling Chen, Chengqi Zhang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0397.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Hongchang Gao, Heng Huang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0467.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ANRL: Attributed Network Representation Learning via Deep Neural Networks.&lt;/strong&gt;
&lt;em&gt;Zhen Zhang, Hongxia Yang, Jiajun Bu, Sheng Zhou, Pinggang Yu, Jianwei Zhang, Martin Ester, Can Wang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0438.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrative Network Embedding via Deep Joint Reconstruction.&lt;/strong&gt;
&lt;em&gt;Di Jin, Meng Ge, Liang Yang, Dongxiao He, Longbiao Wang, Weixiong Zhang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0473.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;node2vec: Scalable Feature Learning for Networks.&lt;/strong&gt;
&lt;em&gt;Aditya Grover, Jure Leskovec.&lt;/em&gt; KDD 2016. &lt;a href="http://www.kdd.org/kdd2016/papers/files/rfp0218-groverA.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/aditya-grover/node2vec"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Representation Learning with Rich Text Information.&lt;/strong&gt;
&lt;em&gt;Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, Edward Y. Chang.&lt;/em&gt; IJCAI 2015. &lt;a href="http://thunlp.org/~yangcheng/publications/ijcai15.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/tadw"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tri-Party Deep Network Representation.&lt;/strong&gt;
&lt;em&gt;Shirui Pan, Jia Wu, Xingquan Zhu, Chengqi Zhang, Yang Wang.&lt;/em&gt; IJCAI 2016. &lt;a href="https://www.ijcai.org/Proceedings/16/Papers/271.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TransNet: Translation-Based Network Representation Learning for Social Relation Extraction.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; IJCAI 2017. &lt;a href="http://thunlp.org/~tcc/publications/ijcai2017_transnet.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/transnet"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PRRE: Personalized Relation Ranking Embedding for Attributed Networks.&lt;/strong&gt;
&lt;em&gt;Sheng Zhou, Hongxia Yang, Xin Wang, Jiajun Bu, Martin Ester, Pinggang Yu, Jianwei Zhang, Can Wang.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3271741" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/zhoushengisnoob/PRRE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RSDNE: Exploring Relaxed Similarity and Dissimilarity from Completely-imbalanced Labels for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Zheng Wang, Xiaojun Ye, Chaokun Wang, YueXin Wu, Changping Wang, Kaiwen Liang.&lt;/em&gt; AAAI 2018. &lt;a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16062/15722" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/zhengwang100/RSDNE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised embedding in attributed networks with outliers.&lt;/strong&gt;
&lt;em&gt;Jiongqian Liang, Peter Jacobs, Jiankai Sun, and Srinivasan Parthasarathy.&lt;/em&gt; SDM 2018. &lt;a href="https://arxiv.org/pdf/1703.08100.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="http://jiongqianliang.com/SEANO/" rel="nofollow"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Representation Learning Framework for Property Graphs.&lt;/strong&gt;
&lt;em&gt;Yifan Hou,Hongzhi Chen,Changji Li,James Cheng,Ming-Chang Yang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330948/p65-hou.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning from Labeled and Unlabeled Vertices in Networks.&lt;/strong&gt;
&lt;em&gt;Wei Ye, Linfei Zhou, Dominik Mautz, Claudia Plant, Christian B?hm.&lt;/em&gt; KDD 2017. &lt;a href="https://dl.acm.org/citation.cfm?id=3098142" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Label Informed Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Jundong Li, Xia Hu.&lt;/em&gt; WSDM 2017. &lt;a href="http://www.public.asu.edu/~jundongl/paper/WSDM17_LANE.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Accelerated Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Jundong Li, Xia Hu.&lt;/em&gt; SDM 2017. &lt;a href="http://www.public.asu.edu/~jundongl/paper/SDM17_AANE.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variation Autoencoder Based Network Representation Learning for Classification.&lt;/strong&gt;
&lt;em&gt;Hang Li, Haozheng Wang, Zhenglu Yang, Masato Odagaki.&lt;/em&gt; ACL 2017. &lt;a href="https://aclweb.org/anthology/P17-3010" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Attributed Signed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Suhang Wang, Charu Aggarwal, Jiliang Tang, Huan Liu.&lt;/em&gt; CIKM 2017. &lt;a href="https://suhangwang.ist.psu.edu/publications/SNEA.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;From Properties to Links: Deep Network Embedding on Incomplete Graphs.&lt;/strong&gt;
&lt;em&gt;Dejian Yang, Senzhang Wang, Chaozhuo Li, Xiaoming Zhang, Zhoujun Li.&lt;/em&gt; CIKM 2017. &lt;a href="https://dl.acm.org/citation.cfm?id=3132847.3132975" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exploring Expert Cognition for Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Qingquan Song, Jundong Li, Xia Ben Hu.&lt;/em&gt; WSDM 2018. &lt;a href="http://www.public.asu.edu/~jundongl/paper/WSDM18_NEEC.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hierarchical Taxonomy Aware Network Embedding.&lt;/strong&gt;
&lt;em&gt;Jianxin Ma, Peng Cui, Xiao Wang, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="https://jianxinma.github.io/assets/NetHiex.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network-Specific Variational Auto-Encoder for Embedding in Attribute Networks.&lt;/strong&gt;
&lt;em&gt;Di Jin, Bingyi Li, Pengfei Jiao, Dongxiao He, Weixiong Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0370.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SPINE: Structural Identity Preserved Inductive Network Embedding.&lt;/strong&gt;
&lt;em&gt;Junliang Guo, Linli Xu, Jingchang Liu.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0333.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Content to Node: Self-translation Network Embedding.&lt;/strong&gt;
&lt;em&gt;Jie Liu, Zhicheng He, Lai Wei, Yalou Huang.&lt;/em&gt; KDD 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3219988" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-dynamic-network" class="anchor" aria-hidden="true" href="#dynamic-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Dynamic Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Network Embedding : An Extended Approach for Skip-gram based Network Embedding.&lt;/strong&gt;
&lt;em&gt;Lun Du, Yun Wang, Guojie Song, Zhicong Lu, Junshan Wang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0288.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Network Embedding by Modeling Triadic Closure Process.&lt;/strong&gt;
&lt;em&gt;Lekui Zhou, Yang Yang, Xiang Ren, Fei Wu, Yueting Zhuang.&lt;/em&gt; AAAI 2018. &lt;a href="http://yangy.org/works/dynamictriad/dynamic_triad.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DepthLGP: Learning Embeddings of Out-of-Sample Nodes in Dynamic Networks.&lt;/strong&gt;
&lt;em&gt;Jianxin Ma, Peng Cui, Wenwu Zhu.&lt;/em&gt; AAAI 2018. &lt;a href="https://jianxinma.github.io/assets/DepthLGP.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TIMERS: Error-Bounded SVD Restart on Dynamic Networks.&lt;/strong&gt;
&lt;em&gt;Ziwei Zhang, Peng Cui, Jian Pei, Xiao Wang, Wenwu Zhu.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.09541.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks.&lt;/strong&gt;
&lt;em&gt;Srijan Kumar,Xikun Zhang,Jure Leskovec.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330895/p1269-kumar.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Attributed Network Embedding for Learning in a Dynamic Environment.&lt;/strong&gt;
&lt;em&gt;Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, Huan Liu.&lt;/em&gt; CIKM 2017. &lt;a href="https://arxiv.org/pdf/1706.01860.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DyRep: Learning Representations over Dynamic Graphs.&lt;/strong&gt;
&lt;em&gt;Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, Hongyuan Zha.&lt;/em&gt; ICLR 2019. &lt;a href="https://openreview.net/forum?id=HyePrhR5KX" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedding Temporal Network via Neighborhood Formation.&lt;/strong&gt;
&lt;em&gt;Yuan Zuo, Guannan Liu, Hao Lin, Jia Guo, Xiaoqian Hu, Junjie Wu.&lt;/em&gt; KDD 2018. &lt;a href="https://zuoyuan.github.io/files/htne_kdd18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Node Embedding over Temporal Graphs.&lt;/strong&gt;
&lt;em&gt;Uriel Singer, Ido Guy, Kira Radinsky.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0640.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Embeddings for User Profiling in Twitter.&lt;/strong&gt;
&lt;em&gt;Shangsong Liang, Xiangliang Zhang, Zhaochun Ren, Evangelos Kanoulas.&lt;/em&gt; KDD 2018. &lt;a href="https://repository.kaust.edu.sa/bitstream/handle/10754/628781/p1764-liang.pdf?sequence=1&amp;amp;isAllowed=y" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NetWalk: A Flexible Deep Embedding Approach for Anomaly Detection in Dynamic Networks.&lt;/strong&gt;
&lt;em&gt;Wenchao Yu, Wei Cheng, Charu Aggarwal, Kai Zhang, Haifeng Chen, Wei Wang.&lt;/em&gt; KDD 2018. &lt;a href="http://www.shichuan.org/hin/topic/Embedding/2018.KDD%202018%20NetWalk_A%20Flexible%20Deep%20Embedding%20Approach%20for%20Anomaly%20Detection%20in%20Dynamic%20Networks.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Optimization for Embedding Highly-Dynamic and Recency-Sensitive Data.&lt;/strong&gt;
&lt;em&gt;Xumin Chen, Peng Cui, Shiqiang Yang.&lt;/em&gt; KDD 2018. &lt;a href="http://pengcui.thumedialab.com/papers/NE-ScalableOptimization.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-heterogeneous-information-network" class="anchor" aria-hidden="true" href="#heterogeneous-information-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Heterogeneous Information Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relation Structure-Aware Heterogeneous Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yuanfu Lu, Chuan Shi, Linmei Hu, Zhiyuan Liu.&lt;/em&gt; AAAI 2019. &lt;a href="https://arxiv.org/abs/1905.08027" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hyperbolic Heterogeneous Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Wang, Yiding Zhang, Chuan Shi.&lt;/em&gt; AAAI 2019. &lt;a href="http://shichuan.org/doc/65.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Latent Representations of Nodes for Classifying in Heterogeneous Social Networks.&lt;/strong&gt;
&lt;em&gt;Yann Jacob, Ludovic Denoyer, Patrick Gallinar.&lt;/em&gt; WSDM 2014. &lt;a href="http://webia.lip6.fr/~gallinar/gallinari/uploads/Teaching/WSDM2014-jacob.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Heterogeneous Network Embedding via Deep Architectures.&lt;/strong&gt;
&lt;em&gt;Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C. Aggarwal, Thomas S. Huang.&lt;/em&gt; KDD 2015. &lt;a href="http://www.ifp.illinois.edu/~chang87/papers/kdd_2015.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;metapath2vec: Scalable Representation Learning for Heterogeneous Networks.&lt;/strong&gt;
&lt;em&gt;Yuxiao Dong, Nitesh V. Chawla, Ananthram Swami.&lt;/em&gt; KDD 2017. &lt;a href="https://www3.nd.edu/~dial/publications/dong2017metapath2vec.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://ericdongyx.github.io/metapath2vec/m2v.html" rel="nofollow"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SHNE: Representation Learning for Semantic-Associated Heterogeneous Networks.&lt;/strong&gt;
&lt;em&gt;Chuxu Zhang, Ananthram Swami, Nitesh V. Chawla.&lt;/em&gt; WSDM 2019. &lt;a href="https://dl.acm.org/citation.cfm?id=3291001" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/chuxuzhang/WSDM2019_SHNE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Are Meta-Paths Necessary?: Revisiting Heterogeneous Graph Embeddings.&lt;/strong&gt;
&lt;em&gt;Rana Hussein, Dingqi Yang, Philippe Cudré-Mauroux.&lt;/em&gt; CIKM 2018. &lt;a href="https://exascale.info/assets/pdf/hussein2018cikm.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Abnormal Event Detection via Heterogeneous Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Shaohua Fan, Chuan Shi, Xiao Wang.&lt;/em&gt; CIKM 2018. &lt;a href="http://shichuan.org/doc/62.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multidimensional Network Embedding with Hierarchical Structures.&lt;/strong&gt;
&lt;em&gt;Yao Ma, Zhaochun Ren, Ziheng Jiang, Jiliang Tang, Dawei Yin.&lt;/em&gt; WSDM 2018. &lt;a href="http://cse.msu.edu/~mayao4/downloads/Multidimensional_Network_Embedding_with_Hierarchical_Structure.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Curriculum Learning for Heterogeneous Star Network Embedding via Deep Reinforcement Learning.&lt;/strong&gt;
&lt;em&gt;Meng Qu, Jian Tang, Jiawei Han.&lt;/em&gt; WSDM 2018. &lt;a href="http://delivery.acm.org/10.1145/3160000/3159711/p468-qu.pdf?ip=203.205.141.123&amp;amp;id=3159711&amp;amp;acc=ACTIVE%20SERVICE&amp;amp;key=39FCDE838982416F%2E39FCDE838982416F%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;amp;__acm__=1519788484_7383495a5c522cbe124e62e4d768f8cc" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generative Adversarial Network based Heterogeneous Bibliographic Network Representation for Personalized Citation Recommendation.&lt;/strong&gt;
&lt;em&gt;J. Han, Xiaoyan Cai, Libin Yang.&lt;/em&gt; AAAI 2018. &lt;a href="https://pdfs.semanticscholar.org/1596/d6487012696ba400fb69904a2c372a08a2be.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distance-aware DAG Embedding for Proximity Search on Heterogeneous Graphs.&lt;/strong&gt;
&lt;em&gt;Zemin Liu, Vincent W. Zheng, Zhou Zhao, Fanwei Zhu, Kevin Chen-Chuan Chang, Minghui Wu, Jing Ying.&lt;/em&gt; AAAI 2018. &lt;a href="https://pdfs.semanticscholar.org/b1cc/127a65c40e71121106d0c663f9b5baf9d6f9.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Representation Learning for Attributed Multiplex Heterogeneous Network.&lt;/strong&gt;
&lt;em&gt;Yukuo Cen,Xu Zou,Jianwei Zhang,Hongxia Yang,Jingren Zhou,Jie Tang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330964/p1358-cen.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Learning on Heterogeneous Information Networks.&lt;/strong&gt;
&lt;em&gt;Binbin Hu,Yuan Fang,Chuan Shi&lt;/em&gt; KDD 2019 &lt;a href="http://delivery.acm.org/10.1145/3340000/3330970/p120-hu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HetGNN: Heterogeneous Graph Neural Network.&lt;/strong&gt;
&lt;em&gt;Chuxu Zhang,Dongjin Song,Chao Huang,Ananthram Swami,Nitesh V. Chawla.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330961/p793-zhang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;IntentGC: a Scalable Graph Convolution Framework Fusing Heterogeneous Information for Recommendation&lt;/strong&gt;
&lt;em&gt;Jun Zhao, Zhou Zhou, Ziyu Guan, Wei Zhao, Ning Wei, Guang Qiu and Xiaofei He.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330686/p2347-zhao.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation.&lt;/strong&gt;
&lt;em&gt;Shaohua Fan, Junxiong Zhu, Xiaotian Han, Chuan Shi, Linmei Hu, Biyu Ma and Yongliang Li.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330673/p2478-fan.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Your Style Your Identity: Leveraging Writing and Photography Styles for Drug Trafficker Identification in Darknet Markets over Attributed Heterogeneous Information Network.&lt;/strong&gt;
&lt;em&gt;Yiming Zhang, Yujie Fan,Wei Song, Shifu HouYanfang Ye, Xin Li,Liang Zhao,Chuan Shi,Jiabin Wang, Qi Xiong.&lt;/em&gt; WWW 2019. &lt;a href="https://www.gwern.net/docs/sr/2019-zhang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HIN2Vec: Explore Meta-paths in Heterogeneous Information Networks for Representation Learning.&lt;/strong&gt;
&lt;em&gt;Tao-yang Fu, Wang-Chien Lee, Zhen Lei.&lt;/em&gt; CIKM 2017. &lt;a href="http://shichuan.org/hin/topic/Embedding/2017.%20CIKM%20HIN2Vec.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction.&lt;/strong&gt;
&lt;em&gt;Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, Qi Liu.&lt;/em&gt; WSDM 2018. &lt;a href="https://arxiv.org/pdf/1712.00732.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ActiveHNE: Active Heterogeneous Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xia Chen, Guoxian Yu, Jun Wang, Carlotta Domeniconi, Zhao Li, Xiangliang Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0294.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unified Embedding Model over Heterogeneous Information Network for Personalized Recommendation.&lt;/strong&gt;
&lt;em&gt;Zekai Wang, Hongzhi Liu, Yingpeng Du, Zhonghai Wu, Xing Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0529.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Easing Embedding Learning by Comprehensive Transcription of Heterogeneous Information Networks.&lt;/strong&gt;
&lt;em&gt;Yu Shi, Qi Zhu, Fang Guo, Chao Zhang, Jiawei Han.&lt;/em&gt; KDD 2018. &lt;a href="https://yu-shi-homepage.github.io/kdd18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PME: Projected Metric Embedding on Heterogeneous Networks for Link Prediction.&lt;/strong&gt;
&lt;em&gt;Hongxu Chen, Hongzhi Yin, Weiqing Wang, Hao Wang, Quoc Viet Hung Nguyen, Xue Li.&lt;/em&gt; KDD 2018. &lt;a href="http://net.pku.edu.cn/daim/hongzhi.yin/papers/KDD18-Hongxu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-bipartite-network" class="anchor" aria-hidden="true" href="#bipartite-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Bipartite Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Collaborative Similarity Embedding for Recommender Systems.&lt;/strong&gt;
&lt;em&gt;Chih-Ming Chen,Chuan-Ju Wang,Ming-Feng Tsai,Yi-Hsuan Yang.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1902.06188.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Node Embeddings in Interaction Graphs.&lt;/strong&gt;
&lt;em&gt;Yao Zhang, Yun Xiong, Xiangnan Kong, Yangyong Zhu.&lt;/em&gt; CIKM 2017. &lt;a href="https://web.cs.wpi.edu/~xkong/publications/papers/cikm17.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hierarchical Representation Learning for Bipartite Graphs.&lt;/strong&gt;
&lt;em&gt;Chong Li, Kunyang Jia, Dan Shen, C.J. Richard Shi, Hongxia Yang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0398.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-directed-network" class="anchor" aria-hidden="true" href="#directed-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Directed Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ATP: Directed Graph Embedding with Asymmetric Transitivity Preservation.&lt;/strong&gt;
&lt;em&gt;Jiankai Sun, Bortik Bandyopadhyay, Armin Bashizade, Jiongqian Liang, P. Sadayappan, Srinivasan Parthasarathy.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/3794" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Asymmetric Transitivity Preserving Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, Wenwu Zhu.&lt;/em&gt; KDD 2016. &lt;a href="http://cuip.thumedialab.com/papers/hoppe.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;"Bridge": Enhanced Signed Directed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yiqi Chen, Tieyun Qian, Huan Liu, Ke Sun.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3271738" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SIDE: Representation Learning in Signed Directed Networks.&lt;/strong&gt;
&lt;em&gt;Junghwan Kim, Haekyu Park, Ji-Eun Lee, U Kang.&lt;/em&gt; WWW 2018. &lt;a href="https://datalab.snu.ac.kr/side/resources/side.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-other-models" class="anchor" aria-hidden="true" href="#other-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Other Models&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Multiplex Network Embedding. （Multiplex Network)&lt;/strong&gt;
&lt;em&gt;Hongming Zhang, Liwei Qiu, Lingling Yi, Yangqiu Song.&lt;/em&gt; IJCAI 2018. &lt;a href="http://www.cse.ust.hk/~yqsong/papers/2018-IJCAI-MultiplexNetworkEmbedding.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structural Deep Embedding for Hyper-Networks. (Hyper-Network)&lt;/strong&gt;
&lt;em&gt;Ke Tu, Peng Cui, Xiao Wang, fei Wang, Wenwu Zhu.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.10146.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Representation Learning for Scale-free Networks. (Scale-free Network)&lt;/strong&gt;
&lt;em&gt;Rui Feng, Yang Yang, Wenjie Hu, Fei Wu, Yueting Zhuang.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.10755.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Co-Regularized Deep Multi-Network Embedding. (Multi-Network)&lt;/strong&gt;
&lt;em&gt;Jingchao Ni, Shiyu Chang, Xiao Liu, Wei Cheng, Haifeng Chen, Dongkuan Xu, Xiang Zhang.&lt;/em&gt; WWW 2018. &lt;a href="https://nijingchao.github.io/paper/www18_dmne.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Joint Link Prediction and Network Alignment via Cross-graph Embedding. (Multi-Network)&lt;/strong&gt;
&lt;em&gt;Xingbo Du, Junchi Yan, Hongyuan Zha.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0312.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DANE: Domain Adaptive Network Embedding. (Multi-Network)&lt;/strong&gt;
&lt;em&gt;Yizhou Zhang, Guojie Song, Lun Du, Shuwen Yang, Yilun Jin.&lt;/em&gt; IJCAI 2019. &lt;a href="https://arxiv.org/abs/1906.00684" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SPARC: Self-Paced Network Representation for Few-Shot Rare Category Characterization. (Few-Shot Learning)&lt;/strong&gt;
&lt;em&gt;Dawei Zhou, Jingrui He, Hongxia Yang, Wei Fan.&lt;/em&gt; KDD 2018. &lt;a href="https://dl.acm.org/authorize?N665885" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-applications" class="anchor" aria-hidden="true" href="#applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Applications&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-natural-language-processing" class="anchor" aria-hidden="true" href="#natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Natural Language Processing&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Personalized Question Routing via Heterogeneous Network Embedding.&lt;/strong&gt;
&lt;em&gt;Zeyu Li, Jyun-Yu Jiang, Yizhou Sun, Wei Wang.&lt;/em&gt; AAAI 2019. &lt;a href="http://web.cs.ucla.edu/~yzsun/papers/2019_AAAI_QR.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks.&lt;/strong&gt;
&lt;em&gt;Jian Tang, Meng Qu, Qiaozhu Mei.&lt;/em&gt; KDD 2015. &lt;a href="https://arxiv.org/pdf/1508.00200.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/mnqu/PTE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-knowledge-graph" class="anchor" aria-hidden="true" href="#knowledge-graph"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Knowledge Graph&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interaction Embeddings for Prediction and Explanation in Knowledge Graphs.&lt;/strong&gt;
&lt;em&gt;Wen Zhang, Bibek Paudel, Wei Zhang, Abraham Bernstein, Huajun Chen.&lt;/em&gt; WSDM 2019. &lt;a href="https://arxiv.org/pdf/1903.04750.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Shared Embedding Based Neural Networks for Knowledge Graph Completion.&lt;/strong&gt;
&lt;em&gt;Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, Xueqi Cheng.&lt;/em&gt; CIKM 2018 &lt;a href="https://dl.acm.org/citation.cfm?id=3271705" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Re-evaluating Embedding-Based Knowledge Graph Completion Methods.&lt;/strong&gt;
&lt;em&gt;Farahnaz Akrami, Lingbing Guo, Wei Hu, Chengkai Li.&lt;/em&gt; CIKM 2018. &lt;a href="http://ranger.uta.edu/~cli/pubs/2018/kgcompletion-cikm18short-akrami.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-social-network" class="anchor" aria-hidden="true" href="#social-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Social Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Learning for Weakly-Supervised Social Network Alignment.&lt;/strong&gt;
&lt;em&gt;Chaozhuo Li, Senzhang Wang, Yukun Wang, Philip Yu, Yanbo Liang, Yun Liu, Zhoujun Li.&lt;/em&gt; AAAI 2019. &lt;a href="https://aaai.org/ojs/index.php/AAAI/article/view/3889" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TransConv: Relationship Embedding in Social Networks.&lt;/strong&gt;
&lt;em&gt;Yi-Yu Lai, Jennifer Neville, Dan Goldwasser.&lt;/em&gt; AAAI 2019. &lt;a href="https://aaai.org/ojs/index.php/AAAI/article/view/4314" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised User Geolocation via Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Afshin Rahimi, Trevor Cohn, Timothy Baldwin.&lt;/em&gt; ACL 2018. &lt;a href="https://arxiv.org/pdf/1804.08049.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MASTER: across Multiple social networks, integrate Attribute and STructure Embedding for Reconciliation.&lt;/strong&gt;
&lt;em&gt;Sen Su, Li Sun, Zhongbao Zhang, Gen Li, Jielun Qu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0537.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MEgo2Vec: Embedding Matched Ego Networks for User Alignment Across Social Networks.&lt;/strong&gt;
&lt;em&gt;Jing Zhang, Bo Chen, Xianming Wang, Hong Chen, Cuiping Li, Fengmei Jin, Guojie Song, Yutao Zhang.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3271705" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Link Prediction via Subgraph Embedding-Based Convex Matrix Completion.&lt;/strong&gt;
&lt;em&gt;Zhu Cao, Linlin Wang, Gerard De melo.&lt;/em&gt; AAAI 2018. &lt;a href="http://iiis.tsinghua.edu.cn/~weblt/papers/link-prediction-subgraphembeddings.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;On Exploring Semantic Meanings of Links for Embedding Social Networks.&lt;/strong&gt;
&lt;em&gt;Linchuan Xu, Xiaokai Wei, Jiannong Cao, Philip S Yu.&lt;/em&gt; WWW 2018. &lt;a href="https://pdfs.semanticscholar.org/ccd3/ede78393628b5f0256ebfccbb4ac293394de.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MCNE: An End-to-End Framework for Learning Multiple Conditional Network Representations of Social Network.&lt;/strong&gt;
&lt;em&gt;Hao Wang,Tong Xu,Qi Liu,Defu Lian,Enhong Chen,Dongfang Du,Han Wu,Wen Su.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330931/p1064-wang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unsupervised Feature Selection in Signed Social Networks.&lt;/strong&gt;
&lt;em&gt;Kewei Cheng, Jundong Li, Huan Liu.&lt;/em&gt; KDD 2017. &lt;a href="http://www.public.asu.edu/~jundongl/paper/KDD17_SignedFS.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Network Embedding with Community Structural Information.&lt;/strong&gt;
&lt;em&gt;Yu Li, Ying Wang, Tingting Zhang, Jiawei Zhang, Yi Chang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0407.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-graph-clustering" class="anchor" aria-hidden="true" href="#graph-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Graph Clustering&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spectral Clustering in Heterogeneous Information Networks.&lt;/strong&gt;
&lt;em&gt;Xiang Li , Ben Kao, Zhaochun Ren, Dawei Yin.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.researchgate.net/profile/Xiang_Li238/publication/332606853_Spectral_Clustering_in_Heterogeneous_Information_Networks/links/5cc035e892851c8d2200aa29/Spectral-Clustering-in-Heterogeneous-Information-Networks.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-view Clustering with Graph Embedding for Connectome Analysis.&lt;/strong&gt;
&lt;em&gt;Guixiang Ma, Lifang He, Chun-Ta Lu, Weixiang Shao, Philip S Yu, Alex D Leow, Ann B Ragin.&lt;/em&gt; CIKM 2017. &lt;a href="https://www.cs.uic.edu/~clu/doc/cikm17_mcge.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Graph Embedding for Ensemble Clustering.&lt;/strong&gt;
&lt;em&gt;Zhiqiang Tao, Hongfu Liu, Jun Li, Zhaowen Wang, Yun Fu.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0494.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variational Graph Embedding and Clustering with Laplacian Eigenmaps.&lt;/strong&gt;
&lt;em&gt;Zitai Chen, Chuan Chen, Zong Zhang, Zibin Zheng, Qingsong Zou.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0297.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-community-detection" class="anchor" aria-hidden="true" href="#community-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Community Detection&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Incorporating Network Embedding into Markov Random Field for Better Community Detection.&lt;/strong&gt;
&lt;em&gt;Di Jin, Xinxin You, Weihao Li, Dongxiao He, Peng Cui, Francoise Fogelman-Soulie, Tanmoy Chakraborty.&lt;/em&gt; AAAI 2019. &lt;a href="http://pengcui.thumedialab.com/papers/NE-MRF.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Unified Framework for Community Detection and Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Xiangkai Zeng, Hao Wang, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun, Bo Zhang, Leyu Lin.&lt;/em&gt; TKDE 2018. &lt;a href="https://ieeexplore.ieee.org/abstract/document/8403293/" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;COSINE: Community-Preserving Social Network Embedding from Information Diffusion Cascades.&lt;/strong&gt;
&lt;em&gt;Yuan Zhang, Tianshu Lyu, Yan Zhang.&lt;/em&gt; AAAI 2018. &lt;a href="https://pdfs.semanticscholar.org/fec8/24c51b59063ba92b66bb7404010954ced5ac.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-facet Network Embedding: Beyond the General Solution of Detection and Representation.&lt;/strong&gt;
&lt;em&gt;Liang Yang, Xiaochun Cao, Yuanfang Guo.&lt;/em&gt; AAAI 2018. &lt;a href="https://yangliang.github.io/pdf/aaai18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Community Detection in Attributed Graphs: An Embedding Approach.&lt;/strong&gt;
&lt;em&gt;Ye Li, Chaofeng Sha, Xin Huang, Yanchun Zhang.&lt;/em&gt; AAAI 2018. &lt;a href="https://www.comp.hkbu.edu.hk/~xinhuang/publications/pdfs/AAAI18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Preserving Proximity and Global Ranking for Node Embedding.&lt;/strong&gt;
&lt;em&gt;Yi-An Lai, Chin-Chi Hsu, Wenhao Chen, Mi-Yen Yeh, Shou-De Lin.&lt;/em&gt; NIPS 2017. &lt;a href="https://pdfs.semanticscholar.org/b692/c82115889115ef3e63fb7e6b23c8eb9c85b3.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Community Embedding with Community Detection and Node Embedding on Graphs.&lt;/strong&gt;
&lt;em&gt;Sandro Cavallari, Vincent W. Zheng, Hongyun Cai, Kevin ChenChuan Chang, Erik Cambria.&lt;/em&gt; CIKM 2017. &lt;a href="https://sentic.net/community-embedding.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-recommendation" class="anchor" aria-hidden="true" href="#recommendation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Recommendation&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Convolutional Neural Networks for Web-Scale Recommender Systems.&lt;/strong&gt;
&lt;em&gt;Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec.&lt;/em&gt; KDD 2018. &lt;a href="https://arxiv.org/pdf/1806.01973.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Is a Single Vector Enough? Exploring Node Polysemy for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Ninghao Liu,Qiaoyu Tan,Yuening Li,Hongxia Yang,Jingren Zhou,Xia Hu.&lt;/em&gt; KDD 2019. &lt;a href="https://arxiv.org/pdf/1905.10668.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender System.&lt;/strong&gt;
&lt;em&gt;Qitian Wu,Hengrui Zhang,Xiaofeng Gao,Peng He,Paul Weng,Han Gao,Guihai Chen.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1903.10433.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-other-applications" class="anchor" aria-hidden="true" href="#other-applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Other Applications&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cash-out User Detection based on Attributed Heterogeneous Information Network with a Hierarchical Attention Mechanism. (Finance)&lt;/strong&gt;
&lt;em&gt;Binbin Hu, Zhiqiang Zhang, Chuan Shi, Jun Zhou, Xiaolong Li, Yuan Qi.&lt;/em&gt; AAAI 2019. &lt;a href="http://shichuan.org/doc/64.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Building Causal Graphs from Medical Literature and Electronic Medical Records. (Medicine)&lt;/strong&gt;
&lt;em&gt;Galia Nordon, Gideon Koren, Varda Shalev, Benny Kimelfeld, Uri Shalit, Kira Radinsky.&lt;/em&gt; AAAI 2019. &lt;a href="http://www.kiraradinsky.com/files/aaai-building-causal.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Attacks on Node Embeddings via Graph Poisoning. (Adversarial)&lt;/strong&gt;
&lt;em&gt;Aleksandar Bojchevski, Stephan Günnemann.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/bojchevski19a/bojchevski19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Compositional Fairness Constraints for Graph Embeddings. (Adversarial)&lt;/strong&gt;
&lt;em&gt;Avishek Bose, William Hamilton.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/bose19a/bose19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gromov-Wasserstein Learning for Graph Matching and Node Embedding. (Graph Matching)&lt;/strong&gt;
&lt;em&gt;Hongteng Xu, Dixin Luo, Hongyuan Zha, Lawrence Carin Duke.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/xu19b/xu19b.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Matching Networks for Learning the Similarity of Graph Structured Objects. (Graph Matching)&lt;/strong&gt;
&lt;em&gt;Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, Pushmeet Kohli.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/li19d/li19d.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MolGAN: An implicit generative model for small molecular graphs. (Molecular Generation)&lt;/strong&gt;
&lt;em&gt;Nicola De Cao, Thomas Kipf.&lt;/em&gt; ICML Workshop 2018. &lt;a href="https://arxiv.org/pdf/1805.11973.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational recurrent neural networks. (Relational Reasoning)&lt;/strong&gt;
&lt;em&gt;Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap.&lt;/em&gt; NeurIPS 2018. &lt;a href="https://arxiv.org/pdf/1806.01822.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Constructing Narrative Event Evolutionary Graph for Script Event Prediction. (Script Event Prediction)&lt;/strong&gt;
&lt;em&gt;Zhongyang Li, Xiao Ding, Ting Liu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://arxiv.org/abs/1805.05081" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/eecrazy/ConstructingNEEG_IJCAI_2018"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Network-embedding Based Method for Author Disambiguation. (Author Disambiguation)&lt;/strong&gt;
&lt;em&gt;Jun Xu, Siqi Shen, Dongsheng Li, Yongquan Fu.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3269272" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Graph Embedding for Ranking Optimization in E-commerce.(E-commerce)&lt;/strong&gt;
&lt;em&gt;Chen Chu, Zhao Li, Beibei Xin, Fengchao Peng, Chuanren Liu, Remo Rohs, Qiong Luo, Jingren Zhou.&lt;/em&gt; CIKM 2018. &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6330176/" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Network-to-Network Model for Content-rich Network Embedding.&lt;/strong&gt;
&lt;em&gt;Zhicheng He,Jie Liu,Na Li,Yalou Huang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330924/p1037-he.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unifying Inter-region Autocorrelation and Intra-region Structures for Spatial Embedding via Collective Adversarial Learning.&lt;/strong&gt;
&lt;em&gt;Yunchao Zhang,Pengyang Wang,Xiaolin Li,Yu Zheng,Yanjie Fu.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330972/p1700-zhang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Neural IR Meets Graph Embedding: A Ranking Model for Product Search.&lt;/strong&gt;
&lt;em&gt;Yuan Zhang,Dong Wang,Yan Zhang.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1901.08286.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-Network Embedding for Multi-Network Alignment.&lt;/strong&gt;
&lt;em&gt;Xiaokai Chu,Xinxin Fan,Di Yao,Zhihua Zhu,Jianhui Huang,Jingping Bi.&lt;/em&gt; WWW 2019. &lt;a href="https://sci-hub.tw/10.1145/3308558.3313499#" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Name Disambiguation in Anonymized Graphs using Network Embedding. (Name Disambiguation)&lt;/strong&gt;
&lt;em&gt;Baichuan Zhang, Mohammad Al Hasan.&lt;/em&gt; CIKM 2017. &lt;a href="https://arxiv.org/pdf/1702.02287.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NetGAN: Generating Graphs via Random Walks. (Graph Generation)&lt;/strong&gt;
&lt;em&gt;Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zügner, Stephan Günnemann.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1803.00816" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Networks as Learnable Physics Engines for Inference and Control. (Physics)&lt;/strong&gt;
&lt;em&gt;Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1806.01242.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational inductive bias for physical construction in humans and machines. (Human physical reasoning)&lt;/strong&gt;
&lt;em&gt;Jessica B. Hamrick, Kelsey R. Allen, Victor Bapst, Tina Zhu, Kevin R. McKee, Joshua B. Tenenbaum, Peter W. Battaglia.&lt;/em&gt; CogSci 2018. &lt;a href="https://arxiv.org/pdf/1806.01203.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>thunlp</author><guid isPermaLink="false">https://github.com/thunlp/NRLPapers</guid><pubDate>Fri, 15 Nov 2019 00:04:00 GMT</pubDate></item><item><title>Wandmalfarbe/pandoc-latex-template #5 in TeX, Today</title><link>https://github.com/Wandmalfarbe/pandoc-latex-template</link><description>&lt;p&gt;&lt;i&gt;A pandoc LaTeX template to convert markdown files to PDF or LaTeX.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="icon.png"&gt;&lt;img src="icon.png" align="right" height="110" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-eisvogel" class="anchor" aria-hidden="true" href="#eisvogel"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eisvogel&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/Wandmalfarbe/pandoc-latex-template" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b87755780c096b231f6f2e8a6192d30318d187a3/68747470733a2f2f7472617669732d63692e6f72672f57616e646d616c66617262652f70616e646f632d6c617465782d74656d706c6174652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/Wandmalfarbe/pandoc-latex-template.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A clean &lt;strong&gt;pandoc LaTeX template&lt;/strong&gt; to convert your markdown files to PDF or LaTeX. It is designed for lecture notes and exercises with a focus on computer science. The template is compatible with pandoc 2.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A custom title page&lt;/th&gt;
&lt;th align="center"&gt;A basic example page&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/custom-titlepage/custom-titlepage.pdf"&gt;&lt;img src="examples/custom-titlepage/custom-titlepage.png" alt="A custom title page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/basic-example/basic-example.pdf"&gt;&lt;img src="examples/basic-example/basic-example.png" alt="A basic example page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install pandoc from &lt;a href="http://pandoc.org/" rel="nofollow"&gt;http://pandoc.org/&lt;/a&gt;. You also need to install &lt;a href="https://en.wikibooks.org/wiki/LaTeX/Installation#Distributions" rel="nofollow"&gt;LaTeX&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download the latest version of the Eisvogel template from &lt;a href="https://github.com/Wandmalfarbe/pandoc-latex-template/releases/latest"&gt;the release page&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extract the downloaded ZIP archive and open the folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Move the template &lt;code&gt;eisvogel.tex&lt;/code&gt; to your pandoc templates folder and rename the file to &lt;code&gt;eisvogel.latex&lt;/code&gt;. The location of the templates folder depends on your operating system:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unix, Linux, macOS: &lt;code&gt;/Users/USERNAME/.local/share/pandoc/templates/&lt;/code&gt; or &lt;code&gt;/Users/USERNAME/.pandoc/templates/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Windows Vista or later: &lt;code&gt;C:\Users\USERNAME\AppData\Roaming\pandoc\templates\&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If there are no folders called &lt;code&gt;templates&lt;/code&gt; or &lt;code&gt;pandoc&lt;/code&gt; you need to create them and put the template &lt;code&gt;eisvogel.latex&lt;/code&gt; inside. You can find the default user data directory on your system by looking at the output of &lt;code&gt;pandoc --version&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Open the terminal and navigate to the folder where your markdown file is located.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Execute the following command&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --from markdown --template eisvogel --listings&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where &lt;code&gt;example.md&lt;/code&gt; is the markdown file you want to convert to PDF.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to have nice headers and footers you need to supply metadata to your document. You can do that with a &lt;a href="http://pandoc.org/MANUAL.html#extension-yaml_metadata_block" rel="nofollow"&gt;YAML metadata block&lt;/a&gt; at the top of your markdown document (see the &lt;a href="examples/basic-example/basic-example.md"&gt;example markdown file&lt;/a&gt;). Your markdown document may look like the following:&lt;/p&gt;
&lt;div class="highlight highlight-source-gfm"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;---&lt;/span&gt;
&lt;span class="pl-ent"&gt;title&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;The Document Title&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-ent"&gt;author&lt;/span&gt;: &lt;span class="pl-s"&gt;[Example Author, Another Author]&lt;/span&gt;
&lt;span class="pl-ent"&gt;date&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;2017-02-20&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-ent"&gt;keywords&lt;/span&gt;: &lt;span class="pl-s"&gt;[Markdown, Example]&lt;/span&gt;
&lt;span class="pl-c"&gt;...&lt;/span&gt;

Here is the actual document text...&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-custom-template-variables" class="anchor" aria-hidden="true" href="#custom-template-variables"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Template Variables&lt;/h3&gt;
&lt;p&gt;This template defines some new variables to control the appearance of the resulting PDF document. The existing template variables from pandoc are all supported and their documentation can be found in &lt;a href="https://pandoc.org/MANUAL.html#variables-for-latex" rel="nofollow"&gt;the pandoc manual&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;turns on the title page when &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-color&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the background color of the title page. The color value must be given as an HTML hex color like &lt;code&gt;D8DE2C&lt;/code&gt; without the leading number sign (&lt;code&gt;#&lt;/code&gt;). When specifying the color in YAML, it is advisable to enclose it in quotes like so &lt;code&gt;titlepage-color: "D8DE2C"&lt;/code&gt; to avoid the truncation of the color (e.g. &lt;code&gt;000000&lt;/code&gt; becoming &lt;code&gt;0&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-text-color&lt;/code&gt; (defaults to &lt;code&gt;5F5F5F&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the text color of the title page&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-rule-color&lt;/code&gt; (defaults to &lt;code&gt;435488&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the color of the rule on the top of the title page&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-rule-height&lt;/code&gt; (defaults to &lt;code&gt;4&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the height of the rule on the top of the title page (in points)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-background&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the path to a background image for the title page. The background image is scaled to cover the entire page. In the examples folder under &lt;code&gt;titlepage-background&lt;/code&gt; are a few example background images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;caption-justification&lt;/code&gt; (defaults to &lt;code&gt;raggedright&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;justification setting for captions (uses the &lt;code&gt;justification&lt;/code&gt; parameter of the &lt;a href="https://ctan.org/pkg/caption?lang=en" rel="nofollow"&gt;caption&lt;/a&gt; package)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;toc-own-page&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;begin new page after table of contents, when &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;listings-disable-line-numbers&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;disables line numbers for all listings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;listings-no-page-break&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;avoid page break inside listings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;disable-header-and-footer&lt;/code&gt; (default to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;disables the header and footer completely on all pages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;header-left&lt;/code&gt; (defaults to the title)&lt;/p&gt;
&lt;p&gt;the text on the left side of the header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;header-center&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the text in the center of the header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;header-right&lt;/code&gt; (defaults to the date)&lt;/p&gt;
&lt;p&gt;the text on the right side of the header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footer-left&lt;/code&gt; (defaults to the author)&lt;/p&gt;
&lt;p&gt;the text on the left side of the footer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footer-center&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the text in the center of the footer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footer-right&lt;/code&gt; (defaults to the page number)&lt;/p&gt;
&lt;p&gt;the text on the right side of the footer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footnotes-pretty&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;prettifies formatting of footnotes (requires package &lt;code&gt;footmisc&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footnotes-disable-backlinks&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;disables making the reference from the footnote at the bottom of the page into a link back to the occurence of the footnote in the main text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;book&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;typeset as book&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;logo&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;path to an image that will be displayed on the title page. The path is always relative to where pandoc is executed. The option &lt;code&gt;--resource-path&lt;/code&gt; has no effect.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;logo-width&lt;/code&gt; (defaults to &lt;code&gt;100&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the width of the logo (in points)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;first-chapter&lt;/code&gt; (defaults to &lt;code&gt;1&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;if typesetting a book with chapter numbers, specifies the number that will be assigned to the first chapter&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;float-placement-figure&lt;/code&gt; (defaults to &lt;code&gt;H&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;Reset the default placement specifier for figure environments to the supplied value e.g. &lt;code&gt;htbp&lt;/code&gt;. The available specifiers are listed below. The first four placement specifiers can be combined.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;h&lt;/code&gt;: Place the float &lt;em&gt;here&lt;/em&gt;, i.e., approximately at the same point it occurs in the source text.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;t&lt;/code&gt;: Place the float at the &lt;em&gt;top&lt;/em&gt; of the page.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt;: Place the float at the &lt;em&gt;bottom&lt;/em&gt; of the page.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;p&lt;/code&gt;: Place the float on the next &lt;em&gt;page&lt;/em&gt; that will contain only floats like figures and tables.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;H&lt;/code&gt;: Place the float &lt;em&gt;HERE&lt;/em&gt; (exactly where it occurs in the source text). The &lt;code&gt;H&lt;/code&gt; specifier is provided by the &lt;a href="https://ctan.org/pkg/float" rel="nofollow"&gt;float package&lt;/a&gt; and may not be used in conjunction with any other placement specifiers.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;table-use-row-colors&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;enables row colors for tables. The default value is &lt;code&gt;false&lt;/code&gt; because the coloring extends beyond the edge of the table and there is currently no way to change that.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;code-block-font-size&lt;/code&gt; (defaults to &lt;code&gt;\small&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;LaTeX command to change the font size for code blocks. The available values are &lt;code&gt;\tiny&lt;/code&gt;, &lt;code&gt;\scriptsize&lt;/code&gt;, &lt;code&gt;\footnotesize&lt;/code&gt;, &lt;code&gt;\small&lt;/code&gt;, &lt;code&gt;\normalsize&lt;/code&gt;, &lt;code&gt;\large&lt;/code&gt;, &lt;code&gt;\Large&lt;/code&gt;, &lt;code&gt;\LARGE&lt;/code&gt;, &lt;code&gt;\huge&lt;/code&gt; and &lt;code&gt;\Huge&lt;/code&gt;. This option will change the font size for default code blocks using the verbatim environment and for code blocks generated with listings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-numbered-sections" class="anchor" aria-hidden="true" href="#numbered-sections"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Numbered Sections&lt;/h3&gt;
&lt;p&gt;For PDFs with &lt;a href="http://pandoc.org/MANUAL.html#options-affecting-specific-writers" rel="nofollow"&gt;numbered sections&lt;/a&gt; use the &lt;code&gt;--number-sections&lt;/code&gt; or &lt;code&gt;-N&lt;/code&gt; option.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --number-sections&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-syntax-highlighting-with-listings" class="anchor" aria-hidden="true" href="#syntax-highlighting-with-listings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syntax Highlighting with Listings&lt;/h3&gt;
&lt;p&gt;You can get syntax highlighting of delimited code blocks by using the LaTeX package listings with the option &lt;code&gt;--listings&lt;/code&gt;. This example will produce the same syntax highlighting as in the example PDF.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --listings&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-syntax-highlighting-without-listings" class="anchor" aria-hidden="true" href="#syntax-highlighting-without-listings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syntax Highlighting Without Listings&lt;/h3&gt;
&lt;p&gt;The following examples show &lt;a href="http://pandoc.org/MANUAL.html#syntax-highlighting" rel="nofollow"&gt;syntax highlighting of delimited code blocks&lt;/a&gt; without using listings. To see a list of all the supported highlight styles, type &lt;code&gt;pandoc --list-highlight-styles&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style pygments&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style kate&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style espresso&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style tango&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-standalone-latex-document" class="anchor" aria-hidden="true" href="#standalone-latex-document"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Standalone LaTeX Document&lt;/h3&gt;
&lt;p&gt;To produce a standalone LaTeX document for compiling with any LaTeX editor use &lt;code&gt;.tex&lt;/code&gt; as an output file extension.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.tex --template eisvogel&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-changing-the-document-language" class="anchor" aria-hidden="true" href="#changing-the-document-language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changing the Document Language&lt;/h3&gt;
&lt;p&gt;The default language of this template is American English. The &lt;code&gt;lang&lt;/code&gt; variable identifies the main language of the document, using a code according to &lt;a href="https://tools.ietf.org/html/bcp47" rel="nofollow"&gt;BCP 47&lt;/a&gt; (e.g. &lt;code&gt;en&lt;/code&gt; or &lt;code&gt;en-GB&lt;/code&gt;). For an incomplete list of the supported language codes see &lt;a href="http://mirrors.ctan.org/language/hyph-utf8/doc/generic/hyph-utf8/hyph-utf8.pdf" rel="nofollow"&gt;the documentation for the hyph-utf8 package (Section 2)&lt;/a&gt;. The following example changes the language to British English:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel -V lang=en-GB&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The following example changes the language to German:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel -V lang=de&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-typesetting-a-book" class="anchor" aria-hidden="true" href="#typesetting-a-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Typesetting a Book&lt;/h3&gt;
&lt;p&gt;To typeset a book supply the template variable &lt;code&gt;-V book&lt;/code&gt; from the command line or via &lt;code&gt;book: true&lt;/code&gt; in the metadata.&lt;/p&gt;
&lt;p&gt;To get the correct chapter headings you need to tell pandoc that it should convert first level headings (indicated by one &lt;code&gt;#&lt;/code&gt; in markdown) to chapters with the command line option &lt;code&gt;--top-level-division=chapter&lt;/code&gt;. Chapter numbers start at 1. If you need to change that, specify &lt;code&gt;first-chapter&lt;/code&gt; in the template variables.&lt;/p&gt;
&lt;p&gt;There will be one blank page before each chapter because the template is two-sided per default. So if you plan to publish your book as a PDF and don’t need a blank page you should add the class option &lt;code&gt;onesided&lt;/code&gt; which can be done by supplying a template variable &lt;code&gt;-V classoption=oneside&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-example-images" class="anchor" aria-hidden="true" href="#example-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example Images&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A green title page&lt;/th&gt;
&lt;th align="center"&gt;A background image on the title page&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/green-titlepage/green-titlepage.pdf"&gt;&lt;img src="examples/green-titlepage/green-titlepage.png" alt="A green title page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/titlepage-background/titlepage-background.pdf"&gt;&lt;img src="examples/titlepage-background/titlepage-background.png" alt="A background image on the title page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;images and tables&lt;/th&gt;
&lt;th align="center"&gt;Code blocks styled without listings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/images-and-tables/images-and-tables.pdf"&gt;&lt;img src="examples/images-and-tables/images-and-tables.png" alt="images and tables" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/without-listings/without-listings.pdf"&gt;&lt;img src="examples/without-listings/without-listings.png" alt="Code blocks styled without listings" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A book&lt;/th&gt;
&lt;th align="center"&gt;Code blocks styled with listings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/book/book.pdf"&gt;&lt;img src="examples/book/book.png" alt="A book" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/listings/listings.pdf"&gt;&lt;img src="examples/listings/listings.png" alt="Code blocks styled with listings" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-credits" class="anchor" aria-hidden="true" href="#credits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credits&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This template includes code for styling block quotations from &lt;a href="https://github.com/aaronwolen/pandoc-letter"&gt;pandoc-letter&lt;/a&gt; by &lt;a href="https://github.com/aaronwolen"&gt;Aaron Wolen&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This project is open source licensed under the BSD 3-Clause License. Please see the &lt;a href="LICENSE"&gt;LICENSE file&lt;/a&gt; for more information.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Wandmalfarbe</author><guid isPermaLink="false">https://github.com/Wandmalfarbe/pandoc-latex-template</guid><pubDate>Fri, 15 Nov 2019 00:05:00 GMT</pubDate></item><item><title>Falmouth-Games-Academy/comp110-journal #6 in TeX, Today</title><link>https://github.com/Falmouth-Games-Academy/comp110-journal</link><description>&lt;p&gt;&lt;i&gt;Base repository for the COMP110 Research Journal assessment&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;This repo does not have a README.&lt;/i&gt;&lt;/p&gt;</description><author>Falmouth-Games-Academy</author><guid isPermaLink="false">https://github.com/Falmouth-Games-Academy/comp110-journal</guid><pubDate>Fri, 15 Nov 2019 00:06:00 GMT</pubDate></item><item><title>Pemrograman2/Tugas #7 in TeX, Today</title><link>https://github.com/Pemrograman2/Tugas</link><description>&lt;p&gt;&lt;i&gt;Tugas Pemrograman 2&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tugas" class="anchor" aria-hidden="true" href="#tugas"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tugas&lt;/h1&gt;
&lt;p&gt;Tugas Pemrograman 2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Folder tugas ditempatkan sesuai dengan kelas masing masing&lt;/li&gt;
&lt;li&gt;Format nama folder = Nama(NPM)&lt;/li&gt;
&lt;li&gt;ditempatkan sesuai bab tugas yang dikerjakan (chapter)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-untuk-mengsubmit-tugas-silahkan-melakukan-pull-request" class="anchor" aria-hidden="true" href="#untuk-mengsubmit-tugas-silahkan-melakukan-pull-request"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;untuk mengsubmit tugas silahkan melakukan pull request&lt;/h1&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pemrograman2</author><guid isPermaLink="false">https://github.com/Pemrograman2/Tugas</guid><pubDate>Fri, 15 Nov 2019 00:07:00 GMT</pubDate></item><item><title>exacity/deeplearningbook-chinese #8 in TeX, Today</title><link>https://github.com/exacity/deeplearningbook-chinese</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Book Chinese Translation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-中文翻译" class="anchor" aria-hidden="true" href="#deep-learning-中文翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning 中文翻译&lt;/h1&gt;
&lt;p&gt;在众多网友的帮助和校对下，中文版终于出版了。尽管还有很多问题，但至少90%的内容是可读的，并且是准确的。
我们尽可能地保留了原书&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;Deep Learning&lt;/a&gt;中的意思并保留原书的语句。&lt;/p&gt;
&lt;p&gt;然而我们水平有限，我们无法消除众多读者的方差。我们仍需要大家的建议和帮助，一起减小翻译的偏差。&lt;/p&gt;
&lt;p&gt;大家所要做的就是阅读，然后汇总你的建议，提issue（最好不要一个一个地提）。如果你确定你的建议不需要商量，可以直接发起PR。&lt;/p&gt;
&lt;p&gt;对应的翻译者：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第1、4、7、10、14、20章及第12.4、12.5节由 @swordyork 负责&lt;/li&gt;
&lt;li&gt;第2、5、8、11、15、18章由 @liber145 负责&lt;/li&gt;
&lt;li&gt;第3、6、9章由 @KevinLee1110 负责&lt;/li&gt;
&lt;li&gt;第13、16、17、19章及第12.1至12.3节由 @futianfan 负责&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-面向的读者" class="anchor" aria-hidden="true" href="#面向的读者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;面向的读者&lt;/h2&gt;
&lt;p&gt;请直接下载&lt;a href="https://github.com/exacity/deeplearningbook-chinese/releases/download/v0.5-beta/dlbook_cn_v0.5-beta.pdf"&gt;PDF&lt;/a&gt;阅读。
不打算提供EPUB等格式，如有需要请自行修改。&lt;/p&gt;
&lt;p&gt;这一版准确性已经有所提高，读者可以以中文版为主、英文版为辅来阅读学习，但我们仍建议研究者阅读&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;原版&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-出版及开源原因" class="anchor" aria-hidden="true" href="#出版及开源原因"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;出版及开源原因&lt;/h2&gt;
&lt;p&gt;本书由人民邮电出版社出版，如果你觉得中文版PDF对你有所帮助，希望你能支持下纸质正版书籍。
如果你觉得中文版不行，希望你能多提建议。非常感谢各位！
纸质版也会进一步更新，需要大家更多的建议和意见，一起完善中文版。&lt;/p&gt;
&lt;p&gt;纸质版目前在人民邮电出版社的异步社区出售，见&lt;a href="http://www.epubit.com.cn/book/details/4278" rel="nofollow"&gt;地址&lt;/a&gt;。
价格不低，但看了样本之后，我们认为物有所值。
注意，我们不会通过媒体进行宣传，希望大家先看电子版内容，再判断是否购买纸质版。&lt;/p&gt;
&lt;p&gt;以下是开源的具体原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们不是文学工作者，不专职翻译。单靠我们，无法给出今天的翻译，众多网友都给我们提出了宝贵的建议，因此开源帮了很大的忙。出版社会给我们稿费（我们也不知道多少，可能2万左右），我们也不好意思自己用，商量之后觉得捐出是最合适的，以所有贡献过的网友的名义（我们把稿费捐给了杉树公益，用于4名贵州高中生三年的生活费，见&lt;a href="https://github.com/exacity/deeplearningbook-chinese/blob/master/donation.pdf"&gt;捐赠情况&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;PDF电子版对于技术类书籍来说是很重要的，随时需要查询，拿着纸质版到处走显然不合适。国外很多技术书籍都有对应的电子版（虽然不一定是正版），而国内的几乎没有。个人认为这是出版社或者作者认为国民素质还没有高到主动为知识付费的境界，所以不愿意"泄露"电子版。时代在进步，我们也需要改变。特别是翻译作品普遍质量不高的情况下，要敢为天下先。&lt;/li&gt;
&lt;li&gt;深度学习发展太快，日新月异，所以我们希望大家更早地学到相关的知识。我觉得原作者开放PDF电子版也有类似的考虑，也就是先阅读后付费。我们认为中国人口素质已经足够高，懂得为知识付费。当然这不是付给我们的，是付给出版社的，出版社再付给原作者。我们不希望中文版的销量因PDF电子版的存在而下滑。出版社只有值回了版权才能在以后引进更多的优秀书籍。我们这个开源翻译先例也不会成为一个反面案例，以后才会有更多的PDF电子版。&lt;/li&gt;
&lt;li&gt;开源也涉及版权问题，出于版权原因，我们不再更新此初版PDF文件，请大家以最终的纸质版为准。（但源码会一直更新）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-致谢" class="anchor" aria-hidden="true" href="#致谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;致谢&lt;/h2&gt;
&lt;p&gt;我们有3个类别的校对人员。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;负责人也就是对应的翻译者。&lt;/li&gt;
&lt;li&gt;简单阅读，对语句不通顺或难以理解的地方提出修改意见。&lt;/li&gt;
&lt;li&gt;中英对比，进行中英对应阅读，排除少翻错翻的情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有校对建议都保存在各章的&lt;code&gt;annotations.txt&lt;/code&gt;文件中。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;章节&lt;/th&gt;
&lt;th&gt;负责人&lt;/th&gt;
&lt;th&gt;简单阅读&lt;/th&gt;
&lt;th&gt;中英对比&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter1_introduction/" rel="nofollow"&gt;第一章 前言&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc, @SiriusXDJ, @corenel, @NeutronT&lt;/td&gt;
&lt;td&gt;@linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter2_linear_algebra/" rel="nofollow"&gt;第二章 线性代数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@SiriusXDJ, @angrymidiao&lt;/td&gt;
&lt;td&gt;@badpoem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter3_probability_and_information_theory/" rel="nofollow"&gt;第三章 概率与信息论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@SiriusXDJ&lt;/td&gt;
&lt;td&gt;@kkpoker, @Peiyan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter4_numerical_computation/" rel="nofollow"&gt;第四章 数值计算&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;@zhangyafeikimi&lt;/td&gt;
&lt;td&gt;@hengqujushi&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter5_machine_learning_basics/" rel="nofollow"&gt;第五章 机器学习基础&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@wheaio, @huangpingchun&lt;/td&gt;
&lt;td&gt;@fairmiracle, @linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter6_deep_feedforward_networks/" rel="nofollow"&gt;第六章 深度前馈网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;David_Chow, @linzhp, @sailordiary&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter7_regularization/" rel="nofollow"&gt;第七章 深度学习中的正则化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@NBZCC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/" rel="nofollow"&gt;第八章 深度模型中的优化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@happynoom, @codeVerySlow&lt;/td&gt;
&lt;td&gt;@huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter9_convolutional_networks/" rel="nofollow"&gt;第九章 卷积网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @corenel&lt;/td&gt;
&lt;td&gt;@zhiding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter10_sequence_modeling_rnn/" rel="nofollow"&gt;第十章 序列建模：循环和递归网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @yinruiqing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter11_practical_methodology/" rel="nofollow"&gt;第十一章 实践方法论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter12_applications/" rel="nofollow"&gt;第十二章 应用&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork, @futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@corenel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter13_linear_factor_models/" rel="nofollow"&gt;第十三章 线性因子模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;@cloudygoose&lt;/td&gt;
&lt;td&gt;@ZhiweiYang&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter14_autoencoders/" rel="nofollow"&gt;第十四章 自编码器&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@Seaball, @huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter15_representation_learning/" rel="nofollow"&gt;第十五章 表示学习&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@cnscottzheng&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter16_structured_probabilistic_modelling/" rel="nofollow"&gt;第十六章 深度学习中的结构化概率模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter17_monte_carlo_methods/" rel="nofollow"&gt;第十七章 蒙特卡罗方法&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter18_confronting_the_partition_function/" rel="nofollow"&gt;第十八章 面对配分函数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@tankeco&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter19_approximate_inference/" rel="nofollow"&gt;第十九章 近似推断&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary, @hengqujushi, huanghaojun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter20_deep_generative_models/" rel="nofollow"&gt;第二十章 深度生成模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;参考文献&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@pkuwwt&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我们会在纸质版正式出版的时候，在书中致谢，正式感谢各位作出贡献的同学！&lt;/p&gt;
&lt;p&gt;还有很多同学提出了不少建议，我们都列在此处。&lt;/p&gt;
&lt;p&gt;@tttwwy @tankeco @fairmiracle @GageGao @huangpingchun @MaHongP @acgtyrant @yanhuibin315 @Buttonwood @titicacafz
@weijy026a @RuiZhang1993 @zymiboxpay @xingkongliang @oisc @tielei @yuduowu @Qingmu @HC-2016 @xiaomingabc
@bengordai @Bojian @JoyFYan @minoriwww @khty2000 @gump88 @zdx3578 @PassStory @imwebson @wlbksy @roachsinai @Elvinczp
@endymecy name:YUE-DaJiong @9578577 @linzhp @cnscottzheng @germany-zhu  @zhangyafeikimi @showgood163 @gump88
@kangqf @NeutronT @badpoem @kkpoker @Seaball @wheaio @angrymidiao @ZhiweiYang @corenel @zhaoyu611 @SiriusXDJ @dfcv24 EmisXXY
FlyingFire vsooda @friskit-china @poerin @ninesunqian @JiaqiYao @Sofring @wenlei @wizyoung @imageslr @@indam @XuLYC
@zhouqingping @freedomRen @runPenguin @pkuwwt @wuqi @tjliupeng @neo0801 @jt827859032 @demolpc @fishInAPool
@xiaolangyuxin @jzj1993 @whatbeg LongXiaJun jzd&lt;/p&gt;
&lt;p&gt;如有遗漏，请务必通知我们，可以发邮件至&lt;code&gt;echo c3dvcmQueW9ya0BnbWFpbC5jb20K | base64 --decode&lt;/code&gt;。
这是我们必须要感谢的，所以不要不好意思。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;排版&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-注意" class="anchor" aria-hidden="true" href="#注意"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;注意&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;各种问题或者建议可以提issue，建议使用中文。&lt;/li&gt;
&lt;li&gt;由于版权问题，我们不能将图片和bib上传，请见谅。&lt;/li&gt;
&lt;li&gt;Due to copyright issues, we would not upload figures and the bib file.&lt;/li&gt;
&lt;li&gt;可用于学习研究目的，不得用于任何商业行为。谢谢！&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-markdown格式" class="anchor" aria-hidden="true" href="#markdown格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Markdown格式&lt;/h2&gt;
&lt;p&gt;这种格式确实比较重要，方便查阅，也方便索引。初步转换后，生成网页，具体见&lt;a href="https://exacity.github.io/deeplearningbook-chinese" rel="nofollow"&gt;deeplearningbook-chinese&lt;/a&gt;。
注意，这种转换没有把图放进去，也不会放图。目前使用单个&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;，基于latex文件转换，以后可能会更改但原则是不直接修改&lt;a href="docs/_posts"&gt;md文件&lt;/a&gt;。
需要的同学可以自行修改&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-html格式" class="anchor" aria-hidden="true" href="#html格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTML格式&lt;/h2&gt;
&lt;p&gt;读者可以使用&lt;a href="https://github.com/coolwanglu/pdf2htmlEX"&gt;pdf2htmlEX&lt;/a&gt;进行转换，直接将PDF转换为HTML。&lt;/p&gt;
&lt;p&gt;Updating.....&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>exacity</author><guid isPermaLink="false">https://github.com/exacity/deeplearningbook-chinese</guid><pubDate>Fri, 15 Nov 2019 00:08:00 GMT</pubDate></item><item><title>openbmc/docs #9 in TeX, Today</title><link>https://github.com/openbmc/docs</link><description>&lt;p&gt;&lt;i&gt;OpenBMC Documentation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-openbmc-documentation" class="anchor" aria-hidden="true" href="#openbmc-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenBMC documentation&lt;/h1&gt;
&lt;p&gt;This repository contains documentation for OpenBMC as a whole. There may
be component-specific documentation in the repository for each component.&lt;/p&gt;
&lt;p&gt;The &lt;a href="features.md"&gt;features&lt;/a&gt; document lists the project's major features
with links to more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-openbmc-usage" class="anchor" aria-hidden="true" href="#openbmc-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenBMC Usage&lt;/h2&gt;
&lt;p&gt;These documents describe how to use OpenBMC, including using the programmatic
interfaces to an OpenBMC system.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="rest-api.md"&gt;rest-api.md&lt;/a&gt;: Introduction to using the OpenBMC REST API&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="console.md"&gt;console.md&lt;/a&gt;: Using the host console&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="host-management.md"&gt;host-management.md&lt;/a&gt;: Performing host management tasks
with OpenBMC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="code-update"&gt;code-update&lt;/a&gt;: Updating OpenBMC and host platform firmware&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-openbmc-development" class="anchor" aria-hidden="true" href="#openbmc-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenBMC Development&lt;/h2&gt;
&lt;p&gt;These documents contain details on developing OpenBMC code itself&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="cheatsheet.md"&gt;cheatsheet.md&lt;/a&gt;: Quick reference for some common
development tasks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;: Guidelines for contributing to
OpenBMC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="kernel-development.md"&gt;kernel-development.md&lt;/a&gt;: Reference for common
kernel development tasks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="REST-cheatsheet.md"&gt;REST-cheatsheet.md&lt;/a&gt;: Quick reference for some common
curl commands usage.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-openbmc-goals" class="anchor" aria-hidden="true" href="#openbmc-goals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenBMC Goals&lt;/h2&gt;
&lt;p&gt;The OpenBMC project's aim is to create a highly extensible framework for BMC
software and implement for data-center computer systems.&lt;/p&gt;
&lt;p&gt;We have a few high-level objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The OpenBMC framework must be extensible, easy to learn, and usable in a
variety of programming languages.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a REST API for external management, and allow for "pluggable"
interfaces for other types of management interactions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a remote host console, accessible over the network&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Persist network configuration settable from REST interface and host&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a robust solution for RTC management, exposed to the host.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compatible with host firmware implementations for basic IPMI communication
between host and BMC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a flexible and hierarchical inventory tracking component&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maintain a sensor database and track thresholds&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-technical-steering-committee" class="anchor" aria-hidden="true" href="#technical-steering-committee"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Technical Steering Committee&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Brad Bishop (chair), IBM&lt;/li&gt;
&lt;li&gt;Nancy Yuen, Google&lt;/li&gt;
&lt;li&gt;Sai Dasari, Facebook&lt;/li&gt;
&lt;li&gt;James Mihm, Intel&lt;/li&gt;
&lt;li&gt;Sagar Dharia, Microsoft&lt;/li&gt;
&lt;li&gt;Supreeth Venkatesh, Arm&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>openbmc</author><guid isPermaLink="false">https://github.com/openbmc/docs</guid><pubDate>Fri, 15 Nov 2019 00:09:00 GMT</pubDate></item></channel></rss>