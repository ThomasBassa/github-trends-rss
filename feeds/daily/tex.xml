<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: TeX, Today</title><link>https://github.com/trending/tex?since=daily</link><description>The top repositories on GitHub for tex, measured daily</description><pubDate>Fri, 07 Feb 2020 01:04:30 GMT</pubDate><lastBuildDate>Fri, 07 Feb 2020 01:04:30 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>abhat222/Data-Science--Cheat-Sheet #1 in TeX, Today</title><link>https://github.com/abhat222/Data-Science--Cheat-Sheet</link><description>&lt;p&gt;&lt;i&gt;Cheat Sheets&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;
Taken from &lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet"&gt;&lt;strong&gt;Link to original GitHub page&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-data-science-cheatsheets" class="anchor" aria-hidden="true" href="#data-science-cheatsheets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data Science Cheatsheets&lt;/h1&gt;
&lt;p&gt;List of Data Science Cheatsheets :&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="Artificial%20Intelligence/README.md"&gt;Artificial Intelligence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Big%20Data/README.md"&gt;Big Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Engineering/README.md"&gt;Data Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Mining/README.md"&gt;Data Mining&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Science/README.md"&gt;Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Visualization/README.md"&gt;Data Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Warehouse/README.md"&gt;Data Warehouse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Deep%20Learning/README.md"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DevOps/README.md"&gt;DevOps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Docker%20&amp;amp;%20Kubernetes/README.md"&gt;Docker &amp;amp; Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Excel/README.md"&gt;Excel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Git/README.md"&gt;Git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Images//README.md"&gt;Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Interview%20Questions/README.md"&gt;Interview Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Linux/README.md"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet#machine-learning"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Mathematics/README.md"&gt;Mathematics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Matlab/README.md"&gt;Matlab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="NLP/README.md"&gt;NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Numpy/README.md"&gt;Numpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Ordinary%20Differential%20Equations/README.md"&gt;Ordinary Differential Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Pandas/README.md"&gt;Pandas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Probability/README.md"&gt;Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Python/README.md"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Scala/README.md"&gt;Scala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="SQL/README.md"&gt;SQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Statistics/README.md"&gt;Statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-python" class="anchor" aria-hidden="true" href="#python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Python/Python_cheatsheet.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/python_cheatsheet.PNG?" alt="Illustration" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Python/spaCy.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/SpaCy.PNG?" alt="Illustration" width="415px" style="max-width:100%;"&gt;&lt;/a&gt;                                   &lt;b&gt;Python Cheat Sheet&lt;/b&gt;                                                                  &lt;b&gt;Spacy Cheat Sheet&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Pandas/pandas_cheat_sheet.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/DA_Pandas.PNG?" alt="Illustration" height="320px" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Pandas/Reading%20and%20Writing%20data%20with%20PANDAS.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/RW_Panda.PNG?" alt="Illustration" width="415px" height="320px" style="max-width:100%;"&gt;&lt;/a&gt;                                   &lt;b&gt;Data Analysis with Pandas&lt;b&gt;                                              &lt;b&gt;Pandas (Reading and Writing Data)&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Python/python-cheatsheets-ds.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/PythonForDS.PNG?" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Numpy/100_numpy_exercises.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/101_numpy_exercises.jpg?" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                                   &lt;b&gt;Python for Data Science&lt;b&gt;                                                                  &lt;b&gt;100 Numpy Exercises&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Python/Python%20for%20Data%20Analysis.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/pythonforDA.PNG?" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Pandas/pandas-10min.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/Pandas_10mins.PNG?" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                             &lt;b&gt;Python for Data Analysis&lt;b&gt;                                                              &lt;b&gt;10 Minutes to Pandas&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;h1&gt;&lt;a id="user-content-r-language" class="anchor" aria-hidden="true" href="#r-language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;R Language&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/R%20Cheat%20Sheet/R%20Cheat%20Sheets.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/R_cheatsheet.PNG?" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/R%20Cheat%20Sheet/R%20Programming%20Cheat%20Sheet.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/R.PNG?" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                                    &lt;b&gt;R Cheat Sheet&lt;b&gt;                                                                     &lt;b&gt;R (Basics &amp;amp; Advanced)&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;h1&gt;&lt;a id="user-content-machine-learning" class="anchor" aria-hidden="true" href="#machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/blob/master/Machine%20Learning/Applied%20Machine%20Learning%20Problem%20Solving%20Framework.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/abhat222/Data-Science--Cheat-Sheet/master/Machine%20Learning/Applied%20Machine%20Learning%20Problem%20Solving%20Framework.jpg" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;       &lt;/a&gt;&lt;a href="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/blob/master/Machine%20Learning/Cheat%20Sheet%20Algorithms%20for%20Supervised%20and%20Unsupervised%20Learning.pdf"&gt;&lt;img src="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/raw/master/Images/supervised-and-unsupervised-learning.PNG?" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                                    &lt;b&gt;R Cheat Sheet&lt;b&gt;                                                                     &lt;b&gt;Machine Learning&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;h1&gt;&lt;a id="user-content-deep-learning" class="anchor" aria-hidden="true" href="#deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/blob/master/Deep%20Learning/Coursera%20Deep%20Learning%20course%20Notes.pdf"&gt;&lt;img src="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/raw/master/Images/coursera-deep-learning.PNG" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;       &lt;/a&gt;&lt;a href="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/blob/master/Deep%20Learning/super-cheatsheet-deep-learning.pdf"&gt;&lt;img src="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/raw/master/Images/super-cheatsheet-deep-learning.PNG" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                                    &lt;b&gt;R Cheat Sheet&lt;b&gt;                                                                     &lt;b&gt;Machine Learning&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/article&gt;&lt;/div&gt;</description><author>abhat222</author><guid isPermaLink="false">https://github.com/abhat222/Data-Science--Cheat-Sheet</guid><pubDate>Fri, 07 Feb 2020 00:01:00 GMT</pubDate></item><item><title>ixy-languages/ixy-languages #2 in TeX, Today</title><link>https://github.com/ixy-languages/ixy-languages</link><description>&lt;p&gt;&lt;i&gt;A high-speed network driver written in C, Rust, Go, C#, Java, OCaml, Haskell, Swift, Javascript, and Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h1&gt;
&lt;p&gt;Ixy is an educational user space network driver for the Intel ixgbe family of 10 Gbit/s NICs (82599ES aka X520, X540, X550, X552, ...).
Its goal is to show that writing a super-fast network driver can be surprisingly simple, check out the &lt;a href="https://github.com/emmericp/ixy"&gt;full description in the main repository of the C implementation&lt;/a&gt; to learn about the basics of user space drivers.
Ixy was originally written in C as lowest common denominator of system programming languages, but it is possible to write user space drivers in any programming language.&lt;/p&gt;
&lt;p&gt;Check out our research paper &lt;a href="https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/the-case-for-writing-network-drivers-in-high-level-languages.pdf" rel="nofollow"&gt;"The Case for Writing Network Drivers in High-Level Languages"&lt;/a&gt; [&lt;a href="https://www.net.in.tum.de/publications/bibtex/highleveldrivers.bib" rel="nofollow"&gt;BibTeX&lt;/a&gt;] or watch the recording of our &lt;a href="https://media.ccc.de/v/35c3-9670-safe_and_secure_drivers_in_high-level_languages" rel="nofollow"&gt;talk at 35C3&lt;/a&gt; to learn more.&lt;/p&gt;
&lt;p&gt;Yes, these drivers are really a full implementation of an actual PCIe driver in these languages; they handle everything from setting up DMA memory to receiving and transmitting packets in a high-level language. You don't need to write any kernel code to build drivers!
Some languages require a few lines of C stubs for features not offered by the language; usually related to getting the memory address of buffers or calling mmap in the right way. But all the core logic is in high-level languages; the implementations are about 1000 lines of code each.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;Full evaluation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;C&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/emmericp/ixy"&gt;ixy.c&lt;/a&gt;*&lt;/td&gt;
&lt;td&gt;Finished&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/ixy-writing-user-space-network-drivers.pdf" rel="nofollow"&gt;Paper&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Rust&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ixy-languages/ixy.rs"&gt;ixy.rs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Finished&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.net.in.tum.de/fileadmin/bibtex/publications/theses/2018-ixy-rust.pdf" rel="nofollow"&gt;Thesis&lt;/a&gt;, &lt;a href="Rust-vs-C-performance.md"&gt;Rust vs. C performance comparison&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Go&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ixy-languages/ixy.go"&gt;ixy.go&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Finished&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.net.in.tum.de/fileadmin/bibtex/publications/theses/2018-ixy-go.pdf" rel="nofollow"&gt;Thesis&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;C#&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ixy-languages/ixy.cs"&gt;ixy.cs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Finished&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.net.in.tum.de/fileadmin/bibtex/publications/theses/2018-ixy-c-sharp.pdf" rel="nofollow"&gt;Thesis&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Java&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ixy-languages/ixy.java"&gt;ixy.java&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Finished&lt;/td&gt;
&lt;td&gt;(WIP), &lt;a href="Java-garbage-collectors.md"&gt;GC comparison&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OCaml&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ixy-languages/ixy.ml"&gt;ixy.ml&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Finished&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ixy-languages/ixy.ml/blob/master/README.md"&gt;Documentation&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Haskell&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ixy-languages/ixy.hs"&gt;ixy.hs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Finished&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.net.in.tum.de/fileadmin/bibtex/publications/theses/2019-ixy-haskell.pdf" rel="nofollow"&gt;Thesis&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Swift&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ixy-languages/ixy.swift"&gt;ixy.swift&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Finished&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ixy-languages/ixy.swift/blob/master/README.md"&gt;Documentation&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Javascript&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ixy-languages/ixy.js"&gt;ixy.js&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Finished&lt;/td&gt;
&lt;td&gt;(WIP)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ixy-languages/ixy.py"&gt;ixy.py&lt;/a&gt;*&lt;/td&gt;
&lt;td&gt;Finished&lt;/td&gt;
&lt;td&gt;(WIP)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;*) also features a VirtIO driver for easy testing in VMs with Vagrant&lt;/p&gt;
&lt;p&gt;This repository here is only a short summary of the project, check out the repositories and full evaluations linked above for all the gory details.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-performance" class="anchor" aria-hidden="true" href="#performance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Performance&lt;/h1&gt;
&lt;p&gt;Our &lt;a href="https://github.com/ixy-languages/benchmark-scripts"&gt;benchmarking script&lt;/a&gt; for the &lt;a href="https://github.com/emmericp/MoonGen"&gt;MoonGen packet generator&lt;/a&gt; loads the forwarder example application with full bidirectional load at 20 Gbit/s with 64 byte packets (29.76 Mpps).
The forwarder then increments one byte in the packet to ensure that the packet is loaded all the way into the L1 cache.
Correct functionality of the forwarder is also tested by the script by validating sequence numbers.&lt;/p&gt;
&lt;p&gt;A main driver of performance for network drivers is sending/receiving packets in batches from/to the NIC.
Ixy can already achieve a high performance with relatively low batch sizes of 32-64 because it is a full user space driver.
Other user space packet processing frameworks like netmap that rely on a kernel driver need larger batch sizes of 512 and above to amortize the larger overhead of communicating with the driver in the kernel.
Running this on a single core of a Xeon E3-1230 v2 CPU yields these throughput results in million packets per second (Mpps) when varying the batch size.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/batches-3.3.png"&gt;&lt;img src="img/batches-3.3.png" alt="Performance with different batch sizes, CPU at 3.3 GHz" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/batches-1.6.png"&gt;&lt;img src="img/batches-1.6.png" alt="Performance with different batch sizes, CPU at 1.6 GHz" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Notes on multi-core:&lt;/em&gt; Some languages implement multi-threading for ixy, but some can't or are limited by the language's design (e.g., the GIL in Python and OCaml). However, this isn't a real problem because multi-threading within one process isn't really necessary.
Network cards can split the traffic at the hardware level (via a feature called RSS), the traffic can then be distributed to independent different processes.
For example, &lt;a href="https://github.com/snabbco/snabb"&gt;Snabb&lt;/a&gt; works like this and many DPDK applications use multiple threads that do not communicate (shared-nothing architecture).&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-latency" class="anchor" aria-hidden="true" href="#latency"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Latency&lt;/h1&gt;
&lt;p&gt;Average and median latency is the same regardless of the programming language as latency is dominated by buffering times, the &lt;a href="https://github.com/ixy-languages/benchmark-scripts"&gt;evaluation script&lt;/a&gt; can sample the latency of up to 1000 packets per second with hardware timestamping (precision: 12.8 nanoseconds).
This yields somewhat interesting results depending on queue sizes and NUMA configuration, see the &lt;a href="https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/ixy-writing-user-space-network-drivers.pdf" rel="nofollow"&gt;ixy paper&lt;/a&gt; for an evaluation.&lt;/p&gt;
&lt;p&gt;Latency spikes induced by languages featuring a garbage collector might not be caught by the test setup above.
We have a second test setup to catch this: we also capture &lt;em&gt;all&lt;/em&gt; packets before and after the device under test with fiber optic taps and use &lt;a href="https://github.com/AP-Frank/MoonGen/tree/moonsniff"&gt;MoonSniff&lt;/a&gt; to acquire hardware timestamps (25.6 nanosecond precision) for all packets.&lt;/p&gt;
&lt;p&gt;Running the forwarder on an Intel Xeon E5-2620 v3 at 2.4 GHz yields the following results for the forwarding latency.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/latency-hdr-hist-1.png"&gt;&lt;img src="img/latency-hdr-hist-1.png" alt="Latency" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/latency-hdr-hist-10.png"&gt;&lt;img src="img/latency-hdr-hist-10.png" alt="Latency" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/latency-hdr-hist-20.png"&gt;&lt;img src="img/latency-hdr-hist-20.png" alt="Latency" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;How to read this graph: the x axis refers to the percentage of packets that can be forwarded in this time. For example, if a language takes longer than 100µs for only one packet in 1000 packets, it shows a value of 100µs at x position 99.9.
This graph focuses on the tail latency, note the logarithmic scaling on the x axis.&lt;/p&gt;
&lt;p&gt;Graphs only include languages that can cope with the offered load.
Measuring an overloaded driver doesn't make sense as latency is dominated by the buffer size in this scenario.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ixy-languages</author><guid isPermaLink="false">https://github.com/ixy-languages/ixy-languages</guid><pubDate>Fri, 07 Feb 2020 00:02:00 GMT</pubDate></item><item><title>vdumoulin/conv_arithmetic #3 in TeX, Today</title><link>https://github.com/vdumoulin/conv_arithmetic</link><description>&lt;p&gt;&lt;i&gt;A technical report on convolution arithmetic in the context of deep learning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-convolution-arithmetic" class="anchor" aria-hidden="true" href="#convolution-arithmetic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolution arithmetic&lt;/h1&gt;
&lt;p&gt;A technical report on convolution arithmetic in the context of deep learning.&lt;/p&gt;
&lt;p&gt;The code and the images of this tutorial are free to use as regulated by the
licence and subject to proper attribution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[1] Vincent Dumoulin, Francesco Visin - &lt;a href="https://arxiv.org/abs/1603.07285" rel="nofollow"&gt;A guide to convolution arithmetic
for deep learning&lt;/a&gt;
(&lt;a href="https://gist.github.com/fvisin/165ca9935392fa9600a6c94664a01214"&gt;BibTeX&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-convolution-animations" class="anchor" aria-hidden="true" href="#convolution-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/no_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/arbitrary_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/arbitrary_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/same_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/same_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/full_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/full_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, no strides&lt;/td&gt;
    &lt;td&gt;Arbitrary padding, no strides&lt;/td&gt;
    &lt;td&gt;Half padding, no strides&lt;/td&gt;
    &lt;td&gt;Full padding, no strides&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_strides.gif"&gt;&lt;img width="150px" src="gif/no_padding_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides.gif"&gt;&lt;img width="150px" src="gif/padding_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides_odd.gif"&gt;&lt;img width="150px" src="gif/padding_strides_odd.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, strides&lt;/td&gt;
    &lt;td&gt;Padding, strides&lt;/td&gt;
    &lt;td&gt;Padding, strides (odd)&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-transposed-convolution-animations" class="anchor" aria-hidden="true" href="#transposed-convolution-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transposed convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/no_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/arbitrary_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/arbitrary_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/same_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/same_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/full_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/full_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, no strides, transposed&lt;/td&gt;
    &lt;td&gt;Arbitrary padding, no strides, transposed&lt;/td&gt;
    &lt;td&gt;Half padding, no strides, transposed&lt;/td&gt;
    &lt;td&gt;Full padding, no strides, transposed&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/no_padding_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/padding_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides_odd_transposed.gif"&gt;&lt;img width="150px" src="gif/padding_strides_odd_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, strides, transposed&lt;/td&gt;
    &lt;td&gt;Padding, strides, transposed&lt;/td&gt;
    &lt;td&gt;Padding, strides, transposed (odd)&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-dilated-convolution-animations" class="anchor" aria-hidden="true" href="#dilated-convolution-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dilated convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/dilation.gif"&gt;&lt;img width="150px" src="gif/dilation.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, no stride, dilation&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-generating-the-makefile" class="anchor" aria-hidden="true" href="#generating-the-makefile"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating the Makefile&lt;/h2&gt;
&lt;p&gt;From the repository's root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ ./bin/generate_makefile&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-generating-the-animations" class="anchor" aria-hidden="true" href="#generating-the-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating the animations&lt;/h2&gt;
&lt;p&gt;From the repository's root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make all_animations&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The animations will be output to the &lt;code&gt;gif&lt;/code&gt; directory. Individual animation steps
will be output in PDF format to the &lt;code&gt;pdf&lt;/code&gt; directory and in PNG format to the
&lt;code&gt;png&lt;/code&gt; directory.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-compiling-the-document" class="anchor" aria-hidden="true" href="#compiling-the-document"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compiling the document&lt;/h2&gt;
&lt;p&gt;From the repository's root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>vdumoulin</author><guid isPermaLink="false">https://github.com/vdumoulin/conv_arithmetic</guid><pubDate>Fri, 07 Feb 2020 00:03:00 GMT</pubDate></item><item><title>soulmachine/leetcode #4 in TeX, Today</title><link>https://github.com/soulmachine/leetcode</link><description>&lt;p&gt;&lt;i&gt;LeetCode题解，151道题完整版&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-leetcode题解" class="anchor" aria-hidden="true" href="#leetcode题解"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;#LeetCode题解&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-在线阅读" class="anchor" aria-hidden="true" href="#在线阅读"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;在线阅读&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.gitbook.com/book/soulmachine/algorithm-essentials/" rel="nofollow"&gt;https://www.gitbook.com/book/soulmachine/algorithm-essentials/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;##PDF下载
&lt;a href="https://github.com/soulmachine/leetcode/raw/master/C%2B%2B/leetcode-cpp.pdf"&gt;LeetCode题解(C++版).pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;C++ 文件夹下是C++版，内容一模一样，代码是用C++写的。&lt;/p&gt;
&lt;p&gt;Java 文件夹下是Java版，目前正在编写中，由于拖延症，不知道猴年马月能完成。&lt;/p&gt;
&lt;p&gt;##LaTeX模板
本书使用的是陈硕开源的&lt;a href="https://github.com/chenshuo/typeset"&gt;模板&lt;/a&gt;。这个模板制作精良，很有taste，感谢陈硕 :)&lt;/p&gt;
&lt;p&gt;##在Windows下编译&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;安装Tex Live 2015 &lt;a href="http://www.tug.org/texlive/" rel="nofollow"&gt;http://www.tug.org/texlive/&lt;/a&gt;。把bin目录例如&lt;code&gt;D:\texlive\2015\bin\win32&lt;/code&gt;加入PATH环境变量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装字体。这个LaTex模板总共使用了10个字体，下载地址 &lt;a href="https://pan.baidu.com/s/1eRFJXnW" rel="nofollow"&gt;https://pan.baidu.com/s/1eRFJXnW&lt;/a&gt; ，有的字体Windows自带了，有的字体Ubuntu自带了，但都不全，还是一次性安装完所有字体比较方便。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装TeXstudio &lt;a href="http://texstudio.sourceforge.net/" rel="nofollow"&gt;http://texstudio.sourceforge.net/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(可选)启动Tex Live Manager，更新所有已安装的软件包。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置TeXstudio。&lt;/p&gt;
&lt;p&gt;启动Texstudio，选择 &lt;code&gt;Options--&amp;gt;Configure Texstudio--&amp;gt;Commands&lt;/code&gt;，XeLaTex 设置为 &lt;code&gt;xelatex -synctex=1 -interaction=nonstopmode %.tex&lt;/code&gt;；&lt;/p&gt;
&lt;p&gt;选择 &lt;code&gt;Options--&amp;gt;Configure Texstudio--&amp;gt;Build&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Build &amp;amp; View 由默认的 PDF Chain 改为 Compile &amp;amp; View；&lt;/p&gt;
&lt;p&gt;Default Compiler 由默认的PdfLaTex 修改为 XeLaTex ；&lt;/p&gt;
&lt;p&gt;PDF Viewer 改为 “Internal PDF Viewer(windowed)”，这样预览时会弹出一个独立的窗口，这样比较方便。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;编译。用TeXstudio打开&lt;code&gt;typeset.tex&lt;/code&gt;，点击界面上的绿色箭头就可以开始编译了。&lt;/p&gt;
&lt;p&gt;在下方的窗口可以看到TeXstudio正在使用的编译命令是&lt;code&gt;xelatex -synctex=1 -interaction=nonstopmode "typeset".tex&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;##在Ubuntu下编译&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;安装Tex Live 2015 &lt;a href="http://www.tug.org/texlive/" rel="nofollow"&gt;http://www.tug.org/texlive/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1.1. 下载TexLive 2015 的ISO 光盘，地址 &lt;a href="http://www.tug.org/texlive/acquire-iso.html" rel="nofollow"&gt;http://www.tug.org/texlive/acquire-iso.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1.2 mount 光盘，&lt;code&gt;sudo ./install-tl&lt;/code&gt; 开始安装&lt;/p&gt;
&lt;p&gt;1.3 加入环境变量&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; sudo vi /etc/profile
 export PATH=$PATH:/usr/local/texlive/2015/bin/x86_64-linux
 export MANPATH=$MANPATH:/usr/local/texlive/2015/texmf-dist/doc/man
 export INFPATH=$INFPATH:/usr/local/texlive/2015/texmf-dist/doc/info
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装字体。这个LaTex模板总共使用了10个字体，下载地址 &lt;a href="https://pan.baidu.com/s/1eRFJXnW" rel="nofollow"&gt;https://pan.baidu.com/s/1eRFJXnW&lt;/a&gt; ，有的字体Windows自带了，有的字体Ubuntu自带了，但都不全，还是一次性安装完所有字体比较方便。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装TeXstudio &lt;a href="http://texstudio.sourceforge.net/" rel="nofollow"&gt;http://texstudio.sourceforge.net/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置TeXstudio。&lt;/p&gt;
&lt;p&gt;启动Texstudio，选择 &lt;code&gt;Options--&amp;gt;Configure Texstudio--&amp;gt;Commands&lt;/code&gt;，XeLaTex 设置为 &lt;code&gt;xelatex -synctex=1 -interaction=nonstopmode %.tex&lt;/code&gt;；&lt;/p&gt;
&lt;p&gt;选择 &lt;code&gt;Options--&amp;gt;Configure Texstudio--&amp;gt;Build&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Build &amp;amp; View 由默认的 PDF Chain 改为 Compile &amp;amp; View；&lt;/p&gt;
&lt;p&gt;Default Compiler 由默认的PdfLaTex 修改为 XeLaTex ；&lt;/p&gt;
&lt;p&gt;PDF Viewer 改为 “Internal PDF Viewer(windowed)”，这样预览时会弹出一个独立的窗口，这样比较方便。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;编译。用TeXstudio打开&lt;code&gt;typeset.tex&lt;/code&gt;，点击界面上的绿色箭头就可以开始编译了。&lt;/p&gt;
&lt;p&gt;在下方的窗口可以看到TeXstudio正在使用的编译命令是&lt;code&gt;xelatex -synctex=1 -interaction=nonstopmode "typeset".tex&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;懒人版镜像。如果不想进行上面繁琐的安装过程，我做好了一个Ubuntu VMware虚拟机镜像，已经装好了 TexLive 2015, TexStudio和字体(详细的安装日志见压缩包注释)，开箱即用，下载地址 &lt;a href="http://pan.baidu.com/s/1cLWkgA" rel="nofollow"&gt;http://pan.baidu.com/s/1cLWkgA&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;##如何贡献代码
编译通过后，就具备了完整的LaTeX编译环境了。&lt;/p&gt;
&lt;p&gt;本书模板已经写好了，基本上不需要很多LaTeX知识就可以动手了。&lt;/p&gt;
&lt;p&gt;欢迎给本书添加内容或纠正错误，在自己本地编译成PDF，预览没问题后，就可以发pull request过来了。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-qq群" class="anchor" aria-hidden="true" href="#qq群"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;QQ群&lt;/h2&gt;
&lt;p&gt;237669375&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-小密圈" class="anchor" aria-hidden="true" href="#小密圈"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;小密圈&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/silicon-job.jpeg"&gt;&lt;img src="%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/silicon-job.jpeg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-algohub" class="anchor" aria-hidden="true" href="#algohub"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AlgoHub&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.algohub.org" rel="nofollow"&gt;https://www.algohub.org&lt;/a&gt; 是我建立的一个刷题网站，即将上线，敬请期待&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-纸质书" class="anchor" aria-hidden="true" href="#纸质书"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;纸质书&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;本书即将由电子工业出版社出版，敬请期待&lt;/strong&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>soulmachine</author><guid isPermaLink="false">https://github.com/soulmachine/leetcode</guid><pubDate>Fri, 07 Feb 2020 00:04:00 GMT</pubDate></item><item><title>Wandmalfarbe/pandoc-latex-template #5 in TeX, Today</title><link>https://github.com/Wandmalfarbe/pandoc-latex-template</link><description>&lt;p&gt;&lt;i&gt;A pandoc LaTeX template to convert markdown files to PDF or LaTeX.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="icon.png"&gt;&lt;img src="icon.png" align="right" height="110" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-eisvogel" class="anchor" aria-hidden="true" href="#eisvogel"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eisvogel&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/Wandmalfarbe/pandoc-latex-template" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b87755780c096b231f6f2e8a6192d30318d187a3/68747470733a2f2f7472617669732d63692e6f72672f57616e646d616c66617262652f70616e646f632d6c617465782d74656d706c6174652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/Wandmalfarbe/pandoc-latex-template.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A clean &lt;strong&gt;pandoc LaTeX template&lt;/strong&gt; to convert your markdown files to PDF or LaTeX. It is designed for lecture notes and exercises with a focus on computer science. The template is compatible with pandoc 2.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A custom title page&lt;/th&gt;
&lt;th align="center"&gt;A basic example page&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/custom-titlepage/custom-titlepage.pdf"&gt;&lt;img src="examples/custom-titlepage/custom-titlepage.png" alt="A custom title page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/basic-example/basic-example.pdf"&gt;&lt;img src="examples/basic-example/basic-example.png" alt="A basic example page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install pandoc from &lt;a href="http://pandoc.org/" rel="nofollow"&gt;http://pandoc.org/&lt;/a&gt;. You also need to install &lt;a href="https://en.wikibooks.org/wiki/LaTeX/Installation#Distributions" rel="nofollow"&gt;LaTeX&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download the latest version of the Eisvogel template from &lt;a href="https://github.com/Wandmalfarbe/pandoc-latex-template/releases/latest"&gt;the release page&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extract the downloaded ZIP archive and open the folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Move the template &lt;code&gt;eisvogel.tex&lt;/code&gt; to your pandoc templates folder and rename the file to &lt;code&gt;eisvogel.latex&lt;/code&gt;. The location of the templates folder depends on your operating system:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unix, Linux, macOS: &lt;code&gt;/Users/USERNAME/.local/share/pandoc/templates/&lt;/code&gt; or &lt;code&gt;/Users/USERNAME/.pandoc/templates/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Windows Vista or later: &lt;code&gt;C:\Users\USERNAME\AppData\Roaming\pandoc\templates\&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If there are no folders called &lt;code&gt;templates&lt;/code&gt; or &lt;code&gt;pandoc&lt;/code&gt; you need to create them and put the template &lt;code&gt;eisvogel.latex&lt;/code&gt; inside. You can find the default user data directory on your system by looking at the output of &lt;code&gt;pandoc --version&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Open the terminal and navigate to the folder where your markdown file is located.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Execute the following command&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --from markdown --template eisvogel --listings&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where &lt;code&gt;example.md&lt;/code&gt; is the markdown file you want to convert to PDF.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to have nice headers and footers you need to supply metadata to your document. You can do that with a &lt;a href="http://pandoc.org/MANUAL.html#extension-yaml_metadata_block" rel="nofollow"&gt;YAML metadata block&lt;/a&gt; at the top of your markdown document (see the &lt;a href="examples/basic-example/basic-example.md"&gt;example markdown file&lt;/a&gt;). Your markdown document may look like the following:&lt;/p&gt;
&lt;div class="highlight highlight-source-gfm"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;---&lt;/span&gt;
&lt;span class="pl-ent"&gt;title&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;The Document Title&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-ent"&gt;author&lt;/span&gt;: &lt;span class="pl-s"&gt;[Example Author, Another Author]&lt;/span&gt;
&lt;span class="pl-ent"&gt;date&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;2017-02-20&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-ent"&gt;keywords&lt;/span&gt;: &lt;span class="pl-s"&gt;[Markdown, Example]&lt;/span&gt;
&lt;span class="pl-c"&gt;...&lt;/span&gt;

Here is the actual document text...&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-custom-template-variables" class="anchor" aria-hidden="true" href="#custom-template-variables"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Template Variables&lt;/h3&gt;
&lt;p&gt;This template defines some new variables to control the appearance of the resulting PDF document. The existing template variables from pandoc are all supported and their documentation can be found in &lt;a href="https://pandoc.org/MANUAL.html#variables-for-latex" rel="nofollow"&gt;the pandoc manual&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;turns on the title page when &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-color&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the background color of the title page. The color value must be given as an HTML hex color like &lt;code&gt;D8DE2C&lt;/code&gt; without the leading number sign (&lt;code&gt;#&lt;/code&gt;). When specifying the color in YAML, it is advisable to enclose it in quotes like so &lt;code&gt;titlepage-color: "D8DE2C"&lt;/code&gt; to avoid the truncation of the color (e.g. &lt;code&gt;000000&lt;/code&gt; becoming &lt;code&gt;0&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-text-color&lt;/code&gt; (defaults to &lt;code&gt;5F5F5F&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the text color of the title page&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-rule-color&lt;/code&gt; (defaults to &lt;code&gt;435488&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the color of the rule on the top of the title page&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-rule-height&lt;/code&gt; (defaults to &lt;code&gt;4&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the height of the rule on the top of the title page (in points)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-background&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the path to a background image for the title page. The background image is scaled to cover the entire page. In the examples folder under &lt;code&gt;titlepage-background&lt;/code&gt; are a few example background images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;page-background&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the path to a background image for any page. The background image is scaled to cover the entire page. In the examples folder under &lt;code&gt;page-background&lt;/code&gt; are a few example background images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;page-background-opacity&lt;/code&gt; (defaults to &lt;code&gt;0.2&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the background image opacity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;caption-justification&lt;/code&gt; (defaults to &lt;code&gt;raggedright&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;justification setting for captions (uses the &lt;code&gt;justification&lt;/code&gt; parameter of the &lt;a href="https://ctan.org/pkg/caption?lang=en" rel="nofollow"&gt;caption&lt;/a&gt; package)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;toc-own-page&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;begin new page after table of contents, when &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;listings-disable-line-numbers&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;disables line numbers for all listings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;listings-no-page-break&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;avoid page break inside listings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;disable-header-and-footer&lt;/code&gt; (default to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;disables the header and footer completely on all pages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;header-left&lt;/code&gt; (defaults to the title)&lt;/p&gt;
&lt;p&gt;the text on the left side of the header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;header-center&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the text in the center of the header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;header-right&lt;/code&gt; (defaults to the date)&lt;/p&gt;
&lt;p&gt;the text on the right side of the header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footer-left&lt;/code&gt; (defaults to the author)&lt;/p&gt;
&lt;p&gt;the text on the left side of the footer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footer-center&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the text in the center of the footer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footer-right&lt;/code&gt; (defaults to the page number)&lt;/p&gt;
&lt;p&gt;the text on the right side of the footer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footnotes-pretty&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;prettifies formatting of footnotes (requires package &lt;code&gt;footmisc&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footnotes-disable-backlinks&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;disables making the reference from the footnote at the bottom of the page into a link back to the occurence of the footnote in the main text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;book&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;typeset as book&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;logo&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;path to an image that will be displayed on the title page. The path is always relative to where pandoc is executed. The option &lt;code&gt;--resource-path&lt;/code&gt; has no effect.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;logo-width&lt;/code&gt; (defaults to &lt;code&gt;100&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the width of the logo (in points)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;first-chapter&lt;/code&gt; (defaults to &lt;code&gt;1&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;if typesetting a book with chapter numbers, specifies the number that will be assigned to the first chapter&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;float-placement-figure&lt;/code&gt; (defaults to &lt;code&gt;H&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;Reset the default placement specifier for figure environments to the supplied value e.g. &lt;code&gt;htbp&lt;/code&gt;. The available specifiers are listed below. The first four placement specifiers can be combined.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;h&lt;/code&gt;: Place the float &lt;em&gt;here&lt;/em&gt;, i.e., approximately at the same point it occurs in the source text.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;t&lt;/code&gt;: Place the float at the &lt;em&gt;top&lt;/em&gt; of the page.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt;: Place the float at the &lt;em&gt;bottom&lt;/em&gt; of the page.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;p&lt;/code&gt;: Place the float on the next &lt;em&gt;page&lt;/em&gt; that will contain only floats like figures and tables.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;H&lt;/code&gt;: Place the float &lt;em&gt;HERE&lt;/em&gt; (exactly where it occurs in the source text). The &lt;code&gt;H&lt;/code&gt; specifier is provided by the &lt;a href="https://ctan.org/pkg/float" rel="nofollow"&gt;float package&lt;/a&gt; and may not be used in conjunction with any other placement specifiers.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;table-use-row-colors&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;enables row colors for tables. The default value is &lt;code&gt;false&lt;/code&gt; because the coloring extends beyond the edge of the table and there is currently no way to change that.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;code-block-font-size&lt;/code&gt; (defaults to &lt;code&gt;\small&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;LaTeX command to change the font size for code blocks. The available values are &lt;code&gt;\tiny&lt;/code&gt;, &lt;code&gt;\scriptsize&lt;/code&gt;, &lt;code&gt;\footnotesize&lt;/code&gt;, &lt;code&gt;\small&lt;/code&gt;, &lt;code&gt;\normalsize&lt;/code&gt;, &lt;code&gt;\large&lt;/code&gt;, &lt;code&gt;\Large&lt;/code&gt;, &lt;code&gt;\LARGE&lt;/code&gt;, &lt;code&gt;\huge&lt;/code&gt; and &lt;code&gt;\Huge&lt;/code&gt;. This option will change the font size for default code blocks using the verbatim environment and for code blocks generated with listings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-required-latex-packages" class="anchor" aria-hidden="true" href="#required-latex-packages"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Required LaTeX Packages&lt;/h2&gt;
&lt;p&gt;LaTeX manages addons and additional functionality in so called packages. You
might get the following error when compiling a document with the Eisvogel
template:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;!&lt;/span&gt; LaTeX Error: File &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;`&lt;/span&gt;footnotebackref.sty&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt; not found.&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;Type X to quit or &amp;lt;RETURN&amp;gt; to proceed,&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;or enter new name. (Default extension: sty)&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;Enter file name:&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;! Emergency stop.&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-s"&gt;&amp;lt;read *&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;LaTeX informs you that the additional package &lt;code&gt;footnotebackref&lt;/code&gt; is required to
render the document.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-texlive" class="anchor" aria-hidden="true" href="#texlive"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Texlive&lt;/h3&gt;
&lt;p&gt;Eisvogel requires a full texlive distribution that can be installed by running
&lt;code&gt;apt-get install texlive-full&lt;/code&gt; in the terminal. Because &lt;code&gt;texlive-full&lt;/code&gt; is very
large (about 5 Gigabytes) you can also install the smaller texlive bundles and
add any missing packages manually.&lt;/p&gt;
&lt;p&gt;A smaller texlive bundle is &lt;code&gt;texlive-latex-extra&lt;/code&gt;. With &lt;code&gt;texlive-latex-extra&lt;/code&gt;
you also need to install these packages manually:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;adjustbox babel-german background bidi collectbox csquotes everypage filehook
footmisc footnotebackref framed fvextra letltxmacro ly1 mdframed mweights
needspace pagecolor sourcecodepro sourcesanspro titling ucharcat ulem
unicode-math upquote xecjk xurl zref
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install them with the following command:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;tlmgr install adjustbox babel-german background bidi collectbox csquotes everypage filehook footmisc footnotebackref framed fvextra letltxmacro ly1 mdframed mweights needspace pagecolor sourcecodepro sourcesanspro titling ucharcat ulem unicode-math upquote xecjk xurl zref&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Additional information about the different texlive packages can be found at
this TeX-StackExchange answer: &lt;a href="https://tex.stackexchange.com/a/504566" rel="nofollow"&gt;https://tex.stackexchange.com/a/504566&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-miktex" class="anchor" aria-hidden="true" href="#miktex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MiKTeX&lt;/h3&gt;
&lt;p&gt;If you don't want to install all missing packages manually, &lt;a href="https://miktex.org/howto/miktex-console" rel="nofollow"&gt;MiKTeX might be
an alternative&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;MiKTeX has the ability to automatically install missing packages.
You can turn this feature on or off. And you can let MiKTeX ask you each time a package has to be installed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Click &lt;code&gt;Settings&lt;/code&gt; to navigate to the settings page.&lt;/li&gt;
&lt;li&gt;Click the &lt;code&gt;General&lt;/code&gt; tab.&lt;/li&gt;
&lt;li&gt;Click one of the radio buttons:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Ask me&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Always install missing packages on-the-fly&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Never install missing packages on-the-fly&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-numbered-sections" class="anchor" aria-hidden="true" href="#numbered-sections"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Numbered Sections&lt;/h3&gt;
&lt;p&gt;For PDFs with &lt;a href="http://pandoc.org/MANUAL.html#options-affecting-specific-writers" rel="nofollow"&gt;numbered sections&lt;/a&gt; use the &lt;code&gt;--number-sections&lt;/code&gt; or &lt;code&gt;-N&lt;/code&gt; option.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --number-sections&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-syntax-highlighting-with-listings" class="anchor" aria-hidden="true" href="#syntax-highlighting-with-listings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syntax Highlighting with Listings&lt;/h3&gt;
&lt;p&gt;You can get syntax highlighting of delimited code blocks by using the LaTeX package listings with the option &lt;code&gt;--listings&lt;/code&gt;. This example will produce the same syntax highlighting as in the example PDF.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --listings&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-syntax-highlighting-without-listings" class="anchor" aria-hidden="true" href="#syntax-highlighting-without-listings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syntax Highlighting Without Listings&lt;/h3&gt;
&lt;p&gt;The following examples show &lt;a href="http://pandoc.org/MANUAL.html#syntax-highlighting" rel="nofollow"&gt;syntax highlighting of delimited code blocks&lt;/a&gt; without using listings. To see a list of all the supported highlight styles, type &lt;code&gt;pandoc --list-highlight-styles&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style pygments&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style kate&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style espresso&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style tango&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-standalone-latex-document" class="anchor" aria-hidden="true" href="#standalone-latex-document"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Standalone LaTeX Document&lt;/h3&gt;
&lt;p&gt;To produce a standalone LaTeX document for compiling with any LaTeX editor use &lt;code&gt;.tex&lt;/code&gt; as an output file extension.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.tex --template eisvogel&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-changing-the-document-language" class="anchor" aria-hidden="true" href="#changing-the-document-language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changing the Document Language&lt;/h3&gt;
&lt;p&gt;The default language of this template is American English. The &lt;code&gt;lang&lt;/code&gt; variable identifies the main language of the document, using a code according to &lt;a href="https://tools.ietf.org/html/bcp47" rel="nofollow"&gt;BCP 47&lt;/a&gt; (e.g. &lt;code&gt;en&lt;/code&gt; or &lt;code&gt;en-GB&lt;/code&gt;). For an incomplete list of the supported language codes see &lt;a href="http://mirrors.ctan.org/language/hyph-utf8/doc/generic/hyph-utf8/hyph-utf8.pdf" rel="nofollow"&gt;the documentation for the hyph-utf8 package (Section 2)&lt;/a&gt;. The following example changes the language to British English:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel -V lang=en-GB&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The following example changes the language to German:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel -V lang=de&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-typesetting-a-book" class="anchor" aria-hidden="true" href="#typesetting-a-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Typesetting a Book&lt;/h3&gt;
&lt;p&gt;To typeset a book supply the template variable &lt;code&gt;-V book&lt;/code&gt; from the command line or via &lt;code&gt;book: true&lt;/code&gt; in the metadata.&lt;/p&gt;
&lt;p&gt;To get the correct chapter headings you need to tell pandoc that it should convert first level headings (indicated by one &lt;code&gt;#&lt;/code&gt; in markdown) to chapters with the command line option &lt;code&gt;--top-level-division=chapter&lt;/code&gt;. Chapter numbers start at 1. If you need to change that, specify &lt;code&gt;first-chapter&lt;/code&gt; in the template variables.&lt;/p&gt;
&lt;p&gt;There will be one blank page before each chapter because the template is two-sided per default. So if you plan to publish your book as a PDF and don’t need a blank page you should add the class option &lt;code&gt;onesided&lt;/code&gt; which can be done by supplying a template variable &lt;code&gt;-V classoption=oneside&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-example-images" class="anchor" aria-hidden="true" href="#example-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example Images&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A green title page&lt;/th&gt;
&lt;th align="center"&gt;A background image on the title page&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/green-titlepage/green-titlepage.pdf"&gt;&lt;img src="examples/green-titlepage/green-titlepage.png" alt="A green title page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/titlepage-background/titlepage-background.pdf"&gt;&lt;img src="examples/titlepage-background/titlepage-background.png" alt="A background image on the title page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;images and tables&lt;/th&gt;
&lt;th align="center"&gt;Code blocks styled without listings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/images-and-tables/images-and-tables.pdf"&gt;&lt;img src="examples/images-and-tables/images-and-tables.png" alt="images and tables" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/without-listings/without-listings.pdf"&gt;&lt;img src="examples/without-listings/without-listings.png" alt="Code blocks styled without listings" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A book&lt;/th&gt;
&lt;th align="center"&gt;Code blocks styled with listings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/book/book.pdf"&gt;&lt;img src="examples/book/book.png" alt="A book" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/listings/listings.pdf"&gt;&lt;img src="examples/listings/listings.png" alt="Code blocks styled with listings" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A background images on all pages&lt;/th&gt;
&lt;th align="center"&gt;CJK Support (when using XeLaTeX)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/page-background/page-background.pdf"&gt;&lt;img src="examples/page-background/page-background.png" alt="A background images on all pages" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/japanese/japanese.pdf"&gt;&lt;img src="examples/japanese/japanese.png" alt="CJK Support (when using XeLaTeX)" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-credits" class="anchor" aria-hidden="true" href="#credits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credits&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This template includes code for styling block quotations from &lt;a href="https://github.com/aaronwolen/pandoc-letter"&gt;pandoc-letter&lt;/a&gt; by &lt;a href="https://github.com/aaronwolen"&gt;Aaron Wolen&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This project is open source licensed under the BSD 3-Clause License. Please see the &lt;a href="LICENSE"&gt;LICENSE file&lt;/a&gt; for more information.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Wandmalfarbe</author><guid isPermaLink="false">https://github.com/Wandmalfarbe/pandoc-latex-template</guid><pubDate>Fri, 07 Feb 2020 00:05:00 GMT</pubDate></item><item><title>pingcap/docs-cn #6 in TeX, Today</title><link>https://github.com/pingcap/docs-cn</link><description>&lt;p&gt;&lt;i&gt;TiDB/TiKV/PD documents in Chinese.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;table data-table-type="yaml-metadata"&gt;
  &lt;thead&gt;
  &lt;tr&gt;
  &lt;th&gt;draft&lt;/th&gt;
  &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
  &lt;tr&gt;
  &lt;td&gt;&lt;div&gt;true&lt;/div&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1&gt;&lt;a id="user-content-tidb-简介" class="anchor" aria-hidden="true" href="#tidb-简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TiDB 简介&lt;/h1&gt;
&lt;p&gt;TiDB 是 PingCAP 公司设计的开源分布式 HTAP (Hybrid Transactional and Analytical Processing) 数据库，结合了传统的 RDBMS 和 NoSQL 的最佳特性。TiDB 兼容 MySQL，支持无限的水平扩展，具备强一致性和高可用性。TiDB 的目标是为 OLTP (Online Transactional Processing) 和 OLAP (Online Analytical Processing) 场景提供一站式的解决方案。&lt;/p&gt;
&lt;p&gt;TiDB 具备如下特性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;高度兼容 MySQL&lt;/p&gt;
&lt;p&gt;&lt;a href="/v3.0/reference/mysql-compatibility.md"&gt;大多数情况下&lt;/a&gt;，无需修改代码即可从 MySQL 轻松迁移至 TiDB，分库分表后的 MySQL 集群亦可通过 TiDB 工具进行实时迁移。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;水平弹性扩展&lt;/p&gt;
&lt;p&gt;通过简单地增加新节点即可实现 TiDB 的水平扩展，按需扩展吞吐或存储，轻松应对高并发、海量数据场景。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分布式事务&lt;/p&gt;
&lt;p&gt;TiDB 100% 支持标准的 ACID 事务。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;真正金融级高可用&lt;/p&gt;
&lt;p&gt;相比于传统主从 (M-S) 复制方案，基于 Raft 的多数派选举协议可以提供金融级的 100% 数据强一致性保证，且在不丢失大多数副本的前提下，可以实现故障的自动恢复 (auto-failover)，无需人工介入。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一站式 HTAP 解决方案&lt;/p&gt;
&lt;p&gt;TiDB 作为典型的 OLTP 行存数据库，同时兼具强大的 OLAP 性能，配合 TiSpark，可提供一站式 HTAP 解决方案，一份存储同时处理 OLTP &amp;amp; OLAP，无需传统繁琐的 ETL 过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;云原生 SQL 数据库&lt;/p&gt;
&lt;p&gt;TiDB 是为云而设计的数据库，支持公有云、私有云和混合云，使部署、配置和维护变得十分简单。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TiDB 的设计目标是 100% 的 OLTP 场景和 80% 的 OLAP 场景，更复杂的 OLAP 分析可以通过 &lt;a href="/v3.0/reference/tispark.md"&gt;TiSpark 项目&lt;/a&gt;来完成。&lt;/p&gt;
&lt;p&gt;TiDB 对业务没有任何侵入性，能优雅的替换传统的数据库中间件、数据库分库分表等 Sharding 方案。同时它也让开发运维人员不用关注数据库 Scale 的细节问题，专注于业务开发，极大的提升研发的生产力。&lt;/p&gt;
&lt;p&gt;三篇文章了解 TiDB 技术内幕：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-1/" rel="nofollow"&gt;说存储&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-2/" rel="nofollow"&gt;说计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-3/" rel="nofollow"&gt;谈调度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pingcap</author><guid isPermaLink="false">https://github.com/pingcap/docs-cn</guid><pubDate>Fri, 07 Feb 2020 00:06:00 GMT</pubDate></item><item><title>terryum/awesome-deep-learning-papers #7 in TeX, Today</title><link>https://github.com/terryum/awesome-deep-learning-papers</link><description>&lt;p&gt;&lt;i&gt;The most cited deep learning papers&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-awesome---most-cited-deep-learning-papers" class="anchor" aria-hidden="true" href="#awesome---most-cited-deep-learning-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome - Most Cited Deep Learning Papers&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://camo.githubusercontent.com/13c4e50d88df7178ae1882a203ed57b641674f94/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667" alt="Awesome" data-canonical-src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[Notice] This list is not being maintained anymore because of the overwhelming amount of deep learning papers published every day since 2017.&lt;/p&gt;
&lt;p&gt;A curated list of the most cited deep learning papers (2012-2016)&lt;/p&gt;
&lt;p&gt;We believe that there exist &lt;em&gt;classic&lt;/em&gt; deep learning papers which are worth reading regardless of their application domain. Rather than providing overwhelming amount of papers, We would like to provide a &lt;em&gt;curated list&lt;/em&gt; of the awesome deep learning papers which are considered as &lt;em&gt;must-reads&lt;/em&gt; in certain research domains.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-background" class="anchor" aria-hidden="true" href="#background"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Background&lt;/h2&gt;
&lt;p&gt;Before this list, there exist other &lt;em&gt;awesome deep learning lists&lt;/em&gt;, for example, &lt;a href="https://github.com/kjw0612/awesome-deep-vision"&gt;Deep Vision&lt;/a&gt; and &lt;a href="https://github.com/kjw0612/awesome-rnn"&gt;Awesome Recurrent Neural Networks&lt;/a&gt;. Also, after this list comes out, another awesome list for deep learning beginners, called &lt;a href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap"&gt;Deep Learning Papers Reading Roadmap&lt;/a&gt;, has been created and loved by many deep learning researchers.&lt;/p&gt;
&lt;p&gt;Although the &lt;em&gt;Roadmap List&lt;/em&gt; includes lots of important deep learning papers, it feels overwhelming for me to read them all. As I mentioned in the introduction, I believe that seminal works can give us lessons regardless of their application domain. Thus, I would like to introduce &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; here as a good starting point of overviewing deep learning researches.&lt;/p&gt;
&lt;p&gt;To get the news for newly released papers everyday, follow my &lt;a href="https://twitter.com/TerryUm_ML" rel="nofollow"&gt;twitter&lt;/a&gt; or &lt;a href="https://www.facebook.com/terryum.io/" rel="nofollow"&gt;facebook page&lt;/a&gt;!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-awesome-list-criteria" class="anchor" aria-hidden="true" href="#awesome-list-criteria"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome list criteria&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A list of &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; published from 2012 to 2016 is suggested.&lt;/li&gt;
&lt;li&gt;If a paper is added to the list, another paper (usually from *More Papers from 2016" section) should be removed to keep top 100 papers. (Thus, removing papers is also important contributions as well as adding papers)&lt;/li&gt;
&lt;li&gt;Papers that are important, but failed to be included in the list, will be listed in &lt;em&gt;More than Top 100&lt;/em&gt; section.&lt;/li&gt;
&lt;li&gt;Please refer to &lt;em&gt;New Papers&lt;/em&gt; and &lt;em&gt;Old Papers&lt;/em&gt; sections for the papers published in recent 6 months or before 2012.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;(Citation criteria)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;lt; 6 months&lt;/strong&gt; : &lt;em&gt;New Papers&lt;/em&gt; (by discussion)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2016&lt;/strong&gt; :  +60 citations or "More Papers from 2016"&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2015&lt;/strong&gt; :  +200 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2014&lt;/strong&gt; :  +400 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2013&lt;/strong&gt; :  +600 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2012&lt;/strong&gt; :  +800 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;~2012&lt;/strong&gt; : &lt;em&gt;Old Papers&lt;/em&gt; (by discussion)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please note that we prefer seminal deep learning papers that can be applied to various researches rather than application papers. For that reason, some papers that meet the criteria may not be accepted while others can be. It depends on the impact of the paper, applicability to other researches scarcity of the research domain, and so on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We need your contributions!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you have any suggestions (missing papers, new papers, key researchers or typos), please feel free to edit and pull a request.
(Please read the &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md"&gt;contributing guide&lt;/a&gt; for further instructions, though just letting me know the title of papers can also be a big contribution to us.)&lt;/p&gt;
&lt;p&gt;(Update) You can download all top-100 papers with &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/fetch_papers.py"&gt;this&lt;/a&gt; and collect all authors' names with &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/get_authors.py"&gt;this&lt;/a&gt;. Also, &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/top100papers.bib"&gt;bib file&lt;/a&gt; for all top-100 papers are available. Thanks, doodhwala, &lt;a href="https://github.com/sunshinemyson"&gt;Sven&lt;/a&gt; and &lt;a href="https://github.com/grepinsight"&gt;grepinsight&lt;/a&gt;!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can anyone contribute the code for obtaining the statistics of the authors of Top-100 papers?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#understanding--generalization--transfer"&gt;Understanding / Generalization / Transfer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimization--training-techniques"&gt;Optimization / Training Techniques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unsupervised--generative-models"&gt;Unsupervised / Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#convolutional-neural-network-models"&gt;Convolutional Network Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image-segmentation--object-detection"&gt;Image Segmentation / Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image--video--etc"&gt;Image / Video / Etc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#natural-language-processing--rnns"&gt;Natural Language Processing / RNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#speech--other-domain"&gt;Speech / Other Domain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement-learning--robotics"&gt;Reinforcement Learning / Robotics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#more-papers-from-2016"&gt;More Papers from 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(More than Top 100)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#new-papers"&gt;New Papers&lt;/a&gt; : Less than 6 months&lt;/li&gt;
&lt;li&gt;&lt;a href="#old-papers"&gt;Old Papers&lt;/a&gt; : Before 2012&lt;/li&gt;
&lt;li&gt;&lt;a href="#hw--sw--dataset"&gt;HW / SW / Dataset&lt;/a&gt; : Technical reports&lt;/li&gt;
&lt;li&gt;&lt;a href="#book--survey--review"&gt;Book / Survey / Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#video-lectures--tutorials--blogs"&gt;Video Lectures / Tutorials / Blogs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#appendix-more-than-top-100"&gt;Appendix: More than Top 100&lt;/a&gt; : More papers not in the list&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-understanding--generalization--transfer" class="anchor" aria-hidden="true" href="#understanding--generalization--transfer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Understanding / Generalization / Transfer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Distilling the knowledge in a neural network&lt;/strong&gt; (2015), G. Hinton et al. &lt;a href="http://arxiv.org/pdf/1503.02531" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep neural networks are easily fooled: High confidence predictions for unrecognizable images&lt;/strong&gt; (2015), A. Nguyen et al. &lt;a href="http://arxiv.org/pdf/1412.1897" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How transferable are features in deep neural networks?&lt;/strong&gt; (2014), J. Yosinski et al. &lt;a href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CNN features off-the-Shelf: An astounding baseline for recognition&lt;/strong&gt; (2014), A. Razavian et al. &lt;a href="http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning and transferring mid-Level image representations using convolutional neural networks&lt;/strong&gt; (2014), M. Oquab et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Visualizing and understanding convolutional networks&lt;/strong&gt; (2014), M. Zeiler and R. Fergus &lt;a href="http://arxiv.org/pdf/1311.2901" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decaf: A deep convolutional activation feature for generic visual recognition&lt;/strong&gt; (2014), J. Donahue et al. &lt;a href="http://arxiv.org/pdf/1310.1531" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-optimization--training-techniques" class="anchor" aria-hidden="true" href="#optimization--training-techniques"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimization / Training Techniques&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Training very deep networks&lt;/strong&gt; (2015), R. Srivastava et al. &lt;a href="http://papers.nips.cc/paper/5850-training-very-deep-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Batch normalization: Accelerating deep network training by reducing internal covariate shift&lt;/strong&gt; (2015), S. Loffe and C. Szegedy &lt;a href="http://arxiv.org/pdf/1502.03167" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification&lt;/strong&gt; (2015), K. He et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dropout: A simple way to prevent neural networks from overfitting&lt;/strong&gt; (2014), N. Srivastava et al. &lt;a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adam: A method for stochastic optimization&lt;/strong&gt; (2014), D. Kingma and J. Ba &lt;a href="http://arxiv.org/pdf/1412.6980" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href="http://arxiv.org/pdf/1207.0580.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random search for hyper-parameter optimization&lt;/strong&gt; (2012) J. Bergstra and Y. Bengio &lt;a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-unsupervised--generative-models" class="anchor" aria-hidden="true" href="#unsupervised--generative-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised / Generative Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pixel recurrent neural networks&lt;/strong&gt; (2016), A. Oord et al. &lt;a href="http://arxiv.org/pdf/1601.06759v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improved techniques for training GANs&lt;/strong&gt; (2016), T. Salimans et al. &lt;a href="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unsupervised representation learning with deep convolutional generative adversarial networks&lt;/strong&gt; (2015), A. Radford et al. &lt;a href="https://arxiv.org/pdf/1511.06434v2" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DRAW: A recurrent neural network for image generation&lt;/strong&gt; (2015), K. Gregor et al. &lt;a href="http://arxiv.org/pdf/1502.04623" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generative adversarial nets&lt;/strong&gt; (2014), I. Goodfellow et al. &lt;a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Auto-encoding variational Bayes&lt;/strong&gt; (2013), D. Kingma and M. Welling &lt;a href="http://arxiv.org/pdf/1312.6114" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Building high-level features using large scale unsupervised learning&lt;/strong&gt; (2013), Q. Le et al. &lt;a href="http://arxiv.org/pdf/1112.6209" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-convolutional-neural-network-models" class="anchor" aria-hidden="true" href="#convolutional-neural-network-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Network Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Rethinking the inception architecture for computer vision&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inception-v4, inception-resnet and the impact of residual connections on learning&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href="http://arxiv.org/pdf/1602.07261" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Identity Mappings in Deep Residual Networks&lt;/strong&gt; (2016), K. He et al. &lt;a href="https://arxiv.org/pdf/1603.05027v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep residual learning for image recognition&lt;/strong&gt; (2016), K. He et al. &lt;a href="http://arxiv.org/pdf/1512.03385" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spatial transformer network&lt;/strong&gt; (2015), M. Jaderberg et al., &lt;a href="http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Going deeper with convolutions&lt;/strong&gt; (2015), C. Szegedy et al.  &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Very deep convolutional networks for large-scale image recognition&lt;/strong&gt; (2014), K. Simonyan and A. Zisserman &lt;a href="http://arxiv.org/pdf/1409.1556" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Return of the devil in the details: delving deep into convolutional nets&lt;/strong&gt; (2014), K. Chatfield et al. &lt;a href="http://arxiv.org/pdf/1405.3531" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OverFeat: Integrated recognition, localization and detection using convolutional networks&lt;/strong&gt; (2013), P. Sermanet et al. &lt;a href="http://arxiv.org/pdf/1312.6229" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maxout networks&lt;/strong&gt; (2013), I. Goodfellow et al. &lt;a href="http://arxiv.org/pdf/1302.4389v4" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Network in network&lt;/strong&gt; (2013), M. Lin et al. &lt;a href="http://arxiv.org/pdf/1312.4400" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ImageNet classification with deep convolutional neural networks&lt;/strong&gt; (2012), A. Krizhevsky et al. &lt;a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-image-segmentation--object-detection" class="anchor" aria-hidden="true" href="#image-segmentation--object-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image: Segmentation / Object Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You only look once: Unified, real-time object detection&lt;/strong&gt; (2016), J. Redmon et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fully convolutional networks for semantic segmentation&lt;/strong&gt; (2015), J. Long et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/strong&gt; (2015), S. Ren et al. &lt;a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fast R-CNN&lt;/strong&gt; (2015), R. Girshick &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/strong&gt; (2014), R. Girshick et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spatial pyramid pooling in deep convolutional networks for visual recognition&lt;/strong&gt; (2014), K. He et al. &lt;a href="http://arxiv.org/pdf/1406.4729" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Semantic image segmentation with deep convolutional nets and fully connected CRFs&lt;/strong&gt;, L. Chen et al. &lt;a href="https://arxiv.org/pdf/1412.7062" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning hierarchical features for scene labeling&lt;/strong&gt; (2013), C. Farabet et al. &lt;a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-image--video--etc" class="anchor" aria-hidden="true" href="#image--video--etc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image / Video / Etc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/strong&gt; (2016), C. Dong et al. &lt;a href="https://arxiv.org/pdf/1501.00092v3.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A neural algorithm of artistic style&lt;/strong&gt; (2015), L. Gatys et al. &lt;a href="https://arxiv.org/pdf/1508.06576" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep visual-semantic alignments for generating image descriptions&lt;/strong&gt; (2015), A. Karpathy and L. Fei-Fei &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Show, attend and tell: Neural image caption generation with visual attention&lt;/strong&gt; (2015), K. Xu et al. &lt;a href="http://arxiv.org/pdf/1502.03044" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Show and tell: A neural image caption generator&lt;/strong&gt; (2015), O. Vinyals et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Long-term recurrent convolutional networks for visual recognition and description&lt;/strong&gt; (2015), J. Donahue et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VQA: Visual question answering&lt;/strong&gt; (2015), S. Antol et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DeepFace: Closing the gap to human-level performance in face verification&lt;/strong&gt; (2014), Y. Taigman et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Large-scale video classification with convolutional neural networks&lt;/strong&gt; (2014), A. Karpathy et al. &lt;a href="http://vision.stanford.edu/pdf/karpathy14.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Two-stream convolutional networks for action recognition in videos&lt;/strong&gt; (2014), K. Simonyan et al. &lt;a href="http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3D convolutional neural networks for human action recognition&lt;/strong&gt; (2013), S. Ji et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;&lt;a id="user-content-natural-language-processing--rnns" class="anchor" aria-hidden="true" href="#natural-language-processing--rnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natural Language Processing / RNNs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Neural Architectures for Named Entity Recognition&lt;/strong&gt; (2016), G. Lample et al. &lt;a href="http://aclweb.org/anthology/N/N16/N16-1030.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exploring the limits of language modeling&lt;/strong&gt; (2016), R. Jozefowicz et al. &lt;a href="http://arxiv.org/pdf/1602.02410" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Teaching machines to read and comprehend&lt;/strong&gt; (2015), K. Hermann et al. &lt;a href="http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Effective approaches to attention-based neural machine translation&lt;/strong&gt; (2015), M. Luong et al. &lt;a href="https://arxiv.org/pdf/1508.04025" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conditional random fields as recurrent neural networks&lt;/strong&gt; (2015), S. Zheng and S. Jayasumana. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory networks&lt;/strong&gt; (2014), J. Weston et al. &lt;a href="https://arxiv.org/pdf/1410.3916" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural turing machines&lt;/strong&gt; (2014), A. Graves et al. &lt;a href="https://arxiv.org/pdf/1410.5401" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural machine translation by jointly learning to align and translate&lt;/strong&gt; (2014), D. Bahdanau et al. &lt;a href="http://arxiv.org/pdf/1409.0473" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sequence to sequence learning with neural networks&lt;/strong&gt; (2014), I. Sutskever et al. &lt;a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning phrase representations using RNN encoder-decoder for statistical machine translation&lt;/strong&gt; (2014), K. Cho et al. &lt;a href="http://arxiv.org/pdf/1406.1078" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A convolutional neural network for modeling sentences&lt;/strong&gt; (2014), N. Kalchbrenner et al. &lt;a href="http://arxiv.org/pdf/1404.2188v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convolutional neural networks for sentence classification&lt;/strong&gt; (2014), Y. Kim &lt;a href="http://arxiv.org/pdf/1408.5882" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Glove: Global vectors for word representation&lt;/strong&gt; (2014), J. Pennington et al. &lt;a href="http://anthology.aclweb.org/D/D14/D14-1162.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed representations of sentences and documents&lt;/strong&gt; (2014), Q. Le and T. Mikolov &lt;a href="http://arxiv.org/pdf/1405.4053" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed representations of words and phrases and their compositionality&lt;/strong&gt; (2013), T. Mikolov et al. &lt;a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficient estimation of word representations in vector space&lt;/strong&gt; (2013), T. Mikolov et al.  &lt;a href="http://arxiv.org/pdf/1301.3781" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recursive deep models for semantic compositionality over a sentiment treebank&lt;/strong&gt; (2013), R. Socher et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generating sequences with recurrent neural networks&lt;/strong&gt; (2013), A. Graves. &lt;a href="https://arxiv.org/pdf/1308.0850" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-speech--other-domain" class="anchor" aria-hidden="true" href="#speech--other-domain"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speech / Other Domain&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end attention-based large vocabulary speech recognition&lt;/strong&gt; (2016), D. Bahdanau et al. &lt;a href="https://arxiv.org/pdf/1508.04395" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep speech 2: End-to-end speech recognition in English and Mandarin&lt;/strong&gt; (2015), D. Amodei et al. &lt;a href="https://arxiv.org/pdf/1512.02595" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speech recognition with deep recurrent neural networks&lt;/strong&gt; (2013), A. Graves &lt;a href="http://arxiv.org/pdf/1303.5778.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href="http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition&lt;/strong&gt; (2012) G. Dahl et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Acoustic modeling using deep belief networks&lt;/strong&gt; (2012), A. Mohamed et al. &lt;a href="http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-reinforcement-learning--robotics" class="anchor" aria-hidden="true" href="#reinforcement-learning--robotics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reinforcement Learning / Robotics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end training of deep visuomotor policies&lt;/strong&gt; (2016), S. Levine et al. &lt;a href="http://www.jmlr.org/papers/volume17/15-522/source/15-522.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection&lt;/strong&gt; (2016), S. Levine et al. &lt;a href="https://arxiv.org/pdf/1603.02199" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Asynchronous methods for deep reinforcement learning&lt;/strong&gt; (2016), V. Mnih et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep Reinforcement Learning with Double Q-Learning&lt;/strong&gt; (2016), H. Hasselt et al. &lt;a href="https://arxiv.org/pdf/1509.06461.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mastering the game of Go with deep neural networks and tree search&lt;/strong&gt; (2016), D. Silver et al. &lt;a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Continuous control with deep reinforcement learning&lt;/strong&gt; (2015), T. Lillicrap et al. &lt;a href="https://arxiv.org/pdf/1509.02971" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Human-level control through deep reinforcement learning&lt;/strong&gt; (2015), V. Mnih et al. &lt;a href="http://www.davidqiu.com:8888/research/nature14236.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep learning for detecting robotic grasps&lt;/strong&gt; (2015), I. Lenz et al. &lt;a href="http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Playing atari with deep reinforcement learning&lt;/strong&gt; (2013), V. Mnih et al. &lt;a href="http://arxiv.org/pdf/1312.5602.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-more-papers-from-2016" class="anchor" aria-hidden="true" href="#more-papers-from-2016"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More Papers from 2016&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Layer Normalization&lt;/strong&gt; (2016), J. Ba et al. &lt;a href="https://arxiv.org/pdf/1607.06450v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning to learn by gradient descent by gradient descent&lt;/strong&gt; (2016), M. Andrychowicz et al. &lt;a href="http://arxiv.org/pdf/1606.04474v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Domain-adversarial training of neural networks&lt;/strong&gt; (2016), Y. Ganin et al. &lt;a href="http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WaveNet: A Generative Model for Raw Audio&lt;/strong&gt; (2016), A. Oord et al. &lt;a href="https://arxiv.org/pdf/1609.03499v2" rel="nofollow"&gt;[pdf]&lt;/a&gt; &lt;a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Colorful image colorization&lt;/strong&gt; (2016), R. Zhang et al. &lt;a href="https://arxiv.org/pdf/1603.08511" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generative visual manipulation on the natural image manifold&lt;/strong&gt; (2016), J. Zhu et al. &lt;a href="https://arxiv.org/pdf/1609.03552" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Texture networks: Feed-forward synthesis of textures and stylized images&lt;/strong&gt; (2016), D Ulyanov et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/ulyanov16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSD: Single shot multibox detector&lt;/strong&gt; (2016), W. Liu et al. &lt;a href="https://arxiv.org/pdf/1512.02325" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&amp;lt; 1MB model size&lt;/strong&gt; (2016), F. Iandola et al. &lt;a href="http://arxiv.org/pdf/1602.07360" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eie: Efficient inference engine on compressed deep neural network&lt;/strong&gt; (2016), S. Han et al. &lt;a href="http://arxiv.org/pdf/1602.01528" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1&lt;/strong&gt; (2016), M. Courbariaux et al. &lt;a href="https://arxiv.org/pdf/1602.02830" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic memory networks for visual and textual question answering&lt;/strong&gt; (2016), C. Xiong et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stacked attention networks for image question answering&lt;/strong&gt; (2016), Z. Yang et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid computing using a neural network with dynamic external memory&lt;/strong&gt; (2016), A. Graves et al. &lt;a href="https://www.gwern.net/docs/2016-graves.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google's neural machine translation system: Bridging the gap between human and machine translation&lt;/strong&gt; (2016), Y. Wu et al. &lt;a href="https://arxiv.org/pdf/1609.08144" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-new-papers" class="anchor" aria-hidden="true" href="#new-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New papers&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Newly published papers (&amp;lt; 6 months) which are worth reading&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (2017), Andrew G. Howard et al. &lt;a href="https://arxiv.org/pdf/1704.04861.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convolutional Sequence to Sequence Learning (2017), Jonas Gehring et al. &lt;a href="https://arxiv.org/pdf/1705.03122" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A Knowledge-Grounded Neural Conversation Model (2017), Marjan Ghazvininejad et al. &lt;a href="https://arxiv.org/pdf/1702.01932" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour (2017), Priya Goyal et al. &lt;a href="https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h3.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TACOTRON: Towards end-to-end speech synthesis (2017), Y. Wang et al. &lt;a href="https://arxiv.org/pdf/1703.10135.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Photo Style Transfer (2017), F. Luan et al. &lt;a href="http://arxiv.org/pdf/1703.07511v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al. &lt;a href="http://arxiv.org/pdf/1703.03864v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deformable Convolutional Networks (2017), J. Dai et al. &lt;a href="http://arxiv.org/pdf/1703.06211v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mask R-CNN (2017), K. He et al. &lt;a href="https://128.84.21.199/pdf/1703.06870" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al. &lt;a href="http://arxiv.org/pdf/1703.05192v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep voice: Real-time neural text-to-speech (2017), S. Arik et al., &lt;a href="http://arxiv.org/pdf/1702.07825v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PixelNet: Representation of the pixels, by the pixels, and for the pixels (2017), A. Bansal et al. &lt;a href="http://arxiv.org/pdf/1702.06506v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Batch renormalization: Towards reducing minibatch dependence in batch-normalized models (2017), S. Ioffe. &lt;a href="https://arxiv.org/abs/1702.03275" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wasserstein GAN (2017), M. Arjovsky et al. &lt;a href="https://arxiv.org/pdf/1701.07875v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding deep learning requires rethinking generalization (2017), C. Zhang et al. &lt;a href="https://arxiv.org/pdf/1611.03530" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Least squares generative adversarial networks (2016), X. Mao et al. &lt;a href="https://arxiv.org/abs/1611.04076v2" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-old-papers" class="anchor" aria-hidden="true" href="#old-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Old Papers&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Classic papers published before 2012&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep sparse rectifier neural networks (2011), X. Glorot et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Natural language processing (almost) from scratch (2011), R. Collobert et al. &lt;a href="http://arxiv.org/pdf/1103.0398" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recurrent neural network based language model (2010), T. Mikolov et al. &lt;a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning mid-level features for recognition (2010), Y. Boureau &lt;a href="http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A practical guide to training restricted boltzmann machines (2010), G. Hinton &lt;a href="http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Why does unsupervised pre-training help deep learning (2010), D. Erhan et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning deep architectures for AI (2009), Y. Bengio. &lt;a href="http://sanghv.com/download/soft/machine%20learning,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%20(2009).pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009), H. Lee et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Greedy layer-wise training of deep networks (2007), Y. Bengio et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reducing the dimensionality of data with neural networks, G. Hinton and R. Salakhutdinov. &lt;a href="http://homes.mpimf-heidelberg.mpg.de/~mhelmsta/pdf/2006%20Hinton%20Salakhudtkinov%20Science.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A fast learning algorithm for deep belief nets (2006), G. Hinton et al. &lt;a href="http://nuyoo.utm.mx/~jjf/rna/A8%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gradient-based learning applied to document recognition (1998), Y. LeCun et al. &lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Long short-term memory (1997), S. Hochreiter and J. Schmidhuber. &lt;a href="http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-hw--sw--dataset" class="anchor" aria-hidden="true" href="#hw--sw--dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HW / SW / Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SQuAD: 100,000+ Questions for Machine Comprehension of Text (2016), Rajpurkar et al. &lt;a href="https://arxiv.org/pdf/1606.05250.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenAI gym (2016), G. Brockman et al. &lt;a href="https://arxiv.org/pdf/1606.01540" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TensorFlow: Large-scale machine learning on heterogeneous distributed systems (2016), M. Abadi et al. &lt;a href="http://arxiv.org/pdf/1603.04467" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Theano: A Python framework for fast computation of mathematical expressions, R. Al-Rfou et al.&lt;/li&gt;
&lt;li&gt;Torch7: A matlab-like environment for machine learning, R. Collobert et al. &lt;a href="https://ronan.collobert.com/pub/matos/2011_torch7_nipsw.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MatConvNet: Convolutional neural networks for matlab (2015), A. Vedaldi and K. Lenc &lt;a href="http://arxiv.org/pdf/1412.4564" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Imagenet large scale visual recognition challenge (2015), O. Russakovsky et al. &lt;a href="http://arxiv.org/pdf/1409.0575" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Caffe: Convolutional architecture for fast feature embedding (2014), Y. Jia et al. &lt;a href="http://arxiv.org/pdf/1408.5093" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-book--survey--review" class="anchor" aria-hidden="true" href="#book--survey--review"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Book / Survey / Review&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;On the Origin of Deep Learning (2017), H. Wang and Bhiksha Raj. &lt;a href="https://arxiv.org/pdf/1702.07800" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Reinforcement Learning: An Overview (2017), Y. Li, &lt;a href="http://arxiv.org/pdf/1701.07274v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Machine Translation and Sequence-to-sequence Models(2017): A Tutorial, G. Neubig. &lt;a href="http://arxiv.org/pdf/1703.01619v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Network and Deep Learning (Book, Jan 2017), Michael Nielsen. &lt;a href="http://neuralnetworksanddeeplearning.com/index.html" rel="nofollow"&gt;[html]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep learning (Book, 2016), Goodfellow et al. &lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;[html]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LSTM: A search space odyssey (2016), K. Greff et al. &lt;a href="https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&amp;amp;utm_medium=social&amp;amp;utm_source=plus.google.com&amp;amp;utm_campaign=buffer" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tutorial on Variational Autoencoders (2016), C. Doersch. &lt;a href="https://arxiv.org/pdf/1606.05908" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton &lt;a href="https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep learning in neural networks: An overview (2015), J. Schmidhuber &lt;a href="http://arxiv.org/pdf/1404.7828" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Representation learning: A review and new perspectives (2013), Y. Bengio et al. &lt;a href="http://arxiv.org/pdf/1206.5538" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-video-lectures--tutorials--blogs" class="anchor" aria-hidden="true" href="#video-lectures--tutorials--blogs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video Lectures / Tutorials / Blogs&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(Lectures)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CS231n, Convolutional Neural Networks for Visual Recognition, Stanford University &lt;a href="http://cs231n.stanford.edu/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CS224d, Deep Learning for Natural Language Processing, Stanford University &lt;a href="http://cs224d.stanford.edu/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Oxford Deep NLP 2017, Deep Learning for Natural Language Processing, University of Oxford &lt;a href="https://github.com/oxford-cs-deepnlp-2017/lectures"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(Tutorials)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NIPS 2016 Tutorials, Long Beach &lt;a href="https://nips.cc/Conferences/2016/Schedule?type=Tutorial" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ICML 2016 Tutorials, New York City &lt;a href="http://techtalks.tv/icml/2016/tutorials/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ICLR 2016 Videos, San Juan &lt;a href="http://videolectures.net/iclr2016_san_juan/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Learning Summer School 2016, Montreal &lt;a href="http://videolectures.net/deeplearning2016_montreal/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bay Area Deep Learning School 2016, Stanford &lt;a href="https://www.bayareadlschool.org/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(Blogs)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenAI &lt;a href="https://www.openai.com/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Distill &lt;a href="http://distill.pub/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Andrej Karpathy Blog &lt;a href="http://karpathy.github.io/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Colah's Blog &lt;a href="http://colah.github.io/" rel="nofollow"&gt;[Web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WildML &lt;a href="http://www.wildml.com/" rel="nofollow"&gt;[Web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FastML &lt;a href="http://www.fastml.com/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TheMorningPaper &lt;a href="https://blog.acolyer.org" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-appendix-more-than-top-100" class="anchor" aria-hidden="true" href="#appendix-more-than-top-100"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Appendix: More than Top 100&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(2016)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A character-level decoder without explicit segmentation for neural machine translation (2016), J. Chung et al. &lt;a href="https://arxiv.org/pdf/1603.06147" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dermatologist-level classification of skin cancer with deep neural networks (2017), A. Esteva et al. &lt;a href="http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html" rel="nofollow"&gt;[html]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Weakly supervised object localization with multi-fold multiple instance learning (2017), R. Gokberk et al. &lt;a href="https://arxiv.org/pdf/1503.00949" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Brain tumor segmentation with deep neural networks (2017), M. Havaei et al. &lt;a href="https://arxiv.org/pdf/1505.03540" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Professor Forcing: A New Algorithm for Training Recurrent Networks (2016), A. Lamb et al. &lt;a href="https://arxiv.org/pdf/1610.09038" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adversarially learned inference (2016), V. Dumoulin et al. &lt;a href="https://ishmaelbelghazi.github.io/ALI/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1606.00704v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding convolutional neural networks (2016), J. Koushik &lt;a href="https://arxiv.org/pdf/1605.09081v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Taking the human out of the loop: A review of bayesian optimization (2016), B. Shahriari et al. &lt;a href="https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adaptive computation time for recurrent neural networks (2016), A. Graves &lt;a href="http://arxiv.org/pdf/1603.08983" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Densely connected convolutional networks (2016), G. Huang et al. &lt;a href="https://arxiv.org/pdf/1608.06993v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Region-based convolutional networks for accurate object detection and segmentation (2016), R. Girshick et al.&lt;/li&gt;
&lt;li&gt;Continuous deep q-learning with model-based acceleration (2016), S. Gu et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/gu16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A thorough examination of the cnn/daily mail reading comprehension task (2016), D. Chen et al. &lt;a href="https://arxiv.org/pdf/1606.02858" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Achieving open vocabulary neural machine translation with hybrid word-character models, M. Luong and C. Manning. &lt;a href="https://arxiv.org/pdf/1604.00788" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Very Deep Convolutional Networks for Natural Language Processing (2016), A. Conneau et al. &lt;a href="https://arxiv.org/pdf/1606.01781" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bag of tricks for efficient text classification (2016), A. Joulin et al. &lt;a href="https://arxiv.org/pdf/1607.01759" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Efficient piecewise training of deep structured models for semantic segmentation (2016), G. Lin et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning to compose neural networks for question answering (2016), J. Andreas et al. &lt;a href="https://arxiv.org/pdf/1601.01705" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perceptual losses for real-time style transfer and super-resolution (2016), J. Johnson et al. &lt;a href="https://arxiv.org/pdf/1603.08155" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reading text in the wild with convolutional neural networks (2016), M. Jaderberg et al. &lt;a href="http://arxiv.org/pdf/1412.1842" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;What makes for effective detection proposals? (2016), J. Hosang et al. &lt;a href="https://arxiv.org/pdf/1502.05082" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks (2016), S. Bell et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Instance-aware semantic segmentation via multi-task network cascades (2016), J. Dai et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conditional image generation with pixelcnn decoders (2016), A. van den Oord et al. &lt;a href="http://papers.nips.cc/paper/6527-tree-structured-reinforcement-learning-for-sequential-object-localization.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep networks with stochastic depth (2016), G. Huang et al., &lt;a href="https://arxiv.org/pdf/1603.09382" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics (2016), Yee Whye Teh et al. &lt;a href="http://www.jmlr.org/papers/volume17/teh16a/teh16a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(2015)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ask your neurons: A neural-based approach to answering questions about images (2015), M. Malinowski et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Exploring models and data for image question answering (2015), M. Ren et al. &lt;a href="http://papers.nips.cc/paper/5640-stochastic-variational-inference-for-hidden-markov-models.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Are you talking to a machine? dataset and methods for multilingual image question (2015), H. Gao et al. &lt;a href="http://papers.nips.cc/paper/5641-are-you-talking-to-a-machine-dataset-and-methods-for-multilingual-image-question.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mind's eye: A recurrent visual representation for image caption generation (2015), X. Chen and C. Zitnick. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Chen_Minds_Eye_A_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;From captions to visual concepts and back (2015), H. Fang et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Towards AI-complete question answering: A set of prerequisite toy tasks (2015), J. Weston et al. &lt;a href="http://arxiv.org/pdf/1502.05698" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ask me anything: Dynamic memory networks for natural language processing (2015), A. Kumar et al. &lt;a href="http://arxiv.org/pdf/1506.07285" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Unsupervised learning of video representations using LSTMs (2015), N. Srivastava et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v37/srivastava15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding (2015), S. Han et al. &lt;a href="https://arxiv.org/pdf/1510.00149" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved semantic representations from tree-structured long short-term memory networks (2015), K. Tai et al. &lt;a href="https://arxiv.org/pdf/1503.00075" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Character-aware neural language models (2015), Y. Kim et al. &lt;a href="https://arxiv.org/pdf/1508.06615" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Grammar as a foreign language (2015), O. Vinyals et al. &lt;a href="http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Trust Region Policy Optimization (2015), J. Schulman et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Beyond short snippents: Deep networks for video classification (2015) &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning Deconvolution Network for Semantic Segmentation (2015), H. Noh et al. &lt;a href="https://arxiv.org/pdf/1505.04366v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning spatiotemporal features with 3d convolutional networks (2015), D. Tran et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding neural networks through deep visualization (2015), J. Yosinski et al. &lt;a href="https://arxiv.org/pdf/1506.06579" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An Empirical Exploration of Recurrent Network Architectures (2015), R. Jozefowicz et al.  &lt;a href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep generative image models using a￼ laplacian pyramid of adversarial networks (2015), E.Denton et al. &lt;a href="http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gated Feedback Recurrent Neural Networks (2015), J. Chung et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v37/chung15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fast and accurate deep network learning by exponential linear units (ELUS) (2015), D. Clevert et al. &lt;a href="https://arxiv.org/pdf/1511.07289.pdf%5Cnhttp://arxiv.org/abs/1511.07289%5Cnhttp://arxiv.org/abs/1511.07289" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pointer networks (2015), O. Vinyals et al. &lt;a href="http://papers.nips.cc/paper/5866-pointer-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Visualizing and Understanding Recurrent Networks (2015), A. Karpathy et al. &lt;a href="https://arxiv.org/pdf/1506.02078" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Attention-based models for speech recognition (2015), J. Chorowski et al. &lt;a href="http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;End-to-end memory networks (2015), S. Sukbaatar et al. &lt;a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Describing videos by exploiting temporal structure (2015), L. Yao et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Describing_Videos_by_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A neural conversational model (2015), O. Vinyals and Q. Le. &lt;a href="https://arxiv.org/pdf/1506.05869.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improving distributional similarity with lessons learned from word embeddings, O. Levy et al. [[pdf]] (&lt;a href="https://www.transacl.org/ojs/index.php/tacl/article/download/570/124" rel="nofollow"&gt;https://www.transacl.org/ojs/index.php/tacl/article/download/570/124&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Transition-Based Dependency Parsing with Stack Long Short-Term Memory (2015), C. Dyer et al. &lt;a href="http://aclweb.org/anthology/P/P15/P15-1033.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs (2015), M. Ballesteros et al. &lt;a href="http://aclweb.org/anthology/D/D15/D15-1041.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Finding function in form: Compositional character models for open vocabulary word representation (2015), W. Ling et al. &lt;a href="http://aclweb.org/anthology/D/D15/D15-1176.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(~2014)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DeepPose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning a Deep Convolutional Network for Image Super-Resolution (2014, C. Dong et al. &lt;a href="https://www.researchgate.net/profile/Chen_Change_Loy/publication/264552416_Lecture_Notes_in_Computer_Science/links/53e583e50cf25d674e9c280e.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recurrent models of visual attention (2014), V. Mnih et al. &lt;a href="http://arxiv.org/pdf/1406.6247.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Empirical evaluation of gated recurrent neural networks on sequence modeling (2014), J. Chung et al. &lt;a href="https://arxiv.org/pdf/1412.3555" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Addressing the rare word problem in neural machine translation (2014), M. Luong et al. &lt;a href="https://arxiv.org/pdf/1410.8206" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;On the properties of neural machine translation: Encoder-decoder approaches (2014), K. Cho et. al.&lt;/li&gt;
&lt;li&gt;Recurrent neural network regularization (2014), W. Zaremba et al. &lt;a href="http://arxiv.org/pdf/1409.2329" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Intriguing properties of neural networks (2014), C. Szegedy et al. &lt;a href="https://arxiv.org/pdf/1312.6199.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Towards end-to-end speech recognition with recurrent neural networks (2014), A. Graves and N. Jaitly. &lt;a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Scalable object detection using deep neural networks (2014), D. Erhan et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;On the importance of initialization and momentum in deep learning (2013), I. Sutskever et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_sutskever13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Regularization of neural networks using dropconnect (2013), L. Wan et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning Hierarchical Features for Scene Labeling (2013), C. Farabet et al. &lt;a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linguistic Regularities in Continuous Space Word Representations (2013), T. Mikolov et al. &lt;a href="http://www.aclweb.org/anthology/N13-1#page=784" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Large scale distributed deep networks (2012), J. Dean et al. &lt;a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A Fast and Accurate Dependency Parser using Neural Networks. Chen and Manning. &lt;a href="http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;Thank you for all your contributions. Please make sure to read the &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md"&gt;contributing guide&lt;/a&gt; before you make a pull request.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://creativecommons.org/publicdomain/zero/1.0/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/60561947585c982aee67ed3e3b25388184cc0aa3/687474703a2f2f6d6972726f72732e6372656174697665636f6d6d6f6e732e6f72672f70726573736b69742f627574746f6e732f38387833312f7376672f63632d7a65726f2e737667" alt="CC0" data-canonical-src="http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To the extent possible under law, &lt;a href="https://www.facebook.com/terryum.io/" rel="nofollow"&gt;Terry T. Um&lt;/a&gt; has waived all copyright and related or neighboring rights to this work.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>terryum</author><guid isPermaLink="false">https://github.com/terryum/awesome-deep-learning-papers</guid><pubDate>Fri, 07 Feb 2020 00:07:00 GMT</pubDate></item><item><title>spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex #8 in TeX, Today</title><link>https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex</link><description>&lt;p&gt;&lt;i&gt;Write Beautiful Curriculum Vitae in LaTex, that ensures twenty seconds reading.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-twenty-seconds-curriculum-vitae-in-latex" class="anchor" aria-hidden="true" href="#twenty-seconds-curriculum-vitae-in-latex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Twenty Seconds Curriculum Vitae in LaTex&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://www.latex-project.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/622808a2e3e436763ab2cd995e82e090d48d08cc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d616465253230776974682d4c615465582d3166343235662e737667" alt="made-with-latex" data-canonical-src="https://img.shields.io/badge/Made%20with-LaTeX-1f425f.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://lbesson.mit-license.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/311762166ef25238116d3cadd22fcb6091edab98/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667" alt="MIT license" data-canonical-src="https://img.shields.io/badge/License-MIT-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex"&gt;&lt;img src="https://camo.githubusercontent.com/94cee5f9e0ae21b66fbe829b4eb9f0419af5a0ca/68747470733a2f2f6261646765732e66726170736f66742e636f6d2f6f732f76332f6f70656e2d736f757263652e7376673f763d313033" alt="Open Source Love svg3" data-canonical-src="https://badges.frapsoft.com/os/v3/open-source.svg?v=103" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/graphs/commit-activity"&gt;&lt;img src="https://camo.githubusercontent.com/0e6a3f975d68b438efec82fef1f9491600606df8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d61696e7461696e65642533462d7965732d677265656e2e737667" alt="Maintenance" data-canonical-src="https://img.shields.io/badge/Maintained%3F-yes-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/issues/"&gt;&lt;img src="https://camo.githubusercontent.com/b8aaf86d4c1124871832f63b543d32c32a4d90fb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f4e61657265656e2f5374726170446f776e2e6a732e737667" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/Naereen/StrapDown.js.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/issues?q=is%3Aissue+is%3Aclosed"&gt;&lt;img src="https://camo.githubusercontent.com/4b5ebe5364b22087d0906bd57c87901689ac19ec/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d636c6f7365642f4e61657265656e2f5374726170446f776e2e6a732e737667" alt="GitHub issues-closed" data-canonical-src="https://img.shields.io/github/issues-closed/Naereen/StrapDown.js.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.paypal.me/CarmineSpagnuolo" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d59f4d8577fa6fd5347e5427f361ee18c0e43099/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617950616c2d446f6e617465253230746f253230417574686f722d626c75652e737667" alt="Donate" data-canonical-src="https://img.shields.io/badge/PayPal-Donate%20to%20Author-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/issues"&gt;&lt;img src="https://camo.githubusercontent.com/d52b9239d76d77ebff4fc954745ee8ba555338ee/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f41736b2532306d652d616e797468696e672d3161626339632e737667" alt="Ask Me Anything !" data-canonical-src="https://img.shields.io/badge/Ask%20me-anything-1abc9c.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/intent/tweet?text=Download%20and%20use%20the%20Twenty%20Seconds%20Curriculum%20Vitae%20in%20LaTex&amp;amp;url=https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex&amp;amp;hashtags=curriculum,resume,templates,cv,latex,interview,r%C3%A9sum%C3%A9" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/83d4084f7b71558e33b08844da5c773a8657e271/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c" alt="Tweet" data-canonical-src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.overleaf.com/latex/templates/twenty-seconds-curriculum-vitae/kfgsngtymkfj" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/73bbb9882e7e0da338106ecb5a9c57c97f63bea9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f417661696c61626c652d4f7665726c6561662d677265656e3f6c6f676f3d646174613a696d6167652f706e673b6261736536342c6956424f5277304b47676f414141414e5355684555674141414341414141416743414d414141424570497247414141414247644254554541414c4750432f7868425141414143426a53464a4e414142364a6741416749514141506f41414143413641414164544141414f7067414141366d41414146334363756c4538414141426656424d5645554141414249724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244774141414163636956684141414166585253546c4d414141386d4e6a3945517a3079495168567a2b37312b2f37392b6650725a55336b393679446d6f33367751456f7331705779522b397453726a776a6b536f463946657a6f336a505a51536d3654696c30526d5234516979736b38594663446533306b644f3861435079676b777043746b55335158776e7875325758773452334d4d3746524943785a54555150614241666e7142585645324d595a4e7645744951582b4d5537532f506330494d4141414142596b74485241434942523149414141414358424957584d414143346a414141754977463470543932414141414233524a545555483541455045536f7842315050687741414158314a524546554f4d7574556c6462776b41513341743255514a69695131467859616f6147796f32447432374e68374678754b38392b396948346d75547a3575532f4a33637a757a4f347430663847593053534c533039497a4d724f7965586d494154326650794862495463425734433476454373556c2b4134464b445658594b7a73427936767149536e53736976547148656d746f3658734c4842415031477477674e545a703332714c487668317337386c3846576d31614b464e714364676f6f474f7a7049464f6945476952375633645062312b6f33794a2f414735707349426e443451745a7a674544492b4d66736d7259795232795058484a36446d543035706c476c7878444f594a652b634c53784651764f6373574332554177734c6b55305a347957562f6955566b32454b4542724b65656373513573474e7659564c436c3039766d4973594366686b374f6b63356269426d494f7936734b633737683841687762436b51734233664759452b7747516b79475179666834784a42412b46454255352f625a39786b2b64476c78376734764c6e6348554e334a68653635626e724678714f383159376831663258767a714c56746a44374569654b50542f79336a356e336c5a3668697865723933373978526549436676436179536546646e706c4e2f65452b4936705267555479352b4a424e6b6a66383950674643654732376c6768744741414141435630525668305a4746305a54706a636d5668644755414d6a41794d4330774d5330784e5651784e7a6f304d6a6f304f5373774d7a6f774d484732554b6b414141416c6445565964475268644755366257396b61575a35414449774d6a41744d4445744d5456554d5463364e4449364e446b724d444d364d444141362b67564141414141456c46546b5375516d4343" alt="Overleaf" data-canonical-src="https://img.shields.io/badge/Available-Overleaf-green?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAABfVBMVEUAAABIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDwAAAAcciVhAAAAfXRSTlMAAA8mNj9EQz0yIQhVz+71+/79+fPrZU3k96yDmo36wQEos1pWyR+9tSrjwjkSoF9Fezo3jPZQSm6Til0RmR4Qiysk8YFcDe30kdO8aCPygkwpCtkU3QXwnxu2WXw4R3MM7FRICxZTUQPaBAfnqBXVE2MYZNvEtIQX+MU7S/Pc0IMAAAABYktHRACIBR1IAAAACXBIWXMAAC4jAAAuIwF4pT92AAAAB3RJTUUH5AEPESoxB1PPhwAAAX1JREFUOMutUldbwkAQ3At2UQJiiQ1FxYaoaGyo2Dt27Nh7FxuK89+9iH4muTz5uS/J3czuzO4t0f8GY0SSLS09IzMrOyeXmIAT2fPyHbITcBW4C4vECsUl+A4FKDVXYKzsBy6vqISnSsivTqHemto6XsLHBAP1GtwgNTZp32qLHvh1s78l8FWm1aKFNqCdgooGOzpIFOiEGiR7V3dPb1+o3yJ/AG5psIBnD4QtZzgEDI+MfsmrYyR2yPXHJ6DmT05plGlxxDOYJe+cLSxFQvOcsWC2UAwsLkU0Z4yWV/iUVk2EKEBrKeecsQ5sGNvYVLCl09vmIsYCfhk7Okc5biBmIOy6sKc77h8AhwbCkQsB3fGYE+wGQkyGQyfh4xJBA+FEBU5/bZ9xk+dGlx7g4vLncHUN3Jhe65bnrFxqO81Y7h1f2XvzqLVtjD7EieKPT/y3j5n3lZ6hixer9379xReICfvCaySeFdnplN/eE+I6pRgUTy5+JBNkjf89PgFCeG27lghtGAAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMC0wMS0xNVQxNzo0Mjo0OSswMzowMHG2UKkAAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjAtMDEtMTVUMTc6NDI6NDkrMDM6MDAA6+gVAAAAAElFTkSuQmCC" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-hot" class="anchor" aria-hidden="true" href="#hot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HOT!&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Support Fontawesome Icons new class option &lt;code&gt;\documentclass[icon]{twentysecondcv}&lt;/code&gt;, using the name of the icon available in the documentation of the package &lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/raw/master/fontawesome.pdf"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The Fontawesome version requires the Fontawesome installed. Notice that the  Fontawesome is already available in Overleaf.com.&lt;/li&gt;
&lt;li&gt;Section with Icon:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;\sectionicon{icon-name}{section-name}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Twenty icon items environment&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;\begin{twentyicon}
  \twentyitemicon
    {icon name}
    {year}
    {title}
    {place}
    {description}
\end{twentyicon}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Twenty items icon short environment&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;\begin{twentyshorticon}
  \twentyitemshorticon
    {icon name}
    {year}
    {description}
\end{twentyshorticon}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-curricula-vitae---résumés" class="anchor" aria-hidden="true" href="#curricula-vitae---résumés"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Curricula Vitae - Résumés&lt;/h1&gt;
&lt;p&gt;A curriculum vitae, otherwise known as a CV or résumé, is a document used by individuals to communicate their work history, education and skill set. This is a style template for your curriculum written in LaTex. The main goal of this template is to provide a curriculum that is able to survive to the résumés screening of "twenty seconds".&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The author assumes no responsibility for the topicality, correctness, completeness or quality of the information provided and for the obtained résumés.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This is designed for computer scientists but there is no limitation to use it for résumés in other disciplines.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-if-you-like-this-curriculum-please-dont-forget-to-leave-a-star-to-help-the-development-and-improvement" class="anchor" aria-hidden="true" href="#if-you-like-this-curriculum-please-dont-forget-to-leave-a-star-to-help-the-development-and-improvement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;If you like this curriculum, please don't forget to leave a star, to help the development and improvement.&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-the-basic-idea-is-kiss---keep-it-simple-stupid" class="anchor" aria-hidden="true" href="#the-basic-idea-is-kiss---keep-it-simple-stupid"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The basic idea is KISS - Keep It Simple, Stupid.&lt;/h2&gt;
&lt;p&gt;In a nutshell &lt;em&gt;&lt;strong&gt;"It is vain to do with more what can be done with fewer"&lt;/strong&gt;&lt;/em&gt; -- Occam's razor --&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;This template has been designed to create a "one-page" résumé is therefore not suitable to create curriculum of more than one-page.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Please do not try to create curriculum more than one-page.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-how-to-describe-your-experiences" class="anchor" aria-hidden="true" href="#how-to-describe-your-experiences"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to describe your experiences?&lt;/h3&gt;
&lt;p&gt;There are many theories about the résumé screening process of "Big" companies.
Resume screeners and the interviewers look in your résumé for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are you smart?&lt;/li&gt;
&lt;li&gt;Can you code (act for what you apply)?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Anyway according to the guidelines of this template you should use a really simple form to describe each items in your résumé:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Accomplished &amp;lt;X&amp;gt; by implementing &amp;lt;Y&amp;gt; which led to &amp;lt;Z&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here's an examples:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Reduced object rendering time by 75% by applying Floyd's algorithm, leading to a 10% reduction in system boot time.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;-- &lt;em&gt;Cracking the Coding Interview, Book, Gayle Laakmann Mcdowell&lt;/em&gt; --&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-toy-résumé-with-fontawesome-icons-document-class-option-icon" class="anchor" aria-hidden="true" href="#toy-résumé-with-fontawesome-icons-document-class-option-icon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Toy Résumé with Fontawesome Icons (document class option &lt;em&gt;icon&lt;/em&gt;)&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/raw/master/Twenty-Seconds_cv_icons.jpg"&gt;&lt;img src="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/raw/master/Twenty-Seconds_cv_icons.jpg" alt="sample résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-build" class="anchor" aria-hidden="true" href="#build"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build&lt;/h1&gt;
&lt;p&gt;This guide walks you to build your résumé.&lt;/p&gt;
&lt;p&gt;Build requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LaTex installation.
&lt;ul&gt;
&lt;li&gt;additionals packages:
&lt;ul&gt;
&lt;li&gt;ClearSans, fontenc&lt;/li&gt;
&lt;li&gt;tikz&lt;/li&gt;
&lt;li&gt;xcolor&lt;/li&gt;
&lt;li&gt;textpos&lt;/li&gt;
&lt;li&gt;ragged2e&lt;/li&gt;
&lt;li&gt;etoolbox&lt;/li&gt;
&lt;li&gt;ifmtarg&lt;/li&gt;
&lt;li&gt;ifthen&lt;/li&gt;
&lt;li&gt;pgffor&lt;/li&gt;
&lt;li&gt;marvosym&lt;/li&gt;
&lt;li&gt;parskip&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-build-through-gnu-make-command" class="anchor" aria-hidden="true" href="#build-through-gnu-make-command"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build through GNU Make command&lt;/h3&gt;
&lt;p&gt;Clean your project résumé.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make clean
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Build your project résumé.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make all
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;-- &lt;em&gt;Alternately you can build through your favorite LaTex editor.&lt;/em&gt; --&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-environment-style-and-list-of-commands" class="anchor" aria-hidden="true" href="#environment-style-and-list-of-commands"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Environment style and list of commands&lt;/h1&gt;
&lt;p&gt;The style is divided in two parts. The former is the left side bar: that contains personal information, profile picture, and information about your professional skills. The second part is the body that should be contains details about your academic studies, professional experiences and all the information that you want (remember the KISS principle).&lt;/p&gt;
&lt;p&gt;The class is &lt;code&gt;\documentclass[icon]{twentysecondcv}&lt;/code&gt;, the &lt;strong&gt;icon&lt;/strong&gt; option enable to use Fontawesome package in sections and twenty items. In order to use the icon option you need to install the Fontawesome package and use the Fontawesome icon name, available in the package documentation &lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/raw/master/fontawesome.pdf"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-profile-environment" class="anchor" aria-hidden="true" href="#profile-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Profile environment&lt;/h3&gt;
&lt;p&gt;These are the command to set up the profile information.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Set up the image profile.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \profilepic{paht_name}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your name.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvname{your name}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your job profile.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvjobtitle{your job title}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your date of birth.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvdate{date}	
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your address.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvaddress{address}		
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your telephone number.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvnumberphone{phone number}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your email.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvmail{email address}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your personal home page.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvsite{home page address}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up a brief description of you.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \about{brief description}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up the skills with chart style. Each skill must is a couple &lt;code&gt;{name/value}&lt;/code&gt;, where the value is a floating point value between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt;. This is an agreement for the graphics issues, the &lt;code&gt;0&lt;/code&gt; correspond to a Fundamental awareness while &lt;code&gt;6&lt;/code&gt; to a Expert awareness level.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \skills{{name skill1/5.8},{name skill2/4}} 
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up the skills with text style.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \skillstext{{name skill1/5.8},{name skill2/4}} 
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To create the profile use the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;\makeprofile
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-body-environment" class="anchor" aria-hidden="true" href="#body-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Body environment&lt;/h3&gt;
&lt;p&gt;The body document part is composed by sections.
In the sections you can put two kinds of list items.&lt;/p&gt;
&lt;p&gt;The first (&lt;em&gt;Twenty items environment&lt;/em&gt;) intends a list of detailed information with four part: &lt;strong&gt;Data&lt;/strong&gt; -- &lt;strong&gt;Title&lt;/strong&gt; -- &lt;strong&gt;Place&lt;/strong&gt; -- &lt;strong&gt;Description&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The second (&lt;em&gt;Twenty items short environment&lt;/em&gt;) intends a fewer informationinformation (you can customize this list more easily): &lt;strong&gt;Data&lt;/strong&gt; -- &lt;strong&gt;Description&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-sections-also-wiht-icon" class="anchor" aria-hidden="true" href="#sections-also-wiht-icon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sections (also wiht icon)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Set up a new section in the body part.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \section{sction name}
  \section{icon name}{section name}, require the icon option in the document declaration.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-twenty-items-environment-also-wiht-icon" class="anchor" aria-hidden="true" href="#twenty-items-environment-also-wiht-icon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Twenty items environment (also wiht icon)&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;\begin{twenty}
  \twentyitem
    {year}
    {title}
    {place}
    {description}
\end{twenty}

\begin{twentyicon}
  \twentyitemicon
    {icon name}
    {year}
    {title}
    {place}
    {description}
\end{twentyicon}
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-twenty-items-short-environment-also-wiht-icon" class="anchor" aria-hidden="true" href="#twenty-items-short-environment-also-wiht-icon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Twenty items short environment (also wiht icon)&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;\begin{twentyshort}
  \twentyitemshort
    {year}
    {description}
\end{twentyshort}

\begin{twentyshorticon}
  \twentyitemshorticon
    {icon name}
    {year}
    {description}
\end{twentyshorticon}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-other-commands" class="anchor" aria-hidden="true" href="#other-commands"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other commands&lt;/h3&gt;
&lt;p&gt;There other two fun command: \icon and \round; that enables to wrap the text in oval shape.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;	\icon{text}
	\round{text, color}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>spagnuolocarmine</author><guid isPermaLink="false">https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex</guid><pubDate>Fri, 07 Feb 2020 00:08:00 GMT</pubDate></item><item><title>cplusplus/draft #9 in TeX, Today</title><link>https://github.com/cplusplus/draft</link><description>&lt;p&gt;&lt;i&gt;C++ standards drafts&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-c-standard-draft-sources" class="anchor" aria-hidden="true" href="#c-standard-draft-sources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;C++ Standard Draft Sources&lt;/h1&gt;
&lt;p&gt;These are the sources used to generate drafts of the C++
standard. These sources should not be considered an ISO publication,
nor should documents generated from them unless officially adopted by
the C++ working group (ISO/IEC JTC1/SC22/WG21).&lt;/p&gt;
&lt;p&gt;Get involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/cplusplus/draft/wiki/How-to-submit-an-editorial-issue"&gt;How to submit an editorial issue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cplusplus/draft/wiki/How-to-tell-if-an-issue-is-editorial"&gt;How to tell if an issue is editorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://isocpp.org/std/submit-issue" rel="nofollow"&gt;How to submit a new issue/defect report&lt;/a&gt; for non-editorial issues&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More information about the C++ standard can be found at &lt;a href="http://isocpp.org/std" rel="nofollow"&gt;isocpp.org&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-getting-started-on-mac-os-x"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-on-mac-os-x" class="anchor" aria-hidden="true" href="#getting-started-on-mac-os-x"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started on Mac OS X&lt;/h2&gt;
&lt;p&gt;Install the &lt;a href="http://tug.org/mactex/" rel="nofollow"&gt;MacTeX distribution&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are on a slow network, you'll want to get the &lt;a href="http://tug.org/mactex/morepackages.html" rel="nofollow"&gt;BasicTeX package&lt;/a&gt; instead,
then run the following command to install the other packages that the draft requires:&lt;/p&gt;
&lt;blockquote&gt;
sudo tlmgr install latexmk isodate substr relsize ulem fixme rsfs extract layouts enumitem l3packages l3kernel&lt;/blockquote&gt;
&lt;a name="user-content-getting-started-on-debian-based-systems"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-on-debian-based-systems" class="anchor" aria-hidden="true" href="#getting-started-on-debian-based-systems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started on Debian-based Systems&lt;/h2&gt;
&lt;p&gt;Install the following packages:&lt;/p&gt;
&lt;blockquote&gt;
sudo apt-get install latexmk texlive-latex-recommended texlive-latex-extra texlive-fonts-recommended&lt;/blockquote&gt;
&lt;a name="user-content-getting-started-on-fedora"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-on-fedora" class="anchor" aria-hidden="true" href="#getting-started-on-fedora"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started on Fedora&lt;/h2&gt;
&lt;p&gt;Install the following packages:&lt;/p&gt;
&lt;blockquote&gt;
dnf install latexmk texlive texlive-isodate texlive-relsize texlive-ulem texlive-fixme texlive-extract texlive-l3kernel texlive-l3packages&lt;/blockquote&gt;
&lt;a name="user-content-getting-started-on-arch-linux"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-on-arch-linux" class="anchor" aria-hidden="true" href="#getting-started-on-arch-linux"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started on Arch Linux&lt;/h2&gt;
&lt;p&gt;Install the following packages:&lt;/p&gt;
&lt;blockquote&gt;
latex-mk from the Arch User Repository.
pacman -S texlive-latexextra&lt;/blockquote&gt;
&lt;a name="user-content-getting-started-on-microsoft-windows"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-on-microsoft-windows" class="anchor" aria-hidden="true" href="#getting-started-on-microsoft-windows"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started on Microsoft Windows&lt;/h2&gt;
&lt;p&gt;Install Perl (for example, using a &lt;a href="https://cygwin.com/install.html" rel="nofollow"&gt;Cygwin installation&lt;/a&gt; and adding perl.
See &lt;a href="https://bennierobinson.com/programming/2016/01/24/perl-windows-2016.html" rel="nofollow"&gt;sample instructions&lt;/a&gt; for more details)&lt;/p&gt;
&lt;p&gt;Install &lt;a href="https://miktex.org/download" rel="nofollow"&gt;MiKTeX&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-instructions"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-instructions" class="anchor" aria-hidden="true" href="#instructions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Instructions&lt;/h2&gt;
&lt;p&gt;To typeset the draft document, from the &lt;code&gt;source&lt;/code&gt; directory:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;run &lt;code&gt;latexmk -pdf std&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That's it! You should now have an &lt;code&gt;std.pdf&lt;/code&gt; containing the typeset draft.&lt;/p&gt;
&lt;a name="user-content-alternative-instructions"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-alternative-instructions" class="anchor" aria-hidden="true" href="#alternative-instructions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Alternative instructions&lt;/h3&gt;
&lt;p&gt;If you can't use latexmk for some reason, you can use the Makefiles instead:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;run &lt;code&gt;make rebuild&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;make reindex&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you can't use latexmk or make for some reason, you can run LaTeX manually instead:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;run &lt;code&gt;pdflatex std&lt;/code&gt; until there are no more changed labels or changed tables&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;makeindex generalindex&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;makeindex libraryindex&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;makeindex grammarindex&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;makeindex impldefindex&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;pdflatex std&lt;/code&gt; once more.&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;makeindex -s basic.gst -o xrefindex.gls xrefindex.glo&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;makeindex -s basic.gst -o xrefdelta.gls xrefdelta.glo&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;pdflatex std&lt;/code&gt; twice more.&lt;/li&gt;
&lt;/ol&gt;
&lt;a name="user-content-generated-input-files"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-generated-input-files" class="anchor" aria-hidden="true" href="#generated-input-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generated input files&lt;/h3&gt;
&lt;p&gt;To regenerate figures from .dot files, run:&lt;/p&gt;
&lt;pre&gt;dot -o&amp;lt;pdfname&amp;gt; -Tpdf &amp;lt;dotfilename&amp;gt;
&lt;/pre&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre&gt;dot -ofigstreampos.pdf -Tpdf figstreampos.dot
&lt;/pre&gt;
&lt;a name="user-content-acknowledgements"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;A great deal of gratitude goes out to Pete Becker for his amazing work
in the original conversion of the C++ standard drafts to LaTeX, and
his subsequent maintenance of the standard drafts up to C++11. Thank
you Pete.&lt;/p&gt;
&lt;p&gt;Thanks to Walter Brown for suggesting the use of &lt;code&gt;latexmk&lt;/code&gt;.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>cplusplus</author><guid isPermaLink="false">https://github.com/cplusplus/draft</guid><pubDate>Fri, 07 Feb 2020 00:09:00 GMT</pubDate></item><item><title>exacity/deeplearningbook-chinese #10 in TeX, Today</title><link>https://github.com/exacity/deeplearningbook-chinese</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Book Chinese Translation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-中文翻译" class="anchor" aria-hidden="true" href="#deep-learning-中文翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning 中文翻译&lt;/h1&gt;
&lt;p&gt;在众多网友的帮助和校对下，中文版终于出版了。尽管还有很多问题，但至少90%的内容是可读的，并且是准确的。
我们尽可能地保留了原书&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;Deep Learning&lt;/a&gt;中的意思并保留原书的语句。&lt;/p&gt;
&lt;p&gt;然而我们水平有限，我们无法消除众多读者的方差。我们仍需要大家的建议和帮助，一起减小翻译的偏差。&lt;/p&gt;
&lt;p&gt;大家所要做的就是阅读，然后汇总你的建议，提issue（最好不要一个一个地提）。如果你确定你的建议不需要商量，可以直接发起PR。&lt;/p&gt;
&lt;p&gt;对应的翻译者：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第1、4、7、10、14、20章及第12.4、12.5节由 @swordyork 负责&lt;/li&gt;
&lt;li&gt;第2、5、8、11、15、18章由 @liber145 负责&lt;/li&gt;
&lt;li&gt;第3、6、9章由 @KevinLee1110 负责&lt;/li&gt;
&lt;li&gt;第13、16、17、19章及第12.1至12.3节由 @futianfan 负责&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-面向的读者" class="anchor" aria-hidden="true" href="#面向的读者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;面向的读者&lt;/h2&gt;
&lt;p&gt;请直接下载&lt;a href="https://github.com/exacity/deeplearningbook-chinese/releases/download/v0.5-beta/dlbook_cn_v0.5-beta.pdf"&gt;PDF&lt;/a&gt;阅读。
不打算提供EPUB等格式，如有需要请自行修改。&lt;/p&gt;
&lt;p&gt;这一版准确性已经有所提高，读者可以以中文版为主、英文版为辅来阅读学习，但我们仍建议研究者阅读&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;原版&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-出版及开源原因" class="anchor" aria-hidden="true" href="#出版及开源原因"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;出版及开源原因&lt;/h2&gt;
&lt;p&gt;本书由人民邮电出版社出版，如果你觉得中文版PDF对你有所帮助，希望你能支持下纸质正版书籍。
如果你觉得中文版不行，希望你能多提建议。非常感谢各位！
纸质版也会进一步更新，需要大家更多的建议和意见，一起完善中文版。&lt;/p&gt;
&lt;p&gt;纸质版目前在人民邮电出版社的异步社区出售，见&lt;a href="http://www.epubit.com.cn/book/details/4278" rel="nofollow"&gt;地址&lt;/a&gt;。
价格不低，但看了样本之后，我们认为物有所值。
注意，我们不会通过媒体进行宣传，希望大家先看电子版内容，再判断是否购买纸质版。&lt;/p&gt;
&lt;p&gt;以下是开源的具体原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们不是文学工作者，不专职翻译。单靠我们，无法给出今天的翻译，众多网友都给我们提出了宝贵的建议，因此开源帮了很大的忙。出版社会给我们稿费（我们也不知道多少，可能2万左右），我们也不好意思自己用，商量之后觉得捐出是最合适的，以所有贡献过的网友的名义（我们把稿费捐给了杉树公益，用于4名贵州高中生三年的生活费，见&lt;a href="https://github.com/exacity/deeplearningbook-chinese/blob/master/donation.pdf"&gt;捐赠情况&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;PDF电子版对于技术类书籍来说是很重要的，随时需要查询，拿着纸质版到处走显然不合适。国外很多技术书籍都有对应的电子版（虽然不一定是正版），而国内的几乎没有。个人认为这是出版社或者作者认为国民素质还没有高到主动为知识付费的境界，所以不愿意"泄露"电子版。时代在进步，我们也需要改变。特别是翻译作品普遍质量不高的情况下，要敢为天下先。&lt;/li&gt;
&lt;li&gt;深度学习发展太快，日新月异，所以我们希望大家更早地学到相关的知识。我觉得原作者开放PDF电子版也有类似的考虑，也就是先阅读后付费。我们认为中国人口素质已经足够高，懂得为知识付费。当然这不是付给我们的，是付给出版社的，出版社再付给原作者。我们不希望中文版的销量因PDF电子版的存在而下滑。出版社只有值回了版权才能在以后引进更多的优秀书籍。我们这个开源翻译先例也不会成为一个反面案例，以后才会有更多的PDF电子版。&lt;/li&gt;
&lt;li&gt;开源也涉及版权问题，出于版权原因，我们不再更新此初版PDF文件，请大家以最终的纸质版为准。（但源码会一直更新）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-致谢" class="anchor" aria-hidden="true" href="#致谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;致谢&lt;/h2&gt;
&lt;p&gt;我们有3个类别的校对人员。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;负责人也就是对应的翻译者。&lt;/li&gt;
&lt;li&gt;简单阅读，对语句不通顺或难以理解的地方提出修改意见。&lt;/li&gt;
&lt;li&gt;中英对比，进行中英对应阅读，排除少翻错翻的情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有校对建议都保存在各章的&lt;code&gt;annotations.txt&lt;/code&gt;文件中。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;章节&lt;/th&gt;
&lt;th&gt;负责人&lt;/th&gt;
&lt;th&gt;简单阅读&lt;/th&gt;
&lt;th&gt;中英对比&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter1_introduction/" rel="nofollow"&gt;第一章 前言&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc, @SiriusXDJ, @corenel, @NeutronT&lt;/td&gt;
&lt;td&gt;@linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter2_linear_algebra/" rel="nofollow"&gt;第二章 线性代数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@SiriusXDJ, @angrymidiao&lt;/td&gt;
&lt;td&gt;@badpoem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter3_probability_and_information_theory/" rel="nofollow"&gt;第三章 概率与信息论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@SiriusXDJ&lt;/td&gt;
&lt;td&gt;@kkpoker, @Peiyan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter4_numerical_computation/" rel="nofollow"&gt;第四章 数值计算&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;@zhangyafeikimi&lt;/td&gt;
&lt;td&gt;@hengqujushi&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter5_machine_learning_basics/" rel="nofollow"&gt;第五章 机器学习基础&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@wheaio, @huangpingchun&lt;/td&gt;
&lt;td&gt;@fairmiracle, @linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter6_deep_feedforward_networks/" rel="nofollow"&gt;第六章 深度前馈网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;David_Chow, @linzhp, @sailordiary&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter7_regularization/" rel="nofollow"&gt;第七章 深度学习中的正则化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@NBZCC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/" rel="nofollow"&gt;第八章 深度模型中的优化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@happynoom, @codeVerySlow&lt;/td&gt;
&lt;td&gt;@huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter9_convolutional_networks/" rel="nofollow"&gt;第九章 卷积网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @corenel&lt;/td&gt;
&lt;td&gt;@zhiding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter10_sequence_modeling_rnn/" rel="nofollow"&gt;第十章 序列建模：循环和递归网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @yinruiqing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter11_practical_methodology/" rel="nofollow"&gt;第十一章 实践方法论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter12_applications/" rel="nofollow"&gt;第十二章 应用&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork, @futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@corenel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter13_linear_factor_models/" rel="nofollow"&gt;第十三章 线性因子模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;@cloudygoose&lt;/td&gt;
&lt;td&gt;@ZhiweiYang&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter14_autoencoders/" rel="nofollow"&gt;第十四章 自编码器&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@Seaball, @huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter15_representation_learning/" rel="nofollow"&gt;第十五章 表示学习&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@cnscottzheng&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter16_structured_probabilistic_modelling/" rel="nofollow"&gt;第十六章 深度学习中的结构化概率模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter17_monte_carlo_methods/" rel="nofollow"&gt;第十七章 蒙特卡罗方法&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter18_confronting_the_partition_function/" rel="nofollow"&gt;第十八章 面对配分函数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@tankeco&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter19_approximate_inference/" rel="nofollow"&gt;第十九章 近似推断&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary, @hengqujushi, huanghaojun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter20_deep_generative_models/" rel="nofollow"&gt;第二十章 深度生成模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;参考文献&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@pkuwwt&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我们会在纸质版正式出版的时候，在书中致谢，正式感谢各位作出贡献的同学！&lt;/p&gt;
&lt;p&gt;还有很多同学提出了不少建议，我们都列在此处。&lt;/p&gt;
&lt;p&gt;@tttwwy @tankeco @fairmiracle @GageGao @huangpingchun @MaHongP @acgtyrant @yanhuibin315 @Buttonwood @titicacafz
@weijy026a @RuiZhang1993 @zymiboxpay @xingkongliang @oisc @tielei @yuduowu @Qingmu @HC-2016 @xiaomingabc
@bengordai @Bojian @JoyFYan @minoriwww @khty2000 @gump88 @zdx3578 @PassStory @imwebson @wlbksy @roachsinai @Elvinczp
@endymecy name:YUE-DaJiong @9578577 @linzhp @cnscottzheng @germany-zhu  @zhangyafeikimi @showgood163 @gump88
@kangqf @NeutronT @badpoem @kkpoker @Seaball @wheaio @angrymidiao @ZhiweiYang @corenel @zhaoyu611 @SiriusXDJ @dfcv24 EmisXXY
FlyingFire vsooda @friskit-china @poerin @ninesunqian @JiaqiYao @Sofring @wenlei @wizyoung @imageslr @@indam @XuLYC
@zhouqingping @freedomRen @runPenguin @pkuwwt @wuqi @tjliupeng @neo0801 @jt827859032 @demolpc @fishInAPool
@xiaolangyuxin @jzj1993 @whatbeg LongXiaJun jzd&lt;/p&gt;
&lt;p&gt;如有遗漏，请务必通知我们，可以发邮件至&lt;code&gt;echo c3dvcmQueW9ya0BnbWFpbC5jb20K | base64 --decode&lt;/code&gt;。
这是我们必须要感谢的，所以不要不好意思。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;排版&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-注意" class="anchor" aria-hidden="true" href="#注意"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;注意&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;各种问题或者建议可以提issue，建议使用中文。&lt;/li&gt;
&lt;li&gt;由于版权问题，我们不能将图片和bib上传，请见谅。&lt;/li&gt;
&lt;li&gt;Due to copyright issues, we would not upload figures and the bib file.&lt;/li&gt;
&lt;li&gt;可用于学习研究目的，不得用于任何商业行为。谢谢！&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-markdown格式" class="anchor" aria-hidden="true" href="#markdown格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Markdown格式&lt;/h2&gt;
&lt;p&gt;这种格式确实比较重要，方便查阅，也方便索引。初步转换后，生成网页，具体见&lt;a href="https://exacity.github.io/deeplearningbook-chinese" rel="nofollow"&gt;deeplearningbook-chinese&lt;/a&gt;。
注意，这种转换没有把图放进去，也不会放图。目前使用单个&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;，基于latex文件转换，以后可能会更改但原则是不直接修改&lt;a href="docs/_posts"&gt;md文件&lt;/a&gt;。
需要的同学可以自行修改&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-html格式" class="anchor" aria-hidden="true" href="#html格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTML格式&lt;/h2&gt;
&lt;p&gt;读者可以使用&lt;a href="https://github.com/coolwanglu/pdf2htmlEX"&gt;pdf2htmlEX&lt;/a&gt;进行转换，直接将PDF转换为HTML。&lt;/p&gt;
&lt;p&gt;Updating.....&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>exacity</author><guid isPermaLink="false">https://github.com/exacity/deeplearningbook-chinese</guid><pubDate>Fri, 07 Feb 2020 00:10:00 GMT</pubDate></item><item><title>sb2nov/resume #11 in TeX, Today</title><link>https://github.com/sb2nov/resume</link><description>&lt;p&gt;&lt;i&gt;Software developer resume in Latex&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;A single-page, one-column resume for software developers. It uses the base latex templates and fonts to provide ease of use and installation when trying to update the resume. The different sections are clearly documented and custom commands are used to provide consistent formatting. The three main sections in the resume are education, experience, and projects.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-motivation" class="anchor" aria-hidden="true" href="#motivation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Motivation&lt;/h3&gt;
&lt;p&gt;I created this template as managing a resume on Google Docs was hard and changing any formatting was too difficult since it had to be applied in multiple places. Most currently available templates either focus on two columns, or are multiple pages long. I personally found the two-column templates hard to focus while multiple-page resumes were just too long to be used in career fairs.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/resume_preview.png"&gt;&lt;img src="/resume_preview.png" alt="Resume Screenshot" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;Format is MIT but all the data is owned by Sourabh Bajaj.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sb2nov</author><guid isPermaLink="false">https://github.com/sb2nov/resume</guid><pubDate>Fri, 07 Feb 2020 00:11:00 GMT</pubDate></item><item><title>zhanwen/MathModel #12 in TeX, Today</title><link>https://github.com/zhanwen/MathModel</link><description>&lt;p&gt;&lt;i&gt;研究生数学建模，数学建模竞赛优秀论文，数学建模算法，LaTeX论文模板，算法思维导图，参考书籍，Matlab软件教程，PPT&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-数学建模资源" class="anchor" aria-hidden="true" href="#数学建模资源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数学建模资源&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-2019-年研究生数模" class="anchor" aria-hidden="true" href="#2019-年研究生数模"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019 年研究生数模&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-20191111-又是双十一比赛结果经过一个半月的评审在这一天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo" class="anchor" aria-hidden="true" href="#20191111-又是双十一比赛结果经过一个半月的评审在这一天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019.11.11 又是双十一，比赛结果经过一个半月的评审，在这一天公布了&lt;a href="2019%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95"&gt;获奖名单&lt;/a&gt;，大家的努力相信都会有所收获。余生还有很多有意义的事情需要我们去做，让我们一起努力。(o^o)&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-20199192019923-比赛已经结束大家耐心等待获奖吧oo" class="anchor" aria-hidden="true" href="#20199192019923-比赛已经结束大家耐心等待获奖吧oo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019.9.19—2019.9.23 比赛已经结束，大家耐心等待获奖吧（(o^^o)）&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-论文提交md5使用方法" class="anchor" aria-hidden="true" href="#论文提交md5使用方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;论文提交（MD5使用方法）&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;MD5文件校验和使用说明：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/blob/master/MD5%E6%96%87%E4%BB%B6%E6%A0%A1%E9%AA%8C%E5%92%8C%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.md"&gt;&lt;strong&gt;MD5文件校验和使用说明&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-论文模版更新" class="anchor" aria-hidden="true" href="#论文模版更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;论文模版更新&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;LaTex 论文模版：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/blob/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/2019%E5%B9%B4Latex%E6%A8%A1%E7%89%88.zip"&gt;&lt;strong&gt;LaTex 论文模版&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;Word 论文模版：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/blob/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/%E2%80%9C%E5%8D%8E%E4%B8%BA%E6%9D%AF%E2%80%9D%E7%AC%AC%E5%8D%81%E5%85%AD%E5%B1%8A%E4%B8%AD%E5%9B%BD%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AE%BA%E6%96%87%E6%A0%BC%E5%BC%8F%E8%A7%84%E8%8C%83.doc"&gt;&lt;strong&gt;Word 论文模版（已更新最新）&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;LaTex 论文模版使用方式：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/tree/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/latex_note.md"&gt;&lt;strong&gt;如何编译 Latex 文件&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-下载方式仓库比较大建议单个文件下载" class="anchor" aria-hidden="true" href="#下载方式仓库比较大建议单个文件下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载方式(仓库比较大，建议单个文件下载)&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/downloaddemo2.gif"&gt;&lt;img src="./images/downloaddemo2.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt;  
&lt;p&gt;&lt;em&gt;主题：&lt;/em&gt; &lt;a href="https://cpipc.chinadegrees.cn/cw/hp/4" rel="nofollow"&gt;&lt;strong&gt;“华为杯”第十六届中国研究生数学建模竞赛&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;报名时间：&lt;/em&gt; &lt;strong&gt;2019年6月1日8:00——9月10日17:00&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;审核时间：&lt;/em&gt; &lt;strong&gt;2019年6月1日8:00——9月12日17:00&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;交费时间：&lt;/em&gt; &lt;strong&gt;2019年7月1日8:00——9月15日17:00&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;比赛时间：&lt;/em&gt; &lt;strong&gt;2019年9月19日8:00——9月23日12:00&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-官网报名地址官网地址" class="anchor" aria-hidden="true" href="#官网报名地址官网地址"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;官网报名地址：&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/4" rel="nofollow"&gt;官网地址&lt;/a&gt;&lt;/h4&gt;
&lt;hr&gt;  
&lt;h3&gt;&lt;a id="user-content-2018915-祝大家比赛开心-_" class="anchor" aria-hidden="true" href="#2018915-祝大家比赛开心-_"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018.9.15 祝大家比赛开心 （^_^）&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-2018919-比赛已经结束大家耐心等待获奖吧o" class="anchor" aria-hidden="true" href="#2018919-比赛已经结束大家耐心等待获奖吧o"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018.9.19 比赛已经结束，大家耐心等待获奖吧（^o^）&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-20181111-比赛结果经过一个半月的评审终于在昨天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo" class="anchor" aria-hidden="true" href="#20181111-比赛结果经过一个半月的评审终于在昨天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018.11.11 比赛结果经过一个半月的评审，终于在昨天公布了&lt;a href="https://github.com/zhanwen/MathModel/tree/master/2018%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/2018%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95"&gt;获奖名单&lt;/a&gt;，大家的努力相信都会有所收获。余生还有很多有意义的事情需要我们去做，让我们一起努力。(o^^o)&lt;/h4&gt;
&lt;hr&gt;  
&lt;h4&gt;&lt;a id="user-content-更新添加比赛官网地址戳这里" class="anchor" aria-hidden="true" href="#更新添加比赛官网地址戳这里"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新/添加比赛官网地址&lt;a href="https://cpipc.chinadegrees.cn/" rel="nofollow"&gt;戳这里&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/4" rel="nofollow"&gt;数学建模竞赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/6" rel="nofollow"&gt;电子设计竞赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/2c9088a5696cbf370169a3f8101510bd" rel="nofollow"&gt;人工智能创新大赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/2c9088a5696cbf370169a3f8934810be" rel="nofollow"&gt;机器人创新设计大赛&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-下载与使用由于整个项目直接下载比较慢可以看方式四" class="anchor" aria-hidden="true" href="#下载与使用由于整个项目直接下载比较慢可以看方式四"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载与使用（由于整个项目直接下载比较慢，可以看方式四）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;方式一：使用 &lt;code&gt;git&lt;/code&gt; 下载。&lt;br&gt;
&lt;code&gt;git clone https://github.com/zhanwen/MathModel.git&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方式二：直接下载压缩包。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/downloaddemo.gif"&gt;&lt;img src="./images/downloaddemo.gif" height="250" width="500" align="center" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方式三
&lt;ul&gt;
&lt;li&gt;可以单个文件下载，选择自己需要的某篇论文，直接在对应的页面点击下载即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/download3.gif"&gt;&lt;img src="./images/download3.gif" height="250" width="500" align="center" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方式四：百度云下载（推荐）
&lt;ul&gt;
&lt;li&gt;使用百度云下载，正常的客户端会出现限速，导致下载的很慢，这里给大家推荐一个绕过百度云下载限速的方式。具体怎么下载，请参照 &lt;a href="https://github.com/iikira/BaiduPCS-Go"&gt;绕过限速&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;该项目的百度云链接 &lt;a href="https://pan.baidu.com/s/1UnngHxNR0EVoyBpKlPxFAw" rel="nofollow"&gt;https://pan.baidu.com/s/1UnngHxNR0EVoyBpKlPxFAw&lt;/a&gt;，密码：&lt;code&gt;ea2n&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-国赛试题" class="anchor" aria-hidden="true" href="#国赛试题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;国赛试题&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2019%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2019年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2018%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2018年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2017%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2017年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2016%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2016年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2015%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2015年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2014%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2014年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2013%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2013年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2012%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2012年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2011%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2011年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2010%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2010年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2009%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2009年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2008%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2008年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2007%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2007年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2006%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2006年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2005%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2005年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2004%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2004年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-国赛论文" class="anchor" aria-hidden="true" href="#国赛论文"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;国赛论文&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2018年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：关于跳台跳水体型系数设置的建模分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：光传送网建模与价值评估&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：对恐怖袭击事件记录数据的量化分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：基于卫星高度计海面高度异常资料获取潮汐调和常数方法及应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：多无人机对组网雷达的协同干扰&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：航站楼扩增评估&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2017年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：无人机在抢险救灾中的优化运用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：面向下一代光通信的 VCSEL 激光器仿真模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：航班恢复问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：基于监控视频的前景目标提取&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：多波次导弹发射中的规划问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：地下物流系统网络&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2016年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：多无人机协同任务规划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：具有遗传性疾病和性状的遗传位点分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：基于无线通信基站的室内三维定位问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：军事行动避空侦察的时机和路线选择&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：粮食最低收购价政策问题研究&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2015年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：水面舰艇编队防空和信息化战争评估模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：数据的多流形结构分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：移动通信中的无线信道“指纹”特征建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：面向节能的单/多列车优化决策问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：数控加工刀具运动的优化控制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：旅游路线规划问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2014年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：小鼠视觉感受区电位信号(LFP)与视觉刺激之间的关系研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：机动目标的跟踪与反跟踪&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：无线通信中的快时变信道建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：人体营养健康角度的中国果蔬发展战略研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：乘用车物流运输计划问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2013年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：变循环发动机部件法建模及优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：功率放大器非线性特性及预失真模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：微蜂窝环境中无线接收信号的特性分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：空气中PM2.5问题的研究 attachment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：中等收入定位与人口度量模型研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：可持续的中国城乡居民养老保险体系的数学模型研究&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2012年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：基因识别问题及其算法实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：基于卫星无源探测的空间飞行器主动段轨道估计与误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：有杆抽油系统的数学建模及诊断&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：基于卫星云图的风失场(云导风)度量模型与算法探讨&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2011年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：基于光的波粒二象性一种猜想的数学仿真&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：吸波材料与微波暗室问题的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：小麦发育后期茎杆抗倒性的数学模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：房地产行业的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2010年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：确定肿瘤的重要基因信息&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：与封堵渍口有关的重物落水后运动过程的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：神经元的形态分类和识别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：特殊工件磨削加工的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2009年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：我国就业人数或城镇登记失业率的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：枪弹头痕迹，自动比对方法的研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：多传感器数据融合与航迹预测&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：110 警车配置及巡逻方案&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2008年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：汶川地震中唐家山堪塞湖泄洪问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：城市道路交通信号实时控制问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;C题：货运列车的编组调度问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;D题：中央空调系统节能设计问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2007年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：建立食品卫生安全保障体系数学模型及改进模型的若干理论问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：械臂运动路径设计问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：探讨提高高速公路路面质量的改进方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：邮政运输网络中的邮路规划和邮车调运&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2006年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：Ad Hoc 网络中的区域划分和资源分配问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：确定高精度参数问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：维修线性流量阀时的内筒设计问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：学生面试问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2005年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：Highway Traveling time Estimate and Optimal Routing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：空中加油&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：城市交通管理中的出租车规划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：仓库容量有限条件下的随机存贮管理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2004年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：发现黄球并定位&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：使用下料问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：售后服务数据的运用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：研究生录取问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-美赛论文" class="anchor" aria-hidden="true" href="#美赛论文"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;美赛论文&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2017年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2016年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2015年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2014年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2013年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2012年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2011年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2010年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2009年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2008年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2007年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2006年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2005年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2004年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-数学建模算法" class="anchor" aria-hidden="true" href="#数学建模算法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数学建模算法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AE%97%E6%B3%95"&gt;经典算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95"&gt;现代算法&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BB%BF%E7%9C%9F"&gt;计算机仿真&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95"&gt;粒子群算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE"&gt;马尔可夫链&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B3%95"&gt;蒙特卡洛法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E6%B3%95"&gt;模拟退火法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"&gt;神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E5%B0%8F%E6%B3%A2%E5%88%86%E6%9E%90"&gt;小波分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95"&gt;遗传算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-教材及课件" class="anchor" aria-hidden="true" href="#教材及课件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;教材及课件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E6%95%99%E6%9D%90%E5%8F%8A%E8%AF%BE%E4%BB%B6/%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6"&gt;国防科技术大学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E6%95%99%E6%9D%90%E5%8F%8A%E8%AF%BE%E4%BB%B6/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E8%AF%BE%E4%BB%B6/PPT%E8%AF%BE%E4%BB%B6"&gt;浙江大学课件&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-数学建模算法思维导图" class="anchor" aria-hidden="true" href="#数学建模算法思维导图"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数学建模算法思维导图&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/Mind"&gt;思维导图&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-matlab-入门教程" class="anchor" aria-hidden="true" href="#matlab-入门教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Matlab 入门教程&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/Matlab%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B"&gt;Matlab入门和在线性代数中的应用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt; 
&lt;h3&gt;&lt;a id="user-content-声明" class="anchor" aria-hidden="true" href="#声明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;声明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;其中有些内容整理自互联网，如有侵权，请联系，我将及时处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-个人微信公众号" class="anchor" aria-hidden="true" href="#个人微信公众号"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;个人微信公众号&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dotzhang&lt;/code&gt;：一名不羁的学僧，我的世界不只有学术。一条迷途的咸鱼，正在游向属于它的天地！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/images/donate/common.jpg"&gt;&lt;img src="/images/donate/common.jpg" width="150" height="150" alt="weixin" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-赞助和支持" class="anchor" aria-hidden="true" href="#赞助和支持"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;赞助和支持&lt;/h3&gt;
&lt;p&gt;这些内容都是我花了不少时间整理出来的, 如果你觉得它对你很有帮助, 请你也分享给需要学习的朋友们。如果你看好我的内容分享, 也可以考虑适当的赞助打赏, 让我有更多的动力去继续分享更好的内容给大家。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;微信&lt;/th&gt;
&lt;th&gt;支付宝&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/donate/weixinpay.jpg"&gt;&lt;img src="images/donate/weixinpay.jpg" width="150" height="150" alt="pay check" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/donate/alipay.jpg"&gt;&lt;img src="images/donate/alipay.jpg" width="150" height="150" alt="pay check" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-联系" class="anchor" aria-hidden="true" href="#联系"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;联系&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Email：&lt;a href="https://mail.google.com/" rel="nofollow"&gt;hanwenme@gmail.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;微  信（有任何问题都可以直接怼我）：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/donate/wechat.png"&gt;&lt;img src="images/donate/wechat.png" width="150" height="150" alt="pay check" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zhanwen</author><guid isPermaLink="false">https://github.com/zhanwen/MathModel</guid><pubDate>Fri, 07 Feb 2020 00:12:00 GMT</pubDate></item><item><title>pllk/cphb #13 in TeX, Today</title><link>https://github.com/pllk/cphb</link><description>&lt;p&gt;&lt;i&gt;Competitive Programmer's Handbook&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-competitive-programmers-handbook" class="anchor" aria-hidden="true" href="#competitive-programmers-handbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Competitive Programmer's Handbook&lt;/h1&gt;
&lt;p&gt;Competitive Programmer's Handbook is a modern introduction to competitive programming.
The book discusses programming tricks and algorithm design techniques relevant in competitive programming.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-cses-problem-set" class="anchor" aria-hidden="true" href="#cses-problem-set"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CSES Problem Set&lt;/h2&gt;
&lt;p&gt;The CSES Problem Set contains a collection of competitive programming problems.
You can practice the techniques presented in the book by solving the problems.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cses.fi/problemset/" rel="nofollow"&gt;https://cses.fi/problemset/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;The license of the book is Creative Commons BY-NC-SA.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-other-books" class="anchor" aria-hidden="true" href="#other-books"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other books&lt;/h2&gt;
&lt;p&gt;Guide to Competitive Programming is a printed book, published by Springer, based on Competitive Programmer's Handbook.
There is also a Russian edition Олимпиадное программирование (Olympiad Programming) and a Korean edition 알고리즘 트레이닝: 프로그래밍 대회 입문 가이드.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cses.fi/book/" rel="nofollow"&gt;https://cses.fi/book/&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pllk</author><guid isPermaLink="false">https://github.com/pllk/cphb</guid><pubDate>Fri, 07 Feb 2020 00:13:00 GMT</pubDate></item><item><title>ml874/Data-Science-Cheatsheet #14 in TeX, Today</title><link>https://github.com/ml874/Data-Science-Cheatsheet</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;strong&gt;Update (2019-12-18)&lt;/strong&gt;: The &lt;em&gt;Data Science Cheatsheet&lt;/em&gt; has evovled into a book! Check out &lt;em&gt;Cracking the Data Science Interview&lt;/em&gt; &lt;a href="https://www.amazon.com/dp/171068013X/ref=sr_1_8?keywords=cracking+the+data+science+interview&amp;amp;qid=1576688426&amp;amp;sr=8-8" rel="nofollow"&gt;here&lt;/a&gt;! This also means that the Cheatsheet will be getting a makeover soon- stay tuned!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-data-science-cheatsheet" class="anchor" aria-hidden="true" href="#data-science-cheatsheet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data Science Cheatsheet&lt;/h1&gt;
&lt;p&gt;This cheatsheet is currently a 9-page reference in basic data science that covers basic concepts in probability, statistics, statistical learning, machine learning, big data frameworks and SQL.&lt;/p&gt;
&lt;p&gt;The cheatsheet is loosely based off of &lt;em&gt;The Data Science Design Manual&lt;/em&gt; by Steven S. Skiena and &lt;em&gt;An Introduction to Statistical Learning&lt;/em&gt; by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.&lt;/p&gt;
&lt;p&gt;Inspired by William Chen's &lt;em&gt;The Only Probability Cheatsheet You'll Ever Need&lt;/em&gt;, located &lt;a href="https://github.com/wzchen/probability_cheatsheet"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-future-additions" class="anchor" aria-hidden="true" href="#future-additions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Additions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Graph Theory&lt;/li&gt;
&lt;li&gt;Algorithms and Data Structures&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Advanced SQL (SQL Part II)&lt;/li&gt;
&lt;li&gt;Data Science on the Cloud- AWS/GCP/Azure&lt;/li&gt;
&lt;li&gt;Linear Algebra&lt;/li&gt;
&lt;li&gt;Data Engineering&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-screenshots" class="anchor" aria-hidden="true" href="#screenshots"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Screenshots&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="Screenshots/screenshot1.png?raw=true"&gt;&lt;img src="Screenshots/screenshot1.png?raw=true" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="Screenshots/screenshot2.png?raw=true"&gt;&lt;img src="Screenshots/screenshot2.png?raw=true" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This work is licensed under a &lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License&lt;/a&gt;.
&lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/6887feb0136db5156c4f4146e3dd2681d06d9c75/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d6e632d73612f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;2018-08-13&lt;/strong&gt; Added Python Data Structures Section&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2018-08-12&lt;/strong&gt; Added Feature Engineering Section&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2018-08-10&lt;/strong&gt;: Added Data Science Cheat Sheet&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;Feel free to suggest comments, updates, and potential improvements!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maverick Lin&lt;/strong&gt;: Reach out to me via &lt;a href="https://www.quora.com/profile/Maverick-Lin" rel="nofollow"&gt;Quora&lt;/a&gt; or through my &lt;a href="http://mavericklin.com/" rel="nofollow"&gt;website&lt;/a&gt;. Cheers.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ml874</author><guid isPermaLink="false">https://github.com/ml874/Data-Science-Cheatsheet</guid><pubDate>Fri, 07 Feb 2020 00:14:00 GMT</pubDate></item><item><title>billryan/resume #15 in TeX, Today</title><link>https://github.com/billryan/resume</link><description>&lt;p&gt;&lt;i&gt;An elegant \LaTeX\ résumé template&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-résumé" class="anchor" aria-hidden="true" href="#résumé"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Résumé&lt;/h1&gt;
&lt;p&gt;Hit branch &lt;a href="https://github.com/billryan/resume/tree/zh_CN"&gt;zh_CN&lt;/a&gt; if you want a Simplified Chinese résumé.&lt;/p&gt;
&lt;p&gt;中文用户请前往 &lt;a href="https://github.com/billryan/resume/tree/zh_CN"&gt;zh_CN&lt;/a&gt; 分支。&lt;/p&gt;
&lt;p&gt;An elegant \LaTeX\ résumé template, compiled with \XeLaTeX. Inspired by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zachscrivena/simple-resume-cv"&gt;zachscrivena/simple-resume-cv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ctan.org/pkg/res" rel="nofollow"&gt;res&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianxu.net/en/files/JianXu_CV.pdf" rel="nofollow"&gt;JianXu's CV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stat.berkeley.edu/~paciorek/computingTips/Latex_template_creating_CV_.html" rel="nofollow"&gt;paciorek's CV/Resume template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.sharelatex.com/blog/2011/03/27/how-to-write-a-latex-class-file-and-design-your-own-cv.html" rel="nofollow"&gt;How to write a LaTeX class file and design your own CV (Part 1) - ShareLaTeX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Easy to further customize or extend&lt;/li&gt;
&lt;li&gt;Full support for unicode characters (e.g. CJK) with \XeLaTeX\&lt;/li&gt;
&lt;li&gt;Perfect Simplified Chinese fonts supported with Adobefonts&lt;/li&gt;
&lt;li&gt;FontAwesome 4.6.3 support&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fork this repository&lt;/li&gt;
&lt;li&gt;Add information about you directly in GitHub&lt;/li&gt;
&lt;li&gt;Compile TeX file to PDF with &lt;a href="https://latexonline.cc/" rel="nofollow"&gt;LaTeX.Online&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Can also use Overleaf for online compilation (&lt;a href="https://www.overleaf.com/" rel="nofollow"&gt;https://www.overleaf.com/&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-sample-output" class="anchor" aria-hidden="true" href="#sample-output"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample Output&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1292567/62409353-3fecfc00-b608-11e9-8e83-84962912c956.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1292567/62409353-3fecfc00-b608-11e9-8e83-84962912c956.png" alt="English" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1292567/62409351-3f546580-b608-11e9-9f6d-d232a68c5451.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1292567/62409351-3f546580-b608-11e9-9f6d-d232a68c5451.png" alt="English with photo" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1292567/62409352-3fecfc00-b608-11e9-8d9e-76243ca3052a.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1292567/62409352-3fecfc00-b608-11e9-8d9e-76243ca3052a.png" alt="简体中文" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/billryan/resume/files/3463503/resume.pdf"&gt;English PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/billryan/resume/files/3463501/resume_photo.pdf"&gt;English with photo PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/billryan/resume/files/3463502/resume-zh_CN.pdf"&gt;简体中文 PDF&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Edit in ShareLaTeX online - &lt;a href="https://www.sharelatex.com/templates/556b27cf0d23e5a8117053d9" rel="nofollow"&gt;https://www.sharelatex.com/templates/556b27cf0d23e5a8117053d9&lt;/a&gt;, &lt;strong&gt;no TeX software install!&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Compile tex on your Computer&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you only need a résumé in English or have installed Adobe Simplified Chinese on your OS, &lt;strong&gt;It would be better to clone only the master branch,&lt;/strong&gt; since the Simplified Chinese fonts files are too large.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/billryan/resume.git --branch master --depth 1 --single-branch &amp;lt;folder&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://opensource.org/licenses/MIT" rel="nofollow"&gt;The MIT License (MIT)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Copyrighted fonts are not subjected to this License.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>billryan</author><guid isPermaLink="false">https://github.com/billryan/resume</guid><pubDate>Fri, 07 Feb 2020 00:15:00 GMT</pubDate></item><item><title>StanfordVL/cs131_notes #16 in TeX, Today</title><link>https://github.com/StanfordVL/cs131_notes</link><description>&lt;p&gt;&lt;i&gt;Class notes for CS 131.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cs131-computer-vision--foundations-and-applications" class="anchor" aria-hidden="true" href="#cs131-computer-vision--foundations-and-applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CS131 Computer Vision:  Foundations and Applications&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-instructions-for-creating-class-notes" class="anchor" aria-hidden="true" href="#instructions-for-creating-class-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Instructions for creating class notes&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-overall-workflow" class="anchor" aria-hidden="true" href="#overall-workflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overall workflow&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Fork this repository by clicking the "Fork" button on the top right of &lt;a href="https://github.com/StanfordVL/cs131_notes"&gt;this
page&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, clone your forked repository into your local machine:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;git clone github.com/YOUR_GITHUB_ACCOUNT/cs131_notes.git
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Give others permission to commit to your forked repository by clicking on
"Settings" and then "Collaborators".&lt;/li&gt;
&lt;li&gt;Write up the class notes.&lt;/li&gt;
&lt;li&gt;Push your changes to your forked repository.&lt;/li&gt;
&lt;li&gt;Send a pull request to the official
&lt;a href="https://github.com/StanfordVL/cs131_notes"&gt;repository&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-downloading-the-software-and-testing-the-repository" class="anchor" aria-hidden="true" href="#downloading-the-software-and-testing-the-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading the software and testing the repository.&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Download &lt;a href="https://www.tug.org/applications/pdftex/" rel="nofollow"&gt;pdfltex&lt;/a&gt; so that you can
compile the tex documents. You can also use &lt;a href="overleaf.com"&gt;Overleaf&lt;/a&gt; or
&lt;a href="sharelatex.com"&gt;Sharelatex&lt;/a&gt; to compile your tex documents.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go into the template folder and compile the format and template files:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;cd template

pdflatex format.tex
bibtex format
pdflatex format.tex

pdflatex template.tex
bibtex template
pdflatex template.tex
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Make sure that template.pdf and format.pdf have been generated and are correctly
displayed. Read format.pdf to understand how we expect the class notes to be
formatted.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-writing-class-notes-for-a-given-lecture" class="anchor" aria-hidden="true" href="#writing-class-notes-for-a-given-lecture"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Writing class notes for a given lecture&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a folder called &lt;code&gt;LectureXX&lt;/code&gt; where &lt;code&gt;XX&lt;/code&gt; is &lt;code&gt;01&lt;/code&gt; for the first lecture
or &lt;code&gt;18&lt;/code&gt; for the eighteenth lecture.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copy over template.tex and bibliography.bib to your folder.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;cp template/template.tex lectureXX/lectureXX.tex
cp template/bibliography.bib lectureXX/bibliography.bib
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="3"&gt;
&lt;li&gt;
&lt;p&gt;Write the lecture notes. Make sure to include pictures, references and
tables. If you add images, make sure to give credit to the source of those
images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compile your lecture template to make sure that it formats correctly.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;cd lectureXX
pdflatex lectureXX.tex
bibtex lectureXX
pdflatex lectureXX.tex
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Submit a pull request to have your lecture notes merged.&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>StanfordVL</author><guid isPermaLink="false">https://github.com/StanfordVL/cs131_notes</guid><pubDate>Fri, 07 Feb 2020 00:16:00 GMT</pubDate></item></channel></rss>