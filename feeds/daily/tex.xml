<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: TeX, Today</title><link>https://github.com/trending/tex?since=daily</link><description>The top repositories on GitHub for tex, measured daily</description><pubDate>Sun, 09 Feb 2020 01:09:42 GMT</pubDate><lastBuildDate>Sun, 09 Feb 2020 01:09:42 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>huzecong/oi-slides #1 in TeX, Today</title><link>https://github.com/huzecong/oi-slides</link><description>&lt;p&gt;&lt;i&gt;我的信息学竞赛讲课课件&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-oi-slides" class="anchor" aria-hidden="true" href="#oi-slides"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;oi-slides&lt;/h1&gt;
&lt;p&gt;本仓库收集了我从2013至2018年的所有信息学竞赛讲课课件。本科期间靠讲课&lt;del&gt;骗&lt;/del&gt;挣了一些钱，但考虑到之后大概不会再出去讲课，受 &lt;a href="https://github.com/Trinkle23897/oi_slides"&gt;Trinkle23897/oi_slides&lt;/a&gt; 启发，在此公开。&lt;/p&gt;
&lt;p&gt;所有课件的使用时间和内容如下表所示。这其中以”题目选讲“命名的课件基本上都是斐波那契课件，由之前若干课件中的例题拼凑而成，没什么价值。许多课件在多次讲课中使用 &lt;del&gt;（反复骗钱）&lt;/del&gt; ，此处上传的均为最近一次的修订版。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;时间&lt;/th&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2013年3月&lt;/td&gt;
&lt;td&gt;&lt;a href="2013-topcoder-problems/2013.3_Vijos%E9%9B%86%E8%AE%AD_TopCoder%E9%A2%98%E7%9B%AE%E9%80%89%E8%AE%B2.pdf"&gt;TopCoder 题目选讲&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;如题。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2014年3月&lt;br&gt;2015年6月&lt;/td&gt;
&lt;td&gt;&lt;a href="2014-codechef-problems/slide.pdf"&gt;CodeChef 题目选讲&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;如题。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2014年12月&lt;/td&gt;
&lt;td&gt;&lt;a href="2014-interesting-problems/slide.pdf"&gt;趣题选讲&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;如题。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2015年5月&lt;br&gt;2015年6月&lt;br&gt;2016年1月&lt;br&gt;2018年1月&lt;/td&gt;
&lt;td&gt;&lt;a href="2015-flow-networks/%E7%BD%91%E7%BB%9C%E6%B5%81_final.pdf"&gt;网络流模型与例题 1&lt;/a&gt; &lt;a href="2015-flow-networks/%E5%A4%87%E7%94%A81/%E7%BD%91%E7%BB%9C%E6%B5%81_%E8%BF%9B%E9%98%B6%E7%AF%87_final.pdf"&gt;2&lt;/a&gt; &lt;a href="2015-flow-networks/%E5%A4%87%E7%94%A82/%E7%BD%91%E7%BB%9C%E6%B5%81_%E8%BF%9B%E9%98%B6%E7%AF%872.pdf"&gt;3&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;将常见的网络流模型分类，总结经验。算是我难得的良心课件。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2015年10月&lt;br&gt;2016年1月&lt;br&gt;2016年11月&lt;br&gt;2017年6月&lt;br&gt;2018年5月&lt;/td&gt;
&lt;td&gt;&lt;a href="2015-noip-graph-theory/%E5%9B%BE%E8%AE%BA_final.pdf"&gt;NOIp 图论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NOIp水平的图论知识，包括最短路、最小生成树、强连通分量。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2015年10月&lt;br&gt;2016年1月&lt;br&gt;2016年11月&lt;br&gt;2017年1月&lt;br&gt;2017年6月&lt;br&gt;2018年5月&lt;/td&gt;
&lt;td&gt;&lt;a href="2015-noip-math/%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95_final.pdf"&gt;NOIp 数学方法&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;NOIp水平的数论及组合数学知识，以及位运算的运用。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2016年1月&lt;/td&gt;
&lt;td&gt;&lt;a href="2016-data-structures/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.pdf"&gt;数据结构及应用&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;高级数据结构入门知识，包括平衡树、动态树、分块。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2016年5月&lt;br&gt;2017年1月&lt;/td&gt;
&lt;td&gt;&lt;a href="2016-dynamic-programming/slide.pdf"&gt;动态规划例题选讲&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;一些省选和NOI水平的动态规划题。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018年1月&lt;/td&gt;
&lt;td&gt;&lt;a href="2018-contest-strategies/%E8%80%83%E8%AF%95%E7%AD%96%E7%95%A5.pdf"&gt;考试策略的人生经验&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;多年考试总结出的人生经验。可能是最有用的一个课件。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018年3月&lt;/td&gt;
&lt;td&gt;&lt;a href="2018-linear-algebra-and-probability/slide_final.pdf"&gt;线性代数、概率与期望&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;线性代数和概率论的基础知识，以及在OI中省选水平的一些应用。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>huzecong</author><guid isPermaLink="false">https://github.com/huzecong/oi-slides</guid><pubDate>Sun, 09 Feb 2020 00:01:00 GMT</pubDate></item><item><title>posquit0/Awesome-CV #2 in TeX, Today</title><link>https://github.com/posquit0/Awesome-CV</link><description>&lt;p&gt;&lt;i&gt;:page_facing_up: Awesome CV is LaTeX template for your outstanding job application&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1 align="center"&gt;&lt;a id="user-content-------------awesome-cv" class="anchor" aria-hidden="true" href="#------------awesome-cv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
  &lt;a href="https://github.com/posquit0/Awesome-CV" title="AwesomeCV Documentation"&gt;
    &lt;img alt="AwesomeCV" src="https://github.com/posquit0/Awesome-CV/raw/master/icon.png" width="200px" height="200px" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;br&gt;
  Awesome CV
&lt;/h1&gt;
&lt;p align="center"&gt;
  LaTeX template for your outstanding job application
&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a href="https://www.paypal.me/posquit0" rel="nofollow"&gt;
    &lt;img alt="Donate" src="https://camo.githubusercontent.com/abbdd7bf97ae7919db5962b255f40aded5189c4f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d626c75652e737667" data-canonical-src="https://img.shields.io/badge/Donate-PayPal-blue.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://circleci.com/gh/posquit0/Awesome-CV" rel="nofollow"&gt;
    &lt;img alt="CircleCI" src="https://camo.githubusercontent.com/d42593802854990d35ca42943e478dd35d6c64c9/68747470733a2f2f636972636c6563692e636f6d2f67682f706f7371756974302f417765736f6d652d43562e7376673f7374796c653d736869656c64" data-canonical-src="https://circleci.com/gh/posquit0/Awesome-CV.svg?style=shield" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;
    &lt;img alt="Example Resume" src="https://camo.githubusercontent.com/836d3a9f44da3462e5c47b6c58bf066bffbaf739/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f726573756d652d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/resume-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/cv.pdf" rel="nofollow"&gt;
    &lt;img alt="Example CV" src="https://camo.githubusercontent.com/8afab53a91bc30d0da18a9ea0cc70f2d0a1571df/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f63762d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/cv-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;
    &lt;img alt="Example Coverletter" src="https://camo.githubusercontent.com/ce88ed0c1af9e5611df67818460447b69572ae9d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f7665726c65747465722d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/coverletter-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-awesome-cv" class="anchor" aria-hidden="true" href="#what-is-awesome-cv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is Awesome CV?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Awesome CV&lt;/strong&gt; is LaTeX template for a &lt;strong&gt;CV(Curriculum Vitae)&lt;/strong&gt;, &lt;strong&gt;Résumé&lt;/strong&gt; or &lt;strong&gt;Cover Letter&lt;/strong&gt; inspired by &lt;a href="https://www.sharelatex.com/templates/cv-or-resume/fancy-cv" rel="nofollow"&gt;Fancy CV&lt;/a&gt;. It is easy to customize your own template, especially since it is really written by a clean, semantic markup.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-donate" class="anchor" aria-hidden="true" href="#donate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Donate&lt;/h2&gt;
&lt;p&gt;Please help keep this project alive! Donations are welcome and will go towards further development of this project.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PayPal: paypal.me/posquit0
BTC: 1Je3DxJVM2a9nTVPNo55SfQwpmxA6N2KKb
BCH: 1Mg1wG7PwHGrHYSWS67TsGSjo5GHEVbF16
ETH: 0x77ED9B4659F80205E9B9C9FB1E26EDB9904AFCC7
QTUM: QZT7D6m3QtTTqp7s4ZWAwLtGDsoHMMaM8E
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Thank you for your support!&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-résumé" class="anchor" aria-hidden="true" href="#résumé"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Résumé&lt;/h4&gt;
&lt;p&gt;You can see &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Page. 1&lt;/th&gt;
&lt;th align="center"&gt;Page. 2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-0.png" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-1.png" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-cover-letter" class="anchor" aria-hidden="true" href="#cover-letter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cover Letter&lt;/h4&gt;
&lt;p&gt;You can see &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Without Sections&lt;/th&gt;
&lt;th align="center"&gt;With Sections&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-0.png" alt="Cover Letter(Traditional)" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-1.png" alt="Cover Letter(Awesome)" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.overleaf.com/latex/templates/awesome-cv/tvmzpvdjfqxp" rel="nofollow"&gt;&lt;strong&gt;Edit Résumé on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.overleaf.com/latex/templates/awesome-cv-cover-letter/pfzzjspkthbk" rel="nofollow"&gt;&lt;strong&gt;Edit Cover Letter on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note:&lt;/em&gt; Above services do not guarantee up-to-date source code of Awesome CV&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Use&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h4&gt;
&lt;p&gt;A full TeX distribution is assumed.  &lt;a href="http://tex.stackexchange.com/q/55437" rel="nofollow"&gt;Various distributions for different operating systems (Windows, Mac, *nix) are available&lt;/a&gt; but TeX Live is recommended.
You can &lt;a href="http://tex.stackexchange.com/q/1092" rel="nofollow"&gt;install TeX from upstream&lt;/a&gt; (recommended; most up-to-date) or use &lt;code&gt;sudo apt-get install texlive-full&lt;/code&gt; if you really want that.  (It's generally a few years behind.)&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h4&gt;
&lt;p&gt;At a command prompt, run&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ xelatex {your-cv}.tex&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This should result in the creation of &lt;code&gt;{your-cv}.pdf&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-credit" class="anchor" aria-hidden="true" href="#credit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credit&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.latex-project.org" rel="nofollow"&gt;&lt;strong&gt;LaTeX&lt;/strong&gt;&lt;/a&gt; is a fantastic typesetting program that a lot of people use these days, especially the math and computer science people in academia.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/furl/latex-fontawesome"&gt;&lt;strong&gt;LaTeX FontAwesome&lt;/strong&gt;&lt;/a&gt; is bindings for FontAwesome icons to be used in XeLaTeX.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/google/roboto"&gt;&lt;strong&gt;Roboto&lt;/strong&gt;&lt;/a&gt; is the default font on Android and ChromeOS, and the recommended font for Google’s visual language, Material Design.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/adobe-fonts/source-sans-pro"&gt;&lt;strong&gt;Source Sans Pro&lt;/strong&gt;&lt;/a&gt; is a set of OpenType fonts that have been designed to work well in user interface (UI) environments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;You are free to take my &lt;code&gt;.tex&lt;/code&gt; file and modify it to create your own resume. Please don't use my resume for anything else without my permission, though!&lt;/p&gt;
&lt;p&gt;If you have any questions, feel free to join me at &lt;code&gt;#posquit0&lt;/code&gt; on Freenode and ask away. Click &lt;a href="https://kiwiirc.com/client/irc.freenode.net/posquit0" rel="nofollow"&gt;here&lt;/a&gt; to connect.&lt;/p&gt;
&lt;p&gt;Good luck!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-see-also" class="anchor" aria-hidden="true" href="#see-also"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;See Also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/posquit0/hugo-awesome-identity"&gt;Awesome Identity&lt;/a&gt; - A single-page Hugo theme to introduce yourself.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>posquit0</author><guid isPermaLink="false">https://github.com/posquit0/Awesome-CV</guid><pubDate>Sun, 09 Feb 2020 00:02:00 GMT</pubDate></item><item><title>ryankeleti/ega #3 in TeX, Today</title><link>https://github.com/ryankeleti/ega</link><description>&lt;p&gt;&lt;i&gt;amateur translation project of Grothendieck's EGA.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ega" class="anchor" aria-hidden="true" href="#ega"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;EGA&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/4e1914a7d99668c07906a48d3defc671988ac1f4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f454741253230302d34362532352d79656c6c6f77"&gt;&lt;img src="https://camo.githubusercontent.com/4e1914a7d99668c07906a48d3defc671988ac1f4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f454741253230302d34362532352d79656c6c6f77" alt="EGA0status" data-canonical-src="https://img.shields.io/badge/EGA%200-46%25-yellow" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/60503bbfce66c5f22cd75cc645cb801d87a12ffe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f454741253230492d3130302532352d627269676874677265656e"&gt;&lt;img src="https://camo.githubusercontent.com/60503bbfce66c5f22cd75cc645cb801d87a12ffe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f454741253230492d3130302532352d627269676874677265656e" alt="EGA1status" data-canonical-src="https://img.shields.io/badge/EGA%20I-100%25-brightgreen" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/dc1724023f1ff56ce9f5daa22b4f4ff324be373f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f45474125323049492d33302532352d726564"&gt;&lt;img src="https://camo.githubusercontent.com/dc1724023f1ff56ce9f5daa22b4f4ff324be373f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f45474125323049492d33302532352d726564" alt="EGA2status" data-canonical-src="https://img.shields.io/badge/EGA%20II-30%25-red" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/080ba5e4e023572ae51888db9a64e56bfa820d0d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4547412532304949492d332532352d726564"&gt;&lt;img src="https://camo.githubusercontent.com/080ba5e4e023572ae51888db9a64e56bfa820d0d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4547412532304949492d332532352d726564" alt="EGA3status" data-canonical-src="https://img.shields.io/badge/EGA%20III-3%25-red" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/b77a3fb807ab81f3b3eed1cd8def15016c7083fc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f45474125323049562d312532352d726564"&gt;&lt;img src="https://camo.githubusercontent.com/b77a3fb807ab81f3b3eed1cd8def15016c7083fc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f45474125323049562d312532352d726564" alt="EGA4status" data-canonical-src="https://img.shields.io/badge/EGA%20IV-1%25-red" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Community translation (French to English) of A. Grothendieck's EGA.
S’il-vous plaît pardonnez-nous, Grothendieck.&lt;/p&gt;
&lt;p&gt;View online &lt;a href="https://ega.fppf.site/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To compile, &lt;code&gt;make book&lt;/code&gt;, &lt;code&gt;make pdfs&lt;/code&gt;, or &lt;code&gt;make all&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pdfs" class="anchor" aria-hidden="true" href="#pdfs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PDFs&lt;/h2&gt;
&lt;p&gt;There is the &lt;a href="https://fppf.site/ega/book-auto.pdf" rel="nofollow"&gt;&lt;strong&gt;full document&lt;/strong&gt;&lt;/a&gt;, or individual sections can be downloaded separately:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://fppf.site/ega/what-auto.pdf" rel="nofollow"&gt;What this is&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fppf.site/ega/intro-auto.pdf" rel="nofollow"&gt;Introduction&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fppf.site/ega/ega0-auto.pdf" rel="nofollow"&gt;EGA 0&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fppf.site/ega/ega1-auto.pdf" rel="nofollow"&gt;EGA I&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fppf.site/ega/ega2-auto.pdf" rel="nofollow"&gt;EGA II&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fppf.site/ega/ega3-auto.pdf" rel="nofollow"&gt;EGA III&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fppf.site/ega/ega4-auto.pdf" rel="nofollow"&gt;EGA IV&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fppf.site/ega/ref-auto.pdf" rel="nofollow"&gt;References&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All the PDFs are auto-compliled every hour if any changes have been made since the last auto-compile, so will always be up to date with the latest commit.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-current-status" class="anchor" aria-hidden="true" href="#current-status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Current status&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;First draft&lt;/th&gt;
&lt;th&gt;Proofreading&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1c31f85e66b3bdb005c73a2794537709b76d1125/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4547412d302d6c6967687467726579"&gt;&lt;img src="https://camo.githubusercontent.com/1c31f85e66b3bdb005c73a2794537709b76d1125/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4547412d302d6c6967687467726579" alt="EGA0" data-canonical-src="https://img.shields.io/badge/EGA-0-lightgrey" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/70c4e0da1ed00e677688a685dad680d9d83733ca/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d34362532352d79656c6c6f77"&gt;&lt;img src="https://camo.githubusercontent.com/70c4e0da1ed00e677688a685dad680d9d83733ca/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d34362532352d79656c6c6f77" alt="EGA0fd" data-canonical-src="https://img.shields.io/badge/-46%25-yellow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d2fe711f29c33e363ae0950fcdb8d4367f4463f8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d372532352d726564"&gt;&lt;img src="https://camo.githubusercontent.com/d2fe711f29c33e363ae0950fcdb8d4367f4463f8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d372532352d726564" alt="EGA0p" data-canonical-src="https://img.shields.io/badge/-7%25-red" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d65e120e8cf8ac048a8d0ffa198b081bf8868f2d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4547412d312d6c6967687467726579"&gt;&lt;img src="https://camo.githubusercontent.com/d65e120e8cf8ac048a8d0ffa198b081bf8868f2d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4547412d312d6c6967687467726579" alt="EGA1" data-canonical-src="https://img.shields.io/badge/EGA-1-lightgrey" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/9fe99e1bd3904528314d5c4e2ecbb182bb3637bc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d3130302532352d677265656e"&gt;&lt;img src="https://camo.githubusercontent.com/9fe99e1bd3904528314d5c4e2ecbb182bb3637bc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d3130302532352d677265656e" alt="EGA1fd" data-canonical-src="https://img.shields.io/badge/-100%25-green" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/9fe99e1bd3904528314d5c4e2ecbb182bb3637bc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d3130302532352d677265656e"&gt;&lt;img src="https://camo.githubusercontent.com/9fe99e1bd3904528314d5c4e2ecbb182bb3637bc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d3130302532352d677265656e" alt="EGA1p" data-canonical-src="https://img.shields.io/badge/-100%25-green" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/14a937054712f47025cdc146245f33a15aeff8cf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4547412d322d6c6967687467726579"&gt;&lt;img src="https://camo.githubusercontent.com/14a937054712f47025cdc146245f33a15aeff8cf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4547412d322d6c6967687467726579" alt="EGA2" data-canonical-src="https://img.shields.io/badge/EGA-2-lightgrey" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/9e897ac43bb89a334574bf62dacc29fb8ecd7904/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d33302532352d6f72616e6765"&gt;&lt;img src="https://camo.githubusercontent.com/9e897ac43bb89a334574bf62dacc29fb8ecd7904/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d33302532352d6f72616e6765" alt="EGA2fd" data-canonical-src="https://img.shields.io/badge/-30%25-orange" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5a277ab7b1e6b47e263b00165c32f6990f8f338f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d302532352d726564"&gt;&lt;img src="https://camo.githubusercontent.com/5a277ab7b1e6b47e263b00165c32f6990f8f338f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d302532352d726564" alt="EGA2p" data-canonical-src="https://img.shields.io/badge/-0%25-red" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/de8c6b0a6054acba07231fb127d6901b072899c5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4547412d332d6c6967687467726579"&gt;&lt;img src="https://camo.githubusercontent.com/de8c6b0a6054acba07231fb127d6901b072899c5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4547412d332d6c6967687467726579" alt="EGA3" data-canonical-src="https://img.shields.io/badge/EGA-3-lightgrey" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/cde8ec44125e99d0e8f58c723003180d0dd027fd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d332532352d726564"&gt;&lt;img src="https://camo.githubusercontent.com/cde8ec44125e99d0e8f58c723003180d0dd027fd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d332532352d726564" alt="EGA3fd" data-canonical-src="https://img.shields.io/badge/-3%25-red" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5a277ab7b1e6b47e263b00165c32f6990f8f338f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d302532352d726564"&gt;&lt;img src="https://camo.githubusercontent.com/5a277ab7b1e6b47e263b00165c32f6990f8f338f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d302532352d726564" alt="EGA3p" data-canonical-src="https://img.shields.io/badge/-0%25-red" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/3a00162e9a5ecb7d24dbb87dfe440b73e6e44e35/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4547412d342d6c6967687467726579"&gt;&lt;img src="https://camo.githubusercontent.com/3a00162e9a5ecb7d24dbb87dfe440b73e6e44e35/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4547412d342d6c6967687467726579" alt="EGA4" data-canonical-src="https://img.shields.io/badge/EGA-4-lightgrey" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c459381a95c49e7020a3d20c176b714357924403/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d312532352d726564"&gt;&lt;img src="https://camo.githubusercontent.com/c459381a95c49e7020a3d20c176b714357924403/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d312532352d726564" alt="EGA4fd" data-canonical-src="https://img.shields.io/badge/-1%25-red" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5a277ab7b1e6b47e263b00165c32f6990f8f338f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d302532352d726564"&gt;&lt;img src="https://camo.githubusercontent.com/5a277ab7b1e6b47e263b00165c32f6990f8f338f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d302532352d726564" alt="EGA4p" data-canonical-src="https://img.shields.io/badge/-0%25-red" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here is the current status of the translation, along with who is currently working on/has worked on which sections.&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Introduction (EGA I) &lt;em&gt;(proofread by @thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Preliminaries (EGA 0_I) &lt;em&gt;(proofread by @thosgood)&lt;/em&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 1. Rings of fractions &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 2. Irreducible spaces. Noetherian spaces &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 3. Supplement on sheaves &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 4. Ringed spaces &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 5. Quasi-coherent sheaves and coherent sheaves &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 6. Flatness &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 7. Adic rings &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Preliminaries (EGA 0_III)
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 8. Representable functors &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 9. Constructible sets &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 10. Supplement on flat modules &lt;em&gt;(@thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 11. Supplement on homological algebra &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 12. Supplement on sheaf cohomology&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 13. Projective limits in homological algebra&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Preliminaries (EGA 0_IV)
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; (14-ε). Summary &lt;em&gt;(@thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 14. Combinatorial dimension of a topological space &lt;em&gt;(@thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 15. M-regular and F-regular sequences&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 16. Dimension and depth of Noetherian local rings&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 17. Regular rings&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 18. Supplement on extensions of algebras&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 19. Formally smooth algebras and Cohen rings&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 20. Derivations and differentials&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 21. Differentials in rings of characteristic p&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 22. Differential criteria for smoothness and regularity&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 23. Japanese rings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; The language of schemes (EGA I) &lt;em&gt;(proofread by @thosgood)&lt;/em&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 0. Summary&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 1. Affine schemes &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 2. Preschemes and their morphisms &lt;em&gt;(@thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 3. Products of preschemes &lt;em&gt;(@thosgood, @ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 4. Subpreschemes and immersions &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 5. Reduced preschemes; separation condition &lt;em&gt;(@thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 6. Finiteness conditions &lt;em&gt;(@thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 7. Rational maps &lt;em&gt;(@thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 8. Chevalley schemes &lt;em&gt;(@thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 9. Supplement on quasi-coherent sheaves &lt;em&gt;(@thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 10. Formal schemes &lt;em&gt;(@thosgood, @ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Elementary global study of some classes of morphisms (EGA II)
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 0. Summary &lt;em&gt;(@ryankeleti / proofread by @thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 1. Affine morphisms &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 2. Homogeneous prime spectra&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 3. Homogeneous prime spectrum of a sheaf of graded algebras&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 4. Projective bundles; Ample sheaves&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 5. Quasi-affine morphisms; quasi-projective morphisms; proper morphisms; projective morphisms &lt;em&gt;(@thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 6. Integral morphisms and finite morphisms&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 7. Valuative criteria&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 8. Blowup schemes; based cones; projective closure &lt;em&gt;(@thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Cohomological study of coherent sheaves (EGA III)
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 0. Summary &lt;em&gt;(@thosgood / proofread by @thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 1. Cohomology of affine schemes &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 2. Cohomological study of projective morphisms&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 3. Finiteness theorem for proper morphisms &lt;em&gt;(@ryankeleti)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 4. The fundamental theorem of proper morphisms. Applications&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 5. An existence theorem for coherent algebraic sheaves&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 6. Local and global Tor functors; Künneth formula&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 7. Base change for homological functors of sheaves of modules&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 8. &lt;del&gt;The duality theorem for projective bundles&lt;/del&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 9. &lt;del&gt;Relative cohomology and local cohomology; local duality&lt;/del&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 10. &lt;del&gt;Relations between projective cohomology and local cohomology. Formal completion technique along a divisor&lt;/del&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 11. &lt;del&gt;Global and local Picard groups&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Local study of schemes and their morphisms (EGA IV)
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 0. Summary &lt;em&gt;(@thosgood)&lt;/em&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 1. Relative finiteness conditions. Constructible sets of preschemes&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 2. Base change and flatness&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 3. Associated prime cycles and primary decomposition&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 4. Change of base field for algebraic preschemes&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 5. Dimension, depth, and regularity of locally Noetherian preschemes&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 6. Flat morphisms of locally Noetherian preschemes&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 7. Relations between a local Noetherian ring and its completion. Excellent rings&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 8. Projective limits of preschemes&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 9. Constructible properties&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 10. Jacobson preschemes&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 11. Topological properties of finitely presented flat morphisms. Flatness criteria&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 12. Fibres of finitely presented flat morphisms&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 13. Equidimensional morphisms&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 14. Universally open morphisms&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 15. Fibres of a universally open morphism&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 16. Differential invariants. Differentially smooth morphisms&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 17. Smooth morphisms, unramified morphisms, and étale morphisms&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 18. Supplement on étale morphisms. Henselian local rings and strictly local rings&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 19. Regular immersions and normal flatness&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 20. Meromorphic functions and pseudo-morphisms&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 21. Divisors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ryankeleti</author><guid isPermaLink="false">https://github.com/ryankeleti/ega</guid><pubDate>Sun, 09 Feb 2020 00:03:00 GMT</pubDate></item><item><title>ElegantLaTeX/ElegantBook #4 in TeX, Today</title><link>https://github.com/ElegantLaTeX/ElegantBook</link><description>&lt;p&gt;&lt;i&gt;ElegantBook: An Elegant LaTeX Template for Books&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;

&lt;p&gt;&lt;a href="https://elegantlatex.org/" rel="nofollow"&gt;Homepage&lt;/a&gt; | &lt;a href="https://github.com/ElegantLaTeX/ElegantBook"&gt;Github&lt;/a&gt; | &lt;a href="https://ctan.org/pkg/elegantbook" rel="nofollow"&gt;CTAN&lt;/a&gt; | &lt;a href="https://github.com/ElegantLaTeX/ElegantBook/releases"&gt;Download&lt;/a&gt; | &lt;a href="https://github.com/ElegantLaTeX/ElegantBook/wiki"&gt;Wiki&lt;/a&gt; | &lt;a href="https://weibo.com/elegantlatex" rel="nofollow"&gt;Weibo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/684c58d836f28bc324a251c3e753ba3c71481820/68747470733a2f2f696d672e736869656c64732e696f2f6374616e2f6c2f656c6567616e74626f6f6b2e737667"&gt;&lt;img src="https://camo.githubusercontent.com/684c58d836f28bc324a251c3e753ba3c71481820/68747470733a2f2f696d672e736869656c64732e696f2f6374616e2f6c2f656c6567616e74626f6f6b2e737667" alt="License" data-canonical-src="https://img.shields.io/ctan/l/elegantbook.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/7f4ad8ba6dbdebc13f4e65c6d14dcfdcff981d14/68747470733a2f2f696d672e736869656c64732e696f2f6374616e2f762f656c6567616e74626f6f6b2e737667"&gt;&lt;img src="https://camo.githubusercontent.com/7f4ad8ba6dbdebc13f4e65c6d14dcfdcff981d14/68747470733a2f2f696d672e736869656c64732e696f2f6374616e2f762f656c6567616e74626f6f6b2e737667" alt="CTAN Version" data-canonical-src="https://img.shields.io/ctan/v/elegantbook.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ec62f75aa0ce5de71152335ce0f43a9be1c871ec/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f456c6567616e744c615465582f456c6567616e74426f6f6b2e737667"&gt;&lt;img src="https://camo.githubusercontent.com/ec62f75aa0ce5de71152335ce0f43a9be1c871ec/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f456c6567616e744c615465582f456c6567616e74426f6f6b2e737667" alt="Github Version" data-canonical-src="https://img.shields.io/github/release/ElegantLaTeX/ElegantBook.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/546a4981217448d5310b03136e37e1d8a38d6f74/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7265706f2d73697a652f456c6567616e744c615465582f456c6567616e74426f6f6b2e737667"&gt;&lt;img src="https://camo.githubusercontent.com/546a4981217448d5310b03136e37e1d8a38d6f74/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7265706f2d73697a652f456c6567616e744c615465582f456c6567616e74426f6f6b2e737667" alt="Repo Size" data-canonical-src="https://img.shields.io/github/repo-size/ElegantLaTeX/ElegantBook.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-elegantbook-优美的-latex-书籍模板" class="anchor" aria-hidden="true" href="#elegantbook-优美的-latex-书籍模板"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ElegantBook 优美的 LaTeX 书籍模板&lt;/h1&gt;
&lt;p&gt;ElegantBook 是为 LaTeX 书籍写作而设计的模板，由 &lt;a href="https://ddswhu.me/" rel="nofollow"&gt;Dongsheng Deng&lt;/a&gt; 和 &lt;a href="https://liam.page/" rel="nofollow"&gt;Liam Huang&lt;/a&gt; 创立，模板创立的初衷是方便我们自己做笔记 &lt;g-emoji class="g-emoji" alias="smile" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png"&gt;😄&lt;/g-emoji&gt;。如果你有其他问题、建议或者报告 bug，可以提交 issues 或者给我们发邮件：&lt;a href="mailto:elegantlatex2e@gmail.com"&gt;elegantlatex2e@gmail.com&lt;/a&gt;。QQ 用户交流群：692108391，欢迎加入。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-重要提示" class="anchor" aria-hidden="true" href="#重要提示"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;重要提示&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;版本提醒：&lt;/strong&gt; 由于新版本进行了重构，并且 3.06 版本对于封面进行了改版，3.06 以后的版本并不兼容之前版本，如果你想把之前版本的文件转为 3.06 以后的版本，请查看&lt;a href="https://github.com/ElegantLaTeX/ElegantBook/wiki/convert"&gt;跨版本转换&lt;/a&gt;。我们强烈建议你使用最新版。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：ElegantLaTeX 项目 &lt;strong&gt;不接受&lt;/strong&gt; 任何非作者预约提交（pull requests）！&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-致谢" class="anchor" aria-hidden="true" href="#致谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;致谢&lt;/h2&gt;
&lt;p&gt;2019 年 5 月 20 日，ElegantBook 模板在 Github 上的 Star 数达到了 100，这对于 ElegantLaTeX 系列模板是一个非常重要的里程碑！在此特别感谢 ChinaTeX 以及 &lt;a href="http://www.latexstudio.net/" rel="nofollow"&gt;LaTeX 工作室&lt;/a&gt;对于本系列模板的大力宣传与推广。LaTeX 工作室网站上有很多精彩的帖子和精致的模板，欢迎大家去挖掘里面的宝藏。这也是我所见到的国内最全面的 LaTeX 相关的网站。&lt;/p&gt;
&lt;p&gt;如果你喜欢我们的模板，你可以在 Github 上收藏我们的模板。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-捐赠" class="anchor" aria-hidden="true" href="#捐赠"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;捐赠&lt;/h2&gt;
&lt;p&gt;如果您非常喜爱我们的模板或者我，你还可以选择捐赠以表达您对我们模板和我的支持。本模板自 3.08 版本发布了捐赠信息之后，收到了超过千元的捐赠（四舍五入就是一个亿），非常感谢！&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/ElegantLaTeX/ElegantBook/wiki/donate.jpg"&gt;&lt;img src="https://github.com/ElegantLaTeX/ElegantBook/wiki/donate.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;赞赏费用的使用解释权归 ElegantLaTeX 所有，并且不接受监督，请自愿理性打赏&lt;/strong&gt;。10 元以上的赞赏，我们将列入捐赠榜，谢谢各位金主！（如果有遗漏，请务必联系我们，这对我们很重要）&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;捐赠者&lt;/th&gt;
&lt;th align="center"&gt;金额&lt;/th&gt;
&lt;th align="center"&gt;时间&lt;/th&gt;
&lt;th align="center"&gt;渠道&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Lerh&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/05/15&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;越过地平线&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/05/15&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;银桑&lt;/td&gt;
&lt;td align="center"&gt;20 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/05/27&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;*空&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/05/30&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.latexstudio.net" rel="nofollow"&gt;latexstudio.net&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;666 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/06/05&lt;/td&gt;
&lt;td align="center"&gt;支付宝&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Cassis&lt;/td&gt;
&lt;td align="center"&gt;11 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/06/30&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;*君&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/07/23&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;P*u&lt;/td&gt;
&lt;td align="center"&gt;50 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/07/30&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;*萌&lt;/td&gt;
&lt;td align="center"&gt;19 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/08/28&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;曲豆豆&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/08/28&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;李博&lt;/td&gt;
&lt;td align="center"&gt;100 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/10/06&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Njustsll&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/10/11&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;刘志阔&lt;/td&gt;
&lt;td align="center"&gt;99.99 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/10/15&lt;/td&gt;
&lt;td align="center"&gt;支付宝&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;*涛&lt;/td&gt;
&lt;td align="center"&gt;16 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/10/17&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;赤霓&lt;/td&gt;
&lt;td align="center"&gt;12 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/10/17&lt;/td&gt;
&lt;td align="center"&gt;支付宝&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;追寻原风景&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/10/28&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;郭德良&lt;/td&gt;
&lt;td align="center"&gt;88 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/11/03&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;自强不息&lt;/td&gt;
&lt;td align="center"&gt;20 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/11/04&lt;/td&gt;
&lt;td align="center"&gt;支付宝&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;读书之虫&lt;/td&gt;
&lt;td align="center"&gt;20 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/11/18&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;*等&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/11/18&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;*哲&lt;/td&gt;
&lt;td align="center"&gt;20 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/11/18&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;佚名&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/11/24&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Jiye Qian&lt;/td&gt;
&lt;td align="center"&gt;66 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/12/04&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;* 阳&lt;/td&gt;
&lt;td align="center"&gt;20 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/12/05&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Catcher&lt;/td&gt;
&lt;td align="center"&gt;11 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/12/08&lt;/td&gt;
&lt;td align="center"&gt;支付宝&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;希尔波特门徒&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/12/09&lt;/td&gt;
&lt;td align="center"&gt;支付宝&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;* 伟&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/12/09&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Simon&lt;/td&gt;
&lt;td align="center"&gt;20 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/12/11&lt;/td&gt;
&lt;td align="center"&gt;支付宝&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;流殇丶浅忆&lt;/td&gt;
&lt;td align="center"&gt;66.60 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/12/18&lt;/td&gt;
&lt;td align="center"&gt;支付宝&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;羽&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/12/20&lt;/td&gt;
&lt;td align="center"&gt;支付宝&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;* 琛&lt;/td&gt;
&lt;td align="center"&gt;15 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/12/20&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;随风&lt;/td&gt;
&lt;td align="center"&gt;20 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/12/27&lt;/td&gt;
&lt;td align="center"&gt;支付宝&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Ws&lt;/td&gt;
&lt;td align="center"&gt;23.30 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/12/28&lt;/td&gt;
&lt;td align="center"&gt;微信&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;再次感谢大家对于模板的喜爱！&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-协议" class="anchor" aria-hidden="true" href="#协议"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;协议&lt;/h2&gt;
&lt;p&gt;本模板发布遵循 LaTeX 项目公共许可证 1.3 c 或更高版本。如果是衍生作品，请务必加入协议声明和模板信息（github、CTAN 地址）。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-衍生作" class="anchor" aria-hidden="true" href="#衍生作"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;衍生作&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/XiangyunHuang/ElegantBookdown"&gt;ElegantBookdown&lt;/a&gt;：&lt;a href="https://github.com/XiangyunHuang"&gt;XiangyunHuang&lt;/a&gt; 开发并维护的基于 ElegantBook 的 Bookdown 模板。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pzhaonet/bookdownplus"&gt;bookdownplus&lt;/a&gt;：应网友要求，&lt;a href="https://github.com/pzhaonet"&gt;pzhaonet&lt;/a&gt; 在 bookdownplus 收录了 ElegantPaper 模板，并为 Mac 做了字体适配。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/annProg/PanBook"&gt;PanBook&lt;/a&gt;：&lt;a href="https://github.com/annProg"&gt;annProg&lt;/a&gt; 开发并维护的基于 Markdown 写作的工作流，收录了 ElegantBook 和 ElegantPaper 模板。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-elegantbook-an-elegant-latex-template-for-books" class="anchor" aria-hidden="true" href="#elegantbook-an-elegant-latex-template-for-books"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ElegantBook: An Elegant LaTeX Template for Books&lt;/h1&gt;
&lt;p&gt;ElegantBook is designed for writing books, created by &lt;a href="https://ddswhu.me/" rel="nofollow"&gt;Dongsheng Deng&lt;/a&gt; and &lt;a href="https://liam.page/" rel="nofollow"&gt;Liam Huang&lt;/a&gt;. Just enjoy it! If you have any questions, suggestions or bug reports, you can create issues or contact us at &lt;a href="mailto:elegantlatex2e@gmail.com"&gt;elegantlatex2e@gmail.com&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-important-notes" class="anchor" aria-hidden="true" href="#important-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Important Notes&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Version Note&lt;/strong&gt;: Users for 2.x, please refer to &lt;a href="https://github.com/ElegantLaTeX/ElegantBook/wiki/convert"&gt;Version Convert&lt;/a&gt;. We strongly recommend that you use the latest version.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important Info&lt;/strong&gt;: For some reasons, &lt;strong&gt;unauthorial&lt;/strong&gt; pull requests are &lt;strong&gt;UNACCEPTABLE&lt;/strong&gt; since May 20, 2019. For those who want to help revise the templates, submit issues or clone to your own repository to modify under the LPPL-1.3c.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;News&lt;/em&gt;: &lt;em&gt;The number of stars on Github for ElegantBook reached 100 on May 20, 2019&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Thank ChinaTeX and &lt;a href="http://www.latexstudio.net/" rel="nofollow"&gt;LaTeX Studio&lt;/a&gt; for their promotion. LaTeX studio offers tons of valuable posts and templates for discovery. It is the most comprehensive website on LaTeX in China. If you like our template, star on Github for supporting us.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-donation" class="anchor" aria-hidden="true" href="#donation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Donation&lt;/h2&gt;
&lt;p&gt;To express your love for our templates and/or our developers, please do not hesitate to tip us. Since last release of 3.08, we have received about a thousand RMB! (The emergence of a millionaire is on the way. Loading... )&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/ElegantLaTeX/ElegantBook/wiki/donate.jpg"&gt;&lt;img src="https://github.com/ElegantLaTeX/ElegantBook/wiki/donate.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The explanation right of the tip usage belongs to ElegantLaTeX with no supervision. Feel free to tip us. Those who donate more than 10 RMB will be recorded in the Donation List. Thank all the tippers! Thank you all.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Tipper&lt;/th&gt;
&lt;th align="center"&gt;Amount&lt;/th&gt;
&lt;th align="center"&gt;Date&lt;/th&gt;
&lt;th align="center"&gt;Channel&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Lerh&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/5/15&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;YueGuoDiPingXian&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/5/15&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;YinSang&lt;/td&gt;
&lt;td align="center"&gt;20 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/5/27&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;* Kong&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/05/30&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.latexstudio.net" rel="nofollow"&gt;latexstudio.net&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;666 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/06/05&lt;/td&gt;
&lt;td align="center"&gt;Alipay&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Cassis&lt;/td&gt;
&lt;td align="center"&gt;11 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/06/30&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;* Jun&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/07/23&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;P*u&lt;/td&gt;
&lt;td align="center"&gt;50 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/07/30&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;* Meng&lt;/td&gt;
&lt;td align="center"&gt;19 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/08/28&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;QuDouDou&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/08/28&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;LI Bo&lt;/td&gt;
&lt;td align="center"&gt;100 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/10/06&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Njustsll&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/10/11&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;LIU ZhiKuo&lt;/td&gt;
&lt;td align="center"&gt;99.99 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/10/15&lt;/td&gt;
&lt;td align="center"&gt;Alipay&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;* Tao&lt;/td&gt;
&lt;td align="center"&gt;16 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/10/17&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;ChiHong&lt;/td&gt;
&lt;td align="center"&gt;12 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/10/17&lt;/td&gt;
&lt;td align="center"&gt;Alipay&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;YuanFengJing&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/10/28&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;GUO DeLiang&lt;/td&gt;
&lt;td align="center"&gt;88 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/11/03&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;ZiQiangBuXi&lt;/td&gt;
&lt;td align="center"&gt;20 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/11/03&lt;/td&gt;
&lt;td align="center"&gt;Alipay&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DuShuZhiChong&lt;/td&gt;
&lt;td align="center"&gt;20 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/11/18&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;* Deng&lt;/td&gt;
&lt;td align="center"&gt;10 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/11/18&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;* Zhe&lt;/td&gt;
&lt;td align="center"&gt;20 RMB&lt;/td&gt;
&lt;td align="center"&gt;2019/11/18&lt;/td&gt;
&lt;td align="center"&gt;Wechat&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This work is released under the LaTeX Project Public License, v1.3c or later.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-derivative-works" class="anchor" aria-hidden="true" href="#derivative-works"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Derivative Works&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/XiangyunHuang/ElegantBookdown"&gt;ElegantBookdown&lt;/a&gt;：&lt;a href="https://github.com/XiangyunHuang"&gt;XiangyunHuang&lt;/a&gt; developed a Bookdown template based on ElegantBook.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pzhaonet/bookdownplus"&gt;bookdownplus&lt;/a&gt;: maintained by &lt;a href="https://github.com/pzhaonet"&gt;pzhaonet&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/annProg/PanBook"&gt;PanBook&lt;/a&gt;：a markdown-based writing workflow Developed by &lt;a href="https://github.com/annProg"&gt;annProg&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ElegantLaTeX</author><guid isPermaLink="false">https://github.com/ElegantLaTeX/ElegantBook</guid><pubDate>Sun, 09 Feb 2020 00:04:00 GMT</pubDate></item><item><title>emorynlp/nlprankings #5 in TeX, Today</title><link>https://github.com/emorynlp/nlprankings</link><description>&lt;p&gt;&lt;i&gt;Ranking of Top Institutes for Natural Language Processing (NLP)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-nlprankings" class="anchor" aria-hidden="true" href="#nlprankings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLPRankings&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://nlprankings.org" rel="nofollow"&gt;http://nlprankings.org&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.mathcs.emory.edu/~choi" rel="nofollow"&gt;Jinho D. Choi&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>emorynlp</author><guid isPermaLink="false">https://github.com/emorynlp/nlprankings</guid><pubDate>Sun, 09 Feb 2020 00:05:00 GMT</pubDate></item><item><title>deedy/Deedy-Resume #6 in TeX, Today</title><link>https://github.com/deedy/Deedy-Resume</link><description>&lt;p&gt;&lt;i&gt;A one page , two asymmetric column resume template in XeTeX that caters to an undergraduate Computer Science student&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deedy-resume" class="anchor" aria-hidden="true" href="#deedy-resume"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deedy-Resume&lt;/h1&gt;
&lt;p&gt;A &lt;strong&gt;one-page&lt;/strong&gt;, &lt;strong&gt;two asymmetric column&lt;/strong&gt; resume template in &lt;strong&gt;XeTeX&lt;/strong&gt; that caters particularly to an &lt;strong&gt;undergraduate Computer Science&lt;/strong&gt; student.
As of &lt;strong&gt;v1.2&lt;/strong&gt;, there is an option to choose from two templates:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;MacFonts&lt;/strong&gt; - uses fonts native to OSX - &lt;em&gt;Helvetica&lt;/em&gt;, &lt;em&gt;Helvetica Neue&lt;/em&gt; (and it's Light and Ultralight versions) and the CJK fonts &lt;em&gt;Heiti SC&lt;/em&gt;, and &lt;em&gt;Heiti TC&lt;/em&gt;. The EULA of these fonts prevents distribution on Open Source.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenFonts&lt;/strong&gt; - uses free, open-source fonts that resemble the above - &lt;em&gt;Lato&lt;/em&gt; (and its various variants) and &lt;em&gt;Raleway&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is licensed under the Apache License 2.0.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-motivation" class="anchor" aria-hidden="true" href="#motivation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Common LaTeX resume-builders such as &lt;a href="http://www.latextemplates.com/template/moderncv-cv-and-cover-letter" rel="nofollow"&gt;&lt;strong&gt;moderncv&lt;/strong&gt;&lt;/a&gt;  and the &lt;a href="https://github.com/afriggeri/cv"&gt;&lt;strong&gt;friggeri-cv&lt;/strong&gt;&lt;/a&gt; look great if you're looking for a multi-page resume with numerous citations, but usually imperfect for making a thorough, single-page one. A lot of companies today search resumes based on &lt;a href="http://www.businessinsider.com/most-big-companies-have-a-tracking-system-that-scans-your-resume-for-keywords-2012-1" rel="nofollow"&gt;keywords&lt;/a&gt; but at the same time require/prefer a one-page resume, especially for undergraduates.&lt;/p&gt;
&lt;p&gt;This template attempts to &lt;strong&gt;look clean&lt;/strong&gt;, highlight &lt;strong&gt;details&lt;/strong&gt;, be a &lt;strong&gt;single page&lt;/strong&gt;, and allow useful &lt;strong&gt;LaTeX templating&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-openfonts" class="anchor" aria-hidden="true" href="#openfonts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenFonts&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/OpenFonts/sample-image.png"&gt;&lt;img src="https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/OpenFonts/sample-image.png" alt="alt tag" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-macfonts" class="anchor" aria-hidden="true" href="#macfonts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MacFonts&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/MacFonts/sample-image.png"&gt;&lt;img src="https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/MacFonts/sample-image.png" alt="alt tag" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Compiles only with &lt;strong&gt;XeTeX&lt;/strong&gt; and required &lt;strong&gt;BibTex&lt;/strong&gt; for compiling publications and the .bib filetype.&lt;/li&gt;
&lt;li&gt;Uses fonts that are usually only available to &lt;strong&gt;Mac&lt;/strong&gt; users such as Helvetica Neue Light.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-availability" class="anchor" aria-hidden="true" href="#availability"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Availability&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;MacFonts version - &lt;a href="http://debarghyadas.com/resume/debarghya-das-resume.pdf" rel="nofollow"&gt;as an online preview&lt;/a&gt; and &lt;a href="https://github.com/deedydas/Deedy-Resume/raw/master/MacFonts/deedy_resume.pdf"&gt;as a direct download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenFonts version - &lt;a href="https://github.com/deedydas/Deedy-Resume/raw/master/OpenFonts/deedy_resume-openfont.pdf"&gt;as a direct download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overleaf&lt;/strong&gt;.com (formerly &lt;strong&gt;WriteLatex&lt;/strong&gt;.com) (v1 fonts/colors changed) - &lt;a href="https://www.writelatex.com/templates/deedy-resume/sqdbztjjghvz#.U2H9Kq1dV18" rel="nofollow"&gt;compilable online&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ShareLatex&lt;/strong&gt;.com (v1 fonts changes) - &lt;a href="https://www.sharelatex.com/templates/cv-or-resume/deedy-resume" rel="nofollow"&gt;compilable online&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-v12" class="anchor" aria-hidden="true" href="#v12"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;v1.2&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Added publications in place of societies.&lt;/li&gt;
&lt;li&gt;Collapsed a portion of education.&lt;/li&gt;
&lt;li&gt;Fixed a bug with alignment of overflowing long last updated dates on the top right.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-v11" class="anchor" aria-hidden="true" href="#v11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;v1.1&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Fixed several compilation bugs with \renewcommand&lt;/li&gt;
&lt;li&gt;Got Open-source fonts (Windows/Linux support)&lt;/li&gt;
&lt;li&gt;Added Last Updated&lt;/li&gt;
&lt;li&gt;Moved Title styling into .sty&lt;/li&gt;
&lt;li&gt;Commented .sty file.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Merge OpenFont and MacFonts as a single sty with options.&lt;/li&gt;
&lt;li&gt;Figure out a smoother way for the document to flow onto the next page.&lt;/li&gt;
&lt;li&gt;Add styling information for a "Projects/Hacks" section.&lt;/li&gt;
&lt;li&gt;Add location/address information&lt;/li&gt;
&lt;li&gt;Fix the hacky 'References' omission outside the .cls file in the MacFonts version.&lt;/li&gt;
&lt;li&gt;Add various styling and section options and allow for multiple pages smoothly.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-known-issues" class="anchor" aria-hidden="true" href="#known-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Known Issues:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Overflows onto second page if any column's contents are more than the vertical limit&lt;/li&gt;
&lt;li&gt;Hacky space on the first bullet point on the second column.&lt;/li&gt;
&lt;li&gt;Hacky redefinition of \refname to omit 'References' text for publications in the MacFonts version.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Copyright 2014 Debarghya Das

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>deedy</author><guid isPermaLink="false">https://github.com/deedy/Deedy-Resume</guid><pubDate>Sun, 09 Feb 2020 00:06:00 GMT</pubDate></item><item><title>spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex #7 in TeX, Today</title><link>https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex</link><description>&lt;p&gt;&lt;i&gt;Write Beautiful Curriculum Vitae in LaTex, that ensures twenty seconds reading.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-twenty-seconds-curriculum-vitae-in-latex" class="anchor" aria-hidden="true" href="#twenty-seconds-curriculum-vitae-in-latex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Twenty Seconds Curriculum Vitae in LaTex&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://www.latex-project.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/622808a2e3e436763ab2cd995e82e090d48d08cc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d616465253230776974682d4c615465582d3166343235662e737667" alt="made-with-latex" data-canonical-src="https://img.shields.io/badge/Made%20with-LaTeX-1f425f.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://lbesson.mit-license.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/311762166ef25238116d3cadd22fcb6091edab98/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667" alt="MIT license" data-canonical-src="https://img.shields.io/badge/License-MIT-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex"&gt;&lt;img src="https://camo.githubusercontent.com/94cee5f9e0ae21b66fbe829b4eb9f0419af5a0ca/68747470733a2f2f6261646765732e66726170736f66742e636f6d2f6f732f76332f6f70656e2d736f757263652e7376673f763d313033" alt="Open Source Love svg3" data-canonical-src="https://badges.frapsoft.com/os/v3/open-source.svg?v=103" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/graphs/commit-activity"&gt;&lt;img src="https://camo.githubusercontent.com/0e6a3f975d68b438efec82fef1f9491600606df8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d61696e7461696e65642533462d7965732d677265656e2e737667" alt="Maintenance" data-canonical-src="https://img.shields.io/badge/Maintained%3F-yes-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/issues/"&gt;&lt;img src="https://camo.githubusercontent.com/b8aaf86d4c1124871832f63b543d32c32a4d90fb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f4e61657265656e2f5374726170446f776e2e6a732e737667" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/Naereen/StrapDown.js.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/issues?q=is%3Aissue+is%3Aclosed"&gt;&lt;img src="https://camo.githubusercontent.com/4b5ebe5364b22087d0906bd57c87901689ac19ec/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d636c6f7365642f4e61657265656e2f5374726170446f776e2e6a732e737667" alt="GitHub issues-closed" data-canonical-src="https://img.shields.io/github/issues-closed/Naereen/StrapDown.js.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.paypal.me/CarmineSpagnuolo" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d59f4d8577fa6fd5347e5427f361ee18c0e43099/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617950616c2d446f6e617465253230746f253230417574686f722d626c75652e737667" alt="Donate" data-canonical-src="https://img.shields.io/badge/PayPal-Donate%20to%20Author-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/issues"&gt;&lt;img src="https://camo.githubusercontent.com/d52b9239d76d77ebff4fc954745ee8ba555338ee/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f41736b2532306d652d616e797468696e672d3161626339632e737667" alt="Ask Me Anything !" data-canonical-src="https://img.shields.io/badge/Ask%20me-anything-1abc9c.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/intent/tweet?text=Download%20and%20use%20the%20Twenty%20Seconds%20Curriculum%20Vitae%20in%20LaTex&amp;amp;url=https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex&amp;amp;hashtags=curriculum,resume,templates,cv,latex,interview,r%C3%A9sum%C3%A9" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/83d4084f7b71558e33b08844da5c773a8657e271/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c" alt="Tweet" data-canonical-src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.overleaf.com/latex/templates/twenty-seconds-curriculum-vitae/kfgsngtymkfj" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/73bbb9882e7e0da338106ecb5a9c57c97f63bea9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f417661696c61626c652d4f7665726c6561662d677265656e3f6c6f676f3d646174613a696d6167652f706e673b6261736536342c6956424f5277304b47676f414141414e5355684555674141414341414141416743414d414141424570497247414141414247644254554541414c4750432f7868425141414143426a53464a4e414142364a6741416749514141506f41414143413641414164544141414f7067414141366d41414146334363756c4538414141426656424d5645554141414249724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244784972447849724478497244774141414163636956684141414166585253546c4d414141386d4e6a3945517a3079495168567a2b37312b2f37392b6650725a55336b393679446d6f33367751456f7331705779522b397453726a776a6b536f463946657a6f336a505a51536d3654696c30526d5234516979736b38594663446533306b644f3861435079676b777043746b55335158776e7875325758773452334d4d3746524943785a54555150614241666e7142585645324d595a4e7645744951582b4d5537532f506330494d4141414142596b74485241434942523149414141414358424957584d414143346a414141754977463470543932414141414233524a545555483541455045536f7842315050687741414158314a524546554f4d7574556c6462776b41513341743255514a69695131467859616f6147796f32447432374e68374678754b38392b396948346d75547a3575532f4a33637a757a4f347430663847593053534c533039497a4d724f7965586d494154326650794862495463425734433476454373556c2b4134464b445658594b7a73427936767149536e53736976547148656d746f3658734c4842415031477477674e545a703332714c487668317337386c3846576d31614b464e714364676f6f474f7a7049464f6945476952375633645062312b6f33794a2f414735707349426e443451745a7a674544492b4d66736d7259795232795058484a36446d543035706c476c7878444f594a652b634c53784651764f6373574332554177734c6b55305a347957562f6955566b32454b4542724b65656373513573474e7659564c436c3039766d4973594366686b374f6b63356269426d494f7936734b633737683841687762436b51734233664759452b7747516b79475179666834784a42412b46454255352f625a39786b2b64476c78376734764c6e6348554e334a68653635626e724678714f383159376831663258767a714c56746a44374569654b50542f79336a356e336c5a3668697865723933373978526549436676436179536546646e706c4e2f65452b4936705267555479352b4a424e6b6a66383950674643654732376c6768744741414141435630525668305a4746305a54706a636d5668644755414d6a41794d4330774d5330784e5651784e7a6f304d6a6f304f5373774d7a6f774d484732554b6b414141416c6445565964475268644755366257396b61575a35414449774d6a41744d4445744d5456554d5463364e4449364e446b724d444d364d444141362b67564141414141456c46546b5375516d4343" alt="Overleaf" data-canonical-src="https://img.shields.io/badge/Available-Overleaf-green?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAABfVBMVEUAAABIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDxIrDwAAAAcciVhAAAAfXRSTlMAAA8mNj9EQz0yIQhVz+71+/79+fPrZU3k96yDmo36wQEos1pWyR+9tSrjwjkSoF9Fezo3jPZQSm6Til0RmR4Qiysk8YFcDe30kdO8aCPygkwpCtkU3QXwnxu2WXw4R3MM7FRICxZTUQPaBAfnqBXVE2MYZNvEtIQX+MU7S/Pc0IMAAAABYktHRACIBR1IAAAACXBIWXMAAC4jAAAuIwF4pT92AAAAB3RJTUUH5AEPESoxB1PPhwAAAX1JREFUOMutUldbwkAQ3At2UQJiiQ1FxYaoaGyo2Dt27Nh7FxuK89+9iH4muTz5uS/J3czuzO4t0f8GY0SSLS09IzMrOyeXmIAT2fPyHbITcBW4C4vECsUl+A4FKDVXYKzsBy6vqISnSsivTqHemto6XsLHBAP1GtwgNTZp32qLHvh1s78l8FWm1aKFNqCdgooGOzpIFOiEGiR7V3dPb1+o3yJ/AG5psIBnD4QtZzgEDI+MfsmrYyR2yPXHJ6DmT05plGlxxDOYJe+cLSxFQvOcsWC2UAwsLkU0Z4yWV/iUVk2EKEBrKeecsQ5sGNvYVLCl09vmIsYCfhk7Okc5biBmIOy6sKc77h8AhwbCkQsB3fGYE+wGQkyGQyfh4xJBA+FEBU5/bZ9xk+dGlx7g4vLncHUN3Jhe65bnrFxqO81Y7h1f2XvzqLVtjD7EieKPT/y3j5n3lZ6hixer9379xReICfvCaySeFdnplN/eE+I6pRgUTy5+JBNkjf89PgFCeG27lghtGAAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMC0wMS0xNVQxNzo0Mjo0OSswMzowMHG2UKkAAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjAtMDEtMTVUMTc6NDI6NDkrMDM6MDAA6+gVAAAAAElFTkSuQmCC" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-hot" class="anchor" aria-hidden="true" href="#hot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HOT!&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Support Fontawesome Icons new class option &lt;code&gt;\documentclass[icon]{twentysecondcv}&lt;/code&gt;, using the name of the icon available in the documentation of the package &lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/raw/master/fontawesome.pdf"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The Fontawesome version requires the Fontawesome installed. Notice that the  Fontawesome is already available in Overleaf.com.&lt;/li&gt;
&lt;li&gt;Section with Icon:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;\sectionicon{icon-name}{section-name}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Twenty icon items environment&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;\begin{twentyicon}
  \twentyitemicon
    {icon name}
    {year}
    {title}
    {place}
    {description}
\end{twentyicon}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Twenty items icon short environment&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;\begin{twentyshorticon}
  \twentyitemshorticon
    {icon name}
    {year}
    {description}
\end{twentyshorticon}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-curricula-vitae---résumés" class="anchor" aria-hidden="true" href="#curricula-vitae---résumés"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Curricula Vitae - Résumés&lt;/h1&gt;
&lt;p&gt;A curriculum vitae, otherwise known as a CV or résumé, is a document used by individuals to communicate their work history, education and skill set. This is a style template for your curriculum written in LaTex. The main goal of this template is to provide a curriculum that is able to survive to the résumés screening of "twenty seconds".&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The author assumes no responsibility for the topicality, correctness, completeness or quality of the information provided and for the obtained résumés.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This is designed for computer scientists but there is no limitation to use it for résumés in other disciplines.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-if-you-like-this-curriculum-please-dont-forget-to-leave-a-star-to-help-the-development-and-improvement" class="anchor" aria-hidden="true" href="#if-you-like-this-curriculum-please-dont-forget-to-leave-a-star-to-help-the-development-and-improvement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;If you like this curriculum, please don't forget to leave a star, to help the development and improvement.&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-the-basic-idea-is-kiss---keep-it-simple-stupid" class="anchor" aria-hidden="true" href="#the-basic-idea-is-kiss---keep-it-simple-stupid"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The basic idea is KISS - Keep It Simple, Stupid.&lt;/h2&gt;
&lt;p&gt;In a nutshell &lt;em&gt;&lt;strong&gt;"It is vain to do with more what can be done with fewer"&lt;/strong&gt;&lt;/em&gt; -- Occam's razor --&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;This template has been designed to create a "one-page" résumé is therefore not suitable to create curriculum of more than one-page.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Please do not try to create curriculum more than one-page.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-how-to-describe-your-experiences" class="anchor" aria-hidden="true" href="#how-to-describe-your-experiences"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to describe your experiences?&lt;/h3&gt;
&lt;p&gt;There are many theories about the résumé screening process of "Big" companies.
Resume screeners and the interviewers look in your résumé for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are you smart?&lt;/li&gt;
&lt;li&gt;Can you code (act for what you apply)?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Anyway according to the guidelines of this template you should use a really simple form to describe each items in your résumé:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Accomplished &amp;lt;X&amp;gt; by implementing &amp;lt;Y&amp;gt; which led to &amp;lt;Z&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here's an examples:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Reduced object rendering time by 75% by applying Floyd's algorithm, leading to a 10% reduction in system boot time.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;-- &lt;em&gt;Cracking the Coding Interview, Book, Gayle Laakmann Mcdowell&lt;/em&gt; --&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-toy-résumé-with-fontawesome-icons-document-class-option-icon" class="anchor" aria-hidden="true" href="#toy-résumé-with-fontawesome-icons-document-class-option-icon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Toy Résumé with Fontawesome Icons (document class option &lt;em&gt;icon&lt;/em&gt;)&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/raw/master/Twenty-Seconds_cv_icons.jpg"&gt;&lt;img src="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/raw/master/Twenty-Seconds_cv_icons.jpg" alt="sample résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-build" class="anchor" aria-hidden="true" href="#build"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build&lt;/h1&gt;
&lt;p&gt;This guide walks you to build your résumé.&lt;/p&gt;
&lt;p&gt;Build requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LaTex installation.
&lt;ul&gt;
&lt;li&gt;additionals packages:
&lt;ul&gt;
&lt;li&gt;ClearSans, fontenc&lt;/li&gt;
&lt;li&gt;tikz&lt;/li&gt;
&lt;li&gt;xcolor&lt;/li&gt;
&lt;li&gt;textpos&lt;/li&gt;
&lt;li&gt;ragged2e&lt;/li&gt;
&lt;li&gt;etoolbox&lt;/li&gt;
&lt;li&gt;ifmtarg&lt;/li&gt;
&lt;li&gt;ifthen&lt;/li&gt;
&lt;li&gt;pgffor&lt;/li&gt;
&lt;li&gt;marvosym&lt;/li&gt;
&lt;li&gt;parskip&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-build-through-gnu-make-command" class="anchor" aria-hidden="true" href="#build-through-gnu-make-command"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build through GNU Make command&lt;/h3&gt;
&lt;p&gt;Clean your project résumé.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make clean
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Build your project résumé.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make all
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;-- &lt;em&gt;Alternately you can build through your favorite LaTex editor.&lt;/em&gt; --&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-environment-style-and-list-of-commands" class="anchor" aria-hidden="true" href="#environment-style-and-list-of-commands"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Environment style and list of commands&lt;/h1&gt;
&lt;p&gt;The style is divided in two parts. The former is the left side bar: that contains personal information, profile picture, and information about your professional skills. The second part is the body that should be contains details about your academic studies, professional experiences and all the information that you want (remember the KISS principle).&lt;/p&gt;
&lt;p&gt;The class is &lt;code&gt;\documentclass[icon]{twentysecondcv}&lt;/code&gt;, the &lt;strong&gt;icon&lt;/strong&gt; option enable to use Fontawesome package in sections and twenty items. In order to use the icon option you need to install the Fontawesome package and use the Fontawesome icon name, available in the package documentation &lt;a href="https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex/raw/master/fontawesome.pdf"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-profile-environment" class="anchor" aria-hidden="true" href="#profile-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Profile environment&lt;/h3&gt;
&lt;p&gt;These are the command to set up the profile information.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Set up the image profile.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \profilepic{paht_name}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your name.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvname{your name}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your job profile.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvjobtitle{your job title}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your date of birth.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvdate{date}	
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your address.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvaddress{address}		
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your telephone number.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvnumberphone{phone number}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your email.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvmail{email address}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up your personal home page.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \cvsite{home page address}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up a brief description of you.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \about{brief description}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up the skills with chart style. Each skill must is a couple &lt;code&gt;{name/value}&lt;/code&gt;, where the value is a floating point value between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt;. This is an agreement for the graphics issues, the &lt;code&gt;0&lt;/code&gt; correspond to a Fundamental awareness while &lt;code&gt;6&lt;/code&gt; to a Expert awareness level.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \skills{{name skill1/5.8},{name skill2/4}} 
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up the skills with text style.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \skillstext{{name skill1/5.8},{name skill2/4}} 
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To create the profile use the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;\makeprofile
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-body-environment" class="anchor" aria-hidden="true" href="#body-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Body environment&lt;/h3&gt;
&lt;p&gt;The body document part is composed by sections.
In the sections you can put two kinds of list items.&lt;/p&gt;
&lt;p&gt;The first (&lt;em&gt;Twenty items environment&lt;/em&gt;) intends a list of detailed information with four part: &lt;strong&gt;Data&lt;/strong&gt; -- &lt;strong&gt;Title&lt;/strong&gt; -- &lt;strong&gt;Place&lt;/strong&gt; -- &lt;strong&gt;Description&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The second (&lt;em&gt;Twenty items short environment&lt;/em&gt;) intends a fewer informationinformation (you can customize this list more easily): &lt;strong&gt;Data&lt;/strong&gt; -- &lt;strong&gt;Description&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-sections-also-wiht-icon" class="anchor" aria-hidden="true" href="#sections-also-wiht-icon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sections (also wiht icon)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Set up a new section in the body part.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  \section{sction name}
  \section{icon name}{section name}, require the icon option in the document declaration.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-twenty-items-environment-also-wiht-icon" class="anchor" aria-hidden="true" href="#twenty-items-environment-also-wiht-icon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Twenty items environment (also wiht icon)&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;\begin{twenty}
  \twentyitem
    {year}
    {title}
    {place}
    {description}
\end{twenty}

\begin{twentyicon}
  \twentyitemicon
    {icon name}
    {year}
    {title}
    {place}
    {description}
\end{twentyicon}
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-twenty-items-short-environment-also-wiht-icon" class="anchor" aria-hidden="true" href="#twenty-items-short-environment-also-wiht-icon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Twenty items short environment (also wiht icon)&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;\begin{twentyshort}
  \twentyitemshort
    {year}
    {description}
\end{twentyshort}

\begin{twentyshorticon}
  \twentyitemshorticon
    {icon name}
    {year}
    {description}
\end{twentyshorticon}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-other-commands" class="anchor" aria-hidden="true" href="#other-commands"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other commands&lt;/h3&gt;
&lt;p&gt;There other two fun command: \icon and \round; that enables to wrap the text in oval shape.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;	\icon{text}
	\round{text, color}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>spagnuolocarmine</author><guid isPermaLink="false">https://github.com/spagnuolocarmine/TwentySecondsCurriculumVitae-LaTex</guid><pubDate>Sun, 09 Feb 2020 00:07:00 GMT</pubDate></item><item><title>HarisIqbal88/PlotNeuralNet #8 in TeX, Today</title><link>https://github.com/HarisIqbal88/PlotNeuralNet</link><description>&lt;p&gt;&lt;i&gt;Latex code for making neural networks diagrams&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-plotneuralnet" class="anchor" aria-hidden="true" href="#plotneuralnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PlotNeuralNet&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.5281/zenodo.2526396" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/89c8c312f40c2d237b2319aececd5740a147b11c/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e323532363339362e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.2526396.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Latex code for drawing neural networks for reports and presentation. Have a look into examples to see how they are made. Additionally, lets consolidate any improvements that you make and fix any bugs to help more people with this code.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Install the following packages on Ubuntu.
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ubuntu 16.04&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install texlive-latex-extra
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ubuntu 18.04.2
Base on this &lt;a href="https://gist.github.com/rain1024/98dd5e2c6c8c28f9ea9d"&gt;website&lt;/a&gt;, please install the following packages.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install texlive-latex-base
sudo apt-get install texlive-fonts-recommended
sudo apt-get install texlive-fonts-extra
sudo apt-get install texlive-latex-extra
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Execute the example as followed.
&lt;pre&gt;&lt;code&gt;cd pyexamples/
bash ../tikzmake.sh test_simple
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Python interface&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add easy legend functionality&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add more layer shapes like TruncatedPyramid, 2DSheet etc&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add examples for RNN and likes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-latex-usage" class="anchor" aria-hidden="true" href="#latex-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Latex usage&lt;/h2&gt;
&lt;p&gt;See &lt;a href="examples"&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory for usage.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-python-usage" class="anchor" aria-hidden="true" href="#python-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python usage&lt;/h2&gt;
&lt;p&gt;First, create a new directory and a new Python file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ mkdir my_project
$ cd my_project
vim my_arch.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the following code to your new file:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; sys
sys.path.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;../&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;from&lt;/span&gt; pycore.tikzeng &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; defined your arch&lt;/span&gt;
arch &lt;span class="pl-k"&gt;=&lt;/span&gt; [
    to_head( &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;..&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; ),
    to_cor(),
    to_begin(),
    to_Conv(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conv1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;512&lt;/span&gt;, &lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;depth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt; ),
    to_Pool(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(conv1-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
    to_Conv(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conv2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(1,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(pool1-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;, &lt;span class="pl-v"&gt;depth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;, &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt; ),
    to_connection( &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conv2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
    to_Pool(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(conv2-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;28&lt;/span&gt;, &lt;span class="pl-v"&gt;depth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;28&lt;/span&gt;, &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;),
    to_SoftMax(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;soft1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;10&lt;/span&gt; ,&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(3,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(pool1-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;caption&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;SOFT&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;  ),
    to_connection(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;soft1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
    to_end()
    ]

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;main&lt;/span&gt;():
    namefile &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;str&lt;/span&gt;(sys.argv[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]).split(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;.&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]
    to_generate(arch, namefile &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;.tex&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; )

&lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;__name__&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;__main__&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:
    main()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, run the program as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bash ../tikzmake.sh my_arch
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;p&gt;Following are some network representations:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/17570785/50308846-c2231880-049c-11e9-8763-3daa1024de78.png"&gt;&lt;img src="https://user-images.githubusercontent.com/17570785/50308846-c2231880-049c-11e9-8763-3daa1024de78.png" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-fcn-8" class="anchor" aria-hidden="true" href="#fcn-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FCN-8&lt;/h6&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/17570785/50308873-e2eb6e00-049c-11e9-9587-9da6bdec011b.png"&gt;&lt;img src="https://user-images.githubusercontent.com/17570785/50308873-e2eb6e00-049c-11e9-9587-9da6bdec011b.png" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-fcn-32" class="anchor" aria-hidden="true" href="#fcn-32"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FCN-32&lt;/h6&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/17570785/50308911-03b3c380-049d-11e9-92d9-ce15669017ad.png"&gt;&lt;img src="https://user-images.githubusercontent.com/17570785/50308911-03b3c380-049d-11e9-92d9-ce15669017ad.png" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-holistically-nested-edge-detection" class="anchor" aria-hidden="true" href="#holistically-nested-edge-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Holistically-Nested Edge Detection&lt;/h6&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>HarisIqbal88</author><guid isPermaLink="false">https://github.com/HarisIqbal88/PlotNeuralNet</guid><pubDate>Sun, 09 Feb 2020 00:08:00 GMT</pubDate></item><item><title>ElegantLaTeX/ElegantNote #9 in TeX, Today</title><link>https://github.com/ElegantLaTeX/ElegantNote</link><description>&lt;p&gt;&lt;i&gt;ElegantNote: An Elegant LaTeX Template for Notes&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;

&lt;h1&gt;&lt;a id="user-content-elegantnote" class="anchor" aria-hidden="true" href="#elegantnote"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ElegantNote&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://elegantlatex.org/" rel="nofollow"&gt;Homepage&lt;/a&gt; | &lt;a href="https://github.com/ElegantLaTeX/ElegantNote"&gt;Github&lt;/a&gt; | &lt;a href="https://ctan.org/pkg/elegantnote" rel="nofollow"&gt;CTAN&lt;/a&gt; | &lt;a href="https://github.com/ElegantLaTeX/ElegantNote/releases"&gt;Download&lt;/a&gt; | &lt;a href="https://github.com/ElegantLaTeX/ElegantNote/wiki"&gt;Wiki&lt;/a&gt; | &lt;a href="https://weibo.com/elegantlatex" rel="nofollow"&gt;Weibo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/3e6b6dcf434f7097c2a98432c91d3dd7dbd552e1/68747470733a2f2f696d672e736869656c64732e696f2f6374616e2f6c2f656c6567616e746e6f74652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/3e6b6dcf434f7097c2a98432c91d3dd7dbd552e1/68747470733a2f2f696d672e736869656c64732e696f2f6374616e2f6c2f656c6567616e746e6f74652e737667" alt="License" data-canonical-src="https://img.shields.io/ctan/l/elegantnote.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/65c04823ac2220d2ccd3756cb470511ba899d980/68747470733a2f2f696d672e736869656c64732e696f2f6374616e2f762f656c6567616e746e6f74652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/65c04823ac2220d2ccd3756cb470511ba899d980/68747470733a2f2f696d672e736869656c64732e696f2f6374616e2f762f656c6567616e746e6f74652e737667" alt="CTAN Version" data-canonical-src="https://img.shields.io/ctan/v/elegantnote.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/b612db8d03ebcac9765440e23f66540a0f96c410/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f456c6567616e744c615465582f456c6567616e744e6f74652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/b612db8d03ebcac9765440e23f66540a0f96c410/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f456c6567616e744c615465582f456c6567616e744e6f74652e737667" alt="Github Version" data-canonical-src="https://img.shields.io/github/release/ElegantLaTeX/ElegantNote.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/10c656ca982c6dfd3c5148545634aaef43ac8cf2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7265706f2d73697a652f456c6567616e744c615465582f456c6567616e744e6f74652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/10c656ca982c6dfd3c5148545634aaef43ac8cf2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7265706f2d73697a652f456c6567616e744c615465582f456c6567616e744e6f74652e737667" alt="Repo Size" data-canonical-src="https://img.shields.io/github/repo-size/ElegantLaTeX/ElegantNote.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ElegantNote is designed for Notes. Just enjoy it! If you have any questions, suggestions or bug reports, you can create issues, pull requests or email us at &lt;a href="mailto:elegantlatex2e@gmail.com"&gt;elegantlatex2e@gmail.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;设计 ElegantNote 是为了方便记录笔记和阅读笔记。如果你有其他问题、建议或者报告 bug，可以提交 issues 或者给我们发邮件：&lt;a href="mailto:elegantlatex2e@gmail.com"&gt;elegantlatex2e@gmail.com&lt;/a&gt;。最近我们新建了一个 QQ 用户交流群（Q 群：692108391），欢迎加入。&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;This work is released under the LaTeX Project Public License, v1.3c or later.&lt;/p&gt;
&lt;p&gt;本模板发布遵循 LaTeX 项目公共许可证 1.3 c 或更高版本。&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ElegantLaTeX</author><guid isPermaLink="false">https://github.com/ElegantLaTeX/ElegantNote</guid><pubDate>Sun, 09 Feb 2020 00:09:00 GMT</pubDate></item><item><title>sb2nov/resume #10 in TeX, Today</title><link>https://github.com/sb2nov/resume</link><description>&lt;p&gt;&lt;i&gt;Software developer resume in Latex&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;A single-page, one-column resume for software developers. It uses the base latex templates and fonts to provide ease of use and installation when trying to update the resume. The different sections are clearly documented and custom commands are used to provide consistent formatting. The three main sections in the resume are education, experience, and projects.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-motivation" class="anchor" aria-hidden="true" href="#motivation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Motivation&lt;/h3&gt;
&lt;p&gt;I created this template as managing a resume on Google Docs was hard and changing any formatting was too difficult since it had to be applied in multiple places. Most currently available templates either focus on two columns, or are multiple pages long. I personally found the two-column templates hard to focus while multiple-page resumes were just too long to be used in career fairs.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/resume_preview.png"&gt;&lt;img src="/resume_preview.png" alt="Resume Screenshot" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;Format is MIT but all the data is owned by Sourabh Bajaj.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sb2nov</author><guid isPermaLink="false">https://github.com/sb2nov/resume</guid><pubDate>Sun, 09 Feb 2020 00:10:00 GMT</pubDate></item><item><title>COPCSE-NTNU/thesis-NTNU #11 in TeX, Today</title><link>https://github.com/COPCSE-NTNU/thesis-NTNU</link><description>&lt;p&gt;&lt;i&gt;An NTNU thesis LaTeX document class for bachelor, master, and PhD theses&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-thesis-ntnu" class="anchor" aria-hidden="true" href="#thesis-ntnu"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;thesis-NTNU&lt;/h1&gt;
&lt;p&gt;An NTNU thesis LaTeX document class for bachelor, master, and PhD theses. It replaces previous templates like &lt;a href="https://github.com/COPCSE-NTNU/bachelor-thesis-NTNU"&gt;https://github.com/COPCSE-NTNU/bachelor-thesis-NTNU&lt;/a&gt; and &lt;a href="https://github.com/COPCSE-NTNU/master-theses-NTNU"&gt;https://github.com/COPCSE-NTNU/master-theses-NTNU&lt;/a&gt; and provides a single template for theses at all study levels.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-setting-up" class="anchor" aria-hidden="true" href="#setting-up"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setting up&lt;/h2&gt;
&lt;p&gt;You can use the template with &lt;a href="http://overleaf.com" rel="nofollow"&gt;Overleaf&lt;/a&gt;, and you are strongly encouraged to do so. The alternative is to install local copy of LaTeX on your laptop (not adviced, huge, difficult).&lt;/p&gt;
&lt;p&gt;You should &lt;strong&gt;fork&lt;/strong&gt; the COPCSE repo so that you have your own files to edit and you can always merge with the upstream changes to the template, in case the template is updated.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-setup-using-overleaf" class="anchor" aria-hidden="true" href="#setup-using-overleaf"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup using Overleaf&lt;/h3&gt;
&lt;p&gt;There are two ways for setting up the &lt;a href="http://overleaf.com" rel="nofollow"&gt;&lt;strong&gt;Overleaf&lt;/strong&gt;&lt;/a&gt; project with the template:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;.zip&lt;/code&gt; copy and upload.&lt;/li&gt;
&lt;li&gt;Fork the the COPCSE repo so that you have your own files to edit.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-building-document-locally" class="anchor" aria-hidden="true" href="#building-document-locally"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building document locally&lt;/h3&gt;
&lt;p&gt;The template also provides a simple &lt;code&gt;Makefile&lt;/code&gt; which allows you to build the document locally. This requires that you have a LaTeX compiler, such as &lt;a href="https://www.tug.org/texlive/" rel="nofollow"&gt;&lt;code&gt;texlive&lt;/code&gt;&lt;/a&gt;, installed locally, which has to provide the commands &lt;code&gt;pdflatex&lt;/code&gt; and &lt;code&gt;biber&lt;/code&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>COPCSE-NTNU</author><guid isPermaLink="false">https://github.com/COPCSE-NTNU/thesis-NTNU</guid><pubDate>Sun, 09 Feb 2020 00:11:00 GMT</pubDate></item><item><title>zhanwen/MathModel #12 in TeX, Today</title><link>https://github.com/zhanwen/MathModel</link><description>&lt;p&gt;&lt;i&gt;研究生数学建模，数学建模竞赛优秀论文，数学建模算法，LaTeX论文模板，算法思维导图，参考书籍，Matlab软件教程，PPT&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-数学建模资源" class="anchor" aria-hidden="true" href="#数学建模资源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数学建模资源&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-2019-年研究生数模" class="anchor" aria-hidden="true" href="#2019-年研究生数模"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019 年研究生数模&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-20191111-又是双十一比赛结果经过一个半月的评审在这一天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo" class="anchor" aria-hidden="true" href="#20191111-又是双十一比赛结果经过一个半月的评审在这一天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019.11.11 又是双十一，比赛结果经过一个半月的评审，在这一天公布了&lt;a href="2019%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95"&gt;获奖名单&lt;/a&gt;，大家的努力相信都会有所收获。余生还有很多有意义的事情需要我们去做，让我们一起努力。(o^o)&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-20199192019923-比赛已经结束大家耐心等待获奖吧oo" class="anchor" aria-hidden="true" href="#20199192019923-比赛已经结束大家耐心等待获奖吧oo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019.9.19—2019.9.23 比赛已经结束，大家耐心等待获奖吧（(o^^o)）&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-论文提交md5使用方法" class="anchor" aria-hidden="true" href="#论文提交md5使用方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;论文提交（MD5使用方法）&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;MD5文件校验和使用说明：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/blob/master/MD5%E6%96%87%E4%BB%B6%E6%A0%A1%E9%AA%8C%E5%92%8C%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.md"&gt;&lt;strong&gt;MD5文件校验和使用说明&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-论文模版更新" class="anchor" aria-hidden="true" href="#论文模版更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;论文模版更新&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;LaTex 论文模版：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/blob/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/2019%E5%B9%B4Latex%E6%A8%A1%E7%89%88.zip"&gt;&lt;strong&gt;LaTex 论文模版&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;Word 论文模版：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/blob/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/%E2%80%9C%E5%8D%8E%E4%B8%BA%E6%9D%AF%E2%80%9D%E7%AC%AC%E5%8D%81%E5%85%AD%E5%B1%8A%E4%B8%AD%E5%9B%BD%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AE%BA%E6%96%87%E6%A0%BC%E5%BC%8F%E8%A7%84%E8%8C%83.doc"&gt;&lt;strong&gt;Word 论文模版（已更新最新）&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;LaTex 论文模版使用方式：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/tree/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/latex_note.md"&gt;&lt;strong&gt;如何编译 Latex 文件&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-下载方式仓库比较大建议单个文件下载" class="anchor" aria-hidden="true" href="#下载方式仓库比较大建议单个文件下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载方式(仓库比较大，建议单个文件下载)&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/downloaddemo2.gif"&gt;&lt;img src="./images/downloaddemo2.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt;  
&lt;p&gt;&lt;em&gt;主题：&lt;/em&gt; &lt;a href="https://cpipc.chinadegrees.cn/cw/hp/4" rel="nofollow"&gt;&lt;strong&gt;“华为杯”第十六届中国研究生数学建模竞赛&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;报名时间：&lt;/em&gt; &lt;strong&gt;2019年6月1日8:00——9月10日17:00&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;审核时间：&lt;/em&gt; &lt;strong&gt;2019年6月1日8:00——9月12日17:00&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;交费时间：&lt;/em&gt; &lt;strong&gt;2019年7月1日8:00——9月15日17:00&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;比赛时间：&lt;/em&gt; &lt;strong&gt;2019年9月19日8:00——9月23日12:00&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-官网报名地址官网地址" class="anchor" aria-hidden="true" href="#官网报名地址官网地址"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;官网报名地址：&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/4" rel="nofollow"&gt;官网地址&lt;/a&gt;&lt;/h4&gt;
&lt;hr&gt;  
&lt;h3&gt;&lt;a id="user-content-2018915-祝大家比赛开心-_" class="anchor" aria-hidden="true" href="#2018915-祝大家比赛开心-_"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018.9.15 祝大家比赛开心 （^_^）&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-2018919-比赛已经结束大家耐心等待获奖吧o" class="anchor" aria-hidden="true" href="#2018919-比赛已经结束大家耐心等待获奖吧o"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018.9.19 比赛已经结束，大家耐心等待获奖吧（^o^）&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-20181111-比赛结果经过一个半月的评审终于在昨天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo" class="anchor" aria-hidden="true" href="#20181111-比赛结果经过一个半月的评审终于在昨天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018.11.11 比赛结果经过一个半月的评审，终于在昨天公布了&lt;a href="https://github.com/zhanwen/MathModel/tree/master/2018%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/2018%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95"&gt;获奖名单&lt;/a&gt;，大家的努力相信都会有所收获。余生还有很多有意义的事情需要我们去做，让我们一起努力。(o^^o)&lt;/h4&gt;
&lt;hr&gt;  
&lt;h4&gt;&lt;a id="user-content-更新添加比赛官网地址戳这里" class="anchor" aria-hidden="true" href="#更新添加比赛官网地址戳这里"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新/添加比赛官网地址&lt;a href="https://cpipc.chinadegrees.cn/" rel="nofollow"&gt;戳这里&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/4" rel="nofollow"&gt;数学建模竞赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/6" rel="nofollow"&gt;电子设计竞赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/2c9088a5696cbf370169a3f8101510bd" rel="nofollow"&gt;人工智能创新大赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/2c9088a5696cbf370169a3f8934810be" rel="nofollow"&gt;机器人创新设计大赛&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-下载与使用由于整个项目直接下载比较慢可以看方式四" class="anchor" aria-hidden="true" href="#下载与使用由于整个项目直接下载比较慢可以看方式四"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载与使用（由于整个项目直接下载比较慢，可以看方式四）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;方式一：使用 &lt;code&gt;git&lt;/code&gt; 下载。&lt;br&gt;
&lt;code&gt;git clone https://github.com/zhanwen/MathModel.git&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方式二：直接下载压缩包。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/downloaddemo.gif"&gt;&lt;img src="./images/downloaddemo.gif" height="250" width="500" align="center" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方式三
&lt;ul&gt;
&lt;li&gt;可以单个文件下载，选择自己需要的某篇论文，直接在对应的页面点击下载即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/download3.gif"&gt;&lt;img src="./images/download3.gif" height="250" width="500" align="center" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方式四：百度云下载（推荐）
&lt;ul&gt;
&lt;li&gt;使用百度云下载，正常的客户端会出现限速，导致下载的很慢，这里给大家推荐一个绕过百度云下载限速的方式。具体怎么下载，请参照 &lt;a href="https://github.com/iikira/BaiduPCS-Go"&gt;绕过限速&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;该项目的百度云链接 &lt;a href="https://pan.baidu.com/s/1UnngHxNR0EVoyBpKlPxFAw" rel="nofollow"&gt;https://pan.baidu.com/s/1UnngHxNR0EVoyBpKlPxFAw&lt;/a&gt;，密码：&lt;code&gt;ea2n&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-国赛试题" class="anchor" aria-hidden="true" href="#国赛试题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;国赛试题&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2019%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2019年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2018%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2018年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2017%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2017年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2016%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2016年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2015%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2015年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2014%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2014年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2013%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2013年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2012%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2012年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2011%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2011年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2010%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2010年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2009%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2009年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2008%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2008年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2007%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2007年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2006%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2006年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2005%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2005年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2004%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2004年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-国赛论文" class="anchor" aria-hidden="true" href="#国赛论文"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;国赛论文&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2018年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：关于跳台跳水体型系数设置的建模分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：光传送网建模与价值评估&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：对恐怖袭击事件记录数据的量化分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：基于卫星高度计海面高度异常资料获取潮汐调和常数方法及应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：多无人机对组网雷达的协同干扰&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：航站楼扩增评估&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2017年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：无人机在抢险救灾中的优化运用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：面向下一代光通信的 VCSEL 激光器仿真模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：航班恢复问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：基于监控视频的前景目标提取&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：多波次导弹发射中的规划问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：地下物流系统网络&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2016年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：多无人机协同任务规划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：具有遗传性疾病和性状的遗传位点分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：基于无线通信基站的室内三维定位问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：军事行动避空侦察的时机和路线选择&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：粮食最低收购价政策问题研究&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2015年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：水面舰艇编队防空和信息化战争评估模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：数据的多流形结构分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：移动通信中的无线信道“指纹”特征建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：面向节能的单/多列车优化决策问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：数控加工刀具运动的优化控制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：旅游路线规划问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2014年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：小鼠视觉感受区电位信号(LFP)与视觉刺激之间的关系研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：机动目标的跟踪与反跟踪&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：无线通信中的快时变信道建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：人体营养健康角度的中国果蔬发展战略研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：乘用车物流运输计划问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2013年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：变循环发动机部件法建模及优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：功率放大器非线性特性及预失真模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：微蜂窝环境中无线接收信号的特性分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：空气中PM2.5问题的研究 attachment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：中等收入定位与人口度量模型研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：可持续的中国城乡居民养老保险体系的数学模型研究&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2012年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：基因识别问题及其算法实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：基于卫星无源探测的空间飞行器主动段轨道估计与误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：有杆抽油系统的数学建模及诊断&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：基于卫星云图的风失场(云导风)度量模型与算法探讨&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2011年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：基于光的波粒二象性一种猜想的数学仿真&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：吸波材料与微波暗室问题的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：小麦发育后期茎杆抗倒性的数学模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：房地产行业的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2010年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：确定肿瘤的重要基因信息&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：与封堵渍口有关的重物落水后运动过程的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：神经元的形态分类和识别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：特殊工件磨削加工的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2009年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：我国就业人数或城镇登记失业率的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：枪弹头痕迹，自动比对方法的研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：多传感器数据融合与航迹预测&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：110 警车配置及巡逻方案&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2008年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：汶川地震中唐家山堪塞湖泄洪问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：城市道路交通信号实时控制问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;C题：货运列车的编组调度问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;D题：中央空调系统节能设计问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2007年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：建立食品卫生安全保障体系数学模型及改进模型的若干理论问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：械臂运动路径设计问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：探讨提高高速公路路面质量的改进方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：邮政运输网络中的邮路规划和邮车调运&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2006年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：Ad Hoc 网络中的区域划分和资源分配问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：确定高精度参数问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：维修线性流量阀时的内筒设计问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：学生面试问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2005年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：Highway Traveling time Estimate and Optimal Routing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：空中加油&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：城市交通管理中的出租车规划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：仓库容量有限条件下的随机存贮管理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2004年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：发现黄球并定位&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：使用下料问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：售后服务数据的运用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：研究生录取问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-美赛论文" class="anchor" aria-hidden="true" href="#美赛论文"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;美赛论文&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2017年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2016年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2015年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2014年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2013年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2012年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2011年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2010年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2009年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2008年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2007年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2006年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2005年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2004年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-数学建模算法" class="anchor" aria-hidden="true" href="#数学建模算法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数学建模算法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AE%97%E6%B3%95"&gt;经典算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95"&gt;现代算法&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BB%BF%E7%9C%9F"&gt;计算机仿真&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95"&gt;粒子群算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE"&gt;马尔可夫链&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B3%95"&gt;蒙特卡洛法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E6%B3%95"&gt;模拟退火法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"&gt;神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E5%B0%8F%E6%B3%A2%E5%88%86%E6%9E%90"&gt;小波分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95"&gt;遗传算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-教材及课件" class="anchor" aria-hidden="true" href="#教材及课件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;教材及课件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E6%95%99%E6%9D%90%E5%8F%8A%E8%AF%BE%E4%BB%B6/%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6"&gt;国防科技术大学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E6%95%99%E6%9D%90%E5%8F%8A%E8%AF%BE%E4%BB%B6/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E8%AF%BE%E4%BB%B6/PPT%E8%AF%BE%E4%BB%B6"&gt;浙江大学课件&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-数学建模算法思维导图" class="anchor" aria-hidden="true" href="#数学建模算法思维导图"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数学建模算法思维导图&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/Mind"&gt;思维导图&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-matlab-入门教程" class="anchor" aria-hidden="true" href="#matlab-入门教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Matlab 入门教程&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/Matlab%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B"&gt;Matlab入门和在线性代数中的应用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt; 
&lt;h3&gt;&lt;a id="user-content-声明" class="anchor" aria-hidden="true" href="#声明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;声明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;其中有些内容整理自互联网，如有侵权，请联系，我将及时处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-个人微信公众号" class="anchor" aria-hidden="true" href="#个人微信公众号"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;个人微信公众号&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dotzhang&lt;/code&gt;：一名不羁的学僧，我的世界不只有学术。一条迷途的咸鱼，正在游向属于它的天地！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/images/donate/common.jpg"&gt;&lt;img src="/images/donate/common.jpg" width="150" height="150" alt="weixin" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-赞助和支持" class="anchor" aria-hidden="true" href="#赞助和支持"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;赞助和支持&lt;/h3&gt;
&lt;p&gt;这些内容都是我花了不少时间整理出来的, 如果你觉得它对你很有帮助, 请你也分享给需要学习的朋友们。如果你看好我的内容分享, 也可以考虑适当的赞助打赏, 让我有更多的动力去继续分享更好的内容给大家。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;微信&lt;/th&gt;
&lt;th&gt;支付宝&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/donate/weixinpay.jpg"&gt;&lt;img src="images/donate/weixinpay.jpg" width="150" height="150" alt="pay check" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/donate/alipay.jpg"&gt;&lt;img src="images/donate/alipay.jpg" width="150" height="150" alt="pay check" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-联系" class="anchor" aria-hidden="true" href="#联系"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;联系&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Email：&lt;a href="https://mail.google.com/" rel="nofollow"&gt;hanwenme@gmail.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;微  信（有任何问题都可以直接怼我）：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/donate/wechat.png"&gt;&lt;img src="images/donate/wechat.png" width="150" height="150" alt="pay check" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zhanwen</author><guid isPermaLink="false">https://github.com/zhanwen/MathModel</guid><pubDate>Sun, 09 Feb 2020 00:12:00 GMT</pubDate></item><item><title>scyue/latex-sigcomm18 #13 in TeX, Today</title><link>https://github.com/scyue/latex-sigcomm18</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-latex-template-for-sigcomm-2018" class="anchor" aria-hidden="true" href="#latex-template-for-sigcomm-2018"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Latex Template for SIGCOMM 2018&lt;/h1&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>scyue</author><guid isPermaLink="false">https://github.com/scyue/latex-sigcomm18</guid><pubDate>Sun, 09 Feb 2020 00:13:00 GMT</pubDate></item><item><title>jiachenli94/Awesome-Interaction-aware-Trajectory-Prediction #14 in TeX, Today</title><link>https://github.com/jiachenli94/Awesome-Interaction-aware-Trajectory-Prediction</link><description>&lt;p&gt;&lt;i&gt;A selection of state-of-the-art research materials on trajectory prediction&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-awesome-interaction-aware-behavior-and-trajectory-prediction" class="anchor" aria-hidden="true" href="#awesome-interaction-aware-behavior-and-trajectory-prediction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome Interaction-aware Behavior and Trajectory Prediction&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/7db523b8b68f5a19da519ca6f7649ced6ce5cbf0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d312e302d6666363962342e737667"&gt;&lt;img src="https://camo.githubusercontent.com/7db523b8b68f5a19da519ca6f7649ced6ce5cbf0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d312e302d6666363962342e737667" alt="Version" data-canonical-src="https://img.shields.io/badge/Version-1.0-ff69b4.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/3ce93c312a25aee59456c18fd1be9db42aa924a2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c617374557064617465642d323031392e31312d6c69676874677265792e737667"&gt;&lt;img src="https://camo.githubusercontent.com/3ce93c312a25aee59456c18fd1be9db42aa924a2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c617374557064617465642d323031392e31312d6c69676874677265792e737667" alt="LastUpdated" data-canonical-src="https://img.shields.io/badge/LastUpdated-2019.11-lightgrey.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c169f833febcd19dc5eae6f3e11fd5eea0d5e261/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f546f7069632d6265686176696f72287472616a6563746f7279292d2d70726564696374696f6e2d79656c6c6f772e7376673f6c6f676f3d676974687562"&gt;&lt;img src="https://camo.githubusercontent.com/c169f833febcd19dc5eae6f3e11fd5eea0d5e261/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f546f7069632d6265686176696f72287472616a6563746f7279292d2d70726564696374696f6e2d79656c6c6f772e7376673f6c6f676f3d676974687562" alt="Topic" data-canonical-src="https://img.shields.io/badge/Topic-behavior(trajectory)--prediction-yellow.svg?logo=github" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="http://hits.dwyl.io/jiachenli94/Interaction-aware-Trajectory-Prediction" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e774f641c9fb064f3315f18f39b9a25f2def6103/687474703a2f2f686974732e6477796c2e696f2f6a69616368656e6c6939342f496e746572616374696f6e2d61776172652d5472616a6563746f72792d50726564696374696f6e2e737667" alt="HitCount" data-canonical-src="http://hits.dwyl.io/jiachenli94/Interaction-aware-Trajectory-Prediction.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a checklist of state-of-the-art research materials (datasets, blogs, papers and public codes) related to trajectory prediction. Wish it could be helpful for both academia and industry. (Still updating)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maintainers&lt;/strong&gt;: &lt;a href="https://jiachenli94.github.io" rel="nofollow"&gt;&lt;strong&gt;Jiachen Li&lt;/strong&gt;&lt;/a&gt;, Hengbo Ma, Jinning Li (University of California, Berkeley)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Emails&lt;/strong&gt;: {jiachen_li, hengbo_ma, jinning_li}@berkeley.edu&lt;/p&gt;
&lt;p&gt;Please feel free to pull request to add new resources or send emails to us for questions, discussion and collaborations.&lt;/p&gt;
&lt;p&gt;Also welcome to check the current research in our &lt;a href="https://msc.berkeley.edu/research/autonomous-vehicle.html" rel="nofollow"&gt;&lt;strong&gt;MSC Lab&lt;/strong&gt;&lt;/a&gt; at UC Berkeley.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Research Intern&lt;/strong&gt;: Please read &lt;a href="https://jiachenli94.github.io/Research_Intern_Opportunities_at_UC_Berkeley.pdf" rel="nofollow"&gt;&lt;strong&gt;this&lt;/strong&gt;&lt;/a&gt; if you want to apply for research intern opportunities in our group.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;a href="https://github.com/jiachenli94/Awesome-Decision-Making-Reinforcement-Learning"&gt;&lt;strong&gt;Here&lt;/strong&gt;&lt;/a&gt; is also a collection of materials for reinforcement learning, decision making and motion planning.&lt;/p&gt;
&lt;p&gt;Please cite our work if you found this useful:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{Jiachen_IROS19,
  title={Conditional Generative Neural System for Probabilistic Trajectory Prediction},
  author={Li, Jiachen and Ma, Hengbo and Tomizuka, Masayoshi},
  booktitle={in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2019},
  organization={IEEE}
}

@inproceedings{Jiachen_ICRA19,
  title={Interaction-aware Multi-agent Tracking and Probabilistic Behavior Prediction via Adversarial Learning},
  author={Li, Jiachen* and Ma, Hengbo* and Tomizuka, Masayoshi},
  booktitle={in 2019 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2019},
  organization={IEEE}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#datasets"&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#vehicles-and-traffic"&gt;Vehicles and Traffic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pedestrians"&gt;Pedestrians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sport-players"&gt;Sport Players&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#literature-and-codes"&gt;&lt;strong&gt;Literature and Codes&lt;/strong&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#survey-papers"&gt;Survey Papers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#physics-systems-with-interaction"&gt;Physics Systems with Interaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#intelligent-vehicles-traffic"&gt;Intelligent Vehicles &amp;amp; Traffic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mobile-robots"&gt;Mobile Robots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pedestrians"&gt;Pedestrians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#vehicle-pedestrians-interaction"&gt;Vehicle-Pedestrians Interaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sport-players"&gt;Sport Players&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#benchmark-and-evaluation-metrics"&gt;Benchmark and Evaluation Metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#others"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-vehicles-and-traffic" class="anchor" aria-hidden="true" href="#vehicles-and-traffic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vehicles and Traffic&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Agents&lt;/th&gt;
&lt;th align="center"&gt;Scenarios&lt;/th&gt;
&lt;th align="center"&gt;Sensors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.interaction-dataset.com/" rel="nofollow"&gt;INTERACTION&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Roundabout / intersection&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.cvlibs.net/datasets/kitti/" rel="nofollow"&gt;KITTI&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Highway / rural areas&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.highd-dataset.com/" rel="nofollow"&gt;HighD&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles&lt;/td&gt;
&lt;td align="center"&gt;Highway&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm" rel="nofollow"&gt;NGSIM&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles&lt;/td&gt;
&lt;td align="center"&gt;Highway&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Tsinghua-Daimler_Cyclist_Detec/tsinghua-daimler_cyclist_detec.html" rel="nofollow"&gt;Cyclists&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Cyclists&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.nuscenes.org/" rel="nofollow"&gt;nuScenes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR / RADAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://bdd-data.berkeley.edu/" rel="nofollow"&gt;BDD100k&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists / people&lt;/td&gt;
&lt;td align="center"&gt;Highway / urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://apolloscape.auto/?source=post_page---------------------------" rel="nofollow"&gt;Apolloscapes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists / people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/udacity/self-driving-car/tree/master/datasets"&gt;Udacity&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.cityscapes-dataset.com/" rel="nofollow"&gt;Cityscapes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles/ people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://cvgl.stanford.edu/projects/uav_data/" rel="nofollow"&gt;Stanford Drone&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.argoverse.org/" rel="nofollow"&gt;Argoverse&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://gamma.umd.edu/researchdirections/autonomousdriving/trafdataset" rel="nofollow"&gt;TRAF&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles/buses/cyclists/bikes / people/animals&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://level5.lyft.com/dataset/" rel="nofollow"&gt;Lyft Level 5&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles/cyclists/people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera/ LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-pedestrians" class="anchor" aria-hidden="true" href="#pedestrians"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pedestrians&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Agents&lt;/th&gt;
&lt;th align="center"&gt;Scenarios&lt;/th&gt;
&lt;th align="center"&gt;Sensors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data" rel="nofollow"&gt;UCY&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Zara / students&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.vision.ee.ethz.ch/en/datasets/" rel="nofollow"&gt;ETH&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.viratdata.org/" rel="nofollow"&gt;VIRAT&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People / vehicles&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.cvlibs.net/datasets/kitti/" rel="nofollow"&gt;KITTI&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Highway / rural areas&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://irc.atr.jp/crest2010_HRI/ATC_dataset/" rel="nofollow"&gt;ATC&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Shopping center&lt;/td&gt;
&lt;td align="center"&gt;Range sensor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/daimler_pedestrian_benchmark_d.html" rel="nofollow"&gt;Daimler&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;From moving vehicle&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.ee.cuhk.edu.hk/~xgwang/grandcentral.html" rel="nofollow"&gt;Central Station&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Inside station&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.html#datasets" rel="nofollow"&gt;Town Center&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban street&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/" rel="nofollow"&gt;Edinburgh&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.cityscapes-dataset.com/login/" rel="nofollow"&gt;Cityscapes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles/ people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.argoverse.org/" rel="nofollow"&gt;Argoverse&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://cvgl.stanford.edu/projects/uav_data/" rel="nofollow"&gt;Stanford Drone&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://trajnet.stanford.edu/" rel="nofollow"&gt;TrajNet&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://data.nvision2.eecs.yorku.ca/PIE_dataset/" rel="nofollow"&gt;PIE&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-sport-players" class="anchor" aria-hidden="true" href="#sport-players"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sport Players&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Agents&lt;/th&gt;
&lt;th align="center"&gt;Scenarios&lt;/th&gt;
&lt;th align="center"&gt;Sensors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://datahub.io/collections/football" rel="nofollow"&gt;Football&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Football field&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-literature-and-codes" class="anchor" aria-hidden="true" href="#literature-and-codes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Literature and Codes&lt;/strong&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-survey-papers" class="anchor" aria-hidden="true" href="#survey-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Survey Papers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Human Motion Trajectory Prediction: A Survey, 2019 [&lt;a href="https://arxiv.org/abs/1905.06113" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A literature review on the prediction of pedestrian behavior in urban scenarios, ITSC 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8569415" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Survey on Vision-Based Path Prediction. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-91131-1_4" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Autonomous vehicles that interact with pedestrians: A survey of theory and practice. [&lt;a href="https://arxiv.org/abs/1805.11773" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Trajectory data mining: an overview. [&lt;a href="https://dl.acm.org/citation.cfm?id=2743025" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A survey on motion prediction and risk assessment for intelligent vehicles. [&lt;a href="https://robomechjournal.springeropen.com/articles/10.1186/s40648-014-0001-z" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-physics-systems-with-interaction" class="anchor" aria-hidden="true" href="#physics-systems-with-interaction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Physics Systems with Interaction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Ivanovic_The_Trajectron_Probabilistic_Multi-Agent_Trajectory_Modeling_With_Dynamic_Spatiotemporal_Graphs_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/StanfordASL/Trajectron"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Interaction Templates for Multi-Robot Systems, IROS 2019. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/8737744/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Factorised Neural Relational  Inference for Multi-Interaction Systems, ICML workshop 2019. [&lt;a href="https://arxiv.org/abs/1905.08721v1" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/ekwebb/fNRI"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Physics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video, 2019. [&lt;a href="https://arxiv.org/pdf/1905.11169v1.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Neural Relational Inference for Interacting Systems, ICML 2018. [&lt;a href="https://arxiv.org/abs/1802.04687v2" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/ethanfetaya/NRI"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Unsupervised Learning of Latent Physical Properties Using Perception-Prediction Networks, UAI 2018. [&lt;a href="http://arxiv.org/abs/1807.09244v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Relational inductive biases, deep learning, and graph networks, 2018. [&lt;a href="https://arxiv.org/abs/1806.01261v3" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions, ICLR 2018. [&lt;a href="http://arxiv.org/abs/1802.10353v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Graph networks as learnable physics engines for inference and control, ICML 2018. [&lt;a href="http://arxiv.org/abs/1806.01242v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Flexible Neural Representation for Physics Prediction, 2018. [&lt;a href="http://arxiv.org/abs/1806.08047v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A simple neural network module for relational reasoning, 2017. [&lt;a href="http://arxiv.org/abs/1706.01427v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;VAIN: Attentional Multi-agent Predictive Modeling, NIPS 2017. [&lt;a href="https://arxiv.org/pdf/1706.06122.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Visual Interaction Networks, 2017. [&lt;a href="http://arxiv.org/abs/1706.01433v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A Compositional Object-Based Approach to Learning Physical Dynamics, ICLR 2017. [&lt;a href="http://arxiv.org/abs/1612.00341v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Interaction Networks for Learning about Objects, Relations and Physics, 2016. [&lt;a href="https://arxiv.org/abs/1612.00222" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/higgsfield/interaction_network_pytorch"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-intelligent-vehicles--traffic" class="anchor" aria-hidden="true" href="#intelligent-vehicles--traffic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Intelligent Vehicles &amp;amp; Traffic&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Forecasting Trajectory and Behavior of Road-Agents Using Spectral Clustering in Graph-LSTMs, 2019 arXiv, &lt;a href="https://arxiv.org/pdf/1912.01118.pdf" rel="nofollow"&gt;Paper&lt;/a&gt;, &lt;a href="https://gamma.umd.edu/researchdirections/autonomousdriving/spectralcows/" rel="nofollow"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Joint Prediction for Kinematic Trajectories in Vehicle-Pedestrian-Mixed Scenes, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Bi_Joint_Prediction_for_Kinematic_Trajectories_in_Vehicle-Pedestrian-Mixed_Scenes_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Analyzing the Variety Loss in the Context of Probabilistic Trajectory Prediction, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Thiede_Analyzing_the_Variety_Loss_in_the_Context_of_Probabilistic_Trajectory_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Looking to Relations for Future Trajectory Forecast, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Looking_to_Relations_for_Future_Trajectory_Forecast_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Jointly Learnable Behavior and Trajectory Planning for Self-Driving Vehicles, IROS 2019. [&lt;a href="https://arxiv.org/abs/1910.04586" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sharing Is Caring: Socially-Compliant Autonomous Intersection Negotiation, IROS 2019. [&lt;a href="https://pdfs.semanticscholar.org/f4b2/021353bba52224eb33923b3b98956e2c9821.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;INFER: INtermediate Representations for FuturE PRediction, IROS 2019. [&lt;a href="https://arxiv.org/abs/1903.10641" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/talsperre/INFER"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Deep Predictive Autonomous Driving Using Multi-Agent Joint Trajectory Prediction and Traffic Rules, IROS 2019. [&lt;a href="http://rllab.snu.ac.kr/publications/papers/2019_iros_predstl.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;NeuroTrajectory: A Neuroevolutionary Approach to Local State Trajectory Learning for Autonomous Vehicles, IROS 2019. [&lt;a href="https://arxiv.org/abs/1906.10971" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Urban Street Trajectory Prediction with Multi-Class LSTM Networks, IROS 2019. [N/A]&lt;/li&gt;
&lt;li&gt;Spatiotemporal Learning of Directional Uncertainty in Urban Environments with Kernel Recurrent Mixture Density Networks, IROS 2019. [&lt;a href="https://ieeexplore.ieee.org/document/8772158" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Conditional generative neural system for probabilistic trajectory prediction, IROS 2019. [&lt;a href="https://arxiv.org/abs/1905.01631" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Interaction-aware multi-agent tracking and probabilistic behavior prediction via adversarial learning, ICRA 2019. [&lt;a href="https://arxiv.org/abs/1904.02390" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generic Tracking and Probabilistic Prediction Framework and Its Application in Autonomous Driving, IEEE Trans. Intell. Transport. Systems, 2019. [&lt;a href="https://www.researchgate.net/publication/334560415_Generic_Tracking_and_Probabilistic_Prediction_Framework_and_Its_Application_in_Autonomous_Driving" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Coordination and trajectory prediction for vehicle interactions via bayesian generative modeling, IV 2019. [&lt;a href="https://arxiv.org/abs/1905.00587" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Wasserstein generative learning with kinematic constraints for probabilistic interactive driving behavior prediction, IV 2019. [&lt;a href="https://ieeexplore.ieee.org/document/8813783" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;GRIP: Graph-based Interaction-aware Trajectory Prediction, ITSC 2019. [&lt;a href="https://arxiv.org/abs/1907.07792" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;AGen: Adaptable Generative Prediction Networks for Autonomous Driving, IV 2019. [&lt;a href="http://www.cs.cmu.edu/~cliu6/files/iv19-1.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;TraPHic: Trajectory Prediction in Dense and Heterogeneous Traffic Using Weighted Interactions, CVPR 2019.  [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chandra_TraPHic_Trajectory_Prediction_in_Dense_and_Heterogeneous_Traffic_Using_Weighted_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/rohanchandra30/TrackNPred"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multi-Step Prediction of Occupancy Grid Maps with Recurrent Neural Networks, CVPR 2019. [&lt;a href="https://arxiv.org/pdf/1812.09395.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Argoverse: 3D Tracking and Forecasting With Rich Maps, CVPR 2019 [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Robust Aleatoric Modeling for Future Vehicle Localization, CVPR 2019. [&lt;a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Hudnell_Robust_Aleatoric_Modeling_for_Future_Vehicle_Localization_CVPRW_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian occupancy prediction for autonomous vehicles, IRC 2019. [paper]&lt;/li&gt;
&lt;li&gt;Context-based path prediction for targets with switching dynamics, 2019.[&lt;a href="https://link.springer.com/article/10.1007/s11263-018-1104-4" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Deep Imitative Models for Flexible Inference, Planning, and Control, 2019. [&lt;a href="https://arxiv.org/abs/1810.06544" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Infer: Intermediate representations for future prediction, 2019. [&lt;a href="https://arxiv.org/abs/1903.10641" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/talsperre/INFER"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multi-agent tensor fusion for contextual trajectory prediction, 2019. [&lt;a href="https://arxiv.org/abs/1904.04776" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Context-Aware Pedestrian Motion Prediction In Urban Intersections, 2018. [&lt;a href="https://arxiv.org/abs/1806.09453" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generic probabilistic interactive situation recognition and prediction: From virtual to real, ITSC 2018. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/8569780" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generic vehicle tracking framework capable of handling occlusions based on modified mixture particle filter, IV 2018. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/8500626" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multi-Modal Trajectory Prediction of Surrounding Vehicles with Maneuver based LSTMs, 2018. [&lt;a href="https://arxiv.org/abs/1805.05499" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sequence-to-sequence prediction of vehicle trajectory via lstm encoder-decoder architecture, 2018. [&lt;a href="https://arxiv.org/abs/1802.06338" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;R2P2: A ReparameteRized Pushforward Policy for diverse, precise generative path forecasting, ECCV 2018. [&lt;a href="https://www.cs.cmu.edu/~nrhineha/R2P2.html" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting trajectories of vehicles using large-scale motion priors, IV 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8500604" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Vehicle trajectory prediction by integrating physics-and maneuver based approaches using interactive multiple models, 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8186191" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Motion Prediction of Traffic Actors for Autonomous Driving using Deep Convolutional Networks, 2018. [&lt;a href="https://arxiv.org/abs/1808.05819v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative multi-agent behavioral cloning, 2018. [&lt;a href="https://www.semanticscholar.org/paper/Generative-Multi-Agent-Behavioral-Cloning-Zhan-Zheng/ccc196ada6ec9cad1e418d7321b0cd6813d9b261" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Deep Sequence Learning with Auxiliary Information for Traffic Prediction, KDD 2018. [&lt;a href="https://arxiv.org/pdf/1806.07380.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/JingqingZ/BaiduTraffic"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multipolicy decision-making for autonomous driving via changepoint-based behavior prediction, 2017. [&lt;a href="https://link.springer.com/article/10.1007/s10514-017-9619-z" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Probabilistic long-term prediction for autonomous vehicles, IV 2017. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/7995726" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Probabilistic vehicle trajectory prediction over occupancy grid map via recurrent neural network, ITSC 2017. [&lt;a href="https://ieeexplore.ieee.org/document/6632960" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Desire: Distant future prediction in dynamic scenes with interacting agents, CVPR 2017. [&lt;a href="https://arxiv.org/abs/1704.04394" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/yadrimz/DESIRE"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Imitating driver behavior with generative adversarial networks, 2017. [&lt;a href="https://arxiv.org/abs/1701.06699" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/sisl/gail-driver"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Infogail: Interpretable imitation learning from visual demonstrations, 2017. [&lt;a href="https://arxiv.org/abs/1703.08840" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/YunzhuLi/InfoGAIL"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Long-term planning by short-term prediction, 2017. [&lt;a href="https://arxiv.org/abs/1602.01580" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Long-term path prediction in urban scenarios using circular distributions, 2017. [&lt;a href="https://www.sciencedirect.com/science/article/pii/S0262885617301853" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Deep learning driven visual path prediction from a single image, 2016. [&lt;a href="https://arxiv.org/abs/1601.07265" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Understanding interactions between traffic participants based on learned behaviors, 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7535554" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Visual path prediction in complex scenes with crowded moving objects, CVPR 2016. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/7780661/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A game-theoretic approach to replanning-aware interactive scene prediction and planning, 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7353203" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Intention-aware online pomdp planning for autonomous driving in a crowd, ICRA 2015. [&lt;a href="https://ieeexplore.ieee.org/document/7139219" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Online maneuver recognition and multimodal trajectory prediction for intersection assistance using non-parametric regression, 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6856480" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Patch to the future: Unsupervised visual prediction, CVPR 2014. [&lt;a href="http://ieeexplore.ieee.org/abstract/document/6909818/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Mobile agent trajectory prediction using bayesian nonparametric reachability trees, 2011. [&lt;a href="https://dspace.mit.edu/handle/1721.1/114899" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-mobile-robots" class="anchor" aria-hidden="true" href="#mobile-robots"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mobile Robots&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Multimodal probabilistic model-based planning for human-robot interaction, ICRA 2018. [&lt;a href="https://arxiv.org/abs/1710.09483" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/StanfordASL/TrafficWeavingCVAE"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Decentralized Non-communicating Multiagent Collision Avoidance with Deep Reinforcement Learning, ICRA 2017. [&lt;a href="https://arxiv.org/abs/1609.07845" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Augmented dictionary learning for motion prediction, ICRA 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7487407" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting future agent motions for dynamic environments, ICMLA 2016. [&lt;a href="https://www.semanticscholar.org/paper/Predicting-Future-Agent-Motions-for-Dynamic-Previtali-Bordallo/2df8179ac7b819bad556b6d185fc2030c40f98fa" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bayesian intention inference for trajectory prediction with an unknown goal destination, IROS 2015. [&lt;a href="http://ieeexplore.ieee.org/abstract/document/7354203/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning to predict trajectories of cooperatively navigating agents, ICRA 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6907442" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pedestrians-1" class="anchor" aria-hidden="true" href="#pedestrians-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pedestrians&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Disentangling Human Dynamics for Pedestrian Locomotion Forecasting with Noisy Supervision, WACV 2020. [&lt;a href="https://arxiv.org/abs/1911.01138" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;STGAT: Modeling Spatial-Temporal Interactions for Human Trajectory Prediction, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_STGAT_Modeling_Spatial-Temporal_Interactions_for_Human_Trajectory_Prediction_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/huang-xx/STGAT"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Instance-Level Future Motion Estimation in a Single Image Based on Ordinal Regression, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Instance-Level_Future_Motion_Estimation_in_a_Single_Image_Based_on_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social and Scene-Aware Trajectory Prediction in Crowded Spaces, ICCV workshop 2019. [&lt;a href="https://arxiv.org/pdf/1909.08840.pdf" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/Oghma/sns-lstm/"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Stochastic Sampling Simulation for Pedestrian Trajectory Prediction, IROS 2019. [&lt;a href="https://arxiv.org/abs/1903.01860" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Long-Term Prediction of Motion Trajectories Using Path Homology Clusters, IROS 2019. [&lt;a href="http://www.csc.kth.se/~fpokorny/static/publications/carvalho2019a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;StarNet: Pedestrian Trajectory Prediction Using Deep Neural Network in Star Topology, IROS 2019. [&lt;a href="https://arxiv.org/pdf/1906.01797.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning Generative Socially-Aware Models of Pedestrian Motion, IROS 2019. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/8760356/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Situation-Aware Pedestrian Trajectory Prediction with Spatio-Temporal Attention Model, CVWW 2019. [&lt;a href="https://arxiv.org/pdf/1902.05437.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Path predictions using object attributes and semantic environment, VISIGRAPP 2019. [&lt;a href="http://mprg.jp/data/MPRG/C_group/C20190225_minoura.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Probabilistic Path Planning using Obstacle Trajectory Prediction, CoDS-COMAD 2019. [&lt;a href="https://dl.acm.org/citation.cfm?id=3297006" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Human Trajectory Prediction using Adversarial Loss, 2019. [&lt;a href="http://www.strc.ch/2019/Kothari_Alahi.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social Ways: Learning Multi-Modal Distributions of Pedestrian Trajectories with GANs, CVPR 2019. [&lt;a href="https://sites.google.com/view/ieeecvf-cvpr2019-precognition" rel="nofollow"&gt;&lt;em&gt;Precognition Workshop&lt;/em&gt;&lt;/a&gt;], [&lt;a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Amirian_Social_Ways_Learning_Multi-Modal_Distributions_of_Pedestrian_Trajectories_With_GANs_CVPRW_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/amiryanj/socialways"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Peeking into the Future: Predicting Future Person Activities and Locations in Videos, CVPR 2019. [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liang_Peeking_Into_the_Future_Predicting_Future_Person_Activities_and_Locations_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/google/next-prediction"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning to Infer Relations for Future Trajectory Forecast, CVPR 2019. [&lt;a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Choi_Learning_to_Infer_Relations_for_Future_Trajectory_Forecast_CVPRW_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;TraPHic: Trajectory Prediction in Dense and Heterogeneous Traffic Using Weighted Interactions, CVPR 2019.  [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chandra_TraPHic_Trajectory_Prediction_in_Dense_and_Heterogeneous_Traffic_Using_Weighted_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Which Way Are You Going? Imitative Decision Learning for Path Forecasting in Dynamic Scenes, CVPR 2019.  [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Which_Way_Are_You_Going_Imitative_Decision_Learning_for_Path_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Overcoming Limitations of Mixture Density Networks: A Sampling and Fitting Framework for Multimodal Future Prediction, CVPR 2019.  [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Makansi_Overcoming_Limitations_of_Mixture_Density_Networks_A_Sampling_and_Fitting_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sophie: An attentive gan for predicting paths compliant to social and physical constraints, CVPR 2019. [&lt;a href="https://arxiv.org/abs/1806.01482" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/hindupuravinash/the-gan-zoo/blob/master/README.md"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian path, pose, and intention prediction through gaussian process dynamical models and pedestrian activity recognition, 2019. [&lt;a href="https://ieeexplore.ieee.org/document/8370119/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multimodal Interaction-aware Motion Prediction for Autonomous Street Crossing, 2019. [&lt;a href="https://arxiv.org/abs/1808.06887" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;The simpler the better: Constant velocity for pedestrian motion prediction, 2019. [&lt;a href="https://arxiv.org/abs/1903.07933" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian trajectory prediction in extremely crowded scenarios, 2019. [&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/30862018" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Srlstm: State refinement for lstm towards pedestrian trajectory prediction, 2019. [&lt;a href="https://arxiv.org/abs/1903.02793" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Location-velocity attention for pedestrian trajectory prediction, WACV 2019. [&lt;a href="https://ieeexplore.ieee.org/document/8659060" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian Trajectory Prediction in Extremely Crowded Scenarios, Sensors, 2019. [&lt;a href="https://www.mdpi.com/1424-8220/19/5/1223/pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A data-driven model for interaction-aware pedestrian motion prediction in object cluttered environments, ICRA 2018. [&lt;a href="https://arxiv.org/abs/1709.08528" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Move, Attend and Predict: An attention-based neural model for people’s movement prediction, Pattern Recognition Letters 2018. [&lt;a href="https://reader.elsevier.com/reader/sd/pii/S016786551830182X?token=1EF2B664B70D2B0C3ECDD07B6D8B664F5113AEA7533CE5F0B564EF9F4EE90D3CC228CDEB348F79FEB4E8CDCD74D4BA31" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;GD-GAN: Generative Adversarial Networks for Trajectory Prediction and Group Detection in Crowds, ACCV 2018, [&lt;a href="https://arxiv.org/pdf/1812.07667.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://www.youtube.com/watch?v=7cCIC_JIfms" rel="nofollow"&gt;demo&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ss-lstm: a hierarchical lstm model for pedestrian trajectory prediction, WACV 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8354239" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social Attention: Modeling Attention in Human Crowds, ICRA 2018. [&lt;a href="https://arxiv.org/abs/1710.04689" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/TNTant/social_lstm"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian prediction by planning using deep neural networks, ICRA 2018. [&lt;a href="https://arxiv.org/abs/1706.05904" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Joint long-term prediction of human motion using a planning-based social force approach, ICRA 2018. [&lt;a href="https://iliad-project.eu/publications/2018-2/joint-long-term-prediction-of-human-motion-using-a-planning-based-social-force-approach/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Human motion prediction under social grouping constraints, IROS 2018. [&lt;a href="http://iliad-project.eu/publications/2018-2/human-motion-prediction-under-social-grouping-constraints/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks, CVPR 2018. [&lt;a href="https://arxiv.org/abs/1803.10892" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/agrimgupta92/sgan"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Group LSTM: Group Trajectory Prediction in Crowded Scenarios, ECCV 2018. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-030-11015-4_18" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Mx-lstm: mixing tracklets and vislets to jointly forecast trajectories and head poses, CVPR 2018. [&lt;a href="https://arxiv.org/abs/1805.00652" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Intent prediction of pedestrians via motion trajectories using stacked recurrent neural networks, 2018. [&lt;a href="http://ieeexplore.ieee.org/document/8481390/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Transferable pedestrian motion prediction models at intersections, 2018. [&lt;a href="https://arxiv.org/abs/1804.00495" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Probabilistic map-based pedestrian motion prediction taking traffic participants into consideration, 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8500562" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A Computationally Efficient Model for Pedestrian Motion Prediction, ECC 2018. [&lt;a href="https://arxiv.org/abs/1803.04702" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Context-aware trajectory prediction, ICPR 2018. [&lt;a href="https://arxiv.org/abs/1705.02503" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Set-based prediction of pedestrians in urban environments considering formalized traffic rules, ITSC 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8569434" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Building prior knowledge: A markov based pedestrian prediction model using urban environmental data, ICARCV 2018. [&lt;a href="https://arxiv.org/abs/1809.06045" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Depth Information Guided Crowd Counting for Complex Crowd Scenes, 2018. [&lt;a href="https://arxiv.org/abs/1803.02256" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Tracking by Prediction: A Deep Generative Model for Mutli-Person Localisation and Tracking, WACV 2018. [&lt;a href="https://arxiv.org/abs/1803.03347" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;“Seeing is Believing”: Pedestrian Trajectory Forecasting Using Visual Frustum of Attention, WACV 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8354238" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty, CVPR 2018. [&lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bhattacharyya_Long-Term_On-Board_Prediction_CVPR_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/apratimbhattacharyya18/onboard_long_term_prediction"&gt;code+data&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Encoding Crowd Interaction with Deep Neural Network for Pedestrian Trajectory Prediction, CVPR 2018. [&lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Encoding_Crowd_Interaction_CVPR_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/ShanghaiTechCVDL/CIDNN"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Walking Ahead: The Headed Social Force Model, 2017. [&lt;a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169734" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Real-time certified probabilistic pedestrian forecasting, 2017. [&lt;a href="https://ieeexplore.ieee.org/document/7959047" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A multiple-predictor approach to human motion prediction, ICRA 2017. [&lt;a href="https://ieeexplore.ieee.org/document/7989265" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Forecasting interactive dynamics of pedestrians with fictitious play, CVPR 2017. [&lt;a href="https://arxiv.org/abs/1604.01431" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Forecast the plausible paths in crowd scenes, IJCAI 2017. [&lt;a href="https://www.ijcai.org/proceedings/2017/386" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bi-prediction: pedestrian trajectory prediction based on bidirectional lstm classification, DICTA 2017. [&lt;a href="https://ieeexplore.ieee.org/document/8227412/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Aggressive, Tense or Shy? Identifying Personality Traits from Crowd Videos, IJCAI 2017. [&lt;a href="https://www.ijcai.org/proceedings/2017/17" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Natural vision based method for predicting pedestrian behaviour in urban environments, ITSC 2017. [&lt;a href="http://ieeexplore.ieee.org/document/8317848/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Human Trajectory Prediction using Spatially aware Deep Attention Models, 2017. [&lt;a href="https://arxiv.org/pdf/1705.09436.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Soft + Hardwired Attention: An LSTM Framework for Human Trajectory Prediction and Abnormal Event Detection, 2017. [&lt;a href="https://arxiv.org/pdf/1702.05552.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Forecasting Interactive Dynamics of Pedestrians with Fictitious Play, CVPR 2017. [&lt;a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Ma_Forecasting_Interactive_Dynamics_CVPR_2017_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social LSTM: Human trajectory prediction in crowded spaces, CVPR 2016. [&lt;a href="http://openaccess.thecvf.com/content_cvpr_2016/html/Alahi_Social_LSTM_Human_CVPR_2016_paper.html" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/quancore/social-lstm"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Comparison and evaluation of pedestrian motion models for vehicle safety systems, ITSC 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7795912" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Age and Group-driven Pedestrian Behaviour: from Observations to Simulations, 2016. [&lt;a href="https://collective-dynamics.eu/index.php/cod/article/view/A3" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Structural-RNN: Deep learning on spatio-temporal graphs, CVPR 2016. [&lt;a href="https://arxiv.org/abs/1511.05298" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/asheshjain399/RNNexp"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Intent-aware long-term prediction of pedestrian motion, ICRA 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7487409" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Context-based detection of pedestrian crossing intention for autonomous driving in urban environments, IROS 2016. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/7759351/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Novel planning-based algorithms for human motion prediction, ICRA 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7487505" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning social etiquette: Human trajectory understanding in crowded scenes, ECCV 2016. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-46484-8_33" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/SajjadMzf/Pedestrian_Datasets_VIS"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;GLMP-realtime pedestrian path prediction using global and local movement patterns, ICRA 2016. [&lt;a href="http://ieeexplore.ieee.org/document/7487768/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Knowledge transfer for scene-specific motion prediction, ECCV 2016. [&lt;a href="https://arxiv.org/abs/1603.06987" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;STF-RNN: Space Time Features-based Recurrent Neural Network for predicting People Next Location, SSCI 2016. [&lt;a href="https://github.com/mhjabreel/STF-RNN"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Goal-directed pedestrian prediction, ICCV 2015. [&lt;a href="https://ieeexplore.ieee.org/document/7406377" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Trajectory analysis and prediction for improved pedestrian safety: Integrated framework and evaluations, 2015. [&lt;a href="https://ieeexplore.ieee.org/document/7225707" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting and recognizing human interactions in public spaces, 2015. [&lt;a href="https://link.springer.com/article/10.1007/s11554-014-0428-8" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning collective crowd behaviors with dynamic pedestrian-agents, 2015. [&lt;a href="https://link.springer.com/article/10.1007/s11263-014-0735-3" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Modeling spatial-temporal dynamics of human movements for predicting future trajectories, AAAI 2015. [&lt;a href="https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10126" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Unsupervised robot learning to predict person motion, ICRA 2015. [&lt;a href="https://ieeexplore.ieee.org/document/7139254" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A controlled interactive multiple model filter for combined pedestrian intention recognition and path prediction, ITSC 2015. [&lt;a href="http://ieeexplore.ieee.org/abstract/document/7313129/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Real-Time Predictive Modeling and Robust Avoidance of Pedestrians with Uncertain, Changing Intentions, 2014. [&lt;a href="https://arxiv.org/abs/1405.5581" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Behavior estimation for a complete framework for human motion prediction in crowded environments, ICRA 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6907734" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian’s trajectory forecast in public traffic with artificial neural network, ICPR 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6977417" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Will the pedestrian cross? A study on pedestrian path prediction, 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6632960" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;BRVO: Predicting pedestrian trajectories using velocity-space reasoning, 2014. [&lt;a href="https://journals.sagepub.com/doi/abs/10.1177/0278364914555543" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Context-based pedestrian path prediction, ECCV 2014. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-10599-4_40" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian path prediction using body language traits, 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6856498/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Online maneuver recognition and multimodal trajectory prediction for intersection assistance using non-parametric regression, 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6856480" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning intentions for improved human motion prediction, 2013. [&lt;a href="https://ieeexplore.ieee.org/document/6766565" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-vehicle-pedestrians-interaction" class="anchor" aria-hidden="true" href="#vehicle-pedestrians-interaction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vehicle-Pedestrians Interaction&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-sport-players-1" class="anchor" aria-hidden="true" href="#sport-players-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sport Players&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Diverse Generation for Multi-Agent Sports Games, CVPR 2019. [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yeh_Diverse_Generation_for_Multi-Agent_Sports_Games_CVPR_2019_paper.html" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Stochastic Prediction of Multi-Agent Interactions from Partial Observations, ICLR 2019. [&lt;a href="http://arxiv.org/abs/1902.09641v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generating Multi-Agent Trajectories using Programmatic Weak Supervision, ICLR 2019. [&lt;a href="http://arxiv.org/abs/1803.07612v6" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative Multi-Agent Behavioral Cloning, ICML 2018. [&lt;a href="http://www.stephanzheng.com/pdf/Zhan_Zheng_Lucey_Yue_Generative_Multi_Agent_Behavioral_Cloning.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Where Will They Go? Predicting Fine-Grained Adversarial Multi-Agent Motion using Conditional Variational Autoencoders, ECCV 2018. [&lt;a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Panna_Felsen_Where_Will_They_ECCV_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Coordinated Multi-Agent Imitation Learning, ICML 2017. [&lt;a href="http://arxiv.org/abs/1703.03121v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generating long-term trajectories using deep hierarchical networks, 2017. [&lt;a href="https://arxiv.org/abs/1706.07138" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning Fine-Grained Spatial Models for Dynamic Sports Play Prediction, ICDM 2014. [&lt;a href="https://ieeexplore.ieee.org/document/7023384/footnotes#footnotes" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative Modeling of Multimodal Multi-Human Behavior, 2018. [&lt;a href="https://arxiv.org/pdf/1803.02015.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-benchmark-and-evaluation-metrics" class="anchor" aria-hidden="true" href="#benchmark-and-evaluation-metrics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmark and Evaluation Metrics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Towards a fatality-aware benchmark of probabilistic reaction prediction in highly interactive driving scenarios, ITSC 2018. [&lt;a href="https://arxiv.org/abs/1809.03478" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;How good is my prediction? Finding a similarity measure for trajectory prediction evaluation, ITSC 2017. [&lt;a href="http://ieeexplore.ieee.org/document/8317825/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Trajnet: Towards a benchmark for human trajectory prediction. [&lt;a href="http://trajnet.epfl.ch/" rel="nofollow"&gt;website&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-others" class="anchor" aria-hidden="true" href="#others"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cyclist trajectory prediction using bidirectional recurrent neural networks, AI 2018. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-030-03991-2_28" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Road infrastructure indicators for trajectory prediction, 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8500678" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using road topology to improve cyclist path prediction, 2017. [&lt;a href="https://ieeexplore.ieee.org/document/7995734/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Trajectory prediction of cyclists using a physical model and an artificial neural network, 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7535484/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jiachenli94</author><guid isPermaLink="false">https://github.com/jiachenli94/Awesome-Interaction-aware-Trajectory-Prediction</guid><pubDate>Sun, 09 Feb 2020 00:14:00 GMT</pubDate></item><item><title>thunlp/NRLPapers #15 in TeX, Today</title><link>https://github.com/thunlp/NRLPapers</link><description>&lt;p&gt;&lt;i&gt;Must-read papers on network representation learning (NRL) / network embedding (NE)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-must-read-papers-on-nrlne" class="anchor" aria-hidden="true" href="#must-read-papers-on-nrlne"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Must-read papers on NRL/NE.&lt;/h2&gt;
&lt;p&gt;NRL: network representation learning. NE: network embedding.&lt;/p&gt;
&lt;p&gt;Contributed by &lt;a href="http://thunlp.org/~tcc/" rel="nofollow"&gt;Cunchao Tu&lt;/a&gt;, Yuan Yao, Zhengyan Zhang, GanquCui, Hao Wang (BUPT), Changxin Tian (BUPT), Jie Zhou and Cheng Yang (BUPT).&lt;/p&gt;
&lt;p&gt;We release &lt;a href="https://github.com/thunlp/openne"&gt;OpenNE&lt;/a&gt;, an open source toolkit for NE/NRL. This repository provides a standard NE/NRL(Network Representation Learning）training and testing framework. Currently, the implemented models in OpenNE include DeepWalk, LINE, node2vec, GraRep, TADW and GCN.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-content" class="anchor" aria-hidden="true" href="#content"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Content&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#survey-papers"&gt;Survey Papers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#models"&gt;Models&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#bacis-models"&gt;Bacis Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#attributed-network"&gt;Attributed Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dynamic-network"&gt;Dynamic Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#heterogeneous-information-network"&gt;Heterogeneous Information Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bipartite-network"&gt;Bipartite Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#directed-network"&gt;Directed Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-models"&gt;Other Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#applications"&gt;Applications&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#natural-language-processing"&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#knowledge-graph"&gt;Knowledge Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#social-network"&gt;Social Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#graph-clustering"&gt;Graph Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#community-detection"&gt;Community Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#recommendation"&gt;Recommendation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-applications"&gt;Other Applications&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-survey-papers" class="anchor" aria-hidden="true" href="#survey-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Survey Papers&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Representation Learning on Graphs: Methods and Applications.&lt;/strong&gt;
&lt;em&gt;William L. Hamilton, Rex Ying, Jure Leskovec.&lt;/em&gt; IEEE Data(base) Engineering Bulletin 2017. &lt;a href="https://arxiv.org/pdf/1709.05584.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Embedding Techniques, Applications, and Performance: A Survey.&lt;/strong&gt;
&lt;em&gt;Palash Goyal, Emilio Ferrara.&lt;/em&gt; Knowledge Based Systems 2017. &lt;a href="https://arxiv.org/pdf/1705.02801.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications.&lt;/strong&gt;
&lt;em&gt;Hongyun Cai, Vincent W. Zheng, Kevin Chen-Chuan Chang.&lt;/em&gt; TKDE 2017. &lt;a href="https://arxiv.org/pdf/1709.07604.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Representation Learning: A Survey.&lt;/strong&gt;
&lt;em&gt;Daokun Zhang, Jie Yin, Xingquan Zhu, Chengqi Zhang.&lt;/em&gt; IEEE Transactions on Big Data 2018. &lt;a href="https://arxiv.org/pdf/1801.05852.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Tutorial on Network Embeddings.&lt;/strong&gt;
&lt;em&gt;Haochen Chen, Bryan Perozzi, Rami Al-Rfou, Steven Skiena.&lt;/em&gt; arxiv 2018. &lt;a href="https://arxiv.org/pdf/1808.02590.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Representation Learning: An Overview.(In Chinese)&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Cheng Yang, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; 2017. &lt;a href="http://engine.scichina.com/publisher/scp/journal/SSI/47/8/10.1360/N112017-00145" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational inductive biases, deep learning, and graph networks.&lt;/strong&gt;
&lt;em&gt;Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu.&lt;/em&gt; arxiv 2018. &lt;a href="https://arxiv.org/pdf/1806.01261.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-models" class="anchor" aria-hidden="true" href="#models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Models&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-bacis-models" class="anchor" aria-hidden="true" href="#bacis-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Bacis Models&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SepNE: Bringing Separability to Network Embedding.&lt;/strong&gt;
&lt;em&gt;Ziyao Li, Liang Zhang, Guojie Song.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4333" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robust Negative Sampling for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Mohammadreza Armandpour, Patrick Ding, Jianhua Huang, Xia Hu.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.stat.tamu.edu/~armand/R-NS.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Structure and Transfer Behaviors Embedding via Deep Prediction Model.&lt;/strong&gt;
&lt;em&gt;Xin Sun, Zenghui Song, Junyu Dong, Yongbo Yu, Claudia Plant, Christian Böhm.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4436" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Simplifying Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Weinberger.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/wu19e/wu19e.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GMNN: Graph Markov Neural Networks.&lt;/strong&gt;
&lt;em&gt;Meng Qu, Yoshua Bengio, Jian Tang.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/qu19a/qu19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stochastic Blockmodels meet Graph Neural Networks.&lt;/strong&gt;
&lt;em&gt;Nikhil Mehta, Lawrence Carin Duke, Piyush Rai.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/mehta19a/mehta19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Disentangled Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, Wenwu Zhu.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/ma19a/ma19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Position-aware Graph Neural Networks.&lt;/strong&gt;
&lt;em&gt;Jiaxuan You, Rex Ying, Jure Leskovec.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/you19b/you19b.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing.&lt;/strong&gt;
&lt;em&gt;Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Harutyunyan, Greg Ver Steeg, Aram Galstyan.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/abu-el-haija19a/abu-el-haija19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph U-Nets.&lt;/strong&gt;
&lt;em&gt;Hongyang Gao, Shuiwang Ji.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/gao19a/gao19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Attention Graph Pooling.&lt;/strong&gt;
&lt;em&gt;Junhyun Lee, Inyeop Lee, Jaewoo Kang.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/lee19c/lee19c.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking.&lt;/strong&gt;
&lt;em&gt;Aleksandar Bojchevski, Stephan Günnemann.&lt;/em&gt; ICLR 2018. &lt;a href="https://arxiv.org/pdf/1707.03815.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling.&lt;/strong&gt;
&lt;em&gt;Jie Chen, Tengfei Ma, Cao Xiao.&lt;/em&gt; ICLR 2018. &lt;a href="https://arxiv.org/pdf/1801.10247.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Attention Networks.&lt;/strong&gt;
&lt;em&gt;Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio.&lt;/em&gt; ICLR 2018. &lt;a href="https://arxiv.org/pdf/1710.10903.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stochastic Training of Graph Convolutional Networks with Variance Reduction.&lt;/strong&gt;
&lt;em&gt;Jianfei Chen, Jun Zhu, Le Song.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1710.10568.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarially Regularized Graph Autoencoder for Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, Chengqi Zhang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0362.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discrete Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiaobo Shen, Shirui Pan, Weiwei Liu, Yew-Soon Ong, Quan-Sen Sun.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0493.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Hashing for Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Qixiang Wang, Shanfeng Wang, Maoguo Gong, Yue Wu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0390.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Inductive Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Ryan A. Rossi, Rong Zhou, Nesreen K. Ahmed.&lt;/em&gt; WWW 2018. &lt;a href="http://ryanrossi.com/pubs/rossi-et-al-WWW18-BigNet.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Active Discriminative Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Li Gao, Hong Yang, Chuan Zhou, Jia Wu, Shirui Pan, Yue Hu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0296.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MILE: A Multi-Level Framework for Scalable Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Jiongqian Liang, Saket Gurukar, Srinivasan Parthasarathy.&lt;/em&gt; arxiv 2018. &lt;a href="https://arxiv.org/pdf/1802.09612.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Out-of-sample extension of graph adjacency spectral embedding.&lt;/strong&gt;
&lt;em&gt;Keith Levin, Farbod Roosta-Khorasani, Michael W. Mahoney, Carey E. Priebe.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1802.06307.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DeepWalk: Online Learning of Social Representations.&lt;/strong&gt;
&lt;em&gt;Bryan Perozzi, Rami Al-Rfou, Steven Skiena.&lt;/em&gt; KDD 2014. &lt;a href="https://arxiv.org/pdf/1403.6652" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/phanein/deepwalk"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non-transitive Hashing with Latent Similarity Componets.&lt;/strong&gt;
&lt;em&gt;Mingdong Ou, Peng Cui, Fei Wang, Jun Wang, Wenwu Zhu.&lt;/em&gt; KDD 2015. &lt;a href="http://cuip.thumedialab.com/papers/KDD-NonTransitiveHashing.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GraRep: Learning Graph Representations with Global Structural Information.&lt;/strong&gt;
&lt;em&gt;Shaosheng Cao, Wei Lu, Qiongkai Xu.&lt;/em&gt; CIKM 2015. &lt;a href="https://www.researchgate.net/profile/Qiongkai_Xu/publication/301417811_GraRep/links/5847ecdb08ae8e63e633b5f2/GraRep.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/ShelsonCao/GraRep"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LINE: Large-scale Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Me.&lt;/em&gt; WWW 2015. &lt;a href="https://arxiv.org/pdf/1503.03578.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/tangjianpku/LINE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Neural Networks for Learning Graph Representations.&lt;/strong&gt;
&lt;em&gt;Shaosheng Cao, Wei Lu, Xiongkai Xu.&lt;/em&gt; AAAI 2016. &lt;a href="https://pdfs.semanticscholar.org/1a37/f07606d60df365d74752857e8ce909f700b3.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/ShelsonCao/DNGR"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Revisiting Semi-supervised Learning with Graph Embeddings.&lt;/strong&gt;
&lt;em&gt;Zhilin Yang, William W. Cohen, Ruslan Salakhutdinov.&lt;/em&gt; ICML 2016. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/yanga16.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Max-Margin DeepWalk: Discriminative Learning of Network Representation.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; IJCAI 2016. &lt;a href="http://thunlp.org/~tcc/publications/ijcai2016_mmdw.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/mmdw"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discriminative Deep RandomWalk for Network Classification.&lt;/strong&gt;
&lt;em&gt;Juzheng Li, Jun Zhu, Bo Zhang.&lt;/em&gt; ACL 2016. &lt;a href="http://www.aclweb.org/anthology/P16-1095" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structural Deep Network Embedding.&lt;/strong&gt;
&lt;em&gt;Daixin Wang, Peng Cui, Wenwu Zhu.&lt;/em&gt; KDD 2016. &lt;a href="http://cuip.thumedialab.com/papers/SDNE.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structural Neighborhood Based Classification of Nodes in a Network.&lt;/strong&gt;
&lt;em&gt;Sharad Nandanwar, M. N. Murty.&lt;/em&gt; KDD 2016. &lt;a href="http://www.kdd.org/kdd2016/papers/files/Paper_679.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Community Preserving Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, Shiqiang Yang.&lt;/em&gt; AAAI 2017. &lt;a href="http://cuip.thumedialab.com/papers/NE-Community.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised Classification with Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Thomas N. Kipf, Max Welling.&lt;/em&gt; ICLR 2017. &lt;a href="https://arxiv.org/pdf/1609.02907.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/tkipf/gcn"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fast Network Embedding Enhancement via High Order Proximity Approximation.&lt;/strong&gt;
&lt;em&gt;Cheng Yang, Maosong Sun, Zhiyuan Liu, Cunchao Tu.&lt;/em&gt; IJCAI 2017. &lt;a href="http://thunlp.org/~tcc/publications/ijcai2017_neu.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/neu"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CANE: Context-Aware Network Embedding for Relation Modeling.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Han Liu, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; ACL 2017. &lt;a href="http://thunlp.org/~tcc/publications/acl2017_cane.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/cane"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A General View for Network Embedding as Matrix Factorization.&lt;/strong&gt;
&lt;em&gt;Xin Liu, Tsuyoshi Murata, Kyoung-Sook Kim, Chatchawan Kotarasu, Chenyi Zhuang.&lt;/em&gt; WSDM 2019. &lt;a href="https://dl.acm.org/citation.cfm?doid=3289600.3291029" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Co-Embedding Attributed Networks.&lt;/strong&gt;
&lt;em&gt;Zaiqiao Meng, Shangsong Liang, Xiangliang Zhang, Hongyan Bao.&lt;/em&gt; WSDM 2019. &lt;a href="https://mine.kaust.edu.sa/Documents/papers/WSDM19attribute.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enhanced Network Embeddings via Exploiting Edge Labels.&lt;/strong&gt;
&lt;em&gt;Haochen Chen, Xiaofei Sun, Yingtao Tian, Bryan Perozzi, Muhao Chen, Steven Skiena.&lt;/em&gt; CIKM 2018. &lt;a href="https://arxiv.org/pdf/1809.05124.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Improve Network Embeddings with Regularization.&lt;/strong&gt;
&lt;em&gt;Yi Zhang, Jianguo Lu, Ofer Shai.&lt;/em&gt; CIKM 2018. &lt;a href="https://jlu.myweb.cs.uwindsor.ca/n2v.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Modeling Multi-way Relations with Hypergraph Embedding.&lt;/strong&gt;
&lt;em&gt;Chia-An Yu, Ching-Lun Tai, Tak-Shing Chan, Yi-Hsuan Yang.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3269274" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/chia-an/HGE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;REGAL: Representation Learning-based Graph Alignment.&lt;/strong&gt;
&lt;em&gt;Mark Heimann, Haoming Shen, Tara Safavi, Danai Koutra.&lt;/em&gt; CIKM 2018. &lt;a href="https://arxiv.org/pdf/1802.06257.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Network Embedding.&lt;/strong&gt;
&lt;em&gt;Quanyu Dai, Qiang Li, Jian Tang, Dan Wang.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.07838.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/sachinbiradar9/Adverserial-Inductive-Deep-Walk"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bernoulli Embeddings for Graphs.&lt;/strong&gt;
&lt;em&gt;Vinith Misra, Sumit Bhatia.&lt;/em&gt; AAAI 2018. &lt;a href="http://sumitbhatia.net/papers/aaai18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GraphGAN: Graph Representation Learning with Generative Adversarial Nets.&lt;/strong&gt;
&lt;em&gt;Hongwei Wang, jia Wang, jialin Wang, MIAO ZHAO, Weinan Zhang, Fuzheng Zhang, Xie Xing, Minyi Guo.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.08267.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HARP: Hierarchical Representation Learning for Networks.&lt;/strong&gt;
&lt;em&gt;Haochen Chen, Bryan Perozzi, Yifan Hu, Steven Skiena.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1706.07845.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/GTmac/HARP"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Social Rank Regulated Large-scale Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yupeng Gu, Yizhou Sun, Yanen Li, Yang Yang.&lt;/em&gt; WWW 2018. &lt;a href="http://yangy.org/works/ge/rare.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Latent Network Summarization: Bridging Network Embedding and Summarization.&lt;/strong&gt;
&lt;em&gt;Di Jin,Ryan Rossi,Danai Koutra,Eunyee Koh,Sungchul Kim,Anup Rao&lt;/em&gt; KDD 2019. &lt;a href="https://arxiv.org/pdf/1811.04461.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching.&lt;/strong&gt;
&lt;em&gt;Dingqi Yang,Paolo Rosso,Bin Li,Philippe Cudre-Mauroux.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330951/p1162-yang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ProGAN: Network Embedding via Proximity Generative Adversarial Network.&lt;/strong&gt;
&lt;em&gt;Hongchang Gao,Jian Pei,Heng Huang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330866/p1308-gao.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Global Alignment Graph Kernel Using Random Features: From Node Embedding to Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Lingfei Wu,Ian En-Hsu Yen,Zhen Zhang,Kun Xu,Liang Zhao,Xi Peng,Yinglong Xia,Charu Aggarwal.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330918/p1418-wu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Graph Embeddings via Sparse Transpose Proximities.&lt;/strong&gt;
&lt;em&gt;Yuan Yin,Zhewei Wei.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330860/p1429-yin.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AutoNRL: Hyperparameter Optimization for Massive Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Ke Tu,Jianxin Ma,Peng Cui,Jian Pei,Wenwu Zhu.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330848/p216-tu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Representation Learning via Hard and Channel-Wise Attention Networks.&lt;/strong&gt;
&lt;em&gt;Hongyang Gao,Shuiwang Ji.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330897/p741-gao.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Training Methods for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Quanyu Dai,Xiao Shen,Liang Zhang,Qiang Li,Dan Wang.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1908.11514.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-relational Network Embeddings with Relational Proximity and Node Attributes.&lt;/strong&gt;
&lt;em&gt;Ming-Han Feng,Chin-Chi Hsu,Cheng-Te Li,Mi-Yen Yeh,Shou-De Lin.&lt;/em&gt; WWW 2019. &lt;a href="https://pdfs.semanticscholar.org/6274/3cbebc142897c6c005f3c12c00b9202ca43f.pdf?_ga=2.108748866.1527570260.1569422306-1231101604.1568798295" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sampled in Pairs and Driven by Text: A New Graph Embedding Framework.&lt;/strong&gt;
&lt;em&gt;Liheng Chen,Yanru Qu,Zhenghui Wang,Weinan Zhang,Ken Chen,Shaodian Zhang,Yong Yu.&lt;/em&gt; WWW 2019. &lt;a href="https://sci-hub.tw/10.1145/3308558.3313520#" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DDGK: Learning Graph Representations via Deep Divergence Graph Kernels.&lt;/strong&gt;
&lt;em&gt;Rami Al-Rfou,Dustin Zelle,Bryan Perozzi.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1904.09671.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tag2Vec: Learning Tag Representations in Tag Networks.&lt;/strong&gt;
&lt;em&gt;Junshan Wang,Zhicong Lu,Guojia Song,Yue Fan,Lun Du,Wei Lin.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1905.03041.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;struc2vec: Learning Node Representations from Structural Identity.&lt;/strong&gt;
&lt;em&gt;Leonardo F. R. Ribeiro, Pedro H. P. Saverese, Daniel R. Figueiredo.&lt;/em&gt; KDD 2017. &lt;a href="https://arxiv.org/pdf/1704.03165.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inductive Representation Learning on Large Graphs.&lt;/strong&gt;
&lt;em&gt;William L. Hamilton, Rex Ying, Jure Leskovec.&lt;/em&gt; NIPS 2017. &lt;a href="https://arxiv.org/pdf/1706.02216.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Graph Embeddings with Embedding Propagation.&lt;/strong&gt;
&lt;em&gt;Alberto Garcia Duran, Mathias Niepert.&lt;/em&gt; NIPS 2017. &lt;a href="https://arxiv.org/pdf/1710.03059.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enhancing the Network Embedding Quality with Structural Similarity.&lt;/strong&gt;
&lt;em&gt;Tianshu Lyu, Yuan Zhang, Yan Zhang.&lt;/em&gt; CIKM 2017. &lt;a href="https://pdfs.semanticscholar.org/e54a/374d7e24260450e2081b93005a491d1b9116.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;An Attention-based Collaboration Framework for Multi-View Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Meng Qu, Jian Tang, Jingbo Shang, Xiang Ren, Ming Zhang, Jiawei Han.&lt;/em&gt; CIKM 2017. &lt;a href="https://arxiv.org/pdf/1709.06636.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;On Embedding Uncertain Graphs.&lt;/strong&gt;
&lt;em&gt;Jiafeng Hu, Reynold Cheng, Zhipeng Huang, Yixang Fang, Siqiang Luo.&lt;/em&gt; CIKM 2017. &lt;a href="https://i.cs.hku.hk/~zphuang/pub/CIKM17.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec.&lt;/strong&gt;
&lt;em&gt;Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang.&lt;/em&gt; WSDM 2018. &lt;a href="https://arxiv.org/pdf/1710.02971.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Conditional Network Embeddings.&lt;/strong&gt;
&lt;em&gt;Bo Kang, Jefrey Lijffijt, Tijl De Bie.&lt;/em&gt; ICLR 2019. &lt;a href="https://arxiv.org/abs/1805.07544" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Graph Infomax.&lt;/strong&gt;
&lt;em&gt;Petar Veličković, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, R Devon Hjelm.&lt;/em&gt; ICLR 2019. &lt;a href="https://arxiv.org/abs/1809.10341" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Anonymous Walk Embeddings.&lt;/strong&gt;
&lt;em&gt;Sergey Ivanov, Evgeny Burnaev.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1805.11921.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fairwalk: Towards Fair Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Tahleen Rahman, Bartlomiej Surma, Michael Backes, Yang Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://yangzhangalmo.github.io/papers/IJCAI19.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph and Autoencoder Based Feature Extraction for Zero-shot Learning.&lt;/strong&gt;
&lt;em&gt;Yang Liu, Deyan Xie, Quanxue Gao, Jungong Han, Shujian Wang, Xinbo Gao.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0421.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Space Embedding.&lt;/strong&gt;
&lt;em&gt;João Pereira, Evgeni Levin, Erik Stroes, Albert Groen.&lt;/em&gt; IJCAI 2019. &lt;a href="https://arxiv.org/abs/1907.13443" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Arbitrary-Order Proximity Preserved Network Embedding.&lt;/strong&gt;
&lt;em&gt;Ziwei Zhang, Peng Cui, Xiao Wang, Jian Pei, Xuanrong Yao, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="http://cuip.thumedialab.com/papers/NE-ArbitraryProximity.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Variational Network Embedding in Wasserstein Space.&lt;/strong&gt;
&lt;em&gt;Dingyuan Zhu, Peng Cui, Daixin Wang, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="http://cuip.thumedialab.com/papers/NE-DeepVariational.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MEGAN: A Generative Adversarial Network for Multi-View Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yiwei Sun, Suhang Wang, Tsung-Yu Hsieh, Xianfeng Tang, Vasant Honavar.&lt;/em&gt; IJCAI 2019. &lt;a href="https://arxiv.org/abs/1909.01084" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Embedding under Partial Monitoring for Evolving Networks&lt;/strong&gt;
&lt;em&gt;Yu Han, Jie Tang, Qian Chen.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0342.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Embedding with Dual Generation Tasks.&lt;/strong&gt;
&lt;em&gt;Jie Liu, Na Li, Zhicheng He.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0709.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Triplet Enhanced AutoEncoder: Model-free Discriminative Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yao Yang, Haoran Chen, Junming Shao.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0745.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Recursive Network Embedding with Regular Equivalence.&lt;/strong&gt;
&lt;em&gt;Ke Tu, Peng Cui, Xiao Wang, Philip S. Yu, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="http://cuip.thumedialab.com/papers/NE-RegularEquivalence.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Structural Node Embeddings via Diffusion Wavelets.&lt;/strong&gt;
&lt;em&gt;Claire Donnat, Marinka Zitnik, David Hallac, Jure Leskovec.&lt;/em&gt; KDD 2018. &lt;a href="https://arxiv.org/pdf/1710.10321.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Paced Network Embedding.&lt;/strong&gt;
&lt;em&gt;Hongchang Gao, Heng Huang.&lt;/em&gt; KDD 2018. &lt;a href="https://par.nsf.gov/servlets/purl/10074506" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Deep Network Representations with Adversarially Regularized Autoencoders.&lt;/strong&gt;
&lt;em&gt;Wenchao Yu, Cheng Zheng, Wei Cheng, Charu Aggarwal, Dongjin Song, Bo Zong, Haifeng Chen, Wei Wang.&lt;/em&gt; KDD 2018. &lt;a href="https://sites.cs.ucsb.edu/~bzong/doc/kdd-18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Large-Scale Learnable Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Hongyang Gao, Zhengyang Wang, Shuiwang Ji.&lt;/em&gt; KDD 2018. &lt;a href="https://arxiv.org/pdf/1808.03965" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-attributed-network" class="anchor" aria-hidden="true" href="#attributed-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Attributed Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Outlier Aware Network Embedding for Attributed Networks.&lt;/strong&gt;
&lt;em&gt;Sambaran Bandyopadhyay, N. Lokesh, M. N. Murty.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/3763" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Large-Scale Heterogeneous Feature Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Qingquan Song, Fan Yang, Xia Hu.&lt;/em&gt; AAAI 2019. &lt;a href="https://aaai.org/ojs/index.php/AAAI/article/view/4276" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Bayesian Optimization on Attributed Graphs.&lt;/strong&gt;
&lt;em&gt;Jiaxu Cui, Bo Yang, Xia Hu.&lt;/em&gt; AAAI 2019. &lt;a href="https://arxiv.org/pdf/1905.13403.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Efficient Attributed Network Embedding via Recursive Randomized Hashing.&lt;/strong&gt;
&lt;em&gt;Wei Wu, Bin Li, Ling Chen, Chengqi Zhang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0397.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Hongchang Gao, Heng Huang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0467.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ANRL: Attributed Network Representation Learning via Deep Neural Networks.&lt;/strong&gt;
&lt;em&gt;Zhen Zhang, Hongxia Yang, Jiajun Bu, Sheng Zhou, Pinggang Yu, Jianwei Zhang, Martin Ester, Can Wang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0438.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrative Network Embedding via Deep Joint Reconstruction.&lt;/strong&gt;
&lt;em&gt;Di Jin, Meng Ge, Liang Yang, Dongxiao He, Longbiao Wang, Weixiong Zhang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0473.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;node2vec: Scalable Feature Learning for Networks.&lt;/strong&gt;
&lt;em&gt;Aditya Grover, Jure Leskovec.&lt;/em&gt; KDD 2016. &lt;a href="http://www.kdd.org/kdd2016/papers/files/rfp0218-groverA.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/aditya-grover/node2vec"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Representation Learning with Rich Text Information.&lt;/strong&gt;
&lt;em&gt;Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, Edward Y. Chang.&lt;/em&gt; IJCAI 2015. &lt;a href="http://thunlp.org/~yangcheng/publications/ijcai15.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/tadw"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tri-Party Deep Network Representation.&lt;/strong&gt;
&lt;em&gt;Shirui Pan, Jia Wu, Xingquan Zhu, Chengqi Zhang, Yang Wang.&lt;/em&gt; IJCAI 2016. &lt;a href="https://www.ijcai.org/Proceedings/16/Papers/271.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TransNet: Translation-Based Network Representation Learning for Social Relation Extraction.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; IJCAI 2017. &lt;a href="http://thunlp.org/~tcc/publications/ijcai2017_transnet.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/transnet"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PRRE: Personalized Relation Ranking Embedding for Attributed Networks.&lt;/strong&gt;
&lt;em&gt;Sheng Zhou, Hongxia Yang, Xin Wang, Jiajun Bu, Martin Ester, Pinggang Yu, Jianwei Zhang, Can Wang.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3271741" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/zhoushengisnoob/PRRE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RSDNE: Exploring Relaxed Similarity and Dissimilarity from Completely-imbalanced Labels for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Zheng Wang, Xiaojun Ye, Chaokun Wang, YueXin Wu, Changping Wang, Kaiwen Liang.&lt;/em&gt; AAAI 2018. &lt;a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16062/15722" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/zhengwang100/RSDNE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised embedding in attributed networks with outliers.&lt;/strong&gt;
&lt;em&gt;Jiongqian Liang, Peter Jacobs, Jiankai Sun, and Srinivasan Parthasarathy.&lt;/em&gt; SDM 2018. &lt;a href="https://arxiv.org/pdf/1703.08100.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="http://jiongqianliang.com/SEANO/" rel="nofollow"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Representation Learning Framework for Property Graphs.&lt;/strong&gt;
&lt;em&gt;Yifan Hou,Hongzhi Chen,Changji Li,James Cheng,Ming-Chang Yang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330948/p65-hou.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning from Labeled and Unlabeled Vertices in Networks.&lt;/strong&gt;
&lt;em&gt;Wei Ye, Linfei Zhou, Dominik Mautz, Claudia Plant, Christian B?hm.&lt;/em&gt; KDD 2017. &lt;a href="https://dl.acm.org/citation.cfm?id=3098142" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Label Informed Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Jundong Li, Xia Hu.&lt;/em&gt; WSDM 2017. &lt;a href="http://www.public.asu.edu/~jundongl/paper/WSDM17_LANE.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Accelerated Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Jundong Li, Xia Hu.&lt;/em&gt; SDM 2017. &lt;a href="http://www.public.asu.edu/~jundongl/paper/SDM17_AANE.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variation Autoencoder Based Network Representation Learning for Classification.&lt;/strong&gt;
&lt;em&gt;Hang Li, Haozheng Wang, Zhenglu Yang, Masato Odagaki.&lt;/em&gt; ACL 2017. &lt;a href="https://aclweb.org/anthology/P17-3010" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Attributed Signed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Suhang Wang, Charu Aggarwal, Jiliang Tang, Huan Liu.&lt;/em&gt; CIKM 2017. &lt;a href="https://suhangwang.ist.psu.edu/publications/SNEA.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;From Properties to Links: Deep Network Embedding on Incomplete Graphs.&lt;/strong&gt;
&lt;em&gt;Dejian Yang, Senzhang Wang, Chaozhuo Li, Xiaoming Zhang, Zhoujun Li.&lt;/em&gt; CIKM 2017. &lt;a href="https://dl.acm.org/citation.cfm?id=3132847.3132975" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exploring Expert Cognition for Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Qingquan Song, Jundong Li, Xia Ben Hu.&lt;/em&gt; WSDM 2018. &lt;a href="http://www.public.asu.edu/~jundongl/paper/WSDM18_NEEC.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hierarchical Taxonomy Aware Network Embedding.&lt;/strong&gt;
&lt;em&gt;Jianxin Ma, Peng Cui, Xiao Wang, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="https://jianxinma.github.io/assets/NetHiex.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network-Specific Variational Auto-Encoder for Embedding in Attribute Networks.&lt;/strong&gt;
&lt;em&gt;Di Jin, Bingyi Li, Pengfei Jiao, Dongxiao He, Weixiong Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0370.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SPINE: Structural Identity Preserved Inductive Network Embedding.&lt;/strong&gt;
&lt;em&gt;Junliang Guo, Linli Xu, Jingchang Liu.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0333.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Content to Node: Self-translation Network Embedding.&lt;/strong&gt;
&lt;em&gt;Jie Liu, Zhicheng He, Lai Wei, Yalou Huang.&lt;/em&gt; KDD 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3219988" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-dynamic-network" class="anchor" aria-hidden="true" href="#dynamic-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Dynamic Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Network Embedding : An Extended Approach for Skip-gram based Network Embedding.&lt;/strong&gt;
&lt;em&gt;Lun Du, Yun Wang, Guojie Song, Zhicong Lu, Junshan Wang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0288.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Network Embedding by Modeling Triadic Closure Process.&lt;/strong&gt;
&lt;em&gt;Lekui Zhou, Yang Yang, Xiang Ren, Fei Wu, Yueting Zhuang.&lt;/em&gt; AAAI 2018. &lt;a href="http://yangy.org/works/dynamictriad/dynamic_triad.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DepthLGP: Learning Embeddings of Out-of-Sample Nodes in Dynamic Networks.&lt;/strong&gt;
&lt;em&gt;Jianxin Ma, Peng Cui, Wenwu Zhu.&lt;/em&gt; AAAI 2018. &lt;a href="https://jianxinma.github.io/assets/DepthLGP.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TIMERS: Error-Bounded SVD Restart on Dynamic Networks.&lt;/strong&gt;
&lt;em&gt;Ziwei Zhang, Peng Cui, Jian Pei, Xiao Wang, Wenwu Zhu.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.09541.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks.&lt;/strong&gt;
&lt;em&gt;Srijan Kumar,Xikun Zhang,Jure Leskovec.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330895/p1269-kumar.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Attributed Network Embedding for Learning in a Dynamic Environment.&lt;/strong&gt;
&lt;em&gt;Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, Huan Liu.&lt;/em&gt; CIKM 2017. &lt;a href="https://arxiv.org/pdf/1706.01860.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DyRep: Learning Representations over Dynamic Graphs.&lt;/strong&gt;
&lt;em&gt;Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, Hongyuan Zha.&lt;/em&gt; ICLR 2019. &lt;a href="https://openreview.net/forum?id=HyePrhR5KX" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedding Temporal Network via Neighborhood Formation.&lt;/strong&gt;
&lt;em&gt;Yuan Zuo, Guannan Liu, Hao Lin, Jia Guo, Xiaoqian Hu, Junjie Wu.&lt;/em&gt; KDD 2018. &lt;a href="https://zuoyuan.github.io/files/htne_kdd18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Node Embedding over Temporal Graphs.&lt;/strong&gt;
&lt;em&gt;Uriel Singer, Ido Guy, Kira Radinsky.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0640.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Embeddings for User Profiling in Twitter.&lt;/strong&gt;
&lt;em&gt;Shangsong Liang, Xiangliang Zhang, Zhaochun Ren, Evangelos Kanoulas.&lt;/em&gt; KDD 2018. &lt;a href="https://repository.kaust.edu.sa/bitstream/handle/10754/628781/p1764-liang.pdf?sequence=1&amp;amp;isAllowed=y" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NetWalk: A Flexible Deep Embedding Approach for Anomaly Detection in Dynamic Networks.&lt;/strong&gt;
&lt;em&gt;Wenchao Yu, Wei Cheng, Charu Aggarwal, Kai Zhang, Haifeng Chen, Wei Wang.&lt;/em&gt; KDD 2018. &lt;a href="http://www.shichuan.org/hin/topic/Embedding/2018.KDD%202018%20NetWalk_A%20Flexible%20Deep%20Embedding%20Approach%20for%20Anomaly%20Detection%20in%20Dynamic%20Networks.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Optimization for Embedding Highly-Dynamic and Recency-Sensitive Data.&lt;/strong&gt;
&lt;em&gt;Xumin Chen, Peng Cui, Shiqiang Yang.&lt;/em&gt; KDD 2018. &lt;a href="http://pengcui.thumedialab.com/papers/NE-ScalableOptimization.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-heterogeneous-information-network" class="anchor" aria-hidden="true" href="#heterogeneous-information-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Heterogeneous Information Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relation Structure-Aware Heterogeneous Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yuanfu Lu, Chuan Shi, Linmei Hu, Zhiyuan Liu.&lt;/em&gt; AAAI 2019. &lt;a href="https://arxiv.org/abs/1905.08027" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hyperbolic Heterogeneous Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Wang, Yiding Zhang, Chuan Shi.&lt;/em&gt; AAAI 2019. &lt;a href="http://shichuan.org/doc/65.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Latent Representations of Nodes for Classifying in Heterogeneous Social Networks.&lt;/strong&gt;
&lt;em&gt;Yann Jacob, Ludovic Denoyer, Patrick Gallinar.&lt;/em&gt; WSDM 2014. &lt;a href="http://webia.lip6.fr/~gallinar/gallinari/uploads/Teaching/WSDM2014-jacob.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Heterogeneous Network Embedding via Deep Architectures.&lt;/strong&gt;
&lt;em&gt;Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C. Aggarwal, Thomas S. Huang.&lt;/em&gt; KDD 2015. &lt;a href="http://www.ifp.illinois.edu/~chang87/papers/kdd_2015.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;metapath2vec: Scalable Representation Learning for Heterogeneous Networks.&lt;/strong&gt;
&lt;em&gt;Yuxiao Dong, Nitesh V. Chawla, Ananthram Swami.&lt;/em&gt; KDD 2017. &lt;a href="https://www3.nd.edu/~dial/publications/dong2017metapath2vec.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://ericdongyx.github.io/metapath2vec/m2v.html" rel="nofollow"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SHNE: Representation Learning for Semantic-Associated Heterogeneous Networks.&lt;/strong&gt;
&lt;em&gt;Chuxu Zhang, Ananthram Swami, Nitesh V. Chawla.&lt;/em&gt; WSDM 2019. &lt;a href="https://dl.acm.org/citation.cfm?id=3291001" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/chuxuzhang/WSDM2019_SHNE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Are Meta-Paths Necessary?: Revisiting Heterogeneous Graph Embeddings.&lt;/strong&gt;
&lt;em&gt;Rana Hussein, Dingqi Yang, Philippe Cudré-Mauroux.&lt;/em&gt; CIKM 2018. &lt;a href="https://exascale.info/assets/pdf/hussein2018cikm.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Abnormal Event Detection via Heterogeneous Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Shaohua Fan, Chuan Shi, Xiao Wang.&lt;/em&gt; CIKM 2018. &lt;a href="http://shichuan.org/doc/62.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multidimensional Network Embedding with Hierarchical Structures.&lt;/strong&gt;
&lt;em&gt;Yao Ma, Zhaochun Ren, Ziheng Jiang, Jiliang Tang, Dawei Yin.&lt;/em&gt; WSDM 2018. &lt;a href="http://cse.msu.edu/~mayao4/downloads/Multidimensional_Network_Embedding_with_Hierarchical_Structure.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Curriculum Learning for Heterogeneous Star Network Embedding via Deep Reinforcement Learning.&lt;/strong&gt;
&lt;em&gt;Meng Qu, Jian Tang, Jiawei Han.&lt;/em&gt; WSDM 2018. &lt;a href="http://delivery.acm.org/10.1145/3160000/3159711/p468-qu.pdf?ip=203.205.141.123&amp;amp;id=3159711&amp;amp;acc=ACTIVE%20SERVICE&amp;amp;key=39FCDE838982416F%2E39FCDE838982416F%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;amp;__acm__=1519788484_7383495a5c522cbe124e62e4d768f8cc" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generative Adversarial Network based Heterogeneous Bibliographic Network Representation for Personalized Citation Recommendation.&lt;/strong&gt;
&lt;em&gt;J. Han, Xiaoyan Cai, Libin Yang.&lt;/em&gt; AAAI 2018. &lt;a href="https://pdfs.semanticscholar.org/1596/d6487012696ba400fb69904a2c372a08a2be.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distance-aware DAG Embedding for Proximity Search on Heterogeneous Graphs.&lt;/strong&gt;
&lt;em&gt;Zemin Liu, Vincent W. Zheng, Zhou Zhao, Fanwei Zhu, Kevin Chen-Chuan Chang, Minghui Wu, Jing Ying.&lt;/em&gt; AAAI 2018. &lt;a href="https://pdfs.semanticscholar.org/b1cc/127a65c40e71121106d0c663f9b5baf9d6f9.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Representation Learning for Attributed Multiplex Heterogeneous Network.&lt;/strong&gt;
&lt;em&gt;Yukuo Cen,Xu Zou,Jianwei Zhang,Hongxia Yang,Jingren Zhou,Jie Tang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330964/p1358-cen.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Learning on Heterogeneous Information Networks.&lt;/strong&gt;
&lt;em&gt;Binbin Hu,Yuan Fang,Chuan Shi&lt;/em&gt; KDD 2019 &lt;a href="http://delivery.acm.org/10.1145/3340000/3330970/p120-hu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HetGNN: Heterogeneous Graph Neural Network.&lt;/strong&gt;
&lt;em&gt;Chuxu Zhang,Dongjin Song,Chao Huang,Ananthram Swami,Nitesh V. Chawla.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330961/p793-zhang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;IntentGC: a Scalable Graph Convolution Framework Fusing Heterogeneous Information for Recommendation&lt;/strong&gt;
&lt;em&gt;Jun Zhao, Zhou Zhou, Ziyu Guan, Wei Zhao, Ning Wei, Guang Qiu and Xiaofei He.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330686/p2347-zhao.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation.&lt;/strong&gt;
&lt;em&gt;Shaohua Fan, Junxiong Zhu, Xiaotian Han, Chuan Shi, Linmei Hu, Biyu Ma and Yongliang Li.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330673/p2478-fan.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Your Style Your Identity: Leveraging Writing and Photography Styles for Drug Trafficker Identification in Darknet Markets over Attributed Heterogeneous Information Network.&lt;/strong&gt;
&lt;em&gt;Yiming Zhang, Yujie Fan,Wei Song, Shifu HouYanfang Ye, Xin Li,Liang Zhao,Chuan Shi,Jiabin Wang, Qi Xiong.&lt;/em&gt; WWW 2019. &lt;a href="https://www.gwern.net/docs/sr/2019-zhang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HIN2Vec: Explore Meta-paths in Heterogeneous Information Networks for Representation Learning.&lt;/strong&gt;
&lt;em&gt;Tao-yang Fu, Wang-Chien Lee, Zhen Lei.&lt;/em&gt; CIKM 2017. &lt;a href="http://shichuan.org/hin/topic/Embedding/2017.%20CIKM%20HIN2Vec.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction.&lt;/strong&gt;
&lt;em&gt;Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, Qi Liu.&lt;/em&gt; WSDM 2018. &lt;a href="https://arxiv.org/pdf/1712.00732.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ActiveHNE: Active Heterogeneous Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xia Chen, Guoxian Yu, Jun Wang, Carlotta Domeniconi, Zhao Li, Xiangliang Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0294.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unified Embedding Model over Heterogeneous Information Network for Personalized Recommendation.&lt;/strong&gt;
&lt;em&gt;Zekai Wang, Hongzhi Liu, Yingpeng Du, Zhonghai Wu, Xing Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0529.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Easing Embedding Learning by Comprehensive Transcription of Heterogeneous Information Networks.&lt;/strong&gt;
&lt;em&gt;Yu Shi, Qi Zhu, Fang Guo, Chao Zhang, Jiawei Han.&lt;/em&gt; KDD 2018. &lt;a href="https://yu-shi-homepage.github.io/kdd18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PME: Projected Metric Embedding on Heterogeneous Networks for Link Prediction.&lt;/strong&gt;
&lt;em&gt;Hongxu Chen, Hongzhi Yin, Weiqing Wang, Hao Wang, Quoc Viet Hung Nguyen, Xue Li.&lt;/em&gt; KDD 2018. &lt;a href="http://net.pku.edu.cn/daim/hongzhi.yin/papers/KDD18-Hongxu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-bipartite-network" class="anchor" aria-hidden="true" href="#bipartite-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Bipartite Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Collaborative Similarity Embedding for Recommender Systems.&lt;/strong&gt;
&lt;em&gt;Chih-Ming Chen,Chuan-Ju Wang,Ming-Feng Tsai,Yi-Hsuan Yang.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1902.06188.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Node Embeddings in Interaction Graphs.&lt;/strong&gt;
&lt;em&gt;Yao Zhang, Yun Xiong, Xiangnan Kong, Yangyong Zhu.&lt;/em&gt; CIKM 2017. &lt;a href="https://web.cs.wpi.edu/~xkong/publications/papers/cikm17.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hierarchical Representation Learning for Bipartite Graphs.&lt;/strong&gt;
&lt;em&gt;Chong Li, Kunyang Jia, Dan Shen, C.J. Richard Shi, Hongxia Yang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0398.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-directed-network" class="anchor" aria-hidden="true" href="#directed-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Directed Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ATP: Directed Graph Embedding with Asymmetric Transitivity Preservation.&lt;/strong&gt;
&lt;em&gt;Jiankai Sun, Bortik Bandyopadhyay, Armin Bashizade, Jiongqian Liang, P. Sadayappan, Srinivasan Parthasarathy.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/3794" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Asymmetric Transitivity Preserving Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, Wenwu Zhu.&lt;/em&gt; KDD 2016. &lt;a href="http://cuip.thumedialab.com/papers/hoppe.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;"Bridge": Enhanced Signed Directed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yiqi Chen, Tieyun Qian, Huan Liu, Ke Sun.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3271738" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SIDE: Representation Learning in Signed Directed Networks.&lt;/strong&gt;
&lt;em&gt;Junghwan Kim, Haekyu Park, Ji-Eun Lee, U Kang.&lt;/em&gt; WWW 2018. &lt;a href="https://datalab.snu.ac.kr/side/resources/side.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-other-models" class="anchor" aria-hidden="true" href="#other-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Other Models&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Multiplex Network Embedding. （Multiplex Network)&lt;/strong&gt;
&lt;em&gt;Hongming Zhang, Liwei Qiu, Lingling Yi, Yangqiu Song.&lt;/em&gt; IJCAI 2018. &lt;a href="http://www.cse.ust.hk/~yqsong/papers/2018-IJCAI-MultiplexNetworkEmbedding.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structural Deep Embedding for Hyper-Networks. (Hyper-Network)&lt;/strong&gt;
&lt;em&gt;Ke Tu, Peng Cui, Xiao Wang, fei Wang, Wenwu Zhu.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.10146.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Representation Learning for Scale-free Networks. (Scale-free Network)&lt;/strong&gt;
&lt;em&gt;Rui Feng, Yang Yang, Wenjie Hu, Fei Wu, Yueting Zhuang.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.10755.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Co-Regularized Deep Multi-Network Embedding. (Multi-Network)&lt;/strong&gt;
&lt;em&gt;Jingchao Ni, Shiyu Chang, Xiao Liu, Wei Cheng, Haifeng Chen, Dongkuan Xu, Xiang Zhang.&lt;/em&gt; WWW 2018. &lt;a href="https://nijingchao.github.io/paper/www18_dmne.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Joint Link Prediction and Network Alignment via Cross-graph Embedding. (Multi-Network)&lt;/strong&gt;
&lt;em&gt;Xingbo Du, Junchi Yan, Hongyuan Zha.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0312.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DANE: Domain Adaptive Network Embedding. (Multi-Network)&lt;/strong&gt;
&lt;em&gt;Yizhou Zhang, Guojie Song, Lun Du, Shuwen Yang, Yilun Jin.&lt;/em&gt; IJCAI 2019. &lt;a href="https://arxiv.org/abs/1906.00684" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SPARC: Self-Paced Network Representation for Few-Shot Rare Category Characterization. (Few-Shot Learning)&lt;/strong&gt;
&lt;em&gt;Dawei Zhou, Jingrui He, Hongxia Yang, Wei Fan.&lt;/em&gt; KDD 2018. &lt;a href="https://dl.acm.org/authorize?N665885" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-applications" class="anchor" aria-hidden="true" href="#applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Applications&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-natural-language-processing" class="anchor" aria-hidden="true" href="#natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Natural Language Processing&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Personalized Question Routing via Heterogeneous Network Embedding.&lt;/strong&gt;
&lt;em&gt;Zeyu Li, Jyun-Yu Jiang, Yizhou Sun, Wei Wang.&lt;/em&gt; AAAI 2019. &lt;a href="http://web.cs.ucla.edu/~yzsun/papers/2019_AAAI_QR.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks.&lt;/strong&gt;
&lt;em&gt;Jian Tang, Meng Qu, Qiaozhu Mei.&lt;/em&gt; KDD 2015. &lt;a href="https://arxiv.org/pdf/1508.00200.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/mnqu/PTE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-knowledge-graph" class="anchor" aria-hidden="true" href="#knowledge-graph"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Knowledge Graph&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interaction Embeddings for Prediction and Explanation in Knowledge Graphs.&lt;/strong&gt;
&lt;em&gt;Wen Zhang, Bibek Paudel, Wei Zhang, Abraham Bernstein, Huajun Chen.&lt;/em&gt; WSDM 2019. &lt;a href="https://arxiv.org/pdf/1903.04750.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Shared Embedding Based Neural Networks for Knowledge Graph Completion.&lt;/strong&gt;
&lt;em&gt;Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, Xueqi Cheng.&lt;/em&gt; CIKM 2018 &lt;a href="https://dl.acm.org/citation.cfm?id=3271705" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Re-evaluating Embedding-Based Knowledge Graph Completion Methods.&lt;/strong&gt;
&lt;em&gt;Farahnaz Akrami, Lingbing Guo, Wei Hu, Chengkai Li.&lt;/em&gt; CIKM 2018. &lt;a href="http://ranger.uta.edu/~cli/pubs/2018/kgcompletion-cikm18short-akrami.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-social-network" class="anchor" aria-hidden="true" href="#social-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Social Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Learning for Weakly-Supervised Social Network Alignment.&lt;/strong&gt;
&lt;em&gt;Chaozhuo Li, Senzhang Wang, Yukun Wang, Philip Yu, Yanbo Liang, Yun Liu, Zhoujun Li.&lt;/em&gt; AAAI 2019. &lt;a href="https://aaai.org/ojs/index.php/AAAI/article/view/3889" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TransConv: Relationship Embedding in Social Networks.&lt;/strong&gt;
&lt;em&gt;Yi-Yu Lai, Jennifer Neville, Dan Goldwasser.&lt;/em&gt; AAAI 2019. &lt;a href="https://aaai.org/ojs/index.php/AAAI/article/view/4314" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised User Geolocation via Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Afshin Rahimi, Trevor Cohn, Timothy Baldwin.&lt;/em&gt; ACL 2018. &lt;a href="https://arxiv.org/pdf/1804.08049.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MASTER: across Multiple social networks, integrate Attribute and STructure Embedding for Reconciliation.&lt;/strong&gt;
&lt;em&gt;Sen Su, Li Sun, Zhongbao Zhang, Gen Li, Jielun Qu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0537.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MEgo2Vec: Embedding Matched Ego Networks for User Alignment Across Social Networks.&lt;/strong&gt;
&lt;em&gt;Jing Zhang, Bo Chen, Xianming Wang, Hong Chen, Cuiping Li, Fengmei Jin, Guojie Song, Yutao Zhang.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3271705" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Link Prediction via Subgraph Embedding-Based Convex Matrix Completion.&lt;/strong&gt;
&lt;em&gt;Zhu Cao, Linlin Wang, Gerard De melo.&lt;/em&gt; AAAI 2018. &lt;a href="http://iiis.tsinghua.edu.cn/~weblt/papers/link-prediction-subgraphembeddings.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;On Exploring Semantic Meanings of Links for Embedding Social Networks.&lt;/strong&gt;
&lt;em&gt;Linchuan Xu, Xiaokai Wei, Jiannong Cao, Philip S Yu.&lt;/em&gt; WWW 2018. &lt;a href="https://pdfs.semanticscholar.org/ccd3/ede78393628b5f0256ebfccbb4ac293394de.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MCNE: An End-to-End Framework for Learning Multiple Conditional Network Representations of Social Network.&lt;/strong&gt;
&lt;em&gt;Hao Wang,Tong Xu,Qi Liu,Defu Lian,Enhong Chen,Dongfang Du,Han Wu,Wen Su.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330931/p1064-wang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unsupervised Feature Selection in Signed Social Networks.&lt;/strong&gt;
&lt;em&gt;Kewei Cheng, Jundong Li, Huan Liu.&lt;/em&gt; KDD 2017. &lt;a href="http://www.public.asu.edu/~jundongl/paper/KDD17_SignedFS.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Network Embedding with Community Structural Information.&lt;/strong&gt;
&lt;em&gt;Yu Li, Ying Wang, Tingting Zhang, Jiawei Zhang, Yi Chang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0407.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-graph-clustering" class="anchor" aria-hidden="true" href="#graph-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Graph Clustering&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spectral Clustering in Heterogeneous Information Networks.&lt;/strong&gt;
&lt;em&gt;Xiang Li , Ben Kao, Zhaochun Ren, Dawei Yin.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.researchgate.net/profile/Xiang_Li238/publication/332606853_Spectral_Clustering_in_Heterogeneous_Information_Networks/links/5cc035e892851c8d2200aa29/Spectral-Clustering-in-Heterogeneous-Information-Networks.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-view Clustering with Graph Embedding for Connectome Analysis.&lt;/strong&gt;
&lt;em&gt;Guixiang Ma, Lifang He, Chun-Ta Lu, Weixiang Shao, Philip S Yu, Alex D Leow, Ann B Ragin.&lt;/em&gt; CIKM 2017. &lt;a href="https://www.cs.uic.edu/~clu/doc/cikm17_mcge.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Graph Embedding for Ensemble Clustering.&lt;/strong&gt;
&lt;em&gt;Zhiqiang Tao, Hongfu Liu, Jun Li, Zhaowen Wang, Yun Fu.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0494.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variational Graph Embedding and Clustering with Laplacian Eigenmaps.&lt;/strong&gt;
&lt;em&gt;Zitai Chen, Chuan Chen, Zong Zhang, Zibin Zheng, Qingsong Zou.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0297.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-community-detection" class="anchor" aria-hidden="true" href="#community-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Community Detection&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Incorporating Network Embedding into Markov Random Field for Better Community Detection.&lt;/strong&gt;
&lt;em&gt;Di Jin, Xinxin You, Weihao Li, Dongxiao He, Peng Cui, Francoise Fogelman-Soulie, Tanmoy Chakraborty.&lt;/em&gt; AAAI 2019. &lt;a href="http://pengcui.thumedialab.com/papers/NE-MRF.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Unified Framework for Community Detection and Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Xiangkai Zeng, Hao Wang, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun, Bo Zhang, Leyu Lin.&lt;/em&gt; TKDE 2018. &lt;a href="https://ieeexplore.ieee.org/abstract/document/8403293/" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;COSINE: Community-Preserving Social Network Embedding from Information Diffusion Cascades.&lt;/strong&gt;
&lt;em&gt;Yuan Zhang, Tianshu Lyu, Yan Zhang.&lt;/em&gt; AAAI 2018. &lt;a href="https://pdfs.semanticscholar.org/fec8/24c51b59063ba92b66bb7404010954ced5ac.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-facet Network Embedding: Beyond the General Solution of Detection and Representation.&lt;/strong&gt;
&lt;em&gt;Liang Yang, Xiaochun Cao, Yuanfang Guo.&lt;/em&gt; AAAI 2018. &lt;a href="https://yangliang.github.io/pdf/aaai18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Community Detection in Attributed Graphs: An Embedding Approach.&lt;/strong&gt;
&lt;em&gt;Ye Li, Chaofeng Sha, Xin Huang, Yanchun Zhang.&lt;/em&gt; AAAI 2018. &lt;a href="https://www.comp.hkbu.edu.hk/~xinhuang/publications/pdfs/AAAI18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Preserving Proximity and Global Ranking for Node Embedding.&lt;/strong&gt;
&lt;em&gt;Yi-An Lai, Chin-Chi Hsu, Wenhao Chen, Mi-Yen Yeh, Shou-De Lin.&lt;/em&gt; NIPS 2017. &lt;a href="https://pdfs.semanticscholar.org/b692/c82115889115ef3e63fb7e6b23c8eb9c85b3.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Community Embedding with Community Detection and Node Embedding on Graphs.&lt;/strong&gt;
&lt;em&gt;Sandro Cavallari, Vincent W. Zheng, Hongyun Cai, Kevin ChenChuan Chang, Erik Cambria.&lt;/em&gt; CIKM 2017. &lt;a href="https://sentic.net/community-embedding.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-recommendation" class="anchor" aria-hidden="true" href="#recommendation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Recommendation&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Convolutional Neural Networks for Web-Scale Recommender Systems.&lt;/strong&gt;
&lt;em&gt;Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec.&lt;/em&gt; KDD 2018. &lt;a href="https://arxiv.org/pdf/1806.01973.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Is a Single Vector Enough? Exploring Node Polysemy for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Ninghao Liu,Qiaoyu Tan,Yuening Li,Hongxia Yang,Jingren Zhou,Xia Hu.&lt;/em&gt; KDD 2019. &lt;a href="https://arxiv.org/pdf/1905.10668.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender System.&lt;/strong&gt;
&lt;em&gt;Qitian Wu,Hengrui Zhang,Xiaofeng Gao,Peng He,Paul Weng,Han Gao,Guihai Chen.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1903.10433.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-other-applications" class="anchor" aria-hidden="true" href="#other-applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Other Applications&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cash-out User Detection based on Attributed Heterogeneous Information Network with a Hierarchical Attention Mechanism. (Finance)&lt;/strong&gt;
&lt;em&gt;Binbin Hu, Zhiqiang Zhang, Chuan Shi, Jun Zhou, Xiaolong Li, Yuan Qi.&lt;/em&gt; AAAI 2019. &lt;a href="http://shichuan.org/doc/64.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Building Causal Graphs from Medical Literature and Electronic Medical Records. (Medicine)&lt;/strong&gt;
&lt;em&gt;Galia Nordon, Gideon Koren, Varda Shalev, Benny Kimelfeld, Uri Shalit, Kira Radinsky.&lt;/em&gt; AAAI 2019. &lt;a href="http://www.kiraradinsky.com/files/aaai-building-causal.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Attacks on Node Embeddings via Graph Poisoning. (Adversarial)&lt;/strong&gt;
&lt;em&gt;Aleksandar Bojchevski, Stephan Günnemann.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/bojchevski19a/bojchevski19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Compositional Fairness Constraints for Graph Embeddings. (Adversarial)&lt;/strong&gt;
&lt;em&gt;Avishek Bose, William Hamilton.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/bose19a/bose19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gromov-Wasserstein Learning for Graph Matching and Node Embedding. (Graph Matching)&lt;/strong&gt;
&lt;em&gt;Hongteng Xu, Dixin Luo, Hongyuan Zha, Lawrence Carin Duke.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/xu19b/xu19b.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Matching Networks for Learning the Similarity of Graph Structured Objects. (Graph Matching)&lt;/strong&gt;
&lt;em&gt;Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, Pushmeet Kohli.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/li19d/li19d.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MolGAN: An implicit generative model for small molecular graphs. (Molecular Generation)&lt;/strong&gt;
&lt;em&gt;Nicola De Cao, Thomas Kipf.&lt;/em&gt; ICML Workshop 2018. &lt;a href="https://arxiv.org/pdf/1805.11973.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational recurrent neural networks. (Relational Reasoning)&lt;/strong&gt;
&lt;em&gt;Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap.&lt;/em&gt; NeurIPS 2018. &lt;a href="https://arxiv.org/pdf/1806.01822.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Constructing Narrative Event Evolutionary Graph for Script Event Prediction. (Script Event Prediction)&lt;/strong&gt;
&lt;em&gt;Zhongyang Li, Xiao Ding, Ting Liu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://arxiv.org/abs/1805.05081" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/eecrazy/ConstructingNEEG_IJCAI_2018"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Network-embedding Based Method for Author Disambiguation. (Author Disambiguation)&lt;/strong&gt;
&lt;em&gt;Jun Xu, Siqi Shen, Dongsheng Li, Yongquan Fu.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3269272" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Graph Embedding for Ranking Optimization in E-commerce.(E-commerce)&lt;/strong&gt;
&lt;em&gt;Chen Chu, Zhao Li, Beibei Xin, Fengchao Peng, Chuanren Liu, Remo Rohs, Qiong Luo, Jingren Zhou.&lt;/em&gt; CIKM 2018. &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6330176/" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Network-to-Network Model for Content-rich Network Embedding.&lt;/strong&gt;
&lt;em&gt;Zhicheng He,Jie Liu,Na Li,Yalou Huang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330924/p1037-he.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unifying Inter-region Autocorrelation and Intra-region Structures for Spatial Embedding via Collective Adversarial Learning.&lt;/strong&gt;
&lt;em&gt;Yunchao Zhang,Pengyang Wang,Xiaolin Li,Yu Zheng,Yanjie Fu.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330972/p1700-zhang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Neural IR Meets Graph Embedding: A Ranking Model for Product Search.&lt;/strong&gt;
&lt;em&gt;Yuan Zhang,Dong Wang,Yan Zhang.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1901.08286.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-Network Embedding for Multi-Network Alignment.&lt;/strong&gt;
&lt;em&gt;Xiaokai Chu,Xinxin Fan,Di Yao,Zhihua Zhu,Jianhui Huang,Jingping Bi.&lt;/em&gt; WWW 2019. &lt;a href="https://sci-hub.tw/10.1145/3308558.3313499#" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Name Disambiguation in Anonymized Graphs using Network Embedding. (Name Disambiguation)&lt;/strong&gt;
&lt;em&gt;Baichuan Zhang, Mohammad Al Hasan.&lt;/em&gt; CIKM 2017. &lt;a href="https://arxiv.org/pdf/1702.02287.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NetGAN: Generating Graphs via Random Walks. (Graph Generation)&lt;/strong&gt;
&lt;em&gt;Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zügner, Stephan Günnemann.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1803.00816" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Networks as Learnable Physics Engines for Inference and Control. (Physics)&lt;/strong&gt;
&lt;em&gt;Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1806.01242.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational inductive bias for physical construction in humans and machines. (Human physical reasoning)&lt;/strong&gt;
&lt;em&gt;Jessica B. Hamrick, Kelsey R. Allen, Victor Bapst, Tina Zhu, Kevin R. McKee, Joshua B. Tenenbaum, Peter W. Battaglia.&lt;/em&gt; CogSci 2018. &lt;a href="https://arxiv.org/pdf/1806.01203.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>thunlp</author><guid isPermaLink="false">https://github.com/thunlp/NRLPapers</guid><pubDate>Sun, 09 Feb 2020 00:15:00 GMT</pubDate></item></channel></rss>