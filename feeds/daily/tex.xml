<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: TeX, Today</title><link>https://github.com/trending/tex?since=daily</link><description>The top repositories on GitHub for tex, measured daily</description><pubDate>Sun, 12 Jan 2020 01:10:14 GMT</pubDate><lastBuildDate>Sun, 12 Jan 2020 01:10:14 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>abhat222/Data-Science--Cheat-Sheet #1 in TeX, Today</title><link>https://github.com/abhat222/Data-Science--Cheat-Sheet</link><description>&lt;p&gt;&lt;i&gt;Cheat Sheets&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;
Taken from &lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet"&gt;&lt;strong&gt;Link to original GitHub page&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-data-science-cheatsheets" class="anchor" aria-hidden="true" href="#data-science-cheatsheets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data Science Cheatsheets&lt;/h1&gt;
&lt;p&gt;List of Data Science Cheatsheets :&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="Artificial%20Intelligence/README.md"&gt;Artificial Intelligence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Big%20Data/README.md"&gt;Big Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Engineering/README.md"&gt;Data Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Mining/README.md"&gt;Data Mining&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Science/README.md"&gt;Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Visualization/README.md"&gt;Data Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Warehouse/README.md"&gt;Data Warehouse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Deep%20Learning/README.md"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DevOps/README.md"&gt;DevOps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Docker%20&amp;amp;%20Kubernetes/README.md"&gt;Docker &amp;amp; Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Excel/README.md"&gt;Excel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Git/README.md"&gt;Git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Images//README.md"&gt;Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Interview%20Questions/README.md"&gt;Interview Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Linux/README.md"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet#machine-learning"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Mathematics/README.md"&gt;Mathematics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Matlab/README.md"&gt;Matlab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="NLP/README.md"&gt;NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Numpy/README.md"&gt;Numpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Ordinary%20Differential%20Equations/README.md"&gt;Ordinary Differential Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Pandas/README.md"&gt;Pandas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Probability/README.md"&gt;Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Python/README.md"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Scala/README.md"&gt;Scala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="SQL/README.md"&gt;SQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Statistics/README.md"&gt;Statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-python" class="anchor" aria-hidden="true" href="#python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Python/Python_cheatsheet.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/python_cheatsheet.PNG?" alt="Illustration" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Python/spaCy.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/SpaCy.PNG?" alt="Illustration" width="415px" style="max-width:100%;"&gt;&lt;/a&gt;                                   &lt;b&gt;Python Cheat Sheet&lt;/b&gt;                                                                  &lt;b&gt;Spacy Cheat Sheet&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Pandas/pandas_cheat_sheet.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/DA_Pandas.PNG?" alt="Illustration" height="320px" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Pandas/Reading%20and%20Writing%20data%20with%20PANDAS.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/RW_Panda.PNG?" alt="Illustration" width="415px" height="320px" style="max-width:100%;"&gt;&lt;/a&gt;                                   &lt;b&gt;Data Analysis with Pandas&lt;b&gt;                                              &lt;b&gt;Pandas (Reading and Writing Data)&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Python/python-cheatsheets-ds.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/PythonForDS.PNG?" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Numpy/100_numpy_exercises.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/101_numpy_exercises.jpg?" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                                   &lt;b&gt;Python for Data Science&lt;b&gt;                                                                  &lt;b&gt;100 Numpy Exercises&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Python/Python%20for%20Data%20Analysis.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/pythonforDA.PNG?" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Pandas/pandas-10min.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/Pandas_10mins.PNG?" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                             &lt;b&gt;Python for Data Analysis&lt;b&gt;                                                              &lt;b&gt;10 Minutes to Pandas&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;h1&gt;&lt;a id="user-content-r-language" class="anchor" aria-hidden="true" href="#r-language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;R Language&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/R%20Cheat%20Sheet/R%20Cheat%20Sheets.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/R_cheatsheet.PNG?" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/R%20Cheat%20Sheet/R%20Programming%20Cheat%20Sheet.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/R.PNG?" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                                    &lt;b&gt;R Cheat Sheet&lt;b&gt;                                                                     &lt;b&gt;R (Basics &amp;amp; Advanced)&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;h1&gt;&lt;a id="user-content-machine-learning" class="anchor" aria-hidden="true" href="#machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/blob/master/Machine%20Learning/Applied%20Machine%20Learning%20Problem%20Solving%20Framework.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/abhat222/Data-Science--Cheat-Sheet/master/Machine%20Learning/Applied%20Machine%20Learning%20Problem%20Solving%20Framework.jpg" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;       &lt;/a&gt;&lt;a href="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/blob/master/Machine%20Learning/Cheat%20Sheet%20Algorithms%20for%20Supervised%20and%20Unsupervised%20Learning.pdf"&gt;&lt;img src="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/raw/master/Images/supervised-and-unsupervised-learning.PNG?" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                                    &lt;b&gt;R Cheat Sheet&lt;b&gt;                                                                     &lt;b&gt;Machine Learning&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;h1&gt;&lt;a id="user-content-deep-learning" class="anchor" aria-hidden="true" href="#deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/blob/master/Deep%20Learning/Coursera%20Deep%20Learning%20course%20Notes.pdf"&gt;&lt;img src="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/raw/master/Images/coursera-deep-learning.PNG" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;       &lt;/a&gt;&lt;a href="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/blob/master/Deep%20Learning/super-cheatsheet-deep-learning.pdf"&gt;&lt;img src="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/raw/master/Images/super-cheatsheet-deep-learning.PNG" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                                    &lt;b&gt;R Cheat Sheet&lt;b&gt;                                                                     &lt;b&gt;Machine Learning&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/article&gt;&lt;/div&gt;</description><author>abhat222</author><guid isPermaLink="false">https://github.com/abhat222/Data-Science--Cheat-Sheet</guid><pubDate>Sun, 12 Jan 2020 00:01:00 GMT</pubDate></item><item><title>pingcap/docs-cn #2 in TeX, Today</title><link>https://github.com/pingcap/docs-cn</link><description>&lt;p&gt;&lt;i&gt;TiDB/TiKV/PD documents in Chinese.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;table data-table-type="yaml-metadata"&gt;
  &lt;thead&gt;
  &lt;tr&gt;
  &lt;th&gt;draft&lt;/th&gt;
  &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
  &lt;tr&gt;
  &lt;td&gt;&lt;div&gt;true&lt;/div&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1&gt;&lt;a id="user-content-tidb-简介" class="anchor" aria-hidden="true" href="#tidb-简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TiDB 简介&lt;/h1&gt;
&lt;p&gt;TiDB 是 PingCAP 公司设计的开源分布式 HTAP (Hybrid Transactional and Analytical Processing) 数据库，结合了传统的 RDBMS 和 NoSQL 的最佳特性。TiDB 兼容 MySQL，支持无限的水平扩展，具备强一致性和高可用性。TiDB 的目标是为 OLTP (Online Transactional Processing) 和 OLAP (Online Analytical Processing) 场景提供一站式的解决方案。&lt;/p&gt;
&lt;p&gt;TiDB 具备如下特性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;高度兼容 MySQL&lt;/p&gt;
&lt;p&gt;&lt;a href="/v3.0/reference/mysql-compatibility.md"&gt;大多数情况下&lt;/a&gt;，无需修改代码即可从 MySQL 轻松迁移至 TiDB，分库分表后的 MySQL 集群亦可通过 TiDB 工具进行实时迁移。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;水平弹性扩展&lt;/p&gt;
&lt;p&gt;通过简单地增加新节点即可实现 TiDB 的水平扩展，按需扩展吞吐或存储，轻松应对高并发、海量数据场景。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分布式事务&lt;/p&gt;
&lt;p&gt;TiDB 100% 支持标准的 ACID 事务。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;真正金融级高可用&lt;/p&gt;
&lt;p&gt;相比于传统主从 (M-S) 复制方案，基于 Raft 的多数派选举协议可以提供金融级的 100% 数据强一致性保证，且在不丢失大多数副本的前提下，可以实现故障的自动恢复 (auto-failover)，无需人工介入。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一站式 HTAP 解决方案&lt;/p&gt;
&lt;p&gt;TiDB 作为典型的 OLTP 行存数据库，同时兼具强大的 OLAP 性能，配合 TiSpark，可提供一站式 HTAP 解决方案，一份存储同时处理 OLTP &amp;amp; OLAP，无需传统繁琐的 ETL 过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;云原生 SQL 数据库&lt;/p&gt;
&lt;p&gt;TiDB 是为云而设计的数据库，支持公有云、私有云和混合云，使部署、配置和维护变得十分简单。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TiDB 的设计目标是 100% 的 OLTP 场景和 80% 的 OLAP 场景，更复杂的 OLAP 分析可以通过 &lt;a href="/v3.0/reference/tispark.md"&gt;TiSpark 项目&lt;/a&gt;来完成。&lt;/p&gt;
&lt;p&gt;TiDB 对业务没有任何侵入性，能优雅的替换传统的数据库中间件、数据库分库分表等 Sharding 方案。同时它也让开发运维人员不用关注数据库 Scale 的细节问题，专注于业务开发，极大的提升研发的生产力。&lt;/p&gt;
&lt;p&gt;三篇文章了解 TiDB 技术内幕：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-1/" rel="nofollow"&gt;说存储&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-2/" rel="nofollow"&gt;说计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-3/" rel="nofollow"&gt;谈调度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pingcap</author><guid isPermaLink="false">https://github.com/pingcap/docs-cn</guid><pubDate>Sun, 12 Jan 2020 00:02:00 GMT</pubDate></item><item><title>jikexueyuanwiki/tensorflow-zh #3 in TeX, Today</title><link>https://github.com/jikexueyuanwiki/tensorflow-zh</link><description>&lt;p&gt;&lt;i&gt;谷歌全新开源人工智能系统TensorFlow官方文档中文版&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-官方文档中文版" class="anchor" aria-hidden="true" href="#tensorflow-官方文档中文版"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 官方文档中文版&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="SOURCE/images/TensorFlow.jpg"&gt;&lt;img src="SOURCE/images/TensorFlow.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-你正在阅读的项目可能会比-android-系统更加深远地影响着世界" class="anchor" aria-hidden="true" href="#你正在阅读的项目可能会比-android-系统更加深远地影响着世界"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;你正在阅读的项目可能会比 Android 系统更加深远地影响着世界！&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-缘起" class="anchor" aria-hidden="true" href="#缘起"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;缘起&lt;/h2&gt;
&lt;p&gt;2015年11月9日，Google发布人工智能系统TensorFlow并宣布开源，同日，极客学院组织在线TensorFlow中文文档翻译。&lt;/p&gt;
&lt;p&gt;机器学习作为人工智能的一种类型，可以让软件根据大量的数据来对未来的情况进行阐述或预判。如今，领先的科技巨头无不在机器学习下予以极大投入。Facebook、苹果、微软，甚至国内的百度。Google 自然也在其中。「TensorFlow」是 Google 多年以来内部的机器学习系统。如今，Google 正在将此系统成为开源系统，并将此系统的参数公布给业界工程师、学者和拥有大量编程能力的技术人员，这意味着什么呢？&lt;/p&gt;
&lt;p&gt;打个不太恰当的比喻，如今 Google 对待 TensorFlow 系统，有点类似于该公司对待旗下移动操作系统 Android。如果更多的数据科学家开始使用 Google 的系统来从事机器学习方面的研究，那么这将有利于 Google 对日益发展的机器学习行业拥有更多的主导权。&lt;/p&gt;
&lt;p&gt;为了让国内的技术人员在最短的时间内迅速掌握这一世界领先的 AI 系统，极客学院 Wiki 团队发起对 TensorFlow 官方文档的中文协同翻译，一周之内，全部翻译认领完成，一个月后，全部30章节翻译校对完成，上线极客学院Wiki平台并提供下载。&lt;/p&gt;
&lt;p&gt;Google TensorFlow项目负责人Jeff Dean为该中文翻译项目回信称："&lt;em&gt;看到能够将TensorFlow翻译成中文我非常激动，我们将TensorFlow开源的主要原因之一是为了让全世界的人们能够从机器学习与人工智能中获益，类似这样的协作翻译能够让更多的人更容易地接触到TensorFlow项目，很期待接下来该项目在全球范围内的应用!&lt;/em&gt;"&lt;/p&gt;
&lt;p&gt;Jeff回信原文：&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="SOURCE/images/jeff.png"&gt;&lt;img src="SOURCE/images/jeff.png" alt="jeff" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;再次衷心感谢每一位为该翻译项目做出贡献的同学，我们会持续关注TensorFlow、AI领域以及其它最新技术的发展、持续维护该协作翻译、持续提供更多更优质的内容，为广大IT学习者们服务！&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-内容来源" class="anchor" aria-hidden="true" href="#内容来源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;内容来源&lt;/h2&gt;
&lt;p&gt;英文官方网站：&lt;br&gt;
&lt;a href="http://tensorflow.org/" rel="nofollow"&gt;http://tensorflow.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官方GitHub仓库：&lt;br&gt;
&lt;a href="https://github.com/tensorflow/tensorflow"&gt;https://github.com/tensorflow/tensorflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;中文版 GitHub 仓库：&lt;br&gt;
&lt;a href="https://github.com/jikexueyuanwiki/tensorflow-zh"&gt;https://github.com/jikexueyuanwiki/tensorflow-zh&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-参与者按认领章节排序" class="anchor" aria-hidden="true" href="#参与者按认领章节排序"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;参与者（按认领章节排序）&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-翻译" class="anchor" aria-hidden="true" href="#翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;翻译&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/PFZheng"&gt;@PFZheng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/linbojin"&gt;@Tony Jin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chenweican"&gt;@chenweican&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bingjin"&gt;@bingjin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/oskycar"&gt;@oskycar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/btpeter"&gt;@btpeter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Warln"&gt;@Warln&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ericxk"&gt;@ericxk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangaicc"&gt;@wangaicc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/TerenceCooper"&gt;@Terence Cooper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhyhooo"&gt;@zhyhooo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/thylaco1eo"&gt;@thylaco1eo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/volvet"&gt;@volvet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhangkom"&gt;@zhangkom&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/derekshang"&gt;@derekshang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lianghyv"&gt;@lianghyv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nb312"&gt;@nb312&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Jim-Zenn"&gt;@Jim-Zenn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/andyiac"&gt;@andyiac&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/TerenceCooper"&gt;@Terence Cooper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/leege100"&gt;@leege100&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-校对" class="anchor" aria-hidden="true" href="#校对"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;校对&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/sstruct"&gt;@yangtze&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ericxk"&gt;@ericxk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WangHong-yang"&gt;@HongyangWang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/LichAmnesia"&gt;@LichAmnesia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhyhooo"&gt;@zhyhooo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/waiwaizheng"&gt;@waiwaizheng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WangHong-yang"&gt;@HongyangWang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorfly"&gt;@tensorfly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lonlonago"&gt;@lonlonago&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jishaoming"&gt;@jishaoming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lucky521"&gt;@lucky521&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/allensummer"&gt;@allensummer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/volvet"&gt;@volvet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ZHNathanielLee"&gt;@ZHNathanielLee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PengFoo"&gt;@pengfoo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/qiaohaijun"&gt;@qiaohaijun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SeikaScarlet"&gt;@Seika&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-进度记录" class="anchor" aria-hidden="true" href="#进度记录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;进度记录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2015-11-10, 谷歌发布全新人工智能系统TensorFlow并宣布开源, 极客学院Wiki启动协同翻译，创建 GitHub 仓库，制定协同规范&lt;/li&gt;
&lt;li&gt;2015-11-18, 所有章节认领完毕，翻译完成18章，校对认领7章，Star数361，fork数100，协同翻译QQ群及技术交流群的TF爱好者将近300人，GitHub搜索TensorFlow排名第二&lt;/li&gt;
&lt;li&gt;2015-12-10, Star数超过500&lt;/li&gt;
&lt;li&gt;2015-12-15, 项目正式上线&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-花絮" class="anchor" aria-hidden="true" href="#花絮"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;花絮&lt;/h2&gt;
&lt;p&gt;在组织翻译的过程中，有些事情令人印象深刻，记录下来，希望以后来学习文档的同学能够明了到手中这份文档的由来：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参加翻译的有学生，也有老师；有专门研究AI/ML的，也有对此感兴趣的；有国内的，也有远在纽约的；有工程技术人员也有博士、专家&lt;/li&gt;
&lt;li&gt;其中一位，&lt;a href="http://www.longmotto.com" rel="nofollow"&gt;恩泽&lt;/a&gt;同学，为了翻译一篇文档，在前一天没有睡觉的情况下坚持翻完，20个小时没有合眼&lt;/li&gt;
&lt;li&gt;还有一位老师，刚从讲台上讲完课，就立即给我们的翻译提修改意见&lt;/li&gt;
&lt;li&gt;很多同学自发的将搭建环境中遇到的问题总结到FAQ里帮助他人&lt;/li&gt;
&lt;li&gt;为了一个翻译细节，经常是来回几次，和其他人讨论完善&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-持续改进" class="anchor" aria-hidden="true" href="#持续改进"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;持续改进&lt;/h2&gt;
&lt;p&gt;这样的一个高技术领域的文档，我们在翻译的过程中，难免会有不完善的地方，希望请大家一起帮助我们持续改进文档的翻译质量，帮助更多的人，方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在GitHub上提Issue或Pull Request，地址为: &lt;a href="https://github.com/jikexueyuanwiki/tensorflow-zh"&gt;https://github.com/jikexueyuanwiki/tensorflow-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;加入TensorFlow技术交流群，与TensorFlower们一起研究交流技术干货--TensorFlow技术交流群：782484288&lt;/li&gt;
&lt;li&gt;对翻译感兴趣？加入协同翻译群：248320884，与翻译大神一道研究TensorFlow的本地化&lt;/li&gt;
&lt;li&gt;给我们写邮件： &lt;a href="mailto:wiki@jikexueyuan.com"&gt;wiki@jikexueyuan.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-感谢支持" class="anchor" aria-hidden="true" href="#感谢支持"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;感谢支持&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://wiki.jikexueyuan.com" rel="nofollow"&gt;极客学院 Wiki&lt;/a&gt; 提供图文教程托管服务&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-离线版本" class="anchor" aria-hidden="true" href="#离线版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;离线版本&lt;/h2&gt;
&lt;p&gt;目前，离线版本(PDF、ePub)可正常下载、使用&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tex-pdf-修订版" class="anchor" aria-hidden="true" href="#tex-pdf-修订版"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tex-PDF 修订版&lt;/h2&gt;
&lt;p&gt;&lt;a href="tex_pdf"&gt;Tex-PDF 修订版&lt;/a&gt; 目前正在编订中，欢迎加入进来一起修订。您可以在此查看&lt;a href="tex_pdf/tensorflow_manual_cn.pdf"&gt;预览版&lt;/a&gt;目前最新状态。&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jikexueyuanwiki</author><guid isPermaLink="false">https://github.com/jikexueyuanwiki/tensorflow-zh</guid><pubDate>Sun, 12 Jan 2020 00:03:00 GMT</pubDate></item><item><title>jzarnett/ece459 #4 in TeX, Today</title><link>https://github.com/jzarnett/ece459</link><description>&lt;p&gt;&lt;i&gt;ECE 459: Programming for Performance&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ece459" class="anchor" aria-hidden="true" href="#ece459"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ece459&lt;/h1&gt;
&lt;p&gt;ECE 459: Programming for Performance&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jzarnett</author><guid isPermaLink="false">https://github.com/jzarnett/ece459</guid><pubDate>Sun, 12 Jan 2020 00:04:00 GMT</pubDate></item><item><title>rstudio/cheatsheets #5 in TeX, Today</title><link>https://github.com/rstudio/cheatsheets</link><description>&lt;p&gt;&lt;i&gt;RStudio Cheat Sheets&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-rstudio-cheat-sheets" class="anchor" aria-hidden="true" href="#rstudio-cheat-sheets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RStudio Cheat Sheets&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="pngs/rstudio-ide.png"&gt;&lt;img src="pngs/rstudio-ide.png" width="364" height="288" align="right" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The cheat sheets make it easy to learn about and use some of our favorite packages. They are published in their respective PDF versions here: &lt;a href="https://www.rstudio.com/resources/cheatsheets/" rel="nofollow"&gt;https://www.rstudio.com/resources/cheatsheets/&lt;/a&gt;, some are also available in the RStudio IDE under Help-Cheatsheets.&lt;/p&gt;
&lt;p&gt;This repository contains the source files of the current, archived and translated versions.&lt;/p&gt;
&lt;p&gt;The cheat sheets use the creative commons copyright. Please see the LICENSE document for more details.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h2&gt;
&lt;p&gt;If you wish to contribute to this effort by translating a cheat sheet, please feel free to use the source Keynote file. To submit a translation, please use a Pull Request via GitHub or email it to us at &lt;a href="mailto:info@rstudio.com"&gt;info@rstudio.com&lt;/a&gt; with the subject "Translated Cheatsheet".&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tips-for-making-a-new-rstudio-cheat-sheet" class="anchor" aria-hidden="true" href="#tips-for-making-a-new-rstudio-cheat-sheet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tips for making a new RStudio cheat sheet&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;RStudio cheat sheets are not meant to be text or documentation!&lt;/strong&gt; They are scannable visual aids that use layout and visual mnemonics to help people zoom to the functions they need. Think of cheat sheets as a quick reference, with the emphasis on quick. Here's an analogy:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A cheat sheet is more like a well-organized computer menu bar that leads you to a command than like a manual that documents each command.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Everything about your cheat sheet should be designed to lead users to essential information &lt;em&gt;quickly&lt;/em&gt;.  If you are summarizing the documentation manual, you are doing it wrong! Here are some tips to help you do it right:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;RStudio cheat sheets are hosted at &lt;a href="https://github.com/rstudio/cheatsheets"&gt;https://github.com/rstudio/cheatsheets&lt;/a&gt;. You can submit new cheat sheets to the repository with a pull request.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The files &lt;a href="https://github.com/rstudio/cheatsheets/blob/master/keynotes/0-template.key"&gt;keynotes/0-template.key&lt;/a&gt; and &lt;a href="https://github.com/rstudio/cheatsheets/blob/master/powerpoints/0-template.pptx"&gt;powerpoints/0-template.ppt&lt;/a&gt; are official templates that contain some helpful tips.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You may find it easiest to create a new cheat sheet by duplicating the most recent Keynote / Powerpoint cheat sheet and then heavily editing it—that's what I do!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-process" class="anchor" aria-hidden="true" href="#process"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Process&lt;/h3&gt;
&lt;p&gt;Budget more time than you expect to make the sheets. So far, I've found this process to be the least time consuming:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Identify which functions to include&lt;/strong&gt; by reading the package web page and vignettes. I try to limit my cheat sheets to the essentials.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Organize the functions&lt;/strong&gt; into meaningful, self-explanatory groups. Each group should address a common problem or task.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Think about how to visualize the purpose of each function.&lt;/strong&gt; Visual mnemonics are easier to scan than text, which all looks the same.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Think about&lt;/strong&gt; what &lt;strong&gt;key mental models&lt;/strong&gt;, definitions, or explanations the cheat sheet should contain in addition to the functions. Ideally, use these to explain the visualizations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sketch out several possible layouts&lt;/strong&gt; for the sheet. Take care to put the more basic and/or pre-requisite content above and to the left of other content. Try to keep related content on the same side of the page. often your final layout will itself be a "mental map" for the topic of the cheat sheet.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Type out all of the explanations and function descriptions&lt;/strong&gt; that you plan to include. Lay them out. Use placeholders for the visuals. Verify that everything fits. White space is very important. Use it to make the sheet scannable and to isolate content groups. Retain white space, even if it means smaller text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Make the visuals.&lt;/strong&gt; They take the longest, so I save them for last or make them as I do step 6.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tweak until happy.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-visual-design" class="anchor" aria-hidden="true" href="#visual-design"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Design&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Use the existing theme&lt;/strong&gt; that you see in the cheatsheets. It is cohesive and black and white printer friendly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Choose a highlight color&lt;/strong&gt; to use throughout your cheat sheet, and repeat this highlight color in the background of the top right corner.  Ideally you could find a color that is different enough from the other cheat sheets that you can quickly tell yours apart when flipping through a booklet of cheatsheets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Use a second color sparingly or not at all&lt;/strong&gt; to draw attention to where it is needed and to differentiate different groupings of content.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Include lots of white space.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Visually differentiate groups of content.&lt;/strong&gt; Backgrounds, boxes, side bars, and headers are helpful here. It is very useful for the user to know immediately where one group of content begins and where one ends. Our "gradation headers" fail here, so think of better solutions if possible.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Align things&lt;/strong&gt; to guides, i.e. align things across the page. It helps define the white space and makes the cheat more orderly and professional.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Make the text no smaller than ~10pt.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;If the letters are white on a colored background&lt;/strong&gt;, make the font thicker - semibold or bold.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Save bold text&lt;/strong&gt; for a simple, important statements, or to draw scanning eyes to important words, such as words that identify the topic discussed. Don't make an entire paragraph bold text.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-content" class="anchor" aria-hidden="true" href="#content"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Content&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Include a hex sticker, IDE screenshot, or other branding material&lt;/strong&gt;. The cheat sheets have a second function as marketing material.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Include a &lt;a href="https://creativecommons.org/" rel="nofollow"&gt;Creative Commons Copyright&lt;/a&gt;&lt;/strong&gt; to make the sheet easy to share. You'll find one baked into every cheat sheet and the template.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Be very concise&lt;/strong&gt; - rely on diagrams where possible.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pay attention to the details!&lt;/strong&gt; Your readers sure will... so be correct.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;If in doubt, leave it out.&lt;/strong&gt; There is a documentation manual after all.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Code comments inform, but fail&lt;/strong&gt; to draw the readers attention. It is better to use arrows, speech bubbles, etc. for important information. If it is not important information, leave it out.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Simple working examples are more helpful than documentation details.&lt;/strong&gt; They meet the user at his or her pain points, demonstrating code, and reminding users how to run it, with the least context shifting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add some concise text to &lt;strong&gt;help the user make sense of your sections and diagrams&lt;/strong&gt;. Images are best, but readers need to be able to interpret them.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-summary" class="anchor" aria-hidden="true" href="#summary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Summary&lt;/h3&gt;
&lt;p&gt;Your cheat sheet has two goals. First, to help users find essential information quickly, and secondly to prevent confusion while doing the above. Your best strategy will be to limit the amount of information you put into the cheat sheet and to lay that information out intuitively and visually. This approach will make your cheat sheet equally useful as a teaching tool, programming tool, or marketing tool.&lt;/p&gt;
&lt;p&gt;p.s. Cheat sheets fall squarely on the &lt;em&gt;human-facing side of software design&lt;/em&gt;. They focus on human attention. What does that mean? When you write documentation, your job is to fill in all of the relevant details—that's a software facing job, you need to know the software to do it. You assume that interested humans will find their way to your details on their own (and understand them when they do!). When you make a cheatsheet, your job flips. You assume that the relevant details already exist in the documentation. Your job is to help interested humans find them and understand them.  Your job is to guide the human's attention. Don't just write, design.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rstudio</author><guid isPermaLink="false">https://github.com/rstudio/cheatsheets</guid><pubDate>Sun, 12 Jan 2020 00:05:00 GMT</pubDate></item><item><title>zhanwen/MathModel #6 in TeX, Today</title><link>https://github.com/zhanwen/MathModel</link><description>&lt;p&gt;&lt;i&gt;研究生数学建模，数学建模竞赛优秀论文，数学建模算法，LaTeX论文模板，算法思维导图，参考书籍，Matlab软件教程，PPT&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-数学建模资源" class="anchor" aria-hidden="true" href="#数学建模资源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数学建模资源&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-2019-年研究生数模" class="anchor" aria-hidden="true" href="#2019-年研究生数模"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019 年研究生数模&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-20191111-又是双十一比赛结果经过一个半月的评审在这一天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo" class="anchor" aria-hidden="true" href="#20191111-又是双十一比赛结果经过一个半月的评审在这一天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019.11.11 又是双十一，比赛结果经过一个半月的评审，在这一天公布了&lt;a href="2019%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95"&gt;获奖名单&lt;/a&gt;，大家的努力相信都会有所收获。余生还有很多有意义的事情需要我们去做，让我们一起努力。(o^o)&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-20199192019923-比赛已经结束大家耐心等待获奖吧oo" class="anchor" aria-hidden="true" href="#20199192019923-比赛已经结束大家耐心等待获奖吧oo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019.9.19—2019.9.23 比赛已经结束，大家耐心等待获奖吧（(o^^o)）&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-论文提交md5使用方法" class="anchor" aria-hidden="true" href="#论文提交md5使用方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;论文提交（MD5使用方法）&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;MD5文件校验和使用说明：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/blob/master/MD5%E6%96%87%E4%BB%B6%E6%A0%A1%E9%AA%8C%E5%92%8C%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.md"&gt;&lt;strong&gt;MD5文件校验和使用说明&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-论文模版更新" class="anchor" aria-hidden="true" href="#论文模版更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;论文模版更新&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;LaTex 论文模版：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/blob/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/2019%E5%B9%B4Latex%E6%A8%A1%E7%89%88.zip"&gt;&lt;strong&gt;LaTex 论文模版&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;Word 论文模版：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/blob/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/%E2%80%9C%E5%8D%8E%E4%B8%BA%E6%9D%AF%E2%80%9D%E7%AC%AC%E5%8D%81%E5%85%AD%E5%B1%8A%E4%B8%AD%E5%9B%BD%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AE%BA%E6%96%87%E6%A0%BC%E5%BC%8F%E8%A7%84%E8%8C%83.doc"&gt;&lt;strong&gt;Word 论文模版（已更新最新）&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;LaTex 论文模版使用方式：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/tree/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/latex_note.md"&gt;&lt;strong&gt;如何编译 Latex 文件&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-下载方式仓库比较大建议单个文件下载" class="anchor" aria-hidden="true" href="#下载方式仓库比较大建议单个文件下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载方式(仓库比较大，建议单个文件下载)&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/downloaddemo2.gif"&gt;&lt;img src="./images/downloaddemo2.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt;  
&lt;p&gt;&lt;em&gt;主题：&lt;/em&gt; &lt;a href="https://cpipc.chinadegrees.cn/cw/hp/4" rel="nofollow"&gt;&lt;strong&gt;“华为杯”第十六届中国研究生数学建模竞赛&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;报名时间：&lt;/em&gt; &lt;strong&gt;2019年6月1日8:00——9月10日17:00&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;审核时间：&lt;/em&gt; &lt;strong&gt;2019年6月1日8:00——9月12日17:00&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;交费时间：&lt;/em&gt; &lt;strong&gt;2019年7月1日8:00——9月15日17:00&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;比赛时间：&lt;/em&gt; &lt;strong&gt;2019年9月19日8:00——9月23日12:00&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-官网报名地址官网地址" class="anchor" aria-hidden="true" href="#官网报名地址官网地址"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;官网报名地址：&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/4" rel="nofollow"&gt;官网地址&lt;/a&gt;&lt;/h4&gt;
&lt;hr&gt;  
&lt;h3&gt;&lt;a id="user-content-2018915-祝大家比赛开心-_" class="anchor" aria-hidden="true" href="#2018915-祝大家比赛开心-_"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018.9.15 祝大家比赛开心 （^_^）&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-2018919-比赛已经结束大家耐心等待获奖吧o" class="anchor" aria-hidden="true" href="#2018919-比赛已经结束大家耐心等待获奖吧o"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018.9.19 比赛已经结束，大家耐心等待获奖吧（^o^）&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-20181111-比赛结果经过一个半月的评审终于在昨天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo" class="anchor" aria-hidden="true" href="#20181111-比赛结果经过一个半月的评审终于在昨天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018.11.11 比赛结果经过一个半月的评审，终于在昨天公布了&lt;a href="https://github.com/zhanwen/MathModel/tree/master/2018%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/2018%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95"&gt;获奖名单&lt;/a&gt;，大家的努力相信都会有所收获。余生还有很多有意义的事情需要我们去做，让我们一起努力。(o^^o)&lt;/h4&gt;
&lt;hr&gt;  
&lt;h4&gt;&lt;a id="user-content-更新添加比赛官网地址戳这里" class="anchor" aria-hidden="true" href="#更新添加比赛官网地址戳这里"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新/添加比赛官网地址&lt;a href="https://cpipc.chinadegrees.cn/" rel="nofollow"&gt;戳这里&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/4" rel="nofollow"&gt;数学建模竞赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/6" rel="nofollow"&gt;电子设计竞赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/2c9088a5696cbf370169a3f8101510bd" rel="nofollow"&gt;人工智能创新大赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/2c9088a5696cbf370169a3f8934810be" rel="nofollow"&gt;机器人创新设计大赛&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-下载与使用由于整个项目直接下载比较慢可以看方式四" class="anchor" aria-hidden="true" href="#下载与使用由于整个项目直接下载比较慢可以看方式四"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载与使用（由于整个项目直接下载比较慢，可以看方式四）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;方式一：使用 &lt;code&gt;git&lt;/code&gt; 下载。&lt;br&gt;
&lt;code&gt;git clone https://github.com/zhanwen/MathModel.git&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方式二：直接下载压缩包。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/downloaddemo.gif"&gt;&lt;img src="./images/downloaddemo.gif" height="250" width="500" align="center" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方式三
&lt;ul&gt;
&lt;li&gt;可以单个文件下载，选择自己需要的某篇论文，直接在对应的页面点击下载即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/download3.gif"&gt;&lt;img src="./images/download3.gif" height="250" width="500" align="center" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方式四：百度云下载（推荐）
&lt;ul&gt;
&lt;li&gt;使用百度云下载，正常的客户端会出现限速，导致下载的很慢，这里给大家推荐一个绕过百度云下载限速的方式。具体怎么下载，请参照 &lt;a href="https://github.com/iikira/BaiduPCS-Go"&gt;绕过限速&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;该项目的百度云链接 &lt;a href="https://pan.baidu.com/s/1UnngHxNR0EVoyBpKlPxFAw" rel="nofollow"&gt;https://pan.baidu.com/s/1UnngHxNR0EVoyBpKlPxFAw&lt;/a&gt;，密码：&lt;code&gt;ea2n&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-国赛试题" class="anchor" aria-hidden="true" href="#国赛试题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;国赛试题&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2019%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2019年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2018%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2018年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2017%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2017年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2016%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2016年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2015%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2015年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2014%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2014年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2013%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2013年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2012%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2012年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2011%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2011年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2010%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2010年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2009%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2009年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2008%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2008年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2007%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2007年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2006%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2006年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2005%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2005年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2004%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2004年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-国赛论文" class="anchor" aria-hidden="true" href="#国赛论文"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;国赛论文&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2018年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：关于跳台跳水体型系数设置的建模分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：光传送网建模与价值评估&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：对恐怖袭击事件记录数据的量化分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：基于卫星高度计海面高度异常资料获取潮汐调和常数方法及应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：多无人机对组网雷达的协同干扰&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：航站楼扩增评估&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2017年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：无人机在抢险救灾中的优化运用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：面向下一代光通信的 VCSEL 激光器仿真模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：航班恢复问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：基于监控视频的前景目标提取&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：多波次导弹发射中的规划问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：地下物流系统网络&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2016年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：多无人机协同任务规划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：具有遗传性疾病和性状的遗传位点分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：基于无线通信基站的室内三维定位问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：军事行动避空侦察的时机和路线选择&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：粮食最低收购价政策问题研究&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2015年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：水面舰艇编队防空和信息化战争评估模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：数据的多流形结构分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：移动通信中的无线信道“指纹”特征建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：面向节能的单/多列车优化决策问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：数控加工刀具运动的优化控制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：旅游路线规划问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2014年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：小鼠视觉感受区电位信号(LFP)与视觉刺激之间的关系研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：机动目标的跟踪与反跟踪&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：无线通信中的快时变信道建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：人体营养健康角度的中国果蔬发展战略研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：乘用车物流运输计划问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2013年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：变循环发动机部件法建模及优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：功率放大器非线性特性及预失真模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：微蜂窝环境中无线接收信号的特性分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：空气中PM2.5问题的研究 attachment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：中等收入定位与人口度量模型研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：可持续的中国城乡居民养老保险体系的数学模型研究&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2012年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：基因识别问题及其算法实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：基于卫星无源探测的空间飞行器主动段轨道估计与误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：有杆抽油系统的数学建模及诊断&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：基于卫星云图的风失场(云导风)度量模型与算法探讨&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2011年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：基于光的波粒二象性一种猜想的数学仿真&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：吸波材料与微波暗室问题的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：小麦发育后期茎杆抗倒性的数学模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：房地产行业的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2010年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：确定肿瘤的重要基因信息&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：与封堵渍口有关的重物落水后运动过程的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：神经元的形态分类和识别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：特殊工件磨削加工的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2009年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：我国就业人数或城镇登记失业率的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：枪弹头痕迹，自动比对方法的研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：多传感器数据融合与航迹预测&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：110 警车配置及巡逻方案&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2008年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：汶川地震中唐家山堪塞湖泄洪问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：城市道路交通信号实时控制问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;C题：货运列车的编组调度问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;D题：中央空调系统节能设计问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2007年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：建立食品卫生安全保障体系数学模型及改进模型的若干理论问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：械臂运动路径设计问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：探讨提高高速公路路面质量的改进方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：邮政运输网络中的邮路规划和邮车调运&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2006年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：Ad Hoc 网络中的区域划分和资源分配问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：确定高精度参数问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：维修线性流量阀时的内筒设计问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：学生面试问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2005年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：Highway Traveling time Estimate and Optimal Routing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：空中加油&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：城市交通管理中的出租车规划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：仓库容量有限条件下的随机存贮管理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2004年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：发现黄球并定位&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：使用下料问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：售后服务数据的运用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：研究生录取问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-美赛论文" class="anchor" aria-hidden="true" href="#美赛论文"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;美赛论文&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2017年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2016年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2015年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2014年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2013年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2012年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2011年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2010年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2009年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2008年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2007年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2006年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2005年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2004年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-数学建模算法" class="anchor" aria-hidden="true" href="#数学建模算法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数学建模算法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AE%97%E6%B3%95"&gt;经典算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95"&gt;现代算法&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BB%BF%E7%9C%9F"&gt;计算机仿真&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95"&gt;粒子群算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE"&gt;马尔可夫链&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B3%95"&gt;蒙特卡洛法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E6%B3%95"&gt;模拟退火法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"&gt;神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E5%B0%8F%E6%B3%A2%E5%88%86%E6%9E%90"&gt;小波分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95"&gt;遗传算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-教材及课件" class="anchor" aria-hidden="true" href="#教材及课件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;教材及课件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E6%95%99%E6%9D%90%E5%8F%8A%E8%AF%BE%E4%BB%B6/%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6"&gt;国防科技术大学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E6%95%99%E6%9D%90%E5%8F%8A%E8%AF%BE%E4%BB%B6/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E8%AF%BE%E4%BB%B6/PPT%E8%AF%BE%E4%BB%B6"&gt;浙江大学课件&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-数学建模算法思维导图" class="anchor" aria-hidden="true" href="#数学建模算法思维导图"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数学建模算法思维导图&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/Mind"&gt;思维导图&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-matlab-入门教程" class="anchor" aria-hidden="true" href="#matlab-入门教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Matlab 入门教程&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/Matlab%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B"&gt;Matlab入门和在线性代数中的应用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt; 
&lt;h3&gt;&lt;a id="user-content-声明" class="anchor" aria-hidden="true" href="#声明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;声明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;其中有些内容整理自互联网，如有侵权，请联系，我将及时处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-个人微信公众号" class="anchor" aria-hidden="true" href="#个人微信公众号"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;个人微信公众号&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dotzhang&lt;/code&gt;：一名不羁的学僧，我的世界不只有学术。一条迷途的咸鱼，正在游向属于它的天地！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/images/donate/common.jpg"&gt;&lt;img src="/images/donate/common.jpg" width="150" height="150" alt="weixin" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-赞助和支持" class="anchor" aria-hidden="true" href="#赞助和支持"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;赞助和支持&lt;/h3&gt;
&lt;p&gt;这些内容都是我花了不少时间整理出来的, 如果你觉得它对你很有帮助, 请你也分享给需要学习的朋友们。如果你看好我的内容分享, 也可以考虑适当的赞助打赏, 让我有更多的动力去继续分享更好的内容给大家。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;微信&lt;/th&gt;
&lt;th&gt;支付宝&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/donate/weixinpay.jpg"&gt;&lt;img src="images/donate/weixinpay.jpg" width="150" height="150" alt="pay check" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/donate/alipay.jpg"&gt;&lt;img src="images/donate/alipay.jpg" width="150" height="150" alt="pay check" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-联系" class="anchor" aria-hidden="true" href="#联系"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;联系&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Email：&lt;a href="https://mail.google.com/" rel="nofollow"&gt;hanwenme@gmail.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;微  信（有任何问题都可以直接怼我）：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/donate/wechat.png"&gt;&lt;img src="images/donate/wechat.png" width="150" height="150" alt="pay check" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zhanwen</author><guid isPermaLink="false">https://github.com/zhanwen/MathModel</guid><pubDate>Sun, 12 Jan 2020 00:06:00 GMT</pubDate></item><item><title>exacity/deeplearningbook-chinese #7 in TeX, Today</title><link>https://github.com/exacity/deeplearningbook-chinese</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Book Chinese Translation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-中文翻译" class="anchor" aria-hidden="true" href="#deep-learning-中文翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning 中文翻译&lt;/h1&gt;
&lt;p&gt;在众多网友的帮助和校对下，中文版终于出版了。尽管还有很多问题，但至少90%的内容是可读的，并且是准确的。
我们尽可能地保留了原书&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;Deep Learning&lt;/a&gt;中的意思并保留原书的语句。&lt;/p&gt;
&lt;p&gt;然而我们水平有限，我们无法消除众多读者的方差。我们仍需要大家的建议和帮助，一起减小翻译的偏差。&lt;/p&gt;
&lt;p&gt;大家所要做的就是阅读，然后汇总你的建议，提issue（最好不要一个一个地提）。如果你确定你的建议不需要商量，可以直接发起PR。&lt;/p&gt;
&lt;p&gt;对应的翻译者：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第1、4、7、10、14、20章及第12.4、12.5节由 @swordyork 负责&lt;/li&gt;
&lt;li&gt;第2、5、8、11、15、18章由 @liber145 负责&lt;/li&gt;
&lt;li&gt;第3、6、9章由 @KevinLee1110 负责&lt;/li&gt;
&lt;li&gt;第13、16、17、19章及第12.1至12.3节由 @futianfan 负责&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-面向的读者" class="anchor" aria-hidden="true" href="#面向的读者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;面向的读者&lt;/h2&gt;
&lt;p&gt;请直接下载&lt;a href="https://github.com/exacity/deeplearningbook-chinese/releases/download/v0.5-beta/dlbook_cn_v0.5-beta.pdf"&gt;PDF&lt;/a&gt;阅读。
不打算提供EPUB等格式，如有需要请自行修改。&lt;/p&gt;
&lt;p&gt;这一版准确性已经有所提高，读者可以以中文版为主、英文版为辅来阅读学习，但我们仍建议研究者阅读&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;原版&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-出版及开源原因" class="anchor" aria-hidden="true" href="#出版及开源原因"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;出版及开源原因&lt;/h2&gt;
&lt;p&gt;本书由人民邮电出版社出版，如果你觉得中文版PDF对你有所帮助，希望你能支持下纸质正版书籍。
如果你觉得中文版不行，希望你能多提建议。非常感谢各位！
纸质版也会进一步更新，需要大家更多的建议和意见，一起完善中文版。&lt;/p&gt;
&lt;p&gt;纸质版目前在人民邮电出版社的异步社区出售，见&lt;a href="http://www.epubit.com.cn/book/details/4278" rel="nofollow"&gt;地址&lt;/a&gt;。
价格不低，但看了样本之后，我们认为物有所值。
注意，我们不会通过媒体进行宣传，希望大家先看电子版内容，再判断是否购买纸质版。&lt;/p&gt;
&lt;p&gt;以下是开源的具体原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们不是文学工作者，不专职翻译。单靠我们，无法给出今天的翻译，众多网友都给我们提出了宝贵的建议，因此开源帮了很大的忙。出版社会给我们稿费（我们也不知道多少，可能2万左右），我们也不好意思自己用，商量之后觉得捐出是最合适的，以所有贡献过的网友的名义（我们把稿费捐给了杉树公益，用于4名贵州高中生三年的生活费，见&lt;a href="https://github.com/exacity/deeplearningbook-chinese/blob/master/donation.pdf"&gt;捐赠情况&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;PDF电子版对于技术类书籍来说是很重要的，随时需要查询，拿着纸质版到处走显然不合适。国外很多技术书籍都有对应的电子版（虽然不一定是正版），而国内的几乎没有。个人认为这是出版社或者作者认为国民素质还没有高到主动为知识付费的境界，所以不愿意"泄露"电子版。时代在进步，我们也需要改变。特别是翻译作品普遍质量不高的情况下，要敢为天下先。&lt;/li&gt;
&lt;li&gt;深度学习发展太快，日新月异，所以我们希望大家更早地学到相关的知识。我觉得原作者开放PDF电子版也有类似的考虑，也就是先阅读后付费。我们认为中国人口素质已经足够高，懂得为知识付费。当然这不是付给我们的，是付给出版社的，出版社再付给原作者。我们不希望中文版的销量因PDF电子版的存在而下滑。出版社只有值回了版权才能在以后引进更多的优秀书籍。我们这个开源翻译先例也不会成为一个反面案例，以后才会有更多的PDF电子版。&lt;/li&gt;
&lt;li&gt;开源也涉及版权问题，出于版权原因，我们不再更新此初版PDF文件，请大家以最终的纸质版为准。（但源码会一直更新）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-致谢" class="anchor" aria-hidden="true" href="#致谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;致谢&lt;/h2&gt;
&lt;p&gt;我们有3个类别的校对人员。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;负责人也就是对应的翻译者。&lt;/li&gt;
&lt;li&gt;简单阅读，对语句不通顺或难以理解的地方提出修改意见。&lt;/li&gt;
&lt;li&gt;中英对比，进行中英对应阅读，排除少翻错翻的情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有校对建议都保存在各章的&lt;code&gt;annotations.txt&lt;/code&gt;文件中。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;章节&lt;/th&gt;
&lt;th&gt;负责人&lt;/th&gt;
&lt;th&gt;简单阅读&lt;/th&gt;
&lt;th&gt;中英对比&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter1_introduction/" rel="nofollow"&gt;第一章 前言&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc, @SiriusXDJ, @corenel, @NeutronT&lt;/td&gt;
&lt;td&gt;@linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter2_linear_algebra/" rel="nofollow"&gt;第二章 线性代数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@SiriusXDJ, @angrymidiao&lt;/td&gt;
&lt;td&gt;@badpoem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter3_probability_and_information_theory/" rel="nofollow"&gt;第三章 概率与信息论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@SiriusXDJ&lt;/td&gt;
&lt;td&gt;@kkpoker, @Peiyan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter4_numerical_computation/" rel="nofollow"&gt;第四章 数值计算&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;@zhangyafeikimi&lt;/td&gt;
&lt;td&gt;@hengqujushi&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter5_machine_learning_basics/" rel="nofollow"&gt;第五章 机器学习基础&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@wheaio, @huangpingchun&lt;/td&gt;
&lt;td&gt;@fairmiracle, @linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter6_deep_feedforward_networks/" rel="nofollow"&gt;第六章 深度前馈网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;David_Chow, @linzhp, @sailordiary&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter7_regularization/" rel="nofollow"&gt;第七章 深度学习中的正则化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@NBZCC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/" rel="nofollow"&gt;第八章 深度模型中的优化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@happynoom, @codeVerySlow&lt;/td&gt;
&lt;td&gt;@huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter9_convolutional_networks/" rel="nofollow"&gt;第九章 卷积网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @corenel&lt;/td&gt;
&lt;td&gt;@zhiding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter10_sequence_modeling_rnn/" rel="nofollow"&gt;第十章 序列建模：循环和递归网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @yinruiqing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter11_practical_methodology/" rel="nofollow"&gt;第十一章 实践方法论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter12_applications/" rel="nofollow"&gt;第十二章 应用&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork, @futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@corenel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter13_linear_factor_models/" rel="nofollow"&gt;第十三章 线性因子模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;@cloudygoose&lt;/td&gt;
&lt;td&gt;@ZhiweiYang&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter14_autoencoders/" rel="nofollow"&gt;第十四章 自编码器&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@Seaball, @huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter15_representation_learning/" rel="nofollow"&gt;第十五章 表示学习&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@cnscottzheng&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter16_structured_probabilistic_modelling/" rel="nofollow"&gt;第十六章 深度学习中的结构化概率模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter17_monte_carlo_methods/" rel="nofollow"&gt;第十七章 蒙特卡罗方法&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter18_confronting_the_partition_function/" rel="nofollow"&gt;第十八章 面对配分函数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@tankeco&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter19_approximate_inference/" rel="nofollow"&gt;第十九章 近似推断&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary, @hengqujushi, huanghaojun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter20_deep_generative_models/" rel="nofollow"&gt;第二十章 深度生成模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;参考文献&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@pkuwwt&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我们会在纸质版正式出版的时候，在书中致谢，正式感谢各位作出贡献的同学！&lt;/p&gt;
&lt;p&gt;还有很多同学提出了不少建议，我们都列在此处。&lt;/p&gt;
&lt;p&gt;@tttwwy @tankeco @fairmiracle @GageGao @huangpingchun @MaHongP @acgtyrant @yanhuibin315 @Buttonwood @titicacafz
@weijy026a @RuiZhang1993 @zymiboxpay @xingkongliang @oisc @tielei @yuduowu @Qingmu @HC-2016 @xiaomingabc
@bengordai @Bojian @JoyFYan @minoriwww @khty2000 @gump88 @zdx3578 @PassStory @imwebson @wlbksy @roachsinai @Elvinczp
@endymecy name:YUE-DaJiong @9578577 @linzhp @cnscottzheng @germany-zhu  @zhangyafeikimi @showgood163 @gump88
@kangqf @NeutronT @badpoem @kkpoker @Seaball @wheaio @angrymidiao @ZhiweiYang @corenel @zhaoyu611 @SiriusXDJ @dfcv24 EmisXXY
FlyingFire vsooda @friskit-china @poerin @ninesunqian @JiaqiYao @Sofring @wenlei @wizyoung @imageslr @@indam @XuLYC
@zhouqingping @freedomRen @runPenguin @pkuwwt @wuqi @tjliupeng @neo0801 @jt827859032 @demolpc @fishInAPool
@xiaolangyuxin @jzj1993 @whatbeg LongXiaJun jzd&lt;/p&gt;
&lt;p&gt;如有遗漏，请务必通知我们，可以发邮件至&lt;code&gt;echo c3dvcmQueW9ya0BnbWFpbC5jb20K | base64 --decode&lt;/code&gt;。
这是我们必须要感谢的，所以不要不好意思。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;排版&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-注意" class="anchor" aria-hidden="true" href="#注意"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;注意&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;各种问题或者建议可以提issue，建议使用中文。&lt;/li&gt;
&lt;li&gt;由于版权问题，我们不能将图片和bib上传，请见谅。&lt;/li&gt;
&lt;li&gt;Due to copyright issues, we would not upload figures and the bib file.&lt;/li&gt;
&lt;li&gt;可用于学习研究目的，不得用于任何商业行为。谢谢！&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-markdown格式" class="anchor" aria-hidden="true" href="#markdown格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Markdown格式&lt;/h2&gt;
&lt;p&gt;这种格式确实比较重要，方便查阅，也方便索引。初步转换后，生成网页，具体见&lt;a href="https://exacity.github.io/deeplearningbook-chinese" rel="nofollow"&gt;deeplearningbook-chinese&lt;/a&gt;。
注意，这种转换没有把图放进去，也不会放图。目前使用单个&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;，基于latex文件转换，以后可能会更改但原则是不直接修改&lt;a href="docs/_posts"&gt;md文件&lt;/a&gt;。
需要的同学可以自行修改&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-html格式" class="anchor" aria-hidden="true" href="#html格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTML格式&lt;/h2&gt;
&lt;p&gt;读者可以使用&lt;a href="https://github.com/coolwanglu/pdf2htmlEX"&gt;pdf2htmlEX&lt;/a&gt;进行转换，直接将PDF转换为HTML。&lt;/p&gt;
&lt;p&gt;Updating.....&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>exacity</author><guid isPermaLink="false">https://github.com/exacity/deeplearningbook-chinese</guid><pubDate>Sun, 12 Jan 2020 00:07:00 GMT</pubDate></item><item><title>jiachenli94/Awesome-Interaction-aware-Trajectory-Prediction #8 in TeX, Today</title><link>https://github.com/jiachenli94/Awesome-Interaction-aware-Trajectory-Prediction</link><description>&lt;p&gt;&lt;i&gt;A selection of state-of-the-art research materials on trajectory prediction&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-awesome-interaction-aware-behavior-and-trajectory-prediction" class="anchor" aria-hidden="true" href="#awesome-interaction-aware-behavior-and-trajectory-prediction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome Interaction-aware Behavior and Trajectory Prediction&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/7db523b8b68f5a19da519ca6f7649ced6ce5cbf0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d312e302d6666363962342e737667"&gt;&lt;img src="https://camo.githubusercontent.com/7db523b8b68f5a19da519ca6f7649ced6ce5cbf0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d312e302d6666363962342e737667" alt="Version" data-canonical-src="https://img.shields.io/badge/Version-1.0-ff69b4.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/3ce93c312a25aee59456c18fd1be9db42aa924a2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c617374557064617465642d323031392e31312d6c69676874677265792e737667"&gt;&lt;img src="https://camo.githubusercontent.com/3ce93c312a25aee59456c18fd1be9db42aa924a2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c617374557064617465642d323031392e31312d6c69676874677265792e737667" alt="LastUpdated" data-canonical-src="https://img.shields.io/badge/LastUpdated-2019.11-lightgrey.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c169f833febcd19dc5eae6f3e11fd5eea0d5e261/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f546f7069632d6265686176696f72287472616a6563746f7279292d2d70726564696374696f6e2d79656c6c6f772e7376673f6c6f676f3d676974687562"&gt;&lt;img src="https://camo.githubusercontent.com/c169f833febcd19dc5eae6f3e11fd5eea0d5e261/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f546f7069632d6265686176696f72287472616a6563746f7279292d2d70726564696374696f6e2d79656c6c6f772e7376673f6c6f676f3d676974687562" alt="Topic" data-canonical-src="https://img.shields.io/badge/Topic-behavior(trajectory)--prediction-yellow.svg?logo=github" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="http://hits.dwyl.io/jiachenli94/Interaction-aware-Trajectory-Prediction" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e774f641c9fb064f3315f18f39b9a25f2def6103/687474703a2f2f686974732e6477796c2e696f2f6a69616368656e6c6939342f496e746572616374696f6e2d61776172652d5472616a6563746f72792d50726564696374696f6e2e737667" alt="HitCount" data-canonical-src="http://hits.dwyl.io/jiachenli94/Interaction-aware-Trajectory-Prediction.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a checklist of state-of-the-art research materials (datasets, blogs, papers and public codes) related to trajectory prediction. Wish it could be helpful for both academia and industry. (Still updating)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maintainers&lt;/strong&gt;: &lt;a href="https://jiachenli94.github.io" rel="nofollow"&gt;&lt;strong&gt;Jiachen Li&lt;/strong&gt;&lt;/a&gt;, Hengbo Ma, Jinning Li (University of California, Berkeley)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Emails&lt;/strong&gt;: {jiachen_li, hengbo_ma, jinning_li}@berkeley.edu&lt;/p&gt;
&lt;p&gt;Please feel free to pull request to add new resources or send emails to us for questions, discussion and collaborations.&lt;/p&gt;
&lt;p&gt;Also welcome to check the current research in our &lt;a href="https://msc.berkeley.edu/research/autonomous-vehicle.html" rel="nofollow"&gt;&lt;strong&gt;MSC Lab&lt;/strong&gt;&lt;/a&gt; at UC Berkeley.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Research Intern&lt;/strong&gt;: Please read &lt;a href="https://jiachenli94.github.io/Research_Intern_Opportunities_at_UC_Berkeley.pdf" rel="nofollow"&gt;&lt;strong&gt;this&lt;/strong&gt;&lt;/a&gt; if you want to apply for research intern opportunities in our group.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;a href="https://github.com/jiachenli94/Awesome-Decision-Making-Reinforcement-Learning"&gt;&lt;strong&gt;Here&lt;/strong&gt;&lt;/a&gt; is also a collection of materials for reinforcement learning, decision making and motion planning.&lt;/p&gt;
&lt;p&gt;Please cite our work if you found this useful:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{Jiachen_IROS19,
  title={Conditional Generative Neural System for Probabilistic Trajectory Prediction},
  author={Li, Jiachen and Ma, Hengbo and Tomizuka, Masayoshi},
  booktitle={in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2019},
  organization={IEEE}
}

@inproceedings{Jiachen_ICRA19,
  title={Interaction-aware Multi-agent Tracking and Probabilistic Behavior Prediction via Adversarial Learning},
  author={Li, Jiachen* and Ma, Hengbo* and Tomizuka, Masayoshi},
  booktitle={in 2019 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2019},
  organization={IEEE}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#datasets"&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#vehicles-and-traffic"&gt;Vehicles and Traffic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pedestrians"&gt;Pedestrians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sport-players"&gt;Sport Players&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#literature-and-codes"&gt;&lt;strong&gt;Literature and Codes&lt;/strong&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#survey-papers"&gt;Survey Papers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#physics-systems-with-interaction"&gt;Physics Systems with Interaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#intelligent-vehicles-traffic"&gt;Intelligent Vehicles &amp;amp; Traffic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mobile-robots"&gt;Mobile Robots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pedestrians"&gt;Pedestrians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#vehicle-pedestrians-interaction"&gt;Vehicle-Pedestrians Interaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sport-players"&gt;Sport Players&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#benchmark-and-evaluation-metrics"&gt;Benchmark and Evaluation Metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#others"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-vehicles-and-traffic" class="anchor" aria-hidden="true" href="#vehicles-and-traffic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vehicles and Traffic&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Agents&lt;/th&gt;
&lt;th align="center"&gt;Scenarios&lt;/th&gt;
&lt;th align="center"&gt;Sensors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.interaction-dataset.com/" rel="nofollow"&gt;INTERACTION&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Roundabout / intersection&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.cvlibs.net/datasets/kitti/" rel="nofollow"&gt;KITTI&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Highway / rural areas&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.highd-dataset.com/" rel="nofollow"&gt;HighD&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles&lt;/td&gt;
&lt;td align="center"&gt;Highway&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm" rel="nofollow"&gt;NGSIM&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles&lt;/td&gt;
&lt;td align="center"&gt;Highway&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Tsinghua-Daimler_Cyclist_Detec/tsinghua-daimler_cyclist_detec.html" rel="nofollow"&gt;Cyclists&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Cyclists&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.nuscenes.org/" rel="nofollow"&gt;nuScenes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR / RADAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://bdd-data.berkeley.edu/" rel="nofollow"&gt;BDD100k&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists / people&lt;/td&gt;
&lt;td align="center"&gt;Highway / urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://apolloscape.auto/?source=post_page---------------------------" rel="nofollow"&gt;Apolloscapes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists / people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/udacity/self-driving-car/tree/master/datasets"&gt;Udacity&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.cityscapes-dataset.com/" rel="nofollow"&gt;Cityscapes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles/ people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://cvgl.stanford.edu/projects/uav_data/" rel="nofollow"&gt;Stanford Drone&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.argoverse.org/" rel="nofollow"&gt;Argoverse&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://gamma.umd.edu/researchdirections/autonomousdriving/trafdataset" rel="nofollow"&gt;TRAF&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles/buses/cyclists/bikes / people/animals&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://level5.lyft.com/dataset/" rel="nofollow"&gt;Lyft Level 5&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles/cyclists/people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera/ LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-pedestrians" class="anchor" aria-hidden="true" href="#pedestrians"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pedestrians&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Agents&lt;/th&gt;
&lt;th align="center"&gt;Scenarios&lt;/th&gt;
&lt;th align="center"&gt;Sensors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data" rel="nofollow"&gt;UCY&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Zara / students&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.vision.ee.ethz.ch/en/datasets/" rel="nofollow"&gt;ETH&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.viratdata.org/" rel="nofollow"&gt;VIRAT&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People / vehicles&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.cvlibs.net/datasets/kitti/" rel="nofollow"&gt;KITTI&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Highway / rural areas&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://irc.atr.jp/crest2010_HRI/ATC_dataset/" rel="nofollow"&gt;ATC&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Shopping center&lt;/td&gt;
&lt;td align="center"&gt;Range sensor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/daimler_pedestrian_benchmark_d.html" rel="nofollow"&gt;Daimler&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;From moving vehicle&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.ee.cuhk.edu.hk/~xgwang/grandcentral.html" rel="nofollow"&gt;Central Station&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Inside station&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.html#datasets" rel="nofollow"&gt;Town Center&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban street&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/" rel="nofollow"&gt;Edinburgh&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.cityscapes-dataset.com/login/" rel="nofollow"&gt;Cityscapes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles/ people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.argoverse.org/" rel="nofollow"&gt;Argoverse&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://cvgl.stanford.edu/projects/uav_data/" rel="nofollow"&gt;Stanford Drone&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://trajnet.stanford.edu/" rel="nofollow"&gt;TrajNet&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://data.nvision2.eecs.yorku.ca/PIE_dataset/" rel="nofollow"&gt;PIE&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-sport-players" class="anchor" aria-hidden="true" href="#sport-players"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sport Players&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Agents&lt;/th&gt;
&lt;th align="center"&gt;Scenarios&lt;/th&gt;
&lt;th align="center"&gt;Sensors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://datahub.io/collections/football" rel="nofollow"&gt;Football&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Football field&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-literature-and-codes" class="anchor" aria-hidden="true" href="#literature-and-codes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Literature and Codes&lt;/strong&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-survey-papers" class="anchor" aria-hidden="true" href="#survey-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Survey Papers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Human Motion Trajectory Prediction: A Survey, 2019 [&lt;a href="https://arxiv.org/abs/1905.06113" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A literature review on the prediction of pedestrian behavior in urban scenarios, ITSC 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8569415" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Survey on Vision-Based Path Prediction. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-91131-1_4" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Autonomous vehicles that interact with pedestrians: A survey of theory and practice. [&lt;a href="https://arxiv.org/abs/1805.11773" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Trajectory data mining: an overview. [&lt;a href="https://dl.acm.org/citation.cfm?id=2743025" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A survey on motion prediction and risk assessment for intelligent vehicles. [&lt;a href="https://robomechjournal.springeropen.com/articles/10.1186/s40648-014-0001-z" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-physics-systems-with-interaction" class="anchor" aria-hidden="true" href="#physics-systems-with-interaction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Physics Systems with Interaction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Ivanovic_The_Trajectron_Probabilistic_Multi-Agent_Trajectory_Modeling_With_Dynamic_Spatiotemporal_Graphs_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/StanfordASL/Trajectron"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Interaction Templates for Multi-Robot Systems, IROS 2019. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/8737744/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Factorised Neural Relational  Inference for Multi-Interaction Systems, ICML workshop 2019. [&lt;a href="https://arxiv.org/abs/1905.08721v1" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/ekwebb/fNRI"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Physics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video, 2019. [&lt;a href="https://arxiv.org/pdf/1905.11169v1.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Neural Relational Inference for Interacting Systems, ICML 2018. [&lt;a href="https://arxiv.org/abs/1802.04687v2" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/ethanfetaya/NRI"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Unsupervised Learning of Latent Physical Properties Using Perception-Prediction Networks, UAI 2018. [&lt;a href="http://arxiv.org/abs/1807.09244v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Relational inductive biases, deep learning, and graph networks, 2018. [&lt;a href="https://arxiv.org/abs/1806.01261v3" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions, ICLR 2018. [&lt;a href="http://arxiv.org/abs/1802.10353v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Graph networks as learnable physics engines for inference and control, ICML 2018. [&lt;a href="http://arxiv.org/abs/1806.01242v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Flexible Neural Representation for Physics Prediction, 2018. [&lt;a href="http://arxiv.org/abs/1806.08047v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A simple neural network module for relational reasoning, 2017. [&lt;a href="http://arxiv.org/abs/1706.01427v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;VAIN: Attentional Multi-agent Predictive Modeling, NIPS 2017. [&lt;a href="https://arxiv.org/pdf/1706.06122.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Visual Interaction Networks, 2017. [&lt;a href="http://arxiv.org/abs/1706.01433v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A Compositional Object-Based Approach to Learning Physical Dynamics, ICLR 2017. [&lt;a href="http://arxiv.org/abs/1612.00341v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Interaction Networks for Learning about Objects, Relations and Physics, 2016. [&lt;a href="https://arxiv.org/abs/1612.00222" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/higgsfield/interaction_network_pytorch"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-intelligent-vehicles--traffic" class="anchor" aria-hidden="true" href="#intelligent-vehicles--traffic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Intelligent Vehicles &amp;amp; Traffic&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Forecasting Trajectory and Behavior of Road-Agents Using Spectral Clustering in Graph-LSTMs, 2019 arXiv, &lt;a href="https://arxiv.org/pdf/1912.01118.pdf" rel="nofollow"&gt;Paper&lt;/a&gt;, &lt;a href="https://gamma.umd.edu/researchdirections/autonomousdriving/spectralcows/" rel="nofollow"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Joint Prediction for Kinematic Trajectories in Vehicle-Pedestrian-Mixed Scenes, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Bi_Joint_Prediction_for_Kinematic_Trajectories_in_Vehicle-Pedestrian-Mixed_Scenes_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Analyzing the Variety Loss in the Context of Probabilistic Trajectory Prediction, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Thiede_Analyzing_the_Variety_Loss_in_the_Context_of_Probabilistic_Trajectory_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Looking to Relations for Future Trajectory Forecast, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Looking_to_Relations_for_Future_Trajectory_Forecast_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Jointly Learnable Behavior and Trajectory Planning for Self-Driving Vehicles, IROS 2019. [&lt;a href="https://arxiv.org/abs/1910.04586" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sharing Is Caring: Socially-Compliant Autonomous Intersection Negotiation, IROS 2019. [&lt;a href="https://pdfs.semanticscholar.org/f4b2/021353bba52224eb33923b3b98956e2c9821.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;INFER: INtermediate Representations for FuturE PRediction, IROS 2019. [&lt;a href="https://arxiv.org/abs/1903.10641" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/talsperre/INFER"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Deep Predictive Autonomous Driving Using Multi-Agent Joint Trajectory Prediction and Traffic Rules, IROS 2019. [&lt;a href="http://rllab.snu.ac.kr/publications/papers/2019_iros_predstl.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;NeuroTrajectory: A Neuroevolutionary Approach to Local State Trajectory Learning for Autonomous Vehicles, IROS 2019. [&lt;a href="https://arxiv.org/abs/1906.10971" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Urban Street Trajectory Prediction with Multi-Class LSTM Networks, IROS 2019. [N/A]&lt;/li&gt;
&lt;li&gt;Spatiotemporal Learning of Directional Uncertainty in Urban Environments with Kernel Recurrent Mixture Density Networks, IROS 2019. [&lt;a href="https://ieeexplore.ieee.org/document/8772158" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Conditional generative neural system for probabilistic trajectory prediction, IROS 2019. [&lt;a href="https://arxiv.org/abs/1905.01631" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Interaction-aware multi-agent tracking and probabilistic behavior prediction via adversarial learning, ICRA 2019. [&lt;a href="https://arxiv.org/abs/1904.02390" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generic Tracking and Probabilistic Prediction Framework and Its Application in Autonomous Driving, IEEE Trans. Intell. Transport. Systems, 2019. [&lt;a href="https://www.researchgate.net/publication/334560415_Generic_Tracking_and_Probabilistic_Prediction_Framework_and_Its_Application_in_Autonomous_Driving" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Coordination and trajectory prediction for vehicle interactions via bayesian generative modeling, IV 2019. [&lt;a href="https://arxiv.org/abs/1905.00587" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Wasserstein generative learning with kinematic constraints for probabilistic interactive driving behavior prediction, IV 2019. [&lt;a href="https://ieeexplore.ieee.org/document/8813783" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;GRIP: Graph-based Interaction-aware Trajectory Prediction, ITSC 2019. [&lt;a href="https://arxiv.org/abs/1907.07792" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;AGen: Adaptable Generative Prediction Networks for Autonomous Driving, IV 2019. [&lt;a href="http://www.cs.cmu.edu/~cliu6/files/iv19-1.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;TraPHic: Trajectory Prediction in Dense and Heterogeneous Traffic Using Weighted Interactions, CVPR 2019.  [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chandra_TraPHic_Trajectory_Prediction_in_Dense_and_Heterogeneous_Traffic_Using_Weighted_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/rohanchandra30/TrackNPred"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multi-Step Prediction of Occupancy Grid Maps with Recurrent Neural Networks, CVPR 2019. [&lt;a href="https://arxiv.org/pdf/1812.09395.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Argoverse: 3D Tracking and Forecasting With Rich Maps, CVPR 2019 [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Robust Aleatoric Modeling for Future Vehicle Localization, CVPR 2019. [&lt;a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Hudnell_Robust_Aleatoric_Modeling_for_Future_Vehicle_Localization_CVPRW_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian occupancy prediction for autonomous vehicles, IRC 2019. [paper]&lt;/li&gt;
&lt;li&gt;Context-based path prediction for targets with switching dynamics, 2019.[&lt;a href="https://link.springer.com/article/10.1007/s11263-018-1104-4" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Deep Imitative Models for Flexible Inference, Planning, and Control, 2019. [&lt;a href="https://arxiv.org/abs/1810.06544" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Infer: Intermediate representations for future prediction, 2019. [&lt;a href="https://arxiv.org/abs/1903.10641" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/talsperre/INFER"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multi-agent tensor fusion for contextual trajectory prediction, 2019. [&lt;a href="https://arxiv.org/abs/1904.04776" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Context-Aware Pedestrian Motion Prediction In Urban Intersections, 2018. [&lt;a href="https://arxiv.org/abs/1806.09453" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generic probabilistic interactive situation recognition and prediction: From virtual to real, ITSC 2018. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/8569780" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generic vehicle tracking framework capable of handling occlusions based on modified mixture particle filter, IV 2018. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/8500626" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multi-Modal Trajectory Prediction of Surrounding Vehicles with Maneuver based LSTMs, 2018. [&lt;a href="https://arxiv.org/abs/1805.05499" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sequence-to-sequence prediction of vehicle trajectory via lstm encoder-decoder architecture, 2018. [&lt;a href="https://arxiv.org/abs/1802.06338" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;R2P2: A ReparameteRized Pushforward Policy for diverse, precise generative path forecasting, ECCV 2018. [&lt;a href="https://www.cs.cmu.edu/~nrhineha/R2P2.html" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting trajectories of vehicles using large-scale motion priors, IV 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8500604" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Vehicle trajectory prediction by integrating physics-and maneuver based approaches using interactive multiple models, 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8186191" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Motion Prediction of Traffic Actors for Autonomous Driving using Deep Convolutional Networks, 2018. [&lt;a href="https://arxiv.org/abs/1808.05819v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative multi-agent behavioral cloning, 2018. [&lt;a href="https://www.semanticscholar.org/paper/Generative-Multi-Agent-Behavioral-Cloning-Zhan-Zheng/ccc196ada6ec9cad1e418d7321b0cd6813d9b261" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Deep Sequence Learning with Auxiliary Information for Traffic Prediction, KDD 2018. [&lt;a href="https://arxiv.org/pdf/1806.07380.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/JingqingZ/BaiduTraffic"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multipolicy decision-making for autonomous driving via changepoint-based behavior prediction, 2017. [&lt;a href="https://link.springer.com/article/10.1007/s10514-017-9619-z" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Probabilistic long-term prediction for autonomous vehicles, IV 2017. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/7995726" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Probabilistic vehicle trajectory prediction over occupancy grid map via recurrent neural network, ITSC 2017. [&lt;a href="https://ieeexplore.ieee.org/document/6632960" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Desire: Distant future prediction in dynamic scenes with interacting agents, CVPR 2017. [&lt;a href="https://arxiv.org/abs/1704.04394" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/yadrimz/DESIRE"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Imitating driver behavior with generative adversarial networks, 2017. [&lt;a href="https://arxiv.org/abs/1701.06699" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/sisl/gail-driver"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Infogail: Interpretable imitation learning from visual demonstrations, 2017. [&lt;a href="https://arxiv.org/abs/1703.08840" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/YunzhuLi/InfoGAIL"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Long-term planning by short-term prediction, 2017. [&lt;a href="https://arxiv.org/abs/1602.01580" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Long-term path prediction in urban scenarios using circular distributions, 2017. [&lt;a href="https://www.sciencedirect.com/science/article/pii/S0262885617301853" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Deep learning driven visual path prediction from a single image, 2016. [&lt;a href="https://arxiv.org/abs/1601.07265" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Understanding interactions between traffic participants based on learned behaviors, 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7535554" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Visual path prediction in complex scenes with crowded moving objects, CVPR 2016. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/7780661/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A game-theoretic approach to replanning-aware interactive scene prediction and planning, 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7353203" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Intention-aware online pomdp planning for autonomous driving in a crowd, ICRA 2015. [&lt;a href="https://ieeexplore.ieee.org/document/7139219" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Online maneuver recognition and multimodal trajectory prediction for intersection assistance using non-parametric regression, 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6856480" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Patch to the future: Unsupervised visual prediction, CVPR 2014. [&lt;a href="http://ieeexplore.ieee.org/abstract/document/6909818/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Mobile agent trajectory prediction using bayesian nonparametric reachability trees, 2011. [&lt;a href="https://dspace.mit.edu/handle/1721.1/114899" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-mobile-robots" class="anchor" aria-hidden="true" href="#mobile-robots"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mobile Robots&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Multimodal probabilistic model-based planning for human-robot interaction, ICRA 2018. [&lt;a href="https://arxiv.org/abs/1710.09483" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/StanfordASL/TrafficWeavingCVAE"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Decentralized Non-communicating Multiagent Collision Avoidance with Deep Reinforcement Learning, ICRA 2017. [&lt;a href="https://arxiv.org/abs/1609.07845" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Augmented dictionary learning for motion prediction, ICRA 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7487407" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting future agent motions for dynamic environments, ICMLA 2016. [&lt;a href="https://www.semanticscholar.org/paper/Predicting-Future-Agent-Motions-for-Dynamic-Previtali-Bordallo/2df8179ac7b819bad556b6d185fc2030c40f98fa" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bayesian intention inference for trajectory prediction with an unknown goal destination, IROS 2015. [&lt;a href="http://ieeexplore.ieee.org/abstract/document/7354203/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning to predict trajectories of cooperatively navigating agents, ICRA 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6907442" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pedestrians-1" class="anchor" aria-hidden="true" href="#pedestrians-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pedestrians&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Disentangling Human Dynamics for Pedestrian Locomotion Forecasting with Noisy Supervision, WACV 2020. [&lt;a href="https://arxiv.org/abs/1911.01138" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;STGAT: Modeling Spatial-Temporal Interactions for Human Trajectory Prediction, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_STGAT_Modeling_Spatial-Temporal_Interactions_for_Human_Trajectory_Prediction_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/huang-xx/STGAT"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Instance-Level Future Motion Estimation in a Single Image Based on Ordinal Regression, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Instance-Level_Future_Motion_Estimation_in_a_Single_Image_Based_on_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social and Scene-Aware Trajectory Prediction in Crowded Spaces, ICCV workshop 2019. [&lt;a href="https://arxiv.org/pdf/1909.08840.pdf" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/Oghma/sns-lstm/"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Stochastic Sampling Simulation for Pedestrian Trajectory Prediction, IROS 2019. [&lt;a href="https://arxiv.org/abs/1903.01860" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Long-Term Prediction of Motion Trajectories Using Path Homology Clusters, IROS 2019. [&lt;a href="http://www.csc.kth.se/~fpokorny/static/publications/carvalho2019a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;StarNet: Pedestrian Trajectory Prediction Using Deep Neural Network in Star Topology, IROS 2019. [&lt;a href="https://arxiv.org/pdf/1906.01797.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning Generative Socially-Aware Models of Pedestrian Motion, IROS 2019. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/8760356/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Situation-Aware Pedestrian Trajectory Prediction with Spatio-Temporal Attention Model, CVWW 2019. [&lt;a href="https://arxiv.org/pdf/1902.05437.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Path predictions using object attributes and semantic environment, VISIGRAPP 2019. [&lt;a href="http://mprg.jp/data/MPRG/C_group/C20190225_minoura.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Probabilistic Path Planning using Obstacle Trajectory Prediction, CoDS-COMAD 2019. [&lt;a href="https://dl.acm.org/citation.cfm?id=3297006" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Human Trajectory Prediction using Adversarial Loss, 2019. [&lt;a href="http://www.strc.ch/2019/Kothari_Alahi.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social Ways: Learning Multi-Modal Distributions of Pedestrian Trajectories with GANs, CVPR 2019. [&lt;a href="https://sites.google.com/view/ieeecvf-cvpr2019-precognition" rel="nofollow"&gt;&lt;em&gt;Precognition Workshop&lt;/em&gt;&lt;/a&gt;], [&lt;a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Amirian_Social_Ways_Learning_Multi-Modal_Distributions_of_Pedestrian_Trajectories_With_GANs_CVPRW_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/amiryanj/socialways"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Peeking into the Future: Predicting Future Person Activities and Locations in Videos, CVPR 2019. [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liang_Peeking_Into_the_Future_Predicting_Future_Person_Activities_and_Locations_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/google/next-prediction"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning to Infer Relations for Future Trajectory Forecast, CVPR 2019. [&lt;a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Choi_Learning_to_Infer_Relations_for_Future_Trajectory_Forecast_CVPRW_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;TraPHic: Trajectory Prediction in Dense and Heterogeneous Traffic Using Weighted Interactions, CVPR 2019.  [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chandra_TraPHic_Trajectory_Prediction_in_Dense_and_Heterogeneous_Traffic_Using_Weighted_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Which Way Are You Going? Imitative Decision Learning for Path Forecasting in Dynamic Scenes, CVPR 2019.  [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Which_Way_Are_You_Going_Imitative_Decision_Learning_for_Path_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Overcoming Limitations of Mixture Density Networks: A Sampling and Fitting Framework for Multimodal Future Prediction, CVPR 2019.  [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Makansi_Overcoming_Limitations_of_Mixture_Density_Networks_A_Sampling_and_Fitting_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sophie: An attentive gan for predicting paths compliant to social and physical constraints, CVPR 2019. [&lt;a href="https://arxiv.org/abs/1806.01482" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/hindupuravinash/the-gan-zoo/blob/master/README.md"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian path, pose, and intention prediction through gaussian process dynamical models and pedestrian activity recognition, 2019. [&lt;a href="https://ieeexplore.ieee.org/document/8370119/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multimodal Interaction-aware Motion Prediction for Autonomous Street Crossing, 2019. [&lt;a href="https://arxiv.org/abs/1808.06887" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;The simpler the better: Constant velocity for pedestrian motion prediction, 2019. [&lt;a href="https://arxiv.org/abs/1903.07933" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian trajectory prediction in extremely crowded scenarios, 2019. [&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/30862018" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Srlstm: State refinement for lstm towards pedestrian trajectory prediction, 2019. [&lt;a href="https://arxiv.org/abs/1903.02793" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Location-velocity attention for pedestrian trajectory prediction, WACV 2019. [&lt;a href="https://ieeexplore.ieee.org/document/8659060" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian Trajectory Prediction in Extremely Crowded Scenarios, Sensors, 2019. [&lt;a href="https://www.mdpi.com/1424-8220/19/5/1223/pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A data-driven model for interaction-aware pedestrian motion prediction in object cluttered environments, ICRA 2018. [&lt;a href="https://arxiv.org/abs/1709.08528" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Move, Attend and Predict: An attention-based neural model for people’s movement prediction, Pattern Recognition Letters 2018. [&lt;a href="https://reader.elsevier.com/reader/sd/pii/S016786551830182X?token=1EF2B664B70D2B0C3ECDD07B6D8B664F5113AEA7533CE5F0B564EF9F4EE90D3CC228CDEB348F79FEB4E8CDCD74D4BA31" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;GD-GAN: Generative Adversarial Networks for Trajectory Prediction and Group Detection in Crowds, ACCV 2018, [&lt;a href="https://arxiv.org/pdf/1812.07667.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://www.youtube.com/watch?v=7cCIC_JIfms" rel="nofollow"&gt;demo&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ss-lstm: a hierarchical lstm model for pedestrian trajectory prediction, WACV 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8354239" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social Attention: Modeling Attention in Human Crowds, ICRA 2018. [&lt;a href="https://arxiv.org/abs/1710.04689" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/TNTant/social_lstm"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian prediction by planning using deep neural networks, ICRA 2018. [&lt;a href="https://arxiv.org/abs/1706.05904" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Joint long-term prediction of human motion using a planning-based social force approach, ICRA 2018. [&lt;a href="https://iliad-project.eu/publications/2018-2/joint-long-term-prediction-of-human-motion-using-a-planning-based-social-force-approach/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Human motion prediction under social grouping constraints, IROS 2018. [&lt;a href="http://iliad-project.eu/publications/2018-2/human-motion-prediction-under-social-grouping-constraints/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks, CVPR 2018. [&lt;a href="https://arxiv.org/abs/1803.10892" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/agrimgupta92/sgan"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Group LSTM: Group Trajectory Prediction in Crowded Scenarios, ECCV 2018. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-030-11015-4_18" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Mx-lstm: mixing tracklets and vislets to jointly forecast trajectories and head poses, CVPR 2018. [&lt;a href="https://arxiv.org/abs/1805.00652" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Intent prediction of pedestrians via motion trajectories using stacked recurrent neural networks, 2018. [&lt;a href="http://ieeexplore.ieee.org/document/8481390/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Transferable pedestrian motion prediction models at intersections, 2018. [&lt;a href="https://arxiv.org/abs/1804.00495" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Probabilistic map-based pedestrian motion prediction taking traffic participants into consideration, 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8500562" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A Computationally Efficient Model for Pedestrian Motion Prediction, ECC 2018. [&lt;a href="https://arxiv.org/abs/1803.04702" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Context-aware trajectory prediction, ICPR 2018. [&lt;a href="https://arxiv.org/abs/1705.02503" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Set-based prediction of pedestrians in urban environments considering formalized traffic rules, ITSC 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8569434" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Building prior knowledge: A markov based pedestrian prediction model using urban environmental data, ICARCV 2018. [&lt;a href="https://arxiv.org/abs/1809.06045" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Depth Information Guided Crowd Counting for Complex Crowd Scenes, 2018. [&lt;a href="https://arxiv.org/abs/1803.02256" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Tracking by Prediction: A Deep Generative Model for Mutli-Person Localisation and Tracking, WACV 2018. [&lt;a href="https://arxiv.org/abs/1803.03347" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;“Seeing is Believing”: Pedestrian Trajectory Forecasting Using Visual Frustum of Attention, WACV 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8354238" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty, CVPR 2018. [&lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bhattacharyya_Long-Term_On-Board_Prediction_CVPR_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/apratimbhattacharyya18/onboard_long_term_prediction"&gt;code+data&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Encoding Crowd Interaction with Deep Neural Network for Pedestrian Trajectory Prediction, CVPR 2018. [&lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Encoding_Crowd_Interaction_CVPR_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/ShanghaiTechCVDL/CIDNN"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Walking Ahead: The Headed Social Force Model, 2017. [&lt;a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169734" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Real-time certified probabilistic pedestrian forecasting, 2017. [&lt;a href="https://ieeexplore.ieee.org/document/7959047" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A multiple-predictor approach to human motion prediction, ICRA 2017. [&lt;a href="https://ieeexplore.ieee.org/document/7989265" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Forecasting interactive dynamics of pedestrians with fictitious play, CVPR 2017. [&lt;a href="https://arxiv.org/abs/1604.01431" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Forecast the plausible paths in crowd scenes, IJCAI 2017. [&lt;a href="https://www.ijcai.org/proceedings/2017/386" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bi-prediction: pedestrian trajectory prediction based on bidirectional lstm classification, DICTA 2017. [&lt;a href="https://ieeexplore.ieee.org/document/8227412/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Aggressive, Tense or Shy? Identifying Personality Traits from Crowd Videos, IJCAI 2017. [&lt;a href="https://www.ijcai.org/proceedings/2017/17" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Natural vision based method for predicting pedestrian behaviour in urban environments, ITSC 2017. [&lt;a href="http://ieeexplore.ieee.org/document/8317848/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Human Trajectory Prediction using Spatially aware Deep Attention Models, 2017. [&lt;a href="https://arxiv.org/pdf/1705.09436.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Soft + Hardwired Attention: An LSTM Framework for Human Trajectory Prediction and Abnormal Event Detection, 2017. [&lt;a href="https://arxiv.org/pdf/1702.05552.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Forecasting Interactive Dynamics of Pedestrians with Fictitious Play, CVPR 2017. [&lt;a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Ma_Forecasting_Interactive_Dynamics_CVPR_2017_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social LSTM: Human trajectory prediction in crowded spaces, CVPR 2016. [&lt;a href="http://openaccess.thecvf.com/content_cvpr_2016/html/Alahi_Social_LSTM_Human_CVPR_2016_paper.html" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/quancore/social-lstm"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Comparison and evaluation of pedestrian motion models for vehicle safety systems, ITSC 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7795912" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Age and Group-driven Pedestrian Behaviour: from Observations to Simulations, 2016. [&lt;a href="https://collective-dynamics.eu/index.php/cod/article/view/A3" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Structural-RNN: Deep learning on spatio-temporal graphs, CVPR 2016. [&lt;a href="https://arxiv.org/abs/1511.05298" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/asheshjain399/RNNexp"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Intent-aware long-term prediction of pedestrian motion, ICRA 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7487409" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Context-based detection of pedestrian crossing intention for autonomous driving in urban environments, IROS 2016. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/7759351/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Novel planning-based algorithms for human motion prediction, ICRA 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7487505" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning social etiquette: Human trajectory understanding in crowded scenes, ECCV 2016. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-46484-8_33" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/SajjadMzf/Pedestrian_Datasets_VIS"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;GLMP-realtime pedestrian path prediction using global and local movement patterns, ICRA 2016. [&lt;a href="http://ieeexplore.ieee.org/document/7487768/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Knowledge transfer for scene-specific motion prediction, ECCV 2016. [&lt;a href="https://arxiv.org/abs/1603.06987" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;STF-RNN: Space Time Features-based Recurrent Neural Network for predicting People Next Location, SSCI 2016. [&lt;a href="https://github.com/mhjabreel/STF-RNN"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Goal-directed pedestrian prediction, ICCV 2015. [&lt;a href="https://ieeexplore.ieee.org/document/7406377" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Trajectory analysis and prediction for improved pedestrian safety: Integrated framework and evaluations, 2015. [&lt;a href="https://ieeexplore.ieee.org/document/7225707" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting and recognizing human interactions in public spaces, 2015. [&lt;a href="https://link.springer.com/article/10.1007/s11554-014-0428-8" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning collective crowd behaviors with dynamic pedestrian-agents, 2015. [&lt;a href="https://link.springer.com/article/10.1007/s11263-014-0735-3" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Modeling spatial-temporal dynamics of human movements for predicting future trajectories, AAAI 2015. [&lt;a href="https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10126" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Unsupervised robot learning to predict person motion, ICRA 2015. [&lt;a href="https://ieeexplore.ieee.org/document/7139254" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A controlled interactive multiple model filter for combined pedestrian intention recognition and path prediction, ITSC 2015. [&lt;a href="http://ieeexplore.ieee.org/abstract/document/7313129/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Real-Time Predictive Modeling and Robust Avoidance of Pedestrians with Uncertain, Changing Intentions, 2014. [&lt;a href="https://arxiv.org/abs/1405.5581" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Behavior estimation for a complete framework for human motion prediction in crowded environments, ICRA 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6907734" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian’s trajectory forecast in public traffic with artificial neural network, ICPR 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6977417" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Will the pedestrian cross? A study on pedestrian path prediction, 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6632960" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;BRVO: Predicting pedestrian trajectories using velocity-space reasoning, 2014. [&lt;a href="https://journals.sagepub.com/doi/abs/10.1177/0278364914555543" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Context-based pedestrian path prediction, ECCV 2014. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-10599-4_40" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian path prediction using body language traits, 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6856498/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Online maneuver recognition and multimodal trajectory prediction for intersection assistance using non-parametric regression, 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6856480" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning intentions for improved human motion prediction, 2013. [&lt;a href="https://ieeexplore.ieee.org/document/6766565" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-vehicle-pedestrians-interaction" class="anchor" aria-hidden="true" href="#vehicle-pedestrians-interaction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vehicle-Pedestrians Interaction&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-sport-players-1" class="anchor" aria-hidden="true" href="#sport-players-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sport Players&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Diverse Generation for Multi-Agent Sports Games, CVPR 2019. [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yeh_Diverse_Generation_for_Multi-Agent_Sports_Games_CVPR_2019_paper.html" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Stochastic Prediction of Multi-Agent Interactions from Partial Observations, ICLR 2019. [&lt;a href="http://arxiv.org/abs/1902.09641v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generating Multi-Agent Trajectories using Programmatic Weak Supervision, ICLR 2019. [&lt;a href="http://arxiv.org/abs/1803.07612v6" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative Multi-Agent Behavioral Cloning, ICML 2018. [&lt;a href="http://www.stephanzheng.com/pdf/Zhan_Zheng_Lucey_Yue_Generative_Multi_Agent_Behavioral_Cloning.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Where Will They Go? Predicting Fine-Grained Adversarial Multi-Agent Motion using Conditional Variational Autoencoders, ECCV 2018. [&lt;a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Panna_Felsen_Where_Will_They_ECCV_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Coordinated Multi-Agent Imitation Learning, ICML 2017. [&lt;a href="http://arxiv.org/abs/1703.03121v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generating long-term trajectories using deep hierarchical networks, 2017. [&lt;a href="https://arxiv.org/abs/1706.07138" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning Fine-Grained Spatial Models for Dynamic Sports Play Prediction, ICDM 2014. [&lt;a href="https://ieeexplore.ieee.org/document/7023384/footnotes#footnotes" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative Modeling of Multimodal Multi-Human Behavior, 2018. [&lt;a href="https://arxiv.org/pdf/1803.02015.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-benchmark-and-evaluation-metrics" class="anchor" aria-hidden="true" href="#benchmark-and-evaluation-metrics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmark and Evaluation Metrics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction, ICCV 2019. [&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Towards a fatality-aware benchmark of probabilistic reaction prediction in highly interactive driving scenarios, ITSC 2018. [&lt;a href="https://arxiv.org/abs/1809.03478" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;How good is my prediction? Finding a similarity measure for trajectory prediction evaluation, ITSC 2017. [&lt;a href="http://ieeexplore.ieee.org/document/8317825/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Trajnet: Towards a benchmark for human trajectory prediction. [&lt;a href="http://trajnet.epfl.ch/" rel="nofollow"&gt;website&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-others" class="anchor" aria-hidden="true" href="#others"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cyclist trajectory prediction using bidirectional recurrent neural networks, AI 2018. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-030-03991-2_28" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Road infrastructure indicators for trajectory prediction, 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8500678" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using road topology to improve cyclist path prediction, 2017. [&lt;a href="https://ieeexplore.ieee.org/document/7995734/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Trajectory prediction of cyclists using a physical model and an artificial neural network, 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7535484/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jiachenli94</author><guid isPermaLink="false">https://github.com/jiachenli94/Awesome-Interaction-aware-Trajectory-Prediction</guid><pubDate>Sun, 12 Jan 2020 00:08:00 GMT</pubDate></item><item><title>vdumoulin/conv_arithmetic #9 in TeX, Today</title><link>https://github.com/vdumoulin/conv_arithmetic</link><description>&lt;p&gt;&lt;i&gt;A technical report on convolution arithmetic in the context of deep learning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-convolution-arithmetic" class="anchor" aria-hidden="true" href="#convolution-arithmetic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolution arithmetic&lt;/h1&gt;
&lt;p&gt;A technical report on convolution arithmetic in the context of deep learning.&lt;/p&gt;
&lt;p&gt;The code and the images of this tutorial are free to use as regulated by the
licence and subject to proper attribution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[1] Vincent Dumoulin, Francesco Visin - &lt;a href="https://arxiv.org/abs/1603.07285" rel="nofollow"&gt;A guide to convolution arithmetic
for deep learning&lt;/a&gt;
(&lt;a href="https://gist.github.com/fvisin/165ca9935392fa9600a6c94664a01214"&gt;BibTeX&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-convolution-animations" class="anchor" aria-hidden="true" href="#convolution-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/no_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/arbitrary_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/arbitrary_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/same_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/same_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/full_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/full_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, no strides&lt;/td&gt;
    &lt;td&gt;Arbitrary padding, no strides&lt;/td&gt;
    &lt;td&gt;Half padding, no strides&lt;/td&gt;
    &lt;td&gt;Full padding, no strides&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_strides.gif"&gt;&lt;img width="150px" src="gif/no_padding_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides.gif"&gt;&lt;img width="150px" src="gif/padding_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides_odd.gif"&gt;&lt;img width="150px" src="gif/padding_strides_odd.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, strides&lt;/td&gt;
    &lt;td&gt;Padding, strides&lt;/td&gt;
    &lt;td&gt;Padding, strides (odd)&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-transposed-convolution-animations" class="anchor" aria-hidden="true" href="#transposed-convolution-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transposed convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/no_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/arbitrary_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/arbitrary_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/same_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/same_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/full_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/full_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, no strides, transposed&lt;/td&gt;
    &lt;td&gt;Arbitrary padding, no strides, transposed&lt;/td&gt;
    &lt;td&gt;Half padding, no strides, transposed&lt;/td&gt;
    &lt;td&gt;Full padding, no strides, transposed&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/no_padding_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/padding_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides_odd_transposed.gif"&gt;&lt;img width="150px" src="gif/padding_strides_odd_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, strides, transposed&lt;/td&gt;
    &lt;td&gt;Padding, strides, transposed&lt;/td&gt;
    &lt;td&gt;Padding, strides, transposed (odd)&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-dilated-convolution-animations" class="anchor" aria-hidden="true" href="#dilated-convolution-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dilated convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/dilation.gif"&gt;&lt;img width="150px" src="gif/dilation.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, no stride, dilation&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-generating-the-makefile" class="anchor" aria-hidden="true" href="#generating-the-makefile"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating the Makefile&lt;/h2&gt;
&lt;p&gt;From the repository's root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ ./bin/generate_makefile&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-generating-the-animations" class="anchor" aria-hidden="true" href="#generating-the-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating the animations&lt;/h2&gt;
&lt;p&gt;From the repository's root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make all_animations&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The animations will be output to the &lt;code&gt;gif&lt;/code&gt; directory. Individual animation steps
will be output in PDF format to the &lt;code&gt;pdf&lt;/code&gt; directory and in PNG format to the
&lt;code&gt;png&lt;/code&gt; directory.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-compiling-the-document" class="anchor" aria-hidden="true" href="#compiling-the-document"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compiling the document&lt;/h2&gt;
&lt;p&gt;From the repository's root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>vdumoulin</author><guid isPermaLink="false">https://github.com/vdumoulin/conv_arithmetic</guid><pubDate>Sun, 12 Jan 2020 00:09:00 GMT</pubDate></item><item><title>openbmc/docs #10 in TeX, Today</title><link>https://github.com/openbmc/docs</link><description>&lt;p&gt;&lt;i&gt;OpenBMC Documentation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-openbmc-documentation" class="anchor" aria-hidden="true" href="#openbmc-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenBMC documentation&lt;/h1&gt;
&lt;p&gt;This repository contains documentation for OpenBMC as a whole. There may
be component-specific documentation in the repository for each component.&lt;/p&gt;
&lt;p&gt;The &lt;a href="features.md"&gt;features&lt;/a&gt; document lists the project's major features
with links to more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-openbmc-usage" class="anchor" aria-hidden="true" href="#openbmc-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenBMC Usage&lt;/h2&gt;
&lt;p&gt;These documents describe how to use OpenBMC, including using the programmatic
interfaces to an OpenBMC system.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="rest-api.md"&gt;rest-api.md&lt;/a&gt;: Introduction to using the OpenBMC REST API&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="console.md"&gt;console.md&lt;/a&gt;: Using the host console&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="host-management.md"&gt;host-management.md&lt;/a&gt;: Performing host management tasks
with OpenBMC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="code-update"&gt;code-update&lt;/a&gt;: Updating OpenBMC and host platform firmware&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-openbmc-development" class="anchor" aria-hidden="true" href="#openbmc-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenBMC Development&lt;/h2&gt;
&lt;p&gt;These documents contain details on developing OpenBMC code itself&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="cheatsheet.md"&gt;cheatsheet.md&lt;/a&gt;: Quick reference for some common
development tasks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;: Guidelines for contributing to
OpenBMC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="kernel-development.md"&gt;kernel-development.md&lt;/a&gt;: Reference for common
kernel development tasks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="REST-cheatsheet.md"&gt;REST-cheatsheet.md&lt;/a&gt;: Quick reference for some common
curl commands usage.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-openbmc-goals" class="anchor" aria-hidden="true" href="#openbmc-goals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenBMC Goals&lt;/h2&gt;
&lt;p&gt;The OpenBMC project's aim is to create a highly extensible framework for BMC
software and implement for data-center computer systems.&lt;/p&gt;
&lt;p&gt;We have a few high-level objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The OpenBMC framework must be extensible, easy to learn, and usable in a
variety of programming languages.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a REST API for external management, and allow for "pluggable"
interfaces for other types of management interactions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a remote host console, accessible over the network&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Persist network configuration settable from REST interface and host&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a robust solution for RTC management, exposed to the host.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compatible with host firmware implementations for basic IPMI communication
between host and BMC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a flexible and hierarchical inventory tracking component&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maintain a sensor database and track thresholds&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-technical-steering-committee" class="anchor" aria-hidden="true" href="#technical-steering-committee"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Technical Steering Committee&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Brad Bishop (chair), IBM&lt;/li&gt;
&lt;li&gt;Nancy Yuen, Google&lt;/li&gt;
&lt;li&gt;Sai Dasari, Facebook&lt;/li&gt;
&lt;li&gt;James Mihm, Intel&lt;/li&gt;
&lt;li&gt;Sagar Dharia, Microsoft&lt;/li&gt;
&lt;li&gt;Supreeth Venkatesh, Arm&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>openbmc</author><guid isPermaLink="false">https://github.com/openbmc/docs</guid><pubDate>Sun, 12 Jan 2020 00:10:00 GMT</pubDate></item><item><title>sb2nov/resume #11 in TeX, Today</title><link>https://github.com/sb2nov/resume</link><description>&lt;p&gt;&lt;i&gt;Software developer resume in Latex&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;A single-page, one-column resume for software developers. It uses the base latex templates and fonts to provide ease of use and installation when trying to update the resume. The different sections are clearly documented and custom commands are used to provide consistent formatting. The three main sections in the resume are education, experience, and projects.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-motivation" class="anchor" aria-hidden="true" href="#motivation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Motivation&lt;/h3&gt;
&lt;p&gt;I created this template as managing a resume on Google Docs was hard and changing any formatting was too difficult since it had to be applied in multiple places. Most currently available templates either focus on two columns, or are multiple pages long. I personally found the two-column templates hard to focus while multiple-page resumes were just too long to be used in career fairs.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/resume_preview.png"&gt;&lt;img src="/resume_preview.png" alt="Resume Screenshot" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;Format is MIT but all the data is owned by Sourabh Bajaj.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sb2nov</author><guid isPermaLink="false">https://github.com/sb2nov/resume</guid><pubDate>Sun, 12 Jan 2020 00:11:00 GMT</pubDate></item><item><title>HarisIqbal88/PlotNeuralNet #12 in TeX, Today</title><link>https://github.com/HarisIqbal88/PlotNeuralNet</link><description>&lt;p&gt;&lt;i&gt;Latex code for making neural networks diagrams&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-plotneuralnet" class="anchor" aria-hidden="true" href="#plotneuralnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PlotNeuralNet&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.5281/zenodo.2526396" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/89c8c312f40c2d237b2319aececd5740a147b11c/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e323532363339362e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.2526396.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Latex code for drawing neural networks for reports and presentation. Have a look into examples to see how they are made. Additionally, lets consolidate any improvements that you make and fix any bugs to help more people with this code.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Install the following packages on Ubuntu.
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ubuntu 16.04&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install texlive-latex-extra
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ubuntu 18.04.2
Base on this &lt;a href="https://gist.github.com/rain1024/98dd5e2c6c8c28f9ea9d"&gt;website&lt;/a&gt;, please install the following packages.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install texlive-latex-base
sudo apt-get install texlive-fonts-recommended
sudo apt-get install texlive-fonts-extra
sudo apt-get install texlive-latex-extra
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Execute the example as followed.
&lt;pre&gt;&lt;code&gt;cd pyexamples/
bash ../tikzmake.sh test_simple
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Python interface&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add easy legend functionality&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add more layer shapes like TruncatedPyramid, 2DSheet etc&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add examples for RNN and likes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-latex-usage" class="anchor" aria-hidden="true" href="#latex-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Latex usage&lt;/h2&gt;
&lt;p&gt;See &lt;a href="examples"&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory for usage.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-python-usage" class="anchor" aria-hidden="true" href="#python-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python usage&lt;/h2&gt;
&lt;p&gt;First, create a new directory and a new Python file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ mkdir my_project
$ cd my_project
vim my_arch.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the following code to your new file:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; sys
sys.path.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;../&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;from&lt;/span&gt; pycore.tikzeng &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; defined your arch&lt;/span&gt;
arch &lt;span class="pl-k"&gt;=&lt;/span&gt; [
    to_head( &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;..&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; ),
    to_cor(),
    to_begin(),
    to_Conv(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conv1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;512&lt;/span&gt;, &lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;depth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt; ),
    to_Pool(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(conv1-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
    to_Conv(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conv2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(1,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(pool1-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;, &lt;span class="pl-v"&gt;depth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;, &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt; ),
    to_connection( &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conv2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
    to_Pool(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(conv2-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;28&lt;/span&gt;, &lt;span class="pl-v"&gt;depth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;28&lt;/span&gt;, &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;),
    to_SoftMax(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;soft1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;10&lt;/span&gt; ,&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(3,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(pool1-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;caption&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;SOFT&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;  ),
    to_connection(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;soft1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
    to_end()
    ]

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;main&lt;/span&gt;():
    namefile &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;str&lt;/span&gt;(sys.argv[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]).split(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;.&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]
    to_generate(arch, namefile &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;.tex&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; )

&lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;__name__&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;__main__&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:
    main()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, run the program as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bash ../tikzmake.sh my_arch
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;p&gt;Following are some network representations:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/17570785/50308846-c2231880-049c-11e9-8763-3daa1024de78.png"&gt;&lt;img src="https://user-images.githubusercontent.com/17570785/50308846-c2231880-049c-11e9-8763-3daa1024de78.png" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-fcn-8" class="anchor" aria-hidden="true" href="#fcn-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FCN-8&lt;/h6&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/17570785/50308873-e2eb6e00-049c-11e9-9587-9da6bdec011b.png"&gt;&lt;img src="https://user-images.githubusercontent.com/17570785/50308873-e2eb6e00-049c-11e9-9587-9da6bdec011b.png" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-fcn-32" class="anchor" aria-hidden="true" href="#fcn-32"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FCN-32&lt;/h6&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/17570785/50308911-03b3c380-049d-11e9-92d9-ce15669017ad.png"&gt;&lt;img src="https://user-images.githubusercontent.com/17570785/50308911-03b3c380-049d-11e9-92d9-ce15669017ad.png" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-holistically-nested-edge-detection" class="anchor" aria-hidden="true" href="#holistically-nested-edge-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Holistically-Nested Edge Detection&lt;/h6&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>HarisIqbal88</author><guid isPermaLink="false">https://github.com/HarisIqbal88/PlotNeuralNet</guid><pubDate>Sun, 12 Jan 2020 00:12:00 GMT</pubDate></item></channel></rss>