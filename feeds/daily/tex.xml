<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: TeX, Today</title><link>https://github.com/trending/tex?since=daily</link><description>The top repositories on GitHub for tex, measured daily</description><pubDate>Wed, 29 Jan 2020 01:06:25 GMT</pubDate><lastBuildDate>Wed, 29 Jan 2020 01:06:25 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>MISP/misp-training #1 in TeX, Today</title><link>https://github.com/MISP/misp-training</link><description>&lt;p&gt;&lt;i&gt;MISP trainings, threat intel and information sharing training materials with source code&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-misp-training-materials" class="anchor" aria-hidden="true" href="#misp-training-materials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MISP Training Materials&lt;/h1&gt;
&lt;p&gt;This repository includes all the training materials in use such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Core MISP (software and standard) trainings&lt;/li&gt;
&lt;li&gt;Threat intelligence and OSINT training&lt;/li&gt;
&lt;li&gt;Building information sharing communities workshop&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All the materials are available with the complete LaTeX source code meant to assist in contributing or extending the training materials. A special attention is given to the open source licensing
given to the materials. We welcome contributions in order to improve the training set for threat intelligence, intelligence gathering and analysis along with specific aspects of information sharing/exchange in information and national security.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-materials" class="anchor" aria-hidden="true" href="#materials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Materials&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Slides (PDF)&lt;/th&gt;
&lt;th&gt;Source Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/0-misp-introduction-to-information-sharing.pdf" rel="nofollow"&gt;0-misp-introduction-to-information-sharing&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/0-misp-introduction-to-information-sharing"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/1-misp-usage.pdf" rel="nofollow"&gt;1-misp-usage&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/1-misp-usage"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/1.2-misp-integration.pdf" rel="nofollow"&gt;1.2-misp-integration&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/1.2-misp-integration"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/1.1-misp-viper-integration.pdf" rel="nofollow"&gt;1.1-misp-viper-integration&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/1.1-misp-viper-integration"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/1.2.1-misp-integration-mail2misp.pdf" rel="nofollow"&gt;1.2.1-misp-integration-mail2misp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/1.2.1-misp-integration-mail2misp"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/2-misp-administration.pdf" rel="nofollow"&gt;2-misp-administration&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/2-misp-administration"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/3-misp-taxonomy-tagging.pdf" rel="nofollow"&gt;3-misp-taxonomy-tagging&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/3-misp-taxonomy-tagging"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/3.1-misp-modules.pdf" rel="nofollow"&gt;3.1-misp-modules&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/3.1-misp-modules"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/3.2-misp-galaxy.pdf" rel="nofollow"&gt;3.2-misp-galaxy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/3.2-misp-galaxy"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/3.3-misp-object-template.pdf" rel="nofollow"&gt;3.3-misp-object-template&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/3.3-misp-object-template"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/6.0-misp-dashboard.pdf" rel="nofollow"&gt;6.0-misp-dashboard&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/6.0-misp-dashboard"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/a.0-contributing.pdf" rel="nofollow"&gt;a.0-contributing&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/a.0-contributing"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/a.1-devintro.pdf" rel="nofollow"&gt;a.1-devintro&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/a.1-devintro"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/a.2-pymisp.pdf" rel="nofollow"&gt;a.2-pymisp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/a.2-pymisp"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/a.3-misp-feed.pdf" rel="nofollow"&gt;a.3-misp-feed&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/a.3-misp-feed"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/a.4-best-practices.pdf" rel="nofollow"&gt;a.4-best-practices&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/a.4-best-practices"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/a.5-decaying-indicators.pdf" rel="nofollow"&gt;a.5-decaying-indicators&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/a.5-decaying-indicators"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/a.6-forensic.pdf" rel="nofollow"&gt;a.6-forensic&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/a.6-forensic"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/a.7-rest-API.pdf" rel="nofollow"&gt;a.7-rest-API&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/a.7-rest-API"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/a.8-dev-hands-on.pdf" rel="nofollow"&gt;a.8-dev-hands-on.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/a.8-dev-hands-on"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/a.9-restsearch-dev.pdf" rel="nofollow"&gt;a.9-restsearch-dev.pdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/a.9-restsearch-dev"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/b.1-best-practices-in-threat-intelligence.pdf" rel="nofollow"&gt;b.1-best-practices-in-threat-intelligence&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/b.1-best-practices-in-threat-intelligence"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/b.2-turning-data-into-actionable-intelligence.pdf" rel="nofollow"&gt;b.2-turning-data-into-actionable-intelligence&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/b.2-turning-data-into-actionable-intelligence"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-complementary-materials" class="anchor" aria-hidden="true" href="#complementary-materials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complementary materials&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Slides (PDF)&lt;/th&gt;
&lt;th&gt;Source Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/misp-training.pdf" rel="nofollow"&gt;complete slide desk in one PDF&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/cheatsheet.pdf" rel="nofollow"&gt;MISP training cheat-sheet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/training-support/compact-cheatsheet"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.misp-project.org/misp-training/usage.pdf" rel="nofollow"&gt;MISP feature list (for the trainers)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/MISP/misp-training/tree/master/training-support/checklist"&gt;source&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-additional-documentation" class="anchor" aria-hidden="true" href="#additional-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Additional documentation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MISP/misp-book/"&gt;MISP Book&lt;/a&gt; - &lt;a href="https://www.circl.lu/doc/misp/book.pdf" rel="nofollow"&gt;PDF&lt;/a&gt; &lt;a href="https://www.circl.lu/doc/misp/book.epub" rel="nofollow"&gt;ePub&lt;/a&gt; &lt;a href="https://www.circl.lu/doc/misp/book.mobi" rel="nofollow"&gt;Kindle mobi&lt;/a&gt; &lt;a href="https://www.circl.lu/doc/misp/" rel="nofollow"&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MISP/best-practices-in-threat-intelligence"&gt;Best Practices in Threat Intelligence&lt;/a&gt; &lt;a href="https://www.misp-project.org/best-practices-in-threat-intelligence.pdf" rel="nofollow"&gt;PDF&lt;/a&gt; &lt;a href="https://www.misp-project.org/best-practices-in-threat-intelligence.html" rel="nofollow"&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.misp-project.org/galaxy.html" rel="nofollow"&gt;MISP Galaxy (HTML)&lt;/a&gt; - &lt;a href="https://www.misp-project.org/galaxy.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.misp-project.org/taxonomies.html" rel="nofollow"&gt;MISP Taxonomies (HTML)&lt;/a&gt; - &lt;a href="https://www.misp-project.org/taxonomies.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.misp-project.org/objects.html" rel="nofollow"&gt;MISP Objects template (HTML)&lt;/a&gt; - &lt;a href="https://www.misp-project.org/objects.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MISP/misp-compliance/blob/master/setting-up-ISACs/guidelines_to_set-up_an_ISAC.md"&gt;Guidelines to setting up an information sharing community such as an ISAC or ISAO&lt;/a&gt; - &lt;a href="https://www.x-isac.org/assets/images/guidelines_to_set-up_an_ISAC.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://misp.github.io/MISP/" rel="nofollow"&gt;Official MISP Install Guides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-misp-training-videos" class="anchor" aria-hidden="true" href="#misp-training-videos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MISP Training videos&lt;/h3&gt;
&lt;p&gt;Sample videos which can be used to understand how the training materials are used in companion with a live MISP demo instance.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=aM7czPsQyaI" rel="nofollow"&gt;MISP Training Module 1 - An Introduction to Cybersecurity Information Sharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=Jqp8CVHtNVk" rel="nofollow"&gt;MISP Training Module 2 - General usage of MISP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-misp-training-support-videos" class="anchor" aria-hidden="true" href="#misp-training-support-videos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MISP Training support videos&lt;/h3&gt;
&lt;p&gt;Those are videos to support MISP trainings or demonstrations at large:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=NYvKLwoBYwc&amp;amp;t=8s" rel="nofollow"&gt;MISP Event graph demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=k3l-CtOgQro" rel="nofollow"&gt;MISP Tutorial - Enablings Feeds&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-misp-training-vms" class="anchor" aria-hidden="true" href="#misp-training-vms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MISP Training VMs&lt;/h3&gt;
&lt;p&gt;Pre-built MISP training VMs are available at &lt;a href="https://www.circl.lu/misp-images/latest/" rel="nofollow"&gt;https://www.circl.lu/misp-images/latest/&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-source-code" class="anchor" aria-hidden="true" href="#source-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source Code&lt;/h2&gt;
&lt;p&gt;The full source code of the training slide decks are available. You'll need to have an operating system with a recent installation of LaTeX including latex-beamer to work with them.&lt;/p&gt;
&lt;p&gt;To build the complete set of training materials:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash build.sh&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The output directory will contain all the generated PDF files and the PDF file called &lt;code&gt;misp-training.pdf&lt;/code&gt; which is the complete handout of all the slides.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: In case the rendering is somewhat broken, it might be related to latex using the styles installed systemwide in &lt;code&gt;/usr/share/texlive/texmf-dist/tex/latex/beamertheme-focus&lt;/code&gt;. Removing this directory will solve the problem.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license-attribution-and-funding" class="anchor" aria-hidden="true" href="#license-attribution-and-funding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License, Attribution and Funding&lt;/h2&gt;
&lt;p&gt;All the materials are dual-licensed under GNU Affero General Public License version 3 or later and
the Creative Commons Attribution-ShareAlike 4.0 International. You can use either one of the licenses depending
of your use case of the training materials.&lt;/p&gt;
&lt;p&gt;The MISP project training materials are co-financed and supported by CIRCL Computer Incident Response Center Luxembourg&lt;a href="https://www.circl.lu/" rel="nofollow"&gt;&lt;/a&gt; and co-financed by a CEF (Connecting Europe Facility) funding under CEF-TC-2016-3 - Cyber Security as &lt;em&gt;Improving MISP as building blocks for next-generation information sharing&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ed88dc23770a892122c7b6627c89966ec43a88d4/68747470733a2f2f7777772e6d6973702d70726f6a6563742e6f72672f6173736574732f696d616765732f656e5f6365662e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/ed88dc23770a892122c7b6627c89966ec43a88d4/68747470733a2f2f7777772e6d6973702d70726f6a6563742e6f72672f6173736574732f696d616765732f656e5f6365662e706e67" alt="" data-canonical-src="https://www.misp-project.org/assets/images/en_cef.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/b0f45d791a4b7ade5184620160a042009be74e01/68747470733a2f2f7777772e636972636c2e6c752f6173736574732f696d616765732f6c6f676f2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/b0f45d791a4b7ade5184620160a042009be74e01/68747470733a2f2f7777772e636972636c2e6c752f6173736574732f696d616765732f6c6f676f2e706e67" alt="" data-canonical-src="https://www.circl.lu/assets/images/logo.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All the source code is available at &lt;a href="https://www.github.com/MISP/misp-training"&gt;https://www.github.com/MISP/misp-training&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you reuse the training materials, don't forget to include the above for attribution.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors-in-alphabetical-order" class="anchor" aria-hidden="true" href="#contributors-in-alphabetical-order"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors in alphabetical order&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Steve Clement &lt;a href="https://github.com/SteveClement"&gt;&lt;g-emoji class="g-emoji" alias="house" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e0.png"&gt;üè†&lt;/g-emoji&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Alexandre Dulaunoy &lt;a href="https://github.com/adulau"&gt;&lt;g-emoji class="g-emoji" alias="house" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e0.png"&gt;üè†&lt;/g-emoji&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Andras Iklody &lt;a href="https://github.com/iglocska"&gt;&lt;g-emoji class="g-emoji" alias="house" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e0.png"&gt;üè†&lt;/g-emoji&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sami Mokaddem &lt;a href="https://github.com/mokaddem"&gt;&lt;g-emoji class="g-emoji" alias="house" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e0.png"&gt;üè†&lt;/g-emoji&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sascha Rommelfangen &lt;a href="https://github.com/rommelfs"&gt;&lt;g-emoji class="g-emoji" alias="house" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e0.png"&gt;üè†&lt;/g-emoji&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Christian Studer &lt;a href="https://github.com/chrisr3d"&gt;&lt;g-emoji class="g-emoji" alias="house" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e0.png"&gt;üè†&lt;/g-emoji&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rapha√´l Vinot &lt;a href="https://github.com/rafiot"&gt;&lt;g-emoji class="g-emoji" alias="house" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e0.png"&gt;üè†&lt;/g-emoji&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gerard Wagener &lt;a href="https://github.com/haegardev"&gt;&lt;g-emoji class="g-emoji" alias="house" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e0.png"&gt;üè†&lt;/g-emoji&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>MISP</author><guid isPermaLink="false">https://github.com/MISP/misp-training</guid><pubDate>Wed, 29 Jan 2020 00:01:00 GMT</pubDate></item><item><title>jriou/wcov #2 in TeX, Today</title><link>https://github.com/jriou/wcov</link><description>&lt;p&gt;&lt;i&gt;Pattern of early human-to-human transmission of Wuhan 2019-nCoV&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pattern-of-early-human-to-human-transmission-of-wuhan-2019-ncov" class="anchor" aria-hidden="true" href="#pattern-of-early-human-to-human-transmission-of-wuhan-2019-ncov"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pattern of early human-to-human transmission of Wuhan 2019-nCoV&lt;/h1&gt;
&lt;p&gt;Julien Riou, MD PhD (&lt;a href="mailto:julien.riou@ispm.unibe.ch"&gt;julien.riou@ispm.unibe.ch&lt;/a&gt;) and Christian L. Althaus, PhD (&lt;a href="mailto:christian.althaus@alumni.ethz.ch"&gt;christian.althaus@alumni.ethz.ch&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Institute of Social and Preventive Medicine, University of Bern, Switzerland&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract.&lt;/strong&gt; On December 31, 2019, the World Health Organization was notified about a cluster of pneumonia of unknown aetiology in the city of Wuhan, China. Chinese authorities later identified a new coronavirus (2019-nCoV) as the causative agent of the outbreak. As of January 24, 2020, 897 cases have been confirmed in China and several other countries. Understanding the transmission characteristics and the potential for sustained human-to-human transmission of 2019-nCoV is critically important for coordinating current screening and containment strategies, and determining whether the outbreak constitutes a public health emergency of international concern (PHEIC). We performed stochastic simulations of early outbreak trajectories that are consistent with the epidemiological findings to date. We found the basic reproduction number, R_0, to be around 2.2 (90% high density interval 1.4--3.8), indicating the potential for sustained human-to-human transmission. Transmission characteristics appear to be of similar magnitude to severe acute respiratory syndrome-related coronavirus (SARS-CoV) and the 1918 pandemic influenza. These findings underline the importance of heightened screening, surveillance and control efforts, particularly at airports and other transportation hubs, in order to prevent further international spread of 2019-nCoV.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="figure/fig_comb.png"&gt;&lt;img src="figure/fig_comb.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure.&lt;/strong&gt; Proportion of simulated epidemics that lead to a cumulative incidence
between 1,000 and 9,700 on January 18, 2020. This can be interpreted
as the combinations of R_0 and k values most compatible with
epidemic data available on 2019-nCoV as of January 23, 2020.
As a comparison, we show the estimates of R_0 and k for the
early human-to-human transmission of SARS-CoV in Singapore and
Beijing, and of 1918 pandemic influenza (Lloyd-Smith et al., 2005;
Fraser et al., 2011; Kucharski et al., 2015).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jriou</author><guid isPermaLink="false">https://github.com/jriou/wcov</guid><pubDate>Wed, 29 Jan 2020 00:02:00 GMT</pubDate></item><item><title>wzchen/probability_cheatsheet #3 in TeX, Today</title><link>https://github.com/wzchen/probability_cheatsheet</link><description>&lt;p&gt;&lt;i&gt;A comprehensive 10-page probability cheatsheet that covers a semester's worth of introduction to probability.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;This cheatsheet is a 10-page reference in probability that covers a semester's worth of introductory probability.&lt;/p&gt;
&lt;p&gt;The cheatsheet is based off of Harvard's introductory probability course, Stat 110. It is co-authored by former Stat 110 Teaching Fellow William Chen and Stat 110 Professor Joe Blitzstein.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.wzchen.com/probability-cheatsheet/" rel="nofollow"&gt;Probability Cheatsheet PDF&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-screenshots" class="anchor" aria-hidden="true" href="#screenshots"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Screenshots&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/73e37b445399a840a788db462c856618872af794/687474703a2f2f692e696d6775722e636f6d2f4f61373368754c2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/73e37b445399a840a788db462c856618872af794/687474703a2f2f692e696d6775722e636f6d2f4f61373368754c2e6a7067" alt="First Page" data-canonical-src="http://i.imgur.com/Oa73huL.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/456680a899a08b8d6614af49a112a12de52de255/687474703a2f2f692e696d6775722e636f6d2f647976573272422e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/456680a899a08b8d6614af49a112a12de52de255/687474703a2f2f692e696d6775722e636f6d2f647976573272422e6a7067" alt="Second Page" data-canonical-src="http://i.imgur.com/dyvW2rB.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This work is licensed under a &lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5cb0fbd8f1965c972542daadbfe2e58ec194e545/687474703a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d6e632d73612f342e302f38387833312e706e67" alt="Creative Commons License" data-canonical-src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-planned-additions" class="anchor" aria-hidden="true" href="#planned-additions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Planned Additions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Monty Hall&lt;/li&gt;
&lt;li&gt;St. Petersburg Paradox&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-20--2015-09-04" class="anchor" aria-hidden="true" href="#20--2015-09-04"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.0 / 2015-09-04&lt;/h3&gt;
&lt;p&gt;Contributor: Joe Blitzstein&lt;/p&gt;
&lt;p&gt;Summary: A huge thanks goes to Professor Joe Blitzstein (@stat110) for his work in polishing up the cheatsheet, adding colors and figures, and officially using it in his Stat 110 class (which is the class that inspired this cheatsheet to begin with). The cheatsheet is now a joint work by William and Professor Blitzstein.&lt;/p&gt;
&lt;p&gt;Added&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Figures&lt;/li&gt;
&lt;li&gt;Probability of an Intersection or Union&lt;/li&gt;
&lt;li&gt;Distributions in R Section&lt;/li&gt;
&lt;li&gt;Colors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fixes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extensive polishing and rephrasing for clarity, consistency, and quality.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moved&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inequalities moved from the back reference sheet to inside the cheatsheet under "Distribution Properties"&lt;/li&gt;
&lt;li&gt;Some sections reordered&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Removed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Repeated definitions of PMF and CDF&lt;/li&gt;
&lt;li&gt;Calculating Probability (2) from Example Problems&lt;/li&gt;
&lt;li&gt;MGF -- Distribution Matching from Example Problems&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-111--2015-03-20" class="anchor" aria-hidden="true" href="#111--2015-03-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.1.1 / 2015-03-20&lt;/h3&gt;
&lt;p&gt;Summary: Cleaned up sections, moved things around to more logical locations, and fixed one error in formula. Also removed some left margins, which allowed room for a Settlers of Catan diagram and the hat-matching problem.&lt;/p&gt;
&lt;p&gt;Added&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Settlers of Catan diagram&lt;/li&gt;
&lt;li&gt;Hat-matching problem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Removed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unused theorems and commands&lt;/li&gt;
&lt;li&gt;Left margins on descriptions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Major Fixes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fixed error in Law of Total Probability formula&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Merged&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Classic Problems is now merged into Example Problems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moved&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reorganized material on Beta distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-11--2015-02-28" class="anchor" aria-hidden="true" href="#11--2015-02-28"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.1 / 2015-02-28&lt;/h3&gt;
&lt;p&gt;Summary: Added a new page of content, including vast improvements to problem solving solutions and a new biohazards section.&lt;/p&gt;
&lt;p&gt;Added&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Probability that the normal distribution falls in a certain interval&lt;/li&gt;
&lt;li&gt;Convolutions section&lt;/li&gt;
&lt;li&gt;Minimum and Maximum of Random Variables problem&lt;/li&gt;
&lt;li&gt;Adam and Eve's Laws problem&lt;/li&gt;
&lt;li&gt;MGF - Distribution Matching problem&lt;/li&gt;
&lt;li&gt;Two Markov Chain problems&lt;/li&gt;
&lt;li&gt;Major improvements to Problem Solving Strategies&lt;/li&gt;
&lt;li&gt;Biohazards Section&lt;/li&gt;
&lt;li&gt;Recommended Resources Section)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Removed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Duplicate Law of Total Expectation section&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moved&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amoeba Spliting and Typos in your Textbook to Example Problems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Major Fixes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fixed solution to "Orderings of i.i.d. random variables"&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-10--2014-07-13" class="anchor" aria-hidden="true" href="#10--2014-07-13"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.0 / 2014-07-13&lt;/h3&gt;
&lt;p&gt;Added&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Probability Cheatsheet&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contact-us" class="anchor" aria-hidden="true" href="#contact-us"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact Us&lt;/h2&gt;
&lt;p&gt;For comments, suggestions, and corrections: contact William, submit a pull request, or submit an issue on Github!&lt;/p&gt;
&lt;p&gt;William Chen - You can message William on &lt;a href="https://www.quora.com/William-Chen-6" rel="nofollow"&gt;Quora&lt;/a&gt;, tweet him at &lt;a href="https://twitter.com/wzchen" rel="nofollow"&gt;@wzchen&lt;/a&gt;, or contact him via &lt;a href="http://www.wzchen.com/" rel="nofollow"&gt;his website&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Joe Blitzstein - You can tweet Joe at &lt;a href="https://twitter.com/stat110" rel="nofollow"&gt;@stat110&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>wzchen</author><guid isPermaLink="false">https://github.com/wzchen/probability_cheatsheet</guid><pubDate>Wed, 29 Jan 2020 00:03:00 GMT</pubDate></item><item><title>billryan/resume #4 in TeX, Today</title><link>https://github.com/billryan/resume</link><description>&lt;p&gt;&lt;i&gt;An elegant \LaTeX\ r√©sum√© template&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-r√©sum√©" class="anchor" aria-hidden="true" href="#r√©sum√©"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;R√©sum√©&lt;/h1&gt;
&lt;p&gt;Hit branch &lt;a href="https://github.com/billryan/resume/tree/zh_CN"&gt;zh_CN&lt;/a&gt; if you want a Simplified Chinese r√©sum√©.&lt;/p&gt;
&lt;p&gt;‰∏≠ÊñáÁî®Êà∑ËØ∑ÂâçÂæÄ &lt;a href="https://github.com/billryan/resume/tree/zh_CN"&gt;zh_CN&lt;/a&gt; ÂàÜÊîØ„ÄÇ&lt;/p&gt;
&lt;p&gt;An elegant \LaTeX\ r√©sum√© template, compiled with \XeLaTeX. Inspired by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zachscrivena/simple-resume-cv"&gt;zachscrivena/simple-resume-cv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ctan.org/pkg/res" rel="nofollow"&gt;res&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianxu.net/en/files/JianXu_CV.pdf" rel="nofollow"&gt;JianXu's CV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stat.berkeley.edu/~paciorek/computingTips/Latex_template_creating_CV_.html" rel="nofollow"&gt;paciorek's CV/Resume template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.sharelatex.com/blog/2011/03/27/how-to-write-a-latex-class-file-and-design-your-own-cv.html" rel="nofollow"&gt;How to write a LaTeX class file and design your own CV (Part 1) - ShareLaTeX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Easy to further customize or extend&lt;/li&gt;
&lt;li&gt;Full support for unicode characters (e.g. CJK) with \XeLaTeX\&lt;/li&gt;
&lt;li&gt;Perfect Simplified Chinese fonts supported with Adobefonts&lt;/li&gt;
&lt;li&gt;FontAwesome 4.6.3 support&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fork this repository&lt;/li&gt;
&lt;li&gt;Add information about you directly in GitHub&lt;/li&gt;
&lt;li&gt;Compile TeX file to PDF with &lt;a href="https://latexonline.cc/" rel="nofollow"&gt;LaTeX.Online&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Can also use Overleaf for online compilation (&lt;a href="https://www.overleaf.com/" rel="nofollow"&gt;https://www.overleaf.com/&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-sample-output" class="anchor" aria-hidden="true" href="#sample-output"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample Output&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1292567/62409353-3fecfc00-b608-11e9-8e83-84962912c956.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1292567/62409353-3fecfc00-b608-11e9-8e83-84962912c956.png" alt="English" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1292567/62409351-3f546580-b608-11e9-9f6d-d232a68c5451.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1292567/62409351-3f546580-b608-11e9-9f6d-d232a68c5451.png" alt="English with photo" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1292567/62409352-3fecfc00-b608-11e9-8d9e-76243ca3052a.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1292567/62409352-3fecfc00-b608-11e9-8d9e-76243ca3052a.png" alt="ÁÆÄ‰Ωì‰∏≠Êñá" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/billryan/resume/files/3463503/resume.pdf"&gt;English PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/billryan/resume/files/3463501/resume_photo.pdf"&gt;English with photo PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/billryan/resume/files/3463502/resume-zh_CN.pdf"&gt;ÁÆÄ‰Ωì‰∏≠Êñá PDF&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Edit in ShareLaTeX online - &lt;a href="https://www.sharelatex.com/templates/556b27cf0d23e5a8117053d9" rel="nofollow"&gt;https://www.sharelatex.com/templates/556b27cf0d23e5a8117053d9&lt;/a&gt;, &lt;strong&gt;no TeX software install!&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Compile tex on your Computer&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you only need a r√©sum√© in English or have installed Adobe Simplified Chinese on your OS, &lt;strong&gt;It would be better to clone only the master branch,&lt;/strong&gt; since the Simplified Chinese fonts files are too large.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/billryan/resume.git --branch master --depth 1 --single-branch &amp;lt;folder&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://opensource.org/licenses/MIT" rel="nofollow"&gt;The MIT License (MIT)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Copyrighted fonts are not subjected to this License.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>billryan</author><guid isPermaLink="false">https://github.com/billryan/resume</guid><pubDate>Wed, 29 Jan 2020 00:04:00 GMT</pubDate></item><item><title>mohuangrui/ucasthesis #5 in TeX, Today</title><link>https://github.com/mohuangrui/ucasthesis</link><description>&lt;p&gt;&lt;i&gt; [ÊúÄÊñ∞Ê†∑Âºè] ‰∏≠ÂõΩÁßëÂ≠¶Èô¢Â§ßÂ≠¶Â≠¶‰ΩçËÆ∫Êñá LaTeX Ê®°Êùø  LaTeX Thesis Template for the University of Chinese Academy of Sciences &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ucasthesis-ÂõΩÁßëÂ§ßÂ≠¶‰ΩçËÆ∫Êñá-latex-Ê®°Êùø-ÊúÄÊñ∞Ê†∑Âºè" class="anchor" aria-hidden="true" href="#ucasthesis-ÂõΩÁßëÂ§ßÂ≠¶‰ΩçËÆ∫Êñá-latex-Ê®°Êùø-ÊúÄÊñ∞Ê†∑Âºè"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;ucasthesis&lt;/code&gt; ÂõΩÁßëÂ§ßÂ≠¶‰ΩçËÆ∫Êñá LaTeX Ê®°Êùø [ÊúÄÊñ∞Ê†∑Âºè]&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-Ê®°Êùø‰∏ãËΩΩ" class="anchor" aria-hidden="true" href="#Ê®°Êùø‰∏ãËΩΩ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ê®°Êùø‰∏ãËΩΩ&lt;/h2&gt;
&lt;p&gt;ËØ∑Âú®È°µÈù¢Âè≥ËæπÁÇπÂáªÔºö&lt;strong&gt;Clone or download -&amp;gt; Download Zip&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ÈáçË¶ÅÂª∫ËÆÆ" class="anchor" aria-hidden="true" href="#ÈáçË¶ÅÂª∫ËÆÆ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÈáçË¶ÅÂª∫ËÆÆ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ÂÖ≥‰∫é ucasthesis ÁºñËØëÂíåËÆæËÆ°ÁöÑÈóÆÈ¢òÔºåËØ∑ÂÖàËØª &lt;strong&gt;Ê®°Êùø‰ΩøÁî®ËØ¥Êòé.pdf&lt;/strong&gt;ÔºåÂ¶ÇÂèëÈóÆÈúÄÈÅµ‰ªé&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"&gt;ÊèêÈóÆÊµÅÁ®ã&lt;/a&gt;„ÄÇ&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®ÈÇÆ‰ª∂‰º†Êí≠ ucasthesis Êó∂ÔºåËØ∑ÂÖàÂà†Èô§ &lt;code&gt;artratex.bat&lt;/code&gt; ‰ª•Èò≤ËåÉ Dos ËÑöÊú¨ÁöÑÊΩúÂú®È£éÈô©„ÄÇ&lt;/li&gt;
&lt;li&gt;ÂºÄÈ¢òÊä•ÂëäËØ∑ËßÅÔºö&lt;a href="https://github.com/mohuangrui/ucasproposal"&gt;ucasproposal: ‰∏≠ÂõΩÁßëÂ≠¶Èô¢Â§ßÂ≠¶ÂºÄÈ¢òÊä•Âëä LaTeX Ê®°Êùø&lt;/a&gt;„ÄÇ&lt;/li&gt;
&lt;li&gt;‰π¶ËÑäÂà∂‰ΩúËØ∑ËßÅÔºö&lt;a href="https://github.com/mohuangrui/latexspine"&gt;latexspine: LaTeX ‰π¶ËÑäÊ®°Êùø&lt;/a&gt;„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-Ê®°ÊùøÁÆÄ‰ªã" class="anchor" aria-hidden="true" href="#Ê®°ÊùøÁÆÄ‰ªã"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ê®°ÊùøÁÆÄ‰ªã&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ucasthesis ‰∏∫Êí∞ÂÜô‰∏≠ÂõΩÁßëÂ≠¶Èô¢Â§ßÂ≠¶&lt;strong&gt;Êú¨&lt;/strong&gt;„ÄÅ&lt;strong&gt;Á°ï&lt;/strong&gt;„ÄÅ&lt;strong&gt;Âçö&lt;/strong&gt;Â≠¶‰ΩçËÆ∫ÊñáÂíå&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-"&gt;&lt;strong&gt;ÂçöÂêé&lt;/strong&gt;&lt;/a&gt;Êä•ÂëäÁöÑ LaTeX Ê®°Áâà„ÄÇucasthesis Êèê‰æõ‰∫ÜÁÆÄÂçïÊòé‰∫ÜÁöÑ&lt;strong&gt;Ê®°Êùø‰ΩøÁî®ËØ¥Êòé.pdf&lt;/strong&gt;„ÄÇÊó†ËÆ∫‰Ω†ÊòØÂê¶ÂÖ∑Êúâ LaTeX ‰ΩøÁî®ÁªèÈ™åÔºåÈÉΩÂèØËæÉ‰∏∫ËΩªÊùæÂú∞‰ΩøÁî®‰ª•ÂÆåÊàêÂ≠¶‰ΩçËÆ∫ÊñáÁöÑÊí∞ÂÜôÂíåÊéíÁâà„ÄÇË∞¢Ë∞¢Â§ßÂÆ∂ÁöÑÊµãËØï„ÄÅÂèçÈ¶àÂíåÊîØÊåÅÔºåÊàë‰ª¨‰∏ÄËµ∑ÁöÑÂä™ÂäõËÆ© ucasthesis ÈùûÂ∏∏Ëç£Âπ∏Âú∞ÂæóÂà∞‰∫ÜÂõΩÁßëÂ§ßÊú¨ÁßëÈÉ®ÈôÜÊô¥ËÄÅÂ∏à„ÄÅÊú¨ÁßëÈÉ®Â≠¶‰ΩçÂäû‰∏Å‰∫ë‰∫ëËÄÅÂ∏àÂíå‰∏≠ÁßëÈô¢Êï∞Â≠¶‰∏éÁ≥ªÁªüÁßëÂ≠¶Á†îÁ©∂Èô¢Âê¥Âáå‰∫ëÁ†îÁ©∂ÂëòÁöÑÊîØÊåÅÔºåÂπ∂ÂæóÂà∞Âê¥Âáå‰∫ëÂ≠¶ÈïøÂú® &lt;a href="http://www.ctex.org/HomePage" rel="nofollow"&gt;CTEX&lt;/a&gt; ÁöÑÂèëÂ∏É„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ËÄÉËôëÂà∞ËÆ∏Â§öÂêåÂ≠¶ÂèØËÉΩÁº∫‰πè LaTeX ‰ΩøÁî®ÁªèÈ™åÔºåucasthesis Â∞Ü LaTeX ÁöÑÂ§çÊùÇÊÄßÈ´òÂ∫¶Â∞ÅË£ÖÔºåÂºÄÊîæÂá∫ÁÆÄÂçïÁöÑÊé•Âè£Ôºå‰ª•‰æøËΩªÊòì‰ΩøÁî®„ÄÇÂêåÊó∂ÔºåÂØπÁî® LaTeX Êí∞ÂÜôËÆ∫ÊñáÁöÑ‰∏Ä‰∫õ‰∏ªË¶ÅÈöæÈ¢òÔºåÂ¶ÇÂà∂Âõæ„ÄÅÂà∂Ë°®„ÄÅÊñáÁåÆÁ¥¢ÂºïÁ≠âÔºåËøõË°å‰∫ÜËØ¶ÁªÜËØ¥ÊòéÔºåÂπ∂Êèê‰æõ‰∫ÜÁõ∏Â∫îÁöÑ‰ª£Á†ÅÊ†∑Êú¨ÔºåÁêÜËß£‰∫Ü‰∏äËø∞ÈóÆÈ¢òÂêéÔºåÂØπ‰∫éÂàùÂ≠¶ËÄÖËÄåË®ÄÔºå‰ΩøÁî®Ê≠§Ê®°ÊùøÊí∞ÂÜôÂ≠¶‰ΩçËÆ∫ÊñáÂ∞Ü‰∏çÂ≠òÂú®ÂÆûË¥®ÊÄßÁöÑÂõ∞Èöæ„ÄÇÊâÄ‰ª•ÔºåÂ¶ÇÊûú‰Ω†ÊòØÂàùÂ≠¶ËÄÖÔºåËØ∑‰∏çË¶ÅÁõ¥Êé•ÊîæÂºÉÔºåÂõ†‰∏∫ÂêåÊ†∑‰∏∫ÂàùÂ≠¶ËÄÖÁöÑÊàëÔºåÂçÅÂàÜÊòéÁôΩËÆ© LaTeX ÁÆÄÂçïÊòìÁî®ÁöÑÈáçË¶ÅÊÄßÔºåËÄåËøôÊ≠£ÊòØ ucasthesis ÊâÄËøΩÊ±ÇÂíå‰ΩìÁé∞ÁöÑ„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ê≠§‰∏≠ÂõΩÁßëÂ≠¶Èô¢Â§ßÂ≠¶Â≠¶‰ΩçËÆ∫ÊñáÊ®°Êùø ucasthesis Âü∫‰∫é‰∏≠ÁßëÈô¢Êï∞Â≠¶‰∏éÁ≥ªÁªüÁßëÂ≠¶Á†îÁ©∂Èô¢Âê¥Âáå‰∫ëÁ†îÁ©∂ÂëòÁöÑ CASthesis Ê®°ÊùøÂèëÂ±ïËÄåÊù•„ÄÇÂΩìÂâç ucasthesis Ê®°ÊùøÊª°Ë∂≥ÊúÄÊñ∞ÁöÑ‰∏≠ÂõΩÁßëÂ≠¶Èô¢Â§ßÂ≠¶Â≠¶‰ΩçËÆ∫ÊñáÊí∞ÂÜôË¶ÅÊ±ÇÂíåÂ∞ÅÈù¢ËÆæÂÆö„ÄÇÂÖºÈ°æÊìç‰ΩúÁ≥ªÁªüÔºöWindowsÔºåLinuxÔºåMacOS Âíå LaTeX ÁºñËØëÂºïÊìéÔºöpdflatexÔºåxelatexÔºålualatex„ÄÇÊîØÊåÅ‰∏≠Êñá‰π¶Á≠æ„ÄÅ‰∏≠ÊñáÊ∏≤Êüì„ÄÅ‰∏≠ÊñáÁ≤ó‰ΩìÊòæÁ§∫„ÄÅÊã∑Ë¥ù PDF ‰∏≠ÁöÑÊñáÊú¨Âà∞ÂÖ∂‰ªñÊñáÊú¨ÁºñËæëÂô®Á≠âÁâπÊÄßÔºà&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE"&gt;Windows Á≥ªÁªü PDF Êã∑Ë¥ù‰π±Á†ÅÁöÑËß£ÂÜ≥ÊñπÊ°àÈúÄËßÅÔºöÂ≠ó‰ΩìÈÖçÁΩÆ&lt;/a&gt;Ôºâ„ÄÇÊ≠§Â§ñÔºåÂØπÊ®°ÊùøÁöÑÊñáÊ°£ÁªìÊûÑËøõË°å‰∫ÜÁ≤æÂøÉËÆæËÆ°ÔºåÊí∞ÂÜô‰∫ÜÁºñËØëËÑöÊú¨ÊèêÈ´òÊ®°ÊùøÁöÑÊòìÁî®ÊÄßÂíå‰ΩøÁî®ÊïàÁéá„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ucasthesis ÁöÑÁõÆÊ†áÂú®‰∫éÁÆÄÂåñÂ≠¶‰ΩçËÆ∫ÊñáÁöÑÊí∞ÂÜôÔºåÂà©Áî® LaTeX Ê†ºÂºè‰∏éÂÜÖÂÆπÂàÜÁ¶ªÁöÑÁâπÂæÅÔºåÊ®°ÊùøÂ∞ÜÊ†ºÂºèËÆæËÆ°Â•ΩÂêéÔºå‰ΩúËÄÖÂèØÂè™ÈúÄÂÖ≥Ê≥®ËÆ∫ÊñáÂÜÖÂÆπ„ÄÇ ÂêåÊó∂Ôºåucasthesis ÊúâÁùÄÊï¥Ê¥Å‰∏ÄËá¥ÁöÑ‰ª£Á†ÅÁªìÊûÑÂíåÊâºË¶ÅÁöÑÊ≥®Ëß£ÔºåÂØπÊñáÊ°£ÁöÑ‰ªîÁªÜÈòÖËØªÂèØ‰∏∫ÂàùÂ≠¶ËÄÖÊèê‰æõ‰∏Ä‰∏™Â≠¶‰π† LaTeX ÁöÑÁ™óÂè£„ÄÇÊ≠§Â§ñÔºåÊ®°ÊùøÁöÑÊû∂ÊûÑÂçÅÂàÜÊ≥®ÈáçÈÄöÁî®ÊÄßÔºå‰∫ãÂÆû‰∏äÔºåucasthesis ‰∏ç‰ªÖÊòØÂõΩÁßëÂ§ßÂ≠¶‰ΩçËÆ∫ÊñáÊ®°ÊùøÔºåÂêåÊó∂ÔºåÈÄöËøáÂ∞ëÈáè‰øÆÊîπÂç≥ÂèØÊàê‰∏∫‰ΩøÁî® LaTeX Êí∞ÂÜô‰∏≠Ëã±ÊñáÊñáÁ´†Êàñ‰π¶Á±çÁöÑÈÄöÁî®Ê®°ÊùøÔºåÂπ∂‰∏∫‰ΩøÁî®ËÄÖÁöÑ‰∏™ÊÄßÂåñËÆæÂÆöÊèê‰æõ‰∫ÜÊé•Âè£„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ÈáçË¶ÅÈÄöÁü•" class="anchor" aria-hidden="true" href="#ÈáçË¶ÅÈÄöÁü•"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÈáçË¶ÅÈÄöÁü•&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;2020-01-09&lt;/code&gt; Ê®°ÊùøÊ†∑ÂºèËøõË°å‰∫Ü‰øÆÊîπÔºåËØ∑Êü•Áúã‰∏ãÈù¢ÁöÑ‰øÆÊîπÊèèËø∞Ôºå‰ª•ÂÜ≥ÂÆöÊòØÂê¶ÈúÄË¶ÅÊõ¥Êñ∞„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-Êõ¥Êñ∞ËÆ∞ÂΩï" class="anchor" aria-hidden="true" href="#Êõ¥Êñ∞ËÆ∞ÂΩï"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Êõ¥Êñ∞ËÆ∞ÂΩï&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2020-01-09&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/223"&gt;NineSH, issue #223&lt;/a&gt; ‰øÆÂ§ç&lt;code&gt;bicaption&lt;/code&gt;ÈîôËØØ„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-12-06&lt;/code&gt; ÁßªÈô§ commit ‰∏≠ÁöÑ‰∫åËøõÂà∂Êñá‰ª∂Ôºå‰ª•ÊûÅÂ§ßÂáèÂ∞ë Fork ÂêéÁöÑÊñá‰ª∂Â§ßÂ∞è„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/198"&gt;huiwenzhang, issue #198&lt;/a&gt; ‰øÆÂ§ç&lt;code&gt;mainmatter&lt;/code&gt;‰∏ã&lt;code&gt;\chapter*&lt;/code&gt;ÁöÑÈ°µÁúâÈîôËØØ„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/195"&gt;Fancy0609, muzimuzhi, issue #195&lt;/a&gt; Ë∞ÉÊï¥Áî±&lt;code&gt;AutoFakeBold&lt;/code&gt;ÊéßÂà∂ÁöÑ‰º™Á≤ó‰ΩìÂä†Á≤óÁ®ãÂ∫¶„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-11&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/190"&gt;Pantrick, issue #190&lt;/a&gt; ÈááÁî® &lt;a href="https://github.com/muzimuzhi"&gt;muzimuzhi&lt;/a&gt; Êèê‰æõÁöÑÊñπÊ≥ïÂÆûÁé∞&lt;code&gt;\advisor{}&lt;/code&gt;Âíå&lt;code&gt;\institute{}&lt;/code&gt;ÁöÑËá™Âä®Êç¢Ë°åÂäüËÉΩ„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-08-01&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/183"&gt;vectorliu, issue #183&lt;/a&gt; ‰øÆÊîπËã±ÊñáÊ®°Âºè‰∏ãÁöÑ&lt;code&gt;plain&lt;/code&gt;ÈÄâÈ°π‰∏∫&lt;code&gt;scheme=plain&lt;/code&gt;‰ª•Ê∂àÈô§ÂØπ&lt;code&gt;Algorithm&lt;/code&gt;Ê†∑ÂºèÁöÑ‰øÆÊîπ„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-06-15&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/177"&gt;HaorenWang, issue #177&lt;/a&gt; Ë∞ÉÊï¥Áü¢Èáè„ÄÅÁü©Èòµ„ÄÅÂº†ÈáèÂ≠ó‰ΩìÊ†∑Âºè„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-06-09&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/170"&gt;DRjy, issue #170&lt;/a&gt; ËΩªÂæÆÁº©ÂáèÁõÆÂΩï‰∏≠ÁºñÂè∑‰∏éÊ†áÈ¢òÁöÑÈó¥Ë∑ùÔºõÊ†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/174"&gt;e71828, issue #174&lt;/a&gt; ËΩªÂæÆÂ¢ûÂä†È°µÁúâ‰∏≠ÁºñÂè∑‰∏éÊ†áÈ¢òÁöÑÈó¥Ë∑ù„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-05-25&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/169"&gt;CDMA2019, issue #169&lt;/a&gt; Êèê‰æõÊ®™ÊéíÂõæË°®ÁéØÂ¢É‰∏ãÈ°µÁúâÈ°µËÑöÁöÑÊ®™ÊéíÔºåÂÖ∑‰Ωì‰ΩøÁî®ËßÅ &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%A8%AA%E6%8E%92%E5%9B%BE%E8%A1%A8"&gt;Ê®™ÊéíÂõæË°®&lt;/a&gt;„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-24&lt;/code&gt; ÊãìÂ±ïÊ®°ÁâàÂÖºÂÆπ &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-"&gt;ÂçöÂêéÊä•Âëä&lt;/a&gt;„ÄÇ‰øÆÂ§ç &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/156"&gt;gsp2014, issue #156&lt;/a&gt; ÊñáÁåÆÂºïÁî®‰∏≠ÁöÑËøûÂ≠óÁ¨¶ÁöÑÈó¥Êñ≠ÊòæÁ§∫Âíå‰∏äÊ†áÂºïÁî®‰∏≠ÈÄóÂè∑‰∏ãÊ≤â„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-19&lt;/code&gt; ‰øÆÂ§ç &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/117"&gt;nihaomiao, issue #117&lt;/a&gt;&lt;code&gt;\mathbf&lt;/code&gt;Â§±ÊïàÈóÆÈ¢ò„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-16&lt;/code&gt; ‰øÆÂ§çÂõΩÈôÖÁîüÈúÄË¶ÅÁöÑ&lt;code&gt;plain&lt;/code&gt;Ê®°Âºè‰∏ãÊó†Ê≥ïÊîπÂèòËã±ÊñáÁ´†Ê†áÈ¢òÂ≠ó‰ΩìÂ§ßÂ∞èÁöÑÈóÆÈ¢ò„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-09&lt;/code&gt; ÂØπÈÉ®ÂàÜÂÆèÂëΩ‰ª§ËøõË°åË∞ÉÊï¥ÔºåÊó†ÂäüËÉΩÂèäÊ†∑Âºè‰∏äÁöÑ‰øÆÊîπ„ÄÇËã•ÈúÄÊõ¥Êñ∞ÔºåÂª∫ËÆÆÂèÇËÄÉ &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%9B%B4%E6%96%B0%E6%8C%87%E5%8D%97"&gt;Êõ¥Êñ∞ÊåáÂçó&lt;/a&gt;„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-04&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/134"&gt;liuy334, songchunlin, issue #134&lt;/a&gt; ÔºåË∞ÉÊï¥Ë°åË∑ù‰Ωø&lt;code&gt;LaTeX&lt;/code&gt;Áâà‰∏é&lt;code&gt;Word&lt;/code&gt;ÁâàÁöÑË°åÊï∞ÂíåÊØèË°åÂ≠óÊï∞Áõ∏‰∏ÄËá¥„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-28&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/49"&gt;zssasa, allenwoods, issue #49&lt;/a&gt; Ôºå‰øÆÂ§ç&lt;code&gt;bicaption&lt;/code&gt;ÂØπ&lt;code&gt;longtable&lt;/code&gt;ÁöÑÂÖºÂÆπÊÄß„ÄÇÊ†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/133"&gt;BowenHou, issue #133&lt;/a&gt; Ôºå‰Ωø‰∏ãÂàíÁ∫øËÉΩÂØπÈïøÊ†áÈ¢òËá™Âä®Êç¢Ë°å„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-25&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/127"&gt;DRjy, muzimuzhi, issue #127&lt;/a&gt; Ôºå‰∏∫&lt;code&gt;ÊëòË¶Å&lt;/code&gt;Á≠âÊó†ÈúÄÂú®ÁõÆÂΩï‰∏≠ÊòæÁ§∫ÁöÑÁªìÊûÑÂÖÉÁ¥†Âª∫Á´ã‰π¶Á≠æ„ÄÇÊ†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/130"&gt;muzimuzhi, issue #130&lt;/a&gt; Ôºå‰øÆÊ≠£ÂØπ&lt;code&gt;\voffset&lt;/code&gt;ÁöÑ‰ΩøÁî®„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-14&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/121"&gt;opt-gaobin, issue #121&lt;/a&gt; Ôºå‰øÆÊ≠£‰∏≠ÊñáÊ†áÁÇπ‰Ωø‰∏ãÂàíÁ∫øÊñ≠ÊéâÁöÑÈóÆÈ¢ò„ÄÇÊ†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/120"&gt;Guoqiang Zhang, email; weili-ict, issue #120&lt;/a&gt; Ôºå‰øÆÂ§ç&lt;code&gt;\proofname&lt;/code&gt;ÂëΩ‰ª§ÂØπ2015Âπ¥ÂèäÊõ¥Êó©&lt;code&gt;LaTeX&lt;/code&gt;ÁºñËØëÂô®ÁöÑÂÖºÂÆπÊÄßÈóÆÈ¢ò„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-02-20&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/100"&gt;opt-gaobin, issue #100&lt;/a&gt; ÔºåÂ¢ûÂä†ÂÆöÁêÜ„ÄÅÂÆö‰πâ„ÄÅËØÅÊòéÁ≠âÊï∞Â≠¶ÁéØÂ¢É„ÄÇÊ†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/102"&gt;DRjy, issue #102&lt;/a&gt; ÔºåË∞ÉÊï¥&lt;code&gt;\mathcal&lt;/code&gt;Â≠ó‰ΩìÊ†∑Âºè„ÄÇÊ†πÊçÆ [zike Liu, email] ÔºåÈÄÇÂΩìÁº©ÂáèÁõÆÂΩïÂàóË°®ÁöÑÁº©Ëøõ„ÄÇÊ†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/105"&gt;xiaoyaoE, issue #105&lt;/a&gt; Ôºå‰ΩøÊï∞Â≠óÂ≠ó‰ΩìÂíåËã±ÊñáÂ≠ó‰Ωì‰∏ÄËá¥„ÄÇÂÆåÂñÑ‰∏≠ÊñáÁâàÂíåÂõΩÈôÖÁâà‰πãÈó¥ÁöÑ‰∏≠Ëã±Ê†ºÂºèÂàáÊç¢„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-01-10&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/57"&gt;mnpengjk, issue #57&lt;/a&gt; Ôºå Â∞ÜÂÖ¨ÂºèÁºñÂè∑ÂâçÂä†ÁÇπÁ∫≥ÂÖ•Ê®°ÁâàÈªòËÆ§ÔºåÊõ¥Â§öËÆ®ËÆ∫ÂèØËßÅÔºö&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82"&gt;ÁêêÂ±ëÁªÜËäÇ&lt;/a&gt; „ÄÇÊ†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/95"&gt;yunyun2019, issue #95&lt;/a&gt; ÔºåÈááÁî® &lt;a href="https://github.com/zepinglee"&gt;zepinglee&lt;/a&gt; Âü∫‰∫éÂõΩÊ†áÊ†∑Âºè‰∏∫&lt;code&gt;ucas&lt;/code&gt;ÊâÄÂÆöÂà∂ÊñáÁåÆÊ†∑ÂºèÔºö&lt;a href="https://github.com/CTeX-org/gbt7714-bibtex-style/tree/ucas"&gt;ucas Ê†∑ÂºèÂàÜÊîØ&lt;/a&gt; ÔºåÊñáÁåÆÊ†∑ÂºèÊõ¥Â§öËÆ®ËÆ∫ÂèØËßÅÔºö&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%96%87%E7%8C%AE%E6%A0%B7%E5%BC%8F"&gt;ÊñáÁåÆÊ†∑Âºè&lt;/a&gt;„ÄÇÊ†πÊçÆ [ÈÇµÂ≤≥Êûó, email] ÔºåÂ∞ÜÈôÑÂΩïÂ§çÂéü‰∏∫Â∏∏ËßÑÁöÑÊéíÁâàËÆæÁΩÆÔºåËã•ÈúÄÂ∞ÜÈôÑÂΩïÁΩÆ‰∫éÂèÇËÄÉÊñáÁåÆÂêéÔºåËØ∑ËßÅÔºö&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82"&gt;ÁêêÂ±ëÁªÜËäÇ&lt;/a&gt;„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-04-03&lt;/code&gt; Ê†πÊçÆÂõΩÁßëÂ§ßÊú¨ÁßëÈÉ®ÈôÜÊô¥ËÄÅÂ∏àÂíåÊú¨ÁßëÈÉ®Â≠¶‰ΩçÂäû‰∏Å‰∫ë‰∫ëËÄÅÂ∏àÁöÑÂ§çÂÆ°ÂÆ°Ê†∏Âª∫ËÆÆÂÜçÊ¨°‰øÆÂ§ç‰∏Ä‰∫õÊ†∑ÂºèÁªÜËäÇÈóÆÈ¢ò„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-04-02&lt;/code&gt; Ê®°ÊùøËøõË°å‰∫ÜÈáçÂ§ßÊõ¥Êñ∞Ôºå‰øÆÂ§ç‰∫ÜÊ†∑Âºè„ÄÅÂ≠ó‰Ωì„ÄÅÊ†ºÂºèÁ≠âËÆ∏Â§öÈóÆÈ¢ò„ÄÇ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ê†πÊçÆÂõΩÁßëÂ§ßÊú¨ÁßëÈÉ®ÈôÜÊô¥ËÄÅÂ∏àÁöÑÂª∫ËÆÆÂØπÊ®°ÁâàÊ†∑ÂºèËøõË°å‰∫ÜËØ∏Â§öÊãìÂ±ïÂíå‰øÆÊ≠£ÔºåÂπ∂ÂÆåÂñÑÂØπÊú¨ÁßëÁîüËÆ∫ÊñáÂÖÉÁ¥†ÁöÑÂÖºÂÆπÊÄß„ÄÇ&lt;/li&gt;
&lt;li&gt;Âú® &lt;a href="https://github.com/CTeX-org/ctex-kit"&gt;ctex&lt;/a&gt; ÂºÄÂèëËÄÖÁöÑÂ∏ÆÂä©‰∏ãËß£ÂÜ≥‰∫ÜÂ¶Ç‰ΩïÂ§öÊ¨°Ë∞ÉÁî®&lt;code&gt;Times New Roman&lt;/code&gt;ËÄå‰∏çÂØºËá¥Èªë‰ΩìË∞ÉÁî®ÈîôËØØÁöÑÈóÆÈ¢ò„ÄÇÊ†πÊçÆ [twn1993, email]Ôºå‰øÆÂ§çÈªòËÆ§Èªë‰Ωì‰∏∫ÂæÆËΩØÈõÖÈªëËÄå‰∏çÊòØ&lt;code&gt;SimHei&lt;/code&gt;ÁöÑÈóÆÈ¢ò„ÄÇ&lt;/li&gt;
&lt;li&gt;ÁπÅÂ§çÊäòËÖæÊµãËØïÂêéÁªà‰∫éÊâæÂá∫‰∏Ä‰∏™Âú®&lt;code&gt;ctex&lt;/code&gt;ÈªòËÆ§Èªë‰ΩìÊõøÊç¢Á≤óÂÆã‰ΩìËÆæÂÆöÁéØÂ¢ÉÂÜÖÂÖ®Â±Ä&lt;code&gt;AutoFakeBold&lt;/code&gt;Â§±ÊïàÁä∂ÊÄÅ‰∏ãÊäòË°∑ÁâπÂÆöÂ≠ó‰ΩìÂ∫ì‰∏çÂÖ®Êù°‰ª∂‰∏ãÁîüÂÉªÂ≠óÊòæÁ§∫ÂíåÁ≥ªÁªüÈªòËÆ§Â≠óÈáç‰∏çÂÖ®Êù°‰ª∂‰∏ãÁ≤óÂÆã‰ΩìÊòæÁ§∫‰ª•Âèä‰∏çÂêåÊìç‰ΩúÁ≥ªÁªü‰∏ãÂ¶Ç‰ΩïÂπ≥Ë°°‰∏äËø∞Â≠óÂ∫ìËá™ÈáçÁüõÁõæËøòÊúâÊ†πÊçÆÊìç‰ΩúÁ≥ªÁªüËá™Âä®Ë∞ÉÁî®ÊâÄÂ∏¶ÊúâÁöÑ&lt;code&gt;Times&lt;/code&gt;Â≠ó‰ΩìÁöÑÊñπÊ°à„ÄÇ&lt;/li&gt;
&lt;li&gt;ËÆæÂÆöËÆ∫ÊñáÂ∞ÅÈù¢ÊçÆËã±ÊñáÂ≠¶‰ΩçÂêçÂ¶ÇËá™Âä®ÂàáÊç¢„ÄÇÂØÜÁ∫ßÊçÆÊòØÂê¶Â°´ÂÜôËá™Âä®ÊòæÁ§∫„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-03-22&lt;/code&gt; ÊºîÁ§∫Ë°®Ê†áÈ¢òÂ±ÖË°®‰∏äÔºåÂä†Á≤óÂõæË°®Ê†áÊ≥®ÔºåËÆæÁΩÆÈïøÂõæË°®Ê†áÈ¢òÊÇ¨ÊåÇÁº©ËøõÔºàÁî±‰∫é&lt;code&gt;bicaption&lt;/code&gt;ÂÆèÂåÖÊó†Ê≥ïÊ≠£Á°ÆÊé•Âèó&lt;code&gt;caption&lt;/code&gt;ÂÆèÂåÖÁöÑ&lt;code&gt;margin&lt;/code&gt;ÈÄâÈ°πÔºåÂõæË°®‰∏≠Ëã±Ê†áÈ¢òÁ¨¨‰∏ÄË°åÊó†Ê≥ïÊ≠£Á°ÆÂêåÊ≠•Áº©ËøõÔºå‰ªéËÄåÊîæÂºÉÁ¨¨‰∏ÄË°åÁöÑÁº©ËøõÔºâÔºåÂº∫Ë∞ÉÂ§öÂõæ‰∏≠Â≠êÂõæÊ†áÈ¢òÁöÑËßÑËåÉ‰ΩøÁî®ÔºåÈÄöËøáÊëòË¶ÅÂíåÁ¨¶Âè∑ÂàóË°®ÊºîÁ§∫Ê†áÈ¢ò‰∏çÂú®ÁõÆÂΩï‰∏≠ÊòæÁ§∫Âç¥‰ªçÂú®È°µÁúâ‰∏≠ÊòæÁ§∫„ÄÇÊ†πÊçÆ [ËµµÊ∞∏Êòé, email]ÔºåËÆæÁΩÆÂèåËØ≠ÂõæË°®Ê†áÈ¢òÂíå&lt;code&gt;bicaption&lt;/code&gt;‰∏çÂú®ÂõæÂΩ¢ÂàóË°®ÂíåË°®Ê†ºÂàóË°®‰∏≠ÊòæÁ§∫Ëã±ÊñáÊ†áÈ¢ò„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-03-21&lt;/code&gt; Ê†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/42"&gt;zhanglinbo, issue #42&lt;/a&gt; Ôºå‰ΩøÁî® &lt;a href="https://github.com/xiaoyao9933/UCASthesis"&gt;xiaoyao9933&lt;/a&gt; Âà∂‰ΩúÁöÑ&lt;code&gt;ucas_logo.pdf&lt;/code&gt;‰ΩøÂ≠¶Ê†°&lt;code&gt;logo&lt;/code&gt;ÊîæÂ§ß‰∏çÂ§±Áúü„ÄÇÊ†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/41"&gt;Starsky Wong, issue #41&lt;/a&gt; ÔºåËÆæÁΩÆÊ†áÈ¢òËã±ÊñáËÆæ‰∏∫&lt;code&gt;Times New Roman&lt;/code&gt;„ÄÇÊ†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/29"&gt;will0n, issue #29&lt;/a&gt; Ôºå&lt;a href="https://github.com/mohuangrui/ucasthesis/issues/26"&gt;Man-Ting-Fang, issue #26&lt;/a&gt; Ôºå&lt;a href="https://github.com/mohuangrui/ucasthesis/issues/12"&gt;diyiliaoya, issue #12&lt;/a&gt; ÔºåÂíå [ËµµÊ∞∏Êòé, email] ÔºåÁü´Ê≠£‰∏Ä‰∫õÊ†ºÂºèÁªÜËäÇÈóÆÈ¢ò„ÄÇÊ†πÊçÆ &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/30"&gt;tangjie1992, issue #30&lt;/a&gt; ÔºåÈÖçÁΩÆÁÆóÊ≥ïÁéØÂ¢É„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-02-04&lt;/code&gt; Âú® &lt;a href="https://github.com/CTeX-org/ctex-kit"&gt;ctex&lt;/a&gt; ÂºÄÂèëËÄÖÁöÑÂ∏ÆÂä©‰∏ã‰øÆÂ§çËØØÁî®Â≠ó‰ΩìÂëΩ‰ª§ÂØºËá¥ÁöÑÁ≤óÂÆã‰ΩìÂºÇÂ∏∏„ÄÇÁÑ∂ÂêéÔºåÂ∞ÜÊ®°ÊùøÂÖºÂÆπÊÄßËøõ‰∏ÄÊ≠•Êâ©Â±ï‰∏∫ÂÖºÂÆπÊìç‰ΩúÁ≥ªÁªü&lt;code&gt;Windows&lt;/code&gt;Ôºå&lt;code&gt;Linux&lt;/code&gt;Ôºå&lt;code&gt;MacOS&lt;/code&gt;Âíå&lt;code&gt;LaTeX &lt;/code&gt;ÁºñËØëÂºïÊìé&lt;code&gt;pdflatex&lt;/code&gt;Ôºå&lt;code&gt;xelatex&lt;/code&gt;Ôºå&lt;code&gt;lualatex&lt;/code&gt;„ÄÇÁßªÈô§&lt;code&gt;microtype&lt;/code&gt;ÂÆèÂåÖ‰ª•ÊèêÈ´òÁºñËØëÊïàÁéá„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-01-28&lt;/code&gt; Âü∫‰∫éÂõΩÁßëÂ§ß&lt;code&gt;2018&lt;/code&gt;Êñ∞ÁâàËÆ∫ÊñáËßÑËåÉËøõË°å‰∫ÜÈáçÂ§ß‰øÆÊîπÔºåÈááÁî®Êñ∞ÁöÑÂ∞ÅÈù¢„ÄÅÂ£∞Êòé„ÄÅÈ°µÁúâÈ°µËÑöÊ†∑Âºè„ÄÇÂ±ïÁ§∫Ê†áÈ¢ò‰∏≠‰ΩøÁî®Êï∞Â≠¶ÂÖ¨Âºè„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2017-05-14&lt;/code&gt; Ê†πÊçÆ [ËµµÊ∞∏Êòé, email] ÔºåÂ¢ûÂä†&lt;code&gt;\citepns{}&lt;/code&gt;Âíå&lt;code&gt;\citetns{}&lt;/code&gt;ÂëΩ‰ª§Êèê‰æõ‰∏äÊ†áÂºïÁî®‰∏ãÊ∑∑ÂêàÈùû‰∏äÊ†áÂºïÁî®ÁöÑÈúÄÊ±Ç„ÄÇÊ†πÊçÆ [ËáßÂÖâÊòé, email] ÔºåÊ∑ªÂä†ËÆæÂÆöËÆ∫Êñá‰∏∫&lt;code&gt;thesis&lt;/code&gt;Êàñ&lt;code&gt;dissertation&lt;/code&gt;ÁöÑÂëΩ‰ª§„ÄÇ&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>mohuangrui</author><guid isPermaLink="false">https://github.com/mohuangrui/ucasthesis</guid><pubDate>Wed, 29 Jan 2020 00:05:00 GMT</pubDate></item><item><title>exacity/deeplearningbook-chinese #6 in TeX, Today</title><link>https://github.com/exacity/deeplearningbook-chinese</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Book Chinese Translation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-‰∏≠ÊñáÁøªËØë" class="anchor" aria-hidden="true" href="#deep-learning-‰∏≠ÊñáÁøªËØë"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning ‰∏≠ÊñáÁøªËØë&lt;/h1&gt;
&lt;p&gt;Âú®‰ºóÂ§öÁΩëÂèãÁöÑÂ∏ÆÂä©ÂíåÊ†°ÂØπ‰∏ãÔºå‰∏≠ÊñáÁâàÁªà‰∫éÂá∫Áâà‰∫Ü„ÄÇÂ∞ΩÁÆ°ËøòÊúâÂæàÂ§öÈóÆÈ¢òÔºå‰ΩÜËá≥Â∞ë90%ÁöÑÂÜÖÂÆπÊòØÂèØËØªÁöÑÔºåÂπ∂‰∏îÊòØÂáÜÁ°ÆÁöÑ„ÄÇ
Êàë‰ª¨Â∞ΩÂèØËÉΩÂú∞‰øùÁïô‰∫ÜÂéü‰π¶&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;Deep Learning&lt;/a&gt;‰∏≠ÁöÑÊÑèÊÄùÂπ∂‰øùÁïôÂéü‰π¶ÁöÑËØ≠Âè•„ÄÇ&lt;/p&gt;
&lt;p&gt;ÁÑ∂ËÄåÊàë‰ª¨Ê∞¥Âπ≥ÊúâÈôêÔºåÊàë‰ª¨Êó†Ê≥ïÊ∂àÈô§‰ºóÂ§öËØªËÄÖÁöÑÊñπÂ∑Æ„ÄÇÊàë‰ª¨‰ªçÈúÄË¶ÅÂ§ßÂÆ∂ÁöÑÂª∫ËÆÆÂíåÂ∏ÆÂä©Ôºå‰∏ÄËµ∑ÂáèÂ∞èÁøªËØëÁöÑÂÅèÂ∑Æ„ÄÇ&lt;/p&gt;
&lt;p&gt;Â§ßÂÆ∂ÊâÄË¶ÅÂÅöÁöÑÂ∞±ÊòØÈòÖËØªÔºåÁÑ∂ÂêéÊ±áÊÄª‰Ω†ÁöÑÂª∫ËÆÆÔºåÊèêissueÔºàÊúÄÂ•Ω‰∏çË¶Å‰∏Ä‰∏™‰∏Ä‰∏™Âú∞ÊèêÔºâ„ÄÇÂ¶ÇÊûú‰Ω†Á°ÆÂÆö‰Ω†ÁöÑÂª∫ËÆÆ‰∏çÈúÄË¶ÅÂïÜÈáèÔºåÂèØ‰ª•Áõ¥Êé•ÂèëËµ∑PR„ÄÇ&lt;/p&gt;
&lt;p&gt;ÂØπÂ∫îÁöÑÁøªËØëËÄÖÔºö&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Á¨¨1„ÄÅ4„ÄÅ7„ÄÅ10„ÄÅ14„ÄÅ20Á´†ÂèäÁ¨¨12.4„ÄÅ12.5ËäÇÁî± @swordyork Ë¥üË¥£&lt;/li&gt;
&lt;li&gt;Á¨¨2„ÄÅ5„ÄÅ8„ÄÅ11„ÄÅ15„ÄÅ18Á´†Áî± @liber145 Ë¥üË¥£&lt;/li&gt;
&lt;li&gt;Á¨¨3„ÄÅ6„ÄÅ9Á´†Áî± @KevinLee1110 Ë¥üË¥£&lt;/li&gt;
&lt;li&gt;Á¨¨13„ÄÅ16„ÄÅ17„ÄÅ19Á´†ÂèäÁ¨¨12.1Ëá≥12.3ËäÇÁî± @futianfan Ë¥üË¥£&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-Èù¢ÂêëÁöÑËØªËÄÖ" class="anchor" aria-hidden="true" href="#Èù¢ÂêëÁöÑËØªËÄÖ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Èù¢ÂêëÁöÑËØªËÄÖ&lt;/h2&gt;
&lt;p&gt;ËØ∑Áõ¥Êé•‰∏ãËΩΩ&lt;a href="https://github.com/exacity/deeplearningbook-chinese/releases/download/v0.5-beta/dlbook_cn_v0.5-beta.pdf"&gt;PDF&lt;/a&gt;ÈòÖËØª„ÄÇ
‰∏çÊâìÁÆóÊèê‰æõEPUBÁ≠âÊ†ºÂºèÔºåÂ¶ÇÊúâÈúÄË¶ÅËØ∑Ëá™Ë°å‰øÆÊîπ„ÄÇ&lt;/p&gt;
&lt;p&gt;Ëøô‰∏ÄÁâàÂáÜÁ°ÆÊÄßÂ∑≤ÁªèÊúâÊâÄÊèêÈ´òÔºåËØªËÄÖÂèØ‰ª•‰ª•‰∏≠ÊñáÁâà‰∏∫‰∏ª„ÄÅËã±ÊñáÁâà‰∏∫ËæÖÊù•ÈòÖËØªÂ≠¶‰π†Ôºå‰ΩÜÊàë‰ª¨‰ªçÂª∫ËÆÆÁ†îÁ©∂ËÄÖÈòÖËØª&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;ÂéüÁâà&lt;/a&gt;„ÄÇ&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-Âá∫ÁâàÂèäÂºÄÊ∫êÂéüÂõ†" class="anchor" aria-hidden="true" href="#Âá∫ÁâàÂèäÂºÄÊ∫êÂéüÂõ†"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Âá∫ÁâàÂèäÂºÄÊ∫êÂéüÂõ†&lt;/h2&gt;
&lt;p&gt;Êú¨‰π¶Áî±‰∫∫Ê∞ëÈÇÆÁîµÂá∫ÁâàÁ§æÂá∫ÁâàÔºåÂ¶ÇÊûú‰Ω†ËßâÂæó‰∏≠ÊñáÁâàPDFÂØπ‰Ω†ÊúâÊâÄÂ∏ÆÂä©ÔºåÂ∏åÊúõ‰Ω†ËÉΩÊîØÊåÅ‰∏ãÁ∫∏Ë¥®Ê≠£Áâà‰π¶Á±ç„ÄÇ
Â¶ÇÊûú‰Ω†ËßâÂæó‰∏≠ÊñáÁâà‰∏çË°åÔºåÂ∏åÊúõ‰Ω†ËÉΩÂ§öÊèêÂª∫ËÆÆ„ÄÇÈùûÂ∏∏ÊÑüË∞¢ÂêÑ‰ΩçÔºÅ
Á∫∏Ë¥®Áâà‰πü‰ºöËøõ‰∏ÄÊ≠•Êõ¥Êñ∞ÔºåÈúÄË¶ÅÂ§ßÂÆ∂Êõ¥Â§öÁöÑÂª∫ËÆÆÂíåÊÑèËßÅÔºå‰∏ÄËµ∑ÂÆåÂñÑ‰∏≠ÊñáÁâà„ÄÇ&lt;/p&gt;
&lt;p&gt;Á∫∏Ë¥®ÁâàÁõÆÂâçÂú®‰∫∫Ê∞ëÈÇÆÁîµÂá∫ÁâàÁ§æÁöÑÂºÇÊ≠•Á§æÂå∫Âá∫ÂîÆÔºåËßÅ&lt;a href="http://www.epubit.com.cn/book/details/4278" rel="nofollow"&gt;Âú∞ÂùÄ&lt;/a&gt;„ÄÇ
‰ª∑Ê†º‰∏ç‰ΩéÔºå‰ΩÜÁúã‰∫ÜÊ†∑Êú¨‰πãÂêéÔºåÊàë‰ª¨ËÆ§‰∏∫Áâ©ÊúâÊâÄÂÄº„ÄÇ
Ê≥®ÊÑèÔºåÊàë‰ª¨‰∏ç‰ºöÈÄöËøáÂ™í‰ΩìËøõË°åÂÆ£‰º†ÔºåÂ∏åÊúõÂ§ßÂÆ∂ÂÖàÁúãÁîµÂ≠êÁâàÂÜÖÂÆπÔºåÂÜçÂà§Êñ≠ÊòØÂê¶Ë¥≠‰π∞Á∫∏Ë¥®Áâà„ÄÇ&lt;/p&gt;
&lt;p&gt;‰ª•‰∏ãÊòØÂºÄÊ∫êÁöÑÂÖ∑‰ΩìÂéüÂõ†Ôºö&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Êàë‰ª¨‰∏çÊòØÊñáÂ≠¶Â∑•‰ΩúËÄÖÔºå‰∏ç‰∏ìËÅåÁøªËØë„ÄÇÂçïÈù†Êàë‰ª¨ÔºåÊó†Ê≥ïÁªôÂá∫‰ªäÂ§©ÁöÑÁøªËØëÔºå‰ºóÂ§öÁΩëÂèãÈÉΩÁªôÊàë‰ª¨ÊèêÂá∫‰∫ÜÂÆùË¥µÁöÑÂª∫ËÆÆÔºåÂõ†Ê≠§ÂºÄÊ∫êÂ∏Æ‰∫ÜÂæàÂ§ßÁöÑÂøô„ÄÇÂá∫ÁâàÁ§æ‰ºöÁªôÊàë‰ª¨Á®øË¥πÔºàÊàë‰ª¨‰πü‰∏çÁü•ÈÅìÂ§öÂ∞ëÔºåÂèØËÉΩ2‰∏áÂ∑¶Âè≥ÔºâÔºåÊàë‰ª¨‰πü‰∏çÂ•ΩÊÑèÊÄùËá™Â∑±Áî®ÔºåÂïÜÈáè‰πãÂêéËßâÂæóÊçêÂá∫ÊòØÊúÄÂêàÈÄÇÁöÑÔºå‰ª•ÊâÄÊúâË¥°ÁåÆËøáÁöÑÁΩëÂèãÁöÑÂêç‰πâÔºàÊàë‰ª¨ÊääÁ®øË¥πÊçêÁªô‰∫ÜÊùâÊ†ëÂÖ¨ÁõäÔºåÁî®‰∫é4ÂêçË¥µÂ∑ûÈ´ò‰∏≠Áîü‰∏âÂπ¥ÁöÑÁîüÊ¥ªË¥πÔºåËßÅ&lt;a href="https://github.com/exacity/deeplearningbook-chinese/blob/master/donation.pdf"&gt;ÊçêËµ†ÊÉÖÂÜµ&lt;/a&gt;Ôºâ„ÄÇ&lt;/li&gt;
&lt;li&gt;PDFÁîµÂ≠êÁâàÂØπ‰∫éÊäÄÊúØÁ±ª‰π¶Á±çÊù•ËØ¥ÊòØÂæàÈáçË¶ÅÁöÑÔºåÈöèÊó∂ÈúÄË¶ÅÊü•ËØ¢ÔºåÊãøÁùÄÁ∫∏Ë¥®ÁâàÂà∞Â§ÑËµ∞ÊòæÁÑ∂‰∏çÂêàÈÄÇ„ÄÇÂõΩÂ§ñÂæàÂ§öÊäÄÊúØ‰π¶Á±çÈÉΩÊúâÂØπÂ∫îÁöÑÁîµÂ≠êÁâàÔºàËôΩÁÑ∂‰∏ç‰∏ÄÂÆöÊòØÊ≠£ÁâàÔºâÔºåËÄåÂõΩÂÜÖÁöÑÂá†‰πéÊ≤°Êúâ„ÄÇ‰∏™‰∫∫ËÆ§‰∏∫ËøôÊòØÂá∫ÁâàÁ§æÊàñËÄÖ‰ΩúËÄÖËÆ§‰∏∫ÂõΩÊ∞ëÁ¥†Ë¥®ËøòÊ≤°ÊúâÈ´òÂà∞‰∏ªÂä®‰∏∫Áü•ËØÜ‰ªòË¥πÁöÑÂ¢ÉÁïåÔºåÊâÄ‰ª•‰∏çÊÑøÊÑè"Ê≥ÑÈú≤"ÁîµÂ≠êÁâà„ÄÇÊó∂‰ª£Âú®ËøõÊ≠•ÔºåÊàë‰ª¨‰πüÈúÄË¶ÅÊîπÂèò„ÄÇÁâπÂà´ÊòØÁøªËØë‰ΩúÂìÅÊôÆÈÅçË¥®Èáè‰∏çÈ´òÁöÑÊÉÖÂÜµ‰∏ãÔºåË¶ÅÊï¢‰∏∫Â§©‰∏ãÂÖà„ÄÇ&lt;/li&gt;
&lt;li&gt;Ê∑±Â∫¶Â≠¶‰π†ÂèëÂ±ïÂ§™Âø´ÔºåÊó•Êñ∞ÊúàÂºÇÔºåÊâÄ‰ª•Êàë‰ª¨Â∏åÊúõÂ§ßÂÆ∂Êõ¥Êó©Âú∞Â≠¶Âà∞Áõ∏ÂÖ≥ÁöÑÁü•ËØÜ„ÄÇÊàëËßâÂæóÂéü‰ΩúËÄÖÂºÄÊîæPDFÁîµÂ≠êÁâà‰πüÊúâÁ±ª‰ººÁöÑËÄÉËôëÔºå‰πüÂ∞±ÊòØÂÖàÈòÖËØªÂêé‰ªòË¥π„ÄÇÊàë‰ª¨ËÆ§‰∏∫‰∏≠ÂõΩ‰∫∫Âè£Á¥†Ë¥®Â∑≤ÁªèË∂≥Â§üÈ´òÔºåÊáÇÂæó‰∏∫Áü•ËØÜ‰ªòË¥π„ÄÇÂΩìÁÑ∂Ëøô‰∏çÊòØ‰ªòÁªôÊàë‰ª¨ÁöÑÔºåÊòØ‰ªòÁªôÂá∫ÁâàÁ§æÁöÑÔºåÂá∫ÁâàÁ§æÂÜç‰ªòÁªôÂéü‰ΩúËÄÖ„ÄÇÊàë‰ª¨‰∏çÂ∏åÊúõ‰∏≠ÊñáÁâàÁöÑÈîÄÈáèÂõ†PDFÁîµÂ≠êÁâàÁöÑÂ≠òÂú®ËÄå‰∏ãÊªë„ÄÇÂá∫ÁâàÁ§æÂè™ÊúâÂÄºÂõû‰∫ÜÁâàÊùÉÊâçËÉΩÂú®‰ª•ÂêéÂºïËøõÊõ¥Â§öÁöÑ‰ºòÁßÄ‰π¶Á±ç„ÄÇÊàë‰ª¨Ëøô‰∏™ÂºÄÊ∫êÁøªËØëÂÖà‰æã‰πü‰∏ç‰ºöÊàê‰∏∫‰∏Ä‰∏™ÂèçÈù¢Ê°à‰æãÔºå‰ª•ÂêéÊâç‰ºöÊúâÊõ¥Â§öÁöÑPDFÁîµÂ≠êÁâà„ÄÇ&lt;/li&gt;
&lt;li&gt;ÂºÄÊ∫ê‰πüÊ∂âÂèäÁâàÊùÉÈóÆÈ¢òÔºåÂá∫‰∫éÁâàÊùÉÂéüÂõ†ÔºåÊàë‰ª¨‰∏çÂÜçÊõ¥Êñ∞Ê≠§ÂàùÁâàPDFÊñá‰ª∂ÔºåËØ∑Â§ßÂÆ∂‰ª•ÊúÄÁªàÁöÑÁ∫∏Ë¥®Áâà‰∏∫ÂáÜ„ÄÇÔºà‰ΩÜÊ∫êÁ†Å‰ºö‰∏ÄÁõ¥Êõ¥Êñ∞Ôºâ&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-Ëá¥Ë∞¢" class="anchor" aria-hidden="true" href="#Ëá¥Ë∞¢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ëá¥Ë∞¢&lt;/h2&gt;
&lt;p&gt;Êàë‰ª¨Êúâ3‰∏™Á±ªÂà´ÁöÑÊ†°ÂØπ‰∫∫Âëò„ÄÇ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ë¥üË¥£‰∫∫‰πüÂ∞±ÊòØÂØπÂ∫îÁöÑÁøªËØëËÄÖ„ÄÇ&lt;/li&gt;
&lt;li&gt;ÁÆÄÂçïÈòÖËØªÔºåÂØπËØ≠Âè•‰∏çÈÄöÈ°∫ÊàñÈöæ‰ª•ÁêÜËß£ÁöÑÂú∞ÊñπÊèêÂá∫‰øÆÊîπÊÑèËßÅ„ÄÇ&lt;/li&gt;
&lt;li&gt;‰∏≠Ëã±ÂØπÊØîÔºåËøõË°å‰∏≠Ëã±ÂØπÂ∫îÈòÖËØªÔºåÊéíÈô§Â∞ëÁøªÈîôÁøªÁöÑÊÉÖÂÜµ„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ÊâÄÊúâÊ†°ÂØπÂª∫ËÆÆÈÉΩ‰øùÂ≠òÂú®ÂêÑÁ´†ÁöÑ&lt;code&gt;annotations.txt&lt;/code&gt;Êñá‰ª∂‰∏≠„ÄÇ&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Á´†ËäÇ&lt;/th&gt;
&lt;th&gt;Ë¥üË¥£‰∫∫&lt;/th&gt;
&lt;th&gt;ÁÆÄÂçïÈòÖËØª&lt;/th&gt;
&lt;th&gt;‰∏≠Ëã±ÂØπÊØî&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter1_introduction/" rel="nofollow"&gt;Á¨¨‰∏ÄÁ´† ÂâçË®Ä&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc, @SiriusXDJ, @corenel, @NeutronT&lt;/td&gt;
&lt;td&gt;@linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter2_linear_algebra/" rel="nofollow"&gt;Á¨¨‰∫åÁ´† Á∫øÊÄß‰ª£Êï∞&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@SiriusXDJ, @angrymidiao&lt;/td&gt;
&lt;td&gt;@badpoem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter3_probability_and_information_theory/" rel="nofollow"&gt;Á¨¨‰∏âÁ´† Ê¶ÇÁéá‰∏é‰ø°ÊÅØËÆ∫&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@SiriusXDJ&lt;/td&gt;
&lt;td&gt;@kkpoker, @Peiyan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter4_numerical_computation/" rel="nofollow"&gt;Á¨¨ÂõõÁ´† Êï∞ÂÄºËÆ°ÁÆó&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;@zhangyafeikimi&lt;/td&gt;
&lt;td&gt;@hengqujushi&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter5_machine_learning_basics/" rel="nofollow"&gt;Á¨¨‰∫îÁ´† Êú∫Âô®Â≠¶‰π†Âü∫Á°Ä&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@wheaio, @huangpingchun&lt;/td&gt;
&lt;td&gt;@fairmiracle, @linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter6_deep_feedforward_networks/" rel="nofollow"&gt;Á¨¨ÂÖ≠Á´† Ê∑±Â∫¶ÂâçÈ¶àÁΩëÁªú&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;David_Chow, @linzhp, @sailordiary&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter7_regularization/" rel="nofollow"&gt;Á¨¨‰∏ÉÁ´† Ê∑±Â∫¶Â≠¶‰π†‰∏≠ÁöÑÊ≠£ÂàôÂåñ&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@NBZCC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/" rel="nofollow"&gt;Á¨¨ÂÖ´Á´† Ê∑±Â∫¶Ê®°Âûã‰∏≠ÁöÑ‰ºòÂåñ&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@happynoom, @codeVerySlow&lt;/td&gt;
&lt;td&gt;@huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter9_convolutional_networks/" rel="nofollow"&gt;Á¨¨‰πùÁ´† Âç∑ÁßØÁΩëÁªú&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @corenel&lt;/td&gt;
&lt;td&gt;@zhiding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter10_sequence_modeling_rnn/" rel="nofollow"&gt;Á¨¨ÂçÅÁ´† Â∫èÂàóÂª∫Ê®°ÔºöÂæ™ÁéØÂíåÈÄíÂΩíÁΩëÁªú&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @yinruiqing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter11_practical_methodology/" rel="nofollow"&gt;Á¨¨ÂçÅ‰∏ÄÁ´† ÂÆûË∑µÊñπÊ≥ïËÆ∫&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter12_applications/" rel="nofollow"&gt;Á¨¨ÂçÅ‰∫åÁ´† Â∫îÁî®&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork, @futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@corenel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter13_linear_factor_models/" rel="nofollow"&gt;Á¨¨ÂçÅ‰∏âÁ´† Á∫øÊÄßÂõ†Â≠êÊ®°Âûã&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;@cloudygoose&lt;/td&gt;
&lt;td&gt;@ZhiweiYang&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter14_autoencoders/" rel="nofollow"&gt;Á¨¨ÂçÅÂõõÁ´† Ëá™ÁºñÁ†ÅÂô®&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@Seaball, @huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter15_representation_learning/" rel="nofollow"&gt;Á¨¨ÂçÅ‰∫îÁ´† Ë°®Á§∫Â≠¶‰π†&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@cnscottzheng&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter16_structured_probabilistic_modelling/" rel="nofollow"&gt;Á¨¨ÂçÅÂÖ≠Á´† Ê∑±Â∫¶Â≠¶‰π†‰∏≠ÁöÑÁªìÊûÑÂåñÊ¶ÇÁéáÊ®°Âûã&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter17_monte_carlo_methods/" rel="nofollow"&gt;Á¨¨ÂçÅ‰∏ÉÁ´† ËíôÁâπÂç°ÁΩóÊñπÊ≥ï&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter18_confronting_the_partition_function/" rel="nofollow"&gt;Á¨¨ÂçÅÂÖ´Á´† Èù¢ÂØπÈÖçÂàÜÂáΩÊï∞&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@tankeco&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter19_approximate_inference/" rel="nofollow"&gt;Á¨¨ÂçÅ‰πùÁ´† Ëøë‰ººÊé®Êñ≠&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary, @hengqujushi, huanghaojun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter20_deep_generative_models/" rel="nofollow"&gt;Á¨¨‰∫åÂçÅÁ´† Ê∑±Â∫¶ÁîüÊàêÊ®°Âûã&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ÂèÇËÄÉÊñáÁåÆ&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@pkuwwt&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Êàë‰ª¨‰ºöÂú®Á∫∏Ë¥®ÁâàÊ≠£ÂºèÂá∫ÁâàÁöÑÊó∂ÂÄôÔºåÂú®‰π¶‰∏≠Ëá¥Ë∞¢ÔºåÊ≠£ÂºèÊÑüË∞¢ÂêÑ‰Ωç‰ΩúÂá∫Ë¥°ÁåÆÁöÑÂêåÂ≠¶ÔºÅ&lt;/p&gt;
&lt;p&gt;ËøòÊúâÂæàÂ§öÂêåÂ≠¶ÊèêÂá∫‰∫Ü‰∏çÂ∞ëÂª∫ËÆÆÔºåÊàë‰ª¨ÈÉΩÂàóÂú®Ê≠§Â§Ñ„ÄÇ&lt;/p&gt;
&lt;p&gt;@tttwwy @tankeco @fairmiracle @GageGao @huangpingchun @MaHongP @acgtyrant @yanhuibin315 @Buttonwood @titicacafz
@weijy026a @RuiZhang1993 @zymiboxpay @xingkongliang @oisc @tielei @yuduowu @Qingmu @HC-2016 @xiaomingabc
@bengordai @Bojian @JoyFYan @minoriwww @khty2000 @gump88 @zdx3578 @PassStory @imwebson @wlbksy @roachsinai @Elvinczp
@endymecy name:YUE-DaJiong @9578577 @linzhp @cnscottzheng @germany-zhu  @zhangyafeikimi @showgood163 @gump88
@kangqf @NeutronT @badpoem @kkpoker @Seaball @wheaio @angrymidiao @ZhiweiYang @corenel @zhaoyu611 @SiriusXDJ @dfcv24 EmisXXY
FlyingFire vsooda @friskit-china @poerin @ninesunqian @JiaqiYao @Sofring @wenlei @wizyoung @imageslr @@indam @XuLYC
@zhouqingping @freedomRen @runPenguin @pkuwwt @wuqi @tjliupeng @neo0801 @jt827859032 @demolpc @fishInAPool
@xiaolangyuxin @jzj1993 @whatbeg LongXiaJun jzd&lt;/p&gt;
&lt;p&gt;Â¶ÇÊúâÈÅóÊºèÔºåËØ∑Âä°ÂøÖÈÄöÁü•Êàë‰ª¨ÔºåÂèØ‰ª•ÂèëÈÇÆ‰ª∂Ëá≥&lt;code&gt;echo c3dvcmQueW9ya0BnbWFpbC5jb20K | base64 --decode&lt;/code&gt;„ÄÇ
ËøôÊòØÊàë‰ª¨ÂøÖÈ°ªË¶ÅÊÑüË∞¢ÁöÑÔºåÊâÄ‰ª•‰∏çË¶Å‰∏çÂ•ΩÊÑèÊÄù„ÄÇ&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;ÊéíÁâà&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-Ê≥®ÊÑè" class="anchor" aria-hidden="true" href="#Ê≥®ÊÑè"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ê≥®ÊÑè&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ÂêÑÁßçÈóÆÈ¢òÊàñËÄÖÂª∫ËÆÆÂèØ‰ª•ÊèêissueÔºåÂª∫ËÆÆ‰ΩøÁî®‰∏≠Êñá„ÄÇ&lt;/li&gt;
&lt;li&gt;Áî±‰∫éÁâàÊùÉÈóÆÈ¢òÔºåÊàë‰ª¨‰∏çËÉΩÂ∞ÜÂõæÁâáÂíåbib‰∏ä‰º†ÔºåËØ∑ËßÅË∞Ö„ÄÇ&lt;/li&gt;
&lt;li&gt;Due to copyright issues, we would not upload figures and the bib file.&lt;/li&gt;
&lt;li&gt;ÂèØÁî®‰∫éÂ≠¶‰π†Á†îÁ©∂ÁõÆÁöÑÔºå‰∏çÂæóÁî®‰∫é‰ªª‰ΩïÂïÜ‰∏öË°å‰∏∫„ÄÇË∞¢Ë∞¢ÔºÅ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-markdownÊ†ºÂºè" class="anchor" aria-hidden="true" href="#markdownÊ†ºÂºè"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MarkdownÊ†ºÂºè&lt;/h2&gt;
&lt;p&gt;ËøôÁßçÊ†ºÂºèÁ°ÆÂÆûÊØîËæÉÈáçË¶ÅÔºåÊñπ‰æøÊü•ÈòÖÔºå‰πüÊñπ‰æøÁ¥¢Âºï„ÄÇÂàùÊ≠•ËΩ¨Êç¢ÂêéÔºåÁîüÊàêÁΩëÈ°µÔºåÂÖ∑‰ΩìËßÅ&lt;a href="https://exacity.github.io/deeplearningbook-chinese" rel="nofollow"&gt;deeplearningbook-chinese&lt;/a&gt;„ÄÇ
Ê≥®ÊÑèÔºåËøôÁßçËΩ¨Êç¢Ê≤°ÊúâÊääÂõæÊîæËøõÂéªÔºå‰πü‰∏ç‰ºöÊîæÂõæ„ÄÇÁõÆÂâç‰ΩøÁî®Âçï‰∏™&lt;a href="scripts/convert2md.sh"&gt;ËÑöÊú¨&lt;/a&gt;ÔºåÂü∫‰∫élatexÊñá‰ª∂ËΩ¨Êç¢Ôºå‰ª•ÂêéÂèØËÉΩ‰ºöÊõ¥Êîπ‰ΩÜÂéüÂàôÊòØ‰∏çÁõ¥Êé•‰øÆÊîπ&lt;a href="docs/_posts"&gt;mdÊñá‰ª∂&lt;/a&gt;„ÄÇ
ÈúÄË¶ÅÁöÑÂêåÂ≠¶ÂèØ‰ª•Ëá™Ë°å‰øÆÊîπ&lt;a href="scripts/convert2md.sh"&gt;ËÑöÊú¨&lt;/a&gt;„ÄÇ&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-htmlÊ†ºÂºè" class="anchor" aria-hidden="true" href="#htmlÊ†ºÂºè"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTMLÊ†ºÂºè&lt;/h2&gt;
&lt;p&gt;ËØªËÄÖÂèØ‰ª•‰ΩøÁî®&lt;a href="https://github.com/coolwanglu/pdf2htmlEX"&gt;pdf2htmlEX&lt;/a&gt;ËøõË°åËΩ¨Êç¢ÔºåÁõ¥Êé•Â∞ÜPDFËΩ¨Êç¢‰∏∫HTML„ÄÇ&lt;/p&gt;
&lt;p&gt;Updating.....&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>exacity</author><guid isPermaLink="false">https://github.com/exacity/deeplearningbook-chinese</guid><pubDate>Wed, 29 Jan 2020 00:06:00 GMT</pubDate></item><item><title>tuhdo/os01 #7 in TeX, Today</title><link>https://github.com/tuhdo/os01</link><description>&lt;p&gt;&lt;i&gt;Bootstrap yourself to write an OS from scratch. A book for self-learner.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://www.paypal.com/cgi-bin/webscr?cmd=_donations&amp;amp;business=tuhdo1710%40gmail%2ecom&amp;amp;lc=VN&amp;amp;item_number=tuhdo&amp;amp;currency_code=USD&amp;amp;bn=PP%2dDonationsBF%3aDonate%2dPayPal%2dgreen%2esvg%3aNonHosted" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d5d24e33e2f4b6fe53987419a21b203c03789a8f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d677265656e2e737667" alt="Donate" data-canonical-src="https://img.shields.io/badge/Donate-PayPal-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-operating-systems-from-0-to-1" class="anchor" aria-hidden="true" href="#operating-systems-from-0-to-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://tuhdo.github.io/os01/" rel="nofollow"&gt;Operating Systems: From 0 to 1&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;This book helps you gain the foundational knowledge required to write an
operating system from scratch. Hence the title, 0 to 1.&lt;/p&gt;
&lt;p&gt;After completing this book, at the very least you will learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;How to write an operating system from scratch by reading hardware datasheets.
In the real world, it works like that. You won't be able to consult Google for
a quick answer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A big picture of how each layer of a computer is related to the other, from hardware to software.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Write code independently. It's pointless to copy and paste code. Real learning
happens when you solve problems on your own. Some examples are given to kick
start, but most problems are yours to conquer. However, the solutions are
available online for you to examine after giving it a good try.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Linux as a development environment and how to use common tools for low-level
programming.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;x86 assembly in-depth.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How a program is structured so that an operating system can run.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to debug a program running directly on hardware with gdb and QEMU.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Linking and loading on bare metal x86_64, with pure C. No standard library. No
runtime overhead.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/tuhdo/os01/blob/master/Operating_Systems_From_0_to_1.pdf"&gt;Download the book&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-the-pedagogy-of-the-book" class="anchor" aria-hidden="true" href="#the-pedagogy-of-the-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The pedagogy of the book&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;You give a poor man a fish and you feed him for a day. You teach him to fish
and you give him an occupation that will feed him for a lifetime.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This has been the guiding principle of the book when I was writing it. The book does
not try to teach you everything, but enough to enable you to learn by yourself.
The book itself, at this point, is quite "complete": once you master part 1 and
part 2 (which consist of 8 chapters), you can drop the book and learn by
yourself. At this point, smart readers should be able to continue on their own.
For example, they can continue their journeys
on &lt;a href="http://wiki.osdev.org/Main_Page" rel="nofollow"&gt;OSDev wiki&lt;/a&gt;; in fact, after you study
everything in part 1 and part 2, you only meet
the &lt;a href="http://wiki.osdev.org/Required_Knowledge" rel="nofollow"&gt;minimum requirement&lt;/a&gt; by OSDev
Wiki (well, not quite, the book actually goes deeper for the suggested topics).
Or, if you consider developing an OS for fun is impractical, you can continue
with a Linux-specific book, such as this free
book &lt;a href="https://0xax.gitbooks.io/linux-insides/content/" rel="nofollow"&gt;Linux Insides&lt;/a&gt;, or other
popular Linux kernel books. The book tries hard to provide you a strong
foundation, and that's why part 1 and part 2 were released first.&lt;/p&gt;
&lt;p&gt;The book teaches you core concepts, such as x86 Assembly, ELF, linking and
debugging on bare metal, etc., but more importantly, where such information
come from. For example, instead of just teaching x86 Assembly, it also teaches
how to use reference manuals from Intel. Learning to read the official
manuals is important because only the hardware manufacturers themselves
understand how their hardware work. If you only learn from the secondary
resources because it is easier, you will never gain a complete understanding of
the hardware you are programming for. Have you ever read a book on Assembly, and
wondered where all the information came from? How does the author know
everything he says is correct? And how one seems to magically know so much about
hardware programming? This book gives pointers to such questions.&lt;/p&gt;
&lt;p&gt;As an example, you should skim through chapter 4, "x86 Assembly and C", to see
how it makes use of the Intel manual, Volume 2. And in
the process, it guides you how to use the official manuals.&lt;/p&gt;
&lt;p&gt;Part 3 is planned as a series of specifications that a reader will implement to
complete each operating system component. It does not contain code aside from a
few examples. Part 3 is just there to shorten the reader's time when reading the
official manuals by giving hints where to read, explaining difficult concepts
and how to use the manuals to debug. In short, the implementation is up to the
reader to work on his or her own; the chapters are just like university assignments.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h1&gt;
&lt;p&gt;Know some circuit concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basic Concepts of Electricity: atoms, electrons, protons, neutrons, current flow.&lt;/li&gt;
&lt;li&gt;Ohm's law&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, if you know absolutely nothing about electricity, you can quickly learn it here:
&lt;a href="http://www.allaboutcircuits.com/textbook/" rel="nofollow"&gt;http://www.allaboutcircuits.com/textbook/&lt;/a&gt;, by reading chapter 1 and chapter 2.&lt;/p&gt;
&lt;p&gt;C programming. In particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Variable and function declarations/definitions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;While and for loops&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pointers and function pointers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fundamental algorithms and data structures in C&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Linux basics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Know how to navigate directory with the command line&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Know how to invoke a command with options&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Know how to pipe output to another program&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Touch typing. Since we are going to use Linux, touch typing helps. I know typing
speed does not relate to problem-solving, but at least your typing speed should
be fast enough not to let it get it the way and degrade the learning experience.&lt;/p&gt;
&lt;p&gt;In general, I assume that the reader has basic C programming knowledge, and can
use an IDE to build and run a program.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-status" class="anchor" aria-hidden="true" href="#status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Status:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Part 1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 1: Complete&lt;/li&gt;
&lt;li&gt;Chapter 2: Complete&lt;/li&gt;
&lt;li&gt;Chapter 3: Almost. Currently, the book relies on the Intel Manual for fully explaining x86 execution environment.&lt;/li&gt;
&lt;li&gt;Chapter 4: Complete&lt;/li&gt;
&lt;li&gt;Chapter 5: Complete&lt;/li&gt;
&lt;li&gt;Chapter 6: Complete&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Part 2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 7: Complete&lt;/li&gt;
&lt;li&gt;Chapter 8: Complete&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Part 3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 9: Incomplete&lt;/li&gt;
&lt;li&gt;Chapter 10: Incomplete&lt;/li&gt;
&lt;li&gt;Chapter 11: Incomplete&lt;/li&gt;
&lt;li&gt;Chapter 12: Incomplete&lt;/li&gt;
&lt;li&gt;Chapter 13: Incomplete&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;... and future chapters not included yet ...&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the future, I hope to expand part 3 to cover more than the first 2 parts. But
for the time being, I will try to finish the above chapters first.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-sample-os" class="anchor" aria-hidden="true" href="#sample-os"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample OS&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/tuhdo/sample-os"&gt;This repository&lt;/a&gt; is the sample OS of the
book that is intended as a reference material for part 3. It covers 10 chapters
of the "System Programming Guide" (Intel Manual Volume 3), along with a simple
keyboard and video driver for input and output. However, at the moment, only the
following features are implemented:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Protected mode.&lt;/li&gt;
&lt;li&gt;Creating and managing processes with TSS (Task State Structure).&lt;/li&gt;
&lt;li&gt;Interrupts&lt;/li&gt;
&lt;li&gt;LAPIC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Paging and I/O are not yet implemented. I will try to implement it as the book progresses.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h1&gt;
&lt;p&gt;If you find any grammatical issues, please report it using Github Issues. Or, if
some sentence or paragraph is difficult to understand, feel free to open an
issue with the following title format: &lt;code&gt;[page number][type] Descriptive Title&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example: &lt;code&gt;[pg.9][grammar] Incorrect verb usage&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;type&lt;/code&gt; can be one of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Typo&lt;/code&gt;: indicates typing mistake.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Grammar&lt;/code&gt;: indicates incorrect grammar usage.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Style&lt;/code&gt;: indicates a style improvement.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Content&lt;/code&gt;: indicates problems with the content.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even better, you can make a pull request with the provided book source. The main
content of the book is in the file "Operating Systems: From 0 to 1.lyx". You can
edit the .txt file, then I will integrate the changes manually. It is a
workaround for now since Lyx can cause a huge diff which makes it impossible to
review changes.&lt;/p&gt;
&lt;p&gt;The book is in development, so please bear with me if the English irritates you.
I really appreciate it.&lt;/p&gt;
&lt;p&gt;Finally, if you like the project and if it is possible, please donate to help
this project and keep it going.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-got-questions" class="anchor" aria-hidden="true" href="#got-questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Got questions?&lt;/h1&gt;
&lt;p&gt;If you have any question related to the material or the development of the book,
feel free to &lt;a href="https://github.com/tuhdo/os01/issues/new"&gt;open a Github issue&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tuhdo</author><guid isPermaLink="false">https://github.com/tuhdo/os01</guid><pubDate>Wed, 29 Jan 2020 00:07:00 GMT</pubDate></item><item><title>terryum/awesome-deep-learning-papers #8 in TeX, Today</title><link>https://github.com/terryum/awesome-deep-learning-papers</link><description>&lt;p&gt;&lt;i&gt;The most cited deep learning papers&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-awesome---most-cited-deep-learning-papers" class="anchor" aria-hidden="true" href="#awesome---most-cited-deep-learning-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome - Most Cited Deep Learning Papers&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://camo.githubusercontent.com/13c4e50d88df7178ae1882a203ed57b641674f94/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667" alt="Awesome" data-canonical-src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[Notice] This list is not being maintained anymore because of the overwhelming amount of deep learning papers published every day since 2017.&lt;/p&gt;
&lt;p&gt;A curated list of the most cited deep learning papers (2012-2016)&lt;/p&gt;
&lt;p&gt;We believe that there exist &lt;em&gt;classic&lt;/em&gt; deep learning papers which are worth reading regardless of their application domain. Rather than providing overwhelming amount of papers, We would like to provide a &lt;em&gt;curated list&lt;/em&gt; of the awesome deep learning papers which are considered as &lt;em&gt;must-reads&lt;/em&gt; in certain research domains.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-background" class="anchor" aria-hidden="true" href="#background"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Background&lt;/h2&gt;
&lt;p&gt;Before this list, there exist other &lt;em&gt;awesome deep learning lists&lt;/em&gt;, for example, &lt;a href="https://github.com/kjw0612/awesome-deep-vision"&gt;Deep Vision&lt;/a&gt; and &lt;a href="https://github.com/kjw0612/awesome-rnn"&gt;Awesome Recurrent Neural Networks&lt;/a&gt;. Also, after this list comes out, another awesome list for deep learning beginners, called &lt;a href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap"&gt;Deep Learning Papers Reading Roadmap&lt;/a&gt;, has been created and loved by many deep learning researchers.&lt;/p&gt;
&lt;p&gt;Although the &lt;em&gt;Roadmap List&lt;/em&gt; includes lots of important deep learning papers, it feels overwhelming for me to read them all. As I mentioned in the introduction, I believe that seminal works can give us lessons regardless of their application domain. Thus, I would like to introduce &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; here as a good starting point of overviewing deep learning researches.&lt;/p&gt;
&lt;p&gt;To get the news for newly released papers everyday, follow my &lt;a href="https://twitter.com/TerryUm_ML" rel="nofollow"&gt;twitter&lt;/a&gt; or &lt;a href="https://www.facebook.com/terryum.io/" rel="nofollow"&gt;facebook page&lt;/a&gt;!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-awesome-list-criteria" class="anchor" aria-hidden="true" href="#awesome-list-criteria"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome list criteria&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A list of &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; published from 2012 to 2016 is suggested.&lt;/li&gt;
&lt;li&gt;If a paper is added to the list, another paper (usually from *More Papers from 2016" section) should be removed to keep top 100 papers. (Thus, removing papers is also important contributions as well as adding papers)&lt;/li&gt;
&lt;li&gt;Papers that are important, but failed to be included in the list, will be listed in &lt;em&gt;More than Top 100&lt;/em&gt; section.&lt;/li&gt;
&lt;li&gt;Please refer to &lt;em&gt;New Papers&lt;/em&gt; and &lt;em&gt;Old Papers&lt;/em&gt; sections for the papers published in recent 6 months or before 2012.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;(Citation criteria)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;lt; 6 months&lt;/strong&gt; : &lt;em&gt;New Papers&lt;/em&gt; (by discussion)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2016&lt;/strong&gt; :  +60 citations or "More Papers from 2016"&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2015&lt;/strong&gt; :  +200 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2014&lt;/strong&gt; :  +400 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2013&lt;/strong&gt; :  +600 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2012&lt;/strong&gt; :  +800 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;~2012&lt;/strong&gt; : &lt;em&gt;Old Papers&lt;/em&gt; (by discussion)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please note that we prefer seminal deep learning papers that can be applied to various researches rather than application papers. For that reason, some papers that meet the criteria may not be accepted while others can be. It depends on the impact of the paper, applicability to other researches scarcity of the research domain, and so on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We need your contributions!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you have any suggestions (missing papers, new papers, key researchers or typos), please feel free to edit and pull a request.
(Please read the &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md"&gt;contributing guide&lt;/a&gt; for further instructions, though just letting me know the title of papers can also be a big contribution to us.)&lt;/p&gt;
&lt;p&gt;(Update) You can download all top-100 papers with &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/fetch_papers.py"&gt;this&lt;/a&gt; and collect all authors' names with &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/get_authors.py"&gt;this&lt;/a&gt;. Also, &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/top100papers.bib"&gt;bib file&lt;/a&gt; for all top-100 papers are available. Thanks, doodhwala, &lt;a href="https://github.com/sunshinemyson"&gt;Sven&lt;/a&gt; and &lt;a href="https://github.com/grepinsight"&gt;grepinsight&lt;/a&gt;!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can anyone contribute the code for obtaining the statistics of the authors of Top-100 papers?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#understanding--generalization--transfer"&gt;Understanding / Generalization / Transfer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimization--training-techniques"&gt;Optimization / Training Techniques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unsupervised--generative-models"&gt;Unsupervised / Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#convolutional-neural-network-models"&gt;Convolutional Network Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image-segmentation--object-detection"&gt;Image Segmentation / Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image--video--etc"&gt;Image / Video / Etc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#natural-language-processing--rnns"&gt;Natural Language Processing / RNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#speech--other-domain"&gt;Speech / Other Domain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement-learning--robotics"&gt;Reinforcement Learning / Robotics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#more-papers-from-2016"&gt;More Papers from 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(More than Top 100)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#new-papers"&gt;New Papers&lt;/a&gt; : Less than 6 months&lt;/li&gt;
&lt;li&gt;&lt;a href="#old-papers"&gt;Old Papers&lt;/a&gt; : Before 2012&lt;/li&gt;
&lt;li&gt;&lt;a href="#hw--sw--dataset"&gt;HW / SW / Dataset&lt;/a&gt; : Technical reports&lt;/li&gt;
&lt;li&gt;&lt;a href="#book--survey--review"&gt;Book / Survey / Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#video-lectures--tutorials--blogs"&gt;Video Lectures / Tutorials / Blogs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#appendix-more-than-top-100"&gt;Appendix: More than Top 100&lt;/a&gt; : More papers not in the list&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-understanding--generalization--transfer" class="anchor" aria-hidden="true" href="#understanding--generalization--transfer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Understanding / Generalization / Transfer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Distilling the knowledge in a neural network&lt;/strong&gt; (2015), G. Hinton et al. &lt;a href="http://arxiv.org/pdf/1503.02531" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep neural networks are easily fooled: High confidence predictions for unrecognizable images&lt;/strong&gt; (2015), A. Nguyen et al. &lt;a href="http://arxiv.org/pdf/1412.1897" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How transferable are features in deep neural networks?&lt;/strong&gt; (2014), J. Yosinski et al. &lt;a href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CNN features off-the-Shelf: An astounding baseline for recognition&lt;/strong&gt; (2014), A. Razavian et al. &lt;a href="http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning and transferring mid-Level image representations using convolutional neural networks&lt;/strong&gt; (2014), M. Oquab et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Visualizing and understanding convolutional networks&lt;/strong&gt; (2014), M. Zeiler and R. Fergus &lt;a href="http://arxiv.org/pdf/1311.2901" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decaf: A deep convolutional activation feature for generic visual recognition&lt;/strong&gt; (2014), J. Donahue et al. &lt;a href="http://arxiv.org/pdf/1310.1531" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-optimization--training-techniques" class="anchor" aria-hidden="true" href="#optimization--training-techniques"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimization / Training Techniques&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Training very deep networks&lt;/strong&gt; (2015), R. Srivastava et al. &lt;a href="http://papers.nips.cc/paper/5850-training-very-deep-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Batch normalization: Accelerating deep network training by reducing internal covariate shift&lt;/strong&gt; (2015), S. Loffe and C. Szegedy &lt;a href="http://arxiv.org/pdf/1502.03167" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification&lt;/strong&gt; (2015), K. He et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dropout: A simple way to prevent neural networks from overfitting&lt;/strong&gt; (2014), N. Srivastava et al. &lt;a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adam: A method for stochastic optimization&lt;/strong&gt; (2014), D. Kingma and J. Ba &lt;a href="http://arxiv.org/pdf/1412.6980" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href="http://arxiv.org/pdf/1207.0580.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random search for hyper-parameter optimization&lt;/strong&gt; (2012) J. Bergstra and Y. Bengio &lt;a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-unsupervised--generative-models" class="anchor" aria-hidden="true" href="#unsupervised--generative-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised / Generative Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pixel recurrent neural networks&lt;/strong&gt; (2016), A. Oord et al. &lt;a href="http://arxiv.org/pdf/1601.06759v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improved techniques for training GANs&lt;/strong&gt; (2016), T. Salimans et al. &lt;a href="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unsupervised representation learning with deep convolutional generative adversarial networks&lt;/strong&gt; (2015), A. Radford et al. &lt;a href="https://arxiv.org/pdf/1511.06434v2" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DRAW: A recurrent neural network for image generation&lt;/strong&gt; (2015), K. Gregor et al. &lt;a href="http://arxiv.org/pdf/1502.04623" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generative adversarial nets&lt;/strong&gt; (2014), I. Goodfellow et al. &lt;a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Auto-encoding variational Bayes&lt;/strong&gt; (2013), D. Kingma and M. Welling &lt;a href="http://arxiv.org/pdf/1312.6114" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Building high-level features using large scale unsupervised learning&lt;/strong&gt; (2013), Q. Le et al. &lt;a href="http://arxiv.org/pdf/1112.6209" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-convolutional-neural-network-models" class="anchor" aria-hidden="true" href="#convolutional-neural-network-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Network Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Rethinking the inception architecture for computer vision&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inception-v4, inception-resnet and the impact of residual connections on learning&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href="http://arxiv.org/pdf/1602.07261" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Identity Mappings in Deep Residual Networks&lt;/strong&gt; (2016), K. He et al. &lt;a href="https://arxiv.org/pdf/1603.05027v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep residual learning for image recognition&lt;/strong&gt; (2016), K. He et al. &lt;a href="http://arxiv.org/pdf/1512.03385" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spatial transformer network&lt;/strong&gt; (2015), M. Jaderberg et al., &lt;a href="http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Going deeper with convolutions&lt;/strong&gt; (2015), C. Szegedy et al.  &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Very deep convolutional networks for large-scale image recognition&lt;/strong&gt; (2014), K. Simonyan and A. Zisserman &lt;a href="http://arxiv.org/pdf/1409.1556" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Return of the devil in the details: delving deep into convolutional nets&lt;/strong&gt; (2014), K. Chatfield et al. &lt;a href="http://arxiv.org/pdf/1405.3531" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OverFeat: Integrated recognition, localization and detection using convolutional networks&lt;/strong&gt; (2013), P. Sermanet et al. &lt;a href="http://arxiv.org/pdf/1312.6229" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maxout networks&lt;/strong&gt; (2013), I. Goodfellow et al. &lt;a href="http://arxiv.org/pdf/1302.4389v4" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Network in network&lt;/strong&gt; (2013), M. Lin et al. &lt;a href="http://arxiv.org/pdf/1312.4400" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ImageNet classification with deep convolutional neural networks&lt;/strong&gt; (2012), A. Krizhevsky et al. &lt;a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-image-segmentation--object-detection" class="anchor" aria-hidden="true" href="#image-segmentation--object-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image: Segmentation / Object Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You only look once: Unified, real-time object detection&lt;/strong&gt; (2016), J. Redmon et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fully convolutional networks for semantic segmentation&lt;/strong&gt; (2015), J. Long et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/strong&gt; (2015), S. Ren et al. &lt;a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fast R-CNN&lt;/strong&gt; (2015), R. Girshick &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/strong&gt; (2014), R. Girshick et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spatial pyramid pooling in deep convolutional networks for visual recognition&lt;/strong&gt; (2014), K. He et al. &lt;a href="http://arxiv.org/pdf/1406.4729" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Semantic image segmentation with deep convolutional nets and fully connected CRFs&lt;/strong&gt;, L. Chen et al. &lt;a href="https://arxiv.org/pdf/1412.7062" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning hierarchical features for scene labeling&lt;/strong&gt; (2013), C. Farabet et al. &lt;a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-image--video--etc" class="anchor" aria-hidden="true" href="#image--video--etc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image / Video / Etc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/strong&gt; (2016), C. Dong et al. &lt;a href="https://arxiv.org/pdf/1501.00092v3.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A neural algorithm of artistic style&lt;/strong&gt; (2015), L. Gatys et al. &lt;a href="https://arxiv.org/pdf/1508.06576" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep visual-semantic alignments for generating image descriptions&lt;/strong&gt; (2015), A. Karpathy and L. Fei-Fei &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Show, attend and tell: Neural image caption generation with visual attention&lt;/strong&gt; (2015), K. Xu et al. &lt;a href="http://arxiv.org/pdf/1502.03044" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Show and tell: A neural image caption generator&lt;/strong&gt; (2015), O. Vinyals et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Long-term recurrent convolutional networks for visual recognition and description&lt;/strong&gt; (2015), J. Donahue et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VQA: Visual question answering&lt;/strong&gt; (2015), S. Antol et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DeepFace: Closing the gap to human-level performance in face verification&lt;/strong&gt; (2014), Y. Taigman et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Large-scale video classification with convolutional neural networks&lt;/strong&gt; (2014), A. Karpathy et al. &lt;a href="http://vision.stanford.edu/pdf/karpathy14.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Two-stream convolutional networks for action recognition in videos&lt;/strong&gt; (2014), K. Simonyan et al. &lt;a href="http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3D convolutional neural networks for human action recognition&lt;/strong&gt; (2013), S. Ji et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;&lt;a id="user-content-natural-language-processing--rnns" class="anchor" aria-hidden="true" href="#natural-language-processing--rnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natural Language Processing / RNNs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Neural Architectures for Named Entity Recognition&lt;/strong&gt; (2016), G. Lample et al. &lt;a href="http://aclweb.org/anthology/N/N16/N16-1030.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exploring the limits of language modeling&lt;/strong&gt; (2016), R. Jozefowicz et al. &lt;a href="http://arxiv.org/pdf/1602.02410" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Teaching machines to read and comprehend&lt;/strong&gt; (2015), K. Hermann et al. &lt;a href="http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Effective approaches to attention-based neural machine translation&lt;/strong&gt; (2015), M. Luong et al. &lt;a href="https://arxiv.org/pdf/1508.04025" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conditional random fields as recurrent neural networks&lt;/strong&gt; (2015), S. Zheng and S. Jayasumana. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory networks&lt;/strong&gt; (2014), J. Weston et al. &lt;a href="https://arxiv.org/pdf/1410.3916" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural turing machines&lt;/strong&gt; (2014), A. Graves et al. &lt;a href="https://arxiv.org/pdf/1410.5401" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural machine translation by jointly learning to align and translate&lt;/strong&gt; (2014), D. Bahdanau et al. &lt;a href="http://arxiv.org/pdf/1409.0473" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sequence to sequence learning with neural networks&lt;/strong&gt; (2014), I. Sutskever et al. &lt;a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning phrase representations using RNN encoder-decoder for statistical machine translation&lt;/strong&gt; (2014), K. Cho et al. &lt;a href="http://arxiv.org/pdf/1406.1078" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A convolutional neural network for modeling sentences&lt;/strong&gt; (2014), N. Kalchbrenner et al. &lt;a href="http://arxiv.org/pdf/1404.2188v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convolutional neural networks for sentence classification&lt;/strong&gt; (2014), Y. Kim &lt;a href="http://arxiv.org/pdf/1408.5882" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Glove: Global vectors for word representation&lt;/strong&gt; (2014), J. Pennington et al. &lt;a href="http://anthology.aclweb.org/D/D14/D14-1162.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed representations of sentences and documents&lt;/strong&gt; (2014), Q. Le and T. Mikolov &lt;a href="http://arxiv.org/pdf/1405.4053" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed representations of words and phrases and their compositionality&lt;/strong&gt; (2013), T. Mikolov et al. &lt;a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficient estimation of word representations in vector space&lt;/strong&gt; (2013), T. Mikolov et al.  &lt;a href="http://arxiv.org/pdf/1301.3781" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recursive deep models for semantic compositionality over a sentiment treebank&lt;/strong&gt; (2013), R. Socher et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generating sequences with recurrent neural networks&lt;/strong&gt; (2013), A. Graves. &lt;a href="https://arxiv.org/pdf/1308.0850" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-speech--other-domain" class="anchor" aria-hidden="true" href="#speech--other-domain"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speech / Other Domain&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end attention-based large vocabulary speech recognition&lt;/strong&gt; (2016), D. Bahdanau et al. &lt;a href="https://arxiv.org/pdf/1508.04395" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep speech 2: End-to-end speech recognition in English and Mandarin&lt;/strong&gt; (2015), D. Amodei et al. &lt;a href="https://arxiv.org/pdf/1512.02595" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speech recognition with deep recurrent neural networks&lt;/strong&gt; (2013), A. Graves &lt;a href="http://arxiv.org/pdf/1303.5778.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href="http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition&lt;/strong&gt; (2012) G. Dahl et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Acoustic modeling using deep belief networks&lt;/strong&gt; (2012), A. Mohamed et al. &lt;a href="http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-reinforcement-learning--robotics" class="anchor" aria-hidden="true" href="#reinforcement-learning--robotics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reinforcement Learning / Robotics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end training of deep visuomotor policies&lt;/strong&gt; (2016), S. Levine et al. &lt;a href="http://www.jmlr.org/papers/volume17/15-522/source/15-522.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection&lt;/strong&gt; (2016), S. Levine et al. &lt;a href="https://arxiv.org/pdf/1603.02199" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Asynchronous methods for deep reinforcement learning&lt;/strong&gt; (2016), V. Mnih et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep Reinforcement Learning with Double Q-Learning&lt;/strong&gt; (2016), H. Hasselt et al. &lt;a href="https://arxiv.org/pdf/1509.06461.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mastering the game of Go with deep neural networks and tree search&lt;/strong&gt; (2016), D. Silver et al. &lt;a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Continuous control with deep reinforcement learning&lt;/strong&gt; (2015), T. Lillicrap et al. &lt;a href="https://arxiv.org/pdf/1509.02971" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Human-level control through deep reinforcement learning&lt;/strong&gt; (2015), V. Mnih et al. &lt;a href="http://www.davidqiu.com:8888/research/nature14236.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep learning for detecting robotic grasps&lt;/strong&gt; (2015), I. Lenz et al. &lt;a href="http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Playing atari with deep reinforcement learning&lt;/strong&gt; (2013), V. Mnih et al. &lt;a href="http://arxiv.org/pdf/1312.5602.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-more-papers-from-2016" class="anchor" aria-hidden="true" href="#more-papers-from-2016"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More Papers from 2016&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Layer Normalization&lt;/strong&gt; (2016), J. Ba et al. &lt;a href="https://arxiv.org/pdf/1607.06450v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning to learn by gradient descent by gradient descent&lt;/strong&gt; (2016), M. Andrychowicz et al. &lt;a href="http://arxiv.org/pdf/1606.04474v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Domain-adversarial training of neural networks&lt;/strong&gt; (2016), Y. Ganin et al. &lt;a href="http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WaveNet: A Generative Model for Raw Audio&lt;/strong&gt; (2016), A. Oord et al. &lt;a href="https://arxiv.org/pdf/1609.03499v2" rel="nofollow"&gt;[pdf]&lt;/a&gt; &lt;a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Colorful image colorization&lt;/strong&gt; (2016), R. Zhang et al. &lt;a href="https://arxiv.org/pdf/1603.08511" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generative visual manipulation on the natural image manifold&lt;/strong&gt; (2016), J. Zhu et al. &lt;a href="https://arxiv.org/pdf/1609.03552" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Texture networks: Feed-forward synthesis of textures and stylized images&lt;/strong&gt; (2016), D Ulyanov et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/ulyanov16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSD: Single shot multibox detector&lt;/strong&gt; (2016), W. Liu et al. &lt;a href="https://arxiv.org/pdf/1512.02325" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&amp;lt; 1MB model size&lt;/strong&gt; (2016), F. Iandola et al. &lt;a href="http://arxiv.org/pdf/1602.07360" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eie: Efficient inference engine on compressed deep neural network&lt;/strong&gt; (2016), S. Han et al. &lt;a href="http://arxiv.org/pdf/1602.01528" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1&lt;/strong&gt; (2016), M. Courbariaux et al. &lt;a href="https://arxiv.org/pdf/1602.02830" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic memory networks for visual and textual question answering&lt;/strong&gt; (2016), C. Xiong et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stacked attention networks for image question answering&lt;/strong&gt; (2016), Z. Yang et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid computing using a neural network with dynamic external memory&lt;/strong&gt; (2016), A. Graves et al. &lt;a href="https://www.gwern.net/docs/2016-graves.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google's neural machine translation system: Bridging the gap between human and machine translation&lt;/strong&gt; (2016), Y. Wu et al. &lt;a href="https://arxiv.org/pdf/1609.08144" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-new-papers" class="anchor" aria-hidden="true" href="#new-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New papers&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Newly published papers (&amp;lt; 6 months) which are worth reading&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (2017), Andrew G. Howard et al. &lt;a href="https://arxiv.org/pdf/1704.04861.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convolutional Sequence to Sequence Learning (2017), Jonas Gehring et al. &lt;a href="https://arxiv.org/pdf/1705.03122" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A Knowledge-Grounded Neural Conversation Model (2017), Marjan Ghazvininejad et al. &lt;a href="https://arxiv.org/pdf/1702.01932" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour (2017), Priya Goyal et al. &lt;a href="https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h3.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TACOTRON: Towards end-to-end speech synthesis (2017), Y. Wang et al. &lt;a href="https://arxiv.org/pdf/1703.10135.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Photo Style Transfer (2017), F. Luan et al. &lt;a href="http://arxiv.org/pdf/1703.07511v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al. &lt;a href="http://arxiv.org/pdf/1703.03864v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deformable Convolutional Networks (2017), J. Dai et al. &lt;a href="http://arxiv.org/pdf/1703.06211v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mask R-CNN (2017), K. He et al. &lt;a href="https://128.84.21.199/pdf/1703.06870" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al. &lt;a href="http://arxiv.org/pdf/1703.05192v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep voice: Real-time neural text-to-speech (2017), S. Arik et al., &lt;a href="http://arxiv.org/pdf/1702.07825v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PixelNet: Representation of the pixels, by the pixels, and for the pixels (2017), A. Bansal et al. &lt;a href="http://arxiv.org/pdf/1702.06506v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Batch renormalization: Towards reducing minibatch dependence in batch-normalized models (2017), S. Ioffe. &lt;a href="https://arxiv.org/abs/1702.03275" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wasserstein GAN (2017), M. Arjovsky et al. &lt;a href="https://arxiv.org/pdf/1701.07875v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding deep learning requires rethinking generalization (2017), C. Zhang et al. &lt;a href="https://arxiv.org/pdf/1611.03530" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Least squares generative adversarial networks (2016), X. Mao et al. &lt;a href="https://arxiv.org/abs/1611.04076v2" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-old-papers" class="anchor" aria-hidden="true" href="#old-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Old Papers&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Classic papers published before 2012&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep sparse rectifier neural networks (2011), X. Glorot et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Natural language processing (almost) from scratch (2011), R. Collobert et al. &lt;a href="http://arxiv.org/pdf/1103.0398" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recurrent neural network based language model (2010), T. Mikolov et al. &lt;a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning mid-level features for recognition (2010), Y. Boureau &lt;a href="http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A practical guide to training restricted boltzmann machines (2010), G. Hinton &lt;a href="http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Why does unsupervised pre-training help deep learning (2010), D. Erhan et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning deep architectures for AI (2009), Y. Bengio. &lt;a href="http://sanghv.com/download/soft/machine%20learning,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%20(2009).pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009), H. Lee et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Greedy layer-wise training of deep networks (2007), Y. Bengio et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reducing the dimensionality of data with neural networks, G. Hinton and R. Salakhutdinov. &lt;a href="http://homes.mpimf-heidelberg.mpg.de/~mhelmsta/pdf/2006%20Hinton%20Salakhudtkinov%20Science.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A fast learning algorithm for deep belief nets (2006), G. Hinton et al. &lt;a href="http://nuyoo.utm.mx/~jjf/rna/A8%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gradient-based learning applied to document recognition (1998), Y. LeCun et al. &lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Long short-term memory (1997), S. Hochreiter and J. Schmidhuber. &lt;a href="http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-hw--sw--dataset" class="anchor" aria-hidden="true" href="#hw--sw--dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HW / SW / Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SQuAD: 100,000+ Questions for Machine Comprehension of Text (2016), Rajpurkar et al. &lt;a href="https://arxiv.org/pdf/1606.05250.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenAI gym (2016), G. Brockman et al. &lt;a href="https://arxiv.org/pdf/1606.01540" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TensorFlow: Large-scale machine learning on heterogeneous distributed systems (2016), M. Abadi et al. &lt;a href="http://arxiv.org/pdf/1603.04467" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Theano: A Python framework for fast computation of mathematical expressions, R. Al-Rfou et al.&lt;/li&gt;
&lt;li&gt;Torch7: A matlab-like environment for machine learning, R. Collobert et al. &lt;a href="https://ronan.collobert.com/pub/matos/2011_torch7_nipsw.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MatConvNet: Convolutional neural networks for matlab (2015), A. Vedaldi and K. Lenc &lt;a href="http://arxiv.org/pdf/1412.4564" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Imagenet large scale visual recognition challenge (2015), O. Russakovsky et al. &lt;a href="http://arxiv.org/pdf/1409.0575" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Caffe: Convolutional architecture for fast feature embedding (2014), Y. Jia et al. &lt;a href="http://arxiv.org/pdf/1408.5093" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-book--survey--review" class="anchor" aria-hidden="true" href="#book--survey--review"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Book / Survey / Review&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;On the Origin of Deep Learning (2017), H. Wang and Bhiksha Raj. &lt;a href="https://arxiv.org/pdf/1702.07800" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Reinforcement Learning: An Overview (2017), Y. Li, &lt;a href="http://arxiv.org/pdf/1701.07274v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Machine Translation and Sequence-to-sequence Models(2017): A Tutorial, G. Neubig. &lt;a href="http://arxiv.org/pdf/1703.01619v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Network and Deep Learning (Book, Jan 2017), Michael Nielsen. &lt;a href="http://neuralnetworksanddeeplearning.com/index.html" rel="nofollow"&gt;[html]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep learning (Book, 2016), Goodfellow et al. &lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;[html]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LSTM: A search space odyssey (2016), K. Greff et al. &lt;a href="https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&amp;amp;utm_medium=social&amp;amp;utm_source=plus.google.com&amp;amp;utm_campaign=buffer" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tutorial on Variational Autoencoders (2016), C. Doersch. &lt;a href="https://arxiv.org/pdf/1606.05908" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton &lt;a href="https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep learning in neural networks: An overview (2015), J. Schmidhuber &lt;a href="http://arxiv.org/pdf/1404.7828" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Representation learning: A review and new perspectives (2013), Y. Bengio et al. &lt;a href="http://arxiv.org/pdf/1206.5538" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-video-lectures--tutorials--blogs" class="anchor" aria-hidden="true" href="#video-lectures--tutorials--blogs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video Lectures / Tutorials / Blogs&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(Lectures)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CS231n, Convolutional Neural Networks for Visual Recognition, Stanford University &lt;a href="http://cs231n.stanford.edu/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CS224d, Deep Learning for Natural Language Processing, Stanford University &lt;a href="http://cs224d.stanford.edu/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Oxford Deep NLP 2017, Deep Learning for Natural Language Processing, University of Oxford &lt;a href="https://github.com/oxford-cs-deepnlp-2017/lectures"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(Tutorials)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NIPS 2016 Tutorials, Long Beach &lt;a href="https://nips.cc/Conferences/2016/Schedule?type=Tutorial" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ICML 2016 Tutorials, New York City &lt;a href="http://techtalks.tv/icml/2016/tutorials/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ICLR 2016 Videos, San Juan &lt;a href="http://videolectures.net/iclr2016_san_juan/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Learning Summer School 2016, Montreal &lt;a href="http://videolectures.net/deeplearning2016_montreal/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bay Area Deep Learning School 2016, Stanford &lt;a href="https://www.bayareadlschool.org/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(Blogs)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenAI &lt;a href="https://www.openai.com/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Distill &lt;a href="http://distill.pub/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Andrej Karpathy Blog &lt;a href="http://karpathy.github.io/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Colah's Blog &lt;a href="http://colah.github.io/" rel="nofollow"&gt;[Web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WildML &lt;a href="http://www.wildml.com/" rel="nofollow"&gt;[Web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FastML &lt;a href="http://www.fastml.com/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TheMorningPaper &lt;a href="https://blog.acolyer.org" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-appendix-more-than-top-100" class="anchor" aria-hidden="true" href="#appendix-more-than-top-100"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Appendix: More than Top 100&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(2016)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A character-level decoder without explicit segmentation for neural machine translation (2016), J. Chung et al. &lt;a href="https://arxiv.org/pdf/1603.06147" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dermatologist-level classification of skin cancer with deep neural networks (2017), A. Esteva et al. &lt;a href="http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html" rel="nofollow"&gt;[html]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Weakly supervised object localization with multi-fold multiple instance learning (2017), R. Gokberk et al. &lt;a href="https://arxiv.org/pdf/1503.00949" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Brain tumor segmentation with deep neural networks (2017), M. Havaei et al. &lt;a href="https://arxiv.org/pdf/1505.03540" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Professor Forcing: A New Algorithm for Training Recurrent Networks (2016), A. Lamb et al. &lt;a href="https://arxiv.org/pdf/1610.09038" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adversarially learned inference (2016), V. Dumoulin et al. &lt;a href="https://ishmaelbelghazi.github.io/ALI/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1606.00704v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding convolutional neural networks (2016), J. Koushik &lt;a href="https://arxiv.org/pdf/1605.09081v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Taking the human out of the loop: A review of bayesian optimization (2016), B. Shahriari et al. &lt;a href="https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adaptive computation time for recurrent neural networks (2016), A. Graves &lt;a href="http://arxiv.org/pdf/1603.08983" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Densely connected convolutional networks (2016), G. Huang et al. &lt;a href="https://arxiv.org/pdf/1608.06993v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Region-based convolutional networks for accurate object detection and segmentation (2016), R. Girshick et al.&lt;/li&gt;
&lt;li&gt;Continuous deep q-learning with model-based acceleration (2016), S. Gu et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/gu16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A thorough examination of the cnn/daily mail reading comprehension task (2016), D. Chen et al. &lt;a href="https://arxiv.org/pdf/1606.02858" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Achieving open vocabulary neural machine translation with hybrid word-character models, M. Luong and C. Manning. &lt;a href="https://arxiv.org/pdf/1604.00788" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Very Deep Convolutional Networks for Natural Language Processing (2016), A. Conneau et al. &lt;a href="https://arxiv.org/pdf/1606.01781" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bag of tricks for efficient text classification (2016), A. Joulin et al. &lt;a href="https://arxiv.org/pdf/1607.01759" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Efficient piecewise training of deep structured models for semantic segmentation (2016), G. Lin et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning to compose neural networks for question answering (2016), J. Andreas et al. &lt;a href="https://arxiv.org/pdf/1601.01705" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perceptual losses for real-time style transfer and super-resolution (2016), J. Johnson et al. &lt;a href="https://arxiv.org/pdf/1603.08155" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reading text in the wild with convolutional neural networks (2016), M. Jaderberg et al. &lt;a href="http://arxiv.org/pdf/1412.1842" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;What makes for effective detection proposals? (2016), J. Hosang et al. &lt;a href="https://arxiv.org/pdf/1502.05082" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks (2016), S. Bell et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Instance-aware semantic segmentation via multi-task network cascades (2016), J. Dai et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conditional image generation with pixelcnn decoders (2016), A. van den Oord et al. &lt;a href="http://papers.nips.cc/paper/6527-tree-structured-reinforcement-learning-for-sequential-object-localization.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep networks with stochastic depth (2016), G. Huang et al., &lt;a href="https://arxiv.org/pdf/1603.09382" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics (2016), Yee Whye Teh et al. &lt;a href="http://www.jmlr.org/papers/volume17/teh16a/teh16a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(2015)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ask your neurons: A neural-based approach to answering questions about images (2015), M. Malinowski et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Exploring models and data for image question answering (2015), M. Ren et al. &lt;a href="http://papers.nips.cc/paper/5640-stochastic-variational-inference-for-hidden-markov-models.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Are you talking to a machine? dataset and methods for multilingual image question (2015), H. Gao et al. &lt;a href="http://papers.nips.cc/paper/5641-are-you-talking-to-a-machine-dataset-and-methods-for-multilingual-image-question.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mind's eye: A recurrent visual representation for image caption generation (2015), X. Chen and C. Zitnick. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Chen_Minds_Eye_A_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;From captions to visual concepts and back (2015), H. Fang et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Towards AI-complete question answering: A set of prerequisite toy tasks (2015), J. Weston et al. &lt;a href="http://arxiv.org/pdf/1502.05698" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ask me anything: Dynamic memory networks for natural language processing (2015), A. Kumar et al. &lt;a href="http://arxiv.org/pdf/1506.07285" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Unsupervised learning of video representations using LSTMs (2015), N. Srivastava et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v37/srivastava15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding (2015), S. Han et al. &lt;a href="https://arxiv.org/pdf/1510.00149" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved semantic representations from tree-structured long short-term memory networks (2015), K. Tai et al. &lt;a href="https://arxiv.org/pdf/1503.00075" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Character-aware neural language models (2015), Y. Kim et al. &lt;a href="https://arxiv.org/pdf/1508.06615" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Grammar as a foreign language (2015), O. Vinyals et al. &lt;a href="http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Trust Region Policy Optimization (2015), J. Schulman et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Beyond short snippents: Deep networks for video classification (2015) &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning Deconvolution Network for Semantic Segmentation (2015), H. Noh et al. &lt;a href="https://arxiv.org/pdf/1505.04366v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning spatiotemporal features with 3d convolutional networks (2015), D. Tran et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding neural networks through deep visualization (2015), J. Yosinski et al. &lt;a href="https://arxiv.org/pdf/1506.06579" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An Empirical Exploration of Recurrent Network Architectures (2015), R. Jozefowicz et al.  &lt;a href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep generative image models using aÔøº laplacian pyramid of adversarial networks (2015), E.Denton et al. &lt;a href="http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gated Feedback Recurrent Neural Networks (2015), J. Chung et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v37/chung15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fast and accurate deep network learning by exponential linear units (ELUS) (2015), D. Clevert et al. &lt;a href="https://arxiv.org/pdf/1511.07289.pdf%5Cnhttp://arxiv.org/abs/1511.07289%5Cnhttp://arxiv.org/abs/1511.07289" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pointer networks (2015), O. Vinyals et al. &lt;a href="http://papers.nips.cc/paper/5866-pointer-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Visualizing and Understanding Recurrent Networks (2015), A. Karpathy et al. &lt;a href="https://arxiv.org/pdf/1506.02078" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Attention-based models for speech recognition (2015), J. Chorowski et al. &lt;a href="http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;End-to-end memory networks (2015), S. Sukbaatar et al. &lt;a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Describing videos by exploiting temporal structure (2015), L. Yao et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Describing_Videos_by_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A neural conversational model (2015), O. Vinyals and Q. Le. &lt;a href="https://arxiv.org/pdf/1506.05869.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improving distributional similarity with lessons learned from word embeddings, O. Levy et al. [[pdf]] (&lt;a href="https://www.transacl.org/ojs/index.php/tacl/article/download/570/124" rel="nofollow"&gt;https://www.transacl.org/ojs/index.php/tacl/article/download/570/124&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Transition-Based Dependency Parsing with Stack Long Short-Term Memory (2015), C. Dyer et al. &lt;a href="http://aclweb.org/anthology/P/P15/P15-1033.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs (2015), M. Ballesteros et al. &lt;a href="http://aclweb.org/anthology/D/D15/D15-1041.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Finding function in form: Compositional character models for open vocabulary word representation (2015), W. Ling et al. &lt;a href="http://aclweb.org/anthology/D/D15/D15-1176.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(~2014)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DeepPose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning a Deep Convolutional Network for Image Super-Resolution (2014, C. Dong et al. &lt;a href="https://www.researchgate.net/profile/Chen_Change_Loy/publication/264552416_Lecture_Notes_in_Computer_Science/links/53e583e50cf25d674e9c280e.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recurrent models of visual attention (2014), V. Mnih et al. &lt;a href="http://arxiv.org/pdf/1406.6247.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Empirical evaluation of gated recurrent neural networks on sequence modeling (2014), J. Chung et al. &lt;a href="https://arxiv.org/pdf/1412.3555" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Addressing the rare word problem in neural machine translation (2014), M. Luong et al. &lt;a href="https://arxiv.org/pdf/1410.8206" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;On the properties of neural machine translation: Encoder-decoder approaches (2014), K. Cho et. al.&lt;/li&gt;
&lt;li&gt;Recurrent neural network regularization (2014), W. Zaremba et al. &lt;a href="http://arxiv.org/pdf/1409.2329" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Intriguing properties of neural networks (2014), C. Szegedy et al. &lt;a href="https://arxiv.org/pdf/1312.6199.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Towards end-to-end speech recognition with recurrent neural networks (2014), A. Graves and N. Jaitly. &lt;a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Scalable object detection using deep neural networks (2014), D. Erhan et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;On the importance of initialization and momentum in deep learning (2013), I. Sutskever et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_sutskever13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Regularization of neural networks using dropconnect (2013), L. Wan et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning Hierarchical Features for Scene Labeling (2013), C. Farabet et al. &lt;a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linguistic Regularities in Continuous Space Word Representations (2013), T. Mikolov et al. &lt;a href="http://www.aclweb.org/anthology/N13-1#page=784" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Large scale distributed deep networks (2012), J. Dean et al. &lt;a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A Fast and Accurate Dependency Parser using Neural Networks. Chen and Manning. &lt;a href="http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;Thank you for all your contributions. Please make sure to read the &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md"&gt;contributing guide&lt;/a&gt; before you make a pull request.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://creativecommons.org/publicdomain/zero/1.0/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/60561947585c982aee67ed3e3b25388184cc0aa3/687474703a2f2f6d6972726f72732e6372656174697665636f6d6d6f6e732e6f72672f70726573736b69742f627574746f6e732f38387833312f7376672f63632d7a65726f2e737667" alt="CC0" data-canonical-src="http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To the extent possible under law, &lt;a href="https://www.facebook.com/terryum.io/" rel="nofollow"&gt;Terry T. Um&lt;/a&gt; has waived all copyright and related or neighboring rights to this work.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>terryum</author><guid isPermaLink="false">https://github.com/terryum/awesome-deep-learning-papers</guid><pubDate>Wed, 29 Jan 2020 00:08:00 GMT</pubDate></item></channel></rss>