<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: GLSL, Today</title><link>https://github.com/trending/glsl?since=daily</link><description>The top repositories on GitHub for glsl, measured daily</description><pubDate>Sun, 01 Dec 2019 01:05:57 GMT</pubDate><lastBuildDate>Sun, 01 Dec 2019 01:05:57 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>bloc97/Anime4K #1 in GLSL, Today</title><link>https://github.com/bloc97/Anime4K</link><description>&lt;p&gt;&lt;i&gt;A High-Quality Real Time Upscaler for Anime Video&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-anime4k" class="anchor" aria-hidden="true" href="#anime4k"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Anime4K&lt;/h1&gt;
&lt;p&gt;Anime4K is a state-of-the-art*, open-source, high-quality real-time anime upscaling algorithm that can be implemented in any programming language.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="results/Main.png?raw=true"&gt;&lt;img src="results/Main.png?raw=true" alt="Thumbnail Image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;*State of the art as of August 2019 in the real-time anime 4K upscaling category, the fastest at achieving reasonable quality. We do not claim this is a superior quality general purpose SISR algorithm compared to machine learning approaches.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Disclaimer: All art assets used are for demonstration and educational purposes. All rights are reserved to their original owners. If you (as a person or a company) own the art and do not wish it to be associated with this project, please contact us at 	&lt;a href="mailto:anime4k.upscale@gmail.com"&gt;anime4k.upscale@gmail.com&lt;/a&gt; and we will gladly take it down.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="results/Comparisons/1_time.png?raw=true"&gt;&lt;img src="results/Comparisons/1_time.png?raw=true" alt="Comparison" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-notice" class="anchor" aria-hidden="true" href="#notice"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Notice&lt;/h2&gt;
&lt;p&gt;We understand that this algorithm is far from perfect, and are working towards a hybrid approach (using Machine Learning) to improve Anime4K.&lt;/p&gt;
&lt;p&gt;The greatest difficulties encountered right now are caused by these issues that other media does not suffer from:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lack of ground truth (No True 4K Anime)&lt;/li&gt;
&lt;li&gt;Few true 1080p anime (Even some anime mastered at 1080p have sprites that were downsampled)&lt;/li&gt;
&lt;li&gt;Non-1080p anime are upsampled to 1080p using simple algorithms, resulting in a blurry 1080p image. Our algorithm has to detect this. (Main reason why waifu2x does not work well on anime)&lt;/li&gt;
&lt;li&gt;UV channels of anime are subsampled (4:2:0), which means the color channels of 1080p anime are in fact 540p, thus there is a lack of 1080p ground truth for the UV channels.&lt;/li&gt;
&lt;li&gt;Simulating H.264/H.265 compression artifacts (for analysis and denoising) is not trivial and is relatively time-consuming.&lt;/li&gt;
&lt;li&gt;Due to the workflow of animation studios and their lack of time/budget, resampling artifacts of individual sprites are present in many modern anime.&lt;/li&gt;
&lt;li&gt;Speed (preferably real-time) is paramount, since we do not want to re-encode video each time the algorithm improves. There is also less risk of permanently altering original content.&lt;/li&gt;
&lt;li&gt;So on...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, we still believe by shrinking the size of VDSR or FSRCNN and using an hybrid approach we can achieve good results.&lt;br&gt;
Stay tuned for more info!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-v10-release-candidate-2" class="anchor" aria-hidden="true" href="#v10-release-candidate-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;v1.0 Release Candidate 2&lt;/h2&gt;
&lt;p&gt;Improved speed.&lt;/p&gt;
&lt;p&gt;Performance is back on par with v0.9 Beta, with only insignificant loss in quality compared to v1.0 RC1. (3ms on RX Vega 64)&lt;/p&gt;
&lt;p&gt;Two more versions are included for less powerful GPUs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Anime4K_Fast (1.5ms)&lt;/li&gt;
&lt;li&gt;Anime4K_UltraFast (1ms) (For potato PCs)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/1.0/RC2_Comparison.png"&gt;&lt;img src="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/1.0/RC2_Comparison.png" alt="ComparisonRC" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;em&gt;Please view in full size on a 4K display for a correct comparison.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-v10-release-candidate" class="anchor" aria-hidden="true" href="#v10-release-candidate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;v1.0 Release Candidate&lt;/h2&gt;
&lt;p&gt;Reduced texture loss, aliasing and banding in Anime4K v1.0 RC at the cost of performance. It now takes 6ms. +2ms for line detection and +1ms for line targeted FXAA.&lt;/p&gt;
&lt;p&gt;What's new:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A line detection algorithm.&lt;/li&gt;
&lt;li&gt;Gradient maximization is only applied near lines using the line detector, instead of indiscriminately affecting the entire image. This has the effect of ignoring textures and out of focus elements.&lt;/li&gt;
&lt;li&gt;Finally, one iteration of targeted FXAA is applied on the lines using the line detector to reduce aliasing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/0_RC.png"&gt;&lt;img src="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/0_RC.png" alt="ComparisonRC" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/1_RC.png"&gt;&lt;img src="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/1_RC.png" alt="ComparisonRC" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/2_RC.png"&gt;&lt;img src="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/2_RC.png" alt="ComparisonRC" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/3_RC.png"&gt;&lt;img src="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/3_RC.png" alt="ComparisonRC" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-glsl-usage-instructions-mpv" class="anchor" aria-hidden="true" href="#glsl-usage-instructions-mpv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GLSL Usage Instructions (MPV)&lt;/h2&gt;
&lt;p&gt;This implementation is &lt;strong&gt;cross platform&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-glsl-installation" class="anchor" aria-hidden="true" href="#glsl-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="GLSL_Instructions.md"&gt;GLSL Installation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Note for developers: For performance, the GLSL shaders use the &lt;code&gt;POSTKERNEL&lt;/code&gt; texture to store the gradient. You might need to make a backup of the &lt;code&gt;POSTKERNEL&lt;/code&gt; texture before applying these shaders and restore it after if your other shaders or rendering engine uses the &lt;code&gt;POSTKERNEL&lt;/code&gt; texture for other purposes. (In MPV's case, it gets ignored.)&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-hlsl-usage-instructions-mpc-be-with-madvr" class="anchor" aria-hidden="true" href="#hlsl-usage-instructions-mpc-be-with-madvr"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HLSL Usage Instructions (MPC-BE with madVR)&lt;/h2&gt;
&lt;p&gt;This implementation is &lt;strong&gt;only for Windows&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This implementation is also &lt;strong&gt;outdated&lt;/strong&gt;, the latest version is developped on GLSL.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-hlsl-installation" class="anchor" aria-hidden="true" href="#hlsl-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="HLSL_Instructions.md"&gt;HLSL Installation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Note for developers: For performance, the HLSL shaders use the Alpha channel to store the gradient. You might need to make a backup of the alpha channel before applying these shaders and restore it after if your rendering engine uses the alpha channel for other purposes. (In MPC-BE's case, it gets ignored.)&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-java-usage-instructions-standalone" class="anchor" aria-hidden="true" href="#java-usage-instructions-standalone"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Java Usage Instructions (Standalone)&lt;/h2&gt;
&lt;p&gt;This implementation is &lt;strong&gt;outdated&lt;/strong&gt;, the latest version is developped on GLSL.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-java-installation" class="anchor" aria-hidden="true" href="#java-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Java_Instructions.md"&gt;Java Installation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Click on the link above to read Java version installation and usage instructions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-projects-that-use-anime4k" class="anchor" aria-hidden="true" href="#projects-that-use-anime4k"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects that use Anime4K&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/yeataro/TD-Anime4K"&gt;https://github.com/yeataro/TD-Anime4K&lt;/a&gt; (Anime4K for TouchDesigner)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/keijiro/UnityAnime4K"&gt;https://github.com/keijiro/UnityAnime4K&lt;/a&gt; (Anime4K for Unity)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/net2cn/Anime4KSharp"&gt;https://github.com/net2cn/Anime4KSharp&lt;/a&gt; (Anime4K Re-Implemented in C#)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/andraantariksa/Anime4K-rs"&gt;https://github.com/andraantariksa/Anime4K-rs&lt;/a&gt; (Anime4K Re-Implemented in Rust)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/k4yt3x/video2x"&gt;https://github.com/k4yt3x/video2x&lt;/a&gt; (Anime Video Upscaling Pipeline)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pseudo-preprint-preview" class="anchor" aria-hidden="true" href="#pseudo-preprint-preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pseudo-Preprint Preview&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-read-full-pseudo-preprint" class="anchor" aria-hidden="true" href="#read-full-pseudo-preprint"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Preprint.md"&gt;Read Full Pseudo-Preprint&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;B. Peng&lt;br&gt;
August 2019&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ad perpetuam memoriam of all who perished in the Kyoto Animation arson attack.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="Preprint.md#abstract"&gt;Abstract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Preprint.md#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Preprint.md#proposed-method"&gt;Proposed Method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Preprint.md#results"&gt;Results and Upscale Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Preprint.md#discussion"&gt;Discussion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Preprint.md#analysis"&gt;Analysis and Comparison to Other Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-abstract" class="anchor" aria-hidden="true" href="#abstract"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a state-of-the-art high-quality real-time SISR algorithm designed to work with Japanese animation and cartoons that is extremely fast &lt;em&gt;(~3ms with Vega 64 GPU)&lt;/em&gt;, temporally coherent, simple to implement &lt;em&gt;(~100 lines of code)&lt;/em&gt;, yet very effective. We find it surprising that this method is not currently used 'en masse', since the intuition leading us to this algorithm is very straightforward.&lt;br&gt;
Remarkably, the proposed method does not use any machine-learning or statistical approach, and is tailored to content that puts importance to well defined lines/edges while tolerates a sacrifice of the finer textures. The proposed algorithm can be quickly described as an iterative algorithm that treats color information as a heightmap and 'pushes' pixels towards probable edges using gradient-ascent. This is very likely what learning-based approaches are already doing under the hood (eg. VDSR&lt;sup&gt;&lt;strong&gt;[1]&lt;/strong&gt;&lt;/sup&gt;, waifu2x&lt;sup&gt;&lt;strong&gt;[2]&lt;/strong&gt;&lt;/sup&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-why-not-just-use-waifu2x" class="anchor" aria-hidden="true" href="#why-not-just-use-waifu2x"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why not just use waifu2x&lt;/h3&gt;
&lt;p&gt;waifu2x is too slow for real time applications.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-why-not-just-use-madvr-with-ngu" class="anchor" aria-hidden="true" href="#why-not-just-use-madvr-with-ngu"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why not just use madVR with NGU&lt;/h3&gt;
&lt;p&gt;NGU is proprietary, this algorithm is licensed under MIT.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-does-fsrcnnx-compare-to-this" class="anchor" aria-hidden="true" href="#how-does-fsrcnnx-compare-to-this"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How does FSRCNNX compare to this&lt;/h3&gt;
&lt;p&gt;Since it performs poorly (perceptually, for anime) compared to other algorithms, it was left out of our visual comparisons.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/FSRCNNX.png"&gt;&lt;img src="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/FSRCNNX.png" alt="ComparisonRC" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: FSRCNNX was not specifically trained/designed for anime. It is however a good general-purpose SISR algorithm for video.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-where-are-the-psnrssim-metrics" class="anchor" aria-hidden="true" href="#where-are-the-psnrssim-metrics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Where are the PSNR/SSIM metrics&lt;/h3&gt;
&lt;p&gt;There are no ground truths of 4K anime.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-why-not-do-psnrssim-on-480p-720p-upscaling" class="anchor" aria-hidden="true" href="#why-not-do-psnrssim-on-480p-720p-upscaling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why not do PSNR/SSIM on 480p-&amp;gt;720p upscaling&lt;/h3&gt;
&lt;p&gt;&lt;a href="FAQ_Detail.md"&gt;Story Time&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comparing PSNR/SSIM on 480p-&amp;gt;720p upscales does not prove and is not a good indicator of 1080p-&amp;gt;2160p upscaling quality. (Eg. poor performance of waifu2x on 1080p anime) 480p anime images have a lot of high frequency information (lines might be thinner than 1 pixel), while 1080p anime images have a lot of redundant information. 1080p-&amp;gt;2160p upscaling on anime is thus objectively easier than 480p-&amp;gt;720p.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-i-think-the-results-are-worse-than-x" class="anchor" aria-hidden="true" href="#i-think-the-results-are-worse-than-x"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I think the results are worse than &amp;lt;x&amp;gt;&lt;/h3&gt;
&lt;p&gt;Surely some people like sharper edges, some like softer ones. Do try it yourself on a few anime before reaching a definite conclusion. People &lt;em&gt;tend&lt;/em&gt; to prefer sharper edges. Also, seeing the comparisons on a 1080p screen is not representative of the final results on a 4K screen, the pixel density and sharpness of the final image is simply not comparable.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-note-for-those-who-think-this-is-not-a-upscaling-algorithm" class="anchor" aria-hidden="true" href="#note-for-those-who-think-this-is-not-a-upscaling-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note for those who think this is not a 'upscaling' algorithm.&lt;/h3&gt;
&lt;p&gt;&lt;a href="Upscaling.md"&gt;Explanation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TL;DR&lt;/p&gt;
&lt;p&gt;Sharpening, De-Blurring and Super-Resolution are equivalent.&lt;br&gt;
Anime4K can de-blur, and is equivalent to a SR algorithm.&lt;br&gt;
A Super-Resolution algorithm can do upscaling.&lt;br&gt;
Thus, Anime4K is an upscaling algorithm.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bloc97</author><guid isPermaLink="false">https://github.com/bloc97/Anime4K</guid><pubDate>Sun, 01 Dec 2019 00:01:00 GMT</pubDate></item><item><title>gl-transitions/gl-transitions #2 in GLSL, Today</title><link>https://github.com/gl-transitions/gl-transitions</link><description>&lt;p&gt;&lt;i&gt;The open collection of GL Transitions&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;ul&gt;
&lt;li&gt;Website: &lt;a href="https://gl-transitions.com" rel="nofollow"&gt;https://gl-transitions.com&lt;/a&gt;  &lt;em&gt;( alternative hosting: &lt;a href="https://gl-transitions.surge.sh/" rel="nofollow"&gt;https://gl-transitions.surge.sh/&lt;/a&gt; )&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;NPM package: &lt;a href="https://www.npmjs.com/package/gl-transitions" rel="nofollow"&gt;https://www.npmjs.com/package/gl-transitions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Libraries for gl-transitions: &lt;a href="https://github.com/gre/gl-transition-libs"&gt;https://github.com/gre/gl-transition-libs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Each commit that gets to &lt;a href="https://github.com/gl-transitions/gl-transitions"&gt;gl-transitions/gl-transitions&lt;/a&gt;'s master automatically generate a new npm minor release.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://badge.fury.io/js/gl-transitions" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c16fcf5e3aa02868d150f87fae8c5f1ab1443883/68747470733a2f2f62616467652e667572792e696f2f6a732f676c2d7472616e736974696f6e732e737667" alt="npm version" data-canonical-src="https://badge.fury.io/js/gl-transitions.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://travis-ci.org/gl-transitions/gl-transitions" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/05725546efbbbba813f3d0a2d487044ce78bc0b3/68747470733a2f2f7472617669732d63692e6f72672f676c2d7472616e736974696f6e732f676c2d7472616e736974696f6e732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/gl-transitions/gl-transitions.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c42ecc6197b0f51a106fb50723f9bc6d2e1f925c/687474703a2f2f692e696d6775722e636f6d2f74573331704a452e676966"&gt;&lt;img src="https://camo.githubusercontent.com/c42ecc6197b0f51a106fb50723f9bc6d2e1f925c/687474703a2f2f692e696d6775722e636f6d2f74573331704a452e676966" style="max-width:100%;"&gt;&lt;/a&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/7e34cd12d5a9afa94f470395b04b0914c978ce01/687474703a2f2f692e696d6775722e636f6d2f555a5a727775552e676966"&gt;&lt;img src="https://camo.githubusercontent.com/7e34cd12d5a9afa94f470395b04b0914c978ce01/687474703a2f2f692e696d6775722e636f6d2f555a5a727775552e676966" style="max-width:100%;"&gt;&lt;/a&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0456d4ed8753fbce027f1174dc8b22da548eeade/687474703a2f2f692e696d6775722e636f6d2f654974426a33582e676966"&gt;&lt;img src="https://camo.githubusercontent.com/0456d4ed8753fbce027f1174dc8b22da548eeade/687474703a2f2f692e696d6775722e636f6d2f654974426a33582e676966" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-gl-transition-specification-v1" class="anchor" aria-hidden="true" href="#gl-transition-specification-v1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GL Transition Specification v1&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;NB. This is a technical documentation, for more informal information, please see &lt;a href="https://gl-transitions.com/" rel="nofollow"&gt;https://gl-transitions.com/&lt;/a&gt; homepage.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This document specifies GL Transition Specification &lt;strong&gt;v1&lt;/strong&gt;, &lt;code&gt;1&lt;/code&gt; as in &lt;code&gt;gl-transitions @ 1&lt;/code&gt; (consistently to the NPM package major). For any breaking changes in this specification, semver will be respected and the major will get bumped.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-a-transition" class="anchor" aria-hidden="true" href="#what-is-a-transition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is a transition?&lt;/h2&gt;
&lt;p&gt;A Transition is an animation that smoothly animates the intermediary steps between 2 textures: &lt;code&gt;from&lt;/code&gt; and &lt;code&gt;to&lt;/code&gt;. The step is specified by a &lt;code&gt;progress&lt;/code&gt; value that moves from &lt;code&gt;0.0&lt;/code&gt; to &lt;code&gt;1.0&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;important feature to respect: When progress is 0.0, exclusively the &lt;code&gt;from&lt;/code&gt; texture must be rendered. When progress is 1.0, exclusively the &lt;code&gt;to&lt;/code&gt; texture must be rendered.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-gl-transition" class="anchor" aria-hidden="true" href="#gl-transition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GL Transition&lt;/h2&gt;
&lt;div class="highlight highlight-source-glsl"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; transition of a simple fade.&lt;/span&gt;
&lt;span class="pl-k"&gt;vec4&lt;/span&gt; transition (&lt;span class="pl-k"&gt;vec2&lt;/span&gt; uv) {
  &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;mix&lt;/span&gt;(
    getFromColor(uv),
    getToColor(uv),
    progress
  );
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A GL Transition is a GLSL code that implements a &lt;code&gt;transition&lt;/code&gt; function which takes a &lt;code&gt;vec2 uv&lt;/code&gt; pixel position and returns a &lt;code&gt;vec4&lt;/code&gt; color. This color represents the mix of the &lt;code&gt;from&lt;/code&gt; to the &lt;code&gt;to&lt;/code&gt; textures based on the variation of a contextual &lt;code&gt;progress&lt;/code&gt; value from &lt;code&gt;0.0&lt;/code&gt; to &lt;code&gt;1.0&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contextual-variables" class="anchor" aria-hidden="true" href="#contextual-variables"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contextual variables&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;progress&lt;/code&gt; (float): a value that &lt;strong&gt;moves from 0.0 to 1.0&lt;/strong&gt; during the transition.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ratio&lt;/code&gt; (float): the ratio of the viewport. It equals &lt;code&gt;width / height&lt;/code&gt;. &lt;em&gt;(width and height are not exposed because you don't need them. A transition code should be scalable to any size. ratio can still be used to preserve some shape ratio, e.g. you want to draw squares)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-contextual-functions" class="anchor" aria-hidden="true" href="#contextual-functions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contextual functions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;vec4 getFromColor(vec2 uv)&lt;/code&gt;: lookup the "from" texture at a given uv coordinate.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vec4 getToColor(vec2 uv)&lt;/code&gt;: lookup the "to" texture at a given uv coordinate.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;don't directly use &lt;code&gt;texture2D&lt;/code&gt; to get a texture pixel out of from and to textures. Instead, use &lt;code&gt;getFromColor(vec2)&lt;/code&gt; and &lt;code&gt;getToColor(vec2)&lt;/code&gt;. That way, the "implementer" can properly implement ratio preserving support as well as chosing a different color for the "out of bound" case.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-transition-parameters" class="anchor" aria-hidden="true" href="#transition-parameters"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transition parameters&lt;/h3&gt;
&lt;p&gt;Transition parameters are parameters than the final user can set to tweak the transition. They are constant over a full run of a transition &lt;em&gt;(no parameter changes when progress moves from 0.0 to 1.0)&lt;/em&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;any constant you define in your transitions are potential parameters to expose.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When you define a transition parameter, you must also define a default value that will get set in case the final user didn't provided it. It's unfortunately not possible to initialize a uniform in GLSL 120 (WebGL 1) but we support commented code &lt;code&gt;// = value&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;div class="highlight highlight-source-glsl"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;uniform&lt;/span&gt; &lt;span class="pl-k"&gt;float&lt;/span&gt; foo; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; = 42.0&lt;/span&gt;
&lt;span class="pl-k"&gt;uniform&lt;/span&gt; &lt;span class="pl-k"&gt;vec2&lt;/span&gt; foo; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; = vec2(42.0, 42.0)&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The following variants are also supported:&lt;/p&gt;
&lt;div class="highlight highlight-source-glsl"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;uniform&lt;/span&gt; &lt;span class="pl-k"&gt;float&lt;/span&gt; foo&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;/*&lt;/span&gt; = 42.0 &lt;span class="pl-c"&gt;*/&lt;/span&gt;&lt;/span&gt;;
&lt;span class="pl-k"&gt;uniform&lt;/span&gt; &lt;span class="pl-k"&gt;vec2&lt;/span&gt; foo &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;/*&lt;/span&gt;= vec2(42.0, 42.0)&lt;span class="pl-c"&gt;*/&lt;/span&gt;&lt;/span&gt;, bar &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;/*&lt;/span&gt; = vec2(1.) &lt;span class="pl-c"&gt;*/&lt;/span&gt;&lt;/span&gt;;
&lt;span class="pl-k"&gt;uniform&lt;/span&gt; &lt;span class="pl-k"&gt;vec2&lt;/span&gt; foo, bar; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; = vec2(1.0, 2.0); // both at the same time ! (needs a ';' if you have this second //, like usual glsl code)&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-gl-transitions" class="anchor" aria-hidden="true" href="#gl-transitions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;gl-transitions&lt;/code&gt;&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;TBD this is not finished to be written. just keeping these notes around...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;If we have duplicated transitions or one transition is more generic than another one, we don't necessary drop the less generic one: it might be more performant and might fit for some users. We also want to keep backward compat'. if we still want to drop it, what we will do is to deprecate it and drop it at the next major bump.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>gl-transitions</author><guid isPermaLink="false">https://github.com/gl-transitions/gl-transitions</guid><pubDate>Sun, 01 Dec 2019 00:02:00 GMT</pubDate></item></channel></rss>