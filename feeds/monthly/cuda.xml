<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Cuda, This month</title><link>https://github.com/trending/cuda?since=monthly</link><description>The top repositories on GitHub for cuda, measured monthly</description><pubDate>Sat, 02 Nov 2019 00:08:02 GMT</pubDate><lastBuildDate>Sat, 02 Nov 2019 00:08:02 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>1400</ttl><item><title>rapidsai/cudf #1 in Cuda, This month</title><link>https://github.com/rapidsai/cudf</link><description>&lt;p&gt;&lt;i&gt;cuDF - GPU DataFrame Library&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cudf---gpu-dataframes" class="anchor" aria-hidden="true" href="#cudf---gpu-dataframes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;div align="left"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/rapids_logo.png"&gt;&lt;img src="img/rapids_logo.png" width="90px" style="max-width:100%;"&gt;&lt;/a&gt; cuDF - GPU DataFrames&lt;/div&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://gpuci.gpuopenanalytics.com/job/gpuCI/job/cudf/job/branches/job/cudf-gpu-branch-0.11/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4d7f2762e2c8e5b40f1e380bf83d8dd7512c3609/68747470733a2f2f67707563692e6770756f70656e616e616c79746963732e636f6d2f6275696c645374617475732f69636f6e3f6a6f623d6770754349253246637564662532466272616e63686573253246637564662d6770752d6272616e63682d302e3131" alt="Build Status" data-canonical-src="https://gpuci.gpuopenanalytics.com/buildStatus/icon?job=gpuCI%2Fcudf%2Fbranches%2Fcudf-gpu-branch-0.11" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; For the latest stable &lt;a href="https://github.com/rapidsai/cudf/blob/master/README.md"&gt;README.md&lt;/a&gt; ensure you are on the &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt;
&lt;p&gt;Built based on the &lt;a href="http://arrow.apache.org/" rel="nofollow"&gt;Apache Arrow&lt;/a&gt; columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.&lt;/p&gt;
&lt;p&gt;cuDF provides a pandas-like API that will be familiar to data engineers &amp;amp; data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming.&lt;/p&gt;
&lt;p&gt;For example, the following snippet downloads a CSV, then uses the GPU to parse it into rows and columns and run calculations:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; cudf, io, requests
&lt;span class="pl-k"&gt;from&lt;/span&gt; io &lt;span class="pl-k"&gt;import&lt;/span&gt; StringIO

url&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://github.com/plotly/datasets/raw/master/tips.csv&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
content &lt;span class="pl-k"&gt;=&lt;/span&gt; requests.get(url).content.decode(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;utf-8&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

tips_df &lt;span class="pl-k"&gt;=&lt;/span&gt; cudf.read_csv(StringIO(content))
tips_df[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tip_percentage&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; tips_df[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tip&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;/&lt;/span&gt;tips_df[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;total_bill&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; display average tip by dining party size&lt;/span&gt;
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(tips_df.groupby(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;size&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;).tip_percentage.mean())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;size
1    21.729201548727808
2    16.571919173482897
3    15.215685473711837
4    14.594900639351332
5    14.149548965142023
6    15.622920072028379
Name: tip_percentage, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For additional examples, browse our complete &lt;a href="https://docs.rapids.ai/api/cudf/stable/" rel="nofollow"&gt;API documentation&lt;/a&gt;, or check out our more detailed &lt;a href="https://github.com/rapidsai/notebooks-contrib"&gt;notebooks&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;Please see the &lt;a href="https://hub.docker.com/r/rapidsai/rapidsai/" rel="nofollow"&gt;Demo Docker Repository&lt;/a&gt;, choosing a tag based on the NVIDIA CUDA version you’re running. This provides a ready to run Docker container with example notebooks and data, showcasing how you can utilize cuDF.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-conda" class="anchor" aria-hidden="true" href="#conda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conda&lt;/h3&gt;
&lt;p&gt;cuDF can be installed with conda (&lt;a href="https://conda.io/miniconda.html" rel="nofollow"&gt;miniconda&lt;/a&gt;, or the full &lt;a href="https://www.anaconda.com/download" rel="nofollow"&gt;Anaconda distribution&lt;/a&gt;) from the &lt;code&gt;rapidsai&lt;/code&gt; channel:&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;cudf version == 0.10&lt;/code&gt; :&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; for CUDA 9.2&lt;/span&gt;
conda install -c rapidsai -c nvidia -c numba -c conda-forge \
    cudf=0.10 python=3.6 cudatoolkit=9.2

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; or, for CUDA 10.0&lt;/span&gt;
conda install -c rapidsai -c nvidia -c numba -c conda-forge \
    cudf=0.10 python=3.6 cudatoolkit=10.0

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; or, for CUDA 10.1&lt;/span&gt;
conda install -c rapidsai -c nvidia -c numba -c conda-forge \
    cudf=0.10 python=3.6 cudatoolkit=10.1&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For the nightly version of &lt;code&gt;cudf&lt;/code&gt; :&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; for CUDA 9.2&lt;/span&gt;
conda install -c rapidsai-nightly -c nvidia -c numba -c conda-forge \
    cudf python=3.6 cudatoolkit=9.2

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; or, for CUDA 10.0&lt;/span&gt;
conda install -c rapidsai-nightly -c nvidia -c numba -c conda-forge \
    cudf python=3.6 cudatoolkit=10.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note: cuDF is supported only on Linux, and with Python versions 3.6 or 3.7.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="https://rapids.ai/start.html" rel="nofollow"&gt;Get RAPIDS version picker&lt;/a&gt; for more OS and version info.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-buildinstall-from-source" class="anchor" aria-hidden="true" href="#buildinstall-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build/Install from Source&lt;/h2&gt;
&lt;p&gt;See build &lt;a href="CONTRIBUTING.md#setting-up-your-build-environment"&gt;instructions&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;Please see our &lt;a href="CONTRIBUTING.md"&gt;guide for contributing to cuDF&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;Find out more details on the &lt;a href="https://rapids.ai/community.html" rel="nofollow"&gt;RAPIDS site&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--open-gpu-data-science" class="anchor" aria-hidden="true" href="#-open-gpu-data-science"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;div align="left"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/rapids_logo.png"&gt;&lt;img src="img/rapids_logo.png" width="265px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/div&gt; Open GPU Data Science&lt;/h2&gt;
&lt;p&gt;The RAPIDS suite of open source software libraries aim to enable execution of end-to-end data science and analytics pipelines entirely on GPUs. It relies on NVIDIA® CUDA® primitives for low-level compute optimization, but exposing that GPU parallelism and high-bandwidth memory speed through user-friendly Python interfaces.&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/rapids_arrow.png"&gt;&lt;img src="img/rapids_arrow.png" width="80%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-apache-arrow-on-gpu" class="anchor" aria-hidden="true" href="#apache-arrow-on-gpu"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Apache Arrow on GPU&lt;/h3&gt;
&lt;p&gt;The GPU version of &lt;a href="https://arrow.apache.org/" rel="nofollow"&gt;Apache Arrow&lt;/a&gt; is a common API that enables efficient interchange of tabular data between processes running on the GPU. End-to-end computation on the GPU avoids unnecessary copying and converting of data off the GPU, reducing compute time and cost for high-performance analytics common in artificial intelligence workloads. As the name implies, cuDF uses the Apache Arrow columnar data format on the GPU. Currently, a subset of the features in Apache Arrow are supported.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rapidsai</author><guid isPermaLink="false">https://github.com/rapidsai/cudf</guid><pubDate>Sat, 02 Nov 2019 00:00:00 GMT</pubDate></item><item><title>hujie-frank/SENet #2 in Cuda, This month</title><link>https://github.com/hujie-frank/SENet</link><description>&lt;p&gt;&lt;i&gt;Squeeze-and-Excitation Networks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-squeeze-and-excitation-networks-paper" class="anchor" aria-hidden="true" href="#squeeze-and-excitation-networks-paper"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Squeeze-and-Excitation Networks &lt;sub&gt;(&lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;)&lt;/sub&gt;&lt;/h1&gt;
&lt;p&gt;By Jie Hu&lt;sup&gt;[1]&lt;/sup&gt;, Li Shen&lt;sup&gt;[2]&lt;/sup&gt;, Gang Sun&lt;sup&gt;[1]&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://momenta.ai/" rel="nofollow"&gt;Momenta&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt; and &lt;a href="http://www.robots.ox.ac.uk/~vgg/" rel="nofollow"&gt;University of Oxford&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-approach" class="anchor" aria-hidden="true" href="#approach"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Approach&lt;/h2&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/hujie-frank/SENet/blob/master/figures/SE-pipeline.jpg"&gt;&lt;img src="https://github.com/hujie-frank/SENet/raw/master/figures/SE-pipeline.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p align="center"&gt;
  Figure 1: Diagram of a Squeeze-and-Excitation building block.
&lt;/p&gt;
&lt;div align="center"&gt;
   &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/hujie-frank/SENet/blob/master/figures/SE-Inception-module.jpg"&gt;&lt;img src="https://github.com/hujie-frank/SENet/raw/master/figures/SE-Inception-module.jpg" width="420" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/hujie-frank/SENet/blob/master/figures/SE-ResNet-module.jpg"&gt;&lt;img src="https://github.com/hujie-frank/SENet/raw/master/figures/SE-ResNet-module.jpg" width="420" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p align="center"&gt;
  Figure 2: Schema of SE-Inception and SE-ResNet modules. We set r=16 in all our models.
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-implementation" class="anchor" aria-hidden="true" href="#implementation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementation&lt;/h2&gt;
&lt;p&gt;In this repository, Squeeze-and-Excitation Networks are implemented by &lt;a href="https://github.com/BVLC/caffe"&gt;Caffe&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-augmentation" class="anchor" aria-hidden="true" href="#augmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Augmentation&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Method&lt;/th&gt;
&lt;th align="center"&gt;Settings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Random Mirror&lt;/td&gt;
&lt;td align="center"&gt;True&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Random Crop&lt;/td&gt;
&lt;td align="center"&gt;8% ~ 100%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Aspect Ratio&lt;/td&gt;
&lt;td align="center"&gt;3/4 ~ 4/3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Random Rotation&lt;/td&gt;
&lt;td align="center"&gt;-10° ~ 10°&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Pixel Jitter&lt;/td&gt;
&lt;td align="center"&gt;-20 ~ 20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-note" class="anchor" aria-hidden="true" href="#note"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;To achieve efficient training and testing, we combine the consecutive operations &lt;em&gt;&lt;strong&gt;channel-wise scale&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;element-wise summation&lt;/strong&gt;&lt;/em&gt; into a single layer &lt;strong&gt;"Axpy"&lt;/strong&gt; in the architectures with skip-connections, resulting in a considerable reduction in memory cost and computational burden.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition, we found that the implementation for &lt;em&gt;&lt;strong&gt;global average pooling&lt;/strong&gt;&lt;/em&gt; on GPU supported by cuDNN and BVLC/caffe is less efficient. In this regard, we re-implement the operation which achieves significant acceleration.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-trained-models" class="anchor" aria-hidden="true" href="#trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trained Models&lt;/h2&gt;
&lt;p&gt;Table 1. Single crop validation error on ImageNet-1k (center 224x224 crop from resized image with shorter side = 256). The SENet-154 is one of our superior models used in &lt;a href="http://image-net.org/challenges/LSVRC/2017/index" rel="nofollow"&gt;ILSVRC 2017 Image Classification Challenge&lt;/a&gt; where we won the 1st place (Team name: &lt;a href="http://image-net.org/challenges/LSVRC/2017/results" rel="nofollow"&gt;WMW&lt;/a&gt;).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Top-1&lt;/th&gt;
&lt;th align="center"&gt;Top-5&lt;/th&gt;
&lt;th align="center"&gt;Size&lt;/th&gt;
&lt;th align="center"&gt;Caffe Model&lt;/th&gt;
&lt;th align="center"&gt;Caffe Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-BN-Inception&lt;/td&gt;
&lt;td align="center"&gt;23.62&lt;/td&gt;
&lt;td align="center"&gt;7.04&lt;/td&gt;
&lt;td align="center"&gt;46 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlTWRRbDZYbVB2WWc/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1qYoPdak" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNet-50&lt;/td&gt;
&lt;td align="center"&gt;22.37&lt;/td&gt;
&lt;td align="center"&gt;6.36&lt;/td&gt;
&lt;td align="center"&gt;107 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlS2QwZHFzM3RjNzg/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1gf5wsLl" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNet-101&lt;/td&gt;
&lt;td align="center"&gt;21.75&lt;/td&gt;
&lt;td align="center"&gt;5.72&lt;/td&gt;
&lt;td align="center"&gt;189 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlTEg4YmcwQ0FoZFU/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1c1FvCWg" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNet-152&lt;/td&gt;
&lt;td align="center"&gt;21.34&lt;/td&gt;
&lt;td align="center"&gt;5.54&lt;/td&gt;
&lt;td align="center"&gt;256 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlcFE0Q2NTcWl3WUE/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1dFEnSzR" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNeXt-50 (32 x 4d)&lt;/td&gt;
&lt;td align="center"&gt;20.97&lt;/td&gt;
&lt;td align="center"&gt;5.54&lt;/td&gt;
&lt;td align="center"&gt;105 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlQ2Z0Q204V1RITjA/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1dFbEmbv" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNeXt-101 (32 x 4d)&lt;/td&gt;
&lt;td align="center"&gt;19.81&lt;/td&gt;
&lt;td align="center"&gt;4.96&lt;/td&gt;
&lt;td align="center"&gt;187 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWleklsNzBiZlprblk/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1qY2wjt6" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SENet-154&lt;/td&gt;
&lt;td align="center"&gt;18.68&lt;/td&gt;
&lt;td align="center"&gt;4.47&lt;/td&gt;
&lt;td align="center"&gt;440 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlbTFZbzFTSXBUTUE/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1o7HdfAE" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here we obtain better performance than those reported in the paper.
We re-train the SENets described in the paper on a single GPU server with 8 NVIDIA Titan X cards, using a mini-batch of 256 and a initial learning rate of 0.1 with more epoches.
In contrast, the results reported in the paper were obtained by training the networks with a larger batch size (1024) and learning rate (0.6) across 4 servers.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-third-party-re-implementations" class="anchor" aria-hidden="true" href="#third-party-re-implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Third-party re-implementations&lt;/h2&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Caffe. SE-mudolues are integrated with a modificated ResNet-50 using a stride 2 in the 3x3 convolution instead of the first 1x1 convolution which obtains better performance: &lt;a href="https://github.com/shicai/SENet-Caffe"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;TensorFlow. SE-modules are integrated with a pre-activation ResNet-50 which follows the setup in &lt;a href="https://github.com/facebook/fb.resnet.torch"&gt;fb.resnet.torch&lt;/a&gt;: &lt;a href="https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ResNet"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;TensorFlow. Simple Tensorflow implementation of SENets using Cifar10: &lt;a href="https://github.com/taki0112/SENet-Tensorflow"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;MatConvNet. All the released SENets are imported into &lt;a href="https://github.com/vlfeat/matconvnet"&gt;MatConvNet&lt;/a&gt;: &lt;a href="https://github.com/albanie/mcnSENets"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;MXNet. SE-modules are integrated with the ResNeXt and more architectures are coming soon: &lt;a href="https://github.com/bruinxiong/SENet.mxnet"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;PyTorch. Implementation of SENets by PyTorch: &lt;a href="https://github.com/moskomule/senet.pytorch"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Chainer. Implementation of SENets by Chainer: &lt;a href="https://github.com/nutszebra/SENets"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use Squeeze-and-Excitation Networks in your research, please cite the paper:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{hu2018senet,
  title={Squeeze-and-Excitation Networks},
  author={Jie Hu and Li Shen and Gang Sun},
  journal={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hujie-frank</author><guid isPermaLink="false">https://github.com/hujie-frank/SENet</guid><pubDate>Sat, 02 Nov 2019 00:00:00 GMT</pubDate></item></channel></rss>