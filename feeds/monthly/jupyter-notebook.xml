<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Jupyter Notebook, This month</title><link>https://github.com/trending/jupyter-notebook?since=monthly</link><description>The top repositories on GitHub for jupyter-notebook, measured monthly</description><pubDate>Thu, 28 Nov 2019 01:06:33 GMT</pubDate><lastBuildDate>Thu, 28 Nov 2019 01:06:33 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>fengdu78/lihang-code #1 in Jupyter Notebook, This month</title><link>https://github.com/fengdu78/lihang-code</link><description>&lt;p&gt;&lt;i&gt;《统计学习方法》的代码实现&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;strong&gt;代码目录&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;第1章 统计学习方法概论&lt;/p&gt;
&lt;p&gt;第2章 感知机&lt;/p&gt;
&lt;p&gt;第3章 k近邻法&lt;/p&gt;
&lt;p&gt;第4章 朴素贝叶斯&lt;/p&gt;
&lt;p&gt;第5章 决策树&lt;/p&gt;
&lt;p&gt;第6章 逻辑斯谛回归&lt;/p&gt;
&lt;p&gt;第7章 支持向量机&lt;/p&gt;
&lt;p&gt;第8章 提升方法&lt;/p&gt;
&lt;p&gt;第9章 EM算法及其推广&lt;/p&gt;
&lt;p&gt;第10章 隐马尔可夫模型&lt;/p&gt;
&lt;p&gt;第11章 条件随机场&lt;/p&gt;
&lt;p&gt;第12章 监督学习方法总结&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;参考：
&lt;a href="https://github.com/wzyonggege/statistical-learning-method"&gt;https://github.com/wzyonggege/statistical-learning-method&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm"&gt;https://github.com/WenDesi/lihang_book_algorithm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.csdn.net/tudaodiaozhale" rel="nofollow"&gt;https://blog.csdn.net/tudaodiaozhale&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码整理和修改：机器学习初学者&lt;/p&gt;
&lt;p&gt;微信公众号：机器学习初学者 &lt;a target="_blank" rel="noopener noreferrer" href="images/gongzhong.jpg"&gt;&lt;img src="images/gongzhong.jpg" alt="gongzhong" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;知识星球：黄博的机器学习圈子&lt;a target="_blank" rel="noopener noreferrer" href="images/zhishixingqiu1.jpg"&gt;&lt;img src="images/zhishixingqiu1.jpg" alt="xingqiu" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/people/fengdu78" rel="nofollow"&gt;知乎&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fengdu78</author><guid isPermaLink="false">https://github.com/fengdu78/lihang-code</guid><pubDate>Thu, 28 Nov 2019 00:01:00 GMT</pubDate></item><item><title>google-research/google-research #2 in Jupyter Notebook, This month</title><link>https://github.com/google-research/google-research</link><description>&lt;p&gt;&lt;i&gt;Google AI Research&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-google-ai-research" class="anchor" aria-hidden="true" href="#google-ai-research"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google AI Research&lt;/h1&gt;
&lt;p&gt;This repository contains code released by
&lt;a href="https://ai.google/research" rel="nofollow"&gt;Google AI Research&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Because the repo is large, we recommend you clone the repo without its history.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/google-research</guid><pubDate>Thu, 28 Nov 2019 00:02:00 GMT</pubDate></item><item><title>dragen1860/TensorFlow-2.x-Tutorials #3 in Jupyter Notebook, This month</title><link>https://github.com/dragen1860/TensorFlow-2.x-Tutorials</link><description>&lt;p&gt;&lt;i&gt;TensorFlow 2.x version's  Tutorials and Examples, including CNN, RNN, GAN, Auto-Encoders, FasterRCNN, GPT, BERT examples, etc. TF 2.0版入门实例代码，实战教程。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-20-tutorials" class="anchor" aria-hidden="true" href="#tensorflow-20-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 2.0 Tutorials&lt;/h1&gt;
&lt;p&gt;Our repo. is the &lt;strong&gt;Winner&lt;/strong&gt; of &lt;a href="https://devpost.com/software/tensorflow-2-0-tutorials" rel="nofollow"&gt;⚡#PoweredByTF 2.0 Challenge!&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="res/tensorflow-2.0.gif"&gt;&lt;img src="res/tensorflow-2.0.gif" width="250" align="middle" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Timeline:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Oct. 1, 2019: TensorFlow 2.0 Stable!&lt;/li&gt;
&lt;li&gt;Aug. 24, 2019: &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf" rel="nofollow"&gt;TensorFlow 2.0 rc0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jun. 8, 2019: &lt;a href="https://twitter.com/fchollet/status/1134583289384120320" rel="nofollow"&gt;TensorFlow 2.0 Beta&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mar. 7, 2019: &lt;a href="https://www.tensorflow.org/alpha" rel="nofollow"&gt;Tensorflow 2.0 Alpha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jan. 11, 2019: &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf" rel="nofollow"&gt;TensorFlow r2.0 preview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Aug. 14, 2018: &lt;a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/bgug1G6a89A" rel="nofollow"&gt;TensorFlow 2.0 is coming&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;p&gt;make sure you are using python 3.x.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU install&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;pip install tensorflow &lt;span class="pl-k"&gt;-&lt;/span&gt;U&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;GPU install&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Install &lt;code&gt;CUDA 10.0&lt;/code&gt;(or after) and &lt;code&gt;cudnn&lt;/code&gt; by yourself. and set &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; up.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;pip install tensorflow&lt;span class="pl-k"&gt;-&lt;/span&gt;gpu  &lt;span class="pl-k"&gt;-&lt;/span&gt;U&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Test installation:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;In [&lt;span class="pl-c1"&gt;2&lt;/span&gt;]: &lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow  &lt;span class="pl-k"&gt;as&lt;/span&gt; tf

In [&lt;span class="pl-c1"&gt;3&lt;/span&gt;]: tf.&lt;span class="pl-c1"&gt;__version__&lt;/span&gt;
Out[&lt;span class="pl-c1"&gt;3&lt;/span&gt;]: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;2.0.0&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
In [&lt;span class="pl-c1"&gt;4&lt;/span&gt;]: tf.test.is_gpu_available()
&lt;span class="pl-c1"&gt;...&lt;/span&gt;
totalMemory: &lt;span class="pl-c1"&gt;3.&lt;/span&gt;&lt;span class="pl-ii"&gt;95GiB&lt;/span&gt; freeMemory: &lt;span class="pl-c1"&gt;3.&lt;/span&gt;&lt;span class="pl-ii"&gt;00GiB&lt;/span&gt;
&lt;span class="pl-c1"&gt;...&lt;/span&gt;
Out[&lt;span class="pl-c1"&gt;4&lt;/span&gt;]: &lt;span class="pl-c1"&gt;True&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-配套tf2视频教程" class="anchor" aria-hidden="true" href="#配套tf2视频教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;配套TF2视频教程&lt;/h1&gt;
&lt;p align="center"&gt;
  &lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;
    &lt;img src="res/cover.png" width="400" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;
    &lt;img src="res/TF_QR_163.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/p&gt; 
&lt;p&gt;TensorFlow 2.0的视频教程链接：&lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;深度学习与TensorFlow 2实战&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;爱可可-爱生活 友情推荐 &lt;a target="_blank" rel="noopener noreferrer" href="res/weibo.jpg"&gt;&lt;img src="res/weibo.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-includes" class="anchor" aria-hidden="true" href="#includes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Includes&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow 2.0 Overview&lt;/li&gt;
&lt;li&gt;TensorFlow 2.0 Basic Usage&lt;/li&gt;
&lt;li&gt;Linear Regression&lt;/li&gt;
&lt;li&gt;MNIST, FashionMNIST&lt;/li&gt;
&lt;li&gt;CIFAR10&lt;/li&gt;
&lt;li&gt;Fully Connected Layer&lt;/li&gt;
&lt;li&gt;VGG16&lt;/li&gt;
&lt;li&gt;Inception Network&lt;/li&gt;
&lt;li&gt;ResNet18&lt;/li&gt;
&lt;li&gt;Naive RNN&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;ColorBot&lt;/li&gt;
&lt;li&gt;Auto-Encoders&lt;/li&gt;
&lt;li&gt;Variational Auto-Encoders&lt;/li&gt;
&lt;li&gt;DCGAN&lt;/li&gt;
&lt;li&gt;CycleGAN&lt;/li&gt;
&lt;li&gt;WGAN&lt;/li&gt;
&lt;li&gt;Pixel2Pixel&lt;/li&gt;
&lt;li&gt;Faster RCNN&lt;/li&gt;
&lt;li&gt;A2C&lt;/li&gt;
&lt;li&gt;GPT&lt;/li&gt;
&lt;li&gt;BERT&lt;/li&gt;
&lt;li&gt;GCN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feel free to submit a &lt;strong&gt;PR&lt;/strong&gt; request to make this repo. more complete!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-refered-repos" class="anchor" aria-hidden="true" href="#refered-repos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Refered Repos.&lt;/h1&gt;
&lt;p&gt;Our work is not built from scratch. Great appreciation to these open works！&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/madalinabuzau/tensorflow-eager-tutorials"&gt;https://github.com/madalinabuzau/tensorflow-eager-tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/herbiebradley/CycleGAN-Tensorflow"&gt;https://github.com/herbiebradley/CycleGAN-Tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb"&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/moono/tf-eager-on-GAN"&gt;https://github.com/moono/tf-eager-on-GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Viredery/tf-eager-fasterrcnn"&gt;https://github.com/Viredery/tf-eager-fasterrcnn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/github/gitignore/blob/master/Python.gitignore"&gt;https://github.com/github/gitignore/blob/master/Python.gitignore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dragen1860</author><guid isPermaLink="false">https://github.com/dragen1860/TensorFlow-2.x-Tutorials</guid><pubDate>Thu, 28 Nov 2019 00:03:00 GMT</pubDate></item><item><title>jackfrued/Python-100-Days #4 in Jupyter Notebook, This month</title><link>https://github.com/jackfrued/Python-100-Days</link><description>&lt;p&gt;&lt;i&gt;Python - 100天从新手到大师&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-python---100天从新手到大师" class="anchor" aria-hidden="true" href="#python---100天从新手到大师"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python - 100天从新手到大师&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;作者：骆昊&lt;/p&gt;
&lt;p&gt;最近有很多想学习Python的小伙伴陆陆续续加入我们的交流群，目前我们的交流群人数已经超过一万人。我们的目标是打造一个优质的Python交流社区，一方面为想学习Python的初学者扫平入门过程中的重重障碍；另一方为新入行的开发者提供问道的途径，帮助他们迅速成长为优秀的职业人；此外，有经验的开发者可以利用这个平台把自己的工作经验无偿分享或有偿提供出来，让大家都能够得到职业技能以及综合素质的全面提升。之前的公开课和线下技术交流活动因为工作的关系荒废了一段时间了，但是各位小伙伴仍然活跃在交流群并一如既往的支持我们，在此向大家表示感谢。近期开始持续更新前15天和最后10天的内容，前15天是写给初学者的，我希望把上手的难度进一步降低，例子程序更加简单清晰；最后10天是Python项目实战和面试相关的东西，我希望内容更详实和完整，尤其是第100天的面试题部分；创作不易，感谢大家的打赏支持，这些钱不会用于购买咖啡而是通过腾讯公益平台捐赠给需要帮助的人（&lt;a href="./%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97.md"&gt;点击&lt;/a&gt;了解捐赠情况）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-qq-group.png"&gt;&lt;img src="./res/python-qq-group.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python应用领域和就业形势分析" class="anchor" aria-hidden="true" href="#python应用领域和就业形势分析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python应用领域和就业形势分析&lt;/h3&gt;
&lt;p&gt;简单的说，Python是一个“优雅”、“明确”、“简单”的编程语言。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;学习曲线低，非专业人士也能上手&lt;/li&gt;
&lt;li&gt;开源系统，拥有强大的生态圈&lt;/li&gt;
&lt;li&gt;解释型语言，完美的平台可移植性&lt;/li&gt;
&lt;li&gt;支持面向对象和函数式编程&lt;/li&gt;
&lt;li&gt;能够通过调用C/C++代码扩展功能&lt;/li&gt;
&lt;li&gt;代码规范程度高，可读性强&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前几个比较流行的领域，Python都有用武之地。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;云基础设施 - Python / Java / Go&lt;/li&gt;
&lt;li&gt;DevOps - Python / Shell / Ruby / Go&lt;/li&gt;
&lt;li&gt;网络爬虫 - Python / PHP / C++&lt;/li&gt;
&lt;li&gt;数据分析挖掘 - Python / R / Scala / Matlab&lt;/li&gt;
&lt;li&gt;机器学习 - Python / R / Java / Lisp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作为一名Python开发者，主要的就业领域包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python服务器后台开发 / 游戏服务器开发 / 数据接口开发工程师&lt;/li&gt;
&lt;li&gt;Python自动化运维工程师&lt;/li&gt;
&lt;li&gt;Python数据分析 / 数据可视化 / 大数据工程师&lt;/li&gt;
&lt;li&gt;Python爬虫工程师&lt;/li&gt;
&lt;li&gt;Python聊天机器人开发 / 图像识别和视觉算法 / 深度学习工程师&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图显示了主要城市Python招聘需求量及薪资待遇排行榜（截止到2018年5月）。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-top-10.png"&gt;&lt;img src="./res/python-top-10.png" alt="Python招聘需求及薪资待遇Top 10" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-bj-salary.png"&gt;&lt;img src="./res/python-bj-salary.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-salary-chengdu.png"&gt;&lt;img src="./res/python-salary-chengdu.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;给初学者的几个建议：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make English as your working language.&lt;/li&gt;
&lt;li&gt;Practice makes perfect.&lt;/li&gt;
&lt;li&gt;All experience comes from mistakes.&lt;/li&gt;
&lt;li&gt;Don't be one of the leeches.&lt;/li&gt;
&lt;li&gt;Either stand out or kicked out.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day0115---python语言基础" class="anchor" aria-hidden="true" href="#day0115---python语言基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day01~15 - &lt;a href="./Day01-15"&gt;Python语言基础&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day01---初识python" class="anchor" aria-hidden="true" href="#day01---初识python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day01 - &lt;a href="./Day01-15/01.%E5%88%9D%E8%AF%86Python.md"&gt;初识Python&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Python简介 - Python的历史 / Python的优缺点 / Python的应用领域&lt;/li&gt;
&lt;li&gt;搭建编程环境 - Windows环境 / Linux环境 / MacOS环境&lt;/li&gt;
&lt;li&gt;从终端运行Python程序 - Hello, world / print函数 / 运行程序&lt;/li&gt;
&lt;li&gt;使用IDLE - 交互式环境(REPL) / 编写多行代码 / 运行程序 / 退出IDLE&lt;/li&gt;
&lt;li&gt;注释 - 注释的作用 / 单行注释 / 多行注释&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day02---语言元素" class="anchor" aria-hidden="true" href="#day02---语言元素"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day02 - &lt;a href="./Day01-15/02.%E8%AF%AD%E8%A8%80%E5%85%83%E7%B4%A0.md"&gt;语言元素&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;程序和进制 - 指令和程序 / 冯诺依曼机 / 二进制和十进制 / 八进制和十六进制&lt;/li&gt;
&lt;li&gt;变量和类型 - 变量的命名 / 变量的使用 / input函数 / 检查变量类型 / 类型转换&lt;/li&gt;
&lt;li&gt;数字和字符串 - 整数 / 浮点数 / 复数 / 字符串 / 字符串基本操作 / 字符编码&lt;/li&gt;
&lt;li&gt;运算符 - 数学运算符 / 赋值运算符 / 比较运算符 / 逻辑运算符 / 身份运算符 / 运算符的优先级&lt;/li&gt;
&lt;li&gt;应用案例 - 华氏温度转换成摄氏温度 / 输入圆的半径计算周长和面积 / 输入年份判断是否是闰年&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day03---分支结构" class="anchor" aria-hidden="true" href="#day03---分支结构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day03 - &lt;a href="./Day01-15/03.%E5%88%86%E6%94%AF%E7%BB%93%E6%9E%84.md"&gt;分支结构&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;分支结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图&lt;/li&gt;
&lt;li&gt;if语句 - 简单的if / if-else结构 / if-elif-else结构 / 嵌套的if&lt;/li&gt;
&lt;li&gt;应用案例 - 用户身份验证 / 英制单位与公制单位互换 / 掷骰子决定做什么 / 百分制成绩转等级制 / 分段函数求值 / 输入三条边的长度如果能构成三角形就计算周长和面积&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day04---循环结构" class="anchor" aria-hidden="true" href="#day04---循环结构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day04 - &lt;a href="./Day01-15/04.%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84.md"&gt;循环结构&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;循环结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图&lt;/li&gt;
&lt;li&gt;while循环 - 基本结构 / break语句 / continue语句&lt;/li&gt;
&lt;li&gt;for循环 - 基本结构 / range类型 / 循环中的分支结构 / 嵌套的循环 / 提前结束程序&lt;/li&gt;
&lt;li&gt;应用案例 - 1~100求和 / 判断素数 / 猜数字游戏 / 打印九九表 / 打印三角形图案 / 猴子吃桃 / 百钱百鸡&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day05---构造程序逻辑" class="anchor" aria-hidden="true" href="#day05---构造程序逻辑"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day05 - &lt;a href="./Day01-15/05.%E6%9E%84%E9%80%A0%E7%A8%8B%E5%BA%8F%E9%80%BB%E8%BE%91.md"&gt;构造程序逻辑&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;经典案例：水仙花数 / 百钱百鸡 / Craps赌博游戏&lt;/li&gt;
&lt;li&gt;练习题目：斐波那契数列 / 完美数 / 素数&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day06---函数和模块的使用" class="anchor" aria-hidden="true" href="#day06---函数和模块的使用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day06 - &lt;a href="./Day01-15/06.%E5%87%BD%E6%95%B0%E5%92%8C%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8.md"&gt;函数和模块的使用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;函数的作用 - 代码的坏味道 / 用函数封装功能模块&lt;/li&gt;
&lt;li&gt;定义函数 - def语句 / 函数名 / 参数列表 / return语句 / 调用自定义函数&lt;/li&gt;
&lt;li&gt;调用函数 - Python内置函数 /  导入模块和函数&lt;/li&gt;
&lt;li&gt;函数的参数 - 默认参数 / 可变参数 / 关键字参数 / 命名关键字参数&lt;/li&gt;
&lt;li&gt;函数的返回值 - 没有返回值  / 返回单个值 / 返回多个值&lt;/li&gt;
&lt;li&gt;作用域问题 - 局部作用域 / 嵌套作用域 / 全局作用域 / 内置作用域 / 和作用域相关的关键字&lt;/li&gt;
&lt;li&gt;用模块管理函数 - 模块的概念 / 用自定义模块管理函数 / 命名冲突的时候会怎样（同一个模块和不同的模块）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day07---字符串和常用数据结构" class="anchor" aria-hidden="true" href="#day07---字符串和常用数据结构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day07 - &lt;a href="./Day01-15/07.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.md"&gt;字符串和常用数据结构&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;字符串的使用 - 计算长度 / 下标运算 / 切片 / 常用方法&lt;/li&gt;
&lt;li&gt;列表基本用法 - 定义列表 / 用下表访问元素 / 下标越界 / 添加元素 / 删除元素 / 修改元素 / 切片 / 循环遍历&lt;/li&gt;
&lt;li&gt;列表常用操作 - 连接 / 复制(复制元素和复制数组) / 长度 / 排序 / 倒转 / 查找&lt;/li&gt;
&lt;li&gt;生成列表 - 使用range创建数字列表 / 生成表达式 / 生成器&lt;/li&gt;
&lt;li&gt;元组的使用 - 定义元组 / 使用元组中的值 / 修改元组变量 / 元组和列表转换&lt;/li&gt;
&lt;li&gt;集合基本用法 - 集合和列表的区别 /  创建集合 / 添加元素 / 删除元素 /  清空&lt;/li&gt;
&lt;li&gt;集合常用操作 - 交集 / 并集 / 差集 / 对称差 / 子集 / 超集&lt;/li&gt;
&lt;li&gt;字典的基本用法 - 字典的特点 / 创建字典 / 添加元素 / 删除元素 / 取值 / 清空&lt;/li&gt;
&lt;li&gt;字典常用操作 - keys()方法 / values()方法 / items()方法 / setdefault()方法&lt;/li&gt;
&lt;li&gt;基础练习 - 跑马灯效果 / 列表找最大元素 / 统计考试成绩的平均分 / Fibonacci数列 / 杨辉三角&lt;/li&gt;
&lt;li&gt;综合案例 - 双色球选号 / 井字棋&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day08---面向对象编程基础" class="anchor" aria-hidden="true" href="#day08---面向对象编程基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day08 - &lt;a href="./Day01-15/08.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80.md"&gt;面向对象编程基础&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;类和对象 - 什么是类 / 什么是对象 / 面向对象其他相关概念&lt;/li&gt;
&lt;li&gt;定义类 - 基本结构 / 属性和方法 / 构造器 / 析构器 / __str__方法&lt;/li&gt;
&lt;li&gt;使用对象 - 创建对象 / 给对象发消息&lt;/li&gt;
&lt;li&gt;面向对象的四大支柱 - 抽象 / 封装 / 继承 / 多态&lt;/li&gt;
&lt;li&gt;基础练习 - 定义学生类 / 定义时钟类 / 定义图形类 / 定义汽车类&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day09---面向对象进阶" class="anchor" aria-hidden="true" href="#day09---面向对象进阶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day09 - &lt;a href="./Day01-15/09.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BF%9B%E9%98%B6.md"&gt;面向对象进阶&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;属性 - 类属性 / 实例属性 / 属性访问器 / 属性修改器 / 属性删除器 / 使用__slots__&lt;/li&gt;
&lt;li&gt;类中的方法 - 实例方法 / 类方法 / 静态方法&lt;/li&gt;
&lt;li&gt;运算符重载 - __add__ / __sub__ / __or__ /__getitem__ / __setitem__ / __len__ / __repr__ / __gt__ / __lt__ / __le__ / __ge__ / __eq__ / __ne__ / __contains__&lt;/li&gt;
&lt;li&gt;类(的对象)之间的关系 - 关联 / 继承 / 依赖&lt;/li&gt;
&lt;li&gt;继承和多态 - 什么是继承 / 继承的语法 / 调用父类方法 / 方法重写 / 类型判定 / 多重继承 / 菱形继承(钻石继承)和C3算法&lt;/li&gt;
&lt;li&gt;综合案例 - 工资结算系统 / 图书自动折扣系统 / 自定义分数类&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day10---图形用户界面和游戏开发" class="anchor" aria-hidden="true" href="#day10---图形用户界面和游戏开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day10 - &lt;a href="./Day01-15/10.%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E5%92%8C%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91.md"&gt;图形用户界面和游戏开发&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;使用tkinter开发GUI程序&lt;/li&gt;
&lt;li&gt;使用pygame三方库开发游戏应用&lt;/li&gt;
&lt;li&gt;“大球吃小球”游戏&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day11---文件和异常" class="anchor" aria-hidden="true" href="#day11---文件和异常"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day11 - &lt;a href="./Day01-15/11.%E6%96%87%E4%BB%B6%E5%92%8C%E5%BC%82%E5%B8%B8.md"&gt;文件和异常&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;读文件 - 读取整个文件 / 逐行读取 / 文件路径&lt;/li&gt;
&lt;li&gt;写文件 - 覆盖写入 / 追加写入 / 文本文件 / 二进制文件&lt;/li&gt;
&lt;li&gt;异常处理 - 异常机制的重要性 / try-except代码块 / else代码块 / finally代码块 / 内置异常类型 / 异常栈 / raise语句&lt;/li&gt;
&lt;li&gt;数据持久化 - CSV文件概述 / csv模块的应用 / JSON数据格式 / json模块的应用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day12---字符串和正则表达式" class="anchor" aria-hidden="true" href="#day12---字符串和正则表达式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day12 - &lt;a href="./Day01-15/12.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.md"&gt;字符串和正则表达式&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;字符串高级操作 - 转义字符 / 原始字符串 / 多行字符串 / in和 not in运算符 / is开头的方法 / join和split方法 / strip相关方法 / pyperclip模块 / 不变字符串和可变字符串 / StringIO的使用&lt;/li&gt;
&lt;li&gt;正则表达式入门 - 正则表达式的作用 / 元字符 / 转义 / 量词 / 分组 / 零宽断言 /贪婪匹配与惰性匹配懒惰 / 使用re模块实现正则表达式操作（匹配、搜索、替换、捕获）&lt;/li&gt;
&lt;li&gt;使用正则表达式 - re模块 / compile函数 / group和groups方法 / match方法 / search方法 / findall和finditer方法 / sub和subn方法 / split方法&lt;/li&gt;
&lt;li&gt;应用案例 - 使用正则表达式验证输入的字符串&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day13---进程和线程" class="anchor" aria-hidden="true" href="#day13---进程和线程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day13 - &lt;a href="./Day01-15/13.%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B.md"&gt;进程和线程&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;进程和线程的概念 - 什么是进程 / 什么是线程 / 多线程的应用场景&lt;/li&gt;
&lt;li&gt;使用进程 - fork函数 / multiprocessing模块 / 进程池 / 进程间通信&lt;/li&gt;
&lt;li&gt;使用线程 - thread模块 / threading模块 / Thread类 / Lock类 / Condition类 / 线程池&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day14---网络编程入门和网络应用开发" class="anchor" aria-hidden="true" href="#day14---网络编程入门和网络应用开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day14 - &lt;a href="./Day01-15/14.%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%E5%92%8C%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91.md"&gt;网络编程入门和网络应用开发&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;计算机网络基础 - 计算机网络发展史 / “TCP-IP”模型 / IP地址 / 端口 / 协议 / 其他相关概念&lt;/li&gt;
&lt;li&gt;网络应用模式 - “客户端-服务器”模式 / “浏览器-服务器”模式&lt;/li&gt;
&lt;li&gt;基于HTTP协议访问网络资源 - 网络API概述 / 访问URL / requests模块 / 解析JSON格式数据&lt;/li&gt;
&lt;li&gt;Python网络编程 - 套接字的概念 / socket模块 /  socket函数 / 创建TCP服务器 / 创建TCP客户端 / 创建UDP服务器 / 创建UDP客户端 / SocketServer模块&lt;/li&gt;
&lt;li&gt;电子邮件 - SMTP协议 / POP3协议 / IMAP协议 / smtplib模块 / poplib模块 / imaplib模块&lt;/li&gt;
&lt;li&gt;短信服务 - 调用短信服务网关&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day15---图像和文档处理" class="anchor" aria-hidden="true" href="#day15---图像和文档处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day15 - &lt;a href="./Day01-15/15.%E5%9B%BE%E5%83%8F%E5%92%8C%E5%8A%9E%E5%85%AC%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86.md"&gt;图像和文档处理&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;用Pillow处理图片 - 图片读写 / 图片合成 / 几何变换 / 色彩转换 / 滤镜效果&lt;/li&gt;
&lt;li&gt;读写Word文档 - 文本内容的处理 / 段落 / 页眉和页脚 / 样式的处理&lt;/li&gt;
&lt;li&gt;读写Excel文件 - xlrd模块 / xlwt模块&lt;/li&gt;
&lt;li&gt;生成PDF文件 - pypdf2模块 / reportlab模块&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day16day20---python语言进阶-" class="anchor" aria-hidden="true" href="#day16day20---python语言进阶-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day16~Day20 - &lt;a href="./Day16-20/16-20.Python%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6.md"&gt;Python语言进阶 &lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;常用数据结构&lt;/li&gt;
&lt;li&gt;函数的高级用法 - “一等公民” / 高阶函数 / Lambda函数 / 作用域和闭包 / 装饰器&lt;/li&gt;
&lt;li&gt;面向对象高级知识 - “三大支柱” / 类与类之间的关系 / 垃圾回收 / 魔术属性和方法 / 混入 / 元类 / 面向对象设计原则 / GoF设计模式&lt;/li&gt;
&lt;li&gt;迭代器和生成器 - 相关魔术方法 / 创建生成器的两种方式 /&lt;/li&gt;
&lt;li&gt;并发和异步编程 - 多线程 / 多进程 / 异步IO / async和await&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day2130---web前端入门" class="anchor" aria-hidden="true" href="#day2130---web前端入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day21~30 - &lt;a href="./Day21-30/21-30.Web%E5%89%8D%E7%AB%AF%E6%A6%82%E8%BF%B0.md"&gt;Web前端入门&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;用HTML标签承载页面内容&lt;/li&gt;
&lt;li&gt;用CSS渲染页面&lt;/li&gt;
&lt;li&gt;用JavaScript处理交互式行为&lt;/li&gt;
&lt;li&gt;jQuery入门和提高&lt;/li&gt;
&lt;li&gt;Vue.js入门&lt;/li&gt;
&lt;li&gt;Element的使用&lt;/li&gt;
&lt;li&gt;Bootstrap的使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day3135---玩转linux操作系统" class="anchor" aria-hidden="true" href="#day3135---玩转linux操作系统"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day31~35 - &lt;a href="./Day31-35/31-35.%E7%8E%A9%E8%BD%ACLinux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.md"&gt;玩转Linux操作系统&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;操作系统发展史和Linux概述&lt;/li&gt;
&lt;li&gt;Linux基础命令&lt;/li&gt;
&lt;li&gt;Linux中的实用程序&lt;/li&gt;
&lt;li&gt;Linux的文件系统&lt;/li&gt;
&lt;li&gt;Vim编辑器的应用&lt;/li&gt;
&lt;li&gt;环境变量和Shell编程&lt;/li&gt;
&lt;li&gt;软件的安装和服务的配置&lt;/li&gt;
&lt;li&gt;网络访问和管理&lt;/li&gt;
&lt;li&gt;其他相关内容&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day3640---数据库基础和进阶" class="anchor" aria-hidden="true" href="#day3640---数据库基础和进阶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day36~40 - &lt;a href="./Day36-40"&gt;数据库基础和进阶&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="./Day36-40/36-38.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93MySQL.md"&gt;关系型数据库MySQL&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;关系型数据库概述&lt;/li&gt;
&lt;li&gt;MySQL的安装和使用&lt;/li&gt;
&lt;li&gt;SQL的使用
&lt;ul&gt;
&lt;li&gt;DDL - 数据定义语言 - create / drop / alter&lt;/li&gt;
&lt;li&gt;DML - 数据操作语言 - insert / delete / update / select&lt;/li&gt;
&lt;li&gt;DCL - 数据控制语言 - grant / revoke&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;相关知识
&lt;ul&gt;
&lt;li&gt;范式理论 - 设计二维表的指导思想&lt;/li&gt;
&lt;li&gt;数据完整性&lt;/li&gt;
&lt;li&gt;数据一致性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在Python中操作MySQL&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="./Day36-40/39-40.NoSQL%E5%85%A5%E9%97%A8.md"&gt;NoSQL入门&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;NoSQL概述&lt;/li&gt;
&lt;li&gt;Redis概述&lt;/li&gt;
&lt;li&gt;Mongo概述&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day4155---实战django" class="anchor" aria-hidden="true" href="#day4155---实战django"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day41~55 - &lt;a href="./Day41-55"&gt;实战Django&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day41---快速上手" class="anchor" aria-hidden="true" href="#day41---快速上手"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day41 - &lt;a href="./Day41-55/41.%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.md"&gt;快速上手&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Web应用工作原理和HTTP协议&lt;/li&gt;
&lt;li&gt;Django框架概述&lt;/li&gt;
&lt;li&gt;5分钟快速上手&lt;/li&gt;
&lt;li&gt;使用视图模板&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day42---深入模型" class="anchor" aria-hidden="true" href="#day42---深入模型"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day42 - &lt;a href="./Day41-55/42.%E6%B7%B1%E5%85%A5%E6%A8%A1%E5%9E%8B.md"&gt;深入模型&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;关系型数据库配置&lt;/li&gt;
&lt;li&gt;管理后台的使用&lt;/li&gt;
&lt;li&gt;使用ORM完成对模型的CRUD操作&lt;/li&gt;
&lt;li&gt;Django模型最佳实践&lt;/li&gt;
&lt;li&gt;模型定义参考&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day43---静态资源和ajax请求" class="anchor" aria-hidden="true" href="#day43---静态资源和ajax请求"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day43 - &lt;a href="./Day41-55/43.%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%92%8CAjax%E8%AF%B7%E6%B1%82.md"&gt;静态资源和Ajax请求&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;加载静态资源&lt;/li&gt;
&lt;li&gt;用Ajax请求获取数据&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day44---表单的应用" class="anchor" aria-hidden="true" href="#day44---表单的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day44 - &lt;a href="./Day41-55/44.%E8%A1%A8%E5%8D%95%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;表单的应用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;表单和表单控件&lt;/li&gt;
&lt;li&gt;跨站请求伪造和CSRF令牌&lt;/li&gt;
&lt;li&gt;Form和ModelForm&lt;/li&gt;
&lt;li&gt;表单验证&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day45---cookie和session" class="anchor" aria-hidden="true" href="#day45---cookie和session"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day45 - &lt;a href="./Day41-55/45.Cookie%E5%92%8CSession.md"&gt;Cookie和Session&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;实现用户跟踪&lt;/li&gt;
&lt;li&gt;cookie和session的关系&lt;/li&gt;
&lt;li&gt;Django框架对session的支持&lt;/li&gt;
&lt;li&gt;视图函数中的cookie读写操作&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day46---报表和日志" class="anchor" aria-hidden="true" href="#day46---报表和日志"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day46 - &lt;a href="./Day41-55/46.%E6%8A%A5%E8%A1%A8%E5%92%8C%E6%97%A5%E5%BF%97.md"&gt;报表和日志&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;通过HttpResponse修改响应头&lt;/li&gt;
&lt;li&gt;使用StreamingHttpResponse处理大文件&lt;/li&gt;
&lt;li&gt;使用xlwt生成Excel报表&lt;/li&gt;
&lt;li&gt;使用reportlab生成PDF报表&lt;/li&gt;
&lt;li&gt;使用ECharts生成前端图表&lt;/li&gt;
&lt;li&gt;配置日志和Django-Debug-Toolbar&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day47---中间件的应用" class="anchor" aria-hidden="true" href="#day47---中间件的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day47 - &lt;a href="./Day41-55/47.%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;中间件的应用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;什么是中间件&lt;/li&gt;
&lt;li&gt;Django框架内置的中间件&lt;/li&gt;
&lt;li&gt;自定义中间件及其应用场景&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day48---前后端分离开发入门" class="anchor" aria-hidden="true" href="#day48---前后端分离开发入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day48 - &lt;a href="./Day41-55/48.%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8.md"&gt;前后端分离开发入门&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;返回JSON格式的数据&lt;/li&gt;
&lt;li&gt;用Vue.js渲染页面&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day49---restful架构和drf入门" class="anchor" aria-hidden="true" href="#day49---restful架构和drf入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day49 - &lt;a href="./Day41-55/49.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E5%85%A5%E9%97%A8.md"&gt;RESTful架构和DRF入门&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day50---restful架构和drf进阶" class="anchor" aria-hidden="true" href="#day50---restful架构和drf进阶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day50 - &lt;a href="./Day41-55/50.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E8%BF%9B%E9%98%B6.md"&gt;RESTful架构和DRF进阶&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day51---使用缓存" class="anchor" aria-hidden="true" href="#day51---使用缓存"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day51 - &lt;a href="./Day41-55/51.%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98.md"&gt;使用缓存&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;网站优化第一定律&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在Django项目中使用Redis提供缓存服务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在视图函数中读写缓存&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用装饰器实现页面缓存&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为数据接口提供缓存服务&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day52---文件上传和富文本编辑" class="anchor" aria-hidden="true" href="#day52---文件上传和富文本编辑"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day52 - &lt;a href="./Day41-55/52.%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%92%8C%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8.md"&gt;文件上传和富文本编辑&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;文件上传表单控件和图片文件预览&lt;/li&gt;
&lt;li&gt;服务器端如何处理上传的文件&lt;/li&gt;
&lt;li&gt;富文本编辑器概述&lt;/li&gt;
&lt;li&gt;wangEditor的使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day53---短信和邮件" class="anchor" aria-hidden="true" href="#day53---短信和邮件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day53 - &lt;a href="./Day41-55/53.%E7%9F%AD%E4%BF%A1%E5%92%8C%E9%82%AE%E4%BB%B6.md"&gt;短信和邮件&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;常用短信网关平台介绍&lt;/li&gt;
&lt;li&gt;使用螺丝帽发送短信&lt;/li&gt;
&lt;li&gt;Django框架对邮件服务的支持&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day54---异步任务和定时任务" class="anchor" aria-hidden="true" href="#day54---异步任务和定时任务"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day54 - &lt;a href="./Day41-55/54.%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.md"&gt;异步任务和定时任务&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;网站优化第二定律&lt;/li&gt;
&lt;li&gt;配置消息队列服务&lt;/li&gt;
&lt;li&gt;在项目中使用celery实现任务异步化&lt;/li&gt;
&lt;li&gt;在项目中使用celery实现定时任务&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day55---单元测试和项目上线" class="anchor" aria-hidden="true" href="#day55---单元测试和项目上线"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day55 - &lt;a href="./Day41-55/55.%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E5%92%8C%E9%A1%B9%E7%9B%AE%E4%B8%8A%E7%BA%BF.md"&gt;单元测试和项目上线&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Python中的单元测试&lt;/li&gt;
&lt;li&gt;Django框架对单元测试的支持&lt;/li&gt;
&lt;li&gt;使用版本控制系统&lt;/li&gt;
&lt;li&gt;配置和使用uWSGI&lt;/li&gt;
&lt;li&gt;动静分离和Nginx配置&lt;/li&gt;
&lt;li&gt;配置HTTPS&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day5660---实战flask" class="anchor" aria-hidden="true" href="#day5660---实战flask"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day56~60 - &lt;a href="./Day56-65"&gt;实战Flask&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day56---flask入门" class="anchor" aria-hidden="true" href="#day56---flask入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day56 - &lt;a href="./Day56-60/56.Flask%E5%85%A5%E9%97%A8.md"&gt;Flask入门&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day57---模板的使用" class="anchor" aria-hidden="true" href="#day57---模板的使用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day57 - &lt;a href="./Day56-60/57.%E6%A8%A1%E6%9D%BF%E7%9A%84%E4%BD%BF%E7%94%A8.md"&gt;模板的使用&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day58---表单的处理" class="anchor" aria-hidden="true" href="#day58---表单的处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day58 - &lt;a href="./Day56-60/58.%E8%A1%A8%E5%8D%95%E7%9A%84%E5%A4%84%E7%90%86.md"&gt;表单的处理&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day59---数据库操作" class="anchor" aria-hidden="true" href="#day59---数据库操作"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day59 - &lt;a href="./Day56-60/59.%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C.md"&gt;数据库操作&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day60---项目实战" class="anchor" aria-hidden="true" href="#day60---项目实战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day60 - &lt;a href="./Day56-60/60.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;项目实战&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-day6165---实战tornado" class="anchor" aria-hidden="true" href="#day6165---实战tornado"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day61~65 - &lt;a href="./Day61-65"&gt;实战Tornado&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day61---预备知识" class="anchor" aria-hidden="true" href="#day61---预备知识"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day61 - &lt;a href="./Day61-65/61.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86.md"&gt;预备知识&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;并发编程&lt;/li&gt;
&lt;li&gt;I/O模式和事件驱动&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day62---tornado入门" class="anchor" aria-hidden="true" href="#day62---tornado入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day62 - &lt;a href="./Day61-65/62.Tornado%E5%85%A5%E9%97%A8.md"&gt;Tornado入门&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Tornado概述&lt;/li&gt;
&lt;li&gt;5分钟上手Tornado&lt;/li&gt;
&lt;li&gt;路由解析&lt;/li&gt;
&lt;li&gt;请求处理器&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day63---异步化" class="anchor" aria-hidden="true" href="#day63---异步化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day63 - &lt;a href="./Day61-65/63.%E5%BC%82%E6%AD%A5%E5%8C%96.md"&gt;异步化&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;aiomysql和aioredis的使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day64---websocket的应用" class="anchor" aria-hidden="true" href="#day64---websocket的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day64 - &lt;a href="./Day61-65/64.WebSocket%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;WebSocket的应用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;WebSocket简介&lt;/li&gt;
&lt;li&gt;WebSocket服务器端编程&lt;/li&gt;
&lt;li&gt;WebSocket客户端编程&lt;/li&gt;
&lt;li&gt;项目：Web聊天室&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day65---项目实战" class="anchor" aria-hidden="true" href="#day65---项目实战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day65 - &lt;a href="./Day61-65/65.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;项目实战&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;前后端分离开发和接口文档的撰写&lt;/li&gt;
&lt;li&gt;使用Vue.js实现前端渲染&lt;/li&gt;
&lt;li&gt;使用ECharts实现报表功能&lt;/li&gt;
&lt;li&gt;使用WebSocket实现推送服务&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day6675---爬虫开发" class="anchor" aria-hidden="true" href="#day6675---爬虫开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day66~75 - &lt;a href="./Day66-75"&gt;爬虫开发&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day66---网络爬虫和相关工具" class="anchor" aria-hidden="true" href="#day66---网络爬虫和相关工具"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day66 - &lt;a href="./Day66-75/66.%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%92%8C%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7.md"&gt;网络爬虫和相关工具&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;网络爬虫的概念及其应用领域&lt;/li&gt;
&lt;li&gt;网络爬虫的合法性探讨&lt;/li&gt;
&lt;li&gt;开发网络爬虫的相关工具&lt;/li&gt;
&lt;li&gt;一个爬虫程序的构成&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day67---数据采集和解析" class="anchor" aria-hidden="true" href="#day67---数据采集和解析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day67 - &lt;a href="./Day66-75/67.%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%92%8C%E8%A7%A3%E6%9E%90.md"&gt;数据采集和解析&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;数据采集的标准和三方库&lt;/li&gt;
&lt;li&gt;页面解析的三种方式：正则表达式解析 / XPath解析 / CSS选择器解析&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day68---存储数据" class="anchor" aria-hidden="true" href="#day68---存储数据"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day68 - &lt;a href="./Day66-75/68.%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE.md"&gt;存储数据&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;如何存储海量数据&lt;/li&gt;
&lt;li&gt;实现数据的缓存&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day69---并发下载" class="anchor" aria-hidden="true" href="#day69---并发下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day69 - &lt;a href="./Day66-75/69.%E5%B9%B6%E5%8F%91%E4%B8%8B%E8%BD%BD.md"&gt;并发下载&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;多线程和多进程&lt;/li&gt;
&lt;li&gt;异步I/O和协程&lt;/li&gt;
&lt;li&gt;async和await关键字的使用&lt;/li&gt;
&lt;li&gt;三方库aiohttp的应用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day70---解析动态内容" class="anchor" aria-hidden="true" href="#day70---解析动态内容"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day70 - &lt;a href="./Day66-75/70.%E8%A7%A3%E6%9E%90%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9.md"&gt;解析动态内容&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;JavaScript逆向工程&lt;/li&gt;
&lt;li&gt;使用Selenium获取动态内容&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day71---表单交互和验证码处理" class="anchor" aria-hidden="true" href="#day71---表单交互和验证码处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day71 - &lt;a href="./Day66-75/71.%E8%A1%A8%E5%8D%95%E4%BA%A4%E4%BA%92%E5%92%8C%E9%AA%8C%E8%AF%81%E7%A0%81%E5%A4%84%E7%90%86.md"&gt;表单交互和验证码处理&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;自动提交表单&lt;/li&gt;
&lt;li&gt;Cookie池的应用&lt;/li&gt;
&lt;li&gt;验证码处理&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day72---scrapy入门" class="anchor" aria-hidden="true" href="#day72---scrapy入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day72 - &lt;a href="./Day66-75/72.Scrapy%E5%85%A5%E9%97%A8.md"&gt;Scrapy入门&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Scrapy爬虫框架概述&lt;/li&gt;
&lt;li&gt;安装和使用Scrapy&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day73---scrapy高级应用" class="anchor" aria-hidden="true" href="#day73---scrapy高级应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day73 - &lt;a href="./Day66-75/73.Scrapy%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8.md"&gt;Scrapy高级应用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Spider的用法&lt;/li&gt;
&lt;li&gt;中间件的应用：下载中间件 / 蜘蛛中间件&lt;/li&gt;
&lt;li&gt;Scrapy对接Selenium抓取动态内容&lt;/li&gt;
&lt;li&gt;Scrapy部署到Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day74---scrapy分布式实现" class="anchor" aria-hidden="true" href="#day74---scrapy分布式实现"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day74 - &lt;a href="./Day66-75/74.Scrapy%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0.md"&gt;Scrapy分布式实现&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;分布式爬虫的原理&lt;/li&gt;
&lt;li&gt;Scrapy分布式实现&lt;/li&gt;
&lt;li&gt;使用Scrapyd实现分布式部署&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day75---爬虫项目实战" class="anchor" aria-hidden="true" href="#day75---爬虫项目实战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day75 - &lt;a href="./Day66-75/75.%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;爬虫项目实战&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;爬取招聘网站数据&lt;/li&gt;
&lt;li&gt;爬取房地产行业数据&lt;/li&gt;
&lt;li&gt;爬取二手车交易平台数据&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day7690---数据处理和机器学习" class="anchor" aria-hidden="true" href="#day7690---数据处理和机器学习"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day76~90 - &lt;a href="./Day76-90"&gt;数据处理和机器学习&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day76---机器学习基础" class="anchor" aria-hidden="true" href="#day76---机器学习基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day76 - &lt;a href="./Day76-90/76.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md"&gt;机器学习基础&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day77---pandas的应用" class="anchor" aria-hidden="true" href="#day77---pandas的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day77 - &lt;a href="./Day76-90/77.Pandas%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;Pandas的应用&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day78---numpy和scipy的应用" class="anchor" aria-hidden="true" href="#day78---numpy和scipy的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day78 - &lt;a href="./Day76-90/78.NumPy%E5%92%8CSciPy%E7%9A%84%E5%BA%94%E7%94%A8"&gt;NumPy和SciPy的应用&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day79---matplotlib和数据可视化" class="anchor" aria-hidden="true" href="#day79---matplotlib和数据可视化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day79 - &lt;a href="./Day76-90/79.Matplotlib%E5%92%8C%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"&gt;Matplotlib和数据可视化&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day80---k最近邻knn分类" class="anchor" aria-hidden="true" href="#day80---k最近邻knn分类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day80 - &lt;a href="./Day76-90/80.k%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB.md"&gt;k最近邻(KNN)分类&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day81---决策树" class="anchor" aria-hidden="true" href="#day81---决策树"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day81 - &lt;a href="./Day76-90/81.%E5%86%B3%E7%AD%96%E6%A0%91.md"&gt;决策树&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day82---贝叶斯分类" class="anchor" aria-hidden="true" href="#day82---贝叶斯分类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day82 - &lt;a href="./Day76-90/82.%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB.md"&gt;贝叶斯分类&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day83---支持向量机svm" class="anchor" aria-hidden="true" href="#day83---支持向量机svm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day83 - &lt;a href="./Day76-90/83.%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.md"&gt;支持向量机(SVM)&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day84---k-均值聚类" class="anchor" aria-hidden="true" href="#day84---k-均值聚类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day84 - &lt;a href="./Day76-90/84.K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB.md"&gt;K-均值聚类&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day85---回归分析" class="anchor" aria-hidden="true" href="#day85---回归分析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day85 - &lt;a href="./Day76-90/85.%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90.md"&gt;回归分析&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day86---大数据分析入门" class="anchor" aria-hidden="true" href="#day86---大数据分析入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day86 - &lt;a href="./Day76-90/86.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%85%A5%E9%97%A8.md"&gt;大数据分析入门&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day87---大数据分析进阶" class="anchor" aria-hidden="true" href="#day87---大数据分析进阶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day87 - &lt;a href="./Day76-90/87.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%BF%9B%E9%98%B6.md"&gt;大数据分析进阶&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day88---tensorflow入门" class="anchor" aria-hidden="true" href="#day88---tensorflow入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day88 - &lt;a href="./Day76-90/88.Tensorflow%E5%85%A5%E9%97%A8.md"&gt;Tensorflow入门&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day89---tensorflow实战" class="anchor" aria-hidden="true" href="#day89---tensorflow实战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day89 - &lt;a href="./Day76-90/89.Tensorflow%E5%AE%9E%E6%88%98.md"&gt;Tensorflow实战&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day90---推荐系统" class="anchor" aria-hidden="true" href="#day90---推荐系统"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day90 - &lt;a href="./Day76-90/90.%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.md"&gt;推荐系统&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-day91100---团队项目开发" class="anchor" aria-hidden="true" href="#day91100---团队项目开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day91~100 - &lt;a href="./Day91-100"&gt;团队项目开发&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第91天团队项目开发的问题和解决方案" class="anchor" aria-hidden="true" href="#第91天团队项目开发的问题和解决方案"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第91天：&lt;a href="./Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md"&gt;团队项目开发的问题和解决方案&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;软件过程模型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;经典过程模型（瀑布模型）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可行性分析（研究做还是不做），输出《可行性分析报告》。&lt;/li&gt;
&lt;li&gt;需求分析（研究做什么），输出《需求规格说明书》和产品界面原型图。&lt;/li&gt;
&lt;li&gt;概要设计和详细设计，输出概念模型图、物理模型图、类图、时序图等。&lt;/li&gt;
&lt;li&gt;编码 / 测试。&lt;/li&gt;
&lt;li&gt;上线 / 维护。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;敏捷开发（Scrum）- 产品所有者、Scrum Master、研发人员 - Sprint&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;产品的Backlog（用户故事、产品原型）。&lt;/li&gt;
&lt;li&gt;计划会议（评估和预算）。&lt;/li&gt;
&lt;li&gt;日常开发（站立会议、番茄工作法、结对编程、测试先行、代码重构……）。&lt;/li&gt;
&lt;li&gt;修复bug（问题描述、重现步骤、测试人员、被指派人）。&lt;/li&gt;
&lt;li&gt;评审会议（Showcase）。&lt;/li&gt;
&lt;li&gt;回顾会议（当前周期做得好和不好的地方）。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;补充：敏捷软件开发宣言&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;个体和互动&lt;/strong&gt; 高于 流程和工具&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工作的软件&lt;/strong&gt; 高于 详尽的文档&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;客户合作&lt;/strong&gt; 高于 合同谈判&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;响应变化&lt;/strong&gt; 高于 遵循计划&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/agile-scrum-sprint-cycle.png"&gt;&lt;img src="./res/agile-scrum-sprint-cycle.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;角色：产品所有者（决定做什么，能对需求拍板的人）、团队负责人（解决各种问题，专注如何更好的工作，屏蔽外部对开发团队的影响）、开发团队（项目执行人员，具体指开发人员和测试人员）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;准备工作：商业案例和资金、合同、憧憬、初始产品需求、初始发布计划、入股、组建团队。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;敏捷团队通常人数为8-10人。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;工作量估算：将开发任务量化，包括原型、Logo设计、UI设计、前端开发等，尽量把每个工作分解到最小任务量，最小任务量标准为工作时间不能超过两天，然后估算总体项目时间。把每个任务都贴在白板上面，白板上分三部分：to do（待完成）、in progress（进行中）和done（已完成）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;项目团队组建&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;团队的构成和角色&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;说明：谢谢付祥英女士绘制了下面这张精美的公司组织架构图。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/company_architecture.png"&gt;&lt;img src="./res/company_architecture.png" alt="company_architecture" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;编程规范和代码审查（flake8、pylint）&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/pylint.png"&gt;&lt;img src="./res/pylint.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python中的一些“惯例”（请参考&lt;a href="Python%E6%83%AF%E4%BE%8B.md"&gt;《Python惯例-如何编写Pythonic的代码》&lt;/a&gt;）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;影响代码可读性的原因：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;代码注释太少或者没有注释&lt;/li&gt;
&lt;li&gt;代码破坏了语言的最佳实践&lt;/li&gt;
&lt;li&gt;反模式编程（意大利面代码、复制-黏贴编程、自负编程、……）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;团队开发工具介绍&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;版本控制：Git、Mercury&lt;/li&gt;
&lt;li&gt;缺陷管理：&lt;a href="https://about.gitlab.com/" rel="nofollow"&gt;Gitlab&lt;/a&gt;、&lt;a href="http://www.redmine.org.cn/" rel="nofollow"&gt;Redmine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;敏捷闭环工具：&lt;a href="https://www.zentao.net/" rel="nofollow"&gt;禅道&lt;/a&gt;、&lt;a href="https://www.atlassian.com/software/jira/features" rel="nofollow"&gt;JIRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;持续集成：&lt;a href="https://jenkins.io/" rel="nofollow"&gt;Jenkins&lt;/a&gt;、&lt;a href="https://travis-ci.org/" rel="nofollow"&gt;Travis-CI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;请参考&lt;a href="Day91-100/%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91.md"&gt;《团队项目开发》&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-项目选题和理解业务" class="anchor" aria-hidden="true" href="#项目选题和理解业务"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目选题和理解业务&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;选题范围设定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;CMS（用户端）：新闻聚合网站、问答/分享社区、影评/书评网站等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MIS（用户端+管理端）：KMS、KPI考核系统、HRS、CRM系统、供应链系统、仓储管理系统等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;App后台（管理端+数据接口）：二手交易类、报刊杂志类、小众电商类、新闻资讯类、旅游类、社交类、阅读类等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其他类型：自身行业背景和工作经验、业务容易理解和把控。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需求理解、模块划分和任务分配&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需求理解：头脑风暴和竞品分析。&lt;/li&gt;
&lt;li&gt;模块划分：画思维导图（XMind），每个模块是一个枝节点，每个具体的功能是一个叶节点（用动词表述），需要确保每个叶节点无法再生出新节点，确定每个叶子节点的重要性、优先级和工作量。&lt;/li&gt;
&lt;li&gt;任务分配：由项目负责人根据上面的指标为每个团队成员分配任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/requirements_by_xmind.png"&gt;&lt;img src="./res/requirements_by_xmind.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;制定项目进度表（每日更新）&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模块&lt;/th&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;th&gt;人员&lt;/th&gt;
&lt;th&gt;状态&lt;/th&gt;
&lt;th&gt;完成&lt;/th&gt;
&lt;th&gt;工时&lt;/th&gt;
&lt;th&gt;计划开始&lt;/th&gt;
&lt;th&gt;实际开始&lt;/th&gt;
&lt;th&gt;计划结束&lt;/th&gt;
&lt;th&gt;实际结束&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;评论&lt;/td&gt;
&lt;td&gt;添加评论&lt;/td&gt;
&lt;td&gt;王大锤&lt;/td&gt;
&lt;td&gt;正在进行&lt;/td&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;删除评论&lt;/td&gt;
&lt;td&gt;王大锤&lt;/td&gt;
&lt;td&gt;等待&lt;/td&gt;
&lt;td&gt;0%&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;查看评论&lt;/td&gt;
&lt;td&gt;白元芳&lt;/td&gt;
&lt;td&gt;正在进行&lt;/td&gt;
&lt;td&gt;20%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;需要进行代码审查&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;评论投票&lt;/td&gt;
&lt;td&gt;白元芳&lt;/td&gt;
&lt;td&gt;等待&lt;/td&gt;
&lt;td&gt;0%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/8&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/8&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OOAD和数据库设计&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;UML（统一建模语言）的类图&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/uml-class-diagram.png"&gt;&lt;img src="./res/uml-class-diagram.png" alt="uml" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过模型创建表（正向工程）&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python manage.py makemigrations app
python manage.py migrate&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用PowerDesigner绘制物理模型图。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/power-designer-pdm.png"&gt;&lt;img src="./res/power-designer-pdm.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过数据表创建模型（反向工程）&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python manage.py inspectdb &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; app/models.py&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-第92天使用docker部署应用" class="anchor" aria-hidden="true" href="#第92天使用docker部署应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第92天：&lt;a href="./Day91-100/92.%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8.md"&gt;使用Docker部署应用&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Docker简介&lt;/li&gt;
&lt;li&gt;安装Docker&lt;/li&gt;
&lt;li&gt;使用Docker创建容器（Nginx、MySQL、Redis、Gitlab、Jenkins）&lt;/li&gt;
&lt;li&gt;构建Docker镜像（Dockerfile的编写和相关指令）&lt;/li&gt;
&lt;li&gt;容器编排（Docker-compose）&lt;/li&gt;
&lt;li&gt;集群管理&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第93天mysql性能优化" class="anchor" aria-hidden="true" href="#第93天mysql性能优化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第93天：&lt;a href="./Day91-100/93.MySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.md"&gt;MySQL性能优化&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第94天网络api接口设计" class="anchor" aria-hidden="true" href="#第94天网络api接口设计"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第94天：&lt;a href="./Day91-100/94.%E7%BD%91%E7%BB%9CAPI%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1.md"&gt;网络API接口设计&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第95天使用django开发商业项目day91-10095使用django开发商业项目md" class="anchor" aria-hidden="true" href="#第95天使用django开发商业项目day91-10095使用django开发商业项目md"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第95天：[使用Django开发商业项目](./Day91-100/95.使用Django开发商业项	目.md)&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-项目开发中的公共问题" class="anchor" aria-hidden="true" href="#项目开发中的公共问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目开发中的公共问题&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;数据库的配置（多数据库、主从复制、数据库路由）&lt;/li&gt;
&lt;li&gt;缓存的配置（分区缓存、键设置、超时设置、主从复制、故障恢复（哨兵））&lt;/li&gt;
&lt;li&gt;日志的配置&lt;/li&gt;
&lt;li&gt;分析和调试（Django-Debug-ToolBar）&lt;/li&gt;
&lt;li&gt;好用的Python模块（日期计算、图像处理、数据加密、三方API）&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-rest-api设计" class="anchor" aria-hidden="true" href="#rest-api设计"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;REST API设计&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;RESTful架构
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2011/09/restful.html" rel="nofollow"&gt;理解RESTful架构&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2014/05/restful_api.html" rel="nofollow"&gt;RESTful API设计指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html" rel="nofollow"&gt;RESTful API最佳实践&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;API接口文档的撰写
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://rap2.taobao.org/" rel="nofollow"&gt;RAP2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yapi.demo.qunar.com/" rel="nofollow"&gt;YAPI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.django-rest-framework.org/" rel="nofollow"&gt;django-REST-framework&lt;/a&gt;的应用&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-项目中的重点难点剖析" class="anchor" aria-hidden="true" href="#项目中的重点难点剖析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目中的重点难点剖析&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;使用缓存缓解数据库压力 - Redis&lt;/li&gt;
&lt;li&gt;使用消息队列做解耦合和削峰 - Celery + RabbitMQ&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第96天软件测试和自动化测试" class="anchor" aria-hidden="true" href="#第96天软件测试和自动化测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第96天：&lt;a href="Day91-100/96.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%92%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95.md"&gt;软件测试和自动化测试&lt;/a&gt;&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-单元测试" class="anchor" aria-hidden="true" href="#单元测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;单元测试&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;测试的种类&lt;/li&gt;
&lt;li&gt;编写单元测试（unittest、pytest、nose2、tox、ddt、……）&lt;/li&gt;
&lt;li&gt;测试覆盖率（coverage）&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-项目部署" class="anchor" aria-hidden="true" href="#项目部署"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目部署&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;部署前的准备工作
&lt;ul&gt;
&lt;li&gt;关键设置（SECRET_KEY / DEBUG / ALLOWED_HOSTS / 缓存 / 数据库）&lt;/li&gt;
&lt;li&gt;HTTPS / CSRF_COOKIE_SECUR  / SESSION_COOKIE_SECURE&lt;/li&gt;
&lt;li&gt;日志相关配置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Linux常用命令回顾&lt;/li&gt;
&lt;li&gt;Linux常用服务的安装和配置&lt;/li&gt;
&lt;li&gt;uWSGI/Gunicorn和Nginx的使用
&lt;ul&gt;
&lt;li&gt;Gunicorn和uWSGI的比较
&lt;ul&gt;
&lt;li&gt;对于不需要大量定制化的简单应用程序，Gunicorn是一个不错的选择，uWSGI的学习曲线比Gunicorn要陡峭得多，Gunicorn的默认参数就已经能够适应大多数应用程序。&lt;/li&gt;
&lt;li&gt;uWSGI支持异构部署。&lt;/li&gt;
&lt;li&gt;由于Nginx本身支持uWSGI，在线上一般都将Nginx和uWSGI捆绑在一起部署，而且uWSGI属于功能齐全且高度定制的WSGI中间件。&lt;/li&gt;
&lt;li&gt;在性能上，Gunicorn和uWSGI其实表现相当。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;使用虚拟化技术（Docker）部署测试环境和生产环境&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-性能测试" class="anchor" aria-hidden="true" href="#性能测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;性能测试&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;AB的使用&lt;/li&gt;
&lt;li&gt;SQLslap的使用&lt;/li&gt;
&lt;li&gt;sysbench的使用&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-自动化测试" class="anchor" aria-hidden="true" href="#自动化测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;自动化测试&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;使用Shell和Python进行自动化测试&lt;/li&gt;
&lt;li&gt;使用Selenium实现自动化测试
&lt;ul&gt;
&lt;li&gt;Selenium IDE&lt;/li&gt;
&lt;li&gt;Selenium WebDriver&lt;/li&gt;
&lt;li&gt;Selenium Remote Control&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;测试工具Robot Framework介绍&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第97天电商网站技术要点剖析" class="anchor" aria-hidden="true" href="#第97天电商网站技术要点剖析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第97天：&lt;a href="./Day91-100/97.%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E8%A6%81%E7%82%B9%E5%89%96%E6%9E%90.md"&gt;电商网站技术要点剖析&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第98天项目部署上线和性能调优" class="anchor" aria-hidden="true" href="#第98天项目部署上线和性能调优"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第98天：&lt;a href="./Day91-100/98.%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E5%92%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98.md"&gt;项目部署上线和性能调优&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;MySQL数据库调优&lt;/li&gt;
&lt;li&gt;Web服务器性能优化
&lt;ul&gt;
&lt;li&gt;Nginx负载均衡配置&lt;/li&gt;
&lt;li&gt;Keepalived实现高可用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;代码性能调优
&lt;ul&gt;
&lt;li&gt;多线程&lt;/li&gt;
&lt;li&gt;异步化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;静态资源访问优化
&lt;ul&gt;
&lt;li&gt;云存储&lt;/li&gt;
&lt;li&gt;CDN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第99天面试中的公共问题" class="anchor" aria-hidden="true" href="#第99天面试中的公共问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第99天：&lt;a href="./Day91-100/99.%E9%9D%A2%E8%AF%95%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E9%97%AE%E9%A2%98.md"&gt;面试中的公共问题&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第100天python面试题集" class="anchor" aria-hidden="true" href="#第100天python面试题集"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第100天：&lt;a href="./Day91-100/100.Python%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86.md"&gt;Python面试题集&lt;/a&gt;&lt;/h4&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jackfrued</author><guid isPermaLink="false">https://github.com/jackfrued/Python-100-Days</guid><pubDate>Thu, 28 Nov 2019 00:04:00 GMT</pubDate></item><item><title>practicalAI/practicalAI #5 in Jupyter Notebook, This month</title><link>https://github.com/practicalAI/practicalAI</link><description>&lt;p&gt;&lt;i&gt;📚 A practical approach to machine learning to enable everyone to learn, explore and build.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
&lt;a href="https://practicalai.me" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/369fcfdcd241a0324ffaee51715926a56c049302/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f70726163746963616c41492f6c6f676f2e706e67" width="200" data-canonical-src="https://practicalai.me/static/img/practicalAI/logo.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;p&gt;A &lt;i&gt;&lt;b&gt;practical&lt;/b&gt;&lt;/i&gt; approach to machine learning.&lt;/p&gt;
&lt;a href="https://github.com/practicalAI/practicalAI"&gt;
&lt;img src="https://camo.githubusercontent.com/c1b6c20adc52e06a1c58218665169097a63bd549/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f70726163746963616c41492f70726163746963616c41492e7376673f7374796c653d736f6369616c266c6162656c3d53746172" data-canonical-src="https://img.shields.io/github/stars/practicalAI/practicalAI.svg?style=social&amp;amp;label=Star" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://www.linkedin.com/company/practicalai-me" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/19c0cf9ba93aa446aa855a0203c46ee39841cba9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374796c652d2d3565626130302e7376673f6c6162656c3d4c696e6b6564496e266c6f676f3d6c696e6b6564696e267374796c653d736f6369616c" data-canonical-src="https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&amp;amp;logo=linkedin&amp;amp;style=social" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://twitter.com/GokuMohandas" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/fd3346389d2255e7c7aa4395d6afb74e7a1df552/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f476f6b754d6f68616e6461732e7376673f6c6162656c3d466f6c6c6f77267374796c653d736f6369616c" data-canonical-src="https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&amp;amp;style=social" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;p&gt;&lt;sub&gt;Created by
&lt;a href="https://goku.me" rel="nofollow"&gt;Goku Mohandas&lt;/a&gt; and
&lt;a href="https://github.com/GokuMohandas/practicalAI/graphs/contributors"&gt;
contributors
&lt;/a&gt;
&lt;/sub&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-notebooks" class="anchor" aria-hidden="true" href="#notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Notebooks&lt;/h2&gt;
&lt;ul&gt;
    &lt;li&gt;
        &lt;g-emoji class="g-emoji" alias="earth_americas" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30e.png"&gt;🌎&lt;/g-emoji&gt; → &lt;a href="https://practicalai.me" rel="nofollow"&gt;https://practicalai.me&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
        All of these notebooks are in &lt;a href="https://tensorflow.org" rel="nofollow"&gt;TensorFlow 2.0 + Keras&lt;/a&gt; but you can find old PyTorch notebooks in the &lt;a href="https://github.com/practicalAI/practicalAI/tree/4ad626098aca25db5628fe67895e738d5a5c2c2a"&gt;v0.1&lt;/a&gt; release.
    &lt;/li&gt;
    &lt;li&gt;
        If you prefer Jupyter Notebooks or want to add/fix content, check out the &lt;a href="https://github.com/practicalAI/practicalAI/tree/master/notebooks"&gt;notebooks&lt;/a&gt; directory.
    &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;
        &lt;h4 align="center"&gt;&lt;a id="user-content-basic-ml" class="anchor" aria-hidden="true" href="#basic-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic ML&lt;/h4&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Basics&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Machine Learning&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Tools&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Deep Learning&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="4"&gt;
        &lt;ul&gt;
            &lt;li&gt;Learn Python basics with notebooks.&lt;/li&gt;
            &lt;li&gt;Use data science libraries like &lt;a href="https://www.numpy.org/" rel="nofollow"&gt;NumPy&lt;/a&gt; and &lt;a href="https://pandas.pydata.org/" rel="nofollow"&gt;Pandas&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Implement basic ML models in &lt;a href="https://www.tensorflow.org/overview/" rel="nofollow"&gt;TensorFlow 2.0 + Keras&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Create deep learning models for improved performance.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/00_Notebooks.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="notebook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d3.png"&gt;📓&lt;/g-emoji&gt; Notebooks&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="chart_with_upwards_trend" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c8.png"&gt;📈&lt;/g-emoji&gt; Linear Regression&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="mag_right" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f50e.png"&gt;🔎&lt;/g-emoji&gt; Data &amp;amp; Models&lt;/td&gt;
        &lt;td&gt;️&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;🖼&lt;/g-emoji&gt; Convolutional Neural Networks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/01_Python.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt; Python&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="bar_chart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png"&gt;📊&lt;/g-emoji&gt; Logistic Regression&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="hammer_and_wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6e0.png"&gt;🛠&lt;/g-emoji&gt; Utilities&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="crown" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f451.png"&gt;👑&lt;/g-emoji&gt; Embeddings&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/02_NumPy.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="1234" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f522.png"&gt;🔢&lt;/g-emoji&gt; NumPy&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;️&lt;g-emoji class="g-emoji" alias="control_knobs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f39b.png"&gt;🎛&lt;/g-emoji&gt; Multilayer Perceptrons&lt;/td&gt;
        &lt;td&gt;️&lt;g-emoji class="g-emoji" alias="scissors" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2702.png"&gt;✂️&lt;/g-emoji&gt; Preprocessing&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;📗&lt;/g-emoji&gt; Recurrent Neural Networks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/03_Pandas.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="panda_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f43c.png"&gt;🐼&lt;/g-emoji&gt; Pandas&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-production-ml" class="anchor" aria-hidden="true" href="#production-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Production ML&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Local&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Applications&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Scale&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="3"&gt;
        &lt;ul&gt;
            &lt;li&gt;Setup your local environment for ML.&lt;/li&gt;
            &lt;li&gt;Wrap your ML in RESTful APIs using &lt;a href="http://flask.pocoo.org/" rel="nofollow"&gt;Flask&lt;/a&gt; to create applications.&lt;/li&gt;
            &lt;li&gt;Standardize and scale your ML applications with &lt;a href="https://www.docker.com/" rel="nofollow"&gt;Docker&lt;/a&gt; and &lt;a href="https://kubernetes.io/" rel="nofollow"&gt;Kubernetes&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Deploy simple and scalable ML workflows using &lt;a href="https://www.kubeflow.org/" rel="nofollow"&gt;Kubeflow&lt;/a&gt;.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;💻&lt;/g-emoji&gt; Local Setup&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="evergreen_tree" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f332.png"&gt;🌲&lt;/g-emoji&gt; Logging&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="whale" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f433.png"&gt;🐳&lt;/g-emoji&gt; Docker&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="handshake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f91d.png"&gt;🤝&lt;/g-emoji&gt; Distributed Training&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt; ML Scripts&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="funeral_urn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26b1.png"&gt;⚱️&lt;/g-emoji&gt; Flask Applications&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="ship" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a2.png"&gt;🚢&lt;/g-emoji&gt; Kubernetes&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="battery" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f50b.png"&gt;🔋&lt;/g-emoji&gt; Databases&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png"&gt;✅&lt;/g-emoji&gt; Unit Tests&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="ocean" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30a.png"&gt;🌊&lt;/g-emoji&gt; Kubeflow&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="closed_lock_with_key" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f510.png"&gt;🔐&lt;/g-emoji&gt; Authentication&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-advanced-ml" class="anchor" aria-hidden="true" href="#advanced-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advanced ML&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;General&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Sequential&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Popular&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="3"&gt;
        &lt;ul&gt;
            &lt;li&gt;Dive into architectural and interpretable advancements in neural networks.&lt;/li&gt;
            &lt;li&gt;Implement state-of-the-art NLP techniques.&lt;/li&gt;
            &lt;li&gt;Learn about popular deep learning algorithms used for generation, time-series, etc.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;🧐 Attention&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="bee" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f41d.png"&gt;🐝&lt;/g-emoji&gt; Transformers&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="performing_arts" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ad.png"&gt;🎭&lt;/g-emoji&gt; Generative Adversarial Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="crystal_ball" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f52e.png"&gt;🔮&lt;/g-emoji&gt; Autoencoders&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="racing_car" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ce.png"&gt;🏎️&lt;/g-emoji&gt; Highway Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="japanese_ogre" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f479.png"&gt;👹&lt;/g-emoji&gt; BERT, GPT2, XLNet&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="8ball" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b1.png"&gt;🎱&lt;/g-emoji&gt; Bayesian Deep Learning&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="spider" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f577.png"&gt;🕷️&lt;/g-emoji&gt; Graph Neural Networks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="droplet" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a7.png"&gt;💧&lt;/g-emoji&gt; Residual Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="clock9" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f558.png"&gt;🕘&lt;/g-emoji&gt; Temporal CNNs&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="cherries" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f352.png"&gt;🍒&lt;/g-emoji&gt; Reinforcement Learning&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-topics" class="anchor" aria-hidden="true" href="#topics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Topics&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Computer Vision&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Natural Language&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Unsupervised Learning&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="4"&gt;
        &lt;ul&gt;
            &lt;li&gt;Learn how to use deep learning for computer vision tasks.&lt;/li&gt;
            &lt;li&gt;Implement techniques for natural language tasks.&lt;/li&gt;
            &lt;li&gt;Derive insights from unlabeled data using unsupervised learning.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="camera_flash" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f8.png"&gt;📸&lt;/g-emoji&gt; Image Recognition&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;📖&lt;/g-emoji&gt; Text classification&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="dango" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f361.png"&gt;🍡&lt;/g-emoji&gt; Clustering&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="alarm_clock" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/23f0.png"&gt;⏰&lt;/g-emoji&gt; Time-series Analysis&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;🖼️&lt;/g-emoji&gt; Image Segmentation&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="speech_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png"&gt;💬&lt;/g-emoji&gt; Named Entity Recognition&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="houses" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3d8.png"&gt;🏘️&lt;/g-emoji&gt; Topic Modeling&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="shopping_cart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6d2.png"&gt;🛒&lt;/g-emoji&gt; Recommendation Systems&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="art" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a8.png"&gt;🎨&lt;/g-emoji&gt; Image Generation&lt;/td&gt;
        &lt;td&gt;🧠 Knowledge Graphs&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="dart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png"&gt;🎯&lt;/g-emoji&gt; One-shot Learning&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="card_file_box" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5c3.png"&gt;🗃️&lt;/g-emoji&gt; Interpretability&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://practicalai.me/#newsletter" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="mailbox_with_mail" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ec.png"&gt;📬&lt;/g-emoji&gt; Newsletter&lt;/a&gt; - Subscribe to get updates on new content&lt;/p&gt;
&lt;div&gt;
&lt;a href="https://twitter.com/practicalai_me" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6729390835283dc393cb7f500d840be48cdac6c6/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f6d656469612f74772e706e67" width="30" data-canonical-src="https://practicalai.me/static/img/media/tw.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.youtube.com/channel/UCgW4K2UDK21kHIzxpjNos7Q" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/786df4e8cd9d5421583bf063cdcf9e3d06d14de5/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f6d656469612f7974622e706e67" width="30" data-canonical-src="https://practicalai.me/static/img/media/ytb.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/practicalAI/practicalAI"&gt;&lt;img src="https://camo.githubusercontent.com/082d52273e2945cb5126868e9cc44d4c91ffe912/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f6d656469612f677468622e706e67" width="30" data-canonical-src="https://practicalai.me/static/img/media/gthb.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.linkedin.com/company/practicalai-me" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/045b09864f16b18059d20d17bfaf906b26ca3aae/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f6d656469612f6c6e6b646e2e706e67" width="30" data-canonical-src="https://practicalai.me/static/img/media/lnkdn.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>practicalAI</author><guid isPermaLink="false">https://github.com/practicalAI/practicalAI</guid><pubDate>Thu, 28 Nov 2019 00:05:00 GMT</pubDate></item><item><title>fengdu78/Data-Science-Notes #6 in Jupyter Notebook, This month</title><link>https://github.com/fengdu78/Data-Science-Notes</link><description>&lt;p&gt;&lt;i&gt;数据科学的笔记以及资料搜集&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-data-science-notes" class="anchor" aria-hidden="true" href="#data-science-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data-Science-Notes&lt;/h1&gt;
&lt;p&gt;数据科学的笔记以及资料搜集，目前尚在更新，部分内容来源于github搜集。&lt;/p&gt;
&lt;p&gt;&lt;a href="0.math"&gt;0.math&lt;/a&gt; （数学基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="1.python-basic"&gt;1.python-basic&lt;/a&gt; （python基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="2.numpy"&gt;2.numpy&lt;/a&gt;（numpy基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="3.pandas"&gt;3.pandas&lt;/a&gt;（pandas基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="4.scipy"&gt;4.scipy&lt;/a&gt;（scipy基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="5.data-visualization"&gt;5.data-visualization&lt;/a&gt;（数据可视化基础，包含matplotlib和seaborn）&lt;/p&gt;
&lt;p&gt;&lt;a href="6.scikit-learn"&gt;6.scikit-learn&lt;/a&gt;（scikit-learn基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="7.machine-learning"&gt;7.machine-learning&lt;/a&gt;（机器学习基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="8.deep-learning"&gt;8.deep-learning&lt;/a&gt;（深度学习基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="9.feature-engineering"&gt;9.feature-engineering&lt;/a&gt;（特征工程基础）&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-参考" class="anchor" aria-hidden="true" href="#参考"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;《统计学习方法》李航&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/donnemartin/data-science-ipython-notebooks"&gt;https://github.com/donnemartin/data-science-ipython-notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apachecn/feature-engineering-for-ml-zh"&gt;https://github.com/apachecn/feature-engineering-for-ml-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datawhalechina/pumpkin-book"&gt;https://github.com/datawhalechina/pumpkin-book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Doraemonzzz/Learning-from-data"&gt;https://github.com/Doraemonzzz/Learning-from-data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wzyonggege/statistical-learning-method"&gt;https://github.com/wzyonggege/statistical-learning-method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm"&gt;https://github.com/WenDesi/lihang_book_algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/course/ml" rel="nofollow"&gt;https://www.coursera.org/course/ml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mooc.guokr.com/note/12/" rel="nofollow"&gt;https://mooc.guokr.com/note/12/&lt;/a&gt; &lt;a href="https://mooc.guokr.com/user/2133483357/" rel="nofollow"&gt;小小人_V&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;《python科学计算》&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-关于作者" class="anchor" aria-hidden="true" href="#关于作者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;关于作者&lt;/h2&gt;
&lt;p&gt;微信公众号：机器学习初学者 &lt;a target="_blank" rel="noopener noreferrer" href="images/gongzhong.jpg"&gt;&lt;img src="images/gongzhong.jpg" alt="gongzhong" style="max-width:100%;"&gt;&lt;/a&gt;
知识星球：黄博的机器学习圈子&lt;a target="_blank" rel="noopener noreferrer" href="images/zhishixingqiu1.jpg"&gt;&lt;img src="images/zhishixingqiu1.jpg" alt="xingqiu" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/people/fengdu78/activities" rel="nofollow"&gt;我的知乎&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意：github下载太慢的话，关注我的公众号：“机器学习初学者”，回复“学习路线”即可下载本仓库的镜像文件，整个仓库压缩成一个iso。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果需要引用这个Repo:&lt;/p&gt;
&lt;p&gt;格式： &lt;code&gt;fengdu78, Data-Science-Notes, (2019), GitHub repository, https://github.com/fengdu78/Data-Science-Notes&lt;/code&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fengdu78</author><guid isPermaLink="false">https://github.com/fengdu78/Data-Science-Notes</guid><pubDate>Thu, 28 Nov 2019 00:06:00 GMT</pubDate></item><item><title>NVIDIA/DeepLearningExamples #7 in Jupyter Notebook, This month</title><link>https://github.com/NVIDIA/DeepLearningExamples</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-nvidia-deep-learning-examples-for-tensor-cores" class="anchor" aria-hidden="true" href="#nvidia-deep-learning-examples-for-tensor-cores"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA Deep Learning Examples for Tensor Cores&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This repository provides the latest deep learning example networks for training.  These examples focus on achieving the best performance and convergence from NVIDIA Volta Tensor Cores.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-nvidia-gpu-cloud-ngc-container-registry" class="anchor" aria-hidden="true" href="#nvidia-gpu-cloud-ngc-container-registry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA GPU Cloud (NGC) Container Registry&lt;/h2&gt;
&lt;p&gt;These examples, along with our NVIDIA deep learning software stack, are provided in a monthly updated Docker container on the NGC container registry (&lt;a href="https://ngc.nvidia.com" rel="nofollow"&gt;https://ngc.nvidia.com&lt;/a&gt;). These containers include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latest NVIDIA examples from this repository&lt;/li&gt;
&lt;li&gt;The latest NVIDIA contributions shared upstream to the respective framework&lt;/li&gt;
&lt;li&gt;The latest NVIDIA Deep Learning software libraries, such as cuDNN, NCCL, cuBLAS, etc. which have all been through a rigorous monthly quality assurance process to ensure that they provide the best possible performance&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/dgx/index.html#nvidia-optimized-frameworks-release-notes" rel="nofollow"&gt;Monthly release notes&lt;/a&gt; for each of the NVIDIA optimized containers&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-directory-structure" class="anchor" aria-hidden="true" href="#directory-structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Directory structure&lt;/h2&gt;
&lt;p&gt;The examples are organized first by framework, such as TensorFlow, PyTorch, etc. and second by use case, such as computer vision, natural language processing, etc. We hope this structure enables you to quickly locate the example networks that best suit your needs. Here are the currently supported models:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-computer-vision" class="anchor" aria-hidden="true" href="#computer-vision"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Computer Vision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ResNet-50&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/MxNet/Classification/RN50v1.5"&gt;MXNet&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/RN50v1.5"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Classification/RN50v1.5"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSD&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Detection/SSD"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mask R-CNN&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Segmentation/MaskRCNN"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;U-Net(industrial)&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/UNet_Industrial"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;U-Net(medical)&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/UNet_Medical"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-natural-language-processing" class="anchor" aria-hidden="true" href="#natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natural Language Processing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GNMT&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Translation/GNMT"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Translation/GNMT"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Translation/Transformer"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BERT&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT"&gt;PyTorch&lt;/a&gt;][&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-recommender-systems" class="anchor" aria-hidden="true" href="#recommender-systems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recommender Systems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NCF&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/NCF"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Recommendation/NCF"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-text-to-speech" class="anchor" aria-hidden="true" href="#text-to-speech"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Text to Speech&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tacotron &amp;amp; WaveGlow&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-speech-recognition" class="anchor" aria-hidden="true" href="#speech-recognition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speech Recognition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jasper&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechRecognition/Jasper"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-nvidia-support" class="anchor" aria-hidden="true" href="#nvidia-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA support&lt;/h2&gt;
&lt;p&gt;In each of the network READMEs, we indicate the level of support that will be provided. The range is from ongoing updates and improvements to a point-in-time release for thought leadership.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-feedback--contributions" class="anchor" aria-hidden="true" href="#feedback--contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Feedback / Contributions&lt;/h2&gt;
&lt;p&gt;We're posting these examples on GitHub to better support the community, facilitate feedback, as well as collect and implement contributions using GitHub Issues and pull requests. We welcome all contributions!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-known-issues" class="anchor" aria-hidden="true" href="#known-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Known issues&lt;/h2&gt;
&lt;p&gt;In each of the network READMEs, we indicate any known issues and encourage the community to provide feedback.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA</author><guid isPermaLink="false">https://github.com/NVIDIA/DeepLearningExamples</guid><pubDate>Thu, 28 Nov 2019 00:07:00 GMT</pubDate></item><item><title>NVIDIA/tacotron2 #8 in Jupyter Notebook, This month</title><link>https://github.com/NVIDIA/tacotron2</link><description>&lt;p&gt;&lt;i&gt;Tacotron 2 - PyTorch implementation with faster-than-realtime inference&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tacotron-2-without-wavenet" class="anchor" aria-hidden="true" href="#tacotron-2-without-wavenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tacotron 2 (without wavenet)&lt;/h1&gt;
&lt;p&gt;PyTorch implementation of &lt;a href="https://arxiv.org/pdf/1712.05884.pdf" rel="nofollow"&gt;Natural TTS Synthesis By Conditioning
Wavenet On Mel Spectrogram Predictions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This implementation includes &lt;strong&gt;distributed&lt;/strong&gt; and &lt;strong&gt;automatic mixed precision&lt;/strong&gt; support
and uses the &lt;a href="https://keithito.com/LJ-Speech-Dataset/" rel="nofollow"&gt;LJSpeech dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Distributed and Automatic Mixed Precision support relies on NVIDIA's &lt;a href="https://github.com/nvidia/apex"&gt;Apex&lt;/a&gt; and &lt;a href="https://github.com/NVIDIA/apex/tree/master/apex/amp"&gt;AMP&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Visit our &lt;a href="https://nv-adlr.github.io/WaveGlow" rel="nofollow"&gt;website&lt;/a&gt; for audio samples using our published &lt;a href="https://drive.google.com/file/d/1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA/view?usp=sharing" rel="nofollow"&gt;Tacotron 2&lt;/a&gt; and
&lt;a href="https://drive.google.com/file/d/1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx/view?usp=sharing" rel="nofollow"&gt;WaveGlow&lt;/a&gt; models.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="tensorboard.png"&gt;&lt;img src="tensorboard.png" alt="Alignment, Predicted Mel Spectrogram, Target Mel Spectrogram" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-requisites" class="anchor" aria-hidden="true" href="#pre-requisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-requisites&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;NVIDIA GPU + CUDA cuDNN&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download and extract the &lt;a href="https://keithito.com/LJ-Speech-Dataset/" rel="nofollow"&gt;LJ Speech dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Clone this repo: &lt;code&gt;git clone https://github.com/NVIDIA/tacotron2.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;CD into this repo: &lt;code&gt;cd tacotron2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Initialize submodule: &lt;code&gt;git submodule init; git submodule update&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Update .wav paths: &lt;code&gt;sed -i -- 's,DUMMY,ljs_dataset_folder/wavs,g' filelists/*.txt&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Alternatively, set &lt;code&gt;load_mel_from_disk=True&lt;/code&gt; in &lt;code&gt;hparams.py&lt;/code&gt; and update mel-spectrogram paths&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Install &lt;a href="https://github.com/pytorch/pytorch#installation"&gt;PyTorch 1.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install &lt;a href="https://github.com/nvidia/apex"&gt;Apex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install python requirements or build docker image
&lt;ul&gt;
&lt;li&gt;Install python requirements: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;python train.py --output_directory=outdir --log_directory=logdir&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;(OPTIONAL) &lt;code&gt;tensorboard --logdir=outdir/logdir&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-training-using-a-pre-trained-model" class="anchor" aria-hidden="true" href="#training-using-a-pre-trained-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training using a pre-trained model&lt;/h2&gt;
&lt;p&gt;Training using a pre-trained model can lead to faster convergence&lt;br&gt;
By default, the dataset dependent text embedding layers are &lt;a href="https://github.com/NVIDIA/tacotron2/blob/master/hparams.py#L22"&gt;ignored&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download our published &lt;a href="https://drive.google.com/file/d/1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA/view?usp=sharing" rel="nofollow"&gt;Tacotron 2&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;&lt;code&gt;python train.py --output_directory=outdir --log_directory=logdir -c tacotron2_statedict.pt --warm_start&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-multi-gpu-distributed-and-automatic-mixed-precision-training" class="anchor" aria-hidden="true" href="#multi-gpu-distributed-and-automatic-mixed-precision-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-GPU (distributed) and Automatic Mixed Precision Training&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;python -m multiproc train.py --output_directory=outdir --log_directory=logdir --hparams=distributed_run=True,fp16_run=True&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-inference-demo" class="anchor" aria-hidden="true" href="#inference-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inference demo&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download our published &lt;a href="https://drive.google.com/file/d/1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA/view?usp=sharing" rel="nofollow"&gt;Tacotron 2&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;Download our published &lt;a href="https://drive.google.com/file/d/1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx/view?usp=sharing" rel="nofollow"&gt;WaveGlow&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;&lt;code&gt;jupyter notebook --ip=127.0.0.1 --port=31337&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Load inference.ipynb&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;N.b.  When performing Mel-Spectrogram to Audio synthesis, make sure Tacotron 2
and the Mel decoder were trained on the same mel-spectrogram representation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-related-repos" class="anchor" aria-hidden="true" href="#related-repos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related repos&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/WaveGlow"&gt;WaveGlow&lt;/a&gt; Faster than real time Flow-based
Generative Network for Speech Synthesis&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/nv-wavenet/"&gt;nv-wavenet&lt;/a&gt; Faster than real time
WaveNet.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This implementation uses code from the following repos: &lt;a href="https://github.com/keithito/tacotron/"&gt;Keith
Ito&lt;/a&gt;, &lt;a href="https://github.com/pseeth/pytorch-stft"&gt;Prem
Seetharaman&lt;/a&gt; as described in our code.&lt;/p&gt;
&lt;p&gt;We are inspired by &lt;a href="https://github.com/r9y9/tacotron_pytorch"&gt;Ryuchi Yamamoto's&lt;/a&gt;
Tacotron PyTorch implementation.&lt;/p&gt;
&lt;p&gt;We are thankful to the Tacotron 2 paper authors, specially Jonathan Shen, Yuxuan
Wang and Zongheng Yang.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA</author><guid isPermaLink="false">https://github.com/NVIDIA/tacotron2</guid><pubDate>Thu, 28 Nov 2019 00:08:00 GMT</pubDate></item><item><title>chenyuntc/pytorch-book #9 in Jupyter Notebook, This month</title><link>https://github.com/chenyuntc/pytorch-book</link><description>&lt;p&gt;&lt;i&gt;PyTorch tutorials and fun projects including neural talk, neural style, poem writing, anime generation &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;这是书籍《深度学习框架PyTorch：入门与实践》的对应代码，但是也可以作为一个独立的PyTorch入门指南和教程。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-更新说明" class="anchor" aria-hidden="true" href="#更新说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新说明&lt;/h2&gt;
&lt;p&gt;Working on migration to Pytorch 1.0, stay tuned!&lt;/p&gt;
&lt;p&gt;已更新到&lt;strong&gt;pytorch 0.4.1 (不是0.4.0)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;已更新到&lt;strong&gt;pytorch 0.4.1 (不是0.4.0)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;已更新到&lt;strong&gt;pytorch 0.4.1 (不是0.4.0)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当前版本的代码是基于pytorch 0.4.1， 如果想使用旧版的 请 &lt;code&gt;git checkout v0.2&lt;/code&gt; 或者 &lt;code&gt;git checkout v0.3&lt;/code&gt;。旧版代码有更好的python2/python3 兼容，CPU/GPU兼容测试。 新版的代码未经过完整测试，已在GPU和python3 下测试通过。但是理论上在python2和CPU上不应该有太多的问题。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-内容" class="anchor" aria-hidden="true" href="#内容"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;内容&lt;/h2&gt;
&lt;p&gt;该书（教程/仓库）的内容如图所示：
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e145e8ea41382f9d5400613da168b4051af115b7/687474703a2f2f377a683433722e636f6d322e7a302e676c622e636c6f7564646e2e636f6d2f64656c2f6d696e646d61702e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/e145e8ea41382f9d5400613da168b4051af115b7/687474703a2f2f377a683433722e636f6d322e7a302e676c622e636c6f7564646e2e636f6d2f64656c2f6d696e646d61702e706e67" alt="思维导图" data-canonical-src="http://7zh43r.com2.z0.glb.clouddn.com/del/mindmap.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;可以看出本教程可以分为两部分：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基础部分&lt;/strong&gt;（前五章）讲解PyTorch内容，这部份介绍了PyTorch中主要的的模块，和深度学习中常用的一些工具。对于这部分内容，这里利用Jupyter Notebook作为教学工具，读者可以结合notebook修改运行，反复实验。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第二章介绍如何安装PyTorch和配置学习环境。同时提供了一个快速入门教程，基于官方的教程简化并更新内容，读者可以花费大约1到2小时的时间快速完成入门任务，而后根据需求再选择深入阅读后续相关章节的内容。&lt;/li&gt;
&lt;li&gt;第三章介绍了PyTorch中多维数组Tensor和动态图autograd/Variable的使用，并配以例子，让读者分别使用Tensor和autograd实现线性回归，比较二者的不同点。除了介绍这二者的基础使用之外，本章还对Tensor的底层设计，以及autograd的计算图原理进行比较深入分析，希望能使得读者能对这些底层知识有更全面的掌握。&lt;/li&gt;
&lt;li&gt;第四章介绍了PyTorch中神经网络模块nn的基础用法，同时讲解了神经网络中“层”，“损失函数”，“优化器”等，最后带领读者用不到50行的代码搭建出曾夺得ImageNet冠军的ResNet。&lt;/li&gt;
&lt;li&gt;第五章介绍了PyTorch中数据加载，GPU加速，持久化和可视化等相关工具。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实战部分&lt;/strong&gt;（第六到十章）利用PyTorch实现了几个酷炫有趣的应用，对于这部分的内容，本仓库给出完整的实现代码，并提供预训练好的模型作为demo，供读者测试。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第六章是承上启下的一章，这一章的目标不是教会读者新函数，新知识，而是结合Kaggle中一个经典的比赛，实现一个深度学习中比较简单的图像二分类问题。在实现过程中，带领读者复习前五章的知识，并提出代码规范以合理的组织程序，代码，使得程序更加可读，可维护。第六章还介绍了在PyTorch中如何进行debug。&lt;/li&gt;
&lt;li&gt;第七章为读者讲解了当前最火爆的生成对抗网络（GAN），带领读者从头实现一个动漫头像生成器，能够利用GAN生成风格多变的动漫头像。&lt;/li&gt;
&lt;li&gt;第八章为读者讲解了风格迁移的相关知识，并带领读者实现风格迁移网络，将自己的照片变成高大上的名画。&lt;/li&gt;
&lt;li&gt;第九章为读者讲解了一些自然语言处理的基础知识，并讲解了CharRNN的原理。而后利用收集了几万首唐诗，训练出了一个可以自动写诗歌的小程序。这个小程序可以控制生成诗歌的&lt;strong&gt;格式&lt;/strong&gt;，&lt;strong&gt;意境&lt;/strong&gt;，还能生成&lt;strong&gt;藏头诗&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;第十章为读者介绍了图像描述任务，并以最新的AI Challenger比赛的数据为例，带领读者实现了一个可以进行简单图像描述的的小程序。&lt;/li&gt;
&lt;li&gt;第十一章（&lt;strong&gt;新增，实验性&lt;/strong&gt;） 由&lt;a href="https://github.com/Diamondfan"&gt;Diamondfan&lt;/a&gt; 编写的语音识别。完善了本项目（本项目已囊括图像，文本，语音三大领域的例子）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Notebook中的文字描述内容属于本书的初稿，有描述不通顺，错别字之处还请谅解&lt;/strong&gt;。本打算删除notebook中描述的内容，只留下代码，但为了方便读者阅读学习，最终还是决定留下。 我会抽空根据书中内容逐字校对这部分内容，但并不对此并不提供具体时间点。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-是否需要买书" class="anchor" aria-hidden="true" href="#是否需要买书"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;是否需要买书&lt;/h2&gt;
&lt;p&gt;书&lt;strong&gt;不是必要的&lt;/strong&gt;，这个仓库包含书中50%以上的文字内容，90%以上的代码，尤其是前几章入门内容，几乎是完全保留了书中的讲解内容。读者即使不买书也能正常使用本教程。&lt;/p&gt;
&lt;p&gt;&lt;del&gt;如果你觉得纸质书的优势吸引你，不妨小破费一笔，支持一下作者这大半年来的工作。同时为了尽可能的方便读者，笔者还专门开通腾讯云的服务，用以保存教程中用到的部分模型，预处理的数据和部分大文件。&lt;/del&gt;
书中的部分内容已经过时，以此仓库内容为准。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-代码说明" class="anchor" aria-hidden="true" href="#代码说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;代码说明&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;代码主要在python3下测试得到最终结果，python2暂未测试。v0.2和v0.3 分支的代码同时经过严格测试支持python2/python3&lt;/li&gt;
&lt;li&gt;实战部分代码同时在GPU和CPU环境下测试通过&lt;/li&gt;
&lt;li&gt;代码已更新兼容到PyTorch &lt;code&gt;0.4.1&lt;/code&gt;, 后续会考虑兼容 &lt;code&gt;v1.0&lt;/code&gt;，但暂无确切时间点。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果你想在PyTorch 0.2.0或0.3下运行,请&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git checkout v0.2 # v0.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果有任何不当，或者有待改进的地方，欢迎读者开issue讨论，或者提交pull request。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-环境配置" class="anchor" aria-hidden="true" href="#环境配置"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;环境配置&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;安装&lt;a href="http://pytorch.org" rel="nofollow"&gt;PyTorch&lt;/a&gt;，请从官网选择指定的版本安装即可，一键安装（即使你使用anaconda，也建议使用pip）。更多的安装方式请参阅书中说明。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;克隆仓库&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;git clone https:&lt;span class="pl-k"&gt;//&lt;/span&gt;github.com&lt;span class="pl-k"&gt;/&lt;/span&gt;chenyuntc&lt;span class="pl-k"&gt;/&lt;/span&gt;PyTorch&lt;span class="pl-k"&gt;-&lt;/span&gt;book.git&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装第三方依赖包&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;cd pytorch&lt;span class="pl-k"&gt;-&lt;/span&gt;book &lt;span class="pl-ii"&gt;&amp;amp;&amp;amp;&lt;/span&gt; pip install &lt;span class="pl-k"&gt;-&lt;/span&gt;r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-visdom打不开及其解决方案" class="anchor" aria-hidden="true" href="#visdom打不开及其解决方案"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visdom打不开及其解决方案&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;新版的visdom已经解决了这个问题,只需要升级即可&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install --upgrade visdom
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之前的&lt;a href="https://github.com/chenyuntc/pytorch-book/blob/2c8366137b691aaa8fbeeea478cc1611c09e15f5/README.md#visdom%E6%89%93%E4%B8%8D%E5%BC%80%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"&gt;解决方案&lt;/a&gt; 不再需要，已删除。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-_" class="anchor" aria-hidden="true" href="#_"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;^_^&lt;/h2&gt;
&lt;p&gt;有任何bug，解释不清楚的地方或者是困惑，欢迎开issue&lt;/p&gt;
&lt;p&gt;欢迎pull requests&lt;/p&gt;
&lt;p&gt;Happy Coding!&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0376580818bbc47cd4b2f29ab6ca684122ba6e9f/687474703a2f2f696d6731342e333630627579696d672e636f6d2f6e312f6a66732f7431333333392f33322f323436333733303139382f3231373438332f65383134386336622f35613431323737644e62643134373063312e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/0376580818bbc47cd4b2f29ab6ca684122ba6e9f/687474703a2f2f696d6731342e333630627579696d672e636f6d2f6e312f6a66732f7431333333392f33322f323436333733303139382f3231373438332f65383134386336622f35613431323737644e62643134373063312e6a7067" alt="" data-canonical-src="http://img14.360buyimg.com/n1/jfs/t13339/32/2463730198/217483/e8148c6b/5a41277dNbd1470c1.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://search.jd.com/Search?keyword=pytorch%20%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5&amp;amp;enc=utf-8&amp;amp;wq=pytorch%20%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5&amp;amp;pvid=8b0d91d7108845ad8cbaf596326f3eb3" rel="nofollow"&gt;京东购买链接&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://search.dangdang.com/?key=pytorch%20%C8%EB%C3%C5%D3%EB%CA%B5%BC%F9&amp;amp;act=input" rel="nofollow"&gt;当当购买链接&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>chenyuntc</author><guid isPermaLink="false">https://github.com/chenyuntc/pytorch-book</guid><pubDate>Thu, 28 Nov 2019 00:09:00 GMT</pubDate></item><item><title>czy36mengfei/tensorflow2_tutorials_chinese #10 in Jupyter Notebook, This month</title><link>https://github.com/czy36mengfei/tensorflow2_tutorials_chinese</link><description>&lt;p&gt;&lt;i&gt;tensorflow2中文教程，持续更新(当前版本:tensorflow2.0)，tag: tensorflow 2.0 tutorials&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow2_tutorials_chinese" class="anchor" aria-hidden="true" href="#tensorflow2_tutorials_chinese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;tensorflow2_tutorials_chinese&lt;/h1&gt;
&lt;p&gt;tensorflow2中文教程，持续更新（不定期更新）&lt;/p&gt;
&lt;p&gt;tensorflow 2.0 正式版已上线， 后面将持续根据TensorFlow2的相关教程和学习资料。&lt;/p&gt;
&lt;p&gt;最新tensorflow教程和相关资源，请关注微信公众号：DoitNLP，
后面我会在DoitNLP上，持续更新深度学习、NLP、Tensorflow的相关教程和前沿资讯，它将成为我们一起学习tensorflow的大本营。&lt;/p&gt;
&lt;p&gt;当前tensorflow版本：tensorflow2.0&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最全Tensorflow 2.0 教程持续更新：&lt;/strong&gt;
&lt;a href="https://zhuanlan.zhihu.com/p/59507137" rel="nofollow"&gt;https://zhuanlan.zhihu.com/p/59507137&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本教程主要由tensorflow2.0官方教程的个人学习复现笔记整理而来，并借鉴了一些keras构造神经网络的方法，中文讲解，方便喜欢阅读中文教程的朋友，tensorflow官方教程：&lt;a href="https://www.tensorflow.org" rel="nofollow"&gt;https://www.tensorflow.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58825020" rel="nofollow"&gt;TensorFlow 2.0 教程- Keras 快速入门&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58825710" rel="nofollow"&gt;TensorFlow 2.0 教程-keras 函数api&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58826227" rel="nofollow"&gt;TensorFlow 2.0 教程-使用keras训练模型&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59481536" rel="nofollow"&gt;TensorFlow 2.0 教程-用keras构建自己的网络层&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59481985" rel="nofollow"&gt;TensorFlow 2.0 教程-keras模型保存和序列化&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59482373" rel="nofollow"&gt;TensorFlow 2.0 教程-eager模式&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59482589" rel="nofollow"&gt;TensorFlow 2.0 教程-Variables&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59482934" rel="nofollow"&gt;TensorFlow 2.0 教程--AutoGraph&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TensorFlow 2.0 深度学习实践&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59506238" rel="nofollow"&gt;TensorFlow2.0 教程-图像分类&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59506402" rel="nofollow"&gt;TensorFlow2.0 教程-文本分类&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59506543" rel="nofollow"&gt;TensorFlow2.0 教程-过拟合和欠拟合&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60232704" rel="nofollow"&gt;TensorFlow2.0教程-结构化数据分类&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60238056" rel="nofollow"&gt;TensorFlow2.0教程-回归&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60485936" rel="nofollow"&gt;TensorFlow2.0教程-保持和读取模型&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TensorFlow 2.0 基础网络结构&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60899040" rel="nofollow"&gt;TensorFlow2教程-基础MLP网络&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60900318" rel="nofollow"&gt;TensorFlow2教程-MLP及深度学习常见技巧&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60900649" rel="nofollow"&gt;TensorFlow2教程-基础CNN网络&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60900902" rel="nofollow"&gt;TensorFlow2教程-CNN变体网络&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60901179" rel="nofollow"&gt;TensorFlow2教程-文本卷积&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60966714" rel="nofollow"&gt;TensorFlow2教程-LSTM和GRU&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61077346" rel="nofollow"&gt;TensorFlow2教程-自编码器&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61080045" rel="nofollow"&gt;TensorFlow2教程-卷积自编码器&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61224215" rel="nofollow"&gt;TensorFlow2教程-词嵌入&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61280722" rel="nofollow"&gt;TensorFlow2教程-DCGAN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61400276" rel="nofollow"&gt;TensorFlow2教程-使用Estimator构建Boosted trees&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TensorFlow 2.0 安装&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61472293" rel="nofollow"&gt;TensorFlow2教程-Ubuntu安装TensorFlow 2.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/62036280" rel="nofollow"&gt;TensorFlow2教程-Windows安装tensorflow2.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;完整tensorflow2.0教程代码请看&lt;a href="https://github.com/czy36mengfei/tensorflow2_tutorials_chinese"&gt;tensorflow2.0：中文教程tensorflow2_tutorials_chinese(欢迎star)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;更多TensorFlow 2.0 入门教程请持续关注专栏：&lt;a href="https://zhuanlan.zhihu.com/c_1091021863043624960" rel="nofollow"&gt;Tensorflow2教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;深度学习入门书籍和资源推荐：&lt;a href="https://zhuanlan.zhihu.com/p/65371424" rel="nofollow"&gt;https://zhuanlan.zhihu.com/p/65371424&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>czy36mengfei</author><guid isPermaLink="false">https://github.com/czy36mengfei/tensorflow2_tutorials_chinese</guid><pubDate>Thu, 28 Nov 2019 00:10:00 GMT</pubDate></item><item><title>nianticlabs/monodepth2 #11 in Jupyter Notebook, This month</title><link>https://github.com/nianticlabs/monodepth2</link><description>&lt;p&gt;&lt;i&gt;Monocular depth estimation from a single image&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-monodepth2" class="anchor" aria-hidden="true" href="#monodepth2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Monodepth2&lt;/h1&gt;
&lt;p&gt;This is the reference PyTorch implementation for training and testing depth estimation models using the method described in&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Digging into Self-Supervised Monocular Depth Prediction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/C.Godard/" rel="nofollow"&gt;Clément Godard&lt;/a&gt;, &lt;a href="http://vision.caltech.edu/~macaodha/" rel="nofollow"&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href="http://www.michaelfirman.co.uk" rel="nofollow"&gt;Michael Firman&lt;/a&gt; and &lt;a href="http://www0.cs.ucl.ac.uk/staff/g.brostow/" rel="nofollow"&gt;Gabriel J. Brostow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1806.01260" rel="nofollow"&gt;ICCV 2019&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="assets/teaser.gif"&gt;&lt;img src="assets/teaser.gif" alt="example input output gif" width="600" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;This code is for non-commercial use; please see the &lt;a href="LICENSE"&gt;license file&lt;/a&gt; for terms.&lt;/p&gt;
&lt;p&gt;If you find our work useful in your research please consider citing our paper:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{monodepth2,
  title     = {Digging into Self-Supervised Monocular Depth Prediction},
  author    = {Cl{\'{e}}ment Godard and
               Oisin {Mac Aodha} and
               Michael Firman and
               Gabriel J. Brostow},
  booktitle = {The International Conference on Computer Vision (ICCV)},
  month = {October},
year = {2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-️-setup" class="anchor" aria-hidden="true" href="#️-setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;⚙️&lt;/g-emoji&gt; Setup&lt;/h2&gt;
&lt;p&gt;Assuming a fresh &lt;a href="https://www.anaconda.com/download/" rel="nofollow"&gt;Anaconda&lt;/a&gt; distribution, you can install the dependencies with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install pytorch=0.4.1 torchvision=0.2.1 -c pytorch
pip install tensorboardX==1.4
conda install opencv=3.3.1   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; just needed for evaluation&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We ran our experiments with PyTorch 0.4.1, CUDA 9.1, Python 3.6.6 and Ubuntu 18.04.
We have also successfully trained models with PyTorch 1.0, and our code is compatible with Python 2.7. You may have issues installing OpenCV version 3.3.1 if you use Python 3.7, we recommend to create a virtual environment with Python 3.6.6 &lt;code&gt;conda create -n monodepth2 python=3.6.6 anaconda &lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;&lt;a id="user-content-️-prediction-for-a-single-image" class="anchor" aria-hidden="true" href="#️-prediction-for-a-single-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;🖼️&lt;/g-emoji&gt; Prediction for a single image&lt;/h2&gt;
&lt;p&gt;You can predict depth for a single image with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python test_simple.py --image_path assets/test_image.jpg --model_name mono+stereo_640x192&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On its first run this will download the &lt;code&gt;mono+stereo_640x192&lt;/code&gt; pretrained model (99MB) into the &lt;code&gt;models/&lt;/code&gt; folder.
We provide the following  options for &lt;code&gt;--model_name&lt;/code&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;--model_name&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;Training modality&lt;/th&gt;
&lt;th&gt;Imagenet pretrained?&lt;/th&gt;
&lt;th&gt;Model resolution&lt;/th&gt;
&lt;th&gt;KITTI abs. rel. error&lt;/th&gt;
&lt;th&gt;delta &amp;lt; 1.25&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.115&lt;/td&gt;
&lt;td&gt;0.877&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip" rel="nofollow"&gt;&lt;code&gt;stereo_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.109&lt;/td&gt;
&lt;td&gt;0.864&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono+stereo_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.106&lt;/td&gt;
&lt;td&gt;0.874&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip" rel="nofollow"&gt;&lt;code&gt;mono_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;0.115&lt;/td&gt;
&lt;td&gt;0.879&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip" rel="nofollow"&gt;&lt;code&gt;stereo_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;0.107&lt;/td&gt;
&lt;td&gt;0.874&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip" rel="nofollow"&gt;&lt;code&gt;mono+stereo_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;0.106&lt;/td&gt;
&lt;td&gt;0.876&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.132&lt;/td&gt;
&lt;td&gt;0.845&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip" rel="nofollow"&gt;&lt;code&gt;stereo_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.130&lt;/td&gt;
&lt;td&gt;0.831&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono+stereo_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.127&lt;/td&gt;
&lt;td&gt;0.836&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can also download models trained on the odometry split with &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_odom_640x192.zip" rel="nofollow"&gt;monocular&lt;/a&gt; and &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_odom_640x192.zip" rel="nofollow"&gt;mono+stereo&lt;/a&gt; training modalities.
Finally, we provide resnet 50 depth estimation models trained with &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_640x192.zip" rel="nofollow"&gt;ImageNet pretrained weights&lt;/a&gt; and &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_no_pt_640x192.zip" rel="nofollow"&gt;trained from scratch&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--kitti-training-data" class="anchor" aria-hidden="true" href="#-kitti-training-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="floppy_disk" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png"&gt;💾&lt;/g-emoji&gt; KITTI training data&lt;/h2&gt;
&lt;p&gt;You can download the entire &lt;a href="http://www.cvlibs.net/datasets/kitti/raw_data.php" rel="nofollow"&gt;raw KITTI dataset&lt;/a&gt; by running:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;wget -i splits/kitti_archives_to_download.txt -P kitti_data/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then unzip with&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; kitti_data
unzip &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;*.zip&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; ..&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; it weighs about &lt;strong&gt;175GB&lt;/strong&gt;, so make sure you have enough space to unzip too!&lt;/p&gt;
&lt;p&gt;Our default settings expect that you have converted the png images to jpeg with this command, &lt;strong&gt;which also deletes the raw KITTI &lt;code&gt;.png&lt;/code&gt; files&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;find kitti_data/ -name &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;*.png&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;|&lt;/span&gt; parallel &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg &amp;amp;&amp;amp; rm {}&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;or&lt;/strong&gt; you can skip this conversion step and train from raw png files by adding the flag &lt;code&gt;--png&lt;/code&gt; when training, at the expense of slower load times.&lt;/p&gt;
&lt;p&gt;The above conversion command creates images which match our experiments, where KITTI &lt;code&gt;.png&lt;/code&gt; images were converted to &lt;code&gt;.jpg&lt;/code&gt; on Ubuntu 16.04 with default chroma subsampling &lt;code&gt;2x2,1x1,1x1&lt;/code&gt;.
We found that Ubuntu 18.04 defaults to &lt;code&gt;2x2,2x2,2x2&lt;/code&gt;, which gives different results, hence the explicit parameter in the conversion command.&lt;/p&gt;
&lt;p&gt;You can also place the KITTI dataset wherever you like and point towards it with the &lt;code&gt;--data_path&lt;/code&gt; flag during training and evaluation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Splits&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The train/test/validation splits are defined in the &lt;code&gt;splits/&lt;/code&gt; folder.
By default, the code will train a depth model using &lt;a href="https://github.com/tinghuiz/SfMLearner"&gt;Zhou's subset&lt;/a&gt; of the standard Eigen split of KITTI, which is designed for monocular training.
You can also train a model using the new &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="nofollow"&gt;benchmark split&lt;/a&gt; or the &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_odometry.php" rel="nofollow"&gt;odometry split&lt;/a&gt; by setting the &lt;code&gt;--split&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custom dataset&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can train on a custom monocular or stereo dataset by writing a new dataloader class which inherits from &lt;code&gt;MonoDataset&lt;/code&gt; – see the &lt;code&gt;KITTIDataset&lt;/code&gt; class in &lt;code&gt;datasets/kitti_dataset.py&lt;/code&gt; for an example.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--training" class="anchor" aria-hidden="true" href="#-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="hourglass_flowing_sand" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/23f3.png"&gt;⏳&lt;/g-emoji&gt; Training&lt;/h2&gt;
&lt;p&gt;By default models and tensorboard event files are saved to &lt;code&gt;~/tmp/&amp;lt;model_name&amp;gt;&lt;/code&gt;.
This can be changed with the &lt;code&gt;--log_dir&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Monocular training:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name mono_model&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Stereo training:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our code defaults to using Zhou's subsampled Eigen training data. For stereo-only training we have to specify that we want to use the full Eigen training set – see paper for details.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name stereo_model \
  --frame_ids 0 --use_stereo --split eigen_full&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Monocular + stereo training:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name mono+stereo_model \
  --frame_ids 0 -1 1 --use_stereo&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-gpus" class="anchor" aria-hidden="true" href="#gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GPUs&lt;/h3&gt;
&lt;p&gt;The code can only be run on a single GPU.
You can specify which GPU to use with the &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; environment variable:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;CUDA_VISIBLE_DEVICES=2 python train.py --model_name mono_model&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All our experiments were performed on a single NVIDIA Titan Xp.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Training modality&lt;/th&gt;
&lt;th&gt;Approximate GPU memory&lt;/th&gt;
&lt;th&gt;Approximate training time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;9GB&lt;/td&gt;
&lt;td&gt;12 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;6GB&lt;/td&gt;
&lt;td&gt;8 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;11GB&lt;/td&gt;
&lt;td&gt;15 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content--finetuning-a-pretrained-model" class="anchor" aria-hidden="true" href="#-finetuning-a-pretrained-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="minidisc" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bd.png"&gt;💽&lt;/g-emoji&gt; Finetuning a pretrained model&lt;/h3&gt;
&lt;p&gt;Add the following to the training command to load an existing model for finetuning:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name finetuned_mono --load_weights_folder &lt;span class="pl-k"&gt;~&lt;/span&gt;/tmp/mono_model/models/weights_19&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content--other-training-options" class="anchor" aria-hidden="true" href="#-other-training-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f527.png"&gt;🔧&lt;/g-emoji&gt; Other training options&lt;/h3&gt;
&lt;p&gt;Run &lt;code&gt;python train.py -h&lt;/code&gt; (or look at &lt;code&gt;options.py&lt;/code&gt;) to see the range of other training options, such as learning rates and ablation settings.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--kitti-evaluation" class="anchor" aria-hidden="true" href="#-kitti-evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="bar_chart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png"&gt;📊&lt;/g-emoji&gt; KITTI evaluation&lt;/h2&gt;
&lt;p&gt;To prepare the ground truth depth maps run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python export_gt_depth.py --data_path kitti_data --split eigen
python export_gt_depth.py --data_path kitti_data --split eigen_benchmark&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;...assuming that you have placed the KITTI dataset in the default location of &lt;code&gt;./kitti_data/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The following example command evaluates the epoch 19 weights of a model named &lt;code&gt;mono_model&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_depth.py --load_weights_folder &lt;span class="pl-k"&gt;~&lt;/span&gt;/tmp/mono_model/models/weights_19/ --eval_mono&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For stereo models, you must use the &lt;code&gt;--eval_stereo&lt;/code&gt; flag (see note below):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_depth.py --load_weights_folder &lt;span class="pl-k"&gt;~&lt;/span&gt;/tmp/stereo_model/models/weights_19/ --eval_stereo&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you train your own model with our code you are likely to see slight differences to the publication results due to randomization in the weights initialization and data loading.&lt;/p&gt;
&lt;p&gt;An additional parameter &lt;code&gt;--eval_split&lt;/code&gt; can be set.
The three different values possible for &lt;code&gt;eval_split&lt;/code&gt; are explained here:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;--eval_split&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;Test set size&lt;/th&gt;
&lt;th&gt;For models trained with...&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;eigen&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;697&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--split eigen_zhou&lt;/code&gt; (default) or &lt;code&gt;--split eigen_full&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The standard Eigen test files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;eigen_benchmark&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;652&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--split eigen_zhou&lt;/code&gt; (default) or &lt;code&gt;--split eigen_full&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Evaluate with the improved ground truth from the &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="nofollow"&gt;new KITTI depth benchmark&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;benchmark&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--split benchmark&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="nofollow"&gt;new KITTI depth benchmark&lt;/a&gt; test files.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Because no ground truth is available for the new KITTI depth benchmark, no scores will be reported  when &lt;code&gt;--eval_split benchmark&lt;/code&gt; is set.
Instead, a set of &lt;code&gt;.png&lt;/code&gt; images will be saved to disk ready for upload to the evaluation server.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;External disparities evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Finally you can also use &lt;code&gt;evaluate_depth.py&lt;/code&gt; to evaluate raw disparities (or inverse depth) from other methods by using the &lt;code&gt;--ext_disp_to_eval&lt;/code&gt; flag:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_depth.py --ext_disp_to_eval &lt;span class="pl-k"&gt;~&lt;/span&gt;/other_method_disp.npy&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;g-emoji class="g-emoji" alias="camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f7.png"&gt;📷&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f7.png"&gt;📷&lt;/g-emoji&gt; Note on stereo evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our stereo models are trained with an effective baseline of &lt;code&gt;0.1&lt;/code&gt; units, while the actual KITTI stereo rig has a baseline of &lt;code&gt;0.54m&lt;/code&gt;. This means a scaling of &lt;code&gt;5.4&lt;/code&gt; must be applied for evaluation.
In addition, for models trained with stereo supervision we disable median scaling.
Setting the &lt;code&gt;--eval_stereo&lt;/code&gt; flag when evaluating will automatically disable median scaling and scale predicted depths by &lt;code&gt;5.4&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;g-emoji class="g-emoji" alias="arrow_heading_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2934.png"&gt;⤴️&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="arrow_heading_down" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2935.png"&gt;⤵️&lt;/g-emoji&gt; Odometry evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We include code for evaluating poses predicted by models trained with &lt;code&gt;--split odom --dataset kitti_odom --data_path /path/to/kitti/odometry/dataset&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this evaluation, the &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_odometry.php" rel="nofollow"&gt;KITTI odometry dataset&lt;/a&gt; &lt;strong&gt;(color, 65GB)&lt;/strong&gt; and &lt;strong&gt;ground truth poses&lt;/strong&gt; zip files must be downloaded.
As above, we assume that the pngs have been converted to jpgs.&lt;/p&gt;
&lt;p&gt;If this data has been unzipped to folder &lt;code&gt;kitti_odom&lt;/code&gt;, a model can be evaluated with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_pose.py --eval_split odom_9 --load_weights_folder ./odom_split.M/models/weights_29 --data_path kitti_odom/
python evaluate_pose.py --eval_split odom_10 --load_weights_folder ./odom_split.M/models/weights_29 --data_path kitti_odom/&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content--precomputed-results" class="anchor" aria-hidden="true" href="#-precomputed-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="package" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e6.png"&gt;📦&lt;/g-emoji&gt; Precomputed results&lt;/h2&gt;
&lt;p&gt;You can download our precomputed disparity predictions from the following links:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Training modality&lt;/th&gt;
&lt;th&gt;Input size&lt;/th&gt;
&lt;th&gt;&lt;code&gt;.npy&lt;/code&gt; filesize&lt;/th&gt;
&lt;th&gt;Eigen disparities&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;343 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;343 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;343 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;914 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;914 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;914 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-️-license" class="anchor" aria-hidden="true" href="#️-license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="woman_judge" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f469-2696.png"&gt;👩‍⚖️&lt;/g-emoji&gt; License&lt;/h2&gt;
&lt;p&gt;Copyright © Niantic, Inc. 2019. Patent Pending.
All rights reserved.
Please see the &lt;a href="LICENSE"&gt;license file&lt;/a&gt; for terms.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>nianticlabs</author><guid isPermaLink="false">https://github.com/nianticlabs/monodepth2</guid><pubDate>Thu, 28 Nov 2019 00:11:00 GMT</pubDate></item><item><title>Pierian-Data/Complete-Python-3-Bootcamp #12 in Jupyter Notebook, This month</title><link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link><description>&lt;p&gt;&lt;i&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-complete-python-3-bootcamp" class="anchor" aria-hidden="true" href="#complete-python-3-bootcamp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete-Python-3-Bootcamp&lt;/h1&gt;
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;
&lt;p&gt;Get it now for 95% off with the link:
&lt;a href="https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB" rel="nofollow"&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pierian-Data</author><guid isPermaLink="false">https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</guid><pubDate>Thu, 28 Nov 2019 00:12:00 GMT</pubDate></item><item><title>rasbt/deeplearning-models #13 in Jupyter Notebook, This month</title><link>https://github.com/rasbt/deeplearning-models</link><description>&lt;p&gt;&lt;i&gt;A collection of various deep learning architectures, models, and tips&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667" alt="Python 3.7" data-canonical-src="https://img.shields.io/badge/Python-3.7-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-deep-learning-models" class="anchor" aria-hidden="true" href="#deep-learning-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning Models&lt;/h1&gt;
&lt;p&gt;A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-traditional-machine-learning" class="anchor" aria-hidden="true" href="#traditional-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Traditional Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Perceptron&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/perceptron.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/perceptron.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/perceptron.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/perceptron.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Logistic Regression&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/logistic-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/logistic-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/logistic-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/logistic-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Softmax Regression (Multinomial Logistic Regression)&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/softmax-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/softmax-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/softmax-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/softmax-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-multilayer-perceptrons" class="anchor" aria-hidden="true" href="#multilayer-perceptrons"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multilayer Perceptrons&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Multilayer Perceptron&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Dropout&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-dropout.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-dropout.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-dropout.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-dropout.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Batch Normalization&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Backpropagation from Scratch&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-lowlevel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-lowlevel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-fromscratch__sigmoid-mse.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-fromscratch__sigmoid-mse.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-basic" class="anchor" aria-hidden="true" href="#basic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/cnn/cnn-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/cnn/cnn-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network with He Initialization&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-he-init.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-he-init.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-concepts" class="anchor" aria-hidden="true" href="#concepts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Concepts&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Replacing Fully-Connnected by Equivalent Convolutional Layers&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/fc-to-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/fc-to-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-fully-convolutional" class="anchor" aria-hidden="true" href="#fully-convolutional"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fully Convolutional&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Fully Convolutional Neural Network&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-allconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-allconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-lenet" class="anchor" aria-hidden="true" href="#lenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LeNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;LeNet-5 on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LeNet-5 on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LeNet-5 on QuickDraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-alexnet" class="anchor" aria-hidden="true" href="#alexnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AlexNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;AlexNet on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-vgg" class="anchor" aria-hidden="true" href="#vgg"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VGG&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network VGG-16&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/cnn/cnn-vgg16.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/cnn/cnn-vgg16.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;VGG-16 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network VGG-19&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg19.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg19.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-densenet" class="anchor" aria-hidden="true" href="#densenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DenseNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DenseNet-121 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-densenet121-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-densenet121-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;DenseNet-121 Image Classifier Trained on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-densenet121-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-densenet121-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-resnet" class="anchor" aria-hidden="true" href="#resnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ResNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ResNet and Residual Blocks&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/resnet-ex-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/resnet-ex-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-18 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-18 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Object Classifier Trained on QuickDraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-50 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet50-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet50-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-50 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-101 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet101-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-101 Trained on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-152 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet152-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet152-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-network-in-network" class="anchor" aria-hidden="true" href="#network-in-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network in Network&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Network in Network CIFAR-10 Classifier&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-metric-learning" class="anchor" aria-hidden="true" href="#metric-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Metric Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Siamese Network with Multilayer Perceptrons&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/metric/siamese-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/metric/siamese-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-autoencoders" class="anchor" aria-hidden="true" href="#autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Autoencoders&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-fully-connected-autoencoders" class="anchor" aria-hidden="true" href="#fully-connected-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fully-connected Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Autoencoder (MNIST)&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Autoencoder (MNIST) + Scikit-Learn Random Forest Classifier&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-basic-with-rf.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-basic-with-rf.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-convolutional-autoencoders" class="anchor" aria-hidden="true" href="#convolutional-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions / Transposed Convolutions&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-deconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-deconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions and Continuous Jaccard Distance&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv-jaccard.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv-jaccard.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions (without pooling operations)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv-nopool.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv-nopool.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-conv-nneighbor.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-conv-nneighbor.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation -- Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation -- Trained on Quickdraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor-quickdraw-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor-quickdraw-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-variational-autoencoders" class="anchor" aria-hidden="true" href="#variational-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Variational Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Variational Autoencoder&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-var.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-var.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Variational Autoencoder&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-var.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-var.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-conditional-variational-autoencoders" class="anchor" aria-hidden="true" href="#conditional-variational-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conditional Variational Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Conditional Variational Autoencoder (with labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cvae.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cvae.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Conditional Variational Autoencoder (without labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cvae_no-out-concat.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cvae_no-out-concat.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Conditional Variational Autoencoder (with labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cnn-cvae.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cnn-cvae.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Conditional Variational Autoencoder (without labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cnn-cvae_no-out-concat.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cnn-cvae_no-out-concat.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-generative-adversarial-networks-gans" class="anchor" aria-hidden="true" href="#generative-adversarial-networks-gans"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks (GANs)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fully Connected GAN on MNIST&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Fully Connected Wasserstein GAN on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/wgan-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/wgan-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional GAN on MNIST&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional GAN on MNIST with Label Smoothing&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan-conv-smoothing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan-conv-smoothing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan-conv-smoothing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan-conv-smoothing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Wasserstein GAN on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/dc-wgan-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/dc-wgan-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-graph-neural-networks-gnns" class="anchor" aria-hidden="true" href="#graph-neural-networks-gnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Graph Neural Networks (GNNs)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most Basic Graph Neural Network with Gaussian Filter on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Basic Graph Neural Network with Edge Prediction on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-edge-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-edge-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Basic Graph Neural Network with Spectral Graph Convolution on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-graph-spectral-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-graph-spectral-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-recurrent-neural-networks-rnns" class="anchor" aria-hidden="true" href="#recurrent-neural-networks-rnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks (RNNs)&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-many-to-one-sentiment-analysis--classification" class="anchor" aria-hidden="true" href="#many-to-one-sentiment-analysis--classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Many-to-one: Sentiment Analysis / Classification&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple single-layer RNN (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_simple_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_simple_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A simple single-layer RNN with packed sequences to ignore padding characters (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_simple_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_simple_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells (IMDB) and pre-trained GloVe word vectors&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells and Own Dataset in CSV Format (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with GRU cells (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer bi-directional RNN (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-many-to-many--sequence-to-sequence" class="anchor" aria-hidden="true" href="#many-to-many--sequence-to-sequence"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Many-to-Many / Sequence-to-Sequence&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple character RNN to generate new text (Charles Dickens)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ordinal-regression" class="anchor" aria-hidden="true" href="#ordinal-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ordinal Regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ordinal Regression CNN -- CORAL w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-coral-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-coral-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ordinal Regression CNN -- Niu et al. 2016 w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-niu-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-niu-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ordinal Regression CNN -- Beckham and Pal 2016 w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-beckham2016-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-beckham2016-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tips-and-tricks" class="anchor" aria-hidden="true" href="#tips-and-tricks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tips and Tricks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cyclical Learning Rate&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/cyclical-learning-rate.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/cyclical-learning-rate.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Annealing with Increasing the Batch Size (w. CIFAR-10 &amp;amp; AlexNet)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/cnn-alexnet-cifar10-batchincrease.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/cnn-alexnet-cifar10-batchincrease.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Gradient Clipping (w. MLP on MNIST)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/gradclipping_mlp.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/gradclipping_mlp.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-transfer-learning" class="anchor" aria-hidden="true" href="#transfer-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transfer Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Transfer Learning Example (VGG16 pre-trained on ImageNet for Cifar-10)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;   [PyTorch: &lt;a href="pytorch_ipynb/transfer/transferlearning-vgg16-cifar10-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/transfer/transferlearning-vgg16-cifar10-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pytorch-workflows-and-mechanics" class="anchor" aria-hidden="true" href="#pytorch-workflows-and-mechanics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Workflows and Mechanics&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-custom-datasets" class="anchor" aria-hidden="true" href="#custom-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- CSV files converted to HDF5&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Face Images from CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from Quickdraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from the Street View House Number (SVHN) Dataset&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Asian Face Dataset (AFAD)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Dating Historical Color Images&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-and-preprocessing" class="anchor" aria-hidden="true" href="#training-and-preprocessing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Preprocessing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Generating Validation Set Splits&lt;br&gt;
[PyTorch]: &lt;a href="pytorch_ipynb/mechanics/validation-splits.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/validation-splits.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Dataloading with Pinned Memory&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-cifar10-pinmem.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-cifar10-pinmem.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Standardizing Images&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-standardized.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-standardized.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Image Transformation Examples&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/torchvision-transform-examples.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/torchvision-transform-examples.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Char-RNN with Own Text File&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sentiment Classification RNN with Own CSV File&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-parallel-computing" class="anchor" aria-hidden="true" href="#parallel-computing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Parallel Computing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Using Multiple GPUs with DataParallel -- VGG-16 Gender Classifier on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-other" class="anchor" aria-hidden="true" href="#other"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Sequential API and hooks&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/mlp-sequential.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/mlp-sequential.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Weight Sharing Within a Layer&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/cnn-weight-sharing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/cnn-weight-sharing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Plotting Live Training Performance in Jupyter Notebooks with just Matplotlib&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/plot-jupyter-matplotlib.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/plot-jupyter-matplotlib.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-autograd" class="anchor" aria-hidden="true" href="#autograd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Autograd&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Getting Gradients of an Intermediate Variable in PyTorch&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/manual-gradients.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/manual-gradients.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tensorflow-workflows-and-mechanics" class="anchor" aria-hidden="true" href="#tensorflow-workflows-and-mechanics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Workflows and Mechanics&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-custom-datasets-1" class="anchor" aria-hidden="true" href="#custom-datasets-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Chunking an Image Dataset for Minibatch Training using NumPy NPZ Archives&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Storing an Image Dataset for Minibatch Training using HDF5&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using Input Pipelines to Read Data from TFRecords Files&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/tfrecords.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/tfrecords.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using Queue Runners to Feed Images Directly from Disk&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/file-queues.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/file-queues.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using TensorFlow's Dataset API&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/dataset-api.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/dataset-api.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-and-preprocessing-1" class="anchor" aria-hidden="true" href="#training-and-preprocessing-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Preprocessing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Saving and Loading Trained Models -- from TensorFlow Checkpoint Files and NumPy NPZ Archives&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/saving-and-reloading-models.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/saving-and-reloading-models.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rasbt</author><guid isPermaLink="false">https://github.com/rasbt/deeplearning-models</guid><pubDate>Thu, 28 Nov 2019 00:13:00 GMT</pubDate></item><item><title>microsoft/QuantumKatas #14 in Jupyter Notebook, This month</title><link>https://github.com/microsoft/QuantumKatas</link><description>&lt;p&gt;&lt;i&gt;Tutorials and programming exercises for learning Q# and quantum computing&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The Quantum Katas are a series of self-paced tutorials to help you learn quantum computing and Q# programming.&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="new" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f195.png"&gt;🆕&lt;/g-emoji&gt; &lt;em&gt;(July 2019)&lt;/em&gt; The Quantum Katas now include Jupyter Notebook tutorials on quantum computing! Each tutorial combines theoretical explanations with Q# code snippets and programming exercises. See &lt;a href="https://mybinder.org/v2/gh/Microsoft/QuantumKatas/master?filepath=index.ipynb" rel="nofollow"&gt;index.ipynb&lt;/a&gt; for the list of all tutorials and instructions on running them online.&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="new" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f195.png"&gt;🆕&lt;/g-emoji&gt; &lt;em&gt;(April 2019)&lt;/em&gt; The Quantum Katas are now available as Jupyter Notebooks! See &lt;a href="https://mybinder.org/v2/gh/Microsoft/QuantumKatas/master?filepath=index.ipynb" rel="nofollow"&gt;index.ipynb&lt;/a&gt; for the list of all Kata Notebooks and instructions on running them online.&lt;/p&gt;
&lt;p&gt;Each kata is a separate project that includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A sequence of tasks progressing from easy to hard.
Each task requires you to fill in some code. The first task might require just one line, and the last one might require rather complicated code.&lt;/li&gt;
&lt;li&gt;A testing framework that sets up, runs, and validates your solutions.
Each task is covered by a &lt;a href="https://docs.microsoft.com/en-us/visualstudio/test/getting-started-with-unit-testing" rel="nofollow"&gt;unit test&lt;/a&gt; which initially fails. Once you write the code to make the test pass, you can move on to the next task.&lt;/li&gt;
&lt;li&gt;Links to quantum computing and Q# reference material you might need to solve the tasks.&lt;/li&gt;
&lt;li&gt;Hints and reference solutions to help you if you're stuck.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#tutorial-topics"&gt;List of tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kata-topics"&gt;List of katas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#run-as-notebook"&gt;Run the katas and tutorials online&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kata-locally"&gt;Run the katas locally&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#download"&gt;Download the Quantum Katas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kata-as-project"&gt;Run a kata as a Q# project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tests"&gt;Run kata tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#docker"&gt;Run katas locally with Docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#code-of-conduct"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-list-of-tutorials-" class="anchor" aria-hidden="true" href="#list-of-tutorials-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;List of tutorials &lt;a name="user-content-tutorial-topics"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-tutorial-topics"&gt;
&lt;/a&gt;&lt;ul&gt;&lt;a name="user-content-tutorial-topics"&gt;
&lt;/a&gt;&lt;li&gt;&lt;a name="user-content-tutorial-topics"&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;a href="./tutorials/ComplexArithmetic/"&gt;Complex arithmetic&lt;/a&gt;&lt;/strong&gt;.
Learn about complex numbers and the mathematics required to work with quantum computing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/LinearAlgebra/"&gt;Linear algebra&lt;/a&gt;&lt;/strong&gt;.
Learn about vectors and matrices used to represent quantum states and quantum operations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/Qubit/"&gt;The qubit&lt;/a&gt;&lt;/strong&gt;.
Learn what a qubit is.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/SingleQubitGates/"&gt;Single-qubit gates&lt;/a&gt;&lt;/strong&gt;.
Learn what a quantum gate is and about the most common single-qubit gates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/MultiQubitSystems/"&gt;Multi-qubit systems&lt;/a&gt;&lt;/strong&gt;.
Learn to represent multi-qubit systems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/MultiQubitGates/"&gt;Multi-qubit gates&lt;/a&gt;&lt;/strong&gt;.
Learn about the most common multi-qubit gates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/ExploringDeutschJozsaAlgorithm/"&gt;Exploring Deutsch–Jozsa algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn to implement classical functions and equivalent quantum oracles, and compare the quantum
solution to the Deutsch–Jozsa problem to a classical one.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/ExploringGroversAlgorithm/"&gt;Exploring Grover's search algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn more about Grover's search algorithm, picking up where the &lt;a href="./GroversAlgorithm/"&gt;Grover's algorithm kata&lt;/a&gt; left off.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-list-of-katas-" class="anchor" aria-hidden="true" href="#list-of-katas-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;List of Katas &lt;a name="user-content-kata-topics"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-kata-topics"&gt;
&lt;h4&gt;&lt;a id="user-content-quantum-computing-concepts" class="anchor" aria-hidden="true" href="#quantum-computing-concepts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quantum computing concepts&lt;/h4&gt;
&lt;/a&gt;&lt;ul&gt;&lt;a name="user-content-kata-topics"&gt;
&lt;/a&gt;&lt;li&gt;&lt;a name="user-content-kata-topics"&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;a href="./BasicGates/"&gt;Basic quantum computing gates&lt;/a&gt;&lt;/strong&gt;.
Learn to apply the most common gates used in quantum computing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./Superposition/"&gt;Superposition&lt;/a&gt;&lt;/strong&gt;.
Learn to prepare superposition states.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./Measurements/"&gt;Measurements&lt;/a&gt;&lt;/strong&gt;.
Learn to distinguish quantum states using measurements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./JointMeasurements/"&gt;Joint measurements&lt;/a&gt;&lt;/strong&gt;.
Learn about using joint (parity) measurements to distinguish quantum states and to perform state transformations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-simple-algorithms" class="anchor" aria-hidden="true" href="#simple-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Simple algorithms&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./Teleportation/"&gt;Teleportation&lt;/a&gt;&lt;/strong&gt;.
Implement standard teleportation protocol and its variations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./SuperdenseCoding/"&gt;Superdense coding&lt;/a&gt;&lt;/strong&gt;.
Implement the superdense coding protocol.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./DeutschJozsaAlgorithm/"&gt;Deutsch–Jozsa algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn about quantum oracles which implement classical functions, and implement Bernstein–Vazirani and Deutsch–Jozsa algorithms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./SimonsAlgorithm/"&gt;Simon's algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn about Simon's algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-grovers-algorithm" class="anchor" aria-hidden="true" href="#grovers-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grover's algorithm&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./GroversAlgorithm/"&gt;Grover's algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn about Grover's search algorithm and how to write quantum oracles to use with it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./SolveSATWithGrover/"&gt;Solving SAT problems using Grover's algorithm&lt;/a&gt;&lt;/strong&gt;.
Explore Grover's search algorithm, using SAT problems as an example. Learn to implement quantum oracles based on the problem description instead of a hard-coded answer. Use Grover's algorithm to solve problems with an unknown number of solutions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./GraphColoring/"&gt;Solving graph coloring problems using Grover's algorithm&lt;/a&gt;&lt;/strong&gt;.
Continue the exploration of Grover's search algorithm, using graph coloring problems as an example.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-entanglement-games" class="anchor" aria-hidden="true" href="#entanglement-games"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Entanglement games&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./CHSHGame/"&gt;CHSH game&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./GHZGame/"&gt;GHZ game&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./MagicSquareGame"&gt;Mermin-Peres magic square game&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-miscellaneous" class="anchor" aria-hidden="true" href="#miscellaneous"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Miscellaneous&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./KeyDistribution_BB84/"&gt;BB84 protocol&lt;/a&gt;&lt;/strong&gt;.
Implement the BB84 key distribution algorithm.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./PhaseEstimation/"&gt;Phase estimation&lt;/a&gt;&lt;/strong&gt;.
Learn about phase estimation algorithms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./QEC_BitFlipCode/"&gt;Bit-flip error correcting code&lt;/a&gt;&lt;/strong&gt;.
Learn about a 3-qubit error correcting code for protecting against bit-flip errors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./RippleCarryAdder/"&gt;Ripple-carry adder&lt;/a&gt;&lt;/strong&gt;.
Build a ripple-carry adder on a quantum computer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./UnitaryPatterns/"&gt;Unitary Patterns*&lt;/a&gt;&lt;/strong&gt;.
Learn to implement unitaries with matrices that follow certain patterns of zero and non-zero elements.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-run-the-katas-and-tutorials-online-" class="anchor" aria-hidden="true" href="#run-the-katas-and-tutorials-online-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run the katas and tutorials online &lt;a name="user-content-run-as-notebook"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-run-as-notebook"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-run-as-notebook"&gt;The Quantum Katas are now available as Jupyter Notebooks! See &lt;/a&gt;&lt;a href="https://mybinder.org/v2/gh/Microsoft/QuantumKatas/master?filepath=index.ipynb" rel="nofollow"&gt;index.ipynb&lt;/a&gt; for the list of all katas and tutorials, and instructions to run them online.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-run-katas-locally-" class="anchor" aria-hidden="true" href="#run-katas-locally-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run katas locally &lt;a name="user-content-kata-locally"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-kata-locally"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-kata-locally"&gt;To use the Quantum Katas locally, you'll need the &lt;/a&gt;&lt;a href="https://docs.microsoft.com/quantum" rel="nofollow"&gt;Quantum Development Kit&lt;/a&gt;, available for Windows 10, macOS, and Linux.
If you don't already have the Quantum Development Kit installed, see &lt;a href="https://docs.microsoft.com/quantum/install-guide/" rel="nofollow"&gt;install guide for the Quantum Development Kit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For a quick Q# programming language reference sheet, see &lt;a href="./quickref/qsharp-quick-reference.pdf"&gt;Q# Language Quick Reference&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-download-the-quantum-katas-" class="anchor" aria-hidden="true" href="#download-the-quantum-katas-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download the Quantum Katas &lt;a name="user-content-download"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-download"&gt;
&lt;p&gt;If you have Git installed, clone the Microsoft/QuantumKatas repository:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/Microsoft/QuantumKatas.git&lt;/pre&gt;&lt;/div&gt;
&lt;/a&gt;&lt;blockquote&gt;&lt;a name="user-content-download"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-download"&gt;[!TIP]
Both Visual Studio 2019 and Visual Studio Code make it easy to clone repositories from within your development environment.
For details, see the &lt;/a&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/devops/repos/git/clone?view=azure-devops&amp;amp;tabs=visual-studio#clone-from-another-git-provider" rel="nofollow"&gt;Visual Studio 2019&lt;/a&gt; and &lt;a href="https://code.visualstudio.com/docs/editor/versioncontrol#_cloning-a-repository" rel="nofollow"&gt;Visual Studio Code&lt;/a&gt; documentation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you don't have Git installed, download the katas from &lt;a href="https://github.com/Microsoft/QuantumKatas/archive/master.zip"&gt;https://github.com/Microsoft/QuantumKatas/archive/master.zip&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run-a-kata-as-a-q-project-" class="anchor" aria-hidden="true" href="#run-a-kata-as-a-q-project-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run a kata as a Q# project &lt;a name="user-content-kata-as-project"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-kata-as-project"&gt;
&lt;p&gt;Each kata is in its own directory as a self-contained Q# project, solution and Jupyter Notebook triplet.
For instance, the BasicGates directory structure is:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;QuantumKatas/
  BasicGates/
    README.md                  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Instructions specific to this kata.&lt;/span&gt;
    .vscode/                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Metadata used by Visual Studio Code.&lt;/span&gt;
    BasicGates.sln             &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Visual Studio 2019 solution file.&lt;/span&gt;
    BasicGates.csproj          &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Project file used to build both classical and quantum code.&lt;/span&gt;
    BasicGates.ipynb           &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Jupyter Notebook front-end for this kata.&lt;/span&gt;

    Tasks.qs                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Q# source code that you will fill as you solve each task.&lt;/span&gt;
    Tests.qs                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Q# tests that verify your solutions.&lt;/span&gt;
    TestSuiteRunner.cs         &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; C# source code used to run the Q# tests.&lt;/span&gt;
    ReferenceImplementation.qs &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Q# source code containing solutions to the tasks.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To open the &lt;strong&gt;BasicGates&lt;/strong&gt; kata in Visual Studio 2019, open the &lt;strong&gt;QuantumKatas/BasicGates/BasicGates.sln&lt;/strong&gt; solution file.&lt;/p&gt;
&lt;p&gt;To open the &lt;strong&gt;BasicGates&lt;/strong&gt; kata in Visual Studio Code, open the &lt;strong&gt;QuantumKatas/BasicGates/&lt;/strong&gt; folder.
Press &lt;strong&gt;Ctrl + Shift + P&lt;/strong&gt; (or &lt;strong&gt;⌘ + Shift + P&lt;/strong&gt; on macOS) to open the &lt;strong&gt;Command Palette&lt;/strong&gt;. Type &lt;strong&gt;Open Folder&lt;/strong&gt; on Windows 10 or Linux or &lt;strong&gt;Open&lt;/strong&gt; on macOS.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[!TIP]
Almost all commands available in Visual Studio Code are in the Command Palette.
If you get stuck, press &lt;strong&gt;Ctrl + Shift + P&lt;/strong&gt; (or &lt;strong&gt;⌘ + Shift + P&lt;/strong&gt; on macOS) and start typing to search through all available commands.&lt;/p&gt;
&lt;p&gt;You can also launch Visual Studio Code from the command line:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ code QuantumKatas/BasicGates/&lt;/pre&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-run-kata-tests-" class="anchor" aria-hidden="true" href="#run-kata-tests-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-kata-as-project"&gt;Run kata tests &lt;/a&gt;&lt;a name="user-content-tests"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-tests"&gt;
&lt;p&gt;Once you have a kata open, it's time to run the tests using the following instructions.
Initially all tests will fail. Don't panic!
Open &lt;strong&gt;Tasks.qs&lt;/strong&gt; and start filling in the code to complete the tasks. Each task is covered by a unit test. Once you fill in the correct code for a task, rebuild the project and re-run the tests, and the corresponding unit test will pass.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-visual-studio-2019" class="anchor" aria-hidden="true" href="#visual-studio-2019"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Studio 2019&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Build the solution.&lt;/li&gt;
&lt;li&gt;From the main menu, open &lt;strong&gt;Test Explorer&lt;/strong&gt; (&lt;strong&gt;Test&lt;/strong&gt; &amp;gt; &lt;strong&gt;Windows&lt;/strong&gt;) and select &lt;strong&gt;Run All&lt;/strong&gt; to run all unit tests at once.&lt;/li&gt;
&lt;li&gt;Work on the tasks in the &lt;strong&gt;Tasks.qs&lt;/strong&gt; file.&lt;/li&gt;
&lt;li&gt;To test your code changes for a task, rebuild the solution and re-run all unit tests using &lt;strong&gt;Run All&lt;/strong&gt;, or run just the test for that task by right-clicking the test and selecting &lt;strong&gt;Run Selected Tests&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-visual-studio-code" class="anchor" aria-hidden="true" href="#visual-studio-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Studio Code&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Press &lt;strong&gt;Ctrl + `&lt;/strong&gt; (or &lt;strong&gt;⌘ + `&lt;/strong&gt; on macOS) to open the integrated terminal.
The terminal should open to the kata directory. If it doesn't, navigate to the folder containing the *.csproj file for the kata using &lt;code&gt;cd&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;dotnet test&lt;/code&gt; in the integrated terminal.
This should build the kata project and run all of the unit tests. All of the unit tests should fail.&lt;/li&gt;
&lt;li&gt;Work on the tasks in the &lt;strong&gt;Tasks.qs&lt;/strong&gt; file.&lt;/li&gt;
&lt;li&gt;To test your code changes for a task, from the integrated terminal run &lt;code&gt;dotnet test&lt;/code&gt; again.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For convenience, a tasks.json configuration file exists for each kata. It allows Visual Studio Code to run the build and test steps from the Command Palette.
Press &lt;strong&gt;Ctrl + Shift + P&lt;/strong&gt; (or &lt;strong&gt;⌘ + Shift + P&lt;/strong&gt; on macOS) to open the Palette and type &lt;strong&gt;Run Build Task&lt;/strong&gt; or &lt;strong&gt;Run Test Task&lt;/strong&gt; and press &lt;strong&gt;Enter&lt;/strong&gt;.&lt;/p&gt;
&lt;/a&gt;&lt;h2&gt;&lt;a id="user-content-run-katas-locally-with-docker-" class="anchor" aria-hidden="true" href="#run-katas-locally-with-docker-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-tests"&gt;Run katas locally with Docker &lt;/a&gt;&lt;a name="user-content-docker"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-docker"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-docker"&gt;You can use the included &lt;/a&gt;&lt;a href="./Dockerfile"&gt;Dockerfile&lt;/a&gt; to create a docker image with all the necessary tools to run the katas from the command line or Jupyter.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href="https://docs.docker.com/install/" rel="nofollow"&gt;Docker&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Build the docker image and tag it &lt;code&gt;katas&lt;/code&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker build -t katas &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Run the image in the container named &lt;code&gt;katas-container&lt;/code&gt; with interactive command-line and redirect container port &lt;code&gt;8888&lt;/code&gt; to local port &lt;code&gt;8888&lt;/code&gt; (needed to run Jupyter):&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker run -it --name katas-container -p 8888:8888 katas /bin/bash&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="4"&gt;
&lt;li&gt;From the same command line that you used to run the container, run the C# version of the &lt;strong&gt;BasicGates&lt;/strong&gt; kata:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; &lt;span class="pl-k"&gt;~&lt;/span&gt;/BasicGates/
dotnet &lt;span class="pl-c1"&gt;test&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Start a Jupyter Notebook within the image for the &lt;strong&gt;BasicGates&lt;/strong&gt; kata:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; &lt;span class="pl-k"&gt;~&lt;/span&gt;/BasicGates/ &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; jupyter notebook --ip=0.0.0.0 --no-browser&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="6"&gt;
&lt;li&gt;Once Jupyter has started, use your browser to open the kata in notebook format. You
will need a token generated by Jupyter when it started on the previous step:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;http://localhost:8888/notebooks/BasicGates.ipynb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To exit a docker container without killing it (daemon mode), press &lt;strong&gt;Ctrl+P, Ctrl+Q&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To re-enter the existing &lt;code&gt;katas-container&lt;/code&gt; (in daemon mode):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker attach katas-container&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once you're done, remove the &lt;code&gt;katas-container&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker rm --force katas-container&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-contributing-" class="anchor" aria-hidden="true" href="#contributing-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing &lt;a name="user-content-contributing"&gt;&lt;/a&gt;&lt;/h1&gt;&lt;a name="user-content-contributing"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-contributing"&gt;This project welcomes contributions and suggestions.  See &lt;/a&gt;&lt;a href=".github/CONTRIBUTING.md"&gt;How Can I Contribute?&lt;/a&gt; for details.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-code-of-conduct-" class="anchor" aria-hidden="true" href="#code-of-conduct-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code of Conduct &lt;a name="user-content-code-of-conduct"&gt;&lt;/a&gt;&lt;/h1&gt;&lt;a name="user-content-code-of-conduct"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-code-of-conduct"&gt;This project has adopted the &lt;/a&gt;&lt;a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;.
For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow"&gt;Code of Conduct FAQ&lt;/a&gt; or
contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>microsoft</author><guid isPermaLink="false">https://github.com/microsoft/QuantumKatas</guid><pubDate>Thu, 28 Nov 2019 00:14:00 GMT</pubDate></item><item><title>tensorflow/tpu #15 in Jupyter Notebook, This month</title><link>https://github.com/tensorflow/tpu</link><description>&lt;p&gt;&lt;i&gt;Reference models and tools for Cloud TPUs.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cloud-tpus" class="anchor" aria-hidden="true" href="#cloud-tpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cloud TPUs&lt;/h1&gt;
&lt;p&gt;This repository is a collection of reference models and tools used with
&lt;a href="https://cloud.google.com/tpu/" rel="nofollow"&gt;Cloud TPUs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The fastest way to get started training a model on a Cloud TPU is by following
the tutorial. Click the button below to launch the tutorial using Google Cloud
Shell.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://console.cloud.google.com/cloudshell/open?git_repo=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftpu&amp;amp;page=shell&amp;amp;tutorial=tools%2Fctpu%2Ftutorial.md" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/acf32864fdae185325f992b48fb2132badef1e9a/687474703a2f2f677374617469632e636f6d2f636c6f75647373682f696d616765732f6f70656e2d62746e2e737667" alt="Open in Cloud Shell" data-canonical-src="http://gstatic.com/cloudssh/images/open-btn.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; This repository is a public mirror, pull requests will not be accepted.
Please file an issue if you have a feature or bug request.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-models" class="anchor" aria-hidden="true" href="#running-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Models&lt;/h2&gt;
&lt;p&gt;To run models in the &lt;code&gt;models&lt;/code&gt; subdirectory, you may need to add the top-level
&lt;code&gt;/models&lt;/code&gt; folder to the Python path with the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export PYTHONPATH="$PYTHONPATH:/path/to/models"
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensorflow</author><guid isPermaLink="false">https://github.com/tensorflow/tpu</guid><pubDate>Thu, 28 Nov 2019 00:15:00 GMT</pubDate></item><item><title>MorvanZhou/PyTorch-Tutorial #16 in Jupyter Notebook, This month</title><link>https://github.com/MorvanZhou/PyTorch-Tutorial</link><description>&lt;p&gt;&lt;i&gt;Build your neural network easy and fast&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;a href="http://pytorch.org/" rel="nofollow"&gt;
    &lt;img width="40%" src="https://github.com/MorvanZhou/PyTorch-Tutorial/raw/master/logo.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;&lt;a id="user-content-if-youd-like-to-use-tensorflow-no-worries-i-made-a-new-tensorflow-tutorial-just-like-pytorch-here-is-the-link-httpsgithubcommorvanzhoutensorflow-tutorial" class="anchor" aria-hidden="true" href="#if-youd-like-to-use-tensorflow-no-worries-i-made-a-new-tensorflow-tutorial-just-like-pytorch-here-is-the-link-httpsgithubcommorvanzhoutensorflow-tutorial"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;If you'd like to use &lt;strong&gt;Tensorflow&lt;/strong&gt;, no worries, I made a new &lt;strong&gt;Tensorflow Tutorial&lt;/strong&gt; just like PyTorch. Here is the link: &lt;a href="https://github.com/MorvanZhou/Tensorflow-Tutorial"&gt;https://github.com/MorvanZhou/Tensorflow-Tutorial&lt;/a&gt;&lt;/h3&gt;
&lt;h1&gt;&lt;a id="user-content-pytorch-tutorials" class="anchor" aria-hidden="true" href="#pytorch-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pyTorch Tutorials&lt;/h1&gt;
&lt;p&gt;In these tutorials for pyTorch, we will build our first Neural Network and try to build some advanced Neural Network architectures developed recent years.&lt;/p&gt;
&lt;p&gt;Thanks for &lt;a href="https://github.com/liufuyang"&gt;liufuyang's&lt;/a&gt; &lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents-notebooks"&gt;&lt;strong&gt;notebook files&lt;/strong&gt;&lt;/a&gt;
which is a great contribution to this tutorial.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pyTorch basic
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/201_torch_numpy.py"&gt;torch and numpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/202_variable.py"&gt;Variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/203_activation.py"&gt;Activation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Build your first network
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py"&gt;Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/302_classification.py"&gt;Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/303_build_nn_quickly.py"&gt;An easy way&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/304_save_reload.py"&gt;Save and reload&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/305_batch_train.py"&gt;Train on batch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/306_optimizer.py"&gt;Optimizers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Advanced neural network
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/401_CNN.py"&gt;CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/402_RNN_classifier.py"&gt;RNN-Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py"&gt;RNN-Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/404_autoencoder.py"&gt;AutoEncoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/405_DQN_Reinforcement_learning.py"&gt;DQN Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/pytorch-A3C"&gt;A3C Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py"&gt;GAN (Generative Adversarial Nets)&lt;/a&gt; / &lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_conditional_GAN.py"&gt;Conditional GAN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Others (WIP)
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/501_why_torch_dynamic_graph.py"&gt;Why torch dynamic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/502_GPU.py"&gt;Train on GPU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/503_dropout.py"&gt;Dropout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py"&gt;Batch Normalization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;For Chinese speakers: All methods mentioned below have their video and text tutorial in Chinese.
Visit &lt;a href="https://morvanzhou.github.io/tutorials/" rel="nofollow"&gt;莫烦 Python&lt;/a&gt; for more.
You can watch my &lt;a href="https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg" rel="nofollow"&gt;Youtube channel&lt;/a&gt; as well.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-regression" class="anchor" aria-hidden="true" href="#regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py"&gt;Regression&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/7491264fba17ff7eb3ec5cce2e0f8db3e58e1c7b/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f312d312d322e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/1-1-2.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-classification" class="anchor" aria-hidden="true" href="#classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/302_classification.py"&gt;Classification&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/302_classification.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/03cf1c4921b6e80da7710c117164b27675351118/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f312d312d332e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/1-1-3.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-cnn" class="anchor" aria-hidden="true" href="#cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/401_CNN.py"&gt;CNN&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/401_CNN.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/338bdc4ea3e2ce897291d8fd5257546395f55a34/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f342d312d322e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/4-1-2.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-rnn" class="anchor" aria-hidden="true" href="#rnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py"&gt;RNN&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/cfb5a384896108d534d3c6c6fadf31f581715737/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f342d332d312e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/4-3-1.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-autoencoder" class="anchor" aria-hidden="true" href="#autoencoder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/404_autoencoder.py"&gt;Autoencoder&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/4c4f5a700b817b646bbed49dffdaa4a0925df8aa/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f342d342d312e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/4-4-1.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/f8f129f7808c6942b067e77b205a2fde2c604845/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f342d342d322e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/4-4-2.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-gan-generative-adversarial-nets" class="anchor" aria-hidden="true" href="#gan-generative-adversarial-nets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py"&gt;GAN (Generative Adversarial Nets)&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/34534d5eb5e962d2761224d6637471b74c06c972/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f342d362d312e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/4-6-1.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-dropout" class="anchor" aria-hidden="true" href="#dropout"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/503_dropout.py"&gt;Dropout&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/503_dropout.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/304540032f3f2ed20f127f22e90ad23005088a37/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f352d332d312e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/5-3-1.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-batch-normalization" class="anchor" aria-hidden="true" href="#batch-normalization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py"&gt;Batch Normalization&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/944f05a2565de729ae03d76bb2cb6f9c48e2e552/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f352d342d322e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/5-4-2.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h1&gt;&lt;a id="user-content-donation" class="anchor" aria-hidden="true" href="#donation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Donation&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;If this does help you, please consider donating to support me for better tutorials. Any contribution is greatly appreciated!&lt;/em&gt;&lt;/p&gt;
&lt;div&gt;
  &lt;a href="https://www.paypal.com/cgi-bin/webscr?cmd=_donations&amp;amp;business=morvanzhou%40gmail%2ecom&amp;amp;lc=C2&amp;amp;item_name=MorvanPython&amp;amp;currency_code=AUD&amp;amp;bn=PP%2dDonationsBF%3abtn_donateCC_LG%2egif%3aNonHosted" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/f75f395714327046e04cbadbee71103f87fd0aa1/68747470733a2f2f7777772e70617970616c6f626a656374732e636f6d2f7765627374617469632f656e5f55532f692f62746e2f706e672f73696c7665722d70696c6c2d70617970616c2d343470782e706e67" alt="Paypal" height="auto" data-canonical-src="https://www.paypalobjects.com/webstatic/en_US/i/btn/png/silver-pill-paypal-44px.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div&gt;
  &lt;a href="https://www.patreon.com/morvan" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/af8133ce68a27bb72e44120b5ec14062fc42dc75/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f696d672f737570706f72742f70617472656f6e2e6a7067" alt="Patreon" height="120" data-canonical-src="https://morvanzhou.github.io/static/img/support/patreon.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>MorvanZhou</author><guid isPermaLink="false">https://github.com/MorvanZhou/PyTorch-Tutorial</guid><pubDate>Thu, 28 Nov 2019 00:16:00 GMT</pubDate></item><item><title>udacity/deep-learning-v2-pytorch #17 in Jupyter Notebook, This month</title><link>https://github.com/udacity/deep-learning-v2-pytorch</link><description>&lt;p&gt;&lt;i&gt;Projects and exercises for the latest Deep Learning ND program https://www.udacity.com/course/deep-learning-nanodegree--nd101&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-pytorch" class="anchor" aria-hidden="true" href="#deep-learning-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning (PyTorch)&lt;/h1&gt;
&lt;p&gt;This repository contains material related to Udacity's &lt;a href="https://www.udacity.com/course/deep-learning-nanodegree--nd101" rel="nofollow"&gt;Deep Learning Nanodegree program&lt;/a&gt;. It consists of a bunch of tutorial notebooks for various deep learning topics. In most cases, the notebooks lead you through implementing models such as convolutional networks, recurrent networks, and GANs. There are other topics covered such as weight initialization and batch normalization.&lt;/p&gt;
&lt;p&gt;There are also notebooks used as projects for the Nanodegree program. In the program itself, the projects are reviewed by real people (Udacity reviewers), but the starting code is available here, as well.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table Of Contents&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-neural-networks" class="anchor" aria-hidden="true" href="#introduction-to-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-neural-networks"&gt;Introduction to Neural Networks&lt;/a&gt;: Learn how to implement gradient descent and apply it to predicting patterns in student admissions data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-analysis-network"&gt;Sentiment Analysis with NumPy&lt;/a&gt;: &lt;a href="http://iamtrask.github.io/" rel="nofollow"&gt;Andrew Trask&lt;/a&gt; leads you through building a sentiment analysis model, predicting if some text is positive or negative.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch"&gt;Introduction to PyTorch&lt;/a&gt;: Learn how to build neural networks in PyTorch and use pre-trained networks for state-of-the-art image classifiers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/convolutional-neural-networks"&gt;Convolutional Neural Networks&lt;/a&gt;: Visualize the output of layers that make up a CNN. Learn how to define and train a CNN for classifying &lt;a href="https://en.wikipedia.org/wiki/MNIST_database" rel="nofollow"&gt;MNIST data&lt;/a&gt;, a handwritten digit database that is notorious in the fields of machine and deep learning. Also, define and train a CNN for classifying images in the &lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow"&gt;CIFAR10 dataset&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/transfer-learning"&gt;Transfer Learning&lt;/a&gt;. In practice, most people don't train their own networks on huge datasets; they use &lt;strong&gt;pre-trained&lt;/strong&gt; networks such as VGGnet. Here you'll use VGGnet to help classify images of flowers without training an end-to-end network from scratch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/weight-initialization"&gt;Weight Initialization&lt;/a&gt;: Explore how initializing network weights affects performance.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/autoencoder"&gt;Autoencoders&lt;/a&gt;: Build models for image compression and de-noising, using feedforward and convolutional networks in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/style-transfer"&gt;Style Transfer&lt;/a&gt;: Extract style and content features from images, using a pre-trained network. Implement style transfer according to the paper, &lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="nofollow"&gt;Image Style Transfer Using Convolutional Neural Networks&lt;/a&gt; by Gatys et. al. Define appropriate losses for iteratively creating a target, style-transferred image of your own design!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-recurrent-neural-networks" class="anchor" aria-hidden="true" href="#recurrent-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/recurrent-neural-networks"&gt;Intro to Recurrent Networks (Time series &amp;amp; Character-level RNN)&lt;/a&gt;: Recurrent neural networks are able to use information about the sequence of data, such as the sequence of characters in text; learn how to implement these in PyTorch for a variety of tasks.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/word2vec-embeddings"&gt;Embeddings (Word2Vec)&lt;/a&gt;: Implement the Word2Vec model to find semantic representations of words for use in natural language processing.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-rnn"&gt;Sentiment Analysis RNN&lt;/a&gt;: Implement a recurrent neural network that can predict if the text of a moview review is positive or negative.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/attention"&gt;Attention&lt;/a&gt;: Implement attention and apply it to annotation vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-generative-adversarial-networks" class="anchor" aria-hidden="true" href="#generative-adversarial-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/gan-mnist"&gt;Generative Adversarial Network on MNIST&lt;/a&gt;: Train a simple generative adversarial network on the MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/batch-norm"&gt;Batch Normalization&lt;/a&gt;: Learn how to improve training rates and network stability with batch normalizations.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/dcgan-svhn"&gt;Deep Convolutional GAN (DCGAN)&lt;/a&gt;: Implement a DCGAN to generate new images based on the Street View House Numbers (SVHN) dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/cycle-gan"&gt;CycleGAN&lt;/a&gt;: Implement a CycleGAN that is designed to learn from unpaired and unlabeled data; use trained generators to transform images from summer to winter and vice versa.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deploying-a-model-with-aws-sagemaker" class="anchor" aria-hidden="true" href="#deploying-a-model-with-aws-sagemaker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deploying a Model (with AWS SageMaker)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/sagemaker-deployment"&gt;All exercise and project notebooks&lt;/a&gt; for the lessons on model deployment can be found in the linked, Github repo. Learn to deploy pre-trained models using AWS SageMaker.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-projects" class="anchor" aria-hidden="true" href="#projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-bikesharing"&gt;Predicting Bike-Sharing Patterns&lt;/a&gt;: Implement a neural network in NumPy to predict bike rentals.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-dog-classification"&gt;Dog Breed Classifier&lt;/a&gt;: Build a convolutional neural network with PyTorch to classify any image (even an image of a face) as a specific dog breed.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-tv-script-generation"&gt;TV Script Generation&lt;/a&gt;: Train a recurrent neural network to generate scripts in the style of dialogue from Seinfeld.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-face-generation"&gt;Face Generation&lt;/a&gt;: Use a DCGAN on the CelebA dataset to generate images of new and realistic human faces.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-elective-material" class="anchor" aria-hidden="true" href="#elective-material"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Elective Material&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/tensorflow/intro-to-tensorflow"&gt;Intro to TensorFlow&lt;/a&gt;: Starting building neural networks with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/keras"&gt;Keras&lt;/a&gt;: Learn to build neural networks and convolutional neural networks with Keras.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-configure-and-manage-your-environment-with-anaconda" class="anchor" aria-hidden="true" href="#configure-and-manage-your-environment-with-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configure and Manage Your Environment with Anaconda&lt;/h2&gt;
&lt;p&gt;Per the Anaconda &lt;a href="http://conda.pydata.org/docs" rel="nofollow"&gt;docs&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Conda is an open source package management system and environment management system
for installing multiple versions of software packages and their dependencies and
switching easily between them. It works on Linux, OS X and Windows, and was created
for Python programs but can package and distribute any software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h2&gt;
&lt;p&gt;Using Anaconda consists of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt; on your computer, by selecting the latest Python version for your operating system. If you already have &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;miniconda&lt;/code&gt; installed, you should be able to skip this step and move on to step 2.&lt;/li&gt;
&lt;li&gt;Create and activate * a new &lt;code&gt;conda&lt;/code&gt; &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;environment&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;* Each time you wish to work on any exercises, activate your &lt;code&gt;conda&lt;/code&gt; environment!&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-1-installation" class="anchor" aria-hidden="true" href="#1-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; the latest version of &lt;code&gt;miniconda&lt;/code&gt; that matches your system.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Linux&lt;/th&gt;
&lt;th&gt;Mac&lt;/th&gt;
&lt;th&gt;Windows&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;64-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86_64.exe" rel="nofollow"&gt;64-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;32-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86.sh" rel="nofollow"&gt;32-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86.exe" rel="nofollow"&gt;32-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt; &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;miniconda&lt;/a&gt; on your machine. Detailed instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-2-create-and-activate-the-environment" class="anchor" aria-hidden="true" href="#2-create-and-activate-the-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Create and Activate the Environment&lt;/h2&gt;
&lt;p&gt;For Windows users, these following commands need to be executed from the &lt;strong&gt;Anaconda prompt&lt;/strong&gt; as opposed to a Windows terminal window. For Mac, a normal terminal window will work.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-git-and-version-control" class="anchor" aria-hidden="true" href="#git-and-version-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Git and version control&lt;/h4&gt;
&lt;p&gt;These instructions also assume you have &lt;code&gt;git&lt;/code&gt; installed for working with Github from a terminal window, but if you do not, you can download that first with the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you'd like to learn more about version control and using &lt;code&gt;git&lt;/code&gt; from the command line, take a look at our &lt;a href="https://www.udacity.com/course/version-control-with-git--ud123" rel="nofollow"&gt;free course: Version Control with Git&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now, we're ready to create our local environment!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clone the repository, and navigate to the downloaded folder. This may take a minute or two to clone due to the included image data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/udacity/deep-learning-v2-pytorch.git
cd deep-learning-v2-pytorch
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;
&lt;p&gt;Create (and activate) a new environment, named &lt;code&gt;deep-learning&lt;/code&gt; with Python 3.6. If prompted to proceed with the install &lt;code&gt;(Proceed [y]/n)&lt;/code&gt; type y.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create -n deep-learning python=3.6
source activate deep-learning
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create --name deep-learning python=3.6
activate deep-learning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point your command line should look something like: &lt;code&gt;(deep-learning) &amp;lt;User&amp;gt;:deep-learning-v2-pytorch &amp;lt;user&amp;gt;$&lt;/code&gt;. The &lt;code&gt;(deep-learning)&lt;/code&gt; indicates that your environment has been activated, and you can proceed with further package installations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install PyTorch and torchvision; this should install the latest version of PyTorch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision -c pytorch 
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch -c pytorch
pip install torchvision
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install a few required pip packages, which are specified in the requirements text file (including OpenCV).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="7"&gt;
&lt;li&gt;That's it!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now most of the &lt;code&gt;deep-learning&lt;/code&gt; libraries are available to you. Very occasionally, you will see a repository with an addition requirements file, which exists should you want to use TensorFlow and Keras, for example. In this case, you're encouraged to install another library to your existing environment, or create a new environment for a specific project.&lt;/p&gt;
&lt;p&gt;Now, assuming your &lt;code&gt;deep-learning&lt;/code&gt; environment is still activated, you can navigate to the main repo and start looking at the notebooks:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd
cd deep-learning-v2-pytorch
jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To exit the environment when you have completed your work session, simply close the terminal window.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>udacity</author><guid isPermaLink="false">https://github.com/udacity/deep-learning-v2-pytorch</guid><pubDate>Thu, 28 Nov 2019 00:17:00 GMT</pubDate></item><item><title>ultralytics/yolov3 #18 in Jupyter Notebook, This month</title><link>https://github.com/ultralytics/yolov3</link><description>&lt;p&gt;&lt;i&gt;YOLOv3 in PyTorch &gt; ONNX &gt; CoreML &gt; iOS&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591130-f7beea00-abc2-11e9-9dc0-d6abcf41d713.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591130-f7beea00-abc2-11e9-9dc0-d6abcf41d713.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
    &lt;td align="center"&gt;
    &lt;a href="https://www.ultralytics.com" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/c7f01c9051691f7f4c6239349b6b55cb5a0871c9/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f756c7472616c79746963732f6c6f676f2f6c6f676f6e616d65313030302e706e67" width="160" data-canonical-src="https://storage.googleapis.com/ultralytics/logo/logoname1000.png" style="max-width:100%;"&gt;&lt;/a&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591093-2b4d4480-abc2-11e9-8b46-d88eb1dabba1.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591093-2b4d4480-abc2-11e9-8b46-d88eb1dabba1.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
          &lt;a href="https://itunes.apple.com/app/id1452689527" rel="nofollow"&gt;
    &lt;img src="https://user-images.githubusercontent.com/26833433/50044365-9b22ac00-0082-11e9-862f-e77aee7aa7b0.png" width="180" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591100-55066b80-abc2-11e9-9647-52c0e045b288.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591100-55066b80-abc2-11e9-9647-52c0e045b288.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC, and &lt;strong&gt;is freely available for redistribution under the GPL-3.0 license&lt;/strong&gt;. For more information please visit &lt;a href="https://www.ultralytics.com" rel="nofollow"&gt;https://www.ultralytics.com&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description&lt;/h1&gt;
&lt;p&gt;The &lt;a href="https://github.com/ultralytics/yolov3"&gt;https://github.com/ultralytics/yolov3&lt;/a&gt; repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux, MacOS and Windows. Training is done on the COCO dataset by default: &lt;a href="https://cocodataset.org/#home" rel="nofollow"&gt;https://cocodataset.org/#home&lt;/a&gt;. &lt;strong&gt;Credit to Joseph Redmon for YOLO:&lt;/strong&gt; &lt;a href="https://pjreddie.com/darknet/yolo/" rel="nofollow"&gt;https://pjreddie.com/darknet/yolo/&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h1&gt;
&lt;p&gt;Python 3.7 or later with the following &lt;code&gt;pip3 install -U -r requirements.txt&lt;/code&gt; packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;numpy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;torch &amp;gt;= 1.1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;opencv-python&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tqdm&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart"&gt;GCP Quickstart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning"&gt;Transfer Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image"&gt;Train Single Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class"&gt;Train Single Class&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data"&gt;Train Custom Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-jupyter-notebook" class="anchor" aria-hidden="true" href="#jupyter-notebook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebook&lt;/h1&gt;
&lt;p&gt;Our Jupyter &lt;a href="https://colab.research.google.com/github/ultralytics/yolov3/blob/master/examples.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt; provides quick training, inference and testing examples.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Start Training:&lt;/strong&gt; &lt;code&gt;python3 train.py&lt;/code&gt; to begin training after downloading COCO data with &lt;code&gt;data/get_coco_dataset.sh&lt;/code&gt;. Each epoch trains on 117,263 images from the train and validate COCO sets, and tests on 5000 images from the COCO validate set.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resume Training:&lt;/strong&gt; &lt;code&gt;python3 train.py --resume&lt;/code&gt; to resume training from &lt;code&gt;weights/last.pt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Plot Training:&lt;/strong&gt; &lt;code&gt;from utils import utils; utils.plot_results()&lt;/code&gt; plots training results from &lt;code&gt;coco_16img.data&lt;/code&gt;, &lt;code&gt;coco_64img.data&lt;/code&gt;, 2 example datasets available in the &lt;code&gt;data/&lt;/code&gt; folder, which train and test on the first 16 and 64 images of the COCO2014-trainval dataset.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/63258271-fe9d5300-c27b-11e9-9a15-95038daf4438.png"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/63258271-fe9d5300-c27b-11e9-9a15-95038daf4438.png" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-image-augmentation" class="anchor" aria-hidden="true" href="#image-augmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Augmentation&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;datasets.py&lt;/code&gt; applies random OpenCV-powered (&lt;a href="https://opencv.org/" rel="nofollow"&gt;https://opencv.org/&lt;/a&gt;) augmentation to the input images in accordance with the following specifications. Augmentation is applied &lt;strong&gt;only&lt;/strong&gt; during training, not during inference. Bounding boxes are automatically tracked and updated with the images. 416 x 416 examples pictured below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Augmentation&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Translation&lt;/td&gt;
&lt;td&gt;+/- 10% (vertical and horizontal)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Rotation&lt;/td&gt;
&lt;td&gt;+/- 5 degrees&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Shear&lt;/td&gt;
&lt;td&gt;+/- 2 degrees (vertical and horizontal)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scale&lt;/td&gt;
&lt;td&gt;+/- 10%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reflection&lt;/td&gt;
&lt;td&gt;50% probability (horizontal-only)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;H&lt;strong&gt;S&lt;/strong&gt;V Saturation&lt;/td&gt;
&lt;td&gt;+/- 50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HS&lt;strong&gt;V&lt;/strong&gt; Intensity&lt;/td&gt;
&lt;td&gt;+/- 50%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/66699231-27beea80-ece5-11e9-9cad-bdf9d82c500a.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/66699231-27beea80-ece5-11e9-9cad-bdf9d82c500a.jpg" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-speed" class="anchor" aria-hidden="true" href="#speed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/deep-learning-vm/" rel="nofollow"&gt;https://cloud.google.com/deep-learning-vm/&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Machine type:&lt;/strong&gt; n1-standard-8 (8 vCPUs, 30 GB memory)&lt;br&gt;
&lt;strong&gt;CPU platform:&lt;/strong&gt; Intel Skylake&lt;br&gt;
&lt;strong&gt;GPUs:&lt;/strong&gt; K80 ($0.20/hr), T4 ($0.35/hr), V100 ($0.83/hr) CUDA with &lt;a href="https://github.com/NVIDIA/apex"&gt;Nvidia Apex&lt;/a&gt; FP16/32&lt;br&gt;
&lt;strong&gt;HDD:&lt;/strong&gt; 100 GB SSD&lt;br&gt;
&lt;strong&gt;Dataset:&lt;/strong&gt; COCO train 2014 (117,263 images)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;GPUs&lt;/th&gt;
&lt;th&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;images/sec&lt;/th&gt;
&lt;th&gt;epoch time&lt;/th&gt;
&lt;th&gt;epoch cost&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;K80&lt;/td&gt;
&lt;td&gt;64 (32x2)&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;175 min&lt;/td&gt;
&lt;td&gt;$0.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T4&lt;/td&gt;
&lt;td&gt;64 (32x2)&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;49 min&lt;/td&gt;
&lt;td&gt;$0.29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T4 x2&lt;/td&gt;
&lt;td&gt;64 (64x1)&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;td&gt;32 min&lt;/td&gt;
&lt;td&gt;$0.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;V100&lt;/td&gt;
&lt;td&gt;64 (32x2)&lt;/td&gt;
&lt;td&gt;115&lt;/td&gt;
&lt;td&gt;17 min&lt;/td&gt;
&lt;td&gt;$0.24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;V100 x2&lt;/td&gt;
&lt;td&gt;64 (64x1)&lt;/td&gt;
&lt;td&gt;150&lt;/td&gt;
&lt;td&gt;13 min&lt;/td&gt;
&lt;td&gt;$0.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2080Ti&lt;/td&gt;
&lt;td&gt;64 (32x2)&lt;/td&gt;
&lt;td&gt;81&lt;/td&gt;
&lt;td&gt;24 min&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2080Ti x2&lt;/td&gt;
&lt;td&gt;64 (64x1)&lt;/td&gt;
&lt;td&gt;140&lt;/td&gt;
&lt;td&gt;14 min&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-inference" class="anchor" aria-hidden="true" href="#inference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inference&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;detect.py&lt;/code&gt; runs inference on any sources:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 detect.py --source ...&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Image:  &lt;code&gt;--source file.jpg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Video:  &lt;code&gt;--source file.mp4&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Directory:  &lt;code&gt;--source dir/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Webcam:  &lt;code&gt;--source 0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;RTSP stream:  &lt;code&gt;--source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;HTTP stream:  &lt;code&gt;--source http://wmccpinetop.axiscam.net/mjpg/video.mjpg&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To run a specific models:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3.cfg --weights weights/yolov3.weights&lt;/code&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067835-51d5b500-cc2f-11e9-982e-843f7f9a6ea2.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067835-51d5b500-cc2f-11e9-982e-843f7f9a6ea2.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3-tiny:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3-tiny.cfg --weights weights/yolov3-tiny.weights&lt;/code&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067834-51d5b500-cc2f-11e9-9357-c485b159a20b.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067834-51d5b500-cc2f-11e9-9357-c485b159a20b.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3-SPP:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3-spp.cfg --weights weights/yolov3-spp.weights&lt;/code&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067833-51d5b500-cc2f-11e9-8208-6fe197809131.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067833-51d5b500-cc2f-11e9-8208-6fe197809131.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pretrained-weights" class="anchor" aria-hidden="true" href="#pretrained-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained Weights&lt;/h1&gt;
&lt;p&gt;Download from: &lt;a href="https://drive.google.com/open?id=1LezFG5g3BCW6iYaV89B2i64cqEUZD7e0" rel="nofollow"&gt;https://drive.google.com/open?id=1LezFG5g3BCW6iYaV89B2i64cqEUZD7e0&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-darknet-conversion" class="anchor" aria-hidden="true" href="#darknet-conversion"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Darknet Conversion&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/ultralytics/yolov3 &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="pl-c1"&gt;cd&lt;/span&gt; yolov3

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; convert darknet cfg/weights to pytorch model&lt;/span&gt;
$ python3  -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;from models import *; convert('cfg/yolov3-spp.cfg', 'weights/yolov3-spp.weights')&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
Success: converted &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3-spp.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; to &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;converted.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; convert cfg/pytorch model to darknet weights&lt;/span&gt;
$ python3  -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;from models import *; convert('cfg/yolov3-spp.cfg', 'weights/yolov3-spp.pt')&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
Success: converted &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3-spp.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; to &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;converted.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-map" class="anchor" aria-hidden="true" href="#map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;mAP&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;test.py --weights weights/yolov3.weights&lt;/code&gt; tests official YOLOv3 weights.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;test.py --weights weights/last.pt&lt;/code&gt; tests latest checkpoint.&lt;/li&gt;
&lt;li&gt;mAPs on COCO2014 using pycocotools.&lt;/li&gt;
&lt;li&gt;mAP@0.5 run at --iou-thres 0.5, mAP@0.5 run at --iou-thres 0.65&lt;/li&gt;
&lt;li&gt;YOLOv3-SPP ultralytics is &lt;code&gt;ultralytics68.pt&lt;/code&gt; with &lt;code&gt;yolov3-spp.cfg&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Darknet results published in &lt;a href="https://arxiv.org/abs/1804.02767" rel="nofollow"&gt;https://arxiv.org/abs/1804.02767&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;i&gt;&lt;/i&gt;&lt;/th&gt;
&lt;th&gt;img-size&lt;/th&gt;
&lt;th&gt;COCO mAP&lt;br&gt;@0.5...0.95&lt;/th&gt;
&lt;th&gt;COCO mAP&lt;br&gt;@0.5&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv3-tiny&lt;br&gt;YOLOv3&lt;br&gt;YOLOv3-SPP&lt;br&gt;&lt;strong&gt;YOLOv3-SPP ultralytics&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;14.0&lt;br&gt;28.7&lt;br&gt;30.5&lt;br&gt;&lt;strong&gt;35.2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;29.0&lt;br&gt;51.5&lt;br&gt;52.3&lt;br&gt;&lt;strong&gt;53.9&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv3-tiny&lt;br&gt;YOLOv3&lt;br&gt;YOLOv3-SPP&lt;br&gt;&lt;strong&gt;YOLOv3-SPP ultralytics&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;416&lt;/td&gt;
&lt;td&gt;16.0&lt;br&gt;31.1&lt;br&gt;33.9&lt;br&gt;&lt;strong&gt;38.8&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;32.9&lt;br&gt;55.3&lt;br&gt;56.8&lt;br&gt;&lt;strong&gt;58.7&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv3-tiny&lt;br&gt;YOLOv3&lt;br&gt;YOLOv3-SPP&lt;br&gt;&lt;strong&gt;YOLOv3-SPP ultralytics&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;608&lt;/td&gt;
&lt;td&gt;16.6&lt;br&gt;33.0&lt;br&gt;37.0&lt;br&gt;&lt;strong&gt;40.4&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;35.5&lt;br&gt;57.9&lt;br&gt;&lt;strong&gt;60.6&lt;/strong&gt;&lt;br&gt;60.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python3 test.py --save-json --img-size 608 --iou-thres 0.65 --weights ultralytics68.pt
Namespace(batch_size=16, cfg=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;cfg/yolov3-spp.cfg&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, conf_thres=0.001, data=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;data/coco.data&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, device=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, img_size=608, iou_thres=0.5, nms_thres=0.5, save_json=True, weights=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ultralytics68.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
Using CUDA device0 _CudaDeviceProperties(name=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Tesla T4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, total_memory=15079MB)

               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 313/313 [06:&lt;span class="pl-k"&gt;52&amp;lt;&lt;/span&gt;00:00,  1.24it/s]
                 all     5e+03  3.58e+04     0.107     0.779      0.59     0.182
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.404 &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;---
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.597 &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;---
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.75      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.438
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.241
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.444
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.511
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;  1 ] = 0.326
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt; 10 ] = 0.533
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.570
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.393
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.614
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.691&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://zenodo.org/badge/latestdoi/146165888" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cd760a8900fd4be0105229509d566b7c9499ef8d/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3134363136353838382e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/146165888.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h1&gt;
&lt;p&gt;Issues should be raised directly in the repository. For additional questions or comments please email Glenn Jocher at &lt;a href="mailto:glenn.jocher@ultralytics.com"&gt;glenn.jocher@ultralytics.com&lt;/a&gt; or visit us at &lt;a href="https://contact.ultralytics.com" rel="nofollow"&gt;https://contact.ultralytics.com&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ultralytics</author><guid isPermaLink="false">https://github.com/ultralytics/yolov3</guid><pubDate>Thu, 28 Nov 2019 00:18:00 GMT</pubDate></item><item><title>susanli2016/NLP-with-Python #19 in Jupyter Notebook, This month</title><link>https://github.com/susanli2016/NLP-with-Python</link><description>&lt;p&gt;&lt;i&gt;Scikit-Learn, NLTK, Spacy, Gensim, Textblob and more&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-nlp-with-python" class="anchor" aria-hidden="true" href="#nlp-with-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP with Python&lt;/h1&gt;
&lt;p&gt;Scikit-Learn, NLTK, Spacy, Gensim, Textblob and more&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>susanli2016</author><guid isPermaLink="false">https://github.com/susanli2016/NLP-with-Python</guid><pubDate>Thu, 28 Nov 2019 00:19:00 GMT</pubDate></item><item><title>rlabbe/Kalman-and-Bayesian-Filters-in-Python #20 in Jupyter Notebook, This month</title><link>https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</link><description>&lt;p&gt;&lt;i&gt;Kalman Filter book using Jupyter Notebook. Focuses on building intuition and experience, not formal proofs.  Includes Kalman filters,extended Kalman filters, unscented Kalman filters, particle filters, and more. All exercises include solutions.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-kalman-and-bayesian-filters-in-python" class="anchor" aria-hidden="true" href="#kalman-and-bayesian-filters-in-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python"&gt;Kalman and Bayesian Filters in Python&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Introductory text for Kalman and Bayesian filters. All code is written in Python, and the book itself is written using Juptyer Notebook so that you can run and modify the code in your browser. What better way to learn?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;"Kalman and Bayesian Filters in Python" looks amazing! ... your book is just what I needed&lt;/strong&gt; - Allen Downey, Professor and O'Reilly author.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thanks for all your work on publishing your introductory text on Kalman Filtering, as well as the Python Kalman Filtering libraries. We’ve been using it internally to teach some key state estimation concepts to folks and it’s been a huge help.&lt;/strong&gt; - Sam Rodkey, SpaceX&lt;/p&gt;
&lt;p&gt;Start reading online now by clicking the binder or Azure badge below:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/70c5b4d050d4019f4f20b170d75679a9316ac5e5/687474703a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="http://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://notebooks.azure.com/import/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c33d8af3d101ffcd6ea73a8d02290b8d829ac52/68747470733a2f2f6e6f7465626f6f6b732e617a7572652e636f6d2f6c61756e63682e706e67" data-canonical-src="https://notebooks.azure.com/launch.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master/animations/05_dog_track.gif"&gt;&lt;img src="https://raw.githubusercontent.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master/animations/05_dog_track.gif" alt="alt tag" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-are-kalman-and-bayesian-filters" class="anchor" aria-hidden="true" href="#what-are-kalman-and-bayesian-filters"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What are Kalman and Bayesian Filters?&lt;/h2&gt;
&lt;p&gt;Sensors are noisy. The world is full of data and events that we want to measure and track, but we cannot rely on sensors to give us perfect information. The GPS in my car reports altitude. Each time I pass the same point in the road it reports a slightly different altitude. My kitchen scale gives me different readings if I weigh the same object twice.&lt;/p&gt;
&lt;p&gt;In simple cases the solution is obvious. If my scale gives slightly different readings I can just take a few readings and average them. Or I can replace it with a more accurate scale. But what do we do when the sensor is very noisy, or the environment makes data collection difficult? We may be trying to track the movement of a low flying aircraft. We may want to create an autopilot for a drone, or ensure that our farm tractor seeded the entire field. I work on computer vision, and I need to track moving objects in images, and the computer vision algorithms create very noisy and unreliable results.&lt;/p&gt;
&lt;p&gt;This book teaches you how to solve these sorts of filtering problems. I use many different algorithms, but they are all based on Bayesian probability. In simple terms Bayesian probability determines what is likely to be true based on past information.&lt;/p&gt;
&lt;p&gt;If I asked you the heading of my car at this moment you would have no idea. You'd proffer a number between 1∘∘ and 360∘∘ degrees, and have a 1 in 360 chance of being right. Now suppose I told you that 2 seconds ago its heading was 243∘∘. In 2 seconds my car could not turn very far so you could make a far more accurate prediction. You are using past information to more accurately infer information about the present or future.&lt;/p&gt;
&lt;p&gt;The world is also noisy. That prediction helps you make a better estimate, but it also subject to noise. I may have just braked for a dog or swerved around a pothole. Strong winds and ice on the road are external influences on the path of my car. In control literature we call this noise though you may not think of it that way.&lt;/p&gt;
&lt;p&gt;There is more to Bayesian probability, but you have the main idea. Knowledge is uncertain, and we alter our beliefs based on the strength of the evidence. Kalman and Bayesian filters blend our noisy and limited knowledge of how a system behaves with the noisy and limited sensor readings to produce the best possible estimate of the state of the system. Our principle is to never discard information.&lt;/p&gt;
&lt;p&gt;Say we are tracking an object and a sensor reports that it suddenly changed direction. Did it really turn, or is the data noisy? It depends. If this is a jet fighter we'd be very inclined to believe the report of a sudden maneuver. If it is a freight train on a straight track we would discount it. We'd further modify our belief depending on how accurate the sensor is. Our beliefs depend on the past and on our knowledge of the system we are tracking and on the characteristics of the sensors.&lt;/p&gt;
&lt;p&gt;The Kalman filter was invented by Rudolf Emil Kálmán to solve this sort of problem in a mathematically optimal way. Its first use was on the Apollo missions to the moon, and since then it has been used in an enormous variety of domains. There are Kalman filters in aircraft, on submarines, and on cruise missiles. Wall street uses them to track the market. They are used in robots, in IoT (Internet of Things) sensors, and in laboratory instruments. Chemical plants use them to control and monitor reactions. They are used to perform medical imaging and to remove noise from cardiac signals. If it involves a sensor and/or time-series data, a Kalman filter or a close relative to the Kalman filter is usually involved.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-motivation" class="anchor" aria-hidden="true" href="#motivation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Motivation&lt;/h2&gt;
&lt;p&gt;The motivation for this book came out of my desire for a gentle introduction to Kalman filtering. I'm a software engineer that spent almost two decades in the avionics field, and so I have always been 'bumping elbows' with the Kalman filter, but never implemented one myself. As I moved into solving tracking problems with computer vision the need became urgent. There are classic textbooks in the field, such as Grewal and Andrew's excellent &lt;em&gt;Kalman Filtering&lt;/em&gt;. But sitting down and trying to read many of these books is a dismal experience if you do not have the required background. Typically the first few chapters fly through several years of undergraduate math, blithely referring you to textbooks on topics such as Itō calculus, and present an entire semester's worth of statistics in a few brief paragraphs. They are good texts for an upper undergraduate course, and an invaluable reference to researchers and professionals, but the going is truly difficult for the more casual reader. Symbology is introduced without explanation, different texts use different terms and variables for the same concept, and the books are almost devoid of examples or worked problems. I often found myself able to parse the words and comprehend the mathematics of a definition, but had no idea as to what real world phenomena they describe. "But what does that &lt;em&gt;mean?&lt;/em&gt;" was my repeated thought.&lt;/p&gt;
&lt;p&gt;However, as I began to finally understand the Kalman filter I realized the underlying concepts are quite straightforward. A few simple probability rules, some intuition about how we integrate disparate knowledge to explain events in our everyday life and the core concepts of the Kalman filter are accessible. Kalman filters have a reputation for difficulty, but shorn of much of the formal terminology the beauty of the subject and of their math became clear to me, and I fell in love with the topic.&lt;/p&gt;
&lt;p&gt;As I began to understand the math and theory more difficulties present themselves. A book or paper's author makes some statement of fact and presents a graph as proof.  Unfortunately, why the statement is true is not clear to me, nor is the method for making that plot obvious. Or maybe I wonder "is this true if R=0?"  Or the author provides pseudocode at such a high level that the implementation is not obvious. Some books offer Matlab code, but I do not have a license to that expensive package. Finally, many books end each chapter with many useful exercises. Exercises which you need to understand if you want to implement Kalman filters for yourself, but exercises with no answers. If you are using the book in a classroom, perhaps this is okay, but it is terrible for the independent reader. I loathe that an author withholds information from me, presumably to avoid 'cheating' by the student in the classroom.&lt;/p&gt;
&lt;p&gt;From my point of view none of this necessary. Certainly if you are designing a Kalman filter for a aircraft or missile you must thoroughly master of all of the mathematics and topics in a typical Kalman filter textbook. I just want to track an image on a screen, or write some code for an Arduino project. I want to know how the plots in the book are made, and chose different parameters than the author chose. I want to run simulations. I want to inject more noise in the signal and see how a filter performs. There are thousands of opportunities for using Kalman filters in everyday code, and yet this fairly straightforward topic is the provenance of rocket scientists and academics.&lt;/p&gt;
&lt;p&gt;I wrote this book to address all of those needs. This is not the book for you if you program navigation computers for Boeing or design radars for Raytheon. Go get an advanced degree at Georgia Tech, UW, or the like, because you'll need it. This book is for the hobbiest, the curious, and the working engineer that needs to filter or smooth data.&lt;/p&gt;
&lt;p&gt;This book is interactive. While you can read it online as static content, I urge you to use it as intended. It is written using Jupyter Notebook, which allows me to combine text, math, Python, and Python output in one place. Every plot, every piece of data in this book is generated from Python that is available to you right inside the notebook. Want to double the value of a parameter? Click on the Python cell, change the parameter's value, and click 'Run'. A new plot or printed output will appear in the book.&lt;/p&gt;
&lt;p&gt;This book has exercises, but it also has the answers. I trust you. If you just need an answer, go ahead and read the answer. If you want to internalize this knowledge, try to implement the exercise before you read the answer.&lt;/p&gt;
&lt;p&gt;This book has supporting libraries for computing statistics, plotting various things related to filters, and for the various filters that we cover. This does require a strong caveat; most of the code is written for didactic purposes. It is rare that I chose the most efficient solution (which often obscures the intent of the code), and in the first parts of the book I did not concern myself with numerical stability. This is important to understand - Kalman filters in aircraft are carefully designed and implemented to be numerically stable; the naive implementation is not stable in many cases. If you are serious about Kalman filters this book will not be the last book you need. My intention is to introduce you to the concepts and mathematics, and to get you to the point where the textbooks are approachable.&lt;/p&gt;
&lt;p&gt;Finally, this book is free. The cost for the books required to learn Kalman filtering is somewhat prohibitive even for a Silicon Valley engineer like myself; I cannot believe they are within the reach of someone in a depressed economy, or a financially struggling student. I have gained so much from free software like Python, and free books like those from Allen B. Downey &lt;a href="http://www.greenteapress.com/" rel="nofollow"&gt;here&lt;/a&gt;. It's time to repay that. So, the book is free, it is hosted on free servers, and it uses only free and open software such as IPython and mathjax to create the book.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-reading-online" class="anchor" aria-hidden="true" href="#reading-online"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reading Online&lt;/h2&gt;
&lt;p&gt;The book is written as a collection of Jupyter Notebooks, an interactive, browser based system that allows you to combine text, Python, and math into your browser. There are multiple ways to read these online, listed below.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-binder" class="anchor" aria-hidden="true" href="#binder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;binder&lt;/h3&gt;
&lt;p&gt;binder serves interactive notebooks online, so you can run the code and change the code within your browser without downloading the book or installing Jupyter.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/70c5b4d050d4019f4f20b170d75679a9316ac5e5/687474703a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="http://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-nbviewer" class="anchor" aria-hidden="true" href="#nbviewer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;nbviewer&lt;/h3&gt;
&lt;p&gt;The website &lt;a href="http://nbviewer.org" rel="nofollow"&gt;http://nbviewer.org&lt;/a&gt; provides an Jupyter Notebook server that renders notebooks stored at github (or elsewhere). The rendering is done in real time when you load the book. You may use &lt;a href="http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb" rel="nofollow"&gt;&lt;em&gt;this nbviewer link&lt;/em&gt;&lt;/a&gt; to access my book via nbviewer. If you read my book today, and then I make a change tomorrow, when you go back tomorrow you will see that change. Notebooks are rendered statically - you can read them, but not modify or run the code.&lt;/p&gt;
&lt;p&gt;nbviewer seems to lag the checked in version by a few days, so you might not be reading the most recent content.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-github" class="anchor" aria-hidden="true" href="#github"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GitHub&lt;/h3&gt;
&lt;p&gt;GitHub is able to render the notebooks directly. The quickest way to view a notebook is to just click on them above. However, it renders the math incorrectly, and I cannot recommend using it if you are doing more than just dipping into the book.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pdf-version" class="anchor" aria-hidden="true" href="#pdf-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PDF Version&lt;/h2&gt;
&lt;p&gt;A PDF version of the book is available &lt;a href="https://drive.google.com/open?id=0By_SW19c1BfhSVFzNHc0SjduNzg" rel="nofollow"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The PDF will usually lag behind what is in github as I don't update it for every minor check in.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-downloading-and-running-the-book" class="anchor" aria-hidden="true" href="#downloading-and-running-the-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading and Running the Book&lt;/h2&gt;
&lt;p&gt;However, this book is intended to be interactive and I recommend using it in that form. It's a little more effort to set up, but worth it. If you install IPython and some supporting libraries on your computer and then clone this book you will be able to run all of the code in the book yourself. You can perform experiments, see how filters react to different data, see how different filters react to the same data, and so on. I find this sort of immediate feedback both vital and invigorating. You do not have to wonder "what happens if". Try it and see!&lt;/p&gt;
&lt;p&gt;The book and supporting software can be downloaded from GitHub by running this command on  the command line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone --depth=1 https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python.git
pip install filterpy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instructions for installation of the IPython ecosystem can be found in the Installation appendix, found &lt;a href="http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/Appendix-A-Installation.ipynb" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once the software is installed you can navigate to the installation directory and run Juptyer notebook with the command line instruction&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open a browser window showing the contents of the base directory. The book is organized into chapters. To read Chapter 2, click on the link for chapter 2. This will cause the browser to open that subdirectory. In each subdirectory there will be one or more IPython Notebooks (all notebooks have a .ipynb file extension). The chapter contents are in the notebook with the same name as the chapter name. There are sometimes supporting notebooks for doing things like generating animations that are displayed in the chapter. These are not intended to be read by the end user, but of course if you are curious as to how an animation is made go ahead and take a look.&lt;/p&gt;
&lt;p&gt;This is admittedly a somewhat cumbersome interface to a book; I am following in the footsteps of several other projects that are somewhat repurposing Jupyter Notebook to generate entire books. I feel the slight annoyances have a huge payoff - instead of having to download a separate code base and run it in an IDE while you try to read a book, all of the code and text is in one place. If you want to alter the code, you may do so and immediately see the effects of your change. If you find a bug, you can make a fix, and push it back to my repository so that everyone in the world benefits. And, of course, you will never encounter a problem I face all the time with traditional books - the book and the code are out of sync with each other, and you are left scratching your head as to which source to trust.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-companion-software" class="anchor" aria-hidden="true" href="#companion-software"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Companion Software&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://pypi.python.org/pypi/filterpy" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2514efa585c6a295744d54c87effb293c6ffee51/687474703a2f2f696d672e736869656c64732e696f2f707970692f762f66696c74657270792e737667" alt="Latest Version" data-canonical-src="http://img.shields.io/pypi/v/filterpy.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I wrote an open source Bayesian filtering Python library called &lt;strong&gt;FilterPy&lt;/strong&gt;. I have made the project available on PyPi, the Python Package Index.  To install from PyPi, at the command line issue the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install filterpy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do not have pip, you may follow the instructions here: &lt;a href="https://pip.pypa.io/en/latest/installing.html" rel="nofollow"&gt;https://pip.pypa.io/en/latest/installing.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All of the filters used in this book as well as others not in this book are implemented in my Python library FilterPy, available &lt;a href="https://github.com/rlabbe/filterpy"&gt;here&lt;/a&gt;. You do not need to download or install this to read the book, but you will likely want to use this library to write your own filters. It includes Kalman filters, Fading Memory filters, H infinity filters, Extended and Unscented filters, least square filters, and many more.  It also includes helper routines that simplify the designing the matrices used by some of the filters, and other code such as Kalman based smoothers.&lt;/p&gt;
&lt;p&gt;FilterPy is hosted github at (&lt;a href="https://github.com/rlabbe/filterpy"&gt;https://github.com/rlabbe/filterpy&lt;/a&gt;).  If you want the bleading edge release you will want to grab a copy from github, and follow your Python installation's instructions for adding it to the Python search path. This might expose you to some instability since you might not get a tested release, but as a benefit you will also get all of the test scripts used to test the library. You can examine these scripts to see many examples of writing and running filters while not in the Jupyter Notebook environment.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-alternative-way-of-running-the-book-in-conda-environment" class="anchor" aria-hidden="true" href="#alternative-way-of-running-the-book-in-conda-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Alternative Way of Running the Book in Conda environment&lt;/h2&gt;
&lt;p&gt;If you have conda or miniconda installed, you can create environment by&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda env update -f environment.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and use&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source activate kf_bf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source deactivate kf_bf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to activate and deactivate the environment.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-issues-or-questions" class="anchor" aria-hidden="true" href="#issues-or-questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Issues or Questions&lt;/h2&gt;
&lt;p&gt;If you have comments, you can write an issue at GitHub so that everyone can read it along with my response. Please don't view it as a way to report bugs only. Alternatively I've created a gitter room for more informal discussion. &lt;a href="https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" alt="Join the chat at https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/005cfe27b7c4520ac0d6b607d6a7e33f5ad4eb6e/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;span&gt;Kalman and Bayesian Filters in Python&lt;/span&gt; by &lt;a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python"&gt;Roger R. Labbe&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by/4.0/" rel="nofollow"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All software in this book, software that supports this book (such as in the the code directory) or used in the generation of the book (in the pdf directory) that is contained in this repository is licensed under the following MIT license:&lt;/p&gt;
&lt;p&gt;The MIT License (MIT)&lt;/p&gt;
&lt;p&gt;Copyright (c) 2015 Roger R. Labbe Jr&lt;/p&gt;
&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt;
&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt;
&lt;p&gt;THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.TION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;rlabbejr at gmail.com&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rlabbe</author><guid isPermaLink="false">https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</guid><pubDate>Thu, 28 Nov 2019 00:20:00 GMT</pubDate></item><item><title>wesm/pydata-book #21 in Jupyter Notebook, This month</title><link>https://github.com/wesm/pydata-book</link><description>&lt;p&gt;&lt;i&gt;Materials and IPython notebooks for "Python for Data Analysis" by Wes McKinney, published by O'Reilly Media&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python-for-data-analysis-2nd-edition" class="anchor" aria-hidden="true" href="#python-for-data-analysis-2nd-edition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python for Data Analysis, 2nd Edition&lt;/h1&gt;
&lt;p&gt;Materials and IPython notebooks for "Python for Data Analysis" by Wes McKinney,
published by O'Reilly Media&lt;/p&gt;
&lt;p&gt;&lt;a href="http://amzn.to/2vvBijB" rel="nofollow"&gt;Buy the book on Amazon&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notebooks.azure.com/import/gh/wesm/pydata-book" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c33d8af3d101ffcd6ea73a8d02290b8d829ac52/68747470733a2f2f6e6f7465626f6f6b732e617a7572652e636f6d2f6c61756e63682e706e67" data-canonical-src="https://notebooks.azure.com/launch.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Follow Wes on Twitter: &lt;a href="https://twitter.com/wesmckinn" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e5ee948ef48fbfa31f868c9dbd301bbd5cf38097/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f7765736d636b696e6e2e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/wesmckinn.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-1st-edition-readers" class="anchor" aria-hidden="true" href="#1st-edition-readers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1st Edition Readers&lt;/h1&gt;
&lt;p&gt;If you are reading the &lt;a href="http://amzn.to/2vvBijB" rel="nofollow"&gt;1st Edition&lt;/a&gt; (published in 2012), please find the
reorganized book materials on the &lt;a href="https://github.com/wesm/pydata-book/tree/1st-edition"&gt;&lt;code&gt;1st-edition&lt;/code&gt; branch&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/BrambleXu/pydata-notebook"&gt;Chinese&lt;/a&gt; by Xu Liang&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ipython-notebooks" class="anchor" aria-hidden="true" href="#ipython-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;IPython Notebooks:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch02.ipynb" rel="nofollow"&gt;Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch03.ipynb" rel="nofollow"&gt;Chapter 3: Built-in Data Structures, Functions, and Files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch04.ipynb" rel="nofollow"&gt;Chapter 4: NumPy Basics: Arrays and Vectorized Computation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch05.ipynb" rel="nofollow"&gt;Chapter 5: Getting Started with pandas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch06.ipynb" rel="nofollow"&gt;Chapter 6: Data Loading, Storage, and File Formats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch07.ipynb" rel="nofollow"&gt;Chapter 7: Data Cleaning and Preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch08.ipynb" rel="nofollow"&gt;Chapter 8: Data Wrangling: Join, Combine, and Reshape&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch09.ipynb" rel="nofollow"&gt;Chapter 9: Plotting and Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch10.ipynb" rel="nofollow"&gt;Chapter 10: Data Aggregation and Group Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch11.ipynb" rel="nofollow"&gt;Chapter 11: Time Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch12.ipynb" rel="nofollow"&gt;Chapter 12: Advanced pandas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch13.ipynb" rel="nofollow"&gt;Chapter 13: Introduction to Modeling Libraries in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch14.ipynb" rel="nofollow"&gt;Chapter 14: Data Analysis Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/appa.ipynb" rel="nofollow"&gt;Appendix A: Advanced NumPy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-code" class="anchor" aria-hidden="true" href="#code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code&lt;/h3&gt;
&lt;p&gt;The code in this repository, including all code samples in the notebooks listed
above, is released under the &lt;a href="LICENSE-CODE"&gt;MIT license&lt;/a&gt;. Read more at the
&lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>wesm</author><guid isPermaLink="false">https://github.com/wesm/pydata-book</guid><pubDate>Thu, 28 Nov 2019 00:21:00 GMT</pubDate></item><item><title>selfteaching/the-craft-of-selfteaching #22 in Jupyter Notebook, This month</title><link>https://github.com/selfteaching/the-craft-of-selfteaching</link><description>&lt;p&gt;&lt;i&gt;One has no future if one couldn't teach themself.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-the-craft-of-selfteaching" class="anchor" aria-hidden="true" href="#the-craft-of-selfteaching"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;the-craft-of-selfteaching&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;One has no future if one couldn't teach themself&lt;a href="#fn1" name="user-content-fn1b"&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-自学是门手艺" class="anchor" aria-hidden="true" href="#自学是门手艺"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;自学是门手艺&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;没有自学能力的人没有未来&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;作者：李笑来&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;特别感谢&lt;strong&gt;霍炬&lt;/strong&gt;（&lt;a href="https://github.com/virushuo"&gt;@virushuo&lt;/a&gt;）、&lt;strong&gt;洪强宁&lt;/strong&gt;（&lt;a href="https://github.com/hongqn"&gt;@hongqn&lt;/a&gt;) 两位良师诤友在此书写作过程中给予我的巨大帮助！&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pseudo-code of selfteaching in Python&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;teach_yourself&lt;/span&gt;(&lt;span class="pl-smi"&gt;anything&lt;/span&gt;):
    &lt;span class="pl-k"&gt;while&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; create():
        learn()
        practice()
    &lt;span class="pl-k"&gt;return&lt;/span&gt; teach_yourself(another)

teach_yourself(coding)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;请先行阅读 &lt;a href="T-appendix.jupyter-installation-and-setup.ipynb"&gt;T-appendix.jupyter-installation-and-setup&lt;/a&gt; 以便在本地安装 &lt;a href="https://github.com/jupyterlab/jupyterlab"&gt;Jupyterlab&lt;/a&gt; 而后就能用更好的体验阅读本书。&lt;/p&gt;
&lt;p&gt;有兴趣帮忙的朋友，请先行阅读 &lt;a href="02.proof-of-work.ipynb"&gt;如何使用 Pull Request 为这本书校对&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;2019 年 3 月 23 日，新增 Markdown 版本：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/selfteaching/the-craft-of-selfteaching/tree/master/markdown"&gt;https://github.com/selfteaching/the-craft-of-selfteaching/tree/master/markdown&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h3&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="01.preface.ipynb"&gt;01.preface（&lt;strong&gt;前言&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="02.proof-of-work.ipynb"&gt;02.proof-of-work（&lt;strong&gt;如何证明你真的读过这本书？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.A.better.teachyourself.ipynb"&gt;Part.1.A.better.teachyourself（&lt;strong&gt;为什么一定要掌握自学能力？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.B.why.start.from.learning.coding.ipynb"&gt;Part.1.B.why.start.from.learning.coding（&lt;strong&gt;为什么把编程当作自学的入口？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.C.must.learn.sth.only.by.reading.ipynb"&gt;Part.1.C.must.learn.sth.only.by.reading（&lt;strong&gt;只靠阅读习得新技能&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.D.preparation.for.reading.ipynb"&gt;Part.1.D.preparation.for.reading（&lt;strong&gt;开始阅读前的一些准备&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.1.entrance.ipynb"&gt;Part.1.E.1.entrance（&lt;strong&gt;入口&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.2.values-and-their-operators.ipynb"&gt;Part.1.E.2.values-and-their-operators（&lt;strong&gt;值及其相应的运算&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.3.controlflow.ipynb"&gt;Part.1.E.3.controlflow（&lt;strong&gt;流程控制&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.4.functions.ipynb"&gt;Part.1.E.4.functions（&lt;strong&gt;函数&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.5.strings.ipynb"&gt;Part.1.E.5.strings（&lt;strong&gt;字符串&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.6.containers.ipynb"&gt;Part.1.E.6.containers（&lt;strong&gt;数据容器&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.7.files.ipynb"&gt;Part.1.E.7.files（&lt;strong&gt;文件&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.F.deal-with-forward-references.ipynb"&gt;Part.1.F.deal-with-forward-references（&lt;strong&gt;如何从容应对含有过多 “过早引用” 的知识？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.G.The-Python-Tutorial-local.ipynb"&gt;Part.1.G.The-Python-Tutorial-local（&lt;strong&gt;官方教程：The Python Tutorial&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.A.clumsy-and-patience.ipynb"&gt;Part.2.A.clumsy-and-patience（&lt;strong&gt;笨拙与耐心&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.B.deliberate-practicing.ipynb"&gt;Part.2.B.deliberate-practicing（&lt;strong&gt;刻意练习&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.C.why-start-from-writing-functions.ipynb"&gt;Part.2.C.why-start-from-writing-functions（&lt;strong&gt;为什么从函数开始？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.1-args.ipynb"&gt;Part.2.D.1-args（&lt;strong&gt;关于参数（上）&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.2-aargs.ipynb"&gt;Part.2.D.2-aargs（&lt;strong&gt;关于参数（下）&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.3-lambda.ipynb"&gt;Part.2.D.3-lambda（&lt;strong&gt;化名与匿名&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.4-recursion.ipynb"&gt;Part.2.D.4-recursion（&lt;strong&gt;递归函数&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.5-docstrings.ipynb"&gt;Part.2.D.5-docstrings（&lt;strong&gt;函数的文档&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.6-modules.ipynb"&gt;Part.2.D.6-modules（&lt;strong&gt;保存到文件的函数&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.7-tdd.ipynb"&gt;Part.2.D.7-tdd（&lt;strong&gt;测试驱动的开发&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.8-main.ipynb"&gt;Part.2.D.8-main（&lt;strong&gt;可执行的 Python 文件&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.E.deliberate-thinking.ipynb"&gt;Part.2.E.deliberate-thinking（&lt;strong&gt;刻意思考&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.A.conquering-difficulties.ipynb"&gt;Part.3.A.conquering-difficulties（&lt;strong&gt;战胜难点&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.B.1.classes-1.ipynb"&gt;Part.3.B.1.classes-1（&lt;strong&gt;类 —— 面向对象编程&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.B.2.classes-2.ipynb"&gt;Part.3.B.2.classes-2（&lt;strong&gt;类 —— Python 的实现&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.B.3.decorator-iterator-generator.ipynb"&gt;Part.3.B.3.decorator-iterator-generator（&lt;strong&gt;函数工具&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.B.4.regex.ipynb"&gt;Part.3.B.4.regex（&lt;strong&gt;正则表达式&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.B.5.bnf-ebnf-pebnf.ipynb"&gt;Part.3.B.5.bnf-ebnf-pebnf（&lt;strong&gt;BNF 以及 EBNF&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.C.breaking-good-and-bad.ipynb"&gt;Part.3.C.breaking-good-and-bad（&lt;strong&gt;拆解&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.D.indispensable-illusion.ipynb"&gt;Part.3.D.indispensable-illusion（&lt;strong&gt;刚需幻觉&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.E.to-be-thorough.ipynb"&gt;Part.3.E.to-be-thorough（&lt;strong&gt;全面 —— 自学的境界&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.F.social-selfteaching.ipynb"&gt;Part.3.F.social-selfteaching（&lt;strong&gt;自学者的社交&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.G.the-golden-age-and-google.ipynb"&gt;Part.3.G.the-golden-age-and-google（&lt;strong&gt;这是自学者的黄金时代&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.H.prevent-focus-drifting.ipynb"&gt;Part.3.H.prevent-focus-drifting（&lt;strong&gt;避免注意力漂移&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Q.good-communiation.ipynb"&gt;Q.good-communiation（&lt;strong&gt;如何成为优秀沟通者&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="R.finale.ipynb"&gt;R.finale（&lt;strong&gt;自学者的终点&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="S.whats-next.ipynb"&gt;S.whats-next（&lt;strong&gt;下一步干什么？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="T-appendix.editor.vscode.ipynb"&gt;T-appendix.editor.vscode（&lt;strong&gt;Visual Studio Code 的安装与配置&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="T-appendix.git-introduction.ipynb"&gt;T-appendix.git-introduction（&lt;strong&gt;Git 简介&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="T-appendix.jupyter-installation-and-setup.ipynb"&gt;T-appendix.jupyter-installation-and-setup（&lt;strong&gt;Jupyterlab 的安装与配置&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="T-appendix.symbols.ipynb"&gt;T-appendix.symbols（&lt;strong&gt;这些符号都代表什么？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;本书的版权协议为 &lt;a href="https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" rel="nofollow"&gt;CC-BY-NC-ND license&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/CC-BY-NC-ND.png?raw=true"&gt;&lt;img src="images/CC-BY-NC-ND.png?raw=true" alt="CC-BY-NC-ND" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;脚注&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name="user-content-fn1"&gt;[1]&lt;/a&gt;：&lt;a href="https://en.oxforddictionaries.com/usage/themselves-or-themself" rel="nofollow"&gt;'Themselves' or 'themself'? -- Oxford Dictionary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="#fn1b"&gt;↑Back to Content↑&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>selfteaching</author><guid isPermaLink="false">https://github.com/selfteaching/the-craft-of-selfteaching</guid><pubDate>Thu, 28 Nov 2019 00:22:00 GMT</pubDate></item><item><title>AtsushiSakai/PythonRobotics #23 in Jupyter Notebook, This month</title><link>https://github.com/AtsushiSakai/PythonRobotics</link><description>&lt;p&gt;&lt;i&gt;Python sample codes for robotics algorithms.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true" align="right" width="300" alt="header pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pythonrobotics" class="anchor" aria-hidden="true" href="#pythonrobotics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PythonRobotics&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/AtsushiSakai/PythonRobotics" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/58f87d5d3604646322c28abd8c5a9b2faa05fa51/68747470733a2f2f7472617669732d63692e6f72672f4174737573686953616b61692f507974686f6e526f626f746963732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/AtsushiSakai/PythonRobotics.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pythonrobotics.readthedocs.io/en/latest/?badge=latest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a60f894ef011c8a7e648348c16aabfdfb603613a/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f707974686f6e726f626f746963732f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/pythonrobotics/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2e66a00c9dcf7ecc1f24189c6055aa7e6da233dc/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f73623237396b787576316265333931673f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/AtsushiSakai/PythonRobotics?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c26144817eba34b4ee9f9a6aee913e6b466218b/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4174737573686953616b61692f507974686f6e526f626f746963732f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/AtsushiSakai/PythonRobotics/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://lgtm.com/projects/g/AtsushiSakai/PythonRobotics/context:python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4c3af4cd47bb2ea2c71cac274f1f7dd392eea893/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f672f4174737573686953616b61692f507974686f6e526f626f746963732e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Language grade: Python" data-canonical-src="https://img.shields.io/lgtm/grade/python/g/AtsushiSakai/PythonRobotics.svg?logo=lgtm&amp;amp;logoWidth=18" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/overview/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c3cd55e61ef2e22ff00427b50b9e7f1c3547de91/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6174737573686973616b61692f707974686f6e726f626f746963732f62616467652f6d6173746572" alt="CodeFactor" data-canonical-src="https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/badge/master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/AtsushiSakai/PythonRobotics"&gt;&lt;img src="https://camo.githubusercontent.com/230f0a1eaa529fa727cad2c9d3c1ace4738bd25d/68747470733a2f2f746f6b65692e72732f62312f6769746875622f4174737573686953616b61692f507974686f6e526f626f74696373" alt="tokei" data-canonical-src="https://tokei.rs/b1/github/AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Python codes for robotics algorithm.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-is-this"&gt;What is this?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#requirements"&gt;Requirements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-use"&gt;How to use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#localization"&gt;Localization&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#extended-kalman-filter-localization"&gt;Extended Kalman Filter localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#particle-filter-localization"&gt;Particle filter localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#histogram-filter-localization"&gt;Histogram filter localization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#mapping"&gt;Mapping&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#gaussian-grid-map"&gt;Gaussian grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ray-casting-grid-map"&gt;Ray casting grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lidar-to-grid-map"&gt;Lidar to grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#k-means-object-clustering"&gt;k-means object clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rectangle-fitting"&gt;Rectangle fitting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#slam"&gt;SLAM&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#iterative-closest-point-icp-matching"&gt;Iterative Closest Point (ICP) Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#fastslam-10"&gt;FastSLAM 1.0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#path-planning"&gt;Path Planning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#dynamic-window-approach"&gt;Dynamic Window Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grid-based-search"&gt;Grid based search&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#dijkstra-algorithm"&gt;Dijkstra algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#a-algorithm"&gt;A* algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#potential-field-algorithm"&gt;Potential Field algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grid-based-coverage-path-planning"&gt;Grid based coverage path planning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#state-lattice-planning"&gt;State Lattice Planning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#biased-polar-sampling"&gt;Biased polar sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lane-sampling"&gt;Lane sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#probabilistic-road-map-prm-planning"&gt;Probabilistic Road-Map (PRM) planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rapidly-exploring-random-trees-rrt"&gt;Rapidly-Exploring Random Trees (RRT)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#rrt"&gt;RRT*&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rrt-with-reeds-shepp-path"&gt;RRT* with reeds-shepp path&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lqr-rrt"&gt;LQR-RRT*&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#quintic-polynomials-planning"&gt;Quintic polynomials planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reeds-shepp-planning"&gt;Reeds Shepp planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lqr-based-path-planning"&gt;LQR based path planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimal-trajectory-in-a-frenet-frame"&gt;Optimal Trajectory in a Frenet Frame&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#path-tracking"&gt;Path Tracking&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#move-to-a-pose-control"&gt;move to a pose control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stanley-control"&gt;Stanley control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rear-wheel-feedback-control"&gt;Rear wheel feedback control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#linearquadratic-regulator-lqr-speed-and-steering-control"&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#model-predictive-speed-and-steering-control"&gt;Model predictive speed and steering control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#nonlinear-model-predictive-control-with-c-gmres"&gt;Nonlinear Model predictive control with C-GMRES&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#arm-navigation"&gt;Arm Navigation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#n-joint-arm-to-point-control"&gt;N joint arm to point control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#arm-navigation-with-obstacle-avoidance"&gt;Arm navigation with obstacle avoidance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#aerial-navigation"&gt;Aerial Navigation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#drone-3d-trajectory-following"&gt;drone 3d trajectory following&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rocket-powered-landing"&gt;rocket powered landing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#bipedal"&gt;Bipedal&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#bipedal-planner-with-inverted-pendulum"&gt;bipedal planner with inverted pendulum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#use-case"&gt;Use-case&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citing"&gt;Citing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#support"&gt;Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#authors"&gt;Authors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-what-is-this" class="anchor" aria-hidden="true" href="#what-is-this"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is this?&lt;/h1&gt;
&lt;p&gt;This is a Python code collection of robotics algorithms, especially for autonomous navigation.&lt;/p&gt;
&lt;p&gt;Features:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Easy to read for understanding each algorithm's basic idea.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Widely used and practical algorithms are selected.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Minimum dependency.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;See this paper for more details:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1808.10703" rel="nofollow"&gt;[1808.10703] PythonRobotics: a Python code collection of robotics algorithms&lt;/a&gt; (&lt;a href="https://github.com/AtsushiSakai/PythonRoboticsPaper/blob/master/python_robotics.bib"&gt;BibTeX&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Python 3.7.x (2.7 is not supported)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;numpy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;scipy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;matplotlib&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;pandas&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.cvxpy.org/index.html" rel="nofollow"&gt;cvxpy&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h1&gt;
&lt;p&gt;This README only shows some examples of this project.&lt;/p&gt;
&lt;p&gt;If you are interested in other examples or mathematical backgrounds of each algorithm,&lt;/p&gt;
&lt;p&gt;You can check the full documentation online: &lt;a href="https://pythonrobotics.readthedocs.io/" rel="nofollow"&gt;https://pythonrobotics.readthedocs.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All animation gifs are stored here: &lt;a href="https://github.com/AtsushiSakai/PythonRoboticsGifs"&gt;AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Clone this repo.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;git clone &lt;a href="https://github.com/AtsushiSakai/PythonRobotics.git"&gt;https://github.com/AtsushiSakai/PythonRobotics.git&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;cd PythonRobotics/&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Install the required libraries. You can use environment.yml with conda command.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;conda env create -f environment.yml&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start="3"&gt;
&lt;li&gt;
&lt;p&gt;Execute python script in each directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add star to this repo if you like it &lt;g-emoji class="g-emoji" alias="smiley" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f603.png"&gt;😃&lt;/g-emoji&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-localization" class="anchor" aria-hidden="true" href="#localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Localization&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-extended-kalman-filter-localization" class="anchor" aria-hidden="true" href="#extended-kalman-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Extended Kalman Filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif" width="640" alt="EKF pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Documentation: &lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/Localization/extended_kalman_filter/extended_kalman_filter_localization.ipynb"&gt;Notebook&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-particle-filter-localization" class="anchor" aria-hidden="true" href="#particle-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Particle filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a sensor fusion localization with Particle Filter(PF).&lt;/p&gt;
&lt;p&gt;The blue line is true trajectory, the black line is dead reckoning trajectory,&lt;/p&gt;
&lt;p&gt;and the red line is estimated trajectory with PF.&lt;/p&gt;
&lt;p&gt;It is assumed that the robot can measure a distance from landmarks (RFID).&lt;/p&gt;
&lt;p&gt;This measurements are used for PF localization.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-histogram-filter-localization" class="anchor" aria-hidden="true" href="#histogram-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Histogram filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a 2D localization example with Histogram filter.&lt;/p&gt;
&lt;p&gt;The red cross is true position, black points are RFID positions.&lt;/p&gt;
&lt;p&gt;The blue grid shows a position probability of histogram filter.&lt;/p&gt;
&lt;p&gt;In this simulation, x,y are unknown, yaw is known.&lt;/p&gt;
&lt;p&gt;The filter integrates speed input and range observations from RFID for localization.&lt;/p&gt;
&lt;p&gt;Initial position is not needed.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-mapping" class="anchor" aria-hidden="true" href="#mapping"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mapping&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-gaussian-grid-map" class="anchor" aria-hidden="true" href="#gaussian-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gaussian grid map&lt;/h2&gt;
&lt;p&gt;This is a 2D Gaussian grid mapping example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ray-casting-grid-map" class="anchor" aria-hidden="true" href="#ray-casting-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ray casting grid map&lt;/h2&gt;
&lt;p&gt;This is a 2D ray casting grid mapping example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-lidar-to-grid-map" class="anchor" aria-hidden="true" href="#lidar-to-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lidar to grid map&lt;/h2&gt;
&lt;p&gt;This example shows how to convert a 2D range measurement to a grid map.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="Mapping/lidar_to_grid_map/animation.gif"&gt;&lt;img src="Mapping/lidar_to_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-k-means-object-clustering" class="anchor" aria-hidden="true" href="#k-means-object-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;k-means object clustering&lt;/h2&gt;
&lt;p&gt;This is a 2D object clustering with k-means algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rectangle-fitting" class="anchor" aria-hidden="true" href="#rectangle-fitting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rectangle fitting&lt;/h2&gt;
&lt;p&gt;This is a 2D rectangle fitting for vehicle detection.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-slam" class="anchor" aria-hidden="true" href="#slam"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SLAM&lt;/h1&gt;
&lt;p&gt;Simultaneous Localization and Mapping(SLAM) examples&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-iterative-closest-point-icp-matching" class="anchor" aria-hidden="true" href="#iterative-closest-point-icp-matching"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Iterative Closest Point (ICP) Matching&lt;/h2&gt;
&lt;p&gt;This is a 2D ICP matching example with singular value decomposition.&lt;/p&gt;
&lt;p&gt;It can calculate a rotation matrix and a translation vector between points to points.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf" rel="nofollow"&gt;Introduction to Mobile Robotics: Iterative Closest Point Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-fastslam-10" class="anchor" aria-hidden="true" href="#fastslam-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FastSLAM 1.0&lt;/h2&gt;
&lt;p&gt;This is a feature based SLAM example using FastSLAM 1.0.&lt;/p&gt;
&lt;p&gt;The blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.&lt;/p&gt;
&lt;p&gt;The red points are particles of FastSLAM.&lt;/p&gt;
&lt;p&gt;Black points are landmarks, blue crosses are estimated landmark positions by FastSLAM.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm" rel="nofollow"&gt;SLAM simulations by Tim Bailey&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-path-planning" class="anchor" aria-hidden="true" href="#path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Path Planning&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-dynamic-window-approach" class="anchor" aria-hidden="true" href="#dynamic-window-approach"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dynamic Window Approach&lt;/h2&gt;
&lt;p&gt;This is a 2D navigation sample code with Dynamic Window Approach.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf" rel="nofollow"&gt;The Dynamic Window Approach to Collision Avoidance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-grid-based-search" class="anchor" aria-hidden="true" href="#grid-based-search"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grid based search&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-dijkstra-algorithm" class="anchor" aria-hidden="true" href="#dijkstra-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dijkstra algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based shortest path planning with Dijkstra's algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-a-algorithm" class="anchor" aria-hidden="true" href="#a-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A* algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based shortest path planning with A star algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt;
&lt;p&gt;Its heuristic is 2D Euclid distance.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-potential-field-algorithm" class="anchor" aria-hidden="true" href="#potential-field-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Potential Field algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based path planning with Potential Field algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif" alt="PotentialField" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, the blue heat map shows potential value on each grid.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf" rel="nofollow"&gt;Robotic Motion Planning:Potential Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-grid-based-coverage-path-planning" class="anchor" aria-hidden="true" href="#grid-based-coverage-path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grid based coverage path planning&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based coverage path planning simulation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif" alt="PotentialField" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-state-lattice-planning" class="anchor" aria-hidden="true" href="#state-lattice-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;State Lattice Planning&lt;/h2&gt;
&lt;p&gt;This script is a path planning code with state lattice planning.&lt;/p&gt;
&lt;p&gt;This code uses the model predictive trajectory generator to solve boundary problem.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://journals.sagepub.com/doi/pdf/10.1177/0278364906075328" rel="nofollow"&gt;Optimal rough terrain trajectory generation for wheeled mobile robots&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.frc.ri.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf" rel="nofollow"&gt;State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-biased-polar-sampling" class="anchor" aria-hidden="true" href="#biased-polar-sampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Biased polar sampling&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-lane-sampling" class="anchor" aria-hidden="true" href="#lane-sampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lane sampling&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-probabilistic-road-map-prm-planning" class="anchor" aria-hidden="true" href="#probabilistic-road-map-prm-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Probabilistic Road-Map (PRM) planning&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif" alt="PRM" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This PRM planner uses Dijkstra method for graph search.&lt;/p&gt;
&lt;p&gt;In the animation, blue points are sampled points,&lt;/p&gt;
&lt;p&gt;Cyan crosses means searched points with Dijkstra method,&lt;/p&gt;
&lt;p&gt;The red line is the final path of PRM.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Probabilistic_roadmap" rel="nofollow"&gt;Probabilistic roadmap - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;　　&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rapidly-exploring-random-trees-rrt" class="anchor" aria-hidden="true" href="#rapidly-exploring-random-trees-rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rapidly-Exploring Random Trees (RRT)&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-rrt" class="anchor" aria-hidden="true" href="#rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RRT*&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a path planning code with RRT*&lt;/p&gt;
&lt;p&gt;Black circles are obstacles, green line is a searched tree, red crosses are start and goal positions.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1005.0416" rel="nofollow"&gt;Incremental Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.419.5503&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-rrt-with-reeds-shepp-path" class="anchor" aria-hidden="true" href="#rrt-with-reeds-shepp-path"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RRT* with reeds-shepp path&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif" alt="Robotics/animation.gif at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Path planning for a car robot with RRT* and reeds shepp path planner.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-lqr-rrt" class="anchor" aria-hidden="true" href="#lqr-rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LQR-RRT*&lt;/h3&gt;
&lt;p&gt;This is a path planning simulation with LQR-RRT*.&lt;/p&gt;
&lt;p&gt;A double integrator motion model is used for LQR local planner.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif" alt="LQRRRT" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://lis.csail.mit.edu/pubs/perez-icra12.pdf" rel="nofollow"&gt;LQR-RRT*: Optimal Sampling-Based Motion Planning with Automatically Derived Extension Heuristics&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/MahanFathi/LQR-RRTstar"&gt;MahanFathi/LQR-RRTstar: LQR-RRT* method is used for random motion planning of a simple pendulum in its phase plot&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-quintic-polynomials-planning" class="anchor" aria-hidden="true" href="#quintic-polynomials-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quintic polynomials planning&lt;/h2&gt;
&lt;p&gt;Motion planning with quintic polynomials.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It can calculate 2D path, velocity, and acceleration profile based on quintic polynomials.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ieeexplore.ieee.org/document/637936/" rel="nofollow"&gt;Local Path Planning And Motion Control For Agv In Positioning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-reeds-shepp-planning" class="anchor" aria-hidden="true" href="#reeds-shepp-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reeds Shepp planning&lt;/h2&gt;
&lt;p&gt;A sample code with Reeds Shepp path planning.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true" alt="RSPlanning" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://planning.cs.uiuc.edu/node822.html" rel="nofollow"&gt;15.3.2 Reeds-Shepp Curves&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf" rel="nofollow"&gt;optimal paths for a car that goes both forwards and backwards&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/ghliu/pyReedsShepp"&gt;ghliu/pyReedsShepp: Implementation of Reeds Shepp curve.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-lqr-based-path-planning" class="anchor" aria-hidden="true" href="#lqr-based-path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LQR based path planning&lt;/h2&gt;
&lt;p&gt;A sample code using LQR based path planning for double integrator model.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true" alt="RSPlanning" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-optimal-trajectory-in-a-frenet-frame" class="anchor" aria-hidden="true" href="#optimal-trajectory-in-a-frenet-frame"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimal Trajectory in a Frenet Frame&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is optimal trajectory generation in a Frenet Frame.&lt;/p&gt;
&lt;p&gt;The cyan line is the target course and black crosses are obstacles.&lt;/p&gt;
&lt;p&gt;The red line is predicted path.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf" rel="nofollow"&gt;Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Cj6tAQe7UCY" rel="nofollow"&gt;Optimal trajectory generation for dynamic street scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-path-tracking" class="anchor" aria-hidden="true" href="#path-tracking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Path Tracking&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-move-to-a-pose-control" class="anchor" aria-hidden="true" href="#move-to-a-pose-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;move to a pose control&lt;/h2&gt;
&lt;p&gt;This is a simulation of moving to a pose control&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://link.springer.com/book/10.1007/978-3-642-20144-8" rel="nofollow"&gt;P. I. Corke, "Robotics, Vision and Control" | SpringerLink p102&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-stanley-control" class="anchor" aria-hidden="true" href="#stanley-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stanley control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with Stanley steering control and PID speed control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://robots.stanford.edu/papers/thrun.stanley05.pdf" rel="nofollow"&gt;Stanley: The robot that won the DARPA grand challenge&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf" rel="nofollow"&gt;Automatic Steering Methods for Autonomous Automobile Path Tracking&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-rear-wheel-feedback-control" class="anchor" aria-hidden="true" href="#rear-wheel-feedback-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rear wheel feedback control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with rear wheel feedback steering control and PID speed control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1604.07446" rel="nofollow"&gt;A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-linearquadratic-regulator-lqr-speed-and-steering-control" class="anchor" aria-hidden="true" href="#linearquadratic-regulator-lqr-speed-and-steering-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with LQR speed and steering control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ieeexplore.ieee.org/document/5940562/" rel="nofollow"&gt;Towards fully autonomous driving: Systems and algorithms - IEEE Conference Publication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-model-predictive-speed-and-steering-control" class="anchor" aria-hidden="true" href="#model-predictive-speed-and-steering-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model predictive speed and steering control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with iterative linear model predictive speed and steering control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif" width="640" alt="MPC pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/model_predictive_speed_and_steer_control/Model_predictive_speed_and_steering_control.ipynb"&gt;notebook&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://grauonline.de/wordpress/?page_id=3244" rel="nofollow"&gt;Real-time Model Predictive Control (MPC), ACADO, Python | Work-is-Playing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-nonlinear-model-predictive-control-with-c-gmres" class="anchor" aria-hidden="true" href="#nonlinear-model-predictive-control-with-c-gmres"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Nonlinear Model predictive control with C-GMRES&lt;/h2&gt;
&lt;p&gt;A motion planning and path tracking simulation with NMPC of C-GMRES&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/cgmres_nmpc/cgmres_nmpc.ipynb"&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-arm-navigation" class="anchor" aria-hidden="true" href="#arm-navigation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arm Navigation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-n-joint-arm-to-point-control" class="anchor" aria-hidden="true" href="#n-joint-arm-to-point-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;N joint arm to point control&lt;/h2&gt;
&lt;p&gt;N joint arm to a point control simulation.&lt;/p&gt;
&lt;p&gt;This is a interactive simulation.&lt;/p&gt;
&lt;p&gt;You can set the goal position of the end effector with left-click on the ploting area.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this simulation N = 10, however, you can change it.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-arm-navigation-with-obstacle-avoidance" class="anchor" aria-hidden="true" href="#arm-navigation-with-obstacle-avoidance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arm navigation with obstacle avoidance&lt;/h2&gt;
&lt;p&gt;Arm navigation with obstacle avoidance simulation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-aerial-navigation" class="anchor" aria-hidden="true" href="#aerial-navigation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Aerial Navigation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-drone-3d-trajectory-following" class="anchor" aria-hidden="true" href="#drone-3d-trajectory-following"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;drone 3d trajectory following&lt;/h2&gt;
&lt;p&gt;This is a 3d trajectory following simulation for a quadrotor.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rocket-powered-landing" class="anchor" aria-hidden="true" href="#rocket-powered-landing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;rocket powered landing&lt;/h2&gt;
&lt;p&gt;This is a 3d trajectory generation simulation for a rocket powered landing.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/AerialNavigation/rocket_powered_landing/rocket_powered_landing.ipynb"&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-bipedal" class="anchor" aria-hidden="true" href="#bipedal"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bipedal&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-bipedal-planner-with-inverted-pendulum" class="anchor" aria-hidden="true" href="#bipedal-planner-with-inverted-pendulum"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;bipedal planner with inverted pendulum&lt;/h2&gt;
&lt;p&gt;This is a bipedal planner for modifying footsteps with inverted pendulum.&lt;/p&gt;
&lt;p&gt;You can set the footsteps and the planner will modify those automatically.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-use-case" class="anchor" aria-hidden="true" href="#use-case"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use-case&lt;/h1&gt;
&lt;p&gt;If this project helps your robotics project, please let me know with &lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Your robot's video, which is using PythonRobotics, is very welcome!!&lt;/p&gt;
&lt;p&gt;This is a list of other user's comment and references:&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md"&gt;users_comments&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h1&gt;
&lt;p&gt;A small PR like bug fix is welcome.&lt;/p&gt;
&lt;p&gt;If your PR is merged multiple times, I will add your account to the author list.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h1&gt;
&lt;p&gt;If you use this project's code for your academic work, we encourage you to cite &lt;a href="https://arxiv.org/abs/1808.10703" rel="nofollow"&gt;our papers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you use this project's code in industry, we'd love to hear from you as well; feel free to reach out to the developers directly.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h1&gt;
&lt;p&gt;If you or your company would like to support this project, please consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.patreon.com/myenigma" rel="nofollow"&gt;Become a backer or sponsor on Patreon&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.paypal.me/myenigmapay/" rel="nofollow"&gt;One-time donation via PayPal&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can add your name or your company logo in README if you are a patron.&lt;/p&gt;
&lt;p&gt;E-mail consultant is also available.&lt;/p&gt;
&lt;p&gt;　&lt;/p&gt;
&lt;p&gt;Your comment using &lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt; is also welcome.&lt;/p&gt;
&lt;p&gt;This is a list: &lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md"&gt;Users comments&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AtsushiSakai/"&gt;Atsushi Sakai&lt;/a&gt; (&lt;a href="https://twitter.com/Atsushi_twi" rel="nofollow"&gt;@Atsushi_twi&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/daniel-s-ingram"&gt;Daniel Ingram&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/jwdinius"&gt;Joe Dinius&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/karanchawla"&gt;Karan Chawla&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/araffin"&gt;Antonin RAFFIN&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AlexisTM"&gt;Alexis Paques&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rsasaki0109"&gt;Ryohei Sasaki&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>AtsushiSakai</author><guid isPermaLink="false">https://github.com/AtsushiSakai/PythonRobotics</guid><pubDate>Thu, 28 Nov 2019 00:23:00 GMT</pubDate></item><item><title>realpython/materials #24 in Jupyter Notebook, This month</title><link>https://github.com/realpython/materials</link><description>&lt;p&gt;&lt;i&gt;Bonus materials, exercises, and example projects for our Python tutorials&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-real-python-materials" class="anchor" aria-hidden="true" href="#real-python-materials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Real Python Materials&lt;/h1&gt;
&lt;p&gt;Bonus materials, exercises, and example projects for our &lt;a href="https://realpython.com" rel="nofollow"&gt;Python tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Build Status: &lt;a href="https://circleci.com/gh/realpython/materials" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/991534e54579264db6c49f9225646cb77dd8fd6a/68747470733a2f2f636972636c6563692e636f6d2f67682f7265616c707974686f6e2f6d6174657269616c732e7376673f7374796c653d737667" alt="CircleCI" data-canonical-src="https://circleci.com/gh/realpython/materials.svg?style=svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-code-style-checks" class="anchor" aria-hidden="true" href="#running-code-style-checks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Code Style Checks&lt;/h2&gt;
&lt;p&gt;We use &lt;a href="http://flake8.pycqa.org/en/latest/" rel="nofollow"&gt;flake8&lt;/a&gt; and &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; to ensure a consistent code style for all of our sample code in this repository.&lt;/p&gt;
&lt;p&gt;Run the following commands to validate your code against the linters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ flake8
$ black --check &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-running-python-code-formatter" class="anchor" aria-hidden="true" href="#running-python-code-formatter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Python Code Formatter&lt;/h2&gt;
&lt;p&gt;We're using a tool called &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; on this repo to ensure consistent formatting. On CI it runs in "check" mode to ensure any new files added to the repo are following PEP 8. If you see linter warnings that say something like "would reformat some_file.py" it means black disagrees with your formatting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The easiest way to resolve these errors is to just run Black locally on the code and then committing those changes, as explained below.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To automatically re-format your code to be consistent with our code style guidelines, run &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; in the repository root folder:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ black &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>realpython</author><guid isPermaLink="false">https://github.com/realpython/materials</guid><pubDate>Thu, 28 Nov 2019 00:24:00 GMT</pubDate></item><item><title>slundberg/shap #25 in Jupyter Notebook, This month</title><link>https://github.com/slundberg/shap</link><description>&lt;p&gt;&lt;i&gt;A unified approach to explain the output of any machine learning model.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_diagram.png"&gt;&lt;img src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_diagram.png" width="400" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/slundberg/shap" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/19de279a6f67f8eea3f52ccecc779c3e1aff55e7/68747470733a2f2f7472617669732d63692e6f72672f736c756e64626572672f736861702e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.org/slundberg/shap.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://mybinder.org/v2/gh/slundberg/shap/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHAP (SHapley Additive exPlanations)&lt;/strong&gt; is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our &lt;a href="#citations"&gt;papers&lt;/a&gt; for details and citations).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-install" class="anchor" aria-hidden="true" href="#install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install&lt;/h2&gt;
&lt;p&gt;SHAP can be installed from either &lt;a href="https://pypi.org/project/shap" rel="nofollow"&gt;PyPI&lt;/a&gt; or &lt;a href="https://anaconda.org/conda-forge/shap" rel="nofollow"&gt;conda-forge&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;pip install shap
&lt;i&gt;or&lt;/i&gt;
conda install -c conda-forge shap
&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-tree-ensemble-example-with-treeexplainer-xgboostlightgbmcatboostscikit-learnpyspark-models" class="anchor" aria-hidden="true" href="#tree-ensemble-example-with-treeexplainer-xgboostlightgbmcatboostscikit-learnpyspark-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tree ensemble example with TreeExplainer (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)&lt;/h2&gt;
&lt;p&gt;While SHAP values can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (&lt;a href="https://arxiv.org/abs/1802.03888" rel="nofollow"&gt;Tree SHAP arXiv paper&lt;/a&gt;). Fast C++ implementations are supported for &lt;em&gt;XGBoost&lt;/em&gt;, &lt;em&gt;LightGBM&lt;/em&gt;, &lt;em&gt;CatBoost&lt;/em&gt;, &lt;em&gt;scikit-learn&lt;/em&gt; and &lt;em&gt;pyspark&lt;/em&gt; tree models:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; xgboost
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load JS visualization code to notebook&lt;/span&gt;
shap.initjs()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train XGBoost model&lt;/span&gt;
X,y &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.boston()
model &lt;span class="pl-k"&gt;=&lt;/span&gt; xgboost.train({&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.01&lt;/span&gt;}, xgboost.DMatrix(X, &lt;span class="pl-v"&gt;label&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;y), &lt;span class="pl-c1"&gt;100&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain the model's predictions using SHAP values&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)&lt;/span&gt;
explainer &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.TreeExplainer(model)
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; explainer.shap_values(X)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)&lt;/span&gt;
shap.force_plot(explainer.expected_value, shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], X.iloc[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:])&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png"&gt;&lt;img width="811" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue (these force plots are introduced in our &lt;a href="https://www.nature.com/articles/s41551-018-0304-0" rel="nofollow"&gt;Nature BME paper&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; visualize the training set predictions&lt;/span&gt;
shap.force_plot(explainer.expected_value, shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png"&gt;&lt;img width="811" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;To understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature's responsibility for a change in the model output, the plot below represents the change in predicted house price as RM (the average number of rooms per house in an area) changes. Vertical dispersion at a single value of RM represents interaction effects with other features. To help reveal these interactions &lt;code&gt;dependence_plot&lt;/code&gt; automatically selects another feature for coloring. In this case coloring by RAD (index of accessibility to radial highways) highlights that the average number of rooms per house has less impact on home price for areas with a high RAD value.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; create a SHAP dependence plot to show the effect of a single feature across the whole dataset&lt;/span&gt;
shap.dependence_plot(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;RM&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dependence_plot.png"&gt;&lt;img width="544" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dependence_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;To get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that a high LSTAT (% lower status of the population) lowers the predicted home price.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; summarize the effects of all the features&lt;/span&gt;
shap.summary_plot(shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot.png"&gt;&lt;img width="483" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;shap.summary_plot(shap_values, X, &lt;span class="pl-v"&gt;plot_type&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;bar&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot_bar.png"&gt;&lt;img width="470" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot_bar.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deep-learning-example-with-deepexplainer-tensorflowkeras-models" class="anchor" aria-hidden="true" href="#deep-learning-example-with-deepexplainer-tensorflowkeras-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep learning example with DeepExplainer (TensorFlow/Keras models)&lt;/h2&gt;
&lt;p&gt;Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with &lt;a href="https://arxiv.org/abs/1704.02685" rel="nofollow"&gt;DeepLIFT&lt;/a&gt; described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ...include code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&lt;/span&gt;

&lt;span class="pl-k"&gt;import&lt;/span&gt; shap
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; select a set of background examples to take an expectation over&lt;/span&gt;
background &lt;span class="pl-k"&gt;=&lt;/span&gt; x_train[np.random.choice(x_train.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], &lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-v"&gt;replace&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain predictions of the model on four images&lt;/span&gt;
e &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.DeepExplainer(model, background)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ...or pass tensors directly&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)&lt;/span&gt;
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; e.shap_values(x_test[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;5&lt;/span&gt;])

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the feature attributions&lt;/span&gt;
shap.image_plot(shap_values, &lt;span class="pl-k"&gt;-&lt;/span&gt;x_test[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;5&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png"&gt;&lt;img width="820" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the 'zero' image the blank middle is important, while for the 'four' image the lack of a connection on top makes it a four instead of a nine.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models" class="anchor" aria-hidden="true" href="#deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)&lt;/h2&gt;
&lt;p&gt;Expected gradients combines ideas from &lt;a href="https://arxiv.org/abs/1703.01365" rel="nofollow"&gt;Integrated Gradients&lt;/a&gt;, SHAP, and &lt;a href="https://arxiv.org/abs/1706.03825" rel="nofollow"&gt;SmoothGrad&lt;/a&gt; into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; keras.applications.vgg16 &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-c1"&gt;VGG16&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; keras.applications.vgg16 &lt;span class="pl-k"&gt;import&lt;/span&gt; preprocess_input
&lt;span class="pl-k"&gt;import&lt;/span&gt; keras.backend &lt;span class="pl-k"&gt;as&lt;/span&gt; K
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;import&lt;/span&gt; json
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load pre-trained model and choose two images to explain&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; VGG16(&lt;span class="pl-v"&gt;weights&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;imagenet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;include_top&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
X,y &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.imagenet50()
to_explain &lt;span class="pl-k"&gt;=&lt;/span&gt; X[[&lt;span class="pl-c1"&gt;39&lt;/span&gt;,&lt;span class="pl-c1"&gt;41&lt;/span&gt;]]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load the ImageNet class names&lt;/span&gt;
url &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
fname &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.cache(url)
&lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(fname) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
    class_names &lt;span class="pl-k"&gt;=&lt;/span&gt; json.load(f)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain how the input to the 7th layer of the model explains the top two classes&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;map2layer&lt;/span&gt;(&lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;layer&lt;/span&gt;):
    feed_dict &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;dict&lt;/span&gt;(&lt;span class="pl-c1"&gt;zip&lt;/span&gt;([model.layers[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].input], [preprocess_input(x.copy())]))
    &lt;span class="pl-k"&gt;return&lt;/span&gt; K.get_session().run(model.layers[layer].input, feed_dict)
e &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.GradientExplainer(
    (model.layers[&lt;span class="pl-c1"&gt;7&lt;/span&gt;].input, model.layers[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;].output),
    map2layer(X, &lt;span class="pl-c1"&gt;7&lt;/span&gt;),
    &lt;span class="pl-v"&gt;local_smoothing&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; std dev of smoothing noise&lt;/span&gt;
)
shap_values,indexes &lt;span class="pl-k"&gt;=&lt;/span&gt; e.shap_values(map2layer(to_explain, &lt;span class="pl-c1"&gt;7&lt;/span&gt;), &lt;span class="pl-v"&gt;ranked_outputs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; get the names for the classes&lt;/span&gt;
index_names &lt;span class="pl-k"&gt;=&lt;/span&gt; np.vectorize(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: class_names[&lt;span class="pl-c1"&gt;str&lt;/span&gt;(x)][&lt;span class="pl-c1"&gt;1&lt;/span&gt;])(indexes)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the explanations&lt;/span&gt;
shap.image_plot(shap_values, to_explain, index_names)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png"&gt;&lt;img width="500" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Predictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using &lt;code&gt;ranked_outputs=2&lt;/code&gt; we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-agnostic-example-with-kernelexplainer-explains-any-function" class="anchor" aria-hidden="true" href="#model-agnostic-example-with-kernelexplainer-explains-any-function"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model agnostic example with KernelExplainer (explains any function)&lt;/h2&gt;
&lt;p&gt;Kernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; sklearn
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap
&lt;span class="pl-k"&gt;from&lt;/span&gt; sklearn.model_selection &lt;span class="pl-k"&gt;import&lt;/span&gt; train_test_split

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; print the JS visualization code to the notebook&lt;/span&gt;
shap.initjs()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train a SVM classifier&lt;/span&gt;
X_train,X_test,Y_train,Y_test &lt;span class="pl-k"&gt;=&lt;/span&gt; train_test_split(&lt;span class="pl-k"&gt;*&lt;/span&gt;shap.datasets.iris(), &lt;span class="pl-v"&gt;test_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.2&lt;/span&gt;, &lt;span class="pl-v"&gt;random_state&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
svm &lt;span class="pl-k"&gt;=&lt;/span&gt; sklearn.svm.SVC(&lt;span class="pl-v"&gt;kernel&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;rbf&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;probability&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
svm.fit(X_train, Y_train)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; use Kernel SHAP to explain test set predictions&lt;/span&gt;
explainer &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.KernelExplainer(svm.predict_proba, X_train, &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; explainer.shap_values(X_test, &lt;span class="pl-v"&gt;nsamples&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the SHAP values for the Setosa output of the first instance&lt;/span&gt;
shap.force_plot(explainer.expected_value[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], X_test.iloc[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png"&gt;&lt;img width="810" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.&lt;/p&gt;
&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the SHAP values for the Setosa output of all instances&lt;/span&gt;
shap.force_plot(explainer.expected_value[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], X_test, &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png"&gt;&lt;img width="813" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-shap-interaction-values" class="anchor" aria-hidden="true" href="#shap-interaction-values"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SHAP Interaction Values&lt;/h2&gt;
&lt;p&gt;SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with &lt;code&gt;shap.TreeExplainer(model).shap_interaction_values(X)&lt;/code&gt;. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png"&gt;&lt;img width="483" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sample-notebooks" class="anchor" aria-hidden="true" href="#sample-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample notebooks&lt;/h2&gt;
&lt;p&gt;The notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-treeexplainer" class="anchor" aria-hidden="true" href="#treeexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TreeExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html" rel="nofollow"&gt;&lt;strong&gt;NHANES survival model with XGBoost and SHAP interaction values&lt;/strong&gt;&lt;/a&gt; - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and &lt;code&gt;shap&lt;/code&gt; to uncover complex risk factor relationships.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html" rel="nofollow"&gt;&lt;strong&gt;Census income classification with LightGBM&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html" rel="nofollow"&gt;&lt;strong&gt;League of Legends Win Prediction with XGBoost&lt;/strong&gt;&lt;/a&gt; - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deepexplainer" class="anchor" aria-hidden="true" href="#deepexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html" rel="nofollow"&gt;&lt;strong&gt;MNIST Digit classification with Keras&lt;/strong&gt;&lt;/a&gt; - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html" rel="nofollow"&gt;&lt;strong&gt;Keras LSTM for IMDB Sentiment Classification&lt;/strong&gt;&lt;/a&gt; - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-gradientexplainer" class="anchor" aria-hidden="true" href="#gradientexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GradientExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html" rel="nofollow"&gt;&lt;strong&gt;Explain an Intermediate Layer of VGG16 on ImageNet&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-linearexplainer" class="anchor" aria-hidden="true" href="#linearexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LinearExplainer&lt;/h3&gt;
&lt;p&gt;For a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covaraince matrix. LinearExplainer supports both of these options.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html" rel="nofollow"&gt;&lt;strong&gt;Sentiment Analysis with Logistic Regression&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kernelexplainer" class="anchor" aria-hidden="true" href="#kernelexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;KernelExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes not assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html" rel="nofollow"&gt;&lt;strong&gt;Census income classification with scikit-learn&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html" rel="nofollow"&gt;&lt;strong&gt;ImageNet VGG16 Model with Keras&lt;/strong&gt;&lt;/a&gt; - Explain the classic VGG16 convolutional nerual network's predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html" rel="nofollow"&gt;&lt;strong&gt;Iris classification&lt;/strong&gt;&lt;/a&gt; - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-documentation-notebooks" class="anchor" aria-hidden="true" href="#documentation-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation notebooks&lt;/h2&gt;
&lt;p&gt;These notebooks comprehensively demonstrate how to use specific functions and objects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/plots/decision_plot.html" rel="nofollow"&gt;&lt;code&gt;shap.decision_plot&lt;/code&gt; and &lt;code&gt;shap.multioutput_decision_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html" rel="nofollow"&gt;&lt;code&gt;shap.dependence_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-methods-unified-by-shap" class="anchor" aria-hidden="true" href="#methods-unified-by-shap"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Methods Unified by SHAP&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;LIME:&lt;/em&gt; Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Why should i trust you?: Explaining the predictions of any classifier." Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Shapley sampling values:&lt;/em&gt; Strumbelj, Erik, and Igor Kononenko. "Explaining prediction models and individual predictions with feature contributions." Knowledge and information systems 41.3 (2014): 647-665.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;DeepLIFT:&lt;/em&gt; Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. "Learning important features through propagating activation differences." arXiv preprint arXiv:1704.02685 (2017).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;QII:&lt;/em&gt; Datta, Anupam, Shayak Sen, and Yair Zick. "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems." Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Layer-wise relevance propagation:&lt;/em&gt; Bach, Sebastian, et al. "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation." PloS one 10.7 (2015): e0130140.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Shapley regression values:&lt;/em&gt; Lipovetsky, Stan, and Michael Conklin. "Analysis of regression in game theory approach." Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Tree interpreter:&lt;/em&gt; Saabas, Ando. Interpreting random forests. &lt;a href="http://blog.datadive.net/interpreting-random-forests/" rel="nofollow"&gt;http://blog.datadive.net/interpreting-random-forests/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-citations" class="anchor" aria-hidden="true" href="#citations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citations&lt;/h2&gt;
&lt;p&gt;The algorithms and visualizations used in this package came primarily out of research in &lt;a href="https://suinlee.cs.washington.edu" rel="nofollow"&gt;Su-In Lee's lab&lt;/a&gt; at the University of Washington. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For general use of SHAP you can read/cite our &lt;a href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions" rel="nofollow"&gt;NeurIPS paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/shap_nips.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For TreeExplainer you can (for now) read/cite our &lt;a href="https://arxiv.org/abs/1905.04610" rel="nofollow"&gt;arXiv paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/treeshap_arxiv.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For &lt;code&gt;force_plot&lt;/code&gt; visualizations and medical applications you can read/cite our &lt;a href="https://www.nature.com/articles/s41551-018-0304-0" rel="nofollow"&gt;Nature Biomedical Engineering paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/nature_bme.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;; &lt;a href="https://rdcu.be/baVbR" rel="nofollow"&gt;free access&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/db8c6ffa9af4cf6d17c411a9f7ad56cc1b508c35/68747470733a2f2f7777772e66616365626f6f6b2e636f6d2f74723f69643d3138393134373039313835353939312665763d5061676556696577266e6f7363726970743d31"&gt;&lt;img height="1" width="1" src="https://camo.githubusercontent.com/db8c6ffa9af4cf6d17c411a9f7ad56cc1b508c35/68747470733a2f2f7777772e66616365626f6f6b2e636f6d2f74723f69643d3138393134373039313835353939312665763d5061676556696577266e6f7363726970743d31" data-canonical-src="https://www.facebook.com/tr?id=189147091855991&amp;amp;ev=PageView&amp;amp;noscript=1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>slundberg</author><guid isPermaLink="false">https://github.com/slundberg/shap</guid><pubDate>Thu, 28 Nov 2019 00:25:00 GMT</pubDate></item></channel></rss>