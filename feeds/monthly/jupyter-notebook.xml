<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Jupyter Notebook, This month</title><link>https://github.com/trending/jupyter-notebook?since=monthly</link><description>The top repositories on GitHub for jupyter-notebook, measured monthly</description><pubDate>Tue, 19 Nov 2019 01:07:15 GMT</pubDate><lastBuildDate>Tue, 19 Nov 2019 01:07:15 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>google-research/google-research #1 in Jupyter Notebook, This month</title><link>https://github.com/google-research/google-research</link><description>&lt;p&gt;&lt;i&gt;Google AI Research&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-google-ai-research" class="anchor" aria-hidden="true" href="#google-ai-research"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google AI Research&lt;/h1&gt;
&lt;p&gt;This repository contains code released by
&lt;a href="https://ai.google/research" rel="nofollow"&gt;Google AI Research&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Because the repo is large, we recommend you clone the repo without its history.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/google-research</guid><pubDate>Tue, 19 Nov 2019 00:01:00 GMT</pubDate></item><item><title>ageron/handson-ml2 #2 in Jupyter Notebook, This month</title><link>https://github.com/ageron/handson-ml2</link><description>&lt;p&gt;&lt;i&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-notebooks" class="anchor" aria-hidden="true" href="#machine-learning-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Notebooks&lt;/h1&gt;
&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in
python. It contains the example code and solutions to the exercises in the second edition of my O'Reilly book &lt;a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="nofollow"&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/04da787d30d234eed450dae6b5283b7210d8d10c/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f3531744f685051426d534c2e5f53583337395f424f312c3230342c3230332c3230305f2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/04da787d30d234eed450dae6b5283b7210d8d10c/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f3531744f685051426d534c2e5f53583337395f424f312c3230342c3230332c3230305f2e6a7067" title="book" width="150" data-canonical-src="https://images-na.ssl-images-amazon.com/images/I/51tOhPQBmSL._SX379_BO1,204,203,200_.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the first edition notebooks, check out &lt;a href="https://github.com/ageron/handson-ml"&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-play-with-these-notebooks-without-having-to-install-anything" class="anchor" aria-hidden="true" href="#want-to-play-with-these-notebooks-without-having-to-install-anything"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to play with these notebooks without having to install anything?&lt;/h3&gt;
&lt;p&gt;Use any of the following services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you save anything you care about.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Open this repository in &lt;a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/" rel="nofollow"&gt;Colaboratory&lt;/a&gt;:
&lt;a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e69988217d15707bdd8b6b27f1d7d53a0dd00af7/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f696d672f636f6c61625f66617669636f6e2e69636f" width="90" data-canonical-src="https://colab.research.google.com/img/colab_favicon.ico" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or open it in &lt;a href="https://mybinder.org/v2/gh/ageron/handson-ml2/master" rel="nofollow"&gt;Binder&lt;/a&gt;:
&lt;a href="https://mybinder.org/v2/gh/ageron/handson-ml2/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/69ea8abed4df43bca4c671b965aeffef2c4f897a/68747470733a2f2f6d61747468696173627573736f6e6e6965722e636f6d2f706f7374732f696d672f62696e6465725f6c6f676f5f313238783132382e706e67" width="90" data-canonical-src="https://matthiasbussonnier.com/posts/img/binder_logo_128x128.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Note&lt;/em&gt;: Most of the time, Binder starts up quickly and works great, but when handson-ml2 is updated, Binder creates a new environment from scratch, and this can take quite some time.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or open it in &lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;Deepnote&lt;/a&gt;:
&lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3fae03be31b768100aa2a800d2cc3b6650c6cd48/68747470733a2f2f7777772e646565706e6f74652e636f6d2f7374617469632f696c6c757374726174696f6e2e706e67" width="150" data-canonical-src="https://www.deepnote.com/static/illustration.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-just-want-to-quickly-look-at-some-notebooks-without-executing-any-code" class="anchor" aria-hidden="true" href="#just-want-to-quickly-look-at-some-notebooks-without-executing-any-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt;
&lt;p&gt;Browse this repository using &lt;a href="http://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;jupyter.org's notebook viewer&lt;/a&gt;:
&lt;a href="http://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/079030b4c39b76eafa0c6c3a5bd18112aafe42dd/68747470733a2f2f6a7570797465722e6f72672f6173736574732f6e61765f6c6f676f2e737667" width="150" data-canonical-src="https://jupyter.org/assets/nav_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: &lt;a href="https://github.com/ageron/handson-ml2/blob/master/index.ipynb"&gt;github.com's notebook viewer&lt;/a&gt; also works but it is slower and the math equations are not always displayed correctly.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-install-this-project-on-your-own-machine" class="anchor" aria-hidden="true" href="#want-to-install-this-project-on-your-own-machine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to install this project on your own machine?&lt;/h3&gt;
&lt;p&gt;If you have a working Python 3.5+ environment and git is installed, then this project and its dependencies can be installed with pip. Open a terminal and run the following commands (do not type the &lt;code&gt;$&lt;/code&gt; signs, they just indicate that this is a terminal command):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml2.git
$ cd handson-ml2
$ python3 -m pip install --user --upgrade pip setuptools
$ # Read `requirements.txt` if you want to use a GPU.
$ python3 -m pip install --user --upgrade -r requirements.txt
$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or using Anaconda:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml2.git
$ cd handson-ml2
$ # Read `environment.yml` if you want to use a GPU.
$ conda env create -f environment.yml
$ conda activate tf2
$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you need more detailed installation instructions, read the &lt;a href="INSTALL.md"&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;I would like to thank everyone who contributed to this project, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park who helped on some of the exercise solutions, and to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ageron</author><guid isPermaLink="false">https://github.com/ageron/handson-ml2</guid><pubDate>Tue, 19 Nov 2019 00:02:00 GMT</pubDate></item><item><title>fengdu78/lihang-code #3 in Jupyter Notebook, This month</title><link>https://github.com/fengdu78/lihang-code</link><description>&lt;p&gt;&lt;i&gt;《统计学习方法》的代码实现&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;strong&gt;代码目录&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;第1章 统计学习方法概论&lt;/p&gt;
&lt;p&gt;第2章 感知机&lt;/p&gt;
&lt;p&gt;第3章 k近邻法&lt;/p&gt;
&lt;p&gt;第4章 朴素贝叶斯&lt;/p&gt;
&lt;p&gt;第5章 决策树&lt;/p&gt;
&lt;p&gt;第6章 逻辑斯谛回归&lt;/p&gt;
&lt;p&gt;第7章 支持向量机&lt;/p&gt;
&lt;p&gt;第8章 提升方法&lt;/p&gt;
&lt;p&gt;第9章 EM算法及其推广&lt;/p&gt;
&lt;p&gt;第10章 隐马尔可夫模型&lt;/p&gt;
&lt;p&gt;第11章 条件随机场&lt;/p&gt;
&lt;p&gt;第12章 监督学习方法总结&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;参考：
&lt;a href="https://github.com/wzyonggege/statistical-learning-method"&gt;https://github.com/wzyonggege/statistical-learning-method&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm"&gt;https://github.com/WenDesi/lihang_book_algorithm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.csdn.net/tudaodiaozhale" rel="nofollow"&gt;https://blog.csdn.net/tudaodiaozhale&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码整理和修改：机器学习初学者&lt;/p&gt;
&lt;p&gt;微信公众号：机器学习初学者 &lt;a target="_blank" rel="noopener noreferrer" href="images/gongzhong.jpg"&gt;&lt;img src="images/gongzhong.jpg" alt="gongzhong" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;知识星球：黄博的机器学习圈子&lt;a target="_blank" rel="noopener noreferrer" href="images/zhishixingqiu1.jpg"&gt;&lt;img src="images/zhishixingqiu1.jpg" alt="xingqiu" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/people/fengdu78" rel="nofollow"&gt;知乎&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fengdu78</author><guid isPermaLink="false">https://github.com/fengdu78/lihang-code</guid><pubDate>Tue, 19 Nov 2019 00:03:00 GMT</pubDate></item><item><title>zergtant/pytorch-handbook #4 in Jupyter Notebook, This month</title><link>https://github.com/zergtant/pytorch-handbook</link><description>&lt;p&gt;&lt;i&gt;pytorch handbook是一本开源的书籍，目标是帮助那些希望和使用PyTorch进行深度学习开发和研究的朋友快速入门，其中包含的Pytorch教程全部通过测试保证可以成功运行&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytorch-中文手册pytorch-handbook" class="anchor" aria-hidden="true" href="#pytorch-中文手册pytorch-handbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch 中文手册（pytorch handbook）&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png"&gt;&lt;img src="https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png" alt="pytorch" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-书籍介绍" class="anchor" aria-hidden="true" href="#书籍介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;书籍介绍&lt;/h2&gt;
&lt;p&gt;这是一本开源的书籍，目标是帮助那些希望和使用PyTorch进行深度学习开发和研究的朋友快速入门。&lt;/p&gt;
&lt;p&gt;由于本人水平有限，在写此教程的时候参考了一些网上的资料，在这里对他们表示敬意，我会在每个引用中附上原文地址，方便大家参考。&lt;/p&gt;
&lt;p&gt;深度学习的技术在飞速的发展，同时PyTorch也在不断更新，且本人会逐步完善相关内容。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-版本说明" class="anchor" aria-hidden="true" href="#版本说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;版本说明&lt;/h2&gt;
&lt;p&gt;由于PyTorch版本更迭，教程的版本会与PyTorch版本，保持一致。&lt;/p&gt;
&lt;p&gt;2019.10.10 PyTorch已经发布1.3的稳定版。&lt;/p&gt;
&lt;p&gt;已经全部测试完毕 代码可完全兼容1.3&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-qq-3群" class="anchor" aria-hidden="true" href="#qq-3群"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;QQ 3群&lt;/h2&gt;
&lt;p&gt;群号：773681699&lt;/p&gt;
&lt;p&gt;扫描二维码&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="Pytorch-Handbook-3.png"&gt;&lt;img src="Pytorch-Handbook-3.png" alt="QR" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="//shang.qq.com/wpa/qunwpa?idkey=ee402d5f0e7732b2171e643d729177ce55ac404eafda5edd9b740d73fabe6a96" rel="nofollow"&gt;点击链接加入群聊 『PyTorch Handbook 交流3群』&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1群(985896536)已满，2群(681980831)已满&lt;/p&gt;
&lt;p&gt;不要再加了&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-说明" class="anchor" aria-hidden="true" href="#说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;说明&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;修改错别字请直接提issue或PR&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PR时请注意版本&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;有问题也请直接提issue&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;感谢&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-第一章pytorch-入门" class="anchor" aria-hidden="true" href="#第一章pytorch-入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一章：PyTorch 入门&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="chapter1/1.1-pytorch-introduction.md"&gt;PyTorch 简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/1.2-pytorch-installation.md"&gt;PyTorch 环境搭建&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/1.3-deep-learning-with-pytorch-60-minute-blitz.md"&gt;PyTorch 深度学习：60分钟快速入门（官方）&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter1/1_tensor_tutorial.ipynb"&gt;张量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/2_autograd_tutorial.ipynb"&gt;Autograd：自动求导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/3_neural_networks_tutorial.ipynb"&gt;神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/4_cifar10_tutorial.ipynb"&gt;训练一个分类器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/5_data_parallel_tutorial.ipynb"&gt;选读：数据并行处理（多GPU）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/1.4-pytorch-resource.md"&gt;相关资源介绍&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-第二章-基础" class="anchor" aria-hidden="true" href="#第二章-基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二章 基础&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第一节-pytorch-基础" class="anchor" aria-hidden="true" href="#第一节-pytorch-基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一节 PyTorch 基础&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.1.pytorch-basics-tensor.ipynb"&gt;张量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.2-pytorch-basics-autograd.ipynb"&gt;自动求导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.3-pytorch-basics-nerual-network.ipynb"&gt;神经网络包nn和优化器optm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.4-pytorch-basics-data-loader.ipynb"&gt;数据的加载和预处理&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第二节-深度学习基础及数学原理" class="anchor" aria-hidden="true" href="#第二节-深度学习基础及数学原理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二节 深度学习基础及数学原理&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.2-deep-learning-basic-mathematics.ipynb"&gt;深度学习基础及数学原理&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第三节-神经网络简介" class="anchor" aria-hidden="true" href="#第三节-神经网络简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三节 神经网络简介&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.3-deep-learning-neural-network-introduction.ipynb"&gt;神经网络简介&lt;/a&gt;  注：本章在本地使用微软的Edge打开会崩溃，请使Chrome Firefox打开查看&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第四节-卷积神经网络" class="anchor" aria-hidden="true" href="#第四节-卷积神经网络"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四节 卷积神经网络&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.4-cnn.ipynb"&gt;卷积神经网络&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第五节-循环神经网络" class="anchor" aria-hidden="true" href="#第五节-循环神经网络"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五节 循环神经网络&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.5-rnn.ipynb"&gt;循环神经网络&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第三章-实践" class="anchor" aria-hidden="true" href="#第三章-实践"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三章 实践&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第一节-logistic回归二元分类" class="anchor" aria-hidden="true" href="#第一节-logistic回归二元分类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一节 logistic回归二元分类&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter3/3.1-logistic-regression.ipynb"&gt;logistic回归二元分类&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第二节-cnnmnist数据集手写数字识别" class="anchor" aria-hidden="true" href="#第二节-cnnmnist数据集手写数字识别"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二节 CNN:MNIST数据集手写数字识别&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter3/3.2-mnist.ipynb"&gt;CNN:MNIST数据集手写数字识别&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第三节-rnn实例通过sin预测cos" class="anchor" aria-hidden="true" href="#第三节-rnn实例通过sin预测cos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三节 RNN实例：通过Sin预测Cos&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter3/3.3-rnn.ipynb"&gt;RNN实例：通过Sin预测Cos&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第四章-提高" class="anchor" aria-hidden="true" href="#第四章-提高"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四章 提高&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第一节-fine-tuning" class="anchor" aria-hidden="true" href="#第一节-fine-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一节 Fine-tuning&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.1-fine-tuning.ipynb"&gt;Fine-tuning&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第二节-可视化" class="anchor" aria-hidden="true" href="#第二节-可视化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二节 可视化&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.2.1-visdom.ipynb"&gt;visdom&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="chapter4/4.2.2-tensorboardx.ipynb"&gt;tensorboardx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="chapter4/4.2.3-cnn-visualizing.ipynb"&gt;可视化理解卷积神经网络&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第三节-fastai" class="anchor" aria-hidden="true" href="#第三节-fastai"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三节 Fast.ai&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.3-fastai.ipynb"&gt;Fast.ai&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第四节-训练的一些技巧" class="anchor" aria-hidden="true" href="#第四节-训练的一些技巧"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四节 训练的一些技巧&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第五节-多gpu并行训练" class="anchor" aria-hidden="true" href="#第五节-多gpu并行训练"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五节 多GPU并行训练&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.5-multiply-gpu-parallel-training.ipynb"&gt;多GPU并行计算&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第五章-应用" class="anchor" aria-hidden="true" href="#第五章-应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五章 应用&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第一节-kaggle介绍" class="anchor" aria-hidden="true" href="#第一节-kaggle介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一节 Kaggle介绍&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter5/5.1-kaggle.md"&gt;Kaggle介绍&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第二节-结构化数据" class="anchor" aria-hidden="true" href="#第二节-结构化数据"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二节 结构化数据&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第三节-计算机视觉" class="anchor" aria-hidden="true" href="#第三节-计算机视觉"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三节 计算机视觉&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter5/5.3-Fashion-MNIST.ipynb"&gt;Fashion MNIST 图像分类&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第四节-自然语言处理" class="anchor" aria-hidden="true" href="#第四节-自然语言处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四节 自然语言处理&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第五节-协同过滤" class="anchor" aria-hidden="true" href="#第五节-协同过滤"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五节 协同过滤&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-第六章-资源" class="anchor" aria-hidden="true" href="#第六章-资源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第六章 资源&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-第七章-附录" class="anchor" aria-hidden="true" href="#第七章-附录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第七章 附录&lt;/h3&gt;
&lt;p&gt;transforms的常用操作总结&lt;/p&gt;
&lt;p&gt;pytorch的损失函数总结&lt;/p&gt;
&lt;p&gt;pytorch的优化器总结&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/60543937b5e790e3bca35357ccc1313f4b5f52b3/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d6e632d73612f332e302f38387833312e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/60543937b5e790e3bca35357ccc1313f4b5f52b3/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d6e632d73612f332e302f38387833312e706e67" alt="" data-canonical-src="https://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn" rel="nofollow"&gt;本作品采用知识共享署名-非商业性使用-相同方式共享 3.0  中国大陆许可协议进行许可&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zergtant</author><guid isPermaLink="false">https://github.com/zergtant/pytorch-handbook</guid><pubDate>Tue, 19 Nov 2019 00:04:00 GMT</pubDate></item><item><title>dragen1860/TensorFlow-2.x-Tutorials #5 in Jupyter Notebook, This month</title><link>https://github.com/dragen1860/TensorFlow-2.x-Tutorials</link><description>&lt;p&gt;&lt;i&gt;TensorFlow 2.x version's  Tutorials and Examples, including CNN, RNN, GAN, Auto-Encoders, FasterRCNN, GPT, BERT examples, etc. TF 2.0版入门实例代码，实战教程。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-20-tutorials" class="anchor" aria-hidden="true" href="#tensorflow-20-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 2.0 Tutorials&lt;/h1&gt;
&lt;p&gt;Our repo. is the &lt;strong&gt;Winner&lt;/strong&gt; of &lt;a href="https://devpost.com/software/tensorflow-2-0-tutorials" rel="nofollow"&gt;⚡#PoweredByTF 2.0 Challenge!&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="res/tensorflow-2.0.gif"&gt;&lt;img src="res/tensorflow-2.0.gif" width="250" align="middle" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Timeline:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Oct. 1, 2019: TensorFlow 2.0 Stable!&lt;/li&gt;
&lt;li&gt;Aug. 24, 2019: &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf" rel="nofollow"&gt;TensorFlow 2.0 rc0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jun. 8, 2019: &lt;a href="https://twitter.com/fchollet/status/1134583289384120320" rel="nofollow"&gt;TensorFlow 2.0 Beta&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mar. 7, 2019: &lt;a href="https://www.tensorflow.org/alpha" rel="nofollow"&gt;Tensorflow 2.0 Alpha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jan. 11, 2019: &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf" rel="nofollow"&gt;TensorFlow r2.0 preview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Aug. 14, 2018: &lt;a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/bgug1G6a89A" rel="nofollow"&gt;TensorFlow 2.0 is coming&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;p&gt;make sure you are using python 3.x.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU install&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;pip install tensorflow &lt;span class="pl-k"&gt;-&lt;/span&gt;U&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;GPU install&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Install &lt;code&gt;CUDA 10.0&lt;/code&gt;(or after) and &lt;code&gt;cudnn&lt;/code&gt; by yourself. and set &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; up.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;pip install tensorflow&lt;span class="pl-k"&gt;-&lt;/span&gt;gpu  &lt;span class="pl-k"&gt;-&lt;/span&gt;U&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Test installation:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;In [&lt;span class="pl-c1"&gt;2&lt;/span&gt;]: &lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow  &lt;span class="pl-k"&gt;as&lt;/span&gt; tf

In [&lt;span class="pl-c1"&gt;3&lt;/span&gt;]: tf.&lt;span class="pl-c1"&gt;__version__&lt;/span&gt;
Out[&lt;span class="pl-c1"&gt;3&lt;/span&gt;]: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;2.0.0&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
In [&lt;span class="pl-c1"&gt;4&lt;/span&gt;]: tf.test.is_gpu_available()
&lt;span class="pl-c1"&gt;...&lt;/span&gt;
totalMemory: &lt;span class="pl-c1"&gt;3.&lt;/span&gt;&lt;span class="pl-ii"&gt;95GiB&lt;/span&gt; freeMemory: &lt;span class="pl-c1"&gt;3.&lt;/span&gt;&lt;span class="pl-ii"&gt;00GiB&lt;/span&gt;
&lt;span class="pl-c1"&gt;...&lt;/span&gt;
Out[&lt;span class="pl-c1"&gt;4&lt;/span&gt;]: &lt;span class="pl-c1"&gt;True&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-配套tf2视频教程" class="anchor" aria-hidden="true" href="#配套tf2视频教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;配套TF2视频教程&lt;/h1&gt;
&lt;p align="center"&gt;
  &lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;
    &lt;img src="res/cover.png" width="400" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;
    &lt;img src="res/TF_QR_163.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/p&gt; 
&lt;p&gt;TensorFlow 2.0的视频教程链接：&lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;深度学习与TensorFlow 2实战&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;爱可可-爱生活 友情推荐 &lt;a target="_blank" rel="noopener noreferrer" href="res/weibo.jpg"&gt;&lt;img src="res/weibo.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-includes" class="anchor" aria-hidden="true" href="#includes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Includes&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow 2.0 Overview&lt;/li&gt;
&lt;li&gt;TensorFlow 2.0 Basic Usage&lt;/li&gt;
&lt;li&gt;Linear Regression&lt;/li&gt;
&lt;li&gt;MNIST, FashionMNIST&lt;/li&gt;
&lt;li&gt;CIFAR10&lt;/li&gt;
&lt;li&gt;Fully Connected Layer&lt;/li&gt;
&lt;li&gt;VGG16&lt;/li&gt;
&lt;li&gt;Inception Network&lt;/li&gt;
&lt;li&gt;ResNet18&lt;/li&gt;
&lt;li&gt;Naive RNN&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;ColorBot&lt;/li&gt;
&lt;li&gt;Auto-Encoders&lt;/li&gt;
&lt;li&gt;Variational Auto-Encoders&lt;/li&gt;
&lt;li&gt;DCGAN&lt;/li&gt;
&lt;li&gt;CycleGAN&lt;/li&gt;
&lt;li&gt;WGAN&lt;/li&gt;
&lt;li&gt;Pixel2Pixel&lt;/li&gt;
&lt;li&gt;Faster RCNN&lt;/li&gt;
&lt;li&gt;A2C&lt;/li&gt;
&lt;li&gt;GPT&lt;/li&gt;
&lt;li&gt;BERT&lt;/li&gt;
&lt;li&gt;GCN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feel free to submit a &lt;strong&gt;PR&lt;/strong&gt; request to make this repo. more complete!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-refered-repos" class="anchor" aria-hidden="true" href="#refered-repos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Refered Repos.&lt;/h1&gt;
&lt;p&gt;Our work is not built from scratch. Great appreciation to these open works！&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/madalinabuzau/tensorflow-eager-tutorials"&gt;https://github.com/madalinabuzau/tensorflow-eager-tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/herbiebradley/CycleGAN-Tensorflow"&gt;https://github.com/herbiebradley/CycleGAN-Tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb"&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/moono/tf-eager-on-GAN"&gt;https://github.com/moono/tf-eager-on-GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Viredery/tf-eager-fasterrcnn"&gt;https://github.com/Viredery/tf-eager-fasterrcnn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/github/gitignore/blob/master/Python.gitignore"&gt;https://github.com/github/gitignore/blob/master/Python.gitignore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dragen1860</author><guid isPermaLink="false">https://github.com/dragen1860/TensorFlow-2.x-Tutorials</guid><pubDate>Tue, 19 Nov 2019 00:05:00 GMT</pubDate></item><item><title>fengdu78/Data-Science-Notes #6 in Jupyter Notebook, This month</title><link>https://github.com/fengdu78/Data-Science-Notes</link><description>&lt;p&gt;&lt;i&gt;数据科学的笔记以及资料搜集&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-data-science-notes" class="anchor" aria-hidden="true" href="#data-science-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data-Science-Notes&lt;/h1&gt;
&lt;p&gt;数据科学的笔记以及资料搜集，目前尚在更新，部分内容来源于github搜集。&lt;/p&gt;
&lt;p&gt;&lt;a href="0.math"&gt;0.math&lt;/a&gt; （数学基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="1.python-basic"&gt;1.python-basic&lt;/a&gt; （python基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="2.numpy"&gt;2.numpy&lt;/a&gt;（numpy基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="3.pandas"&gt;3.pandas&lt;/a&gt;（pandas基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="4.scipy"&gt;4.scipy&lt;/a&gt;（scipy基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="5.data-visualization"&gt;5.data-visualization&lt;/a&gt;（数据可视化基础，包含matplotlib和seaborn）&lt;/p&gt;
&lt;p&gt;&lt;a href="6.scikit-learn"&gt;6.scikit-learn&lt;/a&gt;（scikit-learn基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="7.machine-learning"&gt;7.machine-learning&lt;/a&gt;（机器学习基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="8.deep-learning"&gt;8.deep-learning&lt;/a&gt;（深度学习基础）&lt;/p&gt;
&lt;p&gt;&lt;a href="9.feature-engineering"&gt;9.feature-engineering&lt;/a&gt;（特征工程基础）&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-参考" class="anchor" aria-hidden="true" href="#参考"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;《统计学习方法》李航&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/donnemartin/data-science-ipython-notebooks"&gt;https://github.com/donnemartin/data-science-ipython-notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apachecn/feature-engineering-for-ml-zh"&gt;https://github.com/apachecn/feature-engineering-for-ml-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datawhalechina/pumpkin-book"&gt;https://github.com/datawhalechina/pumpkin-book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Doraemonzzz/Learning-from-data"&gt;https://github.com/Doraemonzzz/Learning-from-data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wzyonggege/statistical-learning-method"&gt;https://github.com/wzyonggege/statistical-learning-method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm"&gt;https://github.com/WenDesi/lihang_book_algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/course/ml" rel="nofollow"&gt;https://www.coursera.org/course/ml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mooc.guokr.com/note/12/" rel="nofollow"&gt;https://mooc.guokr.com/note/12/&lt;/a&gt; &lt;a href="https://mooc.guokr.com/user/2133483357/" rel="nofollow"&gt;小小人_V&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;《python科学计算》&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-关于作者" class="anchor" aria-hidden="true" href="#关于作者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;关于作者&lt;/h2&gt;
&lt;p&gt;微信公众号：机器学习初学者 &lt;a target="_blank" rel="noopener noreferrer" href="images/gongzhong.jpg"&gt;&lt;img src="images/gongzhong.jpg" alt="gongzhong" style="max-width:100%;"&gt;&lt;/a&gt;
知识星球：黄博的机器学习圈子&lt;a target="_blank" rel="noopener noreferrer" href="images/zhishixingqiu1.jpg"&gt;&lt;img src="images/zhishixingqiu1.jpg" alt="xingqiu" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/people/fengdu78/activities" rel="nofollow"&gt;我的知乎&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意：github下载太慢的话，关注我的公众号：“机器学习初学者”，回复“学习路线”即可下载本仓库的镜像文件，整个仓库压缩成一个iso。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果需要引用这个Repo:&lt;/p&gt;
&lt;p&gt;格式： &lt;code&gt;fengdu78, Data-Science-Notes, (2019), GitHub repository, https://github.com/fengdu78/Data-Science-Notes&lt;/code&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fengdu78</author><guid isPermaLink="false">https://github.com/fengdu78/Data-Science-Notes</guid><pubDate>Tue, 19 Nov 2019 00:06:00 GMT</pubDate></item><item><title>ShusenTang/Dive-into-DL-PyTorch #7 in Jupyter Notebook, This month</title><link>https://github.com/ShusenTang/Dive-into-DL-PyTorch</link><description>&lt;p&gt;&lt;i&gt;本项目将《动手学深度学习》(Dive into Deep Learning)原书中的MXNet实现改为PyTorch实现。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="docs/README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="img/cover.png"&gt;&lt;img width="500" src="img/cover.png" alt="封面" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch" rel="nofollow"&gt;本项目&lt;/a&gt;将&lt;a href="http://zh.d2l.ai/" rel="nofollow"&gt;《动手学深度学习》&lt;/a&gt; 原书中MXNet代码实现改为PyTorch实现。原书作者：阿斯顿·张、李沐、扎卡里 C. 立顿、亚历山大 J. 斯莫拉以及其他社区贡献者，GitHub地址：&lt;a href="https://github.com/d2l-ai/d2l-zh"&gt;https://github.com/d2l-ai/d2l-zh&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;此书的&lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;中&lt;/a&gt;&lt;a href="https://d2l.ai/" rel="nofollow"&gt;英&lt;/a&gt;版本存在一些不同，针对此书英文版的PyTorch重构可参考&lt;a href="https://github.com/dsgiitr/d2l-pytorch"&gt;这个项目&lt;/a&gt;。
There are some differences between the &lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;Chinese&lt;/a&gt; and &lt;a href="https://d2l.ai/" rel="nofollow"&gt;English&lt;/a&gt; versions of this book. For the PyTorch modifying of the English version, you can refer to &lt;a href="https://github.com/dsgiitr/d2l-pytorch"&gt;this repo&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简介&lt;/h2&gt;
&lt;p&gt;本仓库主要包含code和docs两个文件夹（外加一些数据存放在data中）。其中code文件夹就是每章相关jupyter notebook代码（基于PyTorch）；docs文件夹就是markdown格式的《动手学深度学习》书中的相关内容，然后利用&lt;a href="https://docsify.js.org/#/zh-cn/" rel="nofollow"&gt;docsify&lt;/a&gt;将网页文档部署到GitHub Pages上，由于原书使用的是MXNet框架，所以docs内容可能与原书略有不同，但是整体内容是一样的。欢迎对本项目做出贡献或提出issue。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-面向人群" class="anchor" aria-hidden="true" href="#面向人群"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;面向人群&lt;/h2&gt;
&lt;p&gt;本项目面向对深度学习感兴趣，尤其是想使用PyTorch进行深度学习的童鞋。本项目并不要求你有任何深度学习或者机器学习的背景知识，你只需了解基础的数学和编程，如基础的线性代数、微分和概率，以及基础的Python编程。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-食用方法" class="anchor" aria-hidden="true" href="#食用方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;食用方法&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-方法一" class="anchor" aria-hidden="true" href="#方法一"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;方法一&lt;/h3&gt;
&lt;p&gt;本仓库包含一些latex公式，但github的markdown原生是不支持公式显示的，而docs文件夹已经利用&lt;a href="https://docsify.js.org/#/zh-cn/" rel="nofollow"&gt;docsify&lt;/a&gt;被部署到了GitHub Pages上，所以查看文档最简便的方法就是直接访问&lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch" rel="nofollow"&gt;本项目网页版&lt;/a&gt;。当然如果你还想跑一下运行相关代码的话还是得把本项目clone下来，然后运行code文件夹下相关代码。&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-方法二" class="anchor" aria-hidden="true" href="#方法二"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;方法二&lt;/h3&gt;
&lt;p&gt;你还可以在本地访问文档，先安装&lt;code&gt;docsify-cli&lt;/code&gt;工具:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;npm i docsify-cli -g&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后将本项目clone到本地:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/ShusenTang/Dive-into-DL-PyTorch.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; Dive-into-DL-PyTorch&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后运行一个本地服务器，这样就可以很方便的在&lt;code&gt;http://localhost:3000&lt;/code&gt;实时访问文档网页渲染效果。&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docsify serve docs&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="read_guide.md"&gt;阅读指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter01_DL-intro/deep-learning-intro.md"&gt;1. 深度学习简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2. 预备知识
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.1_install.md"&gt;2.1 环境配置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.2_tensor.md"&gt;2.2 数据操作&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.3_autograd.md"&gt;2.3 自动求梯度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3. 深度学习基础
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.1_linear-regression.md"&gt;3.1 线性回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.2_linear-regression-scratch.md"&gt;3.2 线性回归的从零开始实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.3_linear-regression-pytorch.md"&gt;3.3 线性回归的简洁实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.4_softmax-regression.md"&gt;3.4 softmax回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.5_fashion-mnist.md"&gt;3.5 图像分类数据集（Fashion-MNIST）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.6_softmax-regression-scratch.md"&gt;3.6 softmax回归的从零开始实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.7_softmax-regression-pytorch.md"&gt;3.7 softmax回归的简洁实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.8_mlp.md"&gt;3.8 多层感知机&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.9_mlp-scratch.md"&gt;3.9 多层感知机的从零开始实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.10_mlp-pytorch.md"&gt;3.10 多层感知机的简洁实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.11_underfit-overfit.md"&gt;3.11 模型选择、欠拟合和过拟合&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.12_weight-decay.md"&gt;3.12 权重衰减&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.13_dropout.md"&gt;3.13 丢弃法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.14_backprop.md"&gt;3.14 正向传播、反向传播和计算图&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.15_numerical-stability-and-init.md"&gt;3.15 数值稳定性和模型初始化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.16_kaggle-house-price.md"&gt;3.16 实战Kaggle比赛：房价预测&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;4. 深度学习计算
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.1_model-construction.md"&gt;4.1 模型构造&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.2_parameters.md"&gt;4.2 模型参数的访问、初始化和共享&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.3_deferred-init.md"&gt;4.3 模型参数的延后初始化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.4_custom-layer.md"&gt;4.4 自定义层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.5_read-write.md"&gt;4.5 读取和存储&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.6_use-gpu.md"&gt;4.6 GPU计算&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;5. 卷积神经网络
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.1_conv-layer.md"&gt;5.1 二维卷积层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.2_padding-and-strides.md"&gt;5.2 填充和步幅&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.3_channels.md"&gt;5.3 多输入通道和多输出通道&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.4_pooling.md"&gt;5.4 池化层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.5_lenet.md"&gt;5.5 卷积神经网络（LeNet）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.6_alexnet.md"&gt;5.6 深度卷积神经网络（AlexNet）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.7_vgg.md"&gt;5.7 使用重复元素的网络（VGG）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.8_nin.md"&gt;5.8 网络中的网络（NiN）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.9_googlenet.md"&gt;5.9 含并行连结的网络（GoogLeNet）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.10_batch-norm.md"&gt;5.10 批量归一化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.11_resnet.md"&gt;5.11 残差网络（ResNet）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.12_densenet.md"&gt;5.12 稠密连接网络（DenseNet）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;6. 循环神经网络
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.1_lang-model.md"&gt;6.1 语言模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.2_rnn.md"&gt;6.2 循环神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.3_lang-model-dataset.md"&gt;6.3 语言模型数据集（周杰伦专辑歌词）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.4_rnn-scratch.md"&gt;6.4 循环神经网络的从零开始实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.5_rnn-pytorch.md"&gt;6.5 循环神经网络的简洁实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.6_bptt.md"&gt;6.6 通过时间反向传播&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.7_gru.md"&gt;6.7 门控循环单元（GRU）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.8_lstm.md"&gt;6.8 长短期记忆（LSTM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.9_deep-rnn.md"&gt;6.9 深度循环神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.10_bi-rnn.md"&gt;6.10 双向循环神经网络&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;7. 优化算法
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.1_optimization-intro.md"&gt;7.1 优化与深度学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.2_gd-sgd.md"&gt;7.2 梯度下降和随机梯度下降&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.3_minibatch-sgd.md"&gt;7.3 小批量随机梯度下降&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.4_momentum.md"&gt;7.4 动量法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.5_adagrad.md"&gt;7.5 AdaGrad算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.6_rmsprop.md"&gt;7.6 RMSProp算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.7_adadelta.md"&gt;7.7 AdaDelta算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.8_adam.md"&gt;7.8 Adam算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;8. 计算性能
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.1_hybridize.md"&gt;8.1 命令式和符号式混合编程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.2_async-computation.md"&gt;8.2 异步计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.3_auto-parallelism.md"&gt;8.3 自动并行计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.4_multiple-gpus.md"&gt;8.4 多GPU计算&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;9. 计算机视觉
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.1_image-augmentation.md"&gt;9.1 图像增广&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.2_fine-tuning.md"&gt;9.2 微调&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.3_bounding-box.md"&gt;9.3 目标检测和边界框&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.4_anchor.md"&gt;9.4 锚框&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.5_multiscale-object-detection.md"&gt;9.5 多尺度目标检测&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.6_object-detection-dataset.md"&gt;9.6 目标检测数据集（皮卡丘）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;待更新...&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;10. 自然语言处理
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.1_word2vec.md"&gt;10.1 词嵌入（word2vec）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.2_approx-training.md"&gt;10.2 近似训练&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.3_word2vec-pytorch.md"&gt;10.3 word2vec的实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.4_fasttext.md"&gt;10.4 子词嵌入（fastText）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.5_glove.md"&gt;10.5 全局向量的词嵌入（GloVe）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.6_similarity-analogy.md"&gt;10.6 求近义词和类比词&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.7_sentiment-analysis-rnn.md"&gt;10.7 文本情感分类：使用循环神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.8_sentiment-analysis-cnn.md"&gt;10.8 文本情感分类：使用卷积神经网络（textCNN）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.9_seq2seq.md"&gt;10.9 编码器—解码器（seq2seq）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.10_beam-search.md"&gt;10.10 束搜索&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.11_attention.md"&gt;10.11 注意力机制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.12_machine-translation.md"&gt;10.12 机器翻译&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;持续更新中......&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-原书地址" class="anchor" aria-hidden="true" href="#原书地址"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;原书地址&lt;/h2&gt;
&lt;p&gt;中文版：&lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;动手学深度学习&lt;/a&gt; | &lt;a href="https://github.com/d2l-ai/d2l-zh"&gt;Github仓库&lt;/a&gt;&lt;br&gt;
English Version: &lt;a href="https://d2l.ai/" rel="nofollow"&gt;Dive into Deep Learning&lt;/a&gt; | &lt;a href="https://github.com/d2l-ai/d2l-en"&gt;Github Repo&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-引用" class="anchor" aria-hidden="true" href="#引用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;引用&lt;/h2&gt;
&lt;p&gt;如果您在研究中使用了这个项目请引用原书:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@book{zhang2019dive,
    title={Dive into Deep Learning},
    author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
    note={\url{http://www.d2l.ai}},
    year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ShusenTang</author><guid isPermaLink="false">https://github.com/ShusenTang/Dive-into-DL-PyTorch</guid><pubDate>Tue, 19 Nov 2019 00:07:00 GMT</pubDate></item><item><title>czy36mengfei/tensorflow2_tutorials_chinese #8 in Jupyter Notebook, This month</title><link>https://github.com/czy36mengfei/tensorflow2_tutorials_chinese</link><description>&lt;p&gt;&lt;i&gt;tensorflow2中文教程，持续更新(当前版本:tensorflow2.0)，tag: tensorflow 2.0 tutorials&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow2_tutorials_chinese" class="anchor" aria-hidden="true" href="#tensorflow2_tutorials_chinese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;tensorflow2_tutorials_chinese&lt;/h1&gt;
&lt;p&gt;tensorflow2中文教程，持续更新（不定期更新）&lt;/p&gt;
&lt;p&gt;tensorflow 2.0 正式版已上线， 后面将持续根据TensorFlow2的相关教程和学习资料。&lt;/p&gt;
&lt;p&gt;最新tensorflow教程和相关资源，请关注微信公众号：DoitNLP，
后面我会在DoitNLP上，持续更新深度学习、NLP、Tensorflow的相关教程和前沿资讯，它将成为我们一起学习tensorflow的大本营。&lt;/p&gt;
&lt;p&gt;当前tensorflow版本：tensorflow2.0&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最全Tensorflow 2.0 教程持续更新：&lt;/strong&gt;
&lt;a href="https://zhuanlan.zhihu.com/p/59507137" rel="nofollow"&gt;https://zhuanlan.zhihu.com/p/59507137&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本教程主要由tensorflow2.0官方教程的个人学习复现笔记整理而来，并借鉴了一些keras构造神经网络的方法，中文讲解，方便喜欢阅读中文教程的朋友，tensorflow官方教程：&lt;a href="https://www.tensorflow.org" rel="nofollow"&gt;https://www.tensorflow.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58825020" rel="nofollow"&gt;TensorFlow 2.0 教程- Keras 快速入门&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58825710" rel="nofollow"&gt;TensorFlow 2.0 教程-keras 函数api&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58826227" rel="nofollow"&gt;TensorFlow 2.0 教程-使用keras训练模型&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59481536" rel="nofollow"&gt;TensorFlow 2.0 教程-用keras构建自己的网络层&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59481985" rel="nofollow"&gt;TensorFlow 2.0 教程-keras模型保存和序列化&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59482373" rel="nofollow"&gt;TensorFlow 2.0 教程-eager模式&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59482589" rel="nofollow"&gt;TensorFlow 2.0 教程-Variables&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59482934" rel="nofollow"&gt;TensorFlow 2.0 教程--AutoGraph&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TensorFlow 2.0 深度学习实践&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59506238" rel="nofollow"&gt;TensorFlow2.0 教程-图像分类&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59506402" rel="nofollow"&gt;TensorFlow2.0 教程-文本分类&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59506543" rel="nofollow"&gt;TensorFlow2.0 教程-过拟合和欠拟合&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60232704" rel="nofollow"&gt;TensorFlow2.0教程-结构化数据分类&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60238056" rel="nofollow"&gt;TensorFlow2.0教程-回归&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60485936" rel="nofollow"&gt;TensorFlow2.0教程-保持和读取模型&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TensorFlow 2.0 基础网络结构&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60899040" rel="nofollow"&gt;TensorFlow2教程-基础MLP网络&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60900318" rel="nofollow"&gt;TensorFlow2教程-MLP及深度学习常见技巧&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60900649" rel="nofollow"&gt;TensorFlow2教程-基础CNN网络&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60900902" rel="nofollow"&gt;TensorFlow2教程-CNN变体网络&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60901179" rel="nofollow"&gt;TensorFlow2教程-文本卷积&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60966714" rel="nofollow"&gt;TensorFlow2教程-LSTM和GRU&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61077346" rel="nofollow"&gt;TensorFlow2教程-自编码器&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61080045" rel="nofollow"&gt;TensorFlow2教程-卷积自编码器&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61224215" rel="nofollow"&gt;TensorFlow2教程-词嵌入&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61280722" rel="nofollow"&gt;TensorFlow2教程-DCGAN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61400276" rel="nofollow"&gt;TensorFlow2教程-使用Estimator构建Boosted trees&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TensorFlow 2.0 安装&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61472293" rel="nofollow"&gt;TensorFlow2教程-Ubuntu安装TensorFlow 2.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/62036280" rel="nofollow"&gt;TensorFlow2教程-Windows安装tensorflow2.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;完整tensorflow2.0教程代码请看&lt;a href="https://github.com/czy36mengfei/tensorflow2_tutorials_chinese"&gt;tensorflow2.0：中文教程tensorflow2_tutorials_chinese(欢迎star)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;更多TensorFlow 2.0 入门教程请持续关注专栏：&lt;a href="https://zhuanlan.zhihu.com/c_1091021863043624960" rel="nofollow"&gt;Tensorflow2教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;深度学习入门书籍和资源推荐：&lt;a href="https://zhuanlan.zhihu.com/p/65371424" rel="nofollow"&gt;https://zhuanlan.zhihu.com/p/65371424&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>czy36mengfei</author><guid isPermaLink="false">https://github.com/czy36mengfei/tensorflow2_tutorials_chinese</guid><pubDate>Tue, 19 Nov 2019 00:08:00 GMT</pubDate></item><item><title>realpython/materials #9 in Jupyter Notebook, This month</title><link>https://github.com/realpython/materials</link><description>&lt;p&gt;&lt;i&gt;Bonus materials, exercises, and example projects for our Python tutorials&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-real-python-materials" class="anchor" aria-hidden="true" href="#real-python-materials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Real Python Materials&lt;/h1&gt;
&lt;p&gt;Bonus materials, exercises, and example projects for our &lt;a href="https://realpython.com" rel="nofollow"&gt;Python tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Build Status: &lt;a href="https://circleci.com/gh/realpython/materials" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/991534e54579264db6c49f9225646cb77dd8fd6a/68747470733a2f2f636972636c6563692e636f6d2f67682f7265616c707974686f6e2f6d6174657269616c732e7376673f7374796c653d737667" alt="CircleCI" data-canonical-src="https://circleci.com/gh/realpython/materials.svg?style=svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-code-style-checks" class="anchor" aria-hidden="true" href="#running-code-style-checks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Code Style Checks&lt;/h2&gt;
&lt;p&gt;We use &lt;a href="http://flake8.pycqa.org/en/latest/" rel="nofollow"&gt;flake8&lt;/a&gt; and &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; to ensure a consistent code style for all of our sample code in this repository.&lt;/p&gt;
&lt;p&gt;Run the following commands to validate your code against the linters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ flake8
$ black --check &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-running-python-code-formatter" class="anchor" aria-hidden="true" href="#running-python-code-formatter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Python Code Formatter&lt;/h2&gt;
&lt;p&gt;We're using a tool called &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; on this repo to ensure consistent formatting. On CI it runs in "check" mode to ensure any new files added to the repo are following PEP 8. If you see linter warnings that say something like "would reformat some_file.py" it means black disagrees with your formatting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The easiest way to resolve these errors is to just run Black locally on the code and then committing those changes, as explained below.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To automatically re-format your code to be consistent with our code style guidelines, run &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; in the repository root folder:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ black &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>realpython</author><guid isPermaLink="false">https://github.com/realpython/materials</guid><pubDate>Tue, 19 Nov 2019 00:09:00 GMT</pubDate></item><item><title>AtsushiSakai/PythonRobotics #10 in Jupyter Notebook, This month</title><link>https://github.com/AtsushiSakai/PythonRobotics</link><description>&lt;p&gt;&lt;i&gt;Python sample codes for robotics algorithms.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true" align="right" width="300" alt="header pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pythonrobotics" class="anchor" aria-hidden="true" href="#pythonrobotics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PythonRobotics&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/AtsushiSakai/PythonRobotics" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/58f87d5d3604646322c28abd8c5a9b2faa05fa51/68747470733a2f2f7472617669732d63692e6f72672f4174737573686953616b61692f507974686f6e526f626f746963732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/AtsushiSakai/PythonRobotics.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pythonrobotics.readthedocs.io/en/latest/?badge=latest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a60f894ef011c8a7e648348c16aabfdfb603613a/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f707974686f6e726f626f746963732f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/pythonrobotics/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2e66a00c9dcf7ecc1f24189c6055aa7e6da233dc/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f73623237396b787576316265333931673f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/AtsushiSakai/PythonRobotics?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c26144817eba34b4ee9f9a6aee913e6b466218b/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4174737573686953616b61692f507974686f6e526f626f746963732f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/AtsushiSakai/PythonRobotics/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://lgtm.com/projects/g/AtsushiSakai/PythonRobotics/context:python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4c3af4cd47bb2ea2c71cac274f1f7dd392eea893/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f672f4174737573686953616b61692f507974686f6e526f626f746963732e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Language grade: Python" data-canonical-src="https://img.shields.io/lgtm/grade/python/g/AtsushiSakai/PythonRobotics.svg?logo=lgtm&amp;amp;logoWidth=18" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/overview/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c3cd55e61ef2e22ff00427b50b9e7f1c3547de91/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6174737573686973616b61692f707974686f6e726f626f746963732f62616467652f6d6173746572" alt="CodeFactor" data-canonical-src="https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/badge/master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/AtsushiSakai/PythonRobotics"&gt;&lt;img src="https://camo.githubusercontent.com/230f0a1eaa529fa727cad2c9d3c1ace4738bd25d/68747470733a2f2f746f6b65692e72732f62312f6769746875622f4174737573686953616b61692f507974686f6e526f626f74696373" alt="tokei" data-canonical-src="https://tokei.rs/b1/github/AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Python codes for robotics algorithm.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-is-this"&gt;What is this?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#requirements"&gt;Requirements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-use"&gt;How to use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#localization"&gt;Localization&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#extended-kalman-filter-localization"&gt;Extended Kalman Filter localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#particle-filter-localization"&gt;Particle filter localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#histogram-filter-localization"&gt;Histogram filter localization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#mapping"&gt;Mapping&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#gaussian-grid-map"&gt;Gaussian grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ray-casting-grid-map"&gt;Ray casting grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lidar-to-grid-map"&gt;Lidar to grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#k-means-object-clustering"&gt;k-means object clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rectangle-fitting"&gt;Rectangle fitting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#slam"&gt;SLAM&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#iterative-closest-point-icp-matching"&gt;Iterative Closest Point (ICP) Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#fastslam-10"&gt;FastSLAM 1.0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#path-planning"&gt;Path Planning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#dynamic-window-approach"&gt;Dynamic Window Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grid-based-search"&gt;Grid based search&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#dijkstra-algorithm"&gt;Dijkstra algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#a-algorithm"&gt;A* algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#potential-field-algorithm"&gt;Potential Field algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grid-based-coverage-path-planning"&gt;Grid based coverage path planning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#state-lattice-planning"&gt;State Lattice Planning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#biased-polar-sampling"&gt;Biased polar sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lane-sampling"&gt;Lane sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#probabilistic-road-map-prm-planning"&gt;Probabilistic Road-Map (PRM) planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rapidly-exploring-random-trees-rrt"&gt;Rapidly-Exploring Random Trees (RRT)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#rrt"&gt;RRT*&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rrt-with-reeds-shepp-path"&gt;RRT* with reeds-shepp path&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lqr-rrt"&gt;LQR-RRT*&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#quintic-polynomials-planning"&gt;Quintic polynomials planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reeds-shepp-planning"&gt;Reeds Shepp planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lqr-based-path-planning"&gt;LQR based path planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimal-trajectory-in-a-frenet-frame"&gt;Optimal Trajectory in a Frenet Frame&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#path-tracking"&gt;Path Tracking&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#move-to-a-pose-control"&gt;move to a pose control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stanley-control"&gt;Stanley control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rear-wheel-feedback-control"&gt;Rear wheel feedback control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#linearquadratic-regulator-lqr-speed-and-steering-control"&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#model-predictive-speed-and-steering-control"&gt;Model predictive speed and steering control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#nonlinear-model-predictive-control-with-c-gmres"&gt;Nonlinear Model predictive control with C-GMRES&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#arm-navigation"&gt;Arm Navigation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#n-joint-arm-to-point-control"&gt;N joint arm to point control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#arm-navigation-with-obstacle-avoidance"&gt;Arm navigation with obstacle avoidance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#aerial-navigation"&gt;Aerial Navigation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#drone-3d-trajectory-following"&gt;drone 3d trajectory following&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rocket-powered-landing"&gt;rocket powered landing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#bipedal"&gt;Bipedal&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#bipedal-planner-with-inverted-pendulum"&gt;bipedal planner with inverted pendulum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#use-case"&gt;Use-case&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citing"&gt;Citing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#support"&gt;Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#authors"&gt;Authors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-what-is-this" class="anchor" aria-hidden="true" href="#what-is-this"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is this?&lt;/h1&gt;
&lt;p&gt;This is a Python code collection of robotics algorithms, especially for autonomous navigation.&lt;/p&gt;
&lt;p&gt;Features:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Easy to read for understanding each algorithm's basic idea.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Widely used and practical algorithms are selected.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Minimum dependency.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;See this paper for more details:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1808.10703" rel="nofollow"&gt;[1808.10703] PythonRobotics: a Python code collection of robotics algorithms&lt;/a&gt; (&lt;a href="https://github.com/AtsushiSakai/PythonRoboticsPaper/blob/master/python_robotics.bib"&gt;BibTeX&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Python 3.7.x (2.7 is not supported)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;numpy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;scipy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;matplotlib&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;pandas&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.cvxpy.org/index.html" rel="nofollow"&gt;cvxpy&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h1&gt;
&lt;p&gt;This README only shows some examples of this project.&lt;/p&gt;
&lt;p&gt;If you are interested in other examples or mathematical backgrounds of each algorithm,&lt;/p&gt;
&lt;p&gt;You can check the full documentation online: &lt;a href="https://pythonrobotics.readthedocs.io/" rel="nofollow"&gt;https://pythonrobotics.readthedocs.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All animation gifs are stored here: &lt;a href="https://github.com/AtsushiSakai/PythonRoboticsGifs"&gt;AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Clone this repo.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;git clone &lt;a href="https://github.com/AtsushiSakai/PythonRobotics.git"&gt;https://github.com/AtsushiSakai/PythonRobotics.git&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;cd PythonRobotics/&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Install the required libraries. You can use environment.yml with conda command.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;conda env create -f environment.yml&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start="3"&gt;
&lt;li&gt;
&lt;p&gt;Execute python script in each directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add star to this repo if you like it &lt;g-emoji class="g-emoji" alias="smiley" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f603.png"&gt;😃&lt;/g-emoji&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-localization" class="anchor" aria-hidden="true" href="#localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Localization&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-extended-kalman-filter-localization" class="anchor" aria-hidden="true" href="#extended-kalman-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Extended Kalman Filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif" width="640" alt="EKF pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Documentation: &lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/Localization/extended_kalman_filter/extended_kalman_filter_localization.ipynb"&gt;Notebook&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-particle-filter-localization" class="anchor" aria-hidden="true" href="#particle-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Particle filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a sensor fusion localization with Particle Filter(PF).&lt;/p&gt;
&lt;p&gt;The blue line is true trajectory, the black line is dead reckoning trajectory,&lt;/p&gt;
&lt;p&gt;and the red line is estimated trajectory with PF.&lt;/p&gt;
&lt;p&gt;It is assumed that the robot can measure a distance from landmarks (RFID).&lt;/p&gt;
&lt;p&gt;This measurements are used for PF localization.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-histogram-filter-localization" class="anchor" aria-hidden="true" href="#histogram-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Histogram filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a 2D localization example with Histogram filter.&lt;/p&gt;
&lt;p&gt;The red cross is true position, black points are RFID positions.&lt;/p&gt;
&lt;p&gt;The blue grid shows a position probability of histogram filter.&lt;/p&gt;
&lt;p&gt;In this simulation, x,y are unknown, yaw is known.&lt;/p&gt;
&lt;p&gt;The filter integrates speed input and range observations from RFID for localization.&lt;/p&gt;
&lt;p&gt;Initial position is not needed.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-mapping" class="anchor" aria-hidden="true" href="#mapping"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mapping&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-gaussian-grid-map" class="anchor" aria-hidden="true" href="#gaussian-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gaussian grid map&lt;/h2&gt;
&lt;p&gt;This is a 2D Gaussian grid mapping example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ray-casting-grid-map" class="anchor" aria-hidden="true" href="#ray-casting-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ray casting grid map&lt;/h2&gt;
&lt;p&gt;This is a 2D ray casting grid mapping example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-lidar-to-grid-map" class="anchor" aria-hidden="true" href="#lidar-to-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lidar to grid map&lt;/h2&gt;
&lt;p&gt;This example shows how to convert a 2D range measurement to a grid map.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="Mapping/lidar_to_grid_map/animation.gif"&gt;&lt;img src="Mapping/lidar_to_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-k-means-object-clustering" class="anchor" aria-hidden="true" href="#k-means-object-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;k-means object clustering&lt;/h2&gt;
&lt;p&gt;This is a 2D object clustering with k-means algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rectangle-fitting" class="anchor" aria-hidden="true" href="#rectangle-fitting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rectangle fitting&lt;/h2&gt;
&lt;p&gt;This is a 2D rectangle fitting for vehicle detection.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-slam" class="anchor" aria-hidden="true" href="#slam"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SLAM&lt;/h1&gt;
&lt;p&gt;Simultaneous Localization and Mapping(SLAM) examples&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-iterative-closest-point-icp-matching" class="anchor" aria-hidden="true" href="#iterative-closest-point-icp-matching"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Iterative Closest Point (ICP) Matching&lt;/h2&gt;
&lt;p&gt;This is a 2D ICP matching example with singular value decomposition.&lt;/p&gt;
&lt;p&gt;It can calculate a rotation matrix and a translation vector between points to points.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf" rel="nofollow"&gt;Introduction to Mobile Robotics: Iterative Closest Point Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-fastslam-10" class="anchor" aria-hidden="true" href="#fastslam-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FastSLAM 1.0&lt;/h2&gt;
&lt;p&gt;This is a feature based SLAM example using FastSLAM 1.0.&lt;/p&gt;
&lt;p&gt;The blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.&lt;/p&gt;
&lt;p&gt;The red points are particles of FastSLAM.&lt;/p&gt;
&lt;p&gt;Black points are landmarks, blue crosses are estimated landmark positions by FastSLAM.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm" rel="nofollow"&gt;SLAM simulations by Tim Bailey&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-path-planning" class="anchor" aria-hidden="true" href="#path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Path Planning&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-dynamic-window-approach" class="anchor" aria-hidden="true" href="#dynamic-window-approach"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dynamic Window Approach&lt;/h2&gt;
&lt;p&gt;This is a 2D navigation sample code with Dynamic Window Approach.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf" rel="nofollow"&gt;The Dynamic Window Approach to Collision Avoidance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-grid-based-search" class="anchor" aria-hidden="true" href="#grid-based-search"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grid based search&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-dijkstra-algorithm" class="anchor" aria-hidden="true" href="#dijkstra-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dijkstra algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based shortest path planning with Dijkstra's algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-a-algorithm" class="anchor" aria-hidden="true" href="#a-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A* algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based shortest path planning with A star algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt;
&lt;p&gt;Its heuristic is 2D Euclid distance.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-potential-field-algorithm" class="anchor" aria-hidden="true" href="#potential-field-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Potential Field algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based path planning with Potential Field algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif" alt="PotentialField" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, the blue heat map shows potential value on each grid.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf" rel="nofollow"&gt;Robotic Motion Planning:Potential Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-grid-based-coverage-path-planning" class="anchor" aria-hidden="true" href="#grid-based-coverage-path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grid based coverage path planning&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based coverage path planning simulation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif" alt="PotentialField" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-state-lattice-planning" class="anchor" aria-hidden="true" href="#state-lattice-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;State Lattice Planning&lt;/h2&gt;
&lt;p&gt;This script is a path planning code with state lattice planning.&lt;/p&gt;
&lt;p&gt;This code uses the model predictive trajectory generator to solve boundary problem.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://journals.sagepub.com/doi/pdf/10.1177/0278364906075328" rel="nofollow"&gt;Optimal rough terrain trajectory generation for wheeled mobile robots&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.frc.ri.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf" rel="nofollow"&gt;State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-biased-polar-sampling" class="anchor" aria-hidden="true" href="#biased-polar-sampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Biased polar sampling&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-lane-sampling" class="anchor" aria-hidden="true" href="#lane-sampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lane sampling&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-probabilistic-road-map-prm-planning" class="anchor" aria-hidden="true" href="#probabilistic-road-map-prm-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Probabilistic Road-Map (PRM) planning&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif" alt="PRM" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This PRM planner uses Dijkstra method for graph search.&lt;/p&gt;
&lt;p&gt;In the animation, blue points are sampled points,&lt;/p&gt;
&lt;p&gt;Cyan crosses means searched points with Dijkstra method,&lt;/p&gt;
&lt;p&gt;The red line is the final path of PRM.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Probabilistic_roadmap" rel="nofollow"&gt;Probabilistic roadmap - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;　　&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rapidly-exploring-random-trees-rrt" class="anchor" aria-hidden="true" href="#rapidly-exploring-random-trees-rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rapidly-Exploring Random Trees (RRT)&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-rrt" class="anchor" aria-hidden="true" href="#rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RRT*&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a path planning code with RRT*&lt;/p&gt;
&lt;p&gt;Black circles are obstacles, green line is a searched tree, red crosses are start and goal positions.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1005.0416" rel="nofollow"&gt;Incremental Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.419.5503&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-rrt-with-reeds-shepp-path" class="anchor" aria-hidden="true" href="#rrt-with-reeds-shepp-path"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RRT* with reeds-shepp path&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif" alt="Robotics/animation.gif at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Path planning for a car robot with RRT* and reeds shepp path planner.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-lqr-rrt" class="anchor" aria-hidden="true" href="#lqr-rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LQR-RRT*&lt;/h3&gt;
&lt;p&gt;This is a path planning simulation with LQR-RRT*.&lt;/p&gt;
&lt;p&gt;A double integrator motion model is used for LQR local planner.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif" alt="LQRRRT" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://lis.csail.mit.edu/pubs/perez-icra12.pdf" rel="nofollow"&gt;LQR-RRT*: Optimal Sampling-Based Motion Planning with Automatically Derived Extension Heuristics&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/MahanFathi/LQR-RRTstar"&gt;MahanFathi/LQR-RRTstar: LQR-RRT* method is used for random motion planning of a simple pendulum in its phase plot&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-quintic-polynomials-planning" class="anchor" aria-hidden="true" href="#quintic-polynomials-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quintic polynomials planning&lt;/h2&gt;
&lt;p&gt;Motion planning with quintic polynomials.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It can calculate 2D path, velocity, and acceleration profile based on quintic polynomials.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ieeexplore.ieee.org/document/637936/" rel="nofollow"&gt;Local Path Planning And Motion Control For Agv In Positioning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-reeds-shepp-planning" class="anchor" aria-hidden="true" href="#reeds-shepp-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reeds Shepp planning&lt;/h2&gt;
&lt;p&gt;A sample code with Reeds Shepp path planning.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true" alt="RSPlanning" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://planning.cs.uiuc.edu/node822.html" rel="nofollow"&gt;15.3.2 Reeds-Shepp Curves&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf" rel="nofollow"&gt;optimal paths for a car that goes both forwards and backwards&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/ghliu/pyReedsShepp"&gt;ghliu/pyReedsShepp: Implementation of Reeds Shepp curve.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-lqr-based-path-planning" class="anchor" aria-hidden="true" href="#lqr-based-path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LQR based path planning&lt;/h2&gt;
&lt;p&gt;A sample code using LQR based path planning for double integrator model.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true" alt="RSPlanning" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-optimal-trajectory-in-a-frenet-frame" class="anchor" aria-hidden="true" href="#optimal-trajectory-in-a-frenet-frame"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimal Trajectory in a Frenet Frame&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is optimal trajectory generation in a Frenet Frame.&lt;/p&gt;
&lt;p&gt;The cyan line is the target course and black crosses are obstacles.&lt;/p&gt;
&lt;p&gt;The red line is predicted path.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf" rel="nofollow"&gt;Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Cj6tAQe7UCY" rel="nofollow"&gt;Optimal trajectory generation for dynamic street scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-path-tracking" class="anchor" aria-hidden="true" href="#path-tracking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Path Tracking&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-move-to-a-pose-control" class="anchor" aria-hidden="true" href="#move-to-a-pose-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;move to a pose control&lt;/h2&gt;
&lt;p&gt;This is a simulation of moving to a pose control&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://link.springer.com/book/10.1007/978-3-642-20144-8" rel="nofollow"&gt;P. I. Corke, "Robotics, Vision and Control" | SpringerLink p102&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-stanley-control" class="anchor" aria-hidden="true" href="#stanley-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stanley control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with Stanley steering control and PID speed control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://robots.stanford.edu/papers/thrun.stanley05.pdf" rel="nofollow"&gt;Stanley: The robot that won the DARPA grand challenge&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf" rel="nofollow"&gt;Automatic Steering Methods for Autonomous Automobile Path Tracking&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-rear-wheel-feedback-control" class="anchor" aria-hidden="true" href="#rear-wheel-feedback-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rear wheel feedback control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with rear wheel feedback steering control and PID speed control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1604.07446" rel="nofollow"&gt;A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-linearquadratic-regulator-lqr-speed-and-steering-control" class="anchor" aria-hidden="true" href="#linearquadratic-regulator-lqr-speed-and-steering-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with LQR speed and steering control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ieeexplore.ieee.org/document/5940562/" rel="nofollow"&gt;Towards fully autonomous driving: Systems and algorithms - IEEE Conference Publication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-model-predictive-speed-and-steering-control" class="anchor" aria-hidden="true" href="#model-predictive-speed-and-steering-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model predictive speed and steering control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with iterative linear model predictive speed and steering control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif" width="640" alt="MPC pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/model_predictive_speed_and_steer_control/Model_predictive_speed_and_steering_control.ipynb"&gt;notebook&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://grauonline.de/wordpress/?page_id=3244" rel="nofollow"&gt;Real-time Model Predictive Control (MPC), ACADO, Python | Work-is-Playing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-nonlinear-model-predictive-control-with-c-gmres" class="anchor" aria-hidden="true" href="#nonlinear-model-predictive-control-with-c-gmres"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Nonlinear Model predictive control with C-GMRES&lt;/h2&gt;
&lt;p&gt;A motion planning and path tracking simulation with NMPC of C-GMRES&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/cgmres_nmpc/cgmres_nmpc.ipynb"&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-arm-navigation" class="anchor" aria-hidden="true" href="#arm-navigation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arm Navigation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-n-joint-arm-to-point-control" class="anchor" aria-hidden="true" href="#n-joint-arm-to-point-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;N joint arm to point control&lt;/h2&gt;
&lt;p&gt;N joint arm to a point control simulation.&lt;/p&gt;
&lt;p&gt;This is a interactive simulation.&lt;/p&gt;
&lt;p&gt;You can set the goal position of the end effector with left-click on the ploting area.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this simulation N = 10, however, you can change it.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-arm-navigation-with-obstacle-avoidance" class="anchor" aria-hidden="true" href="#arm-navigation-with-obstacle-avoidance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arm navigation with obstacle avoidance&lt;/h2&gt;
&lt;p&gt;Arm navigation with obstacle avoidance simulation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-aerial-navigation" class="anchor" aria-hidden="true" href="#aerial-navigation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Aerial Navigation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-drone-3d-trajectory-following" class="anchor" aria-hidden="true" href="#drone-3d-trajectory-following"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;drone 3d trajectory following&lt;/h2&gt;
&lt;p&gt;This is a 3d trajectory following simulation for a quadrotor.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rocket-powered-landing" class="anchor" aria-hidden="true" href="#rocket-powered-landing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;rocket powered landing&lt;/h2&gt;
&lt;p&gt;This is a 3d trajectory generation simulation for a rocket powered landing.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/AerialNavigation/rocket_powered_landing/rocket_powered_landing.ipynb"&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-bipedal" class="anchor" aria-hidden="true" href="#bipedal"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bipedal&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-bipedal-planner-with-inverted-pendulum" class="anchor" aria-hidden="true" href="#bipedal-planner-with-inverted-pendulum"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;bipedal planner with inverted pendulum&lt;/h2&gt;
&lt;p&gt;This is a bipedal planner for modifying footsteps with inverted pendulum.&lt;/p&gt;
&lt;p&gt;You can set the footsteps and the planner will modify those automatically.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-use-case" class="anchor" aria-hidden="true" href="#use-case"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use-case&lt;/h1&gt;
&lt;p&gt;If this project helps your robotics project, please let me know with &lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Your robot's video, which is using PythonRobotics, is very welcome!!&lt;/p&gt;
&lt;p&gt;This is a list of other user's comment and references:&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md"&gt;users_comments&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h1&gt;
&lt;p&gt;A small PR like bug fix is welcome.&lt;/p&gt;
&lt;p&gt;If your PR is merged multiple times, I will add your account to the author list.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h1&gt;
&lt;p&gt;If you use this project's code for your academic work, we encourage you to cite &lt;a href="https://arxiv.org/abs/1808.10703" rel="nofollow"&gt;our papers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you use this project's code in industry, we'd love to hear from you as well; feel free to reach out to the developers directly.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h1&gt;
&lt;p&gt;If you or your company would like to support this project, please consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.patreon.com/myenigma" rel="nofollow"&gt;Become a backer or sponsor on Patreon&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.paypal.me/myenigmapay/" rel="nofollow"&gt;One-time donation via PayPal&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can add your name or your company logo in README if you are a patron.&lt;/p&gt;
&lt;p&gt;E-mail consultant is also available.&lt;/p&gt;
&lt;p&gt;　&lt;/p&gt;
&lt;p&gt;Your comment using &lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt; is also welcome.&lt;/p&gt;
&lt;p&gt;This is a list: &lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md"&gt;Users comments&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AtsushiSakai/"&gt;Atsushi Sakai&lt;/a&gt; (&lt;a href="https://twitter.com/Atsushi_twi" rel="nofollow"&gt;@Atsushi_twi&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/daniel-s-ingram"&gt;Daniel Ingram&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/jwdinius"&gt;Joe Dinius&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/karanchawla"&gt;Karan Chawla&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/araffin"&gt;Antonin RAFFIN&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AlexisTM"&gt;Alexis Paques&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rsasaki0109"&gt;Ryohei Sasaki&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>AtsushiSakai</author><guid isPermaLink="false">https://github.com/AtsushiSakai/PythonRobotics</guid><pubDate>Tue, 19 Nov 2019 00:10:00 GMT</pubDate></item><item><title>susanli2016/NLP-with-Python #11 in Jupyter Notebook, This month</title><link>https://github.com/susanli2016/NLP-with-Python</link><description>&lt;p&gt;&lt;i&gt;Scikit-Learn, NLTK, Spacy, Gensim, Textblob and more&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-nlp-with-python" class="anchor" aria-hidden="true" href="#nlp-with-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP with Python&lt;/h1&gt;
&lt;p&gt;Scikit-Learn, NLTK, Spacy, Gensim, Textblob and more&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>susanli2016</author><guid isPermaLink="false">https://github.com/susanli2016/NLP-with-Python</guid><pubDate>Tue, 19 Nov 2019 00:11:00 GMT</pubDate></item><item><title>jakevdp/PythonDataScienceHandbook #12 in Jupyter Notebook, This month</title><link>https://github.com/jakevdp/PythonDataScienceHandbook</link><description>&lt;p&gt;&lt;i&gt;Python Data Science Handbook: full text in Jupyter Notebooks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python-data-science-handbook" class="anchor" aria-hidden="true" href="#python-data-science-handbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Data Science Handbook&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository contains the entire &lt;a href="http://shop.oreilly.com/product/0636920034919.do" rel="nofollow"&gt;Python Data Science Handbook&lt;/a&gt;, in the form of (free!) Jupyter notebooks.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="notebooks/figures/PDSH-cover.png"&gt;&lt;img src="notebooks/figures/PDSH-cover.png" alt="cover image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use-this-book" class="anchor" aria-hidden="true" href="#how-to-use-this-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Use this Book&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Read the book in its entirety online at &lt;a href="https://jakevdp.github.io/PythonDataScienceHandbook/" rel="nofollow"&gt;https://jakevdp.github.io/PythonDataScienceHandbook/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the code using the Jupyter notebooks available in this repository's &lt;a href="notebooks"&gt;notebooks&lt;/a&gt; directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launch executable versions of these notebooks using &lt;a href="http://colab.research.google.com" rel="nofollow"&gt;Google Colab&lt;/a&gt;: &lt;a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launch a live notebook server with these notebooks using &lt;a href="https://beta.mybinder.org/" rel="nofollow"&gt;binder&lt;/a&gt;: &lt;a href="https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Buy the printed book through &lt;a href="http://shop.oreilly.com/product/0636920034919.do" rel="nofollow"&gt;O'Reilly Media&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;The book was written and tested with Python 3.5, though other Python versions (including Python 2.7) should work in nearly all cases.&lt;/p&gt;
&lt;p&gt;The book introduces the core libraries essential for working with data in Python: particularly &lt;a href="http://ipython.org" rel="nofollow"&gt;IPython&lt;/a&gt;, &lt;a href="http://numpy.org" rel="nofollow"&gt;NumPy&lt;/a&gt;, &lt;a href="http://pandas.pydata.org" rel="nofollow"&gt;Pandas&lt;/a&gt;, &lt;a href="http://matplotlib.org" rel="nofollow"&gt;Matplotlib&lt;/a&gt;, &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;Scikit-Learn&lt;/a&gt;, and related packages.
Familiarity with Python as a language is assumed; if you need a quick introduction to the language itself, see the free companion project,
&lt;a href="https://github.com/jakevdp/WhirlwindTourOfPython"&gt;A Whirlwind Tour of Python&lt;/a&gt;: it's a fast-paced introduction to the Python language aimed at researchers and scientists.&lt;/p&gt;
&lt;p&gt;See &lt;a href="http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" rel="nofollow"&gt;Index.ipynb&lt;/a&gt; for an index of the notebooks available to accompany the text.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-software" class="anchor" aria-hidden="true" href="#software"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Software&lt;/h2&gt;
&lt;p&gt;The code in the book was tested with Python 3.5, though most (but not all) will also work correctly with Python 2.7 and other older Python versions.&lt;/p&gt;
&lt;p&gt;The packages I used to run the code in the book are listed in &lt;a href="requirements.txt"&gt;requirements.txt&lt;/a&gt; (Note that some of these exact version numbers may not be available on your platform: you may have to tweak them for your own use).
To install the requirements using &lt;a href="http://conda.pydata.org" rel="nofollow"&gt;conda&lt;/a&gt;, run the following at the command-line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda install --file requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a stand-alone environment named &lt;code&gt;PDSH&lt;/code&gt; with Python 3.5 and all the required package versions, run the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda create -n PDSH python=3.5 --file requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can read more about using conda environments in the &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;Managing Environments&lt;/a&gt; section of the conda documentation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-code" class="anchor" aria-hidden="true" href="#code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code&lt;/h3&gt;
&lt;p&gt;The code in this repository, including all code samples in the notebooks listed above, is released under the &lt;a href="LICENSE-CODE"&gt;MIT license&lt;/a&gt;. Read more at the &lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-text" class="anchor" aria-hidden="true" href="#text"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Text&lt;/h3&gt;
&lt;p&gt;The text content of the book is released under the &lt;a href="LICENSE-TEXT"&gt;CC-BY-NC-ND license&lt;/a&gt;. Read more at &lt;a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode" rel="nofollow"&gt;Creative Commons&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jakevdp</author><guid isPermaLink="false">https://github.com/jakevdp/PythonDataScienceHandbook</guid><pubDate>Tue, 19 Nov 2019 00:12:00 GMT</pubDate></item><item><title>chenyuntc/pytorch-book #13 in Jupyter Notebook, This month</title><link>https://github.com/chenyuntc/pytorch-book</link><description>&lt;p&gt;&lt;i&gt;PyTorch tutorials and fun projects including neural talk, neural style, poem writing, anime generation &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;这是书籍《深度学习框架PyTorch：入门与实践》的对应代码，但是也可以作为一个独立的PyTorch入门指南和教程。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-更新说明" class="anchor" aria-hidden="true" href="#更新说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新说明&lt;/h2&gt;
&lt;p&gt;Working on migration to Pytorch 1.0, stay tuned!&lt;/p&gt;
&lt;p&gt;已更新到&lt;strong&gt;pytorch 0.4.1 (不是0.4.0)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;已更新到&lt;strong&gt;pytorch 0.4.1 (不是0.4.0)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;已更新到&lt;strong&gt;pytorch 0.4.1 (不是0.4.0)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当前版本的代码是基于pytorch 0.4.1， 如果想使用旧版的 请 &lt;code&gt;git checkout v0.2&lt;/code&gt; 或者 &lt;code&gt;git checkout v0.3&lt;/code&gt;。旧版代码有更好的python2/python3 兼容，CPU/GPU兼容测试。 新版的代码未经过完整测试，已在GPU和python3 下测试通过。但是理论上在python2和CPU上不应该有太多的问题。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-内容" class="anchor" aria-hidden="true" href="#内容"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;内容&lt;/h2&gt;
&lt;p&gt;该书（教程/仓库）的内容如图所示：
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e145e8ea41382f9d5400613da168b4051af115b7/687474703a2f2f377a683433722e636f6d322e7a302e676c622e636c6f7564646e2e636f6d2f64656c2f6d696e646d61702e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/e145e8ea41382f9d5400613da168b4051af115b7/687474703a2f2f377a683433722e636f6d322e7a302e676c622e636c6f7564646e2e636f6d2f64656c2f6d696e646d61702e706e67" alt="思维导图" data-canonical-src="http://7zh43r.com2.z0.glb.clouddn.com/del/mindmap.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;可以看出本教程可以分为两部分：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基础部分&lt;/strong&gt;（前五章）讲解PyTorch内容，这部份介绍了PyTorch中主要的的模块，和深度学习中常用的一些工具。对于这部分内容，这里利用Jupyter Notebook作为教学工具，读者可以结合notebook修改运行，反复实验。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第二章介绍如何安装PyTorch和配置学习环境。同时提供了一个快速入门教程，基于官方的教程简化并更新内容，读者可以花费大约1到2小时的时间快速完成入门任务，而后根据需求再选择深入阅读后续相关章节的内容。&lt;/li&gt;
&lt;li&gt;第三章介绍了PyTorch中多维数组Tensor和动态图autograd/Variable的使用，并配以例子，让读者分别使用Tensor和autograd实现线性回归，比较二者的不同点。除了介绍这二者的基础使用之外，本章还对Tensor的底层设计，以及autograd的计算图原理进行比较深入分析，希望能使得读者能对这些底层知识有更全面的掌握。&lt;/li&gt;
&lt;li&gt;第四章介绍了PyTorch中神经网络模块nn的基础用法，同时讲解了神经网络中“层”，“损失函数”，“优化器”等，最后带领读者用不到50行的代码搭建出曾夺得ImageNet冠军的ResNet。&lt;/li&gt;
&lt;li&gt;第五章介绍了PyTorch中数据加载，GPU加速，持久化和可视化等相关工具。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实战部分&lt;/strong&gt;（第六到十章）利用PyTorch实现了几个酷炫有趣的应用，对于这部分的内容，本仓库给出完整的实现代码，并提供预训练好的模型作为demo，供读者测试。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第六章是承上启下的一章，这一章的目标不是教会读者新函数，新知识，而是结合Kaggle中一个经典的比赛，实现一个深度学习中比较简单的图像二分类问题。在实现过程中，带领读者复习前五章的知识，并提出代码规范以合理的组织程序，代码，使得程序更加可读，可维护。第六章还介绍了在PyTorch中如何进行debug。&lt;/li&gt;
&lt;li&gt;第七章为读者讲解了当前最火爆的生成对抗网络（GAN），带领读者从头实现一个动漫头像生成器，能够利用GAN生成风格多变的动漫头像。&lt;/li&gt;
&lt;li&gt;第八章为读者讲解了风格迁移的相关知识，并带领读者实现风格迁移网络，将自己的照片变成高大上的名画。&lt;/li&gt;
&lt;li&gt;第九章为读者讲解了一些自然语言处理的基础知识，并讲解了CharRNN的原理。而后利用收集了几万首唐诗，训练出了一个可以自动写诗歌的小程序。这个小程序可以控制生成诗歌的&lt;strong&gt;格式&lt;/strong&gt;，&lt;strong&gt;意境&lt;/strong&gt;，还能生成&lt;strong&gt;藏头诗&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;第十章为读者介绍了图像描述任务，并以最新的AI Challenger比赛的数据为例，带领读者实现了一个可以进行简单图像描述的的小程序。&lt;/li&gt;
&lt;li&gt;第十一章（&lt;strong&gt;新增，实验性&lt;/strong&gt;） 由&lt;a href="https://github.com/Diamondfan"&gt;Diamondfan&lt;/a&gt; 编写的语音识别。完善了本项目（本项目已囊括图像，文本，语音三大领域的例子）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Notebook中的文字描述内容属于本书的初稿，有描述不通顺，错别字之处还请谅解&lt;/strong&gt;。本打算删除notebook中描述的内容，只留下代码，但为了方便读者阅读学习，最终还是决定留下。 我会抽空根据书中内容逐字校对这部分内容，但并不对此并不提供具体时间点。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-是否需要买书" class="anchor" aria-hidden="true" href="#是否需要买书"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;是否需要买书&lt;/h2&gt;
&lt;p&gt;书&lt;strong&gt;不是必要的&lt;/strong&gt;，这个仓库包含书中50%以上的文字内容，90%以上的代码，尤其是前几章入门内容，几乎是完全保留了书中的讲解内容。读者即使不买书也能正常使用本教程。&lt;/p&gt;
&lt;p&gt;&lt;del&gt;如果你觉得纸质书的优势吸引你，不妨小破费一笔，支持一下作者这大半年来的工作。同时为了尽可能的方便读者，笔者还专门开通腾讯云的服务，用以保存教程中用到的部分模型，预处理的数据和部分大文件。&lt;/del&gt;
书中的部分内容已经过时，以此仓库内容为准。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-代码说明" class="anchor" aria-hidden="true" href="#代码说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;代码说明&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;代码主要在python3下测试得到最终结果，python2暂未测试。v0.2和v0.3 分支的代码同时经过严格测试支持python2/python3&lt;/li&gt;
&lt;li&gt;实战部分代码同时在GPU和CPU环境下测试通过&lt;/li&gt;
&lt;li&gt;代码已更新兼容到PyTorch &lt;code&gt;0.4.1&lt;/code&gt;, 后续会考虑兼容 &lt;code&gt;v1.0&lt;/code&gt;，但暂无确切时间点。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果你想在PyTorch 0.2.0或0.3下运行,请&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git checkout v0.2 # v0.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果有任何不当，或者有待改进的地方，欢迎读者开issue讨论，或者提交pull request。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-环境配置" class="anchor" aria-hidden="true" href="#环境配置"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;环境配置&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;安装&lt;a href="http://pytorch.org" rel="nofollow"&gt;PyTorch&lt;/a&gt;，请从官网选择指定的版本安装即可，一键安装（即使你使用anaconda，也建议使用pip）。更多的安装方式请参阅书中说明。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;克隆仓库&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;git clone https:&lt;span class="pl-k"&gt;//&lt;/span&gt;github.com&lt;span class="pl-k"&gt;/&lt;/span&gt;chenyuntc&lt;span class="pl-k"&gt;/&lt;/span&gt;PyTorch&lt;span class="pl-k"&gt;-&lt;/span&gt;book.git&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装第三方依赖包&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;cd pytorch&lt;span class="pl-k"&gt;-&lt;/span&gt;book &lt;span class="pl-ii"&gt;&amp;amp;&amp;amp;&lt;/span&gt; pip install &lt;span class="pl-k"&gt;-&lt;/span&gt;r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-visdom打不开及其解决方案" class="anchor" aria-hidden="true" href="#visdom打不开及其解决方案"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visdom打不开及其解决方案&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;新版的visdom已经解决了这个问题,只需要升级即可&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install --upgrade visdom
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之前的&lt;a href="https://github.com/chenyuntc/pytorch-book/blob/2c8366137b691aaa8fbeeea478cc1611c09e15f5/README.md#visdom%E6%89%93%E4%B8%8D%E5%BC%80%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"&gt;解决方案&lt;/a&gt; 不再需要，已删除。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-_" class="anchor" aria-hidden="true" href="#_"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;^_^&lt;/h2&gt;
&lt;p&gt;有任何bug，解释不清楚的地方或者是困惑，欢迎开issue&lt;/p&gt;
&lt;p&gt;欢迎pull requests&lt;/p&gt;
&lt;p&gt;Happy Coding!&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0376580818bbc47cd4b2f29ab6ca684122ba6e9f/687474703a2f2f696d6731342e333630627579696d672e636f6d2f6e312f6a66732f7431333333392f33322f323436333733303139382f3231373438332f65383134386336622f35613431323737644e62643134373063312e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/0376580818bbc47cd4b2f29ab6ca684122ba6e9f/687474703a2f2f696d6731342e333630627579696d672e636f6d2f6e312f6a66732f7431333333392f33322f323436333733303139382f3231373438332f65383134386336622f35613431323737644e62643134373063312e6a7067" alt="" data-canonical-src="http://img14.360buyimg.com/n1/jfs/t13339/32/2463730198/217483/e8148c6b/5a41277dNbd1470c1.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://search.jd.com/Search?keyword=pytorch%20%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5&amp;amp;enc=utf-8&amp;amp;wq=pytorch%20%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5&amp;amp;pvid=8b0d91d7108845ad8cbaf596326f3eb3" rel="nofollow"&gt;京东购买链接&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://search.dangdang.com/?key=pytorch%20%C8%EB%C3%C5%D3%EB%CA%B5%BC%F9&amp;amp;act=input" rel="nofollow"&gt;当当购买链接&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>chenyuntc</author><guid isPermaLink="false">https://github.com/chenyuntc/pytorch-book</guid><pubDate>Tue, 19 Nov 2019 00:13:00 GMT</pubDate></item><item><title>microsoft/QuantumKatas #14 in Jupyter Notebook, This month</title><link>https://github.com/microsoft/QuantumKatas</link><description>&lt;p&gt;&lt;i&gt;Tutorials and programming exercises for learning Q# and quantum computing&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The Quantum Katas are a series of self-paced tutorials to help you learn quantum computing and Q# programming.&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="new" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f195.png"&gt;🆕&lt;/g-emoji&gt; &lt;em&gt;(July 2019)&lt;/em&gt; The Quantum Katas now include Jupyter Notebook tutorials on quantum computing! Each tutorial combines theoretical explanations with Q# code snippets and programming exercises. See &lt;a href="https://mybinder.org/v2/gh/Microsoft/QuantumKatas/master?filepath=index.ipynb" rel="nofollow"&gt;index.ipynb&lt;/a&gt; for the list of all tutorials and instructions on running them online.&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="new" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f195.png"&gt;🆕&lt;/g-emoji&gt; &lt;em&gt;(April 2019)&lt;/em&gt; The Quantum Katas are now available as Jupyter Notebooks! See &lt;a href="https://mybinder.org/v2/gh/Microsoft/QuantumKatas/master?filepath=index.ipynb" rel="nofollow"&gt;index.ipynb&lt;/a&gt; for the list of all Kata Notebooks and instructions on running them online.&lt;/p&gt;
&lt;p&gt;Each kata is a separate project that includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A sequence of tasks progressing from easy to hard.
Each task requires you to fill in some code. The first task might require just one line, and the last one might require rather complicated code.&lt;/li&gt;
&lt;li&gt;A testing framework that sets up, runs, and validates your solutions.
Each task is covered by a &lt;a href="https://docs.microsoft.com/en-us/visualstudio/test/getting-started-with-unit-testing" rel="nofollow"&gt;unit test&lt;/a&gt; which initially fails. Once you write the code to make the test pass, you can move on to the next task.&lt;/li&gt;
&lt;li&gt;Links to quantum computing and Q# reference material you might need to solve the tasks.&lt;/li&gt;
&lt;li&gt;Hints and reference solutions to help you if you're stuck.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#tutorial-topics"&gt;List of tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kata-topics"&gt;List of katas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#run-as-notebook"&gt;Run the katas and tutorials online&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kata-locally"&gt;Run the katas locally&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#download"&gt;Download the Quantum Katas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kata-as-project"&gt;Run a kata as a Q# project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tests"&gt;Run kata tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#docker"&gt;Run katas locally with Docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#code-of-conduct"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-list-of-tutorials-" class="anchor" aria-hidden="true" href="#list-of-tutorials-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;List of tutorials &lt;a name="user-content-tutorial-topics"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-tutorial-topics"&gt;
&lt;/a&gt;&lt;ul&gt;&lt;a name="user-content-tutorial-topics"&gt;
&lt;/a&gt;&lt;li&gt;&lt;a name="user-content-tutorial-topics"&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;a href="./tutorials/ComplexArithmetic/"&gt;Complex arithmetic&lt;/a&gt;&lt;/strong&gt;.
Learn about complex numbers and the mathematics required to work with quantum computing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/LinearAlgebra/"&gt;Linear algebra&lt;/a&gt;&lt;/strong&gt;.
Learn about vectors and matrices used to represent quantum states and quantum operations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/Qubit/"&gt;The qubit&lt;/a&gt;&lt;/strong&gt;.
Learn what a qubit is.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/SingleQubitGates/"&gt;Single-qubit gates&lt;/a&gt;&lt;/strong&gt;.
Learn what a quantum gate is and about the most common single-qubit gates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/MultiQubitSystems/"&gt;Multi-qubit systems&lt;/a&gt;&lt;/strong&gt;.
Learn to represent multi-qubit systems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/MultiQubitGates/"&gt;Multi-qubit gates&lt;/a&gt;&lt;/strong&gt;.
Learn about the most common multi-qubit gates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/ExploringDeutschJozsaAlgorithm/"&gt;Exploring Deutsch–Jozsa algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn to implement classical functions and equivalent quantum oracles, and compare the quantum
solution to the Deutsch–Jozsa problem to a classical one.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/ExploringGroversAlgorithm/"&gt;Exploring Grover's search algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn more about Grover's search algorithm, picking up where the &lt;a href="./GroversAlgorithm/"&gt;Grover's algorithm kata&lt;/a&gt; left off.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-list-of-katas-" class="anchor" aria-hidden="true" href="#list-of-katas-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;List of Katas &lt;a name="user-content-kata-topics"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-kata-topics"&gt;
&lt;h4&gt;&lt;a id="user-content-quantum-computing-concepts" class="anchor" aria-hidden="true" href="#quantum-computing-concepts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quantum computing concepts&lt;/h4&gt;
&lt;/a&gt;&lt;ul&gt;&lt;a name="user-content-kata-topics"&gt;
&lt;/a&gt;&lt;li&gt;&lt;a name="user-content-kata-topics"&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;a href="./BasicGates/"&gt;Basic quantum computing gates&lt;/a&gt;&lt;/strong&gt;.
Learn to apply the most common gates used in quantum computing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./Superposition/"&gt;Superposition&lt;/a&gt;&lt;/strong&gt;.
Learn to prepare superposition states.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./Measurements/"&gt;Measurements&lt;/a&gt;&lt;/strong&gt;.
Learn to distinguish quantum states using measurements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./JointMeasurements/"&gt;Joint measurements&lt;/a&gt;&lt;/strong&gt;.
Learn about using joint (parity) measurements to distinguish quantum states and to perform state transformations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-simple-algorithms" class="anchor" aria-hidden="true" href="#simple-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Simple algorithms&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./Teleportation/"&gt;Teleportation&lt;/a&gt;&lt;/strong&gt;.
Implement standard teleportation protocol and its variations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./SuperdenseCoding/"&gt;Superdense coding&lt;/a&gt;&lt;/strong&gt;.
Implement the superdense coding protocol.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./DeutschJozsaAlgorithm/"&gt;Deutsch–Jozsa algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn about quantum oracles which implement classical functions, and implement Bernstein–Vazirani and Deutsch–Jozsa algorithms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./SimonsAlgorithm/"&gt;Simon's algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn about Simon's algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-grovers-algorithm" class="anchor" aria-hidden="true" href="#grovers-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grover's algorithm&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./GroversAlgorithm/"&gt;Grover's algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn about Grover's search algorithm and how to write quantum oracles to use with it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./SolveSATWithGrover/"&gt;Solving SAT problems using Grover's algorithm&lt;/a&gt;&lt;/strong&gt;.
Explore Grover's search algorithm, using SAT problems as an example. Learn to implement quantum oracles based on the problem description instead of a hard-coded answer. Use Grover's algorithm to solve problems with an unknown number of solutions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./GraphColoring/"&gt;Solving graph coloring problems using Grover's algorithm&lt;/a&gt;&lt;/strong&gt;.
Continue the exploration of Grover's search algorithm, using graph coloring problems as an example.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-entanglement-games" class="anchor" aria-hidden="true" href="#entanglement-games"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Entanglement games&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./CHSHGame/"&gt;CHSH game&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./GHZGame/"&gt;GHZ game&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./MagicSquareGame"&gt;Mermin-Peres magic square game&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-miscellaneous" class="anchor" aria-hidden="true" href="#miscellaneous"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Miscellaneous&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./KeyDistribution_BB84/"&gt;BB84 protocol&lt;/a&gt;&lt;/strong&gt;.
Implement the BB84 key distribution algorithm.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./PhaseEstimation/"&gt;Phase estimation&lt;/a&gt;&lt;/strong&gt;.
Learn about phase estimation algorithms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./QEC_BitFlipCode/"&gt;Bit-flip error correcting code&lt;/a&gt;&lt;/strong&gt;.
Learn about a 3-qubit error correcting code for protecting against bit-flip errors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./RippleCarryAdder/"&gt;Ripple-carry adder&lt;/a&gt;&lt;/strong&gt;.
Build a ripple-carry adder on a quantum computer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./UnitaryPatterns/"&gt;Unitary Patterns*&lt;/a&gt;&lt;/strong&gt;.
Learn to implement unitaries with matrices that follow certain patterns of zero and non-zero elements.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-run-the-katas-and-tutorials-online-" class="anchor" aria-hidden="true" href="#run-the-katas-and-tutorials-online-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run the katas and tutorials online &lt;a name="user-content-run-as-notebook"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-run-as-notebook"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-run-as-notebook"&gt;The Quantum Katas are now available as Jupyter Notebooks! See &lt;/a&gt;&lt;a href="https://mybinder.org/v2/gh/Microsoft/QuantumKatas/master?filepath=index.ipynb" rel="nofollow"&gt;index.ipynb&lt;/a&gt; for the list of all katas and tutorials, and instructions to run them online.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-run-katas-locally-" class="anchor" aria-hidden="true" href="#run-katas-locally-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run katas locally &lt;a name="user-content-kata-locally"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-kata-locally"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-kata-locally"&gt;To use the Quantum Katas locally, you'll need the &lt;/a&gt;&lt;a href="https://docs.microsoft.com/quantum" rel="nofollow"&gt;Quantum Development Kit&lt;/a&gt;, available for Windows 10, macOS, and Linux.
If you don't already have the Quantum Development Kit installed, see &lt;a href="https://docs.microsoft.com/quantum/install-guide/" rel="nofollow"&gt;install guide for the Quantum Development Kit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For a quick Q# programming language reference sheet, see &lt;a href="./quickref/qsharp-quick-reference.pdf"&gt;Q# Language Quick Reference&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-download-the-quantum-katas-" class="anchor" aria-hidden="true" href="#download-the-quantum-katas-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download the Quantum Katas &lt;a name="user-content-download"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-download"&gt;
&lt;p&gt;If you have Git installed, clone the Microsoft/QuantumKatas repository:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/Microsoft/QuantumKatas.git&lt;/pre&gt;&lt;/div&gt;
&lt;/a&gt;&lt;blockquote&gt;&lt;a name="user-content-download"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-download"&gt;[!TIP]
Both Visual Studio 2019 and Visual Studio Code make it easy to clone repositories from within your development environment.
For details, see the &lt;/a&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/devops/repos/git/clone?view=azure-devops&amp;amp;tabs=visual-studio#clone-from-another-git-provider" rel="nofollow"&gt;Visual Studio 2019&lt;/a&gt; and &lt;a href="https://code.visualstudio.com/docs/editor/versioncontrol#_cloning-a-repository" rel="nofollow"&gt;Visual Studio Code&lt;/a&gt; documentation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you don't have Git installed, download the katas from &lt;a href="https://github.com/Microsoft/QuantumKatas/archive/master.zip"&gt;https://github.com/Microsoft/QuantumKatas/archive/master.zip&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run-a-kata-as-a-q-project-" class="anchor" aria-hidden="true" href="#run-a-kata-as-a-q-project-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run a kata as a Q# project &lt;a name="user-content-kata-as-project"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-kata-as-project"&gt;
&lt;p&gt;Each kata is in its own directory as a self-contained Q# project, solution and Jupyter Notebook triplet.
For instance, the BasicGates directory structure is:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;QuantumKatas/
  BasicGates/
    README.md                  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Instructions specific to this kata.&lt;/span&gt;
    .vscode/                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Metadata used by Visual Studio Code.&lt;/span&gt;
    BasicGates.sln             &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Visual Studio 2019 solution file.&lt;/span&gt;
    BasicGates.csproj          &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Project file used to build both classical and quantum code.&lt;/span&gt;
    BasicGates.ipynb           &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Jupyter Notebook front-end for this kata.&lt;/span&gt;

    Tasks.qs                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Q# source code that you will fill as you solve each task.&lt;/span&gt;
    Tests.qs                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Q# tests that verify your solutions.&lt;/span&gt;
    TestSuiteRunner.cs         &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; C# source code used to run the Q# tests.&lt;/span&gt;
    ReferenceImplementation.qs &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Q# source code containing solutions to the tasks.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To open the &lt;strong&gt;BasicGates&lt;/strong&gt; kata in Visual Studio 2019, open the &lt;strong&gt;QuantumKatas/BasicGates/BasicGates.sln&lt;/strong&gt; solution file.&lt;/p&gt;
&lt;p&gt;To open the &lt;strong&gt;BasicGates&lt;/strong&gt; kata in Visual Studio Code, open the &lt;strong&gt;QuantumKatas/BasicGates/&lt;/strong&gt; folder.
Press &lt;strong&gt;Ctrl + Shift + P&lt;/strong&gt; (or &lt;strong&gt;⌘ + Shift + P&lt;/strong&gt; on macOS) to open the &lt;strong&gt;Command Palette&lt;/strong&gt;. Type &lt;strong&gt;Open Folder&lt;/strong&gt; on Windows 10 or Linux or &lt;strong&gt;Open&lt;/strong&gt; on macOS.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[!TIP]
Almost all commands available in Visual Studio Code are in the Command Palette.
If you get stuck, press &lt;strong&gt;Ctrl + Shift + P&lt;/strong&gt; (or &lt;strong&gt;⌘ + Shift + P&lt;/strong&gt; on macOS) and start typing to search through all available commands.&lt;/p&gt;
&lt;p&gt;You can also launch Visual Studio Code from the command line:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ code QuantumKatas/BasicGates/&lt;/pre&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-run-kata-tests-" class="anchor" aria-hidden="true" href="#run-kata-tests-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-kata-as-project"&gt;Run kata tests &lt;/a&gt;&lt;a name="user-content-tests"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-tests"&gt;
&lt;p&gt;Once you have a kata open, it's time to run the tests using the following instructions.
Initially all tests will fail. Don't panic!
Open &lt;strong&gt;Tasks.qs&lt;/strong&gt; and start filling in the code to complete the tasks. Each task is covered by a unit test. Once you fill in the correct code for a task, rebuild the project and re-run the tests, and the corresponding unit test will pass.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-visual-studio-2019" class="anchor" aria-hidden="true" href="#visual-studio-2019"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Studio 2019&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Build the solution.&lt;/li&gt;
&lt;li&gt;From the main menu, open &lt;strong&gt;Test Explorer&lt;/strong&gt; (&lt;strong&gt;Test&lt;/strong&gt; &amp;gt; &lt;strong&gt;Windows&lt;/strong&gt;) and select &lt;strong&gt;Run All&lt;/strong&gt; to run all unit tests at once.&lt;/li&gt;
&lt;li&gt;Work on the tasks in the &lt;strong&gt;Tasks.qs&lt;/strong&gt; file.&lt;/li&gt;
&lt;li&gt;To test your code changes for a task, rebuild the solution and re-run all unit tests using &lt;strong&gt;Run All&lt;/strong&gt;, or run just the test for that task by right-clicking the test and selecting &lt;strong&gt;Run Selected Tests&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-visual-studio-code" class="anchor" aria-hidden="true" href="#visual-studio-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Studio Code&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Press &lt;strong&gt;Ctrl + `&lt;/strong&gt; (or &lt;strong&gt;⌘ + `&lt;/strong&gt; on macOS) to open the integrated terminal.
The terminal should open to the kata directory. If it doesn't, navigate to the folder containing the *.csproj file for the kata using &lt;code&gt;cd&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;dotnet test&lt;/code&gt; in the integrated terminal.
This should build the kata project and run all of the unit tests. All of the unit tests should fail.&lt;/li&gt;
&lt;li&gt;Work on the tasks in the &lt;strong&gt;Tasks.qs&lt;/strong&gt; file.&lt;/li&gt;
&lt;li&gt;To test your code changes for a task, from the integrated terminal run &lt;code&gt;dotnet test&lt;/code&gt; again.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For convenience, a tasks.json configuration file exists for each kata. It allows Visual Studio Code to run the build and test steps from the Command Palette.
Press &lt;strong&gt;Ctrl + Shift + P&lt;/strong&gt; (or &lt;strong&gt;⌘ + Shift + P&lt;/strong&gt; on macOS) to open the Palette and type &lt;strong&gt;Run Build Task&lt;/strong&gt; or &lt;strong&gt;Run Test Task&lt;/strong&gt; and press &lt;strong&gt;Enter&lt;/strong&gt;.&lt;/p&gt;
&lt;/a&gt;&lt;h2&gt;&lt;a id="user-content-run-katas-locally-with-docker-" class="anchor" aria-hidden="true" href="#run-katas-locally-with-docker-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-tests"&gt;Run katas locally with Docker &lt;/a&gt;&lt;a name="user-content-docker"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-docker"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-docker"&gt;You can use the included &lt;/a&gt;&lt;a href="./Dockerfile"&gt;Dockerfile&lt;/a&gt; to create a docker image with all the necessary tools to run the katas from the command line or Jupyter.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href="https://docs.docker.com/install/" rel="nofollow"&gt;Docker&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Build the docker image and tag it &lt;code&gt;katas&lt;/code&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker build -t katas &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Run the image in the container named &lt;code&gt;katas-container&lt;/code&gt; with interactive command-line and redirect container port &lt;code&gt;8888&lt;/code&gt; to local port &lt;code&gt;8888&lt;/code&gt; (needed to run Jupyter):&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker run -it --name katas-container -p 8888:8888 katas /bin/bash&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="4"&gt;
&lt;li&gt;From the same command line that you used to run the container, run the C# version of the &lt;strong&gt;BasicGates&lt;/strong&gt; kata:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; &lt;span class="pl-k"&gt;~&lt;/span&gt;/BasicGates/
dotnet &lt;span class="pl-c1"&gt;test&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Start a Jupyter Notebook within the image for the &lt;strong&gt;BasicGates&lt;/strong&gt; kata:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; &lt;span class="pl-k"&gt;~&lt;/span&gt;/BasicGates/ &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; jupyter notebook --ip=0.0.0.0 --no-browser&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="6"&gt;
&lt;li&gt;Once Jupyter has started, use your browser to open the kata in notebook format. You
will need a token generated by Jupyter when it started on the previous step:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;http://localhost:8888/notebooks/BasicGates.ipynb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To exit a docker container without killing it (daemon mode), press &lt;strong&gt;Ctrl+P, Ctrl+Q&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To re-enter the existing &lt;code&gt;katas-container&lt;/code&gt; (in daemon mode):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker attach katas-container&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once you're done, remove the &lt;code&gt;katas-container&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker rm --force katas-container&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-contributing-" class="anchor" aria-hidden="true" href="#contributing-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing &lt;a name="user-content-contributing"&gt;&lt;/a&gt;&lt;/h1&gt;&lt;a name="user-content-contributing"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-contributing"&gt;This project welcomes contributions and suggestions.  See &lt;/a&gt;&lt;a href=".github/CONTRIBUTING.md"&gt;How Can I Contribute?&lt;/a&gt; for details.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-code-of-conduct-" class="anchor" aria-hidden="true" href="#code-of-conduct-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code of Conduct &lt;a name="user-content-code-of-conduct"&gt;&lt;/a&gt;&lt;/h1&gt;&lt;a name="user-content-code-of-conduct"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-code-of-conduct"&gt;This project has adopted the &lt;/a&gt;&lt;a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;.
For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow"&gt;Code of Conduct FAQ&lt;/a&gt; or
contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>microsoft</author><guid isPermaLink="false">https://github.com/microsoft/QuantumKatas</guid><pubDate>Tue, 19 Nov 2019 00:14:00 GMT</pubDate></item><item><title>ultralytics/yolov3 #15 in Jupyter Notebook, This month</title><link>https://github.com/ultralytics/yolov3</link><description>&lt;p&gt;&lt;i&gt;YOLOv3 in PyTorch &gt; ONNX &gt; CoreML &gt; iOS&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591130-f7beea00-abc2-11e9-9dc0-d6abcf41d713.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591130-f7beea00-abc2-11e9-9dc0-d6abcf41d713.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
    &lt;td align="center"&gt;
    &lt;a href="https://www.ultralytics.com" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/c7f01c9051691f7f4c6239349b6b55cb5a0871c9/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f756c7472616c79746963732f6c6f676f2f6c6f676f6e616d65313030302e706e67" width="160" data-canonical-src="https://storage.googleapis.com/ultralytics/logo/logoname1000.png" style="max-width:100%;"&gt;&lt;/a&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591093-2b4d4480-abc2-11e9-8b46-d88eb1dabba1.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591093-2b4d4480-abc2-11e9-8b46-d88eb1dabba1.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
          &lt;a href="https://itunes.apple.com/app/id1452689527" rel="nofollow"&gt;
    &lt;img src="https://user-images.githubusercontent.com/26833433/50044365-9b22ac00-0082-11e9-862f-e77aee7aa7b0.png" width="180" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591100-55066b80-abc2-11e9-9647-52c0e045b288.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591100-55066b80-abc2-11e9-9647-52c0e045b288.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC, and &lt;strong&gt;is freely available for redistribution under the GPL-3.0 license&lt;/strong&gt;. For more information please visit &lt;a href="https://www.ultralytics.com" rel="nofollow"&gt;https://www.ultralytics.com&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description&lt;/h1&gt;
&lt;p&gt;The &lt;a href="https://github.com/ultralytics/yolov3"&gt;https://github.com/ultralytics/yolov3&lt;/a&gt; repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux, MacOS and Windows. Training is done on the COCO dataset by default: &lt;a href="https://cocodataset.org/#home" rel="nofollow"&gt;https://cocodataset.org/#home&lt;/a&gt;. &lt;strong&gt;Credit to Joseph Redmon for YOLO:&lt;/strong&gt; &lt;a href="https://pjreddie.com/darknet/yolo/" rel="nofollow"&gt;https://pjreddie.com/darknet/yolo/&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h1&gt;
&lt;p&gt;Python 3.7 or later with the following &lt;code&gt;pip3 install -U -r requirements.txt&lt;/code&gt; packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;numpy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;torch &amp;gt;= 1.1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;opencv-python&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tqdm&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart"&gt;GCP Quickstart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning"&gt;Transfer Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image"&gt;Train Single Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class"&gt;Train Single Class&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data"&gt;Train Custom Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-jupyter-notebook" class="anchor" aria-hidden="true" href="#jupyter-notebook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebook&lt;/h1&gt;
&lt;p&gt;Our Jupyter &lt;a href="https://colab.research.google.com/github/ultralytics/yolov3/blob/master/examples.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt; provides quick training, inference and testing examples.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Start Training:&lt;/strong&gt; &lt;code&gt;python3 train.py&lt;/code&gt; to begin training after downloading COCO data with &lt;code&gt;data/get_coco_dataset.sh&lt;/code&gt;. Each epoch trains on 117,263 images from the train and validate COCO sets, and tests on 5000 images from the COCO validate set.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resume Training:&lt;/strong&gt; &lt;code&gt;python3 train.py --resume&lt;/code&gt; to resume training from &lt;code&gt;weights/last.pt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Plot Training:&lt;/strong&gt; &lt;code&gt;from utils import utils; utils.plot_results()&lt;/code&gt; plots training results from &lt;code&gt;coco_16img.data&lt;/code&gt;, &lt;code&gt;coco_64img.data&lt;/code&gt;, 2 example datasets available in the &lt;code&gt;data/&lt;/code&gt; folder, which train and test on the first 16 and 64 images of the COCO2014-trainval dataset.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/63258271-fe9d5300-c27b-11e9-9a15-95038daf4438.png"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/63258271-fe9d5300-c27b-11e9-9a15-95038daf4438.png" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-image-augmentation" class="anchor" aria-hidden="true" href="#image-augmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Augmentation&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;datasets.py&lt;/code&gt; applies random OpenCV-powered (&lt;a href="https://opencv.org/" rel="nofollow"&gt;https://opencv.org/&lt;/a&gt;) augmentation to the input images in accordance with the following specifications. Augmentation is applied &lt;strong&gt;only&lt;/strong&gt; during training, not during inference. Bounding boxes are automatically tracked and updated with the images. 416 x 416 examples pictured below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Augmentation&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Translation&lt;/td&gt;
&lt;td&gt;+/- 10% (vertical and horizontal)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Rotation&lt;/td&gt;
&lt;td&gt;+/- 5 degrees&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Shear&lt;/td&gt;
&lt;td&gt;+/- 2 degrees (vertical and horizontal)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scale&lt;/td&gt;
&lt;td&gt;+/- 10%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reflection&lt;/td&gt;
&lt;td&gt;50% probability (horizontal-only)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;H&lt;strong&gt;S&lt;/strong&gt;V Saturation&lt;/td&gt;
&lt;td&gt;+/- 50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HS&lt;strong&gt;V&lt;/strong&gt; Intensity&lt;/td&gt;
&lt;td&gt;+/- 50%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/66699231-27beea80-ece5-11e9-9cad-bdf9d82c500a.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/66699231-27beea80-ece5-11e9-9cad-bdf9d82c500a.jpg" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-speed" class="anchor" aria-hidden="true" href="#speed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/deep-learning-vm/" rel="nofollow"&gt;https://cloud.google.com/deep-learning-vm/&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Machine type:&lt;/strong&gt; n1-standard-8 (8 vCPUs, 30 GB memory)&lt;br&gt;
&lt;strong&gt;CPU platform:&lt;/strong&gt; Intel Skylake&lt;br&gt;
&lt;strong&gt;GPUs:&lt;/strong&gt; K80 ($0.20/hr), T4 ($0.35/hr), V100 ($0.83/hr) CUDA with &lt;a href="https://github.com/NVIDIA/apex"&gt;Nvidia Apex&lt;/a&gt; FP16/32&lt;br&gt;
&lt;strong&gt;HDD:&lt;/strong&gt; 100 GB SSD&lt;br&gt;
&lt;strong&gt;Dataset:&lt;/strong&gt; COCO train 2014 (117,263 images)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;GPUs&lt;/th&gt;
&lt;th&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;images/sec&lt;/th&gt;
&lt;th&gt;epoch time&lt;/th&gt;
&lt;th&gt;epoch cost&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;K80&lt;/td&gt;
&lt;td&gt;64 (32x2)&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;175 min&lt;/td&gt;
&lt;td&gt;$0.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T4&lt;/td&gt;
&lt;td&gt;64 (32x2)&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;49 min&lt;/td&gt;
&lt;td&gt;$0.29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T4 x2&lt;/td&gt;
&lt;td&gt;64 (64x1)&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;td&gt;32 min&lt;/td&gt;
&lt;td&gt;$0.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;V100&lt;/td&gt;
&lt;td&gt;64 (32x2)&lt;/td&gt;
&lt;td&gt;115&lt;/td&gt;
&lt;td&gt;17 min&lt;/td&gt;
&lt;td&gt;$0.24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;V100 x2&lt;/td&gt;
&lt;td&gt;64 (64x1)&lt;/td&gt;
&lt;td&gt;150&lt;/td&gt;
&lt;td&gt;13 min&lt;/td&gt;
&lt;td&gt;$0.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2080Ti&lt;/td&gt;
&lt;td&gt;64 (32x2)&lt;/td&gt;
&lt;td&gt;81&lt;/td&gt;
&lt;td&gt;24 min&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2080Ti x2&lt;/td&gt;
&lt;td&gt;64 (64x1)&lt;/td&gt;
&lt;td&gt;140&lt;/td&gt;
&lt;td&gt;14 min&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-inference" class="anchor" aria-hidden="true" href="#inference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inference&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;detect.py&lt;/code&gt; runs inference on any sources:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 detect.py --source ...&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Image:  &lt;code&gt;--source file.jpg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Video:  &lt;code&gt;--source file.mp4&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Directory:  &lt;code&gt;--source dir/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Webcam:  &lt;code&gt;--source 0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;RTSP stream:  &lt;code&gt;--source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;HTTP stream:  &lt;code&gt;--source http://wmccpinetop.axiscam.net/mjpg/video.mjpg&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To run a specific models:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3.cfg --weights weights/yolov3.weights&lt;/code&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067835-51d5b500-cc2f-11e9-982e-843f7f9a6ea2.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067835-51d5b500-cc2f-11e9-982e-843f7f9a6ea2.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3-tiny:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3-tiny.cfg --weights weights/yolov3-tiny.weights&lt;/code&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067834-51d5b500-cc2f-11e9-9357-c485b159a20b.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067834-51d5b500-cc2f-11e9-9357-c485b159a20b.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3-SPP:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3-spp.cfg --weights weights/yolov3-spp.weights&lt;/code&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067833-51d5b500-cc2f-11e9-8208-6fe197809131.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067833-51d5b500-cc2f-11e9-8208-6fe197809131.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pretrained-weights" class="anchor" aria-hidden="true" href="#pretrained-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained Weights&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Darknet &lt;code&gt;*.weights&lt;/code&gt; format: &lt;a href="https://pjreddie.com/media/files/yolov3.weights" rel="nofollow"&gt;https://pjreddie.com/media/files/yolov3.weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PyTorch &lt;code&gt;*.pt&lt;/code&gt; format: &lt;a href="https://drive.google.com/drive/folders/1uxgUBemJVw9wZsdpboYbzUN4bcRhsuAI" rel="nofollow"&gt;https://drive.google.com/drive/folders/1uxgUBemJVw9wZsdpboYbzUN4bcRhsuAI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-darknet-conversion" class="anchor" aria-hidden="true" href="#darknet-conversion"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Darknet Conversion&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/ultralytics/yolov3 &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="pl-c1"&gt;cd&lt;/span&gt; yolov3

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; convert darknet cfg/weights to pytorch model&lt;/span&gt;
$ python3  -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;from models import *; convert('cfg/yolov3-spp.cfg', 'weights/yolov3-spp.weights')&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
Success: converted &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3-spp.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; to &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;converted.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; convert cfg/pytorch model to darknet weights&lt;/span&gt;
$ python3  -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;from models import *; convert('cfg/yolov3-spp.cfg', 'weights/yolov3-spp.pt')&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
Success: converted &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3-spp.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; to &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;converted.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-map" class="anchor" aria-hidden="true" href="#map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;mAP&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;test.py --weights weights/yolov3.weights&lt;/code&gt; tests official YOLOv3 weights.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;test.py --weights weights/last.pt&lt;/code&gt; tests most recent checkpoint.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;test.py --weights weights/best.pt&lt;/code&gt; tests best checkpoint.&lt;/li&gt;
&lt;li&gt;Compare to darknet published results &lt;a href="https://arxiv.org/abs/1804.02767" rel="nofollow"&gt;https://arxiv.org/abs/1804.02767&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/ultralytics/yolov3"&gt;ultralytics/yolov3&lt;/a&gt; mAP@0.5 (&lt;a href="https://arxiv.org/abs/1804.02767" rel="nofollow"&gt;darknet&lt;/a&gt;-reported mAP@0.5)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;i&gt;&lt;/i&gt;&lt;/th&gt;
&lt;th&gt;320&lt;/th&gt;
&lt;th&gt;416&lt;/th&gt;
&lt;th&gt;608&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;YOLOv3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;51.8 (51.5)&lt;/td&gt;
&lt;td&gt;55.4 (55.3)&lt;/td&gt;
&lt;td&gt;58.2 (57.9)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;YOLOv3-SPP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;53.7&lt;/td&gt;
&lt;td&gt;57.7&lt;/td&gt;
&lt;td&gt;60.7 (60.6)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;YOLOv3-tiny&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;29.0&lt;/td&gt;
&lt;td&gt;32.9 (33.1)&lt;/td&gt;
&lt;td&gt;35.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python3 test.py --save-json --img-size 608
Namespace(batch_size=16, cfg=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;cfg/yolov3-spp.cfg&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, conf_thres=0.001, data=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;data/coco.data&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, img_size=608, iou_thres=0.5, nms_thres=0.5, save_json=True, weights=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3-spp.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
Using CUDA device0 _CudaDeviceProperties(name=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Tesla T4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, total_memory=15079MB)
                Class    Images   Targets         P         R       mAP        F1: 100% 313/313 [07:&lt;span class="pl-k"&gt;40&amp;lt;&lt;/span&gt;00:00,  2.34s/it]
                  all     5e+03  3.58e+04     0.119     0.788     0.594     0.201
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.367 &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;---
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.607 &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;---
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.75      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.387
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.208
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.392
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.487
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;  1 ] = 0.297
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt; 10 ] = 0.465
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.495
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.332
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.518
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.621

$ python3 test.py --save-json --img-size 416
Namespace(batch_size=16, cfg=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;cfg/yolov3-spp.cfg&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, conf_thres=0.001, data=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;data/coco.data&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, img_size=416, iou_thres=0.5, nms_thres=0.5, save_json=True, weights=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3s-ultralytics.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
Using CUDA device0 _CudaDeviceProperties(name=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Tesla T4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, total_memory=15079MB)
                Class    Images   Targets         P         R       mAP        F1: 100% 313/313 [07:&lt;span class="pl-k"&gt;01&amp;lt;&lt;/span&gt;00:00,  1.41s/it]
                  all     5e+03  3.58e+04      0.11     0.739     0.569     0.185
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.373
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.577
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.75      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.392
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.175
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.403
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.537
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;  1 ] = 0.313
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt; 10 ] = 0.482
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.501
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.266
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.541
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.693&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://zenodo.org/badge/latestdoi/146165888" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cd760a8900fd4be0105229509d566b7c9499ef8d/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3134363136353838382e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/146165888.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h1&gt;
&lt;p&gt;Issues should be raised directly in the repository. For additional questions or comments please email Glenn Jocher at &lt;a href="mailto:glenn.jocher@ultralytics.com"&gt;glenn.jocher@ultralytics.com&lt;/a&gt; or visit us at &lt;a href="https://contact.ultralytics.com" rel="nofollow"&gt;https://contact.ultralytics.com&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ultralytics</author><guid isPermaLink="false">https://github.com/ultralytics/yolov3</guid><pubDate>Tue, 19 Nov 2019 00:15:00 GMT</pubDate></item><item><title>selfteaching/the-craft-of-selfteaching #16 in Jupyter Notebook, This month</title><link>https://github.com/selfteaching/the-craft-of-selfteaching</link><description>&lt;p&gt;&lt;i&gt;One has no future if one couldn't teach themself.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-the-craft-of-selfteaching" class="anchor" aria-hidden="true" href="#the-craft-of-selfteaching"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;the-craft-of-selfteaching&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;One has no future if one couldn't teach themself&lt;a href="#fn1" name="user-content-fn1b"&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-自学是门手艺" class="anchor" aria-hidden="true" href="#自学是门手艺"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;自学是门手艺&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;没有自学能力的人没有未来&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;作者：李笑来&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;特别感谢&lt;strong&gt;霍炬&lt;/strong&gt;（&lt;a href="https://github.com/virushuo"&gt;@virushuo&lt;/a&gt;）、&lt;strong&gt;洪强宁&lt;/strong&gt;（&lt;a href="https://github.com/hongqn"&gt;@hongqn&lt;/a&gt;) 两位良师诤友在此书写作过程中给予我的巨大帮助！&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pseudo-code of selfteaching in Python&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;teach_yourself&lt;/span&gt;(&lt;span class="pl-smi"&gt;anything&lt;/span&gt;):
    &lt;span class="pl-k"&gt;while&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; create():
        learn()
        practice()
    &lt;span class="pl-k"&gt;return&lt;/span&gt; teach_yourself(another)

teach_yourself(coding)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;请先行阅读 &lt;a href="T-appendix.jupyter-installation-and-setup.ipynb"&gt;T-appendix.jupyter-installation-and-setup&lt;/a&gt; 以便在本地安装 &lt;a href="https://github.com/jupyterlab/jupyterlab"&gt;Jupyterlab&lt;/a&gt; 而后就能用更好的体验阅读本书。&lt;/p&gt;
&lt;p&gt;有兴趣帮忙的朋友，请先行阅读 &lt;a href="02.proof-of-work.ipynb"&gt;如何使用 Pull Request 为这本书校对&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;2019 年 3 月 23 日，新增 Markdown 版本：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/selfteaching/the-craft-of-selfteaching/tree/master/markdown"&gt;https://github.com/selfteaching/the-craft-of-selfteaching/tree/master/markdown&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h3&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="01.preface.ipynb"&gt;01.preface（&lt;strong&gt;前言&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="02.proof-of-work.ipynb"&gt;02.proof-of-work（&lt;strong&gt;如何证明你真的读过这本书？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.A.better.teachyourself.ipynb"&gt;Part.1.A.better.teachyourself（&lt;strong&gt;为什么一定要掌握自学能力？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.B.why.start.from.learning.coding.ipynb"&gt;Part.1.B.why.start.from.learning.coding（&lt;strong&gt;为什么把编程当作自学的入口？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.C.must.learn.sth.only.by.reading.ipynb"&gt;Part.1.C.must.learn.sth.only.by.reading（&lt;strong&gt;只靠阅读习得新技能&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.D.preparation.for.reading.ipynb"&gt;Part.1.D.preparation.for.reading（&lt;strong&gt;开始阅读前的一些准备&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.1.entrance.ipynb"&gt;Part.1.E.1.entrance（&lt;strong&gt;入口&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.2.values-and-their-operators.ipynb"&gt;Part.1.E.2.values-and-their-operators（&lt;strong&gt;值及其相应的运算&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.3.controlflow.ipynb"&gt;Part.1.E.3.controlflow（&lt;strong&gt;流程控制&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.4.functions.ipynb"&gt;Part.1.E.4.functions（&lt;strong&gt;函数&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.5.strings.ipynb"&gt;Part.1.E.5.strings（&lt;strong&gt;字符串&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.6.containers.ipynb"&gt;Part.1.E.6.containers（&lt;strong&gt;数据容器&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.E.7.files.ipynb"&gt;Part.1.E.7.files（&lt;strong&gt;文件&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.F.deal-with-forward-references.ipynb"&gt;Part.1.F.deal-with-forward-references（&lt;strong&gt;如何从容应对含有过多 “过早引用” 的知识？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.1.G.The-Python-Tutorial-local.ipynb"&gt;Part.1.G.The-Python-Tutorial-local（&lt;strong&gt;官方教程：The Python Tutorial&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.A.clumsy-and-patience.ipynb"&gt;Part.2.A.clumsy-and-patience（&lt;strong&gt;笨拙与耐心&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.B.deliberate-practicing.ipynb"&gt;Part.2.B.deliberate-practicing（&lt;strong&gt;刻意练习&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.C.why-start-from-writing-functions.ipynb"&gt;Part.2.C.why-start-from-writing-functions（&lt;strong&gt;为什么从函数开始？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.1-args.ipynb"&gt;Part.2.D.1-args（&lt;strong&gt;关于参数（上）&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.2-aargs.ipynb"&gt;Part.2.D.2-aargs（&lt;strong&gt;关于参数（下）&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.3-lambda.ipynb"&gt;Part.2.D.3-lambda（&lt;strong&gt;化名与匿名&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.4-recursion.ipynb"&gt;Part.2.D.4-recursion（&lt;strong&gt;递归函数&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.5-docstrings.ipynb"&gt;Part.2.D.5-docstrings（&lt;strong&gt;函数的文档&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.6-modules.ipynb"&gt;Part.2.D.6-modules（&lt;strong&gt;保存到文件的函数&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.7-tdd.ipynb"&gt;Part.2.D.7-tdd（&lt;strong&gt;测试驱动的开发&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.D.8-main.ipynb"&gt;Part.2.D.8-main（&lt;strong&gt;可执行的 Python 文件&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.2.E.deliberate-thinking.ipynb"&gt;Part.2.E.deliberate-thinking（&lt;strong&gt;刻意思考&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.A.conquering-difficulties.ipynb"&gt;Part.3.A.conquering-difficulties（&lt;strong&gt;战胜难点&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.B.1.classes-1.ipynb"&gt;Part.3.B.1.classes-1（&lt;strong&gt;类 —— 面向对象编程&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.B.2.classes-2.ipynb"&gt;Part.3.B.2.classes-2（&lt;strong&gt;类 —— Python 的实现&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.B.3.decorator-iterator-generator.ipynb"&gt;Part.3.B.3.decorator-iterator-generator（&lt;strong&gt;函数工具&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.B.4.regex.ipynb"&gt;Part.3.B.4.regex（&lt;strong&gt;正则表达式&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.B.5.bnf-ebnf-pebnf.ipynb"&gt;Part.3.B.5.bnf-ebnf-pebnf（&lt;strong&gt;BNF 以及 EBNF&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.C.breaking-good-and-bad.ipynb"&gt;Part.3.C.breaking-good-and-bad（&lt;strong&gt;拆解&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.D.indispensable-illusion.ipynb"&gt;Part.3.D.indispensable-illusion（&lt;strong&gt;刚需幻觉&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.E.to-be-thorough.ipynb"&gt;Part.3.E.to-be-thorough（&lt;strong&gt;全面 —— 自学的境界&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.F.social-selfteaching.ipynb"&gt;Part.3.F.social-selfteaching（&lt;strong&gt;自学者的社交&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.G.the-golden-age-and-google.ipynb"&gt;Part.3.G.the-golden-age-and-google（&lt;strong&gt;这是自学者的黄金时代&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Part.3.H.prevent-focus-drifting.ipynb"&gt;Part.3.H.prevent-focus-drifting（&lt;strong&gt;避免注意力漂移&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Q.good-communiation.ipynb"&gt;Q.good-communiation（&lt;strong&gt;如何成为优秀沟通者&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="R.finale.ipynb"&gt;R.finale（&lt;strong&gt;自学者的终点&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="S.whats-next.ipynb"&gt;S.whats-next（&lt;strong&gt;下一步干什么？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="T-appendix.editor.vscode.ipynb"&gt;T-appendix.editor.vscode（&lt;strong&gt;Visual Studio Code 的安装与配置&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="T-appendix.git-introduction.ipynb"&gt;T-appendix.git-introduction（&lt;strong&gt;Git 简介&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="T-appendix.jupyter-installation-and-setup.ipynb"&gt;T-appendix.jupyter-installation-and-setup（&lt;strong&gt;Jupyterlab 的安装与配置&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="T-appendix.symbols.ipynb"&gt;T-appendix.symbols（&lt;strong&gt;这些符号都代表什么？&lt;/strong&gt;）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;本书的版权协议为 &lt;a href="https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" rel="nofollow"&gt;CC-BY-NC-ND license&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/CC-BY-NC-ND.png?raw=true"&gt;&lt;img src="images/CC-BY-NC-ND.png?raw=true" alt="CC-BY-NC-ND" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;脚注&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name="user-content-fn1"&gt;[1]&lt;/a&gt;：&lt;a href="https://en.oxforddictionaries.com/usage/themselves-or-themself" rel="nofollow"&gt;'Themselves' or 'themself'? -- Oxford Dictionary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="#fn1b"&gt;↑Back to Content↑&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>selfteaching</author><guid isPermaLink="false">https://github.com/selfteaching/the-craft-of-selfteaching</guid><pubDate>Tue, 19 Nov 2019 00:16:00 GMT</pubDate></item><item><title>MorvanZhou/PyTorch-Tutorial #17 in Jupyter Notebook, This month</title><link>https://github.com/MorvanZhou/PyTorch-Tutorial</link><description>&lt;p&gt;&lt;i&gt;Build your neural network easy and fast&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;a href="http://pytorch.org/" rel="nofollow"&gt;
    &lt;img width="40%" src="https://github.com/MorvanZhou/PyTorch-Tutorial/raw/master/logo.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;&lt;a id="user-content-if-youd-like-to-use-tensorflow-no-worries-i-made-a-new-tensorflow-tutorial-just-like-pytorch-here-is-the-link-httpsgithubcommorvanzhoutensorflow-tutorial" class="anchor" aria-hidden="true" href="#if-youd-like-to-use-tensorflow-no-worries-i-made-a-new-tensorflow-tutorial-just-like-pytorch-here-is-the-link-httpsgithubcommorvanzhoutensorflow-tutorial"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;If you'd like to use &lt;strong&gt;Tensorflow&lt;/strong&gt;, no worries, I made a new &lt;strong&gt;Tensorflow Tutorial&lt;/strong&gt; just like PyTorch. Here is the link: &lt;a href="https://github.com/MorvanZhou/Tensorflow-Tutorial"&gt;https://github.com/MorvanZhou/Tensorflow-Tutorial&lt;/a&gt;&lt;/h3&gt;
&lt;h1&gt;&lt;a id="user-content-pytorch-tutorials" class="anchor" aria-hidden="true" href="#pytorch-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pyTorch Tutorials&lt;/h1&gt;
&lt;p&gt;In these tutorials for pyTorch, we will build our first Neural Network and try to build some advanced Neural Network architectures developed recent years.&lt;/p&gt;
&lt;p&gt;Thanks for &lt;a href="https://github.com/liufuyang"&gt;liufuyang's&lt;/a&gt; &lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents-notebooks"&gt;&lt;strong&gt;notebook files&lt;/strong&gt;&lt;/a&gt;
which is a great contribution to this tutorial.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pyTorch basic
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/201_torch_numpy.py"&gt;torch and numpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/202_variable.py"&gt;Variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/203_activation.py"&gt;Activation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Build your first network
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py"&gt;Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/302_classification.py"&gt;Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/303_build_nn_quickly.py"&gt;An easy way&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/304_save_reload.py"&gt;Save and reload&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/305_batch_train.py"&gt;Train on batch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/306_optimizer.py"&gt;Optimizers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Advanced neural network
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/401_CNN.py"&gt;CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/402_RNN_classifier.py"&gt;RNN-Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py"&gt;RNN-Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/404_autoencoder.py"&gt;AutoEncoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/405_DQN_Reinforcement_learning.py"&gt;DQN Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/pytorch-A3C"&gt;A3C Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py"&gt;GAN (Generative Adversarial Nets)&lt;/a&gt; / &lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_conditional_GAN.py"&gt;Conditional GAN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Others (WIP)
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/501_why_torch_dynamic_graph.py"&gt;Why torch dynamic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/502_GPU.py"&gt;Train on GPU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/503_dropout.py"&gt;Dropout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py"&gt;Batch Normalization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;For Chinese speakers: All methods mentioned below have their video and text tutorial in Chinese.
Visit &lt;a href="https://morvanzhou.github.io/tutorials/" rel="nofollow"&gt;莫烦 Python&lt;/a&gt; for more.
You can watch my &lt;a href="https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg" rel="nofollow"&gt;Youtube channel&lt;/a&gt; as well.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-regression" class="anchor" aria-hidden="true" href="#regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py"&gt;Regression&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/7491264fba17ff7eb3ec5cce2e0f8db3e58e1c7b/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f312d312d322e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/1-1-2.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-classification" class="anchor" aria-hidden="true" href="#classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/302_classification.py"&gt;Classification&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/302_classification.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/03cf1c4921b6e80da7710c117164b27675351118/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f312d312d332e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/1-1-3.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-cnn" class="anchor" aria-hidden="true" href="#cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/401_CNN.py"&gt;CNN&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/401_CNN.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/338bdc4ea3e2ce897291d8fd5257546395f55a34/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f342d312d322e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/4-1-2.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-rnn" class="anchor" aria-hidden="true" href="#rnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py"&gt;RNN&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/cfb5a384896108d534d3c6c6fadf31f581715737/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f342d332d312e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/4-3-1.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-autoencoder" class="anchor" aria-hidden="true" href="#autoencoder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/404_autoencoder.py"&gt;Autoencoder&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/4c4f5a700b817b646bbed49dffdaa4a0925df8aa/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f342d342d312e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/4-4-1.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/f8f129f7808c6942b067e77b205a2fde2c604845/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f342d342d322e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/4-4-2.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-gan-generative-adversarial-nets" class="anchor" aria-hidden="true" href="#gan-generative-adversarial-nets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py"&gt;GAN (Generative Adversarial Nets)&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/34534d5eb5e962d2761224d6637471b74c06c972/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f342d362d312e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/4-6-1.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-dropout" class="anchor" aria-hidden="true" href="#dropout"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/503_dropout.py"&gt;Dropout&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/503_dropout.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/304540032f3f2ed20f127f22e90ad23005088a37/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f352d332d312e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/5-3-1.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-batch-normalization" class="anchor" aria-hidden="true" href="#batch-normalization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py"&gt;Batch Normalization&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py"&gt;
    &lt;img src="https://camo.githubusercontent.com/944f05a2565de729ae03d76bb2cb6f9c48e2e552/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f726573756c74732f746f7263682f352d342d322e676966" data-canonical-src="https://morvanzhou.github.io/static/results/torch/5-4-2.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;h1&gt;&lt;a id="user-content-donation" class="anchor" aria-hidden="true" href="#donation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Donation&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;If this does help you, please consider donating to support me for better tutorials. Any contribution is greatly appreciated!&lt;/em&gt;&lt;/p&gt;
&lt;div&gt;
  &lt;a href="https://www.paypal.com/cgi-bin/webscr?cmd=_donations&amp;amp;business=morvanzhou%40gmail%2ecom&amp;amp;lc=C2&amp;amp;item_name=MorvanPython&amp;amp;currency_code=AUD&amp;amp;bn=PP%2dDonationsBF%3abtn_donateCC_LG%2egif%3aNonHosted" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/f75f395714327046e04cbadbee71103f87fd0aa1/68747470733a2f2f7777772e70617970616c6f626a656374732e636f6d2f7765627374617469632f656e5f55532f692f62746e2f706e672f73696c7665722d70696c6c2d70617970616c2d343470782e706e67" alt="Paypal" height="auto" data-canonical-src="https://www.paypalobjects.com/webstatic/en_US/i/btn/png/silver-pill-paypal-44px.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div&gt;
  &lt;a href="https://www.patreon.com/morvan" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/af8133ce68a27bb72e44120b5ec14062fc42dc75/68747470733a2f2f6d6f7276616e7a686f752e6769746875622e696f2f7374617469632f696d672f737570706f72742f70617472656f6e2e6a7067" alt="Patreon" height="120" data-canonical-src="https://morvanzhou.github.io/static/img/support/patreon.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>MorvanZhou</author><guid isPermaLink="false">https://github.com/MorvanZhou/PyTorch-Tutorial</guid><pubDate>Tue, 19 Nov 2019 00:17:00 GMT</pubDate></item><item><title>dotnet/try #18 in Jupyter Notebook, This month</title><link>https://github.com/dotnet/try</link><description>&lt;p&gt;&lt;i&gt;Try .NET provides developers and content authors with tools to create interactive experiences.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-try-net-" class="anchor" aria-hidden="true" href="#try-net-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Try .NET &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/2546640/56708992-deee8780-66ec-11e9-9991-eb85abb1d10a.png"&gt;&lt;img src="https://user-images.githubusercontent.com/2546640/56708992-deee8780-66ec-11e9-9991-eb85abb1d10a.png" width="80px" alt="dotnet bot in space" align="right" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;|| &lt;a href="#contribution-guidelines"&gt;&lt;strong&gt;Contribution Guidelines&lt;/strong&gt;&lt;/a&gt; || &lt;a href="#table-of-contents"&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/a&gt; || &lt;a href="#customers--partners"&gt;&lt;strong&gt;Customers &amp;amp; Partners&lt;/strong&gt;&lt;/a&gt; ||&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/845bd5c6c705e84b68997ab32b7a4437ed366fb7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5472795f2e4e45542d456e61626c65642d3530313037382e737667"&gt;&lt;img src="https://camo.githubusercontent.com/845bd5c6c705e84b68997ab32b7a4437ed366fb7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5472795f2e4e45542d456e61626c65642d3530313037382e737667" alt="Try_.NET Enabled" data-canonical-src="https://img.shields.io/badge/Try_.NET-Enabled-501078.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://dev.azure.com/dnceng/public/_build/latest?definitionId=495&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/93b7ca6217cf06f12405c38f80e27b9578f0ee18/68747470733a2f2f6465762e617a7572652e636f6d2f646e63656e672f7075626c69632f5f617069732f6275696c642f7374617475732f646f746e65742f7472792f7472792d63693f6272616e63684e616d653d6d6173746572" alt="Build Status" data-canonical-src="https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/try/try-ci?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-welcome-to-the-try-net-repo" class="anchor" aria-hidden="true" href="#welcome-to-the-try-net-repo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Welcome to the Try .NET repo.&lt;/h2&gt;
&lt;p&gt;Try .NET provides developers and content authors with tools to create interactive experiences.&lt;/p&gt;
&lt;p&gt;There are different flavors of  Try .NET interactive experiences, from the web experience powered by Blazor (as seen on &lt;a href="https://docs.microsoft.com/en-us/dotnet/csharp/tutorials/intro-to-csharp/hello-world" rel="nofollow"&gt;Microsoft docs&lt;/a&gt;), to interactive documentation for .NET Core with the &lt;a href="DotNetTry.md"&gt;dotnet try global tool&lt;/a&gt;, to &lt;a href="Notebook.md"&gt;.NET Jupyter Notebooks&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-flavors-of-try-net" class="anchor" aria-hidden="true" href="#flavors-of-try-net"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Flavors of Try .NET&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-try-net-online" class="anchor" aria-hidden="true" href="#try-net-online"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Try .NET Online&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/2546640/57144765-c850cc00-6d8f-11e9-982d-50d2b6dc3591.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/2546640/57144765-c850cc00-6d8f-11e9-982d-50d2b6dc3591.gif" width="60%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-interactive-net-core-documentation-with-the-dotnet-try-global-tool" class="anchor" aria-hidden="true" href="#interactive-net-core-documentation-with-the-dotnet-try-global-tool"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Interactive .NET Core documentation with the &lt;code&gt;dotnet try&lt;/code&gt; global tool&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/2546640/57158389-47a2c780-6db1-11e9-96ad-8c6e9ab52853.png"&gt;&lt;img src="https://user-images.githubusercontent.com/2546640/57158389-47a2c780-6db1-11e9-96ad-8c6e9ab52853.png" width="52%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-jupyter-notebooks-with-net" class="anchor" aria-hidden="true" href="#jupyter-notebooks-with-net"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebooks with .NET&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/2546640/67889988-3b13e780-fb26-11e9-91a1-48d5972b5df2.png"&gt;&lt;img src="https://user-images.githubusercontent.com/2546640/67889988-3b13e780-fb26-11e9-91a1-48d5972b5df2.png" width="60%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/2546640/67912370-1b99b080-fb60-11e9-9839-0058d02488cf.png"&gt;&lt;img src="https://user-images.githubusercontent.com/2546640/67912370-1b99b080-fb60-11e9-9839-0058d02488cf.png" width="52%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="DotNetTry.md"&gt;Getting Started with Try .NET&lt;/a&gt;: Interactive documentation generator for .NET Core.&lt;/li&gt;
&lt;li&gt;&lt;a href="Notebook.md"&gt;Getting started with .NET Jupyter Notebooks&lt;/a&gt; : Write and run .NET code in Jupyter Notebooks&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-guidelines" class="anchor" aria-hidden="true" href="#contribution-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution Guidelines&lt;/h2&gt;
&lt;p&gt;As we are still in the early stages of our development, we are unable to take any feature PRs at the moment, but we intend to do so in the future.
If you find an issue or have a feature suggestion, please open an &lt;a href="https://github.com/dotnet/try/issues/new/choose"&gt;issue&lt;/a&gt;. And if you have any feature suggestions, please submit them using the "community suggestions" label.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-customers--partners" class="anchor" aria-hidden="true" href="#customers--partners"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Customers &amp;amp; Partners&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Microsoft Docs&lt;/th&gt;
&lt;th align="center"&gt;&lt;a href="https://azure.microsoft.com/en-us/services/synapse-analytics/" rel="nofollow"&gt;Azure Synapse Analytics &lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;Azure HDInsight (HDI)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Microsoft Docs uses &lt;a href="https://docs.microsoft.com/en-us/dotnet/csharp/tutorials/intro-to-csharp/hello-world" rel="nofollow"&gt;Try .NET&lt;/a&gt; to create interactive documentation. Users can run and edit .NET code in the browser.&lt;/td&gt;
&lt;td align="center"&gt;Azure Synapse Analytics uses the .NET kernel to write and run quick ad-hoc queries in addition to developing complete, end-to-end big data scenarios, such as reading in data, transforming it, and visualizing it&lt;/td&gt;
&lt;td align="center"&gt;You can launch Jupyter notebooks from your HDInsight cluster to run big data queries against the compute resources in that cluster.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dotnet</author><guid isPermaLink="false">https://github.com/dotnet/try</guid><pubDate>Tue, 19 Nov 2019 00:18:00 GMT</pubDate></item><item><title>NVIDIA/tacotron2 #19 in Jupyter Notebook, This month</title><link>https://github.com/NVIDIA/tacotron2</link><description>&lt;p&gt;&lt;i&gt;Tacotron 2 - PyTorch implementation with faster-than-realtime inference&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tacotron-2-without-wavenet" class="anchor" aria-hidden="true" href="#tacotron-2-without-wavenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tacotron 2 (without wavenet)&lt;/h1&gt;
&lt;p&gt;PyTorch implementation of &lt;a href="https://arxiv.org/pdf/1712.05884.pdf" rel="nofollow"&gt;Natural TTS Synthesis By Conditioning
Wavenet On Mel Spectrogram Predictions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This implementation includes &lt;strong&gt;distributed&lt;/strong&gt; and &lt;strong&gt;automatic mixed precision&lt;/strong&gt; support
and uses the &lt;a href="https://keithito.com/LJ-Speech-Dataset/" rel="nofollow"&gt;LJSpeech dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Distributed and Automatic Mixed Precision support relies on NVIDIA's &lt;a href="https://github.com/nvidia/apex"&gt;Apex&lt;/a&gt; and &lt;a href="https://github.com/NVIDIA/apex/tree/master/apex/amp"&gt;AMP&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Visit our &lt;a href="https://nv-adlr.github.io/WaveGlow" rel="nofollow"&gt;website&lt;/a&gt; for audio samples using our published &lt;a href="https://drive.google.com/file/d/1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA/view?usp=sharing" rel="nofollow"&gt;Tacotron 2&lt;/a&gt; and
&lt;a href="https://drive.google.com/file/d/1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx/view?usp=sharing" rel="nofollow"&gt;WaveGlow&lt;/a&gt; models.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="tensorboard.png"&gt;&lt;img src="tensorboard.png" alt="Alignment, Predicted Mel Spectrogram, Target Mel Spectrogram" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-requisites" class="anchor" aria-hidden="true" href="#pre-requisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-requisites&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;NVIDIA GPU + CUDA cuDNN&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download and extract the &lt;a href="https://keithito.com/LJ-Speech-Dataset/" rel="nofollow"&gt;LJ Speech dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Clone this repo: &lt;code&gt;git clone https://github.com/NVIDIA/tacotron2.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;CD into this repo: &lt;code&gt;cd tacotron2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Initialize submodule: &lt;code&gt;git submodule init; git submodule update&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Update .wav paths: &lt;code&gt;sed -i -- 's,DUMMY,ljs_dataset_folder/wavs,g' filelists/*.txt&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Alternatively, set &lt;code&gt;load_mel_from_disk=True&lt;/code&gt; in &lt;code&gt;hparams.py&lt;/code&gt; and update mel-spectrogram paths&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Install &lt;a href="https://github.com/pytorch/pytorch#installation"&gt;PyTorch 1.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install &lt;a href="https://github.com/nvidia/apex"&gt;Apex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install python requirements or build docker image
&lt;ul&gt;
&lt;li&gt;Install python requirements: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;python train.py --output_directory=outdir --log_directory=logdir&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;(OPTIONAL) &lt;code&gt;tensorboard --logdir=outdir/logdir&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-training-using-a-pre-trained-model" class="anchor" aria-hidden="true" href="#training-using-a-pre-trained-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training using a pre-trained model&lt;/h2&gt;
&lt;p&gt;Training using a pre-trained model can lead to faster convergence&lt;br&gt;
By default, the dataset dependent text embedding layers are &lt;a href="https://github.com/NVIDIA/tacotron2/blob/master/hparams.py#L22"&gt;ignored&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download our published &lt;a href="https://drive.google.com/file/d/1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA/view?usp=sharing" rel="nofollow"&gt;Tacotron 2&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;&lt;code&gt;python train.py --output_directory=outdir --log_directory=logdir -c tacotron2_statedict.pt --warm_start&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-multi-gpu-distributed-and-automatic-mixed-precision-training" class="anchor" aria-hidden="true" href="#multi-gpu-distributed-and-automatic-mixed-precision-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-GPU (distributed) and Automatic Mixed Precision Training&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;python -m multiproc train.py --output_directory=outdir --log_directory=logdir --hparams=distributed_run=True,fp16_run=True&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-inference-demo" class="anchor" aria-hidden="true" href="#inference-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inference demo&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download our published &lt;a href="https://drive.google.com/file/d/1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA/view?usp=sharing" rel="nofollow"&gt;Tacotron 2&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;Download our published &lt;a href="https://drive.google.com/file/d/1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx/view?usp=sharing" rel="nofollow"&gt;WaveGlow&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;&lt;code&gt;jupyter notebook --ip=127.0.0.1 --port=31337&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Load inference.ipynb&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;N.b.  When performing Mel-Spectrogram to Audio synthesis, make sure Tacotron 2
and the Mel decoder were trained on the same mel-spectrogram representation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-related-repos" class="anchor" aria-hidden="true" href="#related-repos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related repos&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/WaveGlow"&gt;WaveGlow&lt;/a&gt; Faster than real time Flow-based
Generative Network for Speech Synthesis&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/nv-wavenet/"&gt;nv-wavenet&lt;/a&gt; Faster than real time
WaveNet.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This implementation uses code from the following repos: &lt;a href="https://github.com/keithito/tacotron/"&gt;Keith
Ito&lt;/a&gt;, &lt;a href="https://github.com/pseeth/pytorch-stft"&gt;Prem
Seetharaman&lt;/a&gt; as described in our code.&lt;/p&gt;
&lt;p&gt;We are inspired by &lt;a href="https://github.com/r9y9/tacotron_pytorch"&gt;Ryuchi Yamamoto's&lt;/a&gt;
Tacotron PyTorch implementation.&lt;/p&gt;
&lt;p&gt;We are thankful to the Tacotron 2 paper authors, specially Jonathan Shen, Yuxuan
Wang and Zongheng Yang.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA</author><guid isPermaLink="false">https://github.com/NVIDIA/tacotron2</guid><pubDate>Tue, 19 Nov 2019 00:19:00 GMT</pubDate></item><item><title>slundberg/shap #20 in Jupyter Notebook, This month</title><link>https://github.com/slundberg/shap</link><description>&lt;p&gt;&lt;i&gt;A unified approach to explain the output of any machine learning model.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_diagram.png"&gt;&lt;img src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_diagram.png" width="400" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/slundberg/shap" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/19de279a6f67f8eea3f52ccecc779c3e1aff55e7/68747470733a2f2f7472617669732d63692e6f72672f736c756e64626572672f736861702e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.org/slundberg/shap.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://mybinder.org/v2/gh/slundberg/shap/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHAP (SHapley Additive exPlanations)&lt;/strong&gt; is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our &lt;a href="#citations"&gt;papers&lt;/a&gt; for details and citations).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-install" class="anchor" aria-hidden="true" href="#install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install&lt;/h2&gt;
&lt;p&gt;SHAP can be installed from either &lt;a href="https://pypi.org/project/shap" rel="nofollow"&gt;PyPI&lt;/a&gt; or &lt;a href="https://anaconda.org/conda-forge/shap" rel="nofollow"&gt;conda-forge&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;pip install shap
&lt;i&gt;or&lt;/i&gt;
conda install -c conda-forge shap
&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-tree-ensemble-example-with-treeexplainer-xgboostlightgbmcatboostscikit-learnpyspark-models" class="anchor" aria-hidden="true" href="#tree-ensemble-example-with-treeexplainer-xgboostlightgbmcatboostscikit-learnpyspark-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tree ensemble example with TreeExplainer (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)&lt;/h2&gt;
&lt;p&gt;While SHAP values can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (&lt;a href="https://arxiv.org/abs/1802.03888" rel="nofollow"&gt;Tree SHAP arXiv paper&lt;/a&gt;). Fast C++ implementations are supported for &lt;em&gt;XGBoost&lt;/em&gt;, &lt;em&gt;LightGBM&lt;/em&gt;, &lt;em&gt;CatBoost&lt;/em&gt;, &lt;em&gt;scikit-learn&lt;/em&gt; and &lt;em&gt;pyspark&lt;/em&gt; tree models:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; xgboost
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load JS visualization code to notebook&lt;/span&gt;
shap.initjs()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train XGBoost model&lt;/span&gt;
X,y &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.boston()
model &lt;span class="pl-k"&gt;=&lt;/span&gt; xgboost.train({&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.01&lt;/span&gt;}, xgboost.DMatrix(X, &lt;span class="pl-v"&gt;label&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;y), &lt;span class="pl-c1"&gt;100&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain the model's predictions using SHAP values&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)&lt;/span&gt;
explainer &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.TreeExplainer(model)
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; explainer.shap_values(X)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)&lt;/span&gt;
shap.force_plot(explainer.expected_value, shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], X.iloc[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:])&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png"&gt;&lt;img width="811" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue (these force plots are introduced in our &lt;a href="https://www.nature.com/articles/s41551-018-0304-0" rel="nofollow"&gt;Nature BME paper&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; visualize the training set predictions&lt;/span&gt;
shap.force_plot(explainer.expected_value, shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png"&gt;&lt;img width="811" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;To understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature's responsibility for a change in the model output, the plot below represents the change in predicted house price as RM (the average number of rooms per house in an area) changes. Vertical dispersion at a single value of RM represents interaction effects with other features. To help reveal these interactions &lt;code&gt;dependence_plot&lt;/code&gt; automatically selects another feature for coloring. In this case coloring by RAD (index of accessibility to radial highways) highlights that the average number of rooms per house has less impact on home price for areas with a high RAD value.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; create a SHAP dependence plot to show the effect of a single feature across the whole dataset&lt;/span&gt;
shap.dependence_plot(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;RM&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dependence_plot.png"&gt;&lt;img width="544" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dependence_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;To get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that a high LSTAT (% lower status of the population) lowers the predicted home price.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; summarize the effects of all the features&lt;/span&gt;
shap.summary_plot(shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot.png"&gt;&lt;img width="483" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;shap.summary_plot(shap_values, X, &lt;span class="pl-v"&gt;plot_type&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;bar&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot_bar.png"&gt;&lt;img width="470" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot_bar.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deep-learning-example-with-deepexplainer-tensorflowkeras-models" class="anchor" aria-hidden="true" href="#deep-learning-example-with-deepexplainer-tensorflowkeras-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep learning example with DeepExplainer (TensorFlow/Keras models)&lt;/h2&gt;
&lt;p&gt;Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with &lt;a href="https://arxiv.org/abs/1704.02685" rel="nofollow"&gt;DeepLIFT&lt;/a&gt; described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ...include code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&lt;/span&gt;

&lt;span class="pl-k"&gt;import&lt;/span&gt; shap
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; select a set of background examples to take an expectation over&lt;/span&gt;
background &lt;span class="pl-k"&gt;=&lt;/span&gt; x_train[np.random.choice(x_train.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], &lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-v"&gt;replace&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain predictions of the model on four images&lt;/span&gt;
e &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.DeepExplainer(model, background)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ...or pass tensors directly&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)&lt;/span&gt;
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; e.shap_values(x_test[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;5&lt;/span&gt;])

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the feature attributions&lt;/span&gt;
shap.image_plot(shap_values, &lt;span class="pl-k"&gt;-&lt;/span&gt;x_test[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;5&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png"&gt;&lt;img width="820" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the 'zero' image the blank middle is important, while for the 'four' image the lack of a connection on top makes it a four instead of a nine.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models" class="anchor" aria-hidden="true" href="#deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)&lt;/h2&gt;
&lt;p&gt;Expected gradients combines ideas from &lt;a href="https://arxiv.org/abs/1703.01365" rel="nofollow"&gt;Integrated Gradients&lt;/a&gt;, SHAP, and &lt;a href="https://arxiv.org/abs/1706.03825" rel="nofollow"&gt;SmoothGrad&lt;/a&gt; into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; keras.applications.vgg16 &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-c1"&gt;VGG16&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; keras.applications.vgg16 &lt;span class="pl-k"&gt;import&lt;/span&gt; preprocess_input
&lt;span class="pl-k"&gt;import&lt;/span&gt; keras.backend &lt;span class="pl-k"&gt;as&lt;/span&gt; K
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;import&lt;/span&gt; json
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load pre-trained model and choose two images to explain&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; VGG16(&lt;span class="pl-v"&gt;weights&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;imagenet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;include_top&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
X,y &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.imagenet50()
to_explain &lt;span class="pl-k"&gt;=&lt;/span&gt; X[[&lt;span class="pl-c1"&gt;39&lt;/span&gt;,&lt;span class="pl-c1"&gt;41&lt;/span&gt;]]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load the ImageNet class names&lt;/span&gt;
url &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
fname &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.cache(url)
&lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(fname) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
    class_names &lt;span class="pl-k"&gt;=&lt;/span&gt; json.load(f)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain how the input to the 7th layer of the model explains the top two classes&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;map2layer&lt;/span&gt;(&lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;layer&lt;/span&gt;):
    feed_dict &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;dict&lt;/span&gt;(&lt;span class="pl-c1"&gt;zip&lt;/span&gt;([model.layers[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].input], [preprocess_input(x.copy())]))
    &lt;span class="pl-k"&gt;return&lt;/span&gt; K.get_session().run(model.layers[layer].input, feed_dict)
e &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.GradientExplainer(
    (model.layers[&lt;span class="pl-c1"&gt;7&lt;/span&gt;].input, model.layers[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;].output),
    map2layer(X, &lt;span class="pl-c1"&gt;7&lt;/span&gt;),
    &lt;span class="pl-v"&gt;local_smoothing&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; std dev of smoothing noise&lt;/span&gt;
)
shap_values,indexes &lt;span class="pl-k"&gt;=&lt;/span&gt; e.shap_values(map2layer(to_explain, &lt;span class="pl-c1"&gt;7&lt;/span&gt;), &lt;span class="pl-v"&gt;ranked_outputs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; get the names for the classes&lt;/span&gt;
index_names &lt;span class="pl-k"&gt;=&lt;/span&gt; np.vectorize(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: class_names[&lt;span class="pl-c1"&gt;str&lt;/span&gt;(x)][&lt;span class="pl-c1"&gt;1&lt;/span&gt;])(indexes)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the explanations&lt;/span&gt;
shap.image_plot(shap_values, to_explain, index_names)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png"&gt;&lt;img width="500" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Predictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using &lt;code&gt;ranked_outputs=2&lt;/code&gt; we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-agnostic-example-with-kernelexplainer-explains-any-function" class="anchor" aria-hidden="true" href="#model-agnostic-example-with-kernelexplainer-explains-any-function"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model agnostic example with KernelExplainer (explains any function)&lt;/h2&gt;
&lt;p&gt;Kernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; sklearn
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap
&lt;span class="pl-k"&gt;from&lt;/span&gt; sklearn.model_selection &lt;span class="pl-k"&gt;import&lt;/span&gt; train_test_split

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; print the JS visualization code to the notebook&lt;/span&gt;
shap.initjs()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train a SVM classifier&lt;/span&gt;
X_train,X_test,Y_train,Y_test &lt;span class="pl-k"&gt;=&lt;/span&gt; train_test_split(&lt;span class="pl-k"&gt;*&lt;/span&gt;shap.datasets.iris(), &lt;span class="pl-v"&gt;test_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.2&lt;/span&gt;, &lt;span class="pl-v"&gt;random_state&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
svm &lt;span class="pl-k"&gt;=&lt;/span&gt; sklearn.svm.SVC(&lt;span class="pl-v"&gt;kernel&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;rbf&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;probability&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
svm.fit(X_train, Y_train)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; use Kernel SHAP to explain test set predictions&lt;/span&gt;
explainer &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.KernelExplainer(svm.predict_proba, X_train, &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; explainer.shap_values(X_test, &lt;span class="pl-v"&gt;nsamples&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the SHAP values for the Setosa output of the first instance&lt;/span&gt;
shap.force_plot(explainer.expected_value[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], X_test.iloc[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png"&gt;&lt;img width="810" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.&lt;/p&gt;
&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the SHAP values for the Setosa output of all instances&lt;/span&gt;
shap.force_plot(explainer.expected_value[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], X_test, &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png"&gt;&lt;img width="813" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-shap-interaction-values" class="anchor" aria-hidden="true" href="#shap-interaction-values"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SHAP Interaction Values&lt;/h2&gt;
&lt;p&gt;SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with &lt;code&gt;shap.TreeExplainer(model).shap_interaction_values(X)&lt;/code&gt;. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png"&gt;&lt;img width="483" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sample-notebooks" class="anchor" aria-hidden="true" href="#sample-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample notebooks&lt;/h2&gt;
&lt;p&gt;The notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-treeexplainer" class="anchor" aria-hidden="true" href="#treeexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TreeExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html" rel="nofollow"&gt;&lt;strong&gt;NHANES survival model with XGBoost and SHAP interaction values&lt;/strong&gt;&lt;/a&gt; - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and &lt;code&gt;shap&lt;/code&gt; to uncover complex risk factor relationships.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html" rel="nofollow"&gt;&lt;strong&gt;Census income classification with LightGBM&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html" rel="nofollow"&gt;&lt;strong&gt;League of Legends Win Prediction with XGBoost&lt;/strong&gt;&lt;/a&gt; - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deepexplainer" class="anchor" aria-hidden="true" href="#deepexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html" rel="nofollow"&gt;&lt;strong&gt;MNIST Digit classification with Keras&lt;/strong&gt;&lt;/a&gt; - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html" rel="nofollow"&gt;&lt;strong&gt;Keras LSTM for IMDB Sentiment Classification&lt;/strong&gt;&lt;/a&gt; - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-gradientexplainer" class="anchor" aria-hidden="true" href="#gradientexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GradientExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html" rel="nofollow"&gt;&lt;strong&gt;Explain an Intermediate Layer of VGG16 on ImageNet&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-linearexplainer" class="anchor" aria-hidden="true" href="#linearexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LinearExplainer&lt;/h3&gt;
&lt;p&gt;For a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covaraince matrix. LinearExplainer supports both of these options.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html" rel="nofollow"&gt;&lt;strong&gt;Sentiment Analysis with Logistic Regression&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kernelexplainer" class="anchor" aria-hidden="true" href="#kernelexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;KernelExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes not assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html" rel="nofollow"&gt;&lt;strong&gt;Census income classification with scikit-learn&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html" rel="nofollow"&gt;&lt;strong&gt;ImageNet VGG16 Model with Keras&lt;/strong&gt;&lt;/a&gt; - Explain the classic VGG16 convolutional nerual network's predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html" rel="nofollow"&gt;&lt;strong&gt;Iris classification&lt;/strong&gt;&lt;/a&gt; - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-documentation-notebooks" class="anchor" aria-hidden="true" href="#documentation-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation notebooks&lt;/h2&gt;
&lt;p&gt;These notebooks comprehensively demonstrate how to use specific functions and objects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/plots/decision_plot.html" rel="nofollow"&gt;&lt;code&gt;shap.decision_plot&lt;/code&gt; and &lt;code&gt;shap.multioutput_decision_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html" rel="nofollow"&gt;&lt;code&gt;shap.dependence_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-methods-unified-by-shap" class="anchor" aria-hidden="true" href="#methods-unified-by-shap"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Methods Unified by SHAP&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;LIME:&lt;/em&gt; Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Why should i trust you?: Explaining the predictions of any classifier." Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Shapley sampling values:&lt;/em&gt; Strumbelj, Erik, and Igor Kononenko. "Explaining prediction models and individual predictions with feature contributions." Knowledge and information systems 41.3 (2014): 647-665.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;DeepLIFT:&lt;/em&gt; Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. "Learning important features through propagating activation differences." arXiv preprint arXiv:1704.02685 (2017).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;QII:&lt;/em&gt; Datta, Anupam, Shayak Sen, and Yair Zick. "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems." Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Layer-wise relevance propagation:&lt;/em&gt; Bach, Sebastian, et al. "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation." PloS one 10.7 (2015): e0130140.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Shapley regression values:&lt;/em&gt; Lipovetsky, Stan, and Michael Conklin. "Analysis of regression in game theory approach." Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Tree interpreter:&lt;/em&gt; Saabas, Ando. Interpreting random forests. &lt;a href="http://blog.datadive.net/interpreting-random-forests/" rel="nofollow"&gt;http://blog.datadive.net/interpreting-random-forests/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-citations" class="anchor" aria-hidden="true" href="#citations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citations&lt;/h2&gt;
&lt;p&gt;The algorithms and visualizations used in this package came primarily out of research in &lt;a href="https://suinlee.cs.washington.edu" rel="nofollow"&gt;Su-In Lee's lab&lt;/a&gt; at the University of Washington. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For general use of SHAP you can read/cite our &lt;a href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions" rel="nofollow"&gt;NeurIPS paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/shap_nips.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For TreeExplainer you can (for now) read/cite our &lt;a href="https://arxiv.org/abs/1905.04610" rel="nofollow"&gt;arXiv paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/treeshap_arxiv.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For &lt;code&gt;force_plot&lt;/code&gt; visualizations and medical applications you can read/cite our &lt;a href="https://www.nature.com/articles/s41551-018-0304-0" rel="nofollow"&gt;Nature Biomedical Engineering paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/nature_bme.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;; &lt;a href="https://rdcu.be/baVbR" rel="nofollow"&gt;free access&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/db8c6ffa9af4cf6d17c411a9f7ad56cc1b508c35/68747470733a2f2f7777772e66616365626f6f6b2e636f6d2f74723f69643d3138393134373039313835353939312665763d5061676556696577266e6f7363726970743d31"&gt;&lt;img height="1" width="1" src="https://camo.githubusercontent.com/db8c6ffa9af4cf6d17c411a9f7ad56cc1b508c35/68747470733a2f2f7777772e66616365626f6f6b2e636f6d2f74723f69643d3138393134373039313835353939312665763d5061676556696577266e6f7363726970743d31" data-canonical-src="https://www.facebook.com/tr?id=189147091855991&amp;amp;ev=PageView&amp;amp;noscript=1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>slundberg</author><guid isPermaLink="false">https://github.com/slundberg/shap</guid><pubDate>Tue, 19 Nov 2019 00:20:00 GMT</pubDate></item><item><title>NervanaSystems/distiller #21 in Jupyter Notebook, This month</title><link>https://github.com/NervanaSystems/distiller</link><description>&lt;p&gt;&lt;i&gt;Neural Network Distiller by Intel AI Lab: a Python package for neural network compression research.  https://nervanasystems.github.io/distiller&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt; &lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="imgs/banner1.png"&gt;&lt;img src="imgs/banner1.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/NervanaSystems/distiller/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/8051e9938a1ab39cf002818dfceb6b6092f34d68/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://zenodo.org/badge/latestdoi/130871393" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/209f57cec3fa35da7aed32332b18c8eab3ab1138/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3133303837313339332e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/130871393.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;h3&gt;&lt;a id="user-content-----------wiki-and-tutorials--------------------documentation--------------------getting-started--------------------algorithms--------------------design--------------------faq------" class="anchor" aria-hidden="true" href="#----------wiki-and-tutorials--------------------documentation--------------------getting-started--------------------algorithms--------------------design--------------------faq------"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
    &lt;a href="https://github.com/NervanaSystems/distiller/wiki"&gt;
      Wiki and tutorials
    &lt;/a&gt;
    &lt;span&gt; | &lt;/span&gt;
    &lt;a href="https://nervanasystems.github.io/distiller/index.html" rel="nofollow"&gt;
      Documentation
    &lt;/a&gt;
    &lt;span&gt; | &lt;/span&gt;
    &lt;a href="#getting-started"&gt;
      Getting Started
    &lt;/a&gt;
    &lt;span&gt; | &lt;/span&gt;
    &lt;a href="https://nervanasystems.github.io/distiller/algo_pruning.html" rel="nofollow"&gt;
      Algorithms
    &lt;/a&gt;
    &lt;span&gt; | &lt;/span&gt;
    &lt;a href="https://nervanasystems.github.io/distiller/design.html" rel="nofollow"&gt;
      Design
    &lt;/a&gt;
    &lt;span&gt; | &lt;/span&gt;
    &lt;a href="https://github.com/NervanaSystems/distiller/wiki/Frequently-Asked-Questions-(FAQ)"&gt;
      FAQ
    &lt;/a&gt;
  &lt;/h3&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Distiller&lt;/strong&gt; is an open-source Python package for neural network compression research.&lt;/p&gt;
&lt;p&gt;Network compression can reduce the memory footprint of a neural network, increase its inference speed and save energy. Distiller provides a &lt;a href="http://pytorch.org/" rel="nofollow"&gt;PyTorch&lt;/a&gt; environment for prototyping and analyzing compression algorithms, such as sparsity-inducing methods and low-precision arithmetic.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-note-on-release-03---possible-breaking-changes" class="anchor" aria-hidden="true" href="#note-on-release-03---possible-breaking-changes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note on Release 0.3 - Possible BREAKING Changes&lt;/h4&gt;
&lt;p&gt;As of release 0.3, we've moved some code around to enable proper packaging and installation of Distiller. In addition, we updated Distiller to support PyTorch 1.X, which might also cause older code to break due to some API changes.&lt;br&gt;
If updating from an earlier revision of the code, please make sure to follow the instructions in the &lt;a href="#install-the-package"&gt;install&lt;/a&gt; section to make sure proper installation of Distiller and all dependencies.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#highlighted-features"&gt;Highlighted features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#clone-distiller"&gt;Clone Distiller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#create-a-python-virtual-environment"&gt;Create a Python virtual environment&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#using-virtualenv"&gt;Using virtualenv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-venv"&gt;Using venv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#activate-the-environment"&gt;Activate the environment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#install-the-package"&gt;Install the package&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-started"&gt;Getting Started&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#example-invocations-of-the-sample-application"&gt;Example invocations of the sample application&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#training-only"&gt;Training-only&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-parameter-statistics-of-a-sparsified-model"&gt;Getting parameter statistics of a sparsified model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#post-training-quantization"&gt;Post-training quantization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#explore-the-sample-jupyter-notebooks"&gt;Explore the sample Jupyter notebooks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#set-up-the-classification-datasets"&gt;Set up the classification datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#running-the-tests"&gt;Running the tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#generating-the-html-documentation-site"&gt;Generating the HTML documentation site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#built-with"&gt;Built With&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#versioning"&gt;Versioning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#community"&gt;Community&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#acknowledgments"&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-highlighted-features" class="anchor" aria-hidden="true" href="#highlighted-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Highlighted features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Automatic Compression&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/NervanaSystems/distiller/blob/master/examples/auto_compression/amc"&gt;Automated Model Compression&lt;/a&gt; (AMC)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weight pruning&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Element-wise pruning using magnitude thresholding, sensitivity thresholding, target sparsity level, and activation statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Structured pruning&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Convolution: 2D (kernel-wise), 3D (filter-wise), 4D (layer-wise), and channel-wise structured pruning.&lt;/li&gt;
&lt;li&gt;Fully-connected: column-wise and row-wise structured pruning.&lt;/li&gt;
&lt;li&gt;Structure groups (e.g. structures of 4 filters).&lt;/li&gt;
&lt;li&gt;Structure-ranking with using weights or activations criteria (Lp-norm, APoZ, gradients, random, etc.).&lt;/li&gt;
&lt;li&gt;Support for new structures (e.g. block pruning)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Control&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Soft (mask on forward-pass only) and hard pruning (permanently disconnect neurons)&lt;/li&gt;
&lt;li&gt;Dual weight copies (compute loss on masked weights, but update unmasked weights)&lt;/li&gt;
&lt;li&gt;Model thinning (AKA "network garbage removal") to permanently remove pruned neurons and connections.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Schedule&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Flexible scheduling of pruning, regularization, and learning rate decay (compression scheduling)&lt;/li&gt;
&lt;li&gt;One-shot and iterative pruning (and fine-tuning) are supported.&lt;/li&gt;
&lt;li&gt;Easily control what is performed each training step (e.g. greedy layer by layer pruning to full model pruning).&lt;/li&gt;
&lt;li&gt;Automatic gradual schedule (AGP) for pruning individual connections and complete structures.&lt;/li&gt;
&lt;li&gt;The compression schedule is expressed in a YAML file so that a single file captures the details of experiments.  This &lt;a href="https://en.wikipedia.org/wiki/Dependency_injection" rel="nofollow"&gt;dependency injection&lt;/a&gt; design decouples the Distiller scheduler and library from future extensions of algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Element-wise and filter-wise pruning &lt;strong&gt;sensitivity analysis&lt;/strong&gt; (using L1-norm thresholding). Examine the data from some of the networks we analyzed, using &lt;a href="https://github.com/NervanaSystems/distiller/blob/master/jupyter/sensitivity_analysis.ipynb"&gt;this notebook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Regularization&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;L1-norm element-wise regularization&lt;/li&gt;
&lt;li&gt;Group Lasso an group variance regularization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Quantization&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Automatic mechanism to transform existing models to quantized versions, with customizable bit-width configuration for different layers. No need to re-write the model for different quantization methods.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nervanasystems.github.io/distiller/usage/index.html##post-training-quantization" rel="nofollow"&gt;Post-training quantization&lt;/a&gt; of trained full-precision models, dynamic and static (statistics-based)&lt;/li&gt;
&lt;li&gt;Support for &lt;a href="https://nervanasystems.github.io/distiller/algo_quantization.html#quantization-aware-training" rel="nofollow"&gt;quantization-aware training&lt;/a&gt; in the loop&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Knowledge distillation&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Training with &lt;a href="https://nervanasystems.github.io/distiller/knowledge_distillation.html" rel="nofollow"&gt;knowledge distillation&lt;/a&gt;, in conjunction with the other available pruning / regularization / quantization methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conditional computation&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Sample implementation of Early Exit&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Low rank decomposition&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Sample implementation of &lt;a href="https://github.com/NervanaSystems/distiller/blob/master/jupyter/truncated_svd.ipynb"&gt;truncated SVD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lottery Ticket Hypothesis training&lt;/li&gt;
&lt;li&gt;Export statistics summaries using Pandas dataframes, which makes it easy to slice, query, display and graph the data.&lt;/li&gt;
&lt;li&gt;A set of &lt;a href="https://nervanasystems.github.io/distiller/jupyter/index.html" rel="nofollow"&gt;Jupyter notebooks&lt;/a&gt; to plan experiments and analyze compression results.  The graphs and visualizations you see on this page originate from the included Jupyter notebooks.
&lt;ul&gt;
&lt;li&gt;Take a look at &lt;a href="https://github.com/NervanaSystems/distiller/blob/master/jupyter/alexnet_insights.ipynb"&gt;this notebook&lt;/a&gt;, which compares visual aspects of dense and sparse Alexnet models.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/NervanaSystems/distiller/blob/master/jupyter/model_summary.ipynb"&gt;This notebook&lt;/a&gt; creates performance indicator graphs from model data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sample implementations of published research papers, using library-provided building blocks.  See the  research papers discussions in our &lt;a href="https://nervanasystems.github.io/distiller/model_zoo.html" rel="nofollow"&gt;model-zoo&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Logging to the console, text file and TensorBoard-formatted file.&lt;/li&gt;
&lt;li&gt;Export to &lt;strong&gt;ONNX&lt;/strong&gt; (export of quantized models pending ONNX standardization)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;These instructions will help get Distiller up and running on your local machine.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#clone-distiller"&gt;Clone Distiller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#create-a-python-virtual-environment"&gt;Create a Python virtual environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#install-the-package"&gt;Install the package&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Distiller has only been tested on Ubuntu 16.04 LTS, and with Python 3.5.&lt;/li&gt;
&lt;li&gt;If you are not using a GPU, you might need to make small adjustments to the code.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-clone-distiller" class="anchor" aria-hidden="true" href="#clone-distiller"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Clone Distiller&lt;/h3&gt;
&lt;p&gt;Clone the Distiller code repository from github:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/NervanaSystems/distiller.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The rest of the documentation that follows, assumes that you have cloned your repository to a directory called &lt;code&gt;distiller&lt;/code&gt;. &lt;br&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-create-a-python-virtual-environment" class="anchor" aria-hidden="true" href="#create-a-python-virtual-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Create a Python virtual environment&lt;/h3&gt;
&lt;p&gt;We recommend using a &lt;a href="https://docs.python.org/3/library/venv.html#venv-def" rel="nofollow"&gt;Python virtual environment&lt;/a&gt;, but that of course, is up to you.
There's nothing special about using Distiller in a virtual environment, but we provide some instructions, for completeness.&lt;br&gt;
Before creating the virtual environment, make sure you are located in directory &lt;code&gt;distiller&lt;/code&gt;.  After creating the environment, you should see a directory called &lt;code&gt;distiller/env&lt;/code&gt;.
&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-using-virtualenv" class="anchor" aria-hidden="true" href="#using-virtualenv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using virtualenv&lt;/h4&gt;
&lt;p&gt;If you don't have virtualenv installed, you can find the installation instructions &lt;a href="https://packaging.python.org/guides/installing-using-pip-and-virtualenv/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To create the environment, execute:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m virtualenv env
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a subdirectory named &lt;code&gt;env&lt;/code&gt; where the python virtual environment is stored, and configures the current shell to use it as the default python environment.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-using-venv" class="anchor" aria-hidden="true" href="#using-venv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using venv&lt;/h4&gt;
&lt;p&gt;If you prefer to use &lt;code&gt;venv&lt;/code&gt;, then begin by installing it:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo apt-get install python3-venv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then create the environment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m venv env
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with virtualenv, this creates a directory called &lt;code&gt;distiller/env&lt;/code&gt;.&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-activate-the-environment" class="anchor" aria-hidden="true" href="#activate-the-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Activate the environment&lt;/h4&gt;
&lt;p&gt;The environment activation and deactivation commands for &lt;code&gt;venv&lt;/code&gt; and &lt;code&gt;virtualenv&lt;/code&gt; are the same.&lt;br&gt;
&lt;strong&gt;!NOTE: Make sure to activate the environment, before proceeding with the installation of the dependency packages:&lt;br&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ source env/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-install-the-package" class="anchor" aria-hidden="true" href="#install-the-package"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install the package&lt;/h3&gt;
&lt;p&gt;Finally, install the Distiller package and its dependencies using &lt;code&gt;pip3&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd distiller
$ pip3 install -e .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This installs Distiller in "development mode", meaning any changes made in the code are reflected in the environment without re-running the install command (so no need to re-install after pulling changes from the Git repository).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-required-pytorch-version" class="anchor" aria-hidden="true" href="#required-pytorch-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Required PyTorch Version&lt;/h3&gt;
&lt;p&gt;Distiller is tested using the default installation of PyTorch 1.3.1, which uses CUDA 10.1. We use TorchVision version 0.4.2. These are included in Distiller's &lt;code&gt;requirements.txt&lt;/code&gt; and will be automatically installed when installing the Distiller package as listed above.&lt;/p&gt;
&lt;p&gt;If you do not use CUDA 10.1 in your environment, please refer to &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;PyTorch website&lt;/a&gt; to install the compatible build of PyTorch 1.3.1 and torchvision 0.4.2.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;You can jump head-first into some limited examples of network compression, to get a feeling for the library without too much investment on your part.&lt;/p&gt;
&lt;p&gt;Distiller comes with a sample application for compressing image classification DNNs, &lt;code&gt;compress_classifier.py&lt;/code&gt; located at &lt;code&gt;distiller/examples/classifier_compression&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We'll show you how to use it for some simple use-cases, and will point you to some ready-to-go Jupyter notebooks.&lt;/p&gt;
&lt;p&gt;For more details, there are some other resources you can refer to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/NervanaSystems/distiller/wiki/Frequently-Asked-Questions-(FAQ)"&gt;Frequently-asked questions (FAQ)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nervanasystems.github.io/distiller/model_zoo.html" rel="nofollow"&gt;Model zoo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nervanasystems.github.io/distiller/schedule.html" rel="nofollow"&gt;Compression scheduling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nervanasystems.github.io/distiller/usage.html" rel="nofollow"&gt;Usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nervanasystems.github.io/distiller/prepare_model_quant.html" rel="nofollow"&gt;Preparing a model for quantization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nervanasystems.github.io/distiller/tutorial-lang_model.html" rel="nofollow"&gt;Tutorial: Using Distiller to prune a PyTorch language model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nervanasystems.github.io/distiller/tutorial-struct_pruning.html" rel="nofollow"&gt;Tutorial: Pruning Filters &amp;amp; Channels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nervanasystems.github.io/distiller/tutorial-lang_model_quant.html" rel="nofollow"&gt;Tutorial: Post-Training Quantization of a Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nervanasystems.github.io/distiller/tutorial-gnmt_quant.html" rel="nofollow"&gt;Tutorial: Post-Training Quantization of GNMT (translation model)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/NervanaSystems/distiller/blob/master/examples/quantization/post_train_quant/command_line.md"&gt;Post-training quantization command line examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-example-invocations-of-the-sample-application" class="anchor" aria-hidden="true" href="#example-invocations-of-the-sample-application"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example invocations of the sample application&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#training-only"&gt;Training-only&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-parameter-statistics-of-a-sparsified-model"&gt;Getting parameter statistics of a sparsified model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#post-training-quantization"&gt;Post-training quantization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-only" class="anchor" aria-hidden="true" href="#training-only"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training-only&lt;/h4&gt;
&lt;p&gt;The following will invoke training-only (no compression) of a network named 'simplenet' on the CIFAR10 dataset.  This is roughly based on TorchVision's sample Imagenet training application, so it should look familiar if you've used that application.  In  this example we don't invoke any compression mechanisms: we just train because for fine-tuning after pruning, training is an essential part.&lt;br&gt;&lt;br&gt;
Note that the first time you execute this command, the CIFAR10 code will be downloaded to your machine, which may take a bit of time - please let the download process proceed to completion.&lt;/p&gt;
&lt;p&gt;The path to the CIFAR10 dataset is arbitrary, but in our examples we place the datasets in the same directory level as distiller (i.e. &lt;code&gt;../../../data.cifar10&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;First, change to the sample directory, then invoke the application:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd distiller/examples/classifier_compression
$ python3 compress_classifier.py --arch simplenet_cifar ../../../data.cifar10 -p 30 -j=1 --lr=0.01
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can use a TensorBoard backend to view the training progress (in the diagram below we show a couple of training sessions with different LR values).  For compression sessions, we've added tracing of activation and parameter sparsity levels, and regularization loss.&lt;/p&gt;
 &lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="imgs/simplenet_training.png"&gt;&lt;img src="imgs/simplenet_training.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-getting-parameter-statistics-of-a-sparsified-model" class="anchor" aria-hidden="true" href="#getting-parameter-statistics-of-a-sparsified-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting parameter statistics of a sparsified model&lt;/h4&gt;
&lt;p&gt;We've included in the git repository a few checkpoints of a ResNet20 model that we've trained with 32-bit floats.  Let's load the checkpoint of a model that we've trained with channel-wise Group Lasso regularization.&lt;br&gt;
With the following command-line arguments, the sample application loads the model (&lt;code&gt;--resume&lt;/code&gt;)  and prints statistics about the model weights (&lt;code&gt;--summary=sparsity&lt;/code&gt;).  This is useful if you want to load a previously pruned model, to examine the weights sparsity statistics, for example.  Note that when you &lt;em&gt;resume&lt;/em&gt; a stored checkpoint, you still need to tell the application which network architecture the checkpoint uses (&lt;code&gt;-a=resnet20_cifar&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 compress_classifier.py --resume=../ssl/checkpoints/checkpoint_trained_ch_regularized_dense.pth.tar -a=resnet20_cifar ../../../data.cifar10 --summary=sparsity
&lt;/code&gt;&lt;/pre&gt;
 &lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="imgs/ch_sparsity_stats.png"&gt;&lt;img src="imgs/ch_sparsity_stats.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You should see a text table detailing the various sparsities of the parameter tensors.  The first column is the parameter name, followed by its shape, the number of non-zero elements (NNZ) in the dense model, and in the sparse model.  The next set of columns show the column-wise, row-wise, channel-wise, kernel-wise, filter-wise and element-wise sparsities.
&lt;br&gt;
Wrapping it up are the standard-deviation, mean, and mean of absolute values of the elements.&lt;/p&gt;
&lt;p&gt;In the &lt;a href="https://github.com/NervanaSystems/distiller/blob/master/jupyter/compression_insights.ipynb"&gt;Compression Insights notebook&lt;/a&gt; we use matplotlib to plot a bar chart of this summary, that indeed show non-impressive footprint compression.&lt;/p&gt;
 &lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="imgs/ch_sparsity_stats_barchart.png"&gt;&lt;img src="imgs/ch_sparsity_stats_barchart.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Although the memory footprint compression is very low, this model actually saves 26.6% of the MACs compute.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 compress_classifier.py --resume=../ssl/checkpoints/checkpoint_trained_channel_regularized_resnet20_finetuned.pth.tar -a=resnet20_cifar ../../../data.cifar10 --summary=compute
&lt;/code&gt;&lt;/pre&gt;
 &lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="imgs/ch_compute_stats.png"&gt;&lt;img src="imgs/ch_compute_stats.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-post-training-quantization" class="anchor" aria-hidden="true" href="#post-training-quantization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Post-training quantization&lt;/h4&gt;
&lt;p&gt;This example performs 8-bit quantization of ResNet20 for CIFAR10.  We've included in the git repository the checkpoint of a ResNet20 model that we've trained with 32-bit floats, so we'll take this model and quantize it:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 compress_classifier.py -a resnet20_cifar ../../../data.cifar10 --resume ../ssl/checkpoints/checkpoint_trained_dense.pth.tar --quantize-eval --evaluate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command-line above will save a checkpoint named &lt;code&gt;quantized_checkpoint.pth.tar&lt;/code&gt; containing the quantized model parameters. See more examples &lt;a href="https://github.com/NervanaSystems/distiller/blob/master/examples/quantization/post_train_quant/command_line.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-explore-the-sample-jupyter-notebooks" class="anchor" aria-hidden="true" href="#explore-the-sample-jupyter-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Explore the sample Jupyter notebooks&lt;/h3&gt;
&lt;p&gt;The set of notebooks that come with Distiller is described &lt;a href="https://nervanasystems.github.io/distiller/jupyter.html#using-the-distiller-notebooks" rel="nofollow"&gt;here&lt;/a&gt;, which also explains the steps to install the Jupyter notebook server.&lt;br&gt;
After installing and running the server, take a look at the &lt;a href="https://github.com/NervanaSystems/distiller/blob/master/jupyter/sensitivity_analysis.ipynb"&gt;notebook&lt;/a&gt; covering pruning sensitivity analysis.&lt;/p&gt;
&lt;p&gt;Sensitivity analysis is a long process and this notebook loads CSV files that are the output of several sessions of sensitivity analysis.&lt;/p&gt;
 &lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="imgs/resnet18-sensitivity.png"&gt;&lt;img src="imgs/resnet18-sensitivity.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-set-up-the-classification-datasets" class="anchor" aria-hidden="true" href="#set-up-the-classification-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Set up the classification datasets&lt;/h2&gt;
&lt;p&gt;The sample application for compressing image classification DNNs, &lt;code&gt;compress_classifier.py&lt;/code&gt; located at &lt;code&gt;distiller/examples/classifier_compression&lt;/code&gt;, uses both &lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow"&gt;CIFAR10&lt;/a&gt; and &lt;a href="http://www.image-net.org/" rel="nofollow"&gt;ImageNet&lt;/a&gt; image datasets.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;compress_classifier.py&lt;/code&gt; application will download the CIFAR10 automatically the first time you try to use it (thanks to TorchVision).  The example invocations used  throughout Distiller's documentation assume that you have downloaded the images to directory &lt;code&gt;distiller/../data.cifar10&lt;/code&gt;, but you can place the images anywhere you want (you tell &lt;code&gt;compress_classifier.py&lt;/code&gt; where the dataset is located - or where you want the application to download the dataset to - using a command-line parameter).&lt;/p&gt;
&lt;p&gt;ImageNet needs to be &lt;a href="http://image-net.org/download" rel="nofollow"&gt;downloaded&lt;/a&gt; manually, due to copyright issues.  Facebook has created a &lt;a href="https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset"&gt;set of scripts&lt;/a&gt; to help download and extract the dataset.&lt;/p&gt;
&lt;p&gt;Again, the Distiller documentation assumes the following directory structure for the datasets, but this is just a suggestion:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;distiller
  examples
    classifier_compression
data.imagenet/
    train/
    val/
data.cifar10/
    cifar-10-batches-py/
        batches.meta
        data_batch_1
        data_batch_2
        data_batch_3
        data_batch_4
        data_batch_5
        readme.html
        test_batch
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-running-the-tests" class="anchor" aria-hidden="true" href="#running-the-tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running the tests&lt;/h2&gt;
&lt;p&gt;We are currently light-weight on test and this is an area where contributions will be much appreciated.&lt;br&gt;
There are two types of tests: system tests and unit-tests.  To invoke the unit tests:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd distiller/tests
$ pytest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use CIFAR10 for the system tests, because its size makes for quicker tests.  To invoke the system tests, you need to provide a path to the CIFAR10 dataset which you've already downloaded.  Alternatively, you may invoke &lt;code&gt;full_flow_tests.py&lt;/code&gt; without specifying the location of the CIFAR10 dataset and let the test download the dataset (for the first invocation only).  Note that &lt;code&gt;--cifar1o-path&lt;/code&gt; defaults to the current directory. &lt;br&gt;
The system tests are not short, and are even longer if the test needs to download the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd distiller/tests
$ python full_flow_tests.py --cifar10-path=&amp;lt;some_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The script exits with status 0 if all tests are successful, or status 1 otherwise.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-generating-the-html-documentation-site" class="anchor" aria-hidden="true" href="#generating-the-html-documentation-site"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating the HTML documentation site&lt;/h2&gt;
&lt;p&gt;Install mkdocs and the required packages by executing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pip3 install -r doc-requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To build the project documentation run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd distiller/docs-src
$ mkdocs build --clean
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will create a folder named 'site' which contains the documentation website.
Open distiller/docs/site/index.html to view the documentation home page.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-built-with" class="anchor" aria-hidden="true" href="#built-with"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Built With&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://pytorch.org/" rel="nofollow"&gt;PyTorch&lt;/a&gt; - The tensor and neural network framework used by Distiller.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jupyter.org/" rel="nofollow"&gt;Jupyter&lt;/a&gt; - Notebook serving.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard" rel="nofollow"&gt;TensorBoard&lt;/a&gt; - Used to view training graphs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Cadene/pretrained-models.pytorch"&gt;Cadene&lt;/a&gt; - Pretrained PyTorch models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-versioning" class="anchor" aria-hidden="true" href="#versioning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Versioning&lt;/h2&gt;
&lt;p&gt;We use &lt;a href="http://semver.org/" rel="nofollow"&gt;SemVer&lt;/a&gt; for versioning. For the versions available, see the &lt;a href="https://github.com/NervanaSystems/distiller/tags"&gt;tags on this repository&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This project is licensed under the Apache License 2.0 - see the &lt;a href="LICENSE.md"&gt;LICENSE.md&lt;/a&gt; file for details&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-community" class="anchor" aria-hidden="true" href="#community"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Community&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-github-projects-using-distiller" class="anchor" aria-hidden="true" href="#github-projects-using-distiller"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Github projects using Distiller:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/DeGirum/pruned-models"&gt;DeGirum Pruned Models&lt;/a&gt; - a repository containing pruned models and related information.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-research-papers-citing-distiller" class="anchor" aria-hidden="true" href="#research-papers-citing-distiller"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Research papers citing Distiller:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Hui Guan, Lin Ning, Zhen Lin, Xipeng Shen, Huiyang Zhou, Seung-Hwan Lim.&lt;br&gt;
&lt;em&gt;&lt;a href="https://arxiv.org/abs/1910.14479" rel="nofollow"&gt;In-Place Zero-Space Memory Protection for CNN&lt;/a&gt;&lt;/em&gt;,&lt;br&gt;
In Conference on Neural Information Processing Systems (NeurIPS), 2019.&lt;br&gt;
arXiv:1910.14479, 2019&lt;br&gt;
&lt;a href="https://github.com/guanh01/wot"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maxim Zemlyanikin, Alexander Smorkalov, Tatiana Khanova, Anna Petrovicheva, Grigory Serebryakov.&lt;br&gt;
&lt;em&gt;&lt;a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/LPCV/Zemlyanikin_512KiB_RAM_Is_Enough_Live_Camera_Face_Recognition_DNN_on_ICCVW_2019_paper.html" rel="nofollow"&gt;512KiB RAM Is Enough! Live Camera Face Recognition DNN on MCU&lt;/a&gt;&lt;/em&gt;,&lt;br&gt;
In IEEE International Conference on Computer Vision (ICCV), 2019.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ziheng Wang, Jeremy Wohlwend, Tao Lei.&lt;br&gt;
&lt;em&gt;&lt;a href="https://arxiv.org/abs/1910.04732" rel="nofollow"&gt;Structured Pruning of Large Language Models&lt;/a&gt;&lt;/em&gt;,&lt;br&gt;
arXiv:1910.04732, 2019.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Soroush Ghodrati, Hardik Sharma, Sean Kinzer, Amir Yazdanbakhsh, Kambiz Samadi, Nam Sung Kim, Doug Burger, Hadi Esmaeilzadeh.&lt;br&gt;
&lt;em&gt;&lt;a href="https://arxiv.org/abs/1906.11915" rel="nofollow"&gt;Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through Interleaved Bit-Partitioned Arithmetic&lt;/a&gt;&lt;/em&gt;,&lt;br&gt;
arXiv:1906.11915, 2019.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gil Shomron, Tal Horowitz, Uri Weiser.&lt;br&gt;
&lt;em&gt;&lt;a href="https://ieeexplore.ieee.org/document/8742541" rel="nofollow"&gt;SMT-SA: Simultaneous Multithreading in Systolic Arrays&lt;/a&gt;&lt;/em&gt;,&lt;br&gt;
In IEEE Computer Architecture Letters (CAL), 2019.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shangqian Gao , Cheng Deng , and Heng Huang.&lt;br&gt;
&lt;em&gt;&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html" rel="nofollow"&gt;Cross Domain Model Compression by Structurally Weight Sharing&lt;/a&gt;,&lt;/em&gt;&lt;br&gt;
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8973-8982.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass.&lt;br&gt;
&lt;em&gt;&lt;a href="https://arxiv.org/abs/1906.04164" rel="nofollow"&gt;FAKTA: An Automatic End-to-End Fact Checking System&lt;/a&gt;,&lt;/em&gt;&lt;br&gt;
In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.&lt;br&gt;
&lt;em&gt;&lt;a href="https://arxiv.org/abs/1905.01416" rel="nofollow"&gt;SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training&lt;/a&gt;,&lt;/em&gt;&lt;br&gt;
arXiv:1905.01416, 2019.
&lt;a href="https://github.com/sinreq/sinreq_code"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Goncharenko A., Denisov A., Alyamkin S., Terentev E.&lt;br&gt;
&lt;em&gt;&lt;a href="https://rd.springer.com/chapter/10.1007/978-3-030-20518-8_26" rel="nofollow"&gt;Trainable Thresholds for Neural Network Quantization&lt;/a&gt;,&lt;/em&gt;&lt;br&gt;
In: Rojas I., Joya G., Catala A. (eds) Advances in Computational Intelligence Lecture Notes in Computer Science, vol 11507. Springer, Cham.  International Work-Conference on Artificial Neural Networks (IWANN 2019).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.&lt;br&gt;
&lt;em&gt;&lt;a href="https://arxiv.org/abs/1906.06033" rel="nofollow"&gt;Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks&lt;/a&gt;,&lt;/em&gt;
arXiv:1906.06033, 2019&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.&lt;br&gt;
&lt;em&gt;&lt;a href="https://arxiv.org/abs/1901.09504" rel="nofollow"&gt;Improving Neural Network Quantization without Retraining using Outlier Channel Splitting&lt;/a&gt;,&lt;/em&gt;&lt;br&gt;
arXiv:1901.09504, 2019&lt;br&gt;
&lt;a href="https://github.com/cornell-zhang/dnn-quant-ocs"&gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.&lt;br&gt;
&lt;em&gt;&lt;a href="https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf" rel="nofollow"&gt;Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference&lt;/a&gt;&lt;/em&gt;,&lt;br&gt;
Nvidia Research, 2019.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Norio Nakata.&lt;br&gt;
&lt;em&gt;&lt;a href="https://rd.springer.com/article/10.1007/s11604-018-0804-6" rel="nofollow"&gt;Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging&lt;/a&gt;&lt;/em&gt;,&lt;br&gt;
In Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103–108.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.&lt;br&gt;
&lt;em&gt;&lt;a href="https://arxiv.org/abs/1812.07872" rel="nofollow"&gt;Fast Adjustable Threshold For Uniform Neural Network Quantization&lt;/a&gt;&lt;/em&gt;,&lt;br&gt;
arXiv:1812.07872, 2018&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you used Distiller for your work, please use the following citation:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@misc{neta_zmora_2018_1297430,
  author       = {Neta Zmora and
                  Guy Jacob and
                  Lev Zlotnik and
                  Bar Elharar and
                  Gal Novik},
  title        = {Neural Network Distiller},
  month        = jun,
  year         = 2018,
  doi          = {10.5281/zenodo.1297430},
  url          = {https://doi.org/10.5281/zenodo.1297430}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgments" class="anchor" aria-hidden="true" href="#acknowledgments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;Any published work is built on top of the work of many other people, and the credit belongs to too many people to list here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Python and PyTorch developer communities have shared many invaluable insights, examples and ideas on the Web.&lt;/li&gt;
&lt;li&gt;The authors of the research papers implemented in the &lt;a href="https://nervanasystems.github.io/distiller/model_zoo.html" rel="nofollow"&gt;Distiller model-zoo&lt;/a&gt; have shared their research ideas, theoretical background and results.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;Distiller is released as a reference code for research purposes. It is not an official Intel product, and the level of quality and support may not be as expected from an official product. Additional algorithms and features are planned to be added to the library. Feedback and contributions from the open source and research communities are more than welcome.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NervanaSystems</author><guid isPermaLink="false">https://github.com/NervanaSystems/distiller</guid><pubDate>Tue, 19 Nov 2019 00:21:00 GMT</pubDate></item><item><title>pytorch/tutorials #22 in Jupyter Notebook, This month</title><link>https://github.com/pytorch/tutorials</link><description>&lt;p&gt;&lt;i&gt;PyTorch tutorials.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytorch-tutorials" class="anchor" aria-hidden="true" href="#pytorch-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Tutorials&lt;/h1&gt;
&lt;p&gt;All the tutorials are now presented as sphinx style documentation at:&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-httpspytorchorgtutorials" class="anchor" aria-hidden="true" href="#httpspytorchorgtutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://pytorch.org/tutorials" rel="nofollow"&gt;https://pytorch.org/tutorials&lt;/a&gt;&lt;/h2&gt;
&lt;h1&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h1&gt;
&lt;p&gt;We use sphinx-gallery's &lt;a href="https://sphinx-gallery.github.io/tutorials/plot_notebook.html#sphx-glr-tutorials-plot-notebook-py" rel="nofollow"&gt;notebook styled examples&lt;/a&gt; to create the tutorials. Syntax is very simple. In essence, you write a slightly well formatted python file and it shows up as documentation page.&lt;/p&gt;
&lt;p&gt;Here's how to create a new tutorial:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a notebook styled python file. If you want it executed while inserted into documentation, save the file with suffix &lt;code&gt;tutorial&lt;/code&gt; so that file name is &lt;code&gt;your_tutorial.py&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Put it in one of the beginner_source, intermediate_source, advanced_source based on the level.&lt;/li&gt;
&lt;li&gt;Include it in the right TOC tree at index.rst&lt;/li&gt;
&lt;li&gt;Create a thumbnail in the index file using a command like &lt;code&gt;.. galleryitem:: beginner/your_tutorial.py&lt;/code&gt;. (This is a custom directive. See &lt;code&gt;custom_directives.py&lt;/code&gt; for more info.)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case you prefer to write your tutorial in jupyter, you can use &lt;a href="https://gist.github.com/chsasank/7218ca16f8d022e02a9c0deb94a310fe"&gt;this script&lt;/a&gt; to convert the notebook to python file. After conversion and addition to the project, please make sure the sections headings etc are in logical order.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-building" class="anchor" aria-hidden="true" href="#building"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Start with installing torch, torchvision, and your GPUs latest drivers. Install other requirements using &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If you want to use &lt;code&gt;virtualenv&lt;/code&gt;, make your environment in a &lt;code&gt;venv&lt;/code&gt; directory like: &lt;code&gt;virtualenv ./venv&lt;/code&gt;, then &lt;code&gt;source ./venv/bin/activate&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Then you can build using &lt;code&gt;make docs&lt;/code&gt;. This will download the data, execute the tutorials and build the documentation to &lt;code&gt;docs/&lt;/code&gt; directory. This will take about 60-120 min for systems with GPUs. If you do not have a GPU installed on your system, then see next step.&lt;/li&gt;
&lt;li&gt;You can skip the computationally intensive graph generation by running &lt;code&gt;make html-noplot&lt;/code&gt; to build basic html documentation to &lt;code&gt;_build/html&lt;/code&gt;. This way, you can quickly preview your tutorial.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pytorch</author><guid isPermaLink="false">https://github.com/pytorch/tutorials</guid><pubDate>Tue, 19 Nov 2019 00:22:00 GMT</pubDate></item><item><title>Qiskit/qiskit-iqx-tutorials #23 in Jupyter Notebook, This month</title><link>https://github.com/Qiskit/qiskit-iqx-tutorials</link><description>&lt;p&gt;&lt;i&gt;A collection of Jupyter notebooks showing how to use Qiskit that is synced with the IBM Q Experience&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-qiskit-iqx-tutorials" class="anchor" aria-hidden="true" href="#qiskit-iqx-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Qiskit IQX Tutorials&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/756b44e3a004a334bfb4e9391f5777486cbae6d8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f5169736b69742f7169736b69742d6971782d7475746f7269616c732e7376673f7374796c653d706f706f75742d737175617265" alt="License" data-canonical-src="https://img.shields.io/github/license/Qiskit/qiskit-iqx-tutorials.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;&lt;a href="https://github.com/Qiskit/qiskit-iqx-tutorials/releases"&gt;&lt;img src="https://camo.githubusercontent.com/39a2c46882ab44804e108f132b62387f7aaf3c26/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f5169736b69742f7169736b69742d6971782d7475746f7269616c732e7376673f7374796c653d706f706f75742d737175617265" alt="" data-canonical-src="https://img.shields.io/github/release/Qiskit/qiskit-iqx-tutorials.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Welcome to the &lt;a href="https://www.qiskit.org/" rel="nofollow"&gt;Qiskit&lt;/a&gt; IQX Tutorials!&lt;/p&gt;
&lt;p&gt;In this repository, we've put together a collection of Jupyter notebooks aimed
at teaching people who want to use Qiskit for writing quantum computing
programs, and executing them on one of several backends (online quantum
processors, online simulators, and local simulators). The online quantum
processors are the &lt;a href="https://quantum-computing.ibm.com" rel="nofollow"&gt;IBM Q&lt;/a&gt; devices.&lt;/p&gt;
&lt;p&gt;For our community-contributed tutorials, please check out the
&lt;a href="https://github.com/Qiskit/qiskit-community-tutorials"&gt;qiskit-community-tutorials&lt;/a&gt;
repository.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;The notebooks for these tutorials can be viewed here on GitHub...but for the
full experience, you'll want to interact with them!  The easiest way to do this
is by logging into the &lt;a href="https://quantum-computing.ibm.com/" rel="nofollow"&gt;IBM Quantum
Experience&lt;/a&gt;, which lets you use Jupyter
notebooks, including these tutorials, via the web.&lt;/p&gt;
&lt;p&gt;Please refer to this &lt;a href="INSTALL.md"&gt;installation guide&lt;/a&gt; for setting up Qiskit and
the tutorials on your own machine (this is the recommended way).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;p&gt;We've collected a core reference set of notebooks in this section outlining the
features of Qiskit. We will be keeping them up to date with the latest Qiskit
version.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="qiskit/fundamentals"&gt;Basics&lt;/a&gt; is for those who are getting started.&lt;/li&gt;
&lt;li&gt;&lt;a href="qiskit/advanced/terra"&gt;Terra&lt;/a&gt; is for those who want to study circuits.&lt;/li&gt;
&lt;li&gt;&lt;a href="qiskit/advanced/aer"&gt;Aer&lt;/a&gt; is for those who want to simulate quantum circuits.&lt;/li&gt;
&lt;li&gt;&lt;a href="qiskit/advanced/ignis"&gt;Ignis&lt;/a&gt; is for those who want to study noise.&lt;/li&gt;
&lt;li&gt;&lt;a href="qiskit/advanced/aqua"&gt;Aqua&lt;/a&gt; is for those who want to develop applications on NISQ computers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To go through the Qiskit examples, load up the
&lt;a href="qiskit/1_start_here.ipynb"&gt;start_here.ipynb&lt;/a&gt; notebook and start seeing how
Qiskit works.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-guidelines" class="anchor" aria-hidden="true" href="#contribution-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution Guidelines&lt;/h2&gt;
&lt;p&gt;If you'd like to contribute to Qiskit IQX Tutorials, please take a look at our
&lt;a href=".github/CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;. This project adheres to
Qiskit's &lt;a href=".github/CODE_OF_CONDUCT.md"&gt;code of conduct&lt;/a&gt;. By participating, you
are expect to uphold to this code.&lt;/p&gt;
&lt;p&gt;We use &lt;a href="https://github.com/Qiskit/qiskit-iqx-tutorials/issues"&gt;GitHub issues&lt;/a&gt; for
tracking requests and bugs. Please use our &lt;a href="https://qiskit.slack.com" rel="nofollow"&gt;Slack&lt;/a&gt;
for discussion and simple questions. To join our Slack community, use the
&lt;a href="https://join.slack.com/t/qiskit/shared_invite/enQtNDc2NjUzMjE4Mzc0LTMwZmE0YTM4ZThiNGJmODkzN2Y2NTNlMDIwYWNjYzA2ZmM1YTRlZGQ3OGM0NjcwMjZkZGE0MTA4MGQ1ZTVmYzk" rel="nofollow"&gt;link&lt;/a&gt;.
For questions that are more suited for a forum, we use the Qiskit tag in the
&lt;a href="https://quantumcomputing.stackexchange.com/questions/tagged/qiskit" rel="nofollow"&gt;Stack
Exchange&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-authors-and-citation" class="anchor" aria-hidden="true" href="#authors-and-citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors and Citation&lt;/h2&gt;
&lt;p&gt;Qiskit IQX Tutorials is the work of &lt;a href="https://github.com/Qiskit/qiskit-iqx-tutorials/graphs/contributors"&gt;many
people&lt;/a&gt; who
contribute to the project at different levels. If you use Qiskit, please cite
as per the included &lt;a href="https://github.com/Qiskit/qiskit/blob/master/Qiskit.bib"&gt;BibTeX
file&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Qiskit</author><guid isPermaLink="false">https://github.com/Qiskit/qiskit-iqx-tutorials</guid><pubDate>Tue, 19 Nov 2019 00:23:00 GMT</pubDate></item><item><title>awslabs/amazon-sagemaker-examples #24 in Jupyter Notebook, This month</title><link>https://github.com/awslabs/amazon-sagemaker-examples</link><description>&lt;p&gt;&lt;i&gt;Example notebooks that show how to apply machine learning, deep learning and reinforcement learning in Amazon SageMaker&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-amazon-sagemaker-examples" class="anchor" aria-hidden="true" href="#amazon-sagemaker-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Examples&lt;/h1&gt;
&lt;p&gt;This repository contains example notebooks that show how to apply machine learning and deep learning in &lt;a href="https://aws.amazon.com/sagemaker" rel="nofollow"&gt;Amazon SageMaker&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-ground-truth-labeling-jobs" class="anchor" aria-hidden="true" href="#introduction-to-ground-truth-labeling-jobs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Ground Truth Labeling Jobs&lt;/h3&gt;
&lt;p&gt;These examples provide quick walkthroughs to get you up and running with the labeling job workflow for Amazon SageMaker Ground Truth.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification"&gt;From Unlabeled Data to a Deployed Machine Learning Model: A SageMaker Ground Truth Demonstration for Image Classification&lt;/a&gt; is an end-to-end example that starts with an unlabeled dataset, labels it using the Ground Truth API, analyzes the results, trains an image classification neural net using the annotated dataset, and finally uses the trained model to perform batch and online inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/ground_truth_object_detection_tutorial"&gt;Ground Truth Object Detection Tutorial&lt;/a&gt; is a similar end-to-end example but for an object detection task.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/data_analysis_of_ground_truth_image_classification_output"&gt;Basic Data Analysis of an Image Classification Output Manifest&lt;/a&gt; presents charts to visualize the number of annotations for each class, differentiating between human annotations and automatic labels (if your job used auto-labeling). It also displays sample images in each class, and creates a pdf which concisely displays the full results.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/object_detection_augmented_manifest_training"&gt;Training a Machine Learning Model Using an Output Manifest&lt;/a&gt; introduces the concept of an "augmented manifest" and demonstrates that the output file of a labeling job can be immediately used as the input file to train a SageMaker machine learning model.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/annotation_consolidation"&gt;Annotation Consolidation&lt;/a&gt; demonstrates Amazon SageMaker Ground Truth annotation consolidation techniques for image classification for a completed labeling job.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-applying-machine-learning" class="anchor" aria-hidden="true" href="#introduction-to-applying-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Applying Machine Learning&lt;/h3&gt;
&lt;p&gt;These examples provide a gentle introduction to machine learning concepts as they are applied in practical use cases across a variety of sectors.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/xgboost_direct_marketing"&gt;Targeted Direct Marketing&lt;/a&gt; predicts potential customers that are most likely to convert based on customer and aggregate level metrics, using Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/xgboost_customer_churn"&gt;Predicting Customer Churn&lt;/a&gt; uses customer interaction and service usage data to find those most likely to churn, and then walks through the cost/benefit trade-offs of providing retention incentives.  This uses Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt; to create a highly predictive model.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/linear_time_series_forecast"&gt;Time-series Forecasting&lt;/a&gt; generates a forecast for topline product demand using Amazon SageMaker's Linear Learner algorithm.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/breast_cancer_prediction"&gt;Cancer Prediction&lt;/a&gt; predicts Breast Cancer based on features derived from images, using SageMaker's Linear Learner.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/ensemble_modeling"&gt;Ensembling&lt;/a&gt; predicts income using two Amazon SageMaker models to show the advantages in ensembling.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/video_game_sales"&gt;Video Game Sales&lt;/a&gt; develops a binary prediction model for the success of video games based on review scores.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/gluon_recommender_system"&gt;MXNet Gluon Recommender System&lt;/a&gt; uses neural network embeddings for non-linear matrix factorization to predict user movie ratings on Amazon digital reviews.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/fair_linear_learner"&gt;Fair Linear Learner&lt;/a&gt; is an example of an effective way to create fair linear models with respect to sensitive features.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans"&gt;Population Segmentation of US Census Data using PCA and Kmeans&lt;/a&gt; analyzes US census data and reduces dimensionality using PCA then clusters US counties using KMeans to identify segments of similar counties.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/object2vec_document_embedding"&gt;Document Embedding using Object2Vec&lt;/a&gt; is an example to embed a large collection of documents in a common low-dimensional space, so that the semantic distances between these documents are preserved.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-sagemaker-automatic-model-tuning" class="anchor" aria-hidden="true" href="#sagemaker-automatic-model-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SageMaker Automatic Model Tuning&lt;/h3&gt;
&lt;p&gt;These examples introduce SageMaker's hyperparameter tuning functionality which helps deliver the best possible predictions by running a large number of training jobs to determine which hyperparameter values are the most impactful.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/xgboost_direct_marketing"&gt;XGBoost Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning to improve your model fits for the &lt;a href="introduction_to_applying_machine_learning/xgboost_direct_marketing"&gt;Targeted Direct Marketing&lt;/a&gt; task.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/tensorflow_mnist"&gt;TensorFlow Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built TensorFlow container and MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/mxnet_mnist"&gt;MXNet Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built MXNet container and MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/keras_bring_your_own"&gt;Keras BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with a custom container running a Keras convolutional network on CIFAR-10 data.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/r_bring_your_own"&gt;R BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the custom container from the &lt;a href="advanced_functionality/r_bring_your_own"&gt;Bring Your Own R Algorithm&lt;/a&gt; example.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/analyze_results"&gt;Analyzing Results&lt;/a&gt; is a shared notebook that can be used after each of the above notebooks to provide analysis on how training jobs with different hyperparameters performed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-amazon-algorithms" class="anchor" aria-hidden="true" href="#introduction-to-amazon-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Amazon Algorithms&lt;/h3&gt;
&lt;p&gt;These examples provide quick walkthroughs to get you up and running with Amazon SageMaker's custom developed algorithms.  Most of these algorithms can train on distributed hardware, scale incredibly well, and are faster and cheaper than popular alternatives.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/1P_kmeans_highlevel"&gt;k-means&lt;/a&gt; is our introductory example for Amazon SageMaker.  It walks through the process of clustering MNIST images of handwritten digits using Amazon SageMaker k-means.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/factorization_machines_mnist"&gt;Factorization Machines&lt;/a&gt; showcases Amazon SageMaker's implementation of the algorithm to predict whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/lda_topic_modeling"&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; introduces topic modeling using Amazon SageMaker Latent Dirichlet Allocation (LDA) on a synthetic dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/linear_learner_mnist"&gt;Linear Learner&lt;/a&gt; predicts whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier from Amazon SageMaker Linear Learner.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/ntm_synthetic"&gt;Neural Topic Model (NTM)&lt;/a&gt; uses Amazon SageMaker Neural Topic Model (NTM) to uncover topics in documents from a synthetic data source, where topic distributions are known.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/pca_mnist"&gt;Principal Components Analysis (PCA)&lt;/a&gt; uses Amazon SageMaker PCA to calculate eigendigits from MNIST.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/seq2seq_translation_en-de"&gt;Seq2Seq&lt;/a&gt; uses the Amazon SageMaker Seq2Seq algorithm that's built on top of &lt;a href="https://github.com/awslabs/sockeye"&gt;Sockeye&lt;/a&gt;, which is a sequence-to-sequence framework for Neural Machine Translation based on MXNet.  Seq2Seq implements state-of-the-art encoder-decoder architectures which can also be used for tasks like Abstractive Summarization in addition to Machine Translation.  This notebook shows translation from English to German text.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/imageclassification_caltech"&gt;Image Classification&lt;/a&gt; includes full training and transfer learning examples of Amazon SageMaker's Image Classification algorithm.  This uses a ResNet deep convolutional neural network to classify images from the caltech dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/xgboost_abalone"&gt;XGBoost for regression&lt;/a&gt; predicts the age of abalone (&lt;a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html" rel="nofollow"&gt;Abalone dataset&lt;/a&gt;) using regression from Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/xgboost_mnist"&gt;XGBoost for multi-class classification&lt;/a&gt; uses Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt; to classify handwritten digits from the MNIST dataset as one of the ten digits using a multi-class classifier. Both single machine and distributed use-cases are presented.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/deepar_synthetic"&gt;DeepAR for time series forecasting&lt;/a&gt; illustrates how to use the Amazon SageMaker DeepAR algorithm for time series forecasting on a synthetically generated data set.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/blazingtext_word2vec_text8"&gt;BlazingText Word2Vec&lt;/a&gt; generates Word2Vec embeddings from a cleaned text dump of Wikipedia articles using SageMaker's fast and scalable BlazingText implementation.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object_detection_pascalvoc_coco"&gt;Object Detection&lt;/a&gt; illustrates how to train an object detector using the Amazon SageMaker Object Detection algorithm with different input formats (RecordIO and image).  It uses the Pascal VOC dataset. A third notebook is provided to demonstrate the use of incremental training.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object_detection_birds"&gt;Object detection for bird images&lt;/a&gt; demonstrates how to use the Amazon SageMaker Object Detection algorithm with a public dataset of Bird images.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_movie_recommendation"&gt;Object2Vec for movie recommendation&lt;/a&gt; demonstrates how Object2Vec can be used to model data consisting of pairs of singleton tokens using movie recommendation as a running example.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_multilabel_genre_classification"&gt;Object2Vec for multi-label classification&lt;/a&gt; shows how ObjectToVec algorithm can train on data consisting of pairs of sequences and singleton tokens using the setting of genre prediction of movies based on their plot descriptions.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_sentence_similarity"&gt;Object2Vec for sentence similarity&lt;/a&gt; explains how to train Object2Vec using sequence pairs as input using sentence similarity analysis as the application.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/ipinsights_login"&gt;IP Insights for suspicious logins&lt;/a&gt; shows how to train IP Insights on a login events for a web server to identify suspicious login attempts.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc"&gt;Semantic Segmentation&lt;/a&gt; shows how to train a semantic segmentation algorithm using the Amazon SageMaker Semantic Segmentation algorithm. It also demonstrates how to host the model and produce segmentaion masks and probability of segmentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-rl" class="anchor" aria-hidden="true" href="#amazon-sagemaker-rl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker RL&lt;/h3&gt;
&lt;p&gt;The following provide examples demonstrating different capabilities of Amazon SageMaker RL.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_cartpole_coach"&gt;Cartpole using Coach&lt;/a&gt; demonstrates the simplest usecase of Amazon SageMaker RL using Intel's RL Coach.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_deepracer_robomaker_coach_gazebo"&gt;AWS DeepRacer&lt;/a&gt; demonstrates AWS DeepRacer trainig using RL Coach in the Gazebo environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_hvac_coach_energyplus"&gt;HVAC using EnergyPlus&lt;/a&gt; demonstrates the training of HVAC systems using the EnergyPlus environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_knapsack_coach_custom"&gt;Knapsack Problem&lt;/a&gt; demonstrates how to solve the knapsack problem using a custom environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_mountain_car_coach_gymEnv"&gt;Mountain Car&lt;/a&gt; Mountain car is a classic RL problem. This notebook explains how to solve this using the OpenAI Gym environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_network_compression_ray_custom"&gt;Distributed Neural Network Compression&lt;/a&gt; This notebook explains how to compress ResNets using RL, using a custom environment and the RLLib toolkit.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_objecttracker_robomaker_coach_gazebo"&gt;Turtlebot Tracker&lt;/a&gt; This notebook demonstrates object tracking using AWS Robomaker and RL Coach in the Gazebo environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_portfolio_management_coach_customEnv"&gt;Portfolio Management&lt;/a&gt; This notebook uses a custom Gym environment to manage multiple financial investments.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_predictive_autoscaling_coach_customEnv"&gt;Autoscaling&lt;/a&gt; demonstrates how to adjust load depending on demand. This uses RL Coach and a custom environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_roboschool_ray"&gt;Roboschool&lt;/a&gt; is an open source physics simulator that is commonly used to train RL policies for robotic systems. This notebook demonstrates training a few agents using it.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_roboschool_stable_baselines"&gt;Stable Baselines&lt;/a&gt; In this notebook example, we will make the HalfCheetah agent learn to walk using the stable-baselines, which are a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_traveling_salesman_vehicle_routing_coach"&gt;Travelling Salesman&lt;/a&gt; is a classic NP hard problem, which this notebook solves with AWS SageMaker RL.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_tic_tac_toe_coach_customEnv"&gt;Tic-tac-toe&lt;/a&gt; is a simple implementation of a custom Gym environment to train and deploy an RL agent in Coach that then plays tic-tac-toe interactively in a Jupyter Notebook.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-scientific-details-of-algorithms" class="anchor" aria-hidden="true" href="#scientific-details-of-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Scientific Details of Algorithms&lt;/h3&gt;
&lt;p&gt;These examples provide more thorough mathematical treatment on a select group of algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/streaming_median"&gt;Streaming Median&lt;/a&gt; sequentially introduces concepts used in streaming algorithms, which many SageMaker algorithms rely on to deliver speed and scalability.&lt;/li&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/lda_topic_modeling"&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; dives into Amazon SageMaker's spectral decomposition approach to LDA.&lt;/li&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/linear_learner_class_weights_loss_functions"&gt;Linear Learner features&lt;/a&gt; shows how to use the class weights and loss functions features of the SageMaker Linear Learner algorithm to improve performance on a credit card fraud prediction task&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-advanced-amazon-sagemaker-functionality" class="anchor" aria-hidden="true" href="#advanced-amazon-sagemaker-functionality"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advanced Amazon SageMaker Functionality&lt;/h3&gt;
&lt;p&gt;These examples that showcase unique functionality available in Amazon SageMaker.  They cover a broad range of topics and will utilize a variety of methods, but aim to provide the user with sufficient insight or inspiration to develop within Amazon SageMaker.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="advanced_functionality/data_distribution_types"&gt;Data Distribution Types&lt;/a&gt; showcases the difference between two methods for sending data from S3 to Amazon SageMaker Training instances.  This has particular implication for scalability and accuracy of distributed training.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/handling_kms_encrypted_data"&gt;Encrypting Your Data&lt;/a&gt; shows how to use Server Side KMS encrypted data with Amazon SageMaker training. The IAM role used for S3 access needs to have permissions to encrypt and decrypt data with the KMS key.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/parquet_to_recordio_protobuf"&gt;Using Parquet Data&lt;/a&gt; shows how to bring &lt;a href="https://parquet.apache.org/" rel="nofollow"&gt;Parquet&lt;/a&gt; data sitting in S3 into an Amazon SageMaker Notebook and convert it into the recordIO-protobuf format that many SageMaker algorithms consume.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/working_with_redshift_data"&gt;Connecting to Redshift&lt;/a&gt; demonstrates how to copy data from Redshift to S3 and vice-versa without leaving Amazon SageMaker Notebooks.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/xgboost_bring_your_own_model"&gt;Bring Your Own XGBoost Model&lt;/a&gt; shows how to use Amazon SageMaker Algorithms containers to bring a pre-trained model to a realtime hosted endpoint without ever needing to think about REST APIs.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/kmeans_bring_your_own_model"&gt;Bring Your Own k-means Model&lt;/a&gt; shows how to take a model that's been fit elsewhere and use Amazon SageMaker Algorithms containers to host it.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/r_bring_your_own"&gt;Bring Your Own R Algorithm&lt;/a&gt; shows how to bring your own algorithm container to Amazon SageMaker using the R language.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/install_r_kernel"&gt;Installing the R Kernel&lt;/a&gt; shows how to install the R kernel into an Amazon SageMaker Notebook Instance.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/scikit_bring_your_own"&gt;Bring Your Own scikit Algorithm&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm for training and production-ready hosting.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/mxnet_mnist_byom"&gt;Bring Your Own MXNet Model&lt;/a&gt; shows how to bring a model trained anywhere using MXNet into Amazon SageMaker.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/tensorflow_iris_byom"&gt;Bring Your Own TensorFlow Model&lt;/a&gt; shows how to bring a model trained anywhere using TensorFlow into Amazon SageMaker.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/inference_pipeline_sparkml_xgboost_abalone"&gt;Inference Pipeline with SparkML and XGBoost&lt;/a&gt; shows how to deploy an Inference Pipeline with SparkML for data pre-processing and XGBoost for training on the Abalone dataset. The pre-processing code is written once and used between training and inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/inference_pipeline_sparkml_blazingtext_dbpedia"&gt;Inference Pipeline with SparkML and BlazingText&lt;/a&gt; shows how to deploy an Inference Pipeline with SparkML for data pre-processing and BlazingText for training on the DBPedia dataset. The pre-processing code is written once and used between training and inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/search"&gt;Experiment Management Capabilities with Search&lt;/a&gt; shows how to organize Training Jobs into projects, and track relationships between Models, Endpoints, and Training Jobs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-neo-compilation-jobs" class="anchor" aria-hidden="true" href="#amazon-sagemaker-neo-compilation-jobs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Neo Compilation Jobs&lt;/h3&gt;
&lt;p&gt;These examples provide you an introduction to how to use Neo to optimizes deep learning model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/imageclassification_caltech"&gt;Image Classification&lt;/a&gt; Adapts form &lt;a href="introduction_to_amazon_algorithms/imageclassification_caltech"&gt;image classification&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/mxnet_mnist"&gt;MNIST with MXNet&lt;/a&gt; Adapts form &lt;a href="sagemaker-python-sdk/mxnet_mnist"&gt;mxnet mnist&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/pytorch_torchvision"&gt;Deploying pre-trained PyTorch vision models&lt;/a&gt; shows how to use Amazon SageMaker Neo to compile and optimize pre-trained PyTorch models from TorchVision.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/tensorflow_distributed_mnist"&gt;Distributed TensorFlow&lt;/a&gt; Adapts form &lt;a href="sagemaker-python-sdk/tensorflow_distributed_mnist"&gt;tensorflow mnist&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/xgboost_customer_churn"&gt;Predicting Customer Churn&lt;/a&gt; Adapts form &lt;a href="introduction_to_applying_machine_learning/xgboost_customer_churn"&gt;xgboost customer churn&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-pre-built-framework-containers-and-the-python-sdk" class="anchor" aria-hidden="true" href="#amazon-sagemaker-pre-built-framework-containers-and-the-python-sdk"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Pre-Built Framework Containers and the Python SDK&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-pre-built-deep-learning-framework-containers" class="anchor" aria-hidden="true" href="#pre-built-deep-learning-framework-containers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-Built Deep Learning Framework Containers&lt;/h4&gt;
&lt;p&gt;These examples show you to write idiomatic TensorFlow or MXNet and then train or host in pre-built containers using SageMaker Python SDK.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_cifar10"&gt;Chainer CIFAR-10&lt;/a&gt; trains a VGG image classification network on CIFAR-10 using Chainer (both single machine and multi-machine versions are included)&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_mnist"&gt;Chainer MNIST&lt;/a&gt; trains a basic neural network on MNIST using Chainer (shows how to use local mode)&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_sentiment_analysis"&gt;Chainer sentiment analysis&lt;/a&gt; trains a LSTM network with embeddings to predict text sentiment using Chainer&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/scikit_learn_iris"&gt;IRIS with Scikit-learn&lt;/a&gt; trains a Scikit-learn classifier on IRIS data&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_cifar10"&gt;CIFAR-10 with MXNet Gluon&lt;/a&gt; trains a ResNet-34  image classification model using MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_mnist"&gt;MNIST with MXNet Gluon&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit dataset using MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_mnist"&gt;MNIST with MXNet&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit data using MXNet's symbolic syntax&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_sentiment"&gt;Sentiment Analysis with MXNet Gluon&lt;/a&gt; trains a text classifier using embeddings with MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_abalone_age_predictor_using_layers"&gt;TensorFlow Neural Networks with Layers&lt;/a&gt; trains a basic neural network on the abalone dataset using TensorFlow layers&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_abalone_age_predictor_using_keras"&gt;TensorFlow Networks with Keras&lt;/a&gt; trains a basic neural network on the abalone dataset using TensorFlow and Keras&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators"&gt;Introduction to Estimators in TensorFlow&lt;/a&gt; trains a DNN classifier estimator on the Iris dataset using TensorFlow&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_resnet_cifar10_with_tensorboard"&gt;TensorFlow and TensorBoard&lt;/a&gt; trains a ResNet image classification model on CIFAR-10 using TensorFlow and showcases how to track results using TensorBoard&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_distributed_mnist"&gt;Distributed TensorFlow&lt;/a&gt; trains a simple convolutional neural network on MNIST using TensorFlow&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-pre-built-machine-learning-framework-containers" class="anchor" aria-hidden="true" href="#pre-built-machine-learning-framework-containers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-Built Machine Learning Framework Containers&lt;/h4&gt;
&lt;p&gt;These examples show you how to build Machine Learning models with frameworks like Apache Spark or Scikit-learn using SageMaker Python SDK.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/sparkml_serving_emr_mleap_abalone"&gt;Inference with SparkML Serving&lt;/a&gt; shows how to build an ML model with Apache Spark using Amazon EMR on Abalone dataset and deploy in SageMaker with SageMaker SparkML Serving.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/scikit_learn_inference_pipeline"&gt;Pipeline Inference with Scikit-learn and LinearLearner&lt;/a&gt; builds a ML pipeline using Scikit-learn preprocessing and LinearLearner algorithm in single endpoint&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-using-amazon-sagemaker-with-apache-spark" class="anchor" aria-hidden="true" href="#using-amazon-sagemaker-with-apache-spark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Amazon SageMaker with Apache Spark&lt;/h3&gt;
&lt;p&gt;These examples show how to use Amazon SageMaker for model training, hosting, and inference through Apache Spark using &lt;a href="https://github.com/aws/sagemaker-spark"&gt;SageMaker Spark&lt;/a&gt;. SageMaker Spark allows you to interleave Spark Pipeline stages with Pipeline stages that interact with Amazon SageMaker.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-spark/pyspark_mnist"&gt;MNIST with SageMaker PySpark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-aws-marketplace" class="anchor" aria-hidden="true" href="#aws-marketplace"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AWS Marketplace&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-create-algorithmsmodel-packages-for-listing-in-aws-marketplace-for-machine-learning" class="anchor" aria-hidden="true" href="#create-algorithmsmodel-packages-for-listing-in-aws-marketplace-for-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Create algorithms/model packages for listing in AWS Marketplace for machine learning.&lt;/h4&gt;
&lt;p&gt;This example shows you how to package a model-package/algorithm for listing in AWS Marketplace for machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/creating_marketplace_products"&gt;Creating Algorithm and Model Package - Listing on AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm to create SageMaker Algorithm and SageMaker Model Package entities that can be used with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs and listed on AWS Marketplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-use-algorithms-and-model-packages-from-aws-marketplace-for-machine-learning" class="anchor" aria-hidden="true" href="#use-algorithms-and-model-packages-from-aws-marketplace-for-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use algorithms and model packages from AWS Marketplace for machine learning.&lt;/h4&gt;
&lt;p&gt;These examples show you how to use model-packages and algorithms from AWS Marketplace for machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_algorithms"&gt;Using Algorithms&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_algorithms/amazon_demo_product"&gt;Using Algorithm From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Algorithm with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_algorithms/automl"&gt;Using AutoML algorithm&lt;/a&gt; provides a detailed walkthrough on how to use AutoML algorithm from AWS Marketplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_model_packages"&gt;Using Model Packages&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/amazon_demo_product"&gt;Using Model Packages From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Model Package entities with the enhanced SageMaker Transform/Hosting APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/auto_insurance"&gt;Using models for extracting vehicle metadata&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of auto-insurance claim processing.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/improving_industrial_workplace_safety"&gt;Using models for identifying non-compliance at a workplace&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of generating summary reports for identifying non-compliance at a construction/industrial workplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-under-development" class="anchor" aria-hidden="true" href="#under-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Under Development&lt;/h3&gt;
&lt;p&gt;These Amazon SageMaker examples fully illustrate a concept, but may require some additional configuration on the users part to complete.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;What do I need in order to get started?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The quickest setup to run example notebooks includes:
&lt;ul&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html" rel="nofollow"&gt;AWS account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Proper &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/authentication-and-access-control.html" rel="nofollow"&gt;IAM User and Role&lt;/a&gt; setup&lt;/li&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html" rel="nofollow"&gt;Amazon SageMaker Notebook Instance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html" rel="nofollow"&gt;S3 bucket&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Will these examples work outside of Amazon SageMaker Notebook Instances?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although most examples utilize key Amazon SageMaker functionality like distributed, managed training or real-time hosted endpoints, these notebooks can be run outside of Amazon SageMaker Notebook Instances with minimal modification (updating IAM role definition and installing the necessary libraries).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;How do I contribute my own example notebook?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although we're extremely excited to receive contributions from the community, we're still working on the best mechanism to take in examples from external sources.  Please bear with us in the short-term if pull requests take longer than expected or are closed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>awslabs</author><guid isPermaLink="false">https://github.com/awslabs/amazon-sagemaker-examples</guid><pubDate>Tue, 19 Nov 2019 00:24:00 GMT</pubDate></item><item><title>mahmoud/awesome-python-applications #25 in Jupyter Notebook, This month</title><link>https://github.com/mahmoud/awesome-python-applications</link><description>&lt;p&gt;&lt;i&gt;💿 Free software that works great, and also happens to be open-source Python. &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-awesome-python-applications" class="anchor" aria-hidden="true" href="#awesome-python-applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome Python Applications&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Case studies in successfully shipping Python software&lt;/em&gt; &lt;a href="https://raw.githubusercontent.com/mahmoud/awesome-python-applications/master/atom.xml" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/mahmoud/awesome-python-applications/master/templates/rss.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/mahmoud/awesome-python-applications/master/templates/snake_cd.png"&gt;&lt;img src="https://raw.githubusercontent.com/mahmoud/awesome-python-applications/master/templates/snake_cd.png" width="30%" align="right" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As developers, we spend our days with code. The site you're reading
this on is mostly modules, packages, libraries, frameworks, and the
like. But users see applications.&lt;/p&gt;
&lt;p&gt;When building our own applications, open-source Python applications
are a gold mine of practical patterns that we know work together. A
production application is worth a thousand blog posts and Stack
Overflow answers.&lt;/p&gt;
&lt;p&gt;This document is an always-growing list of &lt;strong&gt;385&lt;/strong&gt;
open-source Python applications arranged by topic, with links to
repositories, docs, and more, generated from &lt;a href="https://github.com/mahmoud/awesome-python-applications/blob/master/projects.yaml"&gt;structured
data&lt;/a&gt;
using &lt;a href="https://github.com/mahmoud/apatite"&gt;apatite&lt;/a&gt;. If you have one
to add or find some information missing, &lt;a href="https://github.com/mahmoud/awesome-python-applications/issues"&gt;please let us
know&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Read &lt;a href="http://sedimental.org/awesome_python_applications.html" rel="nofollow"&gt;&lt;strong&gt;the announcement post&lt;/strong&gt;&lt;/a&gt; to learn more about this list.&lt;br&gt;
Subscribe to &lt;a href="https://raw.githubusercontent.com/mahmoud/awesome-python-applications/master/atom.xml" rel="nofollow"&gt;&lt;strong&gt;the RSS/Atom feed&lt;/strong&gt;&lt;/a&gt; to see new applications added.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#tag-internet"&gt;Internet&lt;/a&gt; &lt;em&gt;(32)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-audio"&gt;Audio&lt;/a&gt; &lt;em&gt;(17)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-video"&gt;Video&lt;/a&gt; &lt;em&gt;(7)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-graphics"&gt;Graphics&lt;/a&gt; &lt;em&gt;(20)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-games"&gt;Games&lt;/a&gt; &lt;em&gt;(10)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-productivity"&gt;Productivity&lt;/a&gt; &lt;em&gt;(24)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-organization"&gt;Organization&lt;/a&gt; &lt;em&gt;(38)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-communication"&gt;Communication&lt;/a&gt; &lt;em&gt;(33)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-education"&gt;Education&lt;/a&gt; &lt;em&gt;(8)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-science"&gt;Science&lt;/a&gt; &lt;em&gt;(22)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-cms"&gt;CMS&lt;/a&gt; &lt;em&gt;(11)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-erp"&gt;ERP&lt;/a&gt; &lt;em&gt;(5)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-static_site"&gt;Static Site&lt;/a&gt; &lt;em&gt;(9)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-dev"&gt;Dev&lt;/a&gt; &lt;em&gt;(166)&lt;/em&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#tag-dev.scm"&gt;SCM&lt;/a&gt; &lt;em&gt;(17)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-dev.code_review"&gt;Code Review&lt;/a&gt; &lt;em&gt;(4)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-dev.storage"&gt;Storage&lt;/a&gt; &lt;em&gt;(16)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-dev.ops"&gt;Ops&lt;/a&gt; &lt;em&gt;(25)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-dev.security"&gt;Security&lt;/a&gt; &lt;em&gt;(24)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-dev.docs"&gt;Docs&lt;/a&gt; &lt;em&gt;(7)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-dev.editor"&gt;Editor&lt;/a&gt; &lt;em&gt;(12)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-dev.pkg_mgr"&gt;Package Managers&lt;/a&gt; &lt;em&gt;(10)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-dev.pkg_repo"&gt;Package Repositories&lt;/a&gt; &lt;em&gt;(5)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-dev.build"&gt;Build&lt;/a&gt; &lt;em&gt;(13)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-dev.shell"&gt;Shell&lt;/a&gt; &lt;em&gt;(3)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-dev-other"&gt;Other Dev projects&lt;/a&gt; &lt;em&gt;(31)&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#tag-misc"&gt;Misc&lt;/a&gt; &lt;em&gt;(12)&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;&lt;a id="user-content-internet" class="anchor" aria-hidden="true" href="#internet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-internet" href="#tag-internet"&gt;Internet&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ArchiveBox&lt;/strong&gt; - (&lt;a href="https://github.com/pirate/ArchiveBox"&gt;Repo&lt;/a&gt;, &lt;a href="https://archivebox.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://github.com/pirate/ArchiveBox/wiki"&gt;Docs&lt;/a&gt;) Self-hosted web archive, for creating local, browsable backups of content from the web. Imports HTML, JS, PDFs, video, subtitles, git repositories, and more, from Pocket, Pinboard, browser history, etc. &lt;code&gt;(organization, linux, windows, docker)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;archivematica&lt;/strong&gt; - (&lt;a href="https://github.com/artefactual/archivematica"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.archivematica.org/en" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.archivematica.org/en/docs" rel="nofollow"&gt;Docs&lt;/a&gt;) Digital preservation system designed to maintain standards-based, long-term access to collections of digital objects, targeted at archivists and librarians. &lt;code&gt;(organization, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Canto&lt;/strong&gt; - (&lt;a href="https://github.com/themoken/canto-next"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Canto_%28news_aggregator%29" rel="nofollow"&gt;WP&lt;/a&gt;) RSS daemon and &lt;a href="https://github.com/themoken/canto-curses"&gt;curses-based client&lt;/a&gt;. &lt;code&gt;(linux, tui)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deluge&lt;/strong&gt; - (&lt;a href="https://github.com/deluge-torrent/deluge"&gt;Repo&lt;/a&gt;, &lt;a href="https://deluge-torrent.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Deluge_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://www.patreon.com/deluge_cas" rel="nofollow"&gt;Fund&lt;/a&gt;) Popular, lightweight, cross-platform BitTorrent client. &lt;code&gt;(linux, windows, mac, server, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Elixire&lt;/strong&gt; - (&lt;a href="https://gitlab.com/elixire/elixire" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://elixi.re/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://gitlab.com/elixire/api-docs" rel="nofollow"&gt;Docs&lt;/a&gt;) Featureful file host and link shortener with API and support for multiple vanity urls. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FlaskBB&lt;/strong&gt; - (&lt;a href="https://github.com/flaskbb/flaskbb"&gt;Repo&lt;/a&gt;, &lt;a href="https://flaskbb.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://forums.flaskbb.org/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://flaskbb.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) A classic web forum application (bulletin board) with a modern look. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;gPodder&lt;/strong&gt; - (&lt;a href="https://github.com/gpodder/gpodder"&gt;Repo&lt;/a&gt;, &lt;a href="https://gpodder.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Simple, mature media aggregator and podcast client. &lt;code&gt;(linux, windows, mac, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hosts&lt;/strong&gt; - (&lt;a href="https://github.com/StevenBlack/hosts"&gt;Repo&lt;/a&gt;) Command-line application which merges reputable &lt;a href="https://en.wikipedia.org/wiki/Hosts_(file)" rel="nofollow"&gt;hosts files&lt;/a&gt; with deduplication for the purpose of blocking undesirable websites via DNS blackhole. &lt;code&gt;(security, linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;httpie&lt;/strong&gt; - (&lt;a href="https://github.com/jakubroztocil/httpie"&gt;Repo&lt;/a&gt;, &lt;a href="https://httpie.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/httpie" rel="nofollow"&gt;PyPI&lt;/a&gt;) Command-line HTTP client with JSON support, syntax highlighting, wget-like downloads, extensions, and more. &lt;code&gt;(dev, linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Isso&lt;/strong&gt; - (&lt;a href="https://github.com/posativ/isso"&gt;Repo&lt;/a&gt;, &lt;a href="https://posativ.org/isso" rel="nofollow"&gt;Home&lt;/a&gt;) Lightweight commenting server, designed as a drop-in replacement for Disqus. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KindleEar&lt;/strong&gt; - (&lt;a href="https://github.com/cdhigh/KindleEar"&gt;Repo&lt;/a&gt;, &lt;a href="https://github.com/cdhigh/KindleEar/blob/master/readme_EN.md"&gt;Docs&lt;/a&gt;) Web application to automatically aggregate RSS into periodical mobi/epub files with images and send it to your kindle or your email. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mylar&lt;/strong&gt; - (&lt;a href="https://github.com/evilhero/mylar"&gt;Repo&lt;/a&gt;) A web-based automated comic book downloader (cbr/cbz) for use with SABnzbd, NZBGet, and torrents. &lt;code&gt;(graphics, linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neubot&lt;/strong&gt; - (&lt;a href="https://github.com/neubot/neubot"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.neubot.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Lightweight agent which collects data for net-neutrality research. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NewsBlur&lt;/strong&gt; - (&lt;a href="https://github.com/samuelclay/NewsBlur"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.newsblur.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Web-based personal news reader. &lt;code&gt;(server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Newspipe&lt;/strong&gt; - (&lt;a href="https://gitlab.com/newspipe/newspipe" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="http://newspipe.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.newspipe.org/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://github.com/newspipe/newspipe"&gt;gh&lt;/a&gt;, &lt;a href="https://newspipe.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based news aggregator and reader. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nsupdate.info&lt;/strong&gt; - (&lt;a href="https://github.com/nsupdate-info/nsupdate.info"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/nsupdate" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://nsupdateinfo.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Featureful dynamic DNS service, using the Dynamic DNS UPDATE protocol (&lt;a href="https://tools.ietf.org/html/rfc2136" rel="nofollow"&gt;RFC 2136&lt;/a&gt;) to update BIND and other major nameservers. &lt;code&gt;(ops, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nyaa&lt;/strong&gt; - (&lt;a href="https://github.com/nyaadevs/nyaa"&gt;Repo&lt;/a&gt;) Bittorrent tracker software built for anime site &lt;a href="https://nyaa.si/" rel="nofollow"&gt;nyaa.si&lt;/a&gt;. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pi-Hole&lt;/strong&gt; - (&lt;a href="https://github.com/pi-hole/pi-hole"&gt;Repo&lt;/a&gt;, &lt;a href="https://pi-hole.net/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Pi-hole" rel="nofollow"&gt;WP&lt;/a&gt;) Linux network-level advertisement and internet tracker blocking application which acts as a DNS sinkhole, and (optionally) a DHCP server, intended for use on a private network. &lt;code&gt;(linux, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Planet&lt;/strong&gt; - (&lt;a href="https://github.com/python/planet"&gt;Repo&lt;/a&gt;, &lt;a href="https://web.archive.org/web/20051029095046/http%3A/www.planetplanet.org" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Planet_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) RSS and Atom feed aggregator, designed to collect posts from the weblogs of members of an Internet community and display them on a single page. Used to power &lt;a href="https://planetpython.org/" rel="nofollow"&gt;Planet Python&lt;/a&gt; and many more. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pol&lt;/strong&gt; - (&lt;a href="https://github.com/taroved/pol"&gt;Repo&lt;/a&gt;, &lt;a href="https://politepol.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Web application which allows users to subscribe to changes on a web site via an autogenerated RSS feed. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyLoad&lt;/strong&gt; - (&lt;a href="https://github.com/pyload/pyload"&gt;Repo&lt;/a&gt;, &lt;a href="https://pyload.net/" rel="nofollow"&gt;Home&lt;/a&gt;) Download manager with a web interface and API. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Qute Browser&lt;/strong&gt; - (&lt;a href="https://github.com/qutebrowser/qutebrowser"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.qutebrowser.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Keyboard-driven, minimal, &lt;code&gt;vim&lt;/code&gt;-like browser based on PyQt5. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reddit&lt;/strong&gt; - (&lt;a href="https://github.com/reddit-archive/reddit"&gt;Repo&lt;/a&gt;, &lt;a href="http://reddit.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Social news forum with voting, commenting, karma, and more. (Archival repo from 2017.) &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SABnzbd&lt;/strong&gt; - (&lt;a href="https://github.com/sabnzbd/sabnzbd"&gt;Repo&lt;/a&gt;, &lt;a href="https://sabnzbd.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://sabnzbd.org/wiki" rel="nofollow"&gt;Docs&lt;/a&gt;) Simple, cross-platform newsreader for downloading from Usenet. Supports many integrations and 16 languages. &lt;code&gt;(linux, windows, mac, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Searx&lt;/strong&gt; - (&lt;a href="https://github.com/asciimoo/searx"&gt;Repo&lt;/a&gt;, &lt;a href="https://asciimoo.github.io/searx" rel="nofollow"&gt;Docs&lt;/a&gt;) Self-hosted metasearch engine, aggregating results from more than 70 services while avoiding tracking and profiling. &lt;code&gt;(security, server, flask)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;speedtest-cli&lt;/strong&gt; - (&lt;a href="https://github.com/sivel/speedtest-cli"&gt;Repo&lt;/a&gt;) Command-line interface for testing Internet bandwidth using &lt;a href="https://speedtest.net" rel="nofollow"&gt;speedtest.net&lt;/a&gt;. &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;streamlink&lt;/strong&gt; - (&lt;a href="https://github.com/streamlink/streamlink"&gt;Repo&lt;/a&gt;, &lt;a href="https://streamlink.github.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/streamlink" rel="nofollow"&gt;PyPI&lt;/a&gt;) Command-line utility that extracts streams from various services and pipes them into a video player of choice. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;syncserver&lt;/strong&gt; - (&lt;a href="https://github.com/mozilla-services/syncserver"&gt;Repo&lt;/a&gt;, &lt;a href="https://mozilla-services.readthedocs.io/en/latest/howtos/run-sync-1.5.html" rel="nofollow"&gt;Docs&lt;/a&gt;) All-in-one package for running a self-hosted Mozilla Firefox Sync server. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tribler&lt;/strong&gt; - (&lt;a href="https://github.com/Tribler/tribler"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.tribler.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Tribler" rel="nofollow"&gt;WP&lt;/a&gt;) Privacy enhanced BitTorrent client with P2P content discovery. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;You-Get&lt;/strong&gt; - (&lt;a href="https://github.com/soimort/you-get"&gt;Repo&lt;/a&gt;, &lt;a href="https://you-get.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Command-line program to browserlessly scrape and stream video, audio, and images from web sites. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;youtube-dl&lt;/strong&gt; - (&lt;a href="https://github.com/rg3/youtube-dl"&gt;Repo&lt;/a&gt;, &lt;a href="http://rg3.github.io/youtube-dl" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/youtube_dl" rel="nofollow"&gt;PyPI&lt;/a&gt;) Command-line program to browserlessly archive video and audio from YouTube and hundreds of other sites. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ZeroNet&lt;/strong&gt; - (&lt;a href="https://github.com/HelloZeroNet/ZeroNet"&gt;Repo&lt;/a&gt;, &lt;a href="https://zeronet.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/ZeroNet" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://zeronet.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Open, free, and uncensorable websites, using Bitcoin cryptography and BitTorrent network. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-audio" class="anchor" aria-hidden="true" href="#audio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-audio" href="#tag-audio"&gt;Audio&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Beets&lt;/strong&gt; - (&lt;a href="https://github.com/beetbox/beets"&gt;Repo&lt;/a&gt;, &lt;a href="http://beets.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/beets" rel="nofollow"&gt;PyPI&lt;/a&gt;) Feature-rich command-line music library manager with web UI, duplicate detection, transcoding, and tagging support, integrating with MusicBrainz, Discogs, and more. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exaile&lt;/strong&gt; - (&lt;a href="https://github.com/exaile/exaile"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Exaile" rel="nofollow"&gt;WP&lt;/a&gt;) Cross-platform audio player, tag editor, and library organizer. &lt;code&gt;(linux, windows, mac, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frescobaldi&lt;/strong&gt; - (&lt;a href="https://github.com/wbsoft/frescobaldi"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Frescobaldi_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) An editor for &lt;a href="https://en.wikipedia.org/wiki/LilyPond" rel="nofollow"&gt;LilyPond&lt;/a&gt; music files. &lt;code&gt;(linux, windows, mac, qt)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Friture&lt;/strong&gt; - (&lt;a href="https://github.com/tlecomte/friture"&gt;Repo&lt;/a&gt;, &lt;a href="http://friture.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Visualizes and analyzes live audio data in real-time, including scope, spectrum analyzer, rolling 2D spectrogram, and more. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Funkwhale&lt;/strong&gt; - (&lt;a href="https://dev.funkwhale.audio/funkwhale/funkwhale" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://funkwhale.audio/en_US" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://docs.funkwhale.audio/" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based, community-driven project that lets you listen and share music and audio within a decentralized, open network. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GNU Radio&lt;/strong&gt; - (&lt;a href="https://github.com/gnuradio/gnuradio"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.gnuradio.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/GNU_Radio" rel="nofollow"&gt;WP&lt;/a&gt;) Software development toolkit that provides signal processing blocks to implement software-defined radios and signal-processing systems. &lt;code&gt;(linux, windows, mac, cpp, qt)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GNU Solfege&lt;/strong&gt; - (&lt;a href="http://git.savannah.gnu.org/cgit/solfege.git" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/GNU_Solfege" rel="nofollow"&gt;WP&lt;/a&gt;) An ear-training program intended to help musicians improve their skills. &lt;code&gt;(linux, windows, mac, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mopidy&lt;/strong&gt; - (&lt;a href="https://github.com/mopidy/mopidy"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.mopidy.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Extensible music player server with plugin support for a wide range of services. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Music Player&lt;/strong&gt; - (&lt;a href="https://github.com/albertz/music-player"&gt;Repo&lt;/a&gt;, &lt;a href="http://albertz.github.io/music-player" rel="nofollow"&gt;Home&lt;/a&gt;) A simple music player designed around an infinite intelligent playlist, with support for headless playback. &lt;code&gt;(linux, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MusicBrainz Picard&lt;/strong&gt; - (&lt;a href="https://github.com/metabrainz/picard"&gt;Repo&lt;/a&gt;, &lt;a href="https://picard.musicbrainz.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/MusicBrainz_Picard" rel="nofollow"&gt;WP&lt;/a&gt;) Automatically identify, tag, and organize music albums and other digital audio recordings. &lt;code&gt;(linux, windows, mac, qt)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Musikernel&lt;/strong&gt; - (&lt;a href="https://github.com/j3ffhubb/musikernel"&gt;Repo&lt;/a&gt;) All-in-one Digital Audio Workstation (DAW) with a suite of instrument and effect plugins. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PuddleTag&lt;/strong&gt; - (&lt;a href="https://github.com/keithgg/puddletag"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Puddletag" rel="nofollow"&gt;WP&lt;/a&gt;) An audio tag (metadata) editor for audio file formats. &lt;code&gt;(linux, qt4)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Quod Libet&lt;/strong&gt; - (&lt;a href="https://github.com/quodlibet/quodlibet"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Quod_Libet_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) Cross-platform audio player, tag editor, and library organizer. &lt;code&gt;(linux, windows, mac, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SoundConverter&lt;/strong&gt; - (&lt;a href="https://github.com/kassoulet/soundconverter"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/GNOME_SoundConverter" rel="nofollow"&gt;WP&lt;/a&gt;) A GNOME-based audio file transcoder. &lt;code&gt;(linux, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SoundGrain&lt;/strong&gt; - (&lt;a href="https://github.com/belangeo/soundgrain"&gt;Repo&lt;/a&gt;, &lt;a href="http://ajaxsoundstudio.com/software/soundgrain" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;amp;hosted_button_id=9CA99DH6ES3HA" rel="nofollow"&gt;Fund&lt;/a&gt;) Graphical interface designed for drawing and editing trajectories to control &lt;a href="https://en.wikipedia.org/wiki/Granular_synthesis" rel="nofollow"&gt;granular sound synthesis&lt;/a&gt;. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Supysonic&lt;/strong&gt; - (&lt;a href="https://github.com/spl0k/supysonic"&gt;Repo&lt;/a&gt;) Implementation of the &lt;a href="http://www.subsonic.org/" rel="nofollow"&gt;Subsonic server API&lt;/a&gt;, with support for browsing, streaming, transcoding, scrobbling, and more. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Whipper&lt;/strong&gt; - (&lt;a href="https://github.com/whipper-team/whipper"&gt;Repo&lt;/a&gt;) A CLI-based CD Audio ripper designed for accuracy over speed, with support for overriding hardware caches, accuracy verification, MusicBrainz metadata lookup, hidden tracks, FLAC, and much more. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-video" class="anchor" aria-hidden="true" href="#video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-video" href="#tag-video"&gt;Video&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Flowblade&lt;/strong&gt; - (&lt;a href="https://github.com/jliljebl/flowblade"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Flowblade" rel="nofollow"&gt;WP&lt;/a&gt;) Multitrack, non-linear video editing software for Linux. &lt;code&gt;(linux, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open Streaming Platform&lt;/strong&gt; - (&lt;a href="https://gitlab.com/Deamos/flask-nginx-rtmp-manager" rel="nofollow"&gt;Repo&lt;/a&gt;) Self-hosted video streaming and recording server, designed as an alternative to Twitch and YouTube. &lt;code&gt;(games, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenShot&lt;/strong&gt; - (&lt;a href="https://github.com/OpenShot/openshot-qt"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.openshot.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/OpenShot" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://www.patreon.com/openshot" rel="nofollow"&gt;Fund&lt;/a&gt;) A cross-platform video editor for FreeBSD, Linux, macOS, and Windows. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pitivi&lt;/strong&gt; - (&lt;a href="https://gitlab.gnome.org/GNOME/pitivi" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Pitivi" rel="nofollow"&gt;WP&lt;/a&gt;) Non-linear video editor for Linux, based on GStreamer. &lt;code&gt;(linux, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plumi&lt;/strong&gt; - (&lt;a href="https://github.com/plumi/plumi.app"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Plumi" rel="nofollow"&gt;WP&lt;/a&gt;) Video sharing content management system based on &lt;a href="https://en.wikipedia.org/wiki/Plone_(software)" rel="nofollow"&gt;Plone&lt;/a&gt;. &lt;code&gt;(cms, server, plone)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyVideo&lt;/strong&gt; - (&lt;a href="https://github.com/pyvideo/pyvideo"&gt;Repo&lt;/a&gt;, &lt;a href="https://pyvideo.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Static media index custom-built for the Python community, and all the content our meetings and conferences produce. &lt;code&gt;(static_site, linux, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vidcutter&lt;/strong&gt; - (&lt;a href="https://github.com/ozmartian/vidcutter"&gt;Repo&lt;/a&gt;) GUI and CLI aiming to be the fastest and simplest way to cut and join video. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-graphics" class="anchor" aria-hidden="true" href="#graphics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-graphics" href="#tag-graphics"&gt;Graphics&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;cartoonify / Draw This.&lt;/strong&gt; - (&lt;a href="https://github.com/danmacnish/cartoonify"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.kapwing.com/cartoonify" rel="nofollow"&gt;Home&lt;/a&gt;) Turn a photograph into a toddler's drawing. Automatically! &lt;code&gt;(console, docker, hardware)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cura&lt;/strong&gt; - (&lt;a href="https://github.com/Ultimaker/Cura"&gt;Repo&lt;/a&gt;, &lt;a href="https://ultimaker.com/software/ultimaker-cura" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Cura_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://ultimaker.com/en/resources/manuals/software" rel="nofollow"&gt;Docs&lt;/a&gt;) Popular desktop software for preparation and control of 3D printing, integrated with CAD workflows. &lt;code&gt;(linux, windows, mac, corp, hardware)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DrawBot&lt;/strong&gt; - (&lt;a href="https://github.com/typemytype/drawbot"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.drawbot.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/DrawBot" rel="nofollow"&gt;WP&lt;/a&gt;) A powerful programmatic 2D drawing application for MacOS X which generates graphics from Python scripts. &lt;code&gt;(education, dev, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FreeCAD&lt;/strong&gt; - (&lt;a href="https://github.com/FreeCAD/FreeCAD"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/FreeCAD" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://salt.bountysource.com/teams/freecad" rel="nofollow"&gt;Fund&lt;/a&gt;) General-purpose parametric 3D CAD modeler and a building information modeling (BIM) software with finite-element-method (FEM) support. &lt;code&gt;(linux, windows, mac, cpp, qt)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gaphor&lt;/strong&gt; - (&lt;a href="https://github.com/gaphor/gaphor"&gt;Repo&lt;/a&gt;, &lt;a href="https://gaphor.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Simple &lt;a href="https://en.wikipedia.org/wiki/Unified_Modeling_Language" rel="nofollow"&gt;UML&lt;/a&gt; modeling tool designed for beginners. &lt;code&gt;(docs, linux, windows, mac, flatpak, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lector&lt;/strong&gt; - (&lt;a href="https://github.com/BasioMeusPuga/Lector"&gt;Repo&lt;/a&gt;) Desktop ebook reader and browser, with support for many formats, including comic book archives. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MakeHuman&lt;/strong&gt; - (&lt;a href="https://bitbucket.org/MakeHuman/makehuman" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/MakeHuman" rel="nofollow"&gt;WP&lt;/a&gt;) 3D computer graphics software designed for the prototyping of photo realistic humanoids. &lt;code&gt;(linux, windows, mac, qt)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Meshroom&lt;/strong&gt; - (&lt;a href="https://github.com/alicevision/meshroom"&gt;Repo&lt;/a&gt;, &lt;a href="http://alicevision.github.io/" rel="nofollow"&gt;Home&lt;/a&gt;) Photogrammetry pipeline, for turning photographs into 3D models. &lt;code&gt;(linux, windows, mac, qt)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mylar&lt;/strong&gt; - (&lt;a href="https://github.com/evilhero/mylar"&gt;Repo&lt;/a&gt;) A web-based automated comic book downloader (cbr/cbz) for use with SABnzbd, NZBGet, and torrents. &lt;code&gt;(internet, linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MyPaint&lt;/strong&gt; - (&lt;a href="https://github.com/mypaint/mypaint"&gt;Repo&lt;/a&gt;, &lt;a href="http://mypaint.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/MyPaint" rel="nofollow"&gt;WP&lt;/a&gt;) Raster graphics editor for digital painters with a focus on painting rather than image manipulation. &lt;code&gt;(linux, windows, mac, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NFO Viewer&lt;/strong&gt; - (&lt;a href="https://github.com/otsaloma/nfoview"&gt;Repo&lt;/a&gt;, &lt;a href="https://otsaloma.io/nfoview" rel="nofollow"&gt;Home&lt;/a&gt;) A simple viewer for NFO files and the ASCII art therein, with preset fonts, encodings, automatic window sizing, and clickable hyperlinks. &lt;code&gt;(misc, linux, windows)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OCRFeeder&lt;/strong&gt; - (&lt;a href="https://gitlab.gnome.org/GNOME/ocrfeeder" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/OCRFeeder" rel="nofollow"&gt;WP&lt;/a&gt;) An optical character recognition suite for GNOME, with support for command-line OCR engines like CuneiForm, GOCR, Ocrad and Tesseract. &lt;code&gt;(linux, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OCRopus&lt;/strong&gt; - (&lt;a href="https://github.com/tmbdev/ocropy"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/OCRopus" rel="nofollow"&gt;WP&lt;/a&gt;) Document analysis and optical character recognition (OCR) system. &lt;code&gt;(linux, mac, console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Octoprint&lt;/strong&gt; - (&lt;a href="https://github.com/foosel/OctoPrint"&gt;Repo&lt;/a&gt;, &lt;a href="https://octoprint.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.patreon.com/foosel" rel="nofollow"&gt;Fund&lt;/a&gt;) Web-based controller for consumer 3D printers. &lt;code&gt;(server, flask, hardware)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PhotoCollage&lt;/strong&gt; - (&lt;a href="https://github.com/adrienverge/PhotoCollage"&gt;Repo&lt;/a&gt;) Automatically lays out a photo collage to fill out a given poster space. &lt;code&gt;(linux, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Photonix&lt;/strong&gt; - (&lt;a href="https://github.com/damianmoore/photonix"&gt;Repo&lt;/a&gt;, &lt;a href="https://photonix.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://demo.photonix.org/" rel="nofollow"&gt;Demo&lt;/a&gt;) Web-based photo management, featuring smart filtering with object recognition, location awareness, color analysis, and more. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pynocchio&lt;/strong&gt; - (&lt;a href="https://github.com/mstuttgart/pynocchio"&gt;Repo&lt;/a&gt;, &lt;a href="https://mstuttgart.github.io/pynocchio" rel="nofollow"&gt;Home&lt;/a&gt;) Minimalist comic reader, supporting many common image and archive formats. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Quru Image Server&lt;/strong&gt; - (&lt;a href="https://github.com/quru/qis"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.quruimageserver.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://images.quru.com/demo" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://github.com/quru/qis/blob/master/doc/overview.md"&gt;Docs&lt;/a&gt;) High-performance web server for creating and delivering dynamic images. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SK1&lt;/strong&gt; - (&lt;a href="https://github.com/sk1project/sk1-wx"&gt;Repo&lt;/a&gt;, &lt;a href="https://sk1project.net/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/SK1_%28program%29" rel="nofollow"&gt;WP&lt;/a&gt;) Feature-rich, cross-platform illustration program. &lt;code&gt;(linux, windows, mac, gtk, wx)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Thumbor&lt;/strong&gt; - (&lt;a href="https://github.com/thumbor/thumbor"&gt;Repo&lt;/a&gt;, &lt;a href="http://thumbor.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://thumbor.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Photo thumbnail service with resizing, flipping, and smart cropping of images. &lt;code&gt;(dev, server)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-games" class="anchor" aria-hidden="true" href="#games"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-games" href="#tag-games"&gt;Games&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Cataclysm: Dark Days Ahead (Launcher)&lt;/strong&gt; - (&lt;a href="https://github.com/remyroy/CDDA-Game-Launcher"&gt;Repo&lt;/a&gt;, &lt;a href="https://cataclysmdda.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Launcher for popular FOSS game &lt;a href="https://cataclysmdda.org/" rel="nofollow"&gt;CDDA&lt;/a&gt;, which supports automatic updates and mod management. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frets on Fire X&lt;/strong&gt; - (&lt;a href="https://github.com/fofix/fofix"&gt;Repo&lt;/a&gt;) Highly customizable rhythm game supporting many modes of guitar, bass, drum, and vocal gameplay for up to four players. &lt;code&gt;(linux, windows, pygame)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lucas Chess&lt;/strong&gt; - (&lt;a href="https://github.com/lukasmonk/lucaschess"&gt;Repo&lt;/a&gt;, &lt;a href="http://lucaschess.pythonanywhere.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Featureful chess client for Windows, with some Linux support. &lt;code&gt;(linux, windows, qt4)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lutris&lt;/strong&gt; - (&lt;a href="https://github.com/lutris/lutris"&gt;Repo&lt;/a&gt;, &lt;a href="https://lutris.net/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Lutris" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://www.patreon.com/lutris" rel="nofollow"&gt;Fund&lt;/a&gt;) Gaming platform for GNU/Linux, managing game installations with a unified interface. &lt;code&gt;(linux, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open Streaming Platform&lt;/strong&gt; - (&lt;a href="https://gitlab.com/Deamos/flask-nginx-rtmp-manager" rel="nofollow"&gt;Repo&lt;/a&gt;) Self-hosted video streaming and recording server, designed as an alternative to Twitch and YouTube. &lt;code&gt;(video, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyChess&lt;/strong&gt; - (&lt;a href="https://github.com/pychess/pychess"&gt;Repo&lt;/a&gt;, &lt;a href="http://pychess.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/PyChess" rel="nofollow"&gt;WP&lt;/a&gt;) Advanced chess client, suitable for new, casual, and competitive play. &lt;code&gt;(linux, windows, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pyfa&lt;/strong&gt; - (&lt;a href="https://github.com/pyfa-org/Pyfa"&gt;Repo&lt;/a&gt;) Python Fitting Assistant, cross-platform experimentation tool for &lt;a href="https://en.wikipedia.org/wiki/Eve_Online" rel="nofollow"&gt;EVE Online&lt;/a&gt; ship fittings. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PySolFC&lt;/strong&gt; - (&lt;a href="https://github.com/shlomif/PySolFC"&gt;Repo&lt;/a&gt;, &lt;a href="https://pysolfc.sourceforge.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://f-droid.org/en/packages/org.lufebe16.pysolfc" rel="nofollow"&gt;Android&lt;/a&gt;) Highly-portable collection of solitaire card games. &lt;code&gt;(linux, windows, android, kivy, tk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;term2048&lt;/strong&gt; - (&lt;a href="https://github.com/bfontaine/term2048"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.python.org/pypi/term2048" rel="nofollow"&gt;PyPI&lt;/a&gt;) TUI version of &lt;a href="http://gabrielecirulli.github.io/2048/" rel="nofollow"&gt;2048&lt;/a&gt;. &lt;code&gt;(linux, mac, tui)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unknown Horizons&lt;/strong&gt; - (&lt;a href="https://github.com/unknown-horizons/unknown-horizons"&gt;Repo&lt;/a&gt;, &lt;a href="http://unknown-horizons.org/" rel="nofollow"&gt;Home&lt;/a&gt;) 2D real-time strategy simulation with an emphasis on economy and city building. (Not unlike Age of Empires) &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-productivity" class="anchor" aria-hidden="true" href="#productivity"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-productivity" href="#tag-productivity"&gt;Productivity&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Autokey&lt;/strong&gt; - (&lt;a href="https://github.com/autokey/autokey"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/AutoKey" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://pypi.org/project/autokey" rel="nofollow"&gt;PyPI&lt;/a&gt;) Desktop automation utility for Linux and X11. &lt;code&gt;(linux, gtk, qt)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bleachbit&lt;/strong&gt; - (&lt;a href="https://github.com/bleachbit/bleachbit"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.bleachbit.org/" rel="nofollow"&gt;Home&lt;/a&gt;) System cleaner designed to free disk space and maintain privacy. &lt;code&gt;(linux, windows, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BorgBackup&lt;/strong&gt; - (&lt;a href="https://github.com/borgbackup/borg"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.borgbackup.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Deduplicating backup system with optional encryption and other features. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bup&lt;/strong&gt; - (&lt;a href="https://github.com/Bup/Bup"&gt;Repo&lt;/a&gt;, &lt;a href="https://bup.github.io/" rel="nofollow"&gt;Home&lt;/a&gt;) Efficient backup system based on the git packfile format, providing fast incremental saves and global deduplication. &lt;code&gt;(linux, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Excalibur&lt;/strong&gt; - (&lt;a href="https://github.com/camelot-dev/excalibur"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.tryexcalibur.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Web interface to extract tabular data from PDFs. &lt;code&gt;(linux, windows)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Glances&lt;/strong&gt; - (&lt;a href="https://github.com/nicolargo/glances"&gt;Repo&lt;/a&gt;, &lt;a href="https://nicolargo.github.io/glances" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://glances.readthedocs.io/en/stable" rel="nofollow"&gt;Docs&lt;/a&gt;) A cross-platform top/htop alternative, providing an overview of system resources. &lt;code&gt;(ops, linux, windows, mac, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;gmvault&lt;/strong&gt; - (&lt;a href="https://github.com/gaubert/gmvault"&gt;Repo&lt;/a&gt;, &lt;a href="http://gmvault.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Tool for backing up gmail accounts. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gridsync&lt;/strong&gt; - (&lt;a href="https://github.com/gridsync/gridsync"&gt;Repo&lt;/a&gt;) Cross-platform GUI built to synchronize local directories with Tahoe-LAFS storage grids. &lt;code&gt;(storage, linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GTimeLog&lt;/strong&gt; - (&lt;a href="https://github.com/gtimelog/gtimelog"&gt;Repo&lt;/a&gt;, &lt;a href="https://gtimelog.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://ko-fi.com/mgedmin" rel="nofollow"&gt;Fund&lt;/a&gt;, &lt;a href="https://gtimelog.org/docs.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Desktop-based time tracker with support for logging billable/non-billable work. &lt;code&gt;(organization, linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kibitzr&lt;/strong&gt; - (&lt;a href="https://github.com/kibitzr/kibitzr"&gt;Repo&lt;/a&gt;, &lt;a href="https://kibitzr.github.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/kibitzr" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://kibitzr.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Self-hosted personal assistant server for automating routine tasks. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mackup&lt;/strong&gt; - (&lt;a href="https://github.com/lra/mackup"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/mackup" rel="nofollow"&gt;PyPI&lt;/a&gt;) Utility to back up and synchronize application settings, with support for several storage backends (e.g., Dropbox, Git), and dozens of applications. &lt;code&gt;(linux, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Metamorphose&lt;/strong&gt; - (&lt;a href="https://github.com/metamorphose/metamorphose2"&gt;Repo&lt;/a&gt;, &lt;a href="http://file-folder-ren.sourceforge.net/" rel="nofollow"&gt;Home&lt;/a&gt;) Graphical mass renaming program for files and folders. &lt;code&gt;(linux, windows, mac, wx)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nuxeo Drive&lt;/strong&gt; - (&lt;a href="https://github.com/nuxeo/nuxeo-drive"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.nuxeo.com/products/drive-desktop-sync" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://doc.nuxeo.com/client-apps/nuxeo-drive" rel="nofollow"&gt;Docs&lt;/a&gt;) Cross-platform desktop synchronization client for the Nuxeo platform. &lt;code&gt;(storage, linux, windows, mac, console, appimage, lgpl, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nvda&lt;/strong&gt; - (&lt;a href="https://github.com/nvaccess/nvda"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.nvaccess.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Non-Visual Desktop Access, a powerful screen reader for Windows. &lt;code&gt;(windows, wx)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plover&lt;/strong&gt; - (&lt;a href="https://github.com/openstenoproject/plover"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.openstenoproject.org/plover" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.openstenoproject.org/donate" rel="nofollow"&gt;Fund&lt;/a&gt;, &lt;a href="https://github.com/openstenoproject/plover/wiki"&gt;Docs&lt;/a&gt;) Background service for automatic translation of stenography movements to keystrokes, enabling typing speeds in excess of 200WPM in any application. &lt;code&gt;(linux, windows, mac, hardware, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Psono&lt;/strong&gt; - (&lt;a href="https://gitlab.com/psono/psono-server" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://psono.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.psono.pw/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://doc.psono.com/" rel="nofollow"&gt;Docs&lt;/a&gt;) Server-based password manager, built for teams. &lt;code&gt;(security, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ranger&lt;/strong&gt; - (&lt;a href="https://github.com/ranger/ranger"&gt;Repo&lt;/a&gt;, &lt;a href="https://ranger.github.io/" rel="nofollow"&gt;Home&lt;/a&gt;) TUI (&lt;a href="https://en.wikipedia.org/wiki/Text-based_user_interface" rel="nofollow"&gt;Text User Interface&lt;/a&gt;) file manager, inspired by vim. &lt;code&gt;(linux, tui)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Redash&lt;/strong&gt; - (&lt;a href="https://github.com/getredash/redash"&gt;Repo&lt;/a&gt;, &lt;a href="https://redash.io/" rel="nofollow"&gt;Home&lt;/a&gt;) Data visualization and dashboard construction geared toward business intelligence, used by Mozilla, SoundCloud, Sentry, and others. &lt;code&gt;(server, flask)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ReproZip&lt;/strong&gt; - (&lt;a href="https://github.com/VIDA-NYU/reprozip"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.reprozip.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://examples.reprozip.org/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://docs.reprozip.org/" rel="nofollow"&gt;Docs&lt;/a&gt;) Command-line tool which automatically builds reproducible experiments archives from console commands, designed for use in computational science. &lt;code&gt;(science, linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sunflower&lt;/strong&gt; - (&lt;a href="https://github.com/MeanEYE/Sunflower"&gt;Repo&lt;/a&gt;, &lt;a href="http://sunflower-fm.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Small and highly-customizable twin-panel file manager for Linux with plugin support. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Superset&lt;/strong&gt; - (&lt;a href="https://github.com/apache/incubator-superset"&gt;Repo&lt;/a&gt;, &lt;a href="http://superset.apache.org/" rel="nofollow"&gt;Docs&lt;/a&gt;) Data exploration, visualization, and business intelligence web application. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VisiData&lt;/strong&gt; - (&lt;a href="https://github.com/saulpw/visidata"&gt;Repo&lt;/a&gt;, &lt;a href="https://visidata.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://patreon.com/saulpw" rel="nofollow"&gt;Fund&lt;/a&gt;, &lt;a href="https://pypi.org/project/visidata" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://visidata.org/docs" rel="nofollow"&gt;Docs&lt;/a&gt;) Interactive multitool for exploring, analyzing, and converting datasets in the terminal. &lt;code&gt;(linux, mac, tui)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vorta&lt;/strong&gt; - (&lt;a href="https://github.com/borgbase/vorta"&gt;Repo&lt;/a&gt;, &lt;a href="https://vorta.borgbase.com/" rel="nofollow"&gt;Home&lt;/a&gt;) GUI backup client built on top of &lt;a href="https://borgbackup.readthedocs.io/" rel="nofollow"&gt;BorgBackup&lt;/a&gt;. &lt;code&gt;(linux, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;wttr.in&lt;/strong&gt; - (&lt;a href="https://github.com/chubin/wttr.in"&gt;Repo&lt;/a&gt;, &lt;a href="http://wttr.in/" rel="nofollow"&gt;Home&lt;/a&gt;) Weather forecast service that supports various representations, suitable for the terminal or web browser. &lt;code&gt;(server, flask)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-organization" class="anchor" aria-hidden="true" href="#organization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-organization" href="#tag-organization"&gt;Organization&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Ambar&lt;/strong&gt; - (&lt;a href="https://github.com/RD17/ambar"&gt;Repo&lt;/a&gt;, &lt;a href="https://ambar.cloud/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://app.ambar.cloud/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://ambar.cloud/docs/system-requirements" rel="nofollow"&gt;Docs&lt;/a&gt;) Document search engine with automated crawling, OCR, tagging, and instant full-text search. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ArchiveBox&lt;/strong&gt; - (&lt;a href="https://github.com/pirate/ArchiveBox"&gt;Repo&lt;/a&gt;, &lt;a href="https://archivebox.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://github.com/pirate/ArchiveBox/wiki"&gt;Docs&lt;/a&gt;) Self-hosted web archive, for creating local, browsable backups of content from the web. Imports HTML, JS, PDFs, video, subtitles, git repositories, and more, from Pocket, Pinboard, browser history, etc. &lt;code&gt;(internet, linux, windows, docker)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;archivematica&lt;/strong&gt; - (&lt;a href="https://github.com/artefactual/archivematica"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.archivematica.org/en" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.archivematica.org/en/docs" rel="nofollow"&gt;Docs&lt;/a&gt;) Digital preservation system designed to maintain standards-based, long-term access to collections of digital objects, targeted at archivists and librarians. &lt;code&gt;(internet, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Baby Buddy&lt;/strong&gt; - (&lt;a href="https://github.com/cdubz/babybuddy"&gt;Repo&lt;/a&gt;, &lt;a href="http://demo.baby-buddy.net/" rel="nofollow"&gt;Demo&lt;/a&gt;) Mobile-friendly web application which helps caregivers track sleep, feedings, diaper changes, and tummy time to learn about and predict baby's needs without (as much) guesswork. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;beancount&lt;/strong&gt; - (&lt;a href="https://bitbucket.org/blais/beancount" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="http://furius.ca/beancount" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://github.com/beancount/beancount"&gt;gh&lt;/a&gt;, &lt;a href="https://pypi.org/project/beancount" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://docs.google.com/document/d/1RaondTJCS_IUPBHFNdT8oqFKJjVJDsfsn6JEjBG04eA/edit" rel="nofollow"&gt;Docs&lt;/a&gt;) A double-entry bookkeeping language to define financial transaction records in plain text, then generate a variety of reports, via CLI and web interface. (See also, &lt;a href="https://plaintextaccounting.org/" rel="nofollow"&gt;Plain Text Accounting&lt;/a&gt;). &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Byro&lt;/strong&gt; - (&lt;a href="https://github.com/byro/byro"&gt;Repo&lt;/a&gt;, &lt;a href="https://byro.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based membership administration tool for small and medium sized clubs/NGOs/associations of all kinds, with a focus on the DACH region. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calibre&lt;/strong&gt; - (&lt;a href="https://github.com/kovidgoyal/calibre"&gt;Repo&lt;/a&gt;, &lt;a href="https://calibre-ebook.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Calibre_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://www.patreon.com/kovidgoyal" rel="nofollow"&gt;Fund&lt;/a&gt;) E-book manager designed for viewing, converting, editing, and cataloging e-books in all major formats. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calibre-Web&lt;/strong&gt; - (&lt;a href="https://github.com/janeczku/calibre-web"&gt;Repo&lt;/a&gt;) Web application providing a clean interface for browsing, reading, and downloading ebooks using an existing &lt;a href="https://calibre-ebook.com/" rel="nofollow"&gt;Calibre&lt;/a&gt; database. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CherryTree&lt;/strong&gt; - (&lt;a href="https://github.com/giuspen/cherrytree"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.giuspen.com/cherrytree" rel="nofollow"&gt;Home&lt;/a&gt;) Hierarchical wiki-like personal notepad, featuring rich text and syntax highlighting. &lt;code&gt;(linux, windows, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Collaborate&lt;/strong&gt; - (&lt;a href="https://github.com/propublica/django-collaborative"&gt;Repo&lt;/a&gt;, &lt;a href="https://propublica.gitbook.io/collaborate-user-manual" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based collaboration tool designed by &lt;a href="https://www.propublica.org/nerds/making-collaborative-data-projects-easier-our-new-tool-collaborate-is-here" rel="nofollow"&gt;Propublica&lt;/a&gt; for newsrooms to share datasets, with a workflow built around assigning tips and maintaining contacts. &lt;code&gt;(communication, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CouchPotato&lt;/strong&gt; - (&lt;a href="https://github.com/CouchPotato/CouchPotatoServer"&gt;Repo&lt;/a&gt;, &lt;a href="http://couchpota.to/" rel="nofollow"&gt;Home&lt;/a&gt;) Personal video recorder focused on movies, with support for usenet and torrents. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dupeGuru&lt;/strong&gt; - (&lt;a href="https://github.com/arsenetar/dupeguru"&gt;Repo&lt;/a&gt;, &lt;a href="https://dupeguru.voltaicideas.net/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://dupeguru.voltaicideas.net/help/en" rel="nofollow"&gt;Docs&lt;/a&gt;) Cross-platform GUI tool to find duplicate files. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dvc (Data Version Control)&lt;/strong&gt; - (&lt;a href="https://github.com/iterative/dvc"&gt;Repo&lt;/a&gt;, &lt;a href="https://dvc.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://dvc.org/doc" rel="nofollow"&gt;Docs&lt;/a&gt;) Command-line tool for version control over data used in machine learning projects. Aims to replace Excel and other tools used to track and deploy model versions. &lt;code&gt;(scm, linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fava&lt;/strong&gt; - (&lt;a href="https://github.com/beancount/fava"&gt;Repo&lt;/a&gt;, &lt;a href="https://fava.pythonanywhere.com/huge-example-file/income_statement" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://beancount.github.io/fava" rel="nofollow"&gt;Docs&lt;/a&gt;) Web interface for the double-entry bookkeeping software &lt;a href="http://furius.ca/beancount/" rel="nofollow"&gt;Beancount&lt;/a&gt; with a focus on features and usability. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gramps&lt;/strong&gt; - (&lt;a href="https://github.com/gramps-project/gramps"&gt;Repo&lt;/a&gt;, &lt;a href="https://gramps-project.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Genealogy software that is both intuitive for hobbyists and feature-complete for professional genealogists. &lt;code&gt;(linux, windows, mac, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GTimeLog&lt;/strong&gt; - (&lt;a href="https://github.com/gtimelog/gtimelog"&gt;Repo&lt;/a&gt;, &lt;a href="https://gtimelog.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://ko-fi.com/mgedmin" rel="nofollow"&gt;Fund&lt;/a&gt;, &lt;a href="https://gtimelog.org/docs.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Desktop-based time tracker with support for logging billable/non-billable work. &lt;code&gt;(productivity, linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Headphones&lt;/strong&gt; - (&lt;a href="https://github.com/rembo10/headphones"&gt;Repo&lt;/a&gt;, &lt;a href="https://github.com/rembo10/headphones/wiki"&gt;Docs&lt;/a&gt;) Web-based digital music library for automating music downloads through Usenet and torrents. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ihatemoney&lt;/strong&gt; - (&lt;a href="https://github.com/spiral-project/ihatemoney"&gt;Repo&lt;/a&gt;, &lt;a href="https://ihatemoney.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://ihatemoney.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Web application made to ease shared budget management by keeping track of who bought what, when, and for whom. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Indico&lt;/strong&gt; - (&lt;a href="https://github.com/indico/indico"&gt;Repo&lt;/a&gt;, &lt;a href="https://getindico.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://sandbox.getindico.io/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://docs.getindico.io/en/stable/installation" rel="nofollow"&gt;Docs&lt;/a&gt;) Feature-rich web application designed at &lt;a href="https://en.wikipedia.org/wiki/CERN" rel="nofollow"&gt;CERN&lt;/a&gt; for managing events, with support for conference organization workflow, from content management to receiving and reviewing abstracts/papers, event registration, payment integration, room booking, and more. &lt;code&gt;(communication, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Invenio&lt;/strong&gt; - (&lt;a href="https://github.com/inveniosoftware/invenio"&gt;Repo&lt;/a&gt;, &lt;a href="https://invenio.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Customizable platform for running a trusted digital repository. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;jrnl&lt;/strong&gt; - (&lt;a href="https://github.com/maebert/jrnl"&gt;Repo&lt;/a&gt;, &lt;a href="http://jrnl.sh/" rel="nofollow"&gt;Home&lt;/a&gt;) Simple, ecncrypted journal application for your command line. &lt;code&gt;(linux, windows, mac, homebrew)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LazyLibrarian&lt;/strong&gt; - (&lt;a href="https://gitlab.com/LazyLibrarian/LazyLibrarian" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/LazyLibrarian" rel="nofollow"&gt;Forum&lt;/a&gt;, &lt;a href="https://lazylibrarian.gitlab.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based digital library organizer with support for following authors and automatic metadata retrieval. &lt;code&gt;(linux, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mayan&lt;/strong&gt; - (&lt;a href="https://gitlab.com/mayan-edms/mayan-edms" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.mayan-edms.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.paypal.me/MayanEDMS" rel="nofollow"&gt;Fund&lt;/a&gt;, &lt;a href="https://pypi.org/project/mayan-edms/3.2.7" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://docs.mayan-edms.com/" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based document management system, designed to store, introspect, and categorize files, with OCR, preview, label, signing, and sending capabilities. Also featuring workflow system, role-based access control, and REST API. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MLflow&lt;/strong&gt; - (&lt;a href="https://github.com/mlflow/mlflow"&gt;Repo&lt;/a&gt;, &lt;a href="https://mlflow.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://mlflow.org/docs/latest/index.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Integrated command-line application and web service, supporting an end-to-end machine-learning workflow around tracking, packaging, and deploying. Developed by &lt;a href="https://docs.databricks.com/applications/mlflow/index.html" rel="nofollow"&gt;Databricks&lt;/a&gt;. &lt;code&gt;(dev, linux, mac, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenLibrary&lt;/strong&gt; - (&lt;a href="https://github.com/internetarchive/openlibrary"&gt;Repo&lt;/a&gt;, &lt;a href="https://openlibrary.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Open_Library" rel="nofollow"&gt;WP&lt;/a&gt;) Web application for an open, editable library catalog, used by &lt;a href="https://archive.org/" rel="nofollow"&gt;The Internet Archive&lt;/a&gt; towards building a web page for every book ever published. &lt;code&gt;(linux, windows, mac, docker)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paperwork&lt;/strong&gt; - (&lt;a href="https://gitlab.gnome.org/World/OpenPaperwork/paperwork" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://openpaper.work/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.patreon.com/openpaper" rel="nofollow"&gt;Fund&lt;/a&gt;, &lt;a href="https://gitlab.gnome.org/World/OpenPaperwork/paperwork/wikis/home" rel="nofollow"&gt;Docs&lt;/a&gt;) Personal document manager for organizing scanned documents and PDFs, with support for OCR, automatic tagging, and search. &lt;code&gt;(linux, windows, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pinry&lt;/strong&gt; - (&lt;a href="https://github.com/pinry/pinry"&gt;Repo&lt;/a&gt;, &lt;a href="https://getpinry.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Tiling image board system for saving, tagging, and sharing images, videos, and websites, like a self-hosted Pinterest. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pretalx&lt;/strong&gt; - (&lt;a href="https://github.com/pretalx/pretalx"&gt;Repo&lt;/a&gt;, &lt;a href="https://pretalx.com/p/about" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.patreon.com/rixx" rel="nofollow"&gt;Fund&lt;/a&gt;) Web-based conference planning tool, with support for Calls for Papers (CFP), scheduling, and speaker management. &lt;code&gt;(communication, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyMedusa&lt;/strong&gt; - (&lt;a href="https://github.com/pymedusa/Medusa"&gt;Repo&lt;/a&gt;, &lt;a href="https://pymedusa.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Video library manager for TV shows, with automatic download support. &lt;code&gt;(linux, windows)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Radicale&lt;/strong&gt; - (&lt;a href="https://github.com/Kozea/Radicale"&gt;Repo&lt;/a&gt;, &lt;a href="https://radicale.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://radicale.org/documentation" rel="nofollow"&gt;Docs&lt;/a&gt;) Simple CalDAV (calendar) and CardDAV (contact) server. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RedNotebook&lt;/strong&gt; - (&lt;a href="https://github.com/jendrikseipp/rednotebook"&gt;Repo&lt;/a&gt;, &lt;a href="https://rednotebook.sourceforge.io/downloads.html" rel="nofollow"&gt;Home&lt;/a&gt;) Desktop journal designed for rich text, media, and template-based entries, which can be tagged and searched, as well as exported to plain text, HTML, Latex, or PDF. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scholia&lt;/strong&gt; - (&lt;a href="https://github.com/fnielsen/scholia"&gt;Repo&lt;/a&gt;, &lt;a href="https://tools.wmflabs.org/scholia" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.wikidata.org/wiki/Wikidata%3AScholia" rel="nofollow"&gt;Docs&lt;/a&gt;) Python package and web application for interacting with scholarly information on &lt;a href="https://www.wikidata.org/" rel="nofollow"&gt;Wikidata&lt;/a&gt;. &lt;code&gt;(science, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Senaite&lt;/strong&gt; - (&lt;a href="https://github.com/senaite/senaite.lims"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.senaite.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Web-based, mobile-first laboratory information management system (LIMS). &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SiCKRAGE&lt;/strong&gt; - (&lt;a href="https://git.sickrage.ca/SiCKRAGE/sickrage" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://github.com/SiCKRAGE/SiCKRAGE"&gt;gh&lt;/a&gt;, &lt;a href="https://git.sickrage.ca/SiCKRAGE/sickrage/wikis/FAQ%27s-and-Fixes" rel="nofollow"&gt;Docs&lt;/a&gt;) Video library manager with support for automatic TV show archival. &lt;code&gt;(linux, windows)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Taiga&lt;/strong&gt; - (&lt;a href="https://github.com/taigaio/taiga-back"&gt;Repo&lt;/a&gt;, &lt;a href="https://taiga.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="http://taigaio.github.io/taiga-doc/dist" rel="nofollow"&gt;Docs&lt;/a&gt;) Web application built for managing projects with agile development processes. &lt;code&gt;(dev, server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wikid Pad&lt;/strong&gt; - (&lt;a href="https://github.com/WikidPad/WikidPad"&gt;Repo&lt;/a&gt;, &lt;a href="http://wikidpad.sourceforge.net/" rel="nofollow"&gt;Home&lt;/a&gt;) Desktop wiki notebook for storing your thoughts and ideas. &lt;code&gt;(linux, windows, mac, wx)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xandikos&lt;/strong&gt; - (&lt;a href="https://github.com/jelmer/xandikos"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.xandikos.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Lightweight but relatively complete CardDAV/CalDAV server which backs up changes in a Git repository. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zim Wiki&lt;/strong&gt; - (&lt;a href="https://github.com/jaap-karssenberg/zim-desktop-wiki"&gt;Repo&lt;/a&gt;, &lt;a href="http://zim-wiki.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Desktop wiki designed for note-taking, list-making, and drafting. &lt;code&gt;(linux, windows, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-communication" class="anchor" aria-hidden="true" href="#communication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-communication" href="#tag-communication"&gt;Communication&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Abilian SBE&lt;/strong&gt; - (&lt;a href="https://github.com/abilian/abilian-sbe"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.abilian.com/" rel="nofollow"&gt;Home&lt;/a&gt;) A "Social Business Engine" with features including lightweight document management, discussions, wikis, timelines, and more. &lt;code&gt;(cms, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Askbot&lt;/strong&gt; - (&lt;a href="https://github.com/ASKBOT/askbot-devel"&gt;Repo&lt;/a&gt;, &lt;a href="https://askbot.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Q&amp;amp;A web platform similar to StackOverflow, complete with tagging, reputation, badges, and more. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bitmessage&lt;/strong&gt; - (&lt;a href="https://github.com/Bitmessage/PyBitmessage"&gt;Repo&lt;/a&gt;, &lt;a href="https://bitmessage.org/wiki/Main_Page" rel="nofollow"&gt;Docs&lt;/a&gt;) Reference client for Bitmessage, a peer-to-peer encrypted decentralised communication protocol. &lt;code&gt;(linux, windows, mac, kivy, qt4, tui)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Collaborate&lt;/strong&gt; - (&lt;a href="https://github.com/propublica/django-collaborative"&gt;Repo&lt;/a&gt;, &lt;a href="https://propublica.gitbook.io/collaborate-user-manual" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based collaboration tool designed by &lt;a href="https://www.propublica.org/nerds/making-collaborative-data-projects-easier-our-new-tool-collaborate-is-here" rel="nofollow"&gt;Propublica&lt;/a&gt; for newsrooms to share datasets, with a workflow built around assigning tips and maintaining contacts. &lt;code&gt;(organization, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dak&lt;/strong&gt; - (&lt;a href="https://salsa.debian.org/ftp-team/dak" rel="nofollow"&gt;Repo&lt;/a&gt;) Collection of programs used to maintain the Debian project's email archives. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Django Wiki&lt;/strong&gt; - (&lt;a href="https://github.com/django-wiki/django-wiki"&gt;Repo&lt;/a&gt;, &lt;a href="https://demo.django-wiki.org/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://django-wiki.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) A simple and mature web-based wiki. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docassemble&lt;/strong&gt; - (&lt;a href="https://github.com/jhpyle/docassemble"&gt;Repo&lt;/a&gt;, &lt;a href="https://docassemble.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://docassemble.org/docs.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Platform for creating mobile-friendly web-based interviews, collecting responses, and much more. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formspree&lt;/strong&gt; - (&lt;a href="https://github.com/formspree/formspree"&gt;Repo&lt;/a&gt;, &lt;a href="https://formspree.io/" rel="nofollow"&gt;Home&lt;/a&gt;) Web server which turns an HTML form submission into an email, without registration, JavaScript, or custom Python. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gajim&lt;/strong&gt; - (&lt;a href="https://dev.gajim.org/gajim/gajim" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Gajim" rel="nofollow"&gt;WP&lt;/a&gt;) Lightweight, cross-platform instant messaging client for the XMPP protocol. &lt;code&gt;(linux, windows, mac, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GlobaLeaks&lt;/strong&gt; - (&lt;a href="https://github.com/globaleaks/GlobaLeaks"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.globaleaks.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Web application to enable secure and anonymous whistleblowing initiatives. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hangups&lt;/strong&gt; - (&lt;a href="https://github.com/tdryer/hangups"&gt;Repo&lt;/a&gt;, &lt;a href="https://snapcraft.io/hangups" rel="nofollow"&gt;Snap&lt;/a&gt;, &lt;a href="https://hangups.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Third-party instant messenger for &lt;a href="https://en.wikipedia.org/wiki/Google_Hangouts" rel="nofollow"&gt;Google Hangouts&lt;/a&gt;, with support for group messaging and other proprietary features. &lt;code&gt;(linux, mac, docker, snap)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hawkpost&lt;/strong&gt; - (&lt;a href="https://github.com/whitesmith/hawkpost"&gt;Repo&lt;/a&gt;, &lt;a href="https://hawkpost.co/" rel="nofollow"&gt;Home&lt;/a&gt;) Web application which enables receiving encrypted messages from less technical senders. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Helios Voting&lt;/strong&gt; - (&lt;a href="https://github.com/benadida/helios-server"&gt;Repo&lt;/a&gt;, &lt;a href="http://heliosvoting.org/" rel="nofollow"&gt;Home&lt;/a&gt;) End-to-end verifiable voting system. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inboxen&lt;/strong&gt; - (&lt;a href="https://github.com/Inboxen/Inboxen"&gt;Repo&lt;/a&gt;, &lt;a href="https://inboxen.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://inboxen.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Web application which provides an infinite number of unique email inboxes, for segmenting services and maintaining privacy. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Indico&lt;/strong&gt; - (&lt;a href="https://github.com/indico/indico"&gt;Repo&lt;/a&gt;, &lt;a href="https://getindico.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://sandbox.getindico.io/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://docs.getindico.io/en/stable/installation" rel="nofollow"&gt;Docs&lt;/a&gt;) Feature-rich web application designed at &lt;a href="https://en.wikipedia.org/wiki/CERN" rel="nofollow"&gt;CERN&lt;/a&gt; for managing events, with support for conference organization workflow, from content management to receiving and reviewing abstracts/papers, event registration, payment integration, room booking, and more. &lt;code&gt;(organization, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Magic Wormhole&lt;/strong&gt; - (&lt;a href="https://github.com/warner/magic-wormhole"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/magic-wormhole" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://magic-wormhole.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Security- and speed-focused file transfer tool with support for files, text, and directories. &lt;code&gt;(linux, mac, console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mailman&lt;/strong&gt; - (&lt;a href="https://gitlab.com/mailman/mailman" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.list.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/GNU_Mailman" rel="nofollow"&gt;WP&lt;/a&gt;) The original listserv, a web application and email server for managing subscriptions and discussion archives. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mailpile&lt;/strong&gt; - (&lt;a href="https://github.com/mailpile/Mailpile"&gt;Repo&lt;/a&gt;, &lt;a href="https://mailpile.is/" rel="nofollow"&gt;Home&lt;/a&gt;) Fast email client with user-friendly encryption and privacy features. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mailu&lt;/strong&gt; - (&lt;a href="https://github.com/Mailu/Mailu"&gt;Repo&lt;/a&gt;, &lt;a href="https://mailu.io/" rel="nofollow"&gt;Home&lt;/a&gt;) Full-featured mail server designed for easy setup and maintenance, supporting IMAP, IMAP+, SMTP, and Submission, as well as a slew of advanced features. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modoboa&lt;/strong&gt; - (&lt;a href="https://github.com/modoboa/modoboa"&gt;Repo&lt;/a&gt;, &lt;a href="https://modoboa.org/en" rel="nofollow"&gt;Home&lt;/a&gt;) Mail hosting and management platform including web UI based on Django. Provides useful components such as an admin panel and webmail. Integrates with Postfix or Dovecot. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MoinMoin&lt;/strong&gt; - (&lt;a href="https://bitbucket.org/thomaswaldmann/moin-2.0" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="http://moinmo.in/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/MoinMoin" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://moin-20.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Python's own web-based wiki software, used for &lt;a href="https://wiki.python.org/moin/" rel="nofollow"&gt;the official Python wiki&lt;/a&gt; and many others. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OfflineIMAP&lt;/strong&gt; - (&lt;a href="https://github.com/OfflineIMAP/offlineimap"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.offlineimap.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/OfflineIMAP" rel="nofollow"&gt;WP&lt;/a&gt;) IMAP reader and synchronizer. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OnionShare&lt;/strong&gt; - (&lt;a href="https://github.com/micahflee/onionshare"&gt;Repo&lt;/a&gt;, &lt;a href="https://onionshare.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://github.com/micahflee/onionshare/wiki"&gt;Docs&lt;/a&gt;) Secure and anonymous file sharing over &lt;a href="https://en.wikipedia.org/wiki/Tor_(anonymity_network)" rel="nofollow"&gt;Tor&lt;/a&gt; services. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pootle&lt;/strong&gt; - (&lt;a href="https://github.com/translate/pootle"&gt;Repo&lt;/a&gt;, &lt;a href="http://pootle.translatehouse.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Pootle" rel="nofollow"&gt;WP&lt;/a&gt;) Web application for collaborative translation. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pretalx&lt;/strong&gt; - (&lt;a href="https://github.com/pretalx/pretalx"&gt;Repo&lt;/a&gt;, &lt;a href="https://pretalx.com/p/about" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.patreon.com/rixx" rel="nofollow"&gt;Fund&lt;/a&gt;) Web-based conference planning tool, with support for Calls for Papers (CFP), scheduling, and speaker management. &lt;code&gt;(organization, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pycsw&lt;/strong&gt; - (&lt;a href="https://github.com/geopython/pycsw"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Pycsw" rel="nofollow"&gt;WP&lt;/a&gt;) Full implementation of the OpenGIS Catalogue Service Implementation Specification. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RapidSMS&lt;/strong&gt; - (&lt;a href="https://github.com/rapidsms/rapidsms"&gt;Repo&lt;/a&gt;, &lt;a href="http://rapidsms.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="http://readthedocs.org/docs/rapidsms" rel="nofollow"&gt;Docs&lt;/a&gt;) Interactive SMS text messaging platform. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SecureDrop&lt;/strong&gt; - (&lt;a href="https://github.com/freedomofpress/securedrop"&gt;Repo&lt;/a&gt;, &lt;a href="https://securedrop.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://docs.securedrop.org/" rel="nofollow"&gt;Docs&lt;/a&gt;) Whistleblower submission system for media organizations to securely accept documents from anonymous sources. Originally created by &lt;a href="https://en.wikipedia.org/wiki/Aaron_Swartz" rel="nofollow"&gt;Aaron Swartz&lt;/a&gt; and currently managed by the &lt;a href="https://en.wikipedia.org/wiki/Freedom_of_the_Press_Foundation" rel="nofollow"&gt;Freedom of the Press Foundation&lt;/a&gt;. &lt;code&gt;(server, flask)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Socialhome&lt;/strong&gt; - (&lt;a href="https://git.feneas.org/socialhome/socialhome" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://socialhome.network/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://github.com/jaywink/socialhome"&gt;gh&lt;/a&gt;, &lt;a href="https://socialhome.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Web application enabling users to build a federated personal profile with social networking functionality. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Synapse&lt;/strong&gt; - (&lt;a href="https://github.com/matrix-org/synapse"&gt;Repo&lt;/a&gt;, &lt;a href="https://riot.im/app#/home" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.patreon.com/matrixdotorg/overview" rel="nofollow"&gt;Fund&lt;/a&gt;) Reference server for the &lt;a href="https://matrix.org" rel="nofollow"&gt;matrix.org&lt;/a&gt; distributed chat protocol. Used daily by tens of thousands at &lt;a href="https://riot.im/app/" rel="nofollow"&gt;riot.im&lt;/a&gt;. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Virtaal&lt;/strong&gt; - (&lt;a href="https://github.com/translate/virtaal"&gt;Repo&lt;/a&gt;, &lt;a href="http://virtaal.translatehouse.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Cross-platform GUI for performing translation, with support for a variety of formats. &lt;code&gt;(linux, windows, mac, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weblate&lt;/strong&gt; - (&lt;a href="https://github.com/WeblateOrg/weblate"&gt;Repo&lt;/a&gt;, &lt;a href="https://weblate.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/Weblate" rel="nofollow"&gt;PyPI&lt;/a&gt;) Web based localization tool with tight version control integration. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zulip&lt;/strong&gt; - (&lt;a href="https://github.com/zulip/zulip"&gt;Repo&lt;/a&gt;, &lt;a href="https://zulip.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Zulip" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://zulip.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Powerful chat server and web client with support for threaded conversations. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-education" class="anchor" aria-hidden="true" href="#education"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-education" href="#tag-education"&gt;Education&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Anki&lt;/strong&gt; - (&lt;a href="https://github.com/dae/anki"&gt;Repo&lt;/a&gt;, &lt;a href="https://apps.ankiweb.net/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://apps.ankiweb.net/docs/manual.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Powerful desktop application for flash cards and memorization. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DrawBot&lt;/strong&gt; - (&lt;a href="https://github.com/typemytype/drawbot"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.drawbot.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/DrawBot" rel="nofollow"&gt;WP&lt;/a&gt;) A powerful programmatic 2D drawing application for MacOS X which generates graphics from Python scripts. &lt;code&gt;(graphics, dev, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kolibri&lt;/strong&gt; - (&lt;a href="https://github.com/learningequality/kolibri"&gt;Repo&lt;/a&gt;, &lt;a href="https://learningequality.org/kolibri" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://kolibridemo.learningequality.org/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://pypi.org/project/kolibri" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://kolibri.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Self-hostable learning web application targeted at making high quality education technology available in low-resource communities (e.g., rural schools, refugee camps, orphanages, non-formal school systems, and prison systems). &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mnemosyne&lt;/strong&gt; - (&lt;a href="https://github.com/mnemosyne-proj/mnemosyne"&gt;Repo&lt;/a&gt;, &lt;a href="https://mnemosyne-proj.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Spaced-repetition flashcard program for efficient memorization. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NBGrader&lt;/strong&gt; - (&lt;a href="https://github.com/jupyter/nbgrader"&gt;Repo&lt;/a&gt;, &lt;a href="https://nbgrader.readthedocs.io/en/stable" rel="nofollow"&gt;Docs&lt;/a&gt;) Jupyter-based application which enables educators to create, assign, and grade assignments in notebook form. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open edX Platform&lt;/strong&gt; - (&lt;a href="https://github.com/edx/edx-platform"&gt;Repo&lt;/a&gt;, &lt;a href="http://open.edx.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/EdX#Open_edX" rel="nofollow"&gt;WP&lt;/a&gt;) Platform for online education providers, powering &lt;a href="https://en.wikipedia.org/wiki/EdX" rel="nofollow"&gt;edX&lt;/a&gt;. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RELATE&lt;/strong&gt; - (&lt;a href="https://github.com/inducer/relate"&gt;Repo&lt;/a&gt;, &lt;a href="https://documen.tician.de/relate" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based courseware with support for course planning and versioning, scheduling, testing, and grading. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tutor&lt;/strong&gt; - (&lt;a href="https://github.com/overhangio/tutor"&gt;Repo&lt;/a&gt;, &lt;a href="https://docs.tutor.overhang.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Docker-based Open edX distribution, both for production and local development, with a goal of easing deployment, customization, upgrading, and scaling. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-science" class="anchor" aria-hidden="true" href="#science"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-science" href="#tag-science"&gt;Science&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;AnuGA&lt;/strong&gt; - (&lt;a href="https://github.com/GeoscienceAustralia/anuga_core"&gt;Repo&lt;/a&gt;, &lt;a href="https://anuga.anu.edu.au/" rel="nofollow"&gt;Home&lt;/a&gt;) Advanced simulation of the shallow water equation, for modeling tsunamis, dam breaks, and floods. &lt;code&gt;(linux, windows)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Artisan&lt;/strong&gt; - (&lt;a href="https://github.com/artisan-roaster-scope/artisan"&gt;Repo&lt;/a&gt;, &lt;a href="https://artisan-scope.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://artisan-scope.org/docs/quick-start-guide" rel="nofollow"&gt;Docs&lt;/a&gt;) Desktop visual scope for coffee roasters, which helps coffee roasters record, analyze, and control roast profiles. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASCEND&lt;/strong&gt; - (&lt;a href="http://code.ascend4.org/ascend/trunk" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="http://ascend4.org/Main_Page" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/ASCEND" rel="nofollow"&gt;WP&lt;/a&gt;) Mathematical chemical process modelling system developed at Carnegie Mellon University since late 1978. &lt;code&gt;(linux, windows, mac, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CellProfiler&lt;/strong&gt; - (&lt;a href="https://github.com/CellProfiler/CellProfiler"&gt;Repo&lt;/a&gt;, &lt;a href="http://cellprofiler.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://cellprofiler.org/cpa" rel="nofollow"&gt;Manual&lt;/a&gt;, &lt;a href="https://github.com/CellProfiler/CellProfiler/wiki"&gt;Docs&lt;/a&gt;) Interactive data exploration, analysis, and classification of biological image sets. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;cellxgene&lt;/strong&gt; - (&lt;a href="https://github.com/chanzuckerberg/cellxgene"&gt;Repo&lt;/a&gt;, &lt;a href="https://chanzuckerberg.github.io/cellxgene" rel="nofollow"&gt;Home&lt;/a&gt;) Web-based interactive explorer for single-cell transcriptomics data. &lt;code&gt;(linux, windows, mac, fnd)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CKAN&lt;/strong&gt; - (&lt;a href="https://github.com/ckan/ckan"&gt;Repo&lt;/a&gt;, &lt;a href="https://ckan.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Data management system (DMS) which makes it easy to publish, share, and use data. Data hubs powered by CKAN include &lt;a href="https://datahub.io" rel="nofollow"&gt;datahub.io&lt;/a&gt;, &lt;a href="https://catalog.data.gov" rel="nofollow"&gt;catalog.data.gov&lt;/a&gt;, and &lt;a href="https://europeandataportal.eu/data/en/dataset" rel="nofollow"&gt;europeandataportal.eu&lt;/a&gt;, among many other sites. &lt;code&gt;(server, flask)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CoCalc&lt;/strong&gt; - (&lt;a href="https://github.com/sagemathinc/cocalc"&gt;Repo&lt;/a&gt;, &lt;a href="https://cocalc.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/CoCalc" rel="nofollow"&gt;WP&lt;/a&gt;) Collaborative calculation in the cloud, with support for the scientific Python stack, SageMath, R, LaTeX, Markdown, and more. Also features chat, course management, and other supporting functionality. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dissem.in&lt;/strong&gt; - (&lt;a href="https://github.com/dissemin/dissemin"&gt;Repo&lt;/a&gt;, &lt;a href="https://dissem.in/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://dev.dissem.in/" rel="nofollow"&gt;Docs&lt;/a&gt;) Web platform to help researchers upload their papers to open-access repositories. &lt;code&gt;(server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;InVesalius&lt;/strong&gt; - (&lt;a href="https://github.com/invesalius/invesalius3"&gt;Repo&lt;/a&gt;, &lt;a href="https://invesalius.github.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/InVesalius" rel="nofollow"&gt;WP&lt;/a&gt;) Generates virtual reconstructions of structures in the human body for medical purposes, including CT and MRI scans. &lt;code&gt;(linux, windows, mac, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Manim&lt;/strong&gt; - (&lt;a href="https://github.com/3b1b/manim"&gt;Repo&lt;/a&gt;, &lt;a href="https://manim.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Animation engine for explanatory math videos, primarily designed for &lt;a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw" rel="nofollow"&gt;works by 3blue1brown&lt;/a&gt;. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mayavi&lt;/strong&gt; - (&lt;a href="https://github.com/enthought/mayavi"&gt;Repo&lt;/a&gt;, &lt;a href="http://docs.enthought.com/mayavi/mayavi" rel="nofollow"&gt;Home&lt;/a&gt;) General purpose, cross-platform tool for 2-D and 3-D scientific data visualization. &lt;code&gt;(linux, windows, mac, qt4)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mosaic&lt;/strong&gt; - (&lt;a href="https://github.com/usnistgov/mosaic"&gt;Repo&lt;/a&gt;, &lt;a href="https://pages.nist.gov/mosaic" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pages.nist.gov/mosaic/html/index.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Desktop-based single molecule analysis toolbox that automatically decodes multi-state nanopore data. &lt;code&gt;(linux, windows, mac, gov)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;odemis&lt;/strong&gt; - (&lt;a href="https://github.com/delmic/odemis"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.delmic.com/microscopy-software-odemis" rel="nofollow"&gt;Home&lt;/a&gt;) Desktop imaging workflow software for Delmic microscopes, supporting autofocus, coordinate history, and OME-TIFF and HDF5 export. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OPEM&lt;/strong&gt; - (&lt;a href="https://github.com/ECSIM/opem"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.ecsim.ir/opem/doc" rel="nofollow"&gt;Docs&lt;/a&gt;) A modeling tool for evaluating the performance of &lt;a href="https://en.wikipedia.org/wiki/Proton-exchange_membrane_fuel_cell" rel="nofollow"&gt;proton exchange membrane (PEM) fuel cells&lt;/a&gt;. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orange&lt;/strong&gt; - (&lt;a href="https://github.com/biolab/orange3"&gt;Repo&lt;/a&gt;, &lt;a href="https://orange.biolab.si/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Orange_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) Component-based data mining software for graphical interactive data analysis and visualization. &lt;code&gt;(linux, windows, mac, qt4, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pybliographer&lt;/strong&gt; - (&lt;a href="https://github.com/GNOME/pybliographer"&gt;Repo&lt;/a&gt;, &lt;a href="https://pybliographer.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Bibliographic database manager with a user-friendly desktop UI. &lt;code&gt;(linux, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ReproZip&lt;/strong&gt; - (&lt;a href="https://github.com/VIDA-NYU/reprozip"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.reprozip.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://examples.reprozip.org/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://docs.reprozip.org/" rel="nofollow"&gt;Docs&lt;/a&gt;) Command-line tool which automatically builds reproducible experiments archives from console commands, designed for use in computational science. &lt;code&gt;(productivity, linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sage Math&lt;/strong&gt; - (&lt;a href="https://git.sagemath.org/sage.git" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.sagemath.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/SageMath" rel="nofollow"&gt;WP&lt;/a&gt;) Cross-platform computer algebra system with features covering many aspects of mathematics, including algebra, combinatorics, graph theory, numerical analysis, number theory, calculus, and statistics. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scholia&lt;/strong&gt; - (&lt;a href="https://github.com/fnielsen/scholia"&gt;Repo&lt;/a&gt;, &lt;a href="https://tools.wmflabs.org/scholia" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.wikidata.org/wiki/Wikidata%3AScholia" rel="nofollow"&gt;Docs&lt;/a&gt;) Python package and web application for interacting with scholarly information on &lt;a href="https://www.wikidata.org/" rel="nofollow"&gt;Wikidata&lt;/a&gt;. &lt;code&gt;(organization, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SOFA Statistics&lt;/strong&gt; - (&lt;a href="https://code.launchpad.net/sofastatistics" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.sofastatistics.com/" rel="nofollow"&gt;Home&lt;/a&gt;) User-friendly statistics and analysis with a learn-as-you-go approach. &lt;code&gt;(linux, windows, mac, wx)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Taguette&lt;/strong&gt; - (&lt;a href="https://gitlab.com/remram44/taguette" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.taguette.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://github.com/remram44/taguette"&gt;gh&lt;/a&gt;, &lt;a href="https://pypi.org/project/taguette" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://www.taguette.org/getting-started.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based qualitative research tool supporting importing, tagging, highlighting, and exporting many document formats. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Veusz&lt;/strong&gt; - (&lt;a href="https://github.com/veusz/veusz"&gt;Repo&lt;/a&gt;, &lt;a href="https://veusz.github.io/" rel="nofollow"&gt;Home&lt;/a&gt;) 2D and 3D scientific plotting, designed to produce publication-ready PDF or SVG graphs. &lt;code&gt;(linux, windows, mac, qt)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-cms" class="anchor" aria-hidden="true" href="#cms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-cms" href="#tag-cms"&gt;CMS&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Abilian SBE&lt;/strong&gt; - (&lt;a href="https://github.com/abilian/abilian-sbe"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.abilian.com/" rel="nofollow"&gt;Home&lt;/a&gt;) A "Social Business Engine" with features including lightweight document management, discussions, wikis, timelines, and more. &lt;code&gt;(communication, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Django-CMS&lt;/strong&gt; - (&lt;a href="https://github.com/divio/django-cms"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.django-cms.org/en" rel="nofollow"&gt;Home&lt;/a&gt;) Enterprise content management system based on the Django framework with version control, multi-site support, and more. &lt;code&gt;(server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ella&lt;/strong&gt; - (&lt;a href="https://github.com/ella/ella"&gt;Repo&lt;/a&gt;, &lt;a href="https://ella.readthedocs.io/en/latest/index.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Django-based content management system with a focus on high-traffic news sites and Internet magazines. &lt;code&gt;(server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mezzanine&lt;/strong&gt; - (&lt;a href="https://github.com/stephenmcd/mezzanine"&gt;Repo&lt;/a&gt;, &lt;a href="http://mezzanine.jupo.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Consistent and flexible content management platform built on the Django framework. &lt;code&gt;(server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plone&lt;/strong&gt; - (&lt;a href="https://github.com/plone/Plone"&gt;Repo&lt;/a&gt;, &lt;a href="https://plone.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Plone_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) Extensible enterprise content management system built on Zope. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plumi&lt;/strong&gt; - (&lt;a href="https://github.com/plumi/plumi.app"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Plumi" rel="nofollow"&gt;WP&lt;/a&gt;) Video sharing content management system based on &lt;a href="https://en.wikipedia.org/wiki/Plone_(software)" rel="nofollow"&gt;Plone&lt;/a&gt;. &lt;code&gt;(video, server, plone)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pretix&lt;/strong&gt; - (&lt;a href="https://github.com/pretix/pretix"&gt;Repo&lt;/a&gt;, &lt;a href="https://pretix.eu/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pretix.eu/about/en/blog" rel="nofollow"&gt;Blog&lt;/a&gt;, &lt;a href="https://pypi.org/project/pretix" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://docs.pretix.eu/en/latest/development/index.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based ticketing software, with support for customizable storefronts, direct payments, box office, and reporting. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyCon&lt;/strong&gt; - (&lt;a href="https://github.com/PyCon/pycon"&gt;Repo&lt;/a&gt;, &lt;a href="https://us.pycon.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pycon.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Content management and conference organization web application, based on Django and &lt;a href="https://github.com/pinax/symposion"&gt;Symposion&lt;/a&gt;. &lt;code&gt;(server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Saleor&lt;/strong&gt; - (&lt;a href="https://github.com/mirumee/saleor"&gt;Repo&lt;/a&gt;, &lt;a href="https://getsaleor.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Modular, high-performance e-commerce storefront built with Django, GraphQL, and ReactJS. &lt;code&gt;(server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shuup&lt;/strong&gt; - (&lt;a href="https://github.com/shuup/shuup"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.shuup.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://shuup.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Storefront web application, with support for single- and multi-marketplace models. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wagtail&lt;/strong&gt; - (&lt;a href="https://github.com/wagtail/wagtail"&gt;Repo&lt;/a&gt;, &lt;a href="https://wagtail.io/" rel="nofollow"&gt;Home&lt;/a&gt;) A Django content management system focused on flexibility and user experience. &lt;code&gt;(server, django)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-erp" class="anchor" aria-hidden="true" href="#erp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-erp" href="#tag-erp"&gt;ERP&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ERP5&lt;/strong&gt; - (&lt;a href="https://lab.nexedi.com/nexedi/erp5" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://erp5.nexedi.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/ERP5" rel="nofollow"&gt;WP&lt;/a&gt;) Web-based ERP, CRM, DMS, and Big Data system with hundreds of built-in modules, designed for corporate scalability. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ERPNext&lt;/strong&gt; - (&lt;a href="https://github.com/frappe/erpnext"&gt;Repo&lt;/a&gt;, &lt;a href="https://erpnext.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/ERPNext" rel="nofollow"&gt;WP&lt;/a&gt;) Web-based ERP system with accounting, inventory, CRM, sales, procurement, project management, and HR. Built on &lt;a href="https://github.com/frappe/frappe"&gt;Frappe&lt;/a&gt; and MariaDB. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frepple&lt;/strong&gt; - (&lt;a href="https://github.com/frePPLe/frepple"&gt;Repo&lt;/a&gt;, &lt;a href="https://frepple.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://frepple.com/docs/current/user-guide/index.php" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based supply chain planning for production planning and scheduling. &lt;code&gt;(linux, windows, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Odoo&lt;/strong&gt; - (&lt;a href="https://github.com/odoo/odoo"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.odoo.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Odoo" rel="nofollow"&gt;WP&lt;/a&gt;) Web-based ERP and CRM with many built-in modules, plus thousands of apps to suit any business. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tryton&lt;/strong&gt; - (&lt;a href="https://hg.tryton.org/trytond" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.tryton.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Tryton" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://docs.tryton.org/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Modular web-based ERP, designed for companies of all sizes. &lt;code&gt;(server, fdn)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-static-site" class="anchor" aria-hidden="true" href="#static-site"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-static_site" href="#tag-static_site"&gt;Static Site&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Cactus&lt;/strong&gt; - (&lt;a href="https://github.com/eudicots/Cactus"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/cactus" rel="nofollow"&gt;PyPI&lt;/a&gt;) Static website generator using Django templates. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Chert&lt;/strong&gt; - (&lt;a href="https://github.com/mahmoud/chert"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/chert" rel="nofollow"&gt;PyPI&lt;/a&gt;) Static site generator with built-in support for listicles, created by this humble author, used to power &lt;a href="https://calver.org" rel="nofollow"&gt;calver.org&lt;/a&gt;, &lt;a href="https://zerover.org" rel="nofollow"&gt;zerover.org&lt;/a&gt;, and &lt;a href="https://sedimental.org/" rel="nofollow"&gt;sedimental.org&lt;/a&gt;, the author's blog. Mostly here as an easter egg :) &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grow&lt;/strong&gt; - (&lt;a href="https://github.com/grow/grow"&gt;Repo&lt;/a&gt;, &lt;a href="https://grow.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/grow" rel="nofollow"&gt;PyPI&lt;/a&gt;) Static site generator optimized for building interactive, localized microsites, with a focus on workflow and maintainability. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hyde&lt;/strong&gt; - (&lt;a href="https://github.com/hyde/hyde"&gt;Repo&lt;/a&gt;, &lt;a href="http://hyde.github.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/hyde" rel="nofollow"&gt;PyPI&lt;/a&gt;) Static site generator which began as the Python counterpart to &lt;a href="https://github.com/jekyll/jekyll"&gt;Jekyll&lt;/a&gt;. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lektor&lt;/strong&gt; - (&lt;a href="https://github.com/lektor/lektor"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.getlektor.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Static site generator with built-in admin console and minimal desktop application. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nikola&lt;/strong&gt; - (&lt;a href="https://github.com/getnikola/nikola"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.getnikola.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/nikola" rel="nofollow"&gt;PyPI&lt;/a&gt;) Command-line static site generator with incremental rebuilds and support for Markdown, reST, Jupyter notebooks, and HTML. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pelican&lt;/strong&gt; - (&lt;a href="https://github.com/getpelican/pelican"&gt;Repo&lt;/a&gt;, &lt;a href="https://blog.getpelican.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/pelican" rel="nofollow"&gt;PyPI&lt;/a&gt;) Command-line static site generator that supports Markdown and reST syntax. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prosopopee&lt;/strong&gt; - (&lt;a href="https://github.com/Psycojoker/prosopopee"&gt;Repo&lt;/a&gt;, &lt;a href="https://surleschemins.fr/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://pypi.org/project/prosopopee" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://prosopopee.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) A static site generator designed for photographers and others who tell stories with pictures. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyVideo&lt;/strong&gt; - (&lt;a href="https://github.com/pyvideo/pyvideo"&gt;Repo&lt;/a&gt;, &lt;a href="https://pyvideo.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Static media index custom-built for the Python community, and all the content our meetings and conferences produce. &lt;code&gt;(video, linux, server)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-dev" class="anchor" aria-hidden="true" href="#dev"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev" href="#tag-dev"&gt;Dev&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Projects related to software development and adjacent technical areas.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-scm" class="anchor" aria-hidden="true" href="#scm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev.scm" href="#tag-dev.scm"&gt;SCM&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Allura&lt;/strong&gt; - (&lt;a href="https://github.com/apache/allura"&gt;Repo&lt;/a&gt;, &lt;a href="https://allura.apache.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Apache_Allura" rel="nofollow"&gt;WP&lt;/a&gt;) Software &lt;a href="https://en.wikipedia.org/wiki/Forge_(software)" rel="nofollow"&gt;forge&lt;/a&gt;, with support for git, hg, and svn. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dvc (Data Version Control)&lt;/strong&gt; - (&lt;a href="https://github.com/iterative/dvc"&gt;Repo&lt;/a&gt;, &lt;a href="https://dvc.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://dvc.org/doc" rel="nofollow"&gt;Docs&lt;/a&gt;) Command-line tool for version control over data used in machine learning projects. Aims to replace Excel and other tools used to track and deploy model versions. &lt;code&gt;(organization, linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Git Cola&lt;/strong&gt; - (&lt;a href="https://github.com/git-cola/git-cola"&gt;Repo&lt;/a&gt;, &lt;a href="https://git-cola.github.io/" rel="nofollow"&gt;Home&lt;/a&gt;) Featureful cross-platform GUI wrapper for &lt;code&gt;git&lt;/code&gt;. &lt;code&gt;(linux, windows, mac, qt4, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gitless&lt;/strong&gt; - (&lt;a href="https://github.com/sdg-mit/gitless"&gt;Repo&lt;/a&gt;, &lt;a href="https://gitless.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/gitless" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://gitless.com/#documentation" rel="nofollow"&gt;Docs&lt;/a&gt;) Simple version control system built on top of Git. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GNU Bazaar&lt;/strong&gt; - (&lt;a href="https://code.launchpad.net/bzr" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="http://bazaar.canonical.com/en" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/GNU_Bazaar" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="http://doc.bazaar.canonical.com/en" rel="nofollow"&gt;Docs&lt;/a&gt;) Distributed and client-server revision control system. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kallithea&lt;/strong&gt; - (&lt;a href="https://kallithea-scm.org/repos/kallithea" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Kallithea_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) Software &lt;a href="https://en.wikipedia.org/wiki/Forge_(software)" rel="nofollow"&gt;forge&lt;/a&gt; for Mercurial and Git with a built-in push/pull server, full text search, and code-review. Forked from RhodeCode in 2014. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Klaus&lt;/strong&gt; - (&lt;a href="https://github.com/jonashaag/klaus"&gt;Repo&lt;/a&gt;, &lt;a href="http://klausdemo.lophus.org/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://pypi.org/project/klaus" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://github.com/jonashaag/klaus/wiki"&gt;Docs&lt;/a&gt;) pip-installable web-based viewer for git repositories that "just works". &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Launchpad&lt;/strong&gt; - (&lt;a href="https://launchpad.net/launchpad" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://launchpad.net/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Launchpad_%28website%29" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://dev.launchpad.net/" rel="nofollow"&gt;Docs&lt;/a&gt;) Software forge designed and run by Canonical, with support for Git and &lt;a href="https://en.wikipedia.org/wiki/GNU_Bazaar" rel="nofollow"&gt;Bazaar&lt;/a&gt;. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mercurial&lt;/strong&gt; - (&lt;a href="https://www.mercurial-scm.org/repo/hg-stable" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.mercurial-scm.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Mercurial" rel="nofollow"&gt;WP&lt;/a&gt;) Cross-platform distributed revision-control system designed for high performance and advanced branching/merging capabilities. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pagure&lt;/strong&gt; - (&lt;a href="https://pagure.io/pagure" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://pagure.io/" rel="nofollow"&gt;Home&lt;/a&gt;) Software &lt;a href="https://en.wikipedia.org/wiki/Forge_(software)" rel="nofollow"&gt;forge&lt;/a&gt; focused on git and developed by the Fedora engineering team. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patchwork&lt;/strong&gt; - (&lt;a href="https://github.com/getpatchwork/patchwork"&gt;Repo&lt;/a&gt;, &lt;a href="http://jk.ozlabs.org/projects/patchwork" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://patchwork.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based patch tracking system designed to facilitate code contribution to an open-source project. Designed and used for Linux kernel subsystem development. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RabbitVCS&lt;/strong&gt; - (&lt;a href="https://github.com/rabbitvcs/rabbitvcs"&gt;Repo&lt;/a&gt;, &lt;a href="http://rabbitvcs.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="http://wiki.rabbitvcs.org/wiki" rel="nofollow"&gt;Docs&lt;/a&gt;) Tools providing straightforward graphical access to Subversion or Git within a variety of clients, including as Nautilus, Thunar, Nemo, Caja, and the command line. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RhodeCode&lt;/strong&gt; - (&lt;a href="https://code.rhodecode.com/rhodecode-enterprise-ce" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://rhodecode.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/RhodeCode" rel="nofollow"&gt;WP&lt;/a&gt;) Self-hosted platform for behind-the-firewall source code management, providing centralized control over Git, Mercurial, and Subversion. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Roundup&lt;/strong&gt; - (&lt;a href="http://hg.code.sf.net/p/roundup/code" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Roundup_%28issue_tracker%29" rel="nofollow"&gt;WP&lt;/a&gt;) Highly-customizable issue tracking system featuring command-line, web, and email interfaces, used by the official Python bug tracker at &lt;a href="https://bugs.python.org" rel="nofollow"&gt;bugs.python.org&lt;/a&gt;. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TortoiseHg&lt;/strong&gt; - (&lt;a href="https://bitbucket.org/tortoisehg/thg/src" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://tortoisehg.bitbucket.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://bitbucket.org/tortoisehg/thg/wiki/developers/Home" rel="nofollow"&gt;Docs&lt;/a&gt;) Windows shell extension and a series of applications for the Mercurial distributed revision control system. Also includes GNOME and CLI support. &lt;code&gt;(linux, windows, qt4, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Trac&lt;/strong&gt; - (&lt;a href="https://github.com/edgewall/trac"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Trac" rel="nofollow"&gt;WP&lt;/a&gt;) Enhanced web-based wiki and issue tracking system for software development projects. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ViewVC&lt;/strong&gt; - (&lt;a href="https://github.com/viewvc/viewvc"&gt;Repo&lt;/a&gt;, &lt;a href="http://viewvc.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Browser interface for CVS and Subversion version control repositories. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-code-review" class="anchor" aria-hidden="true" href="#code-review"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev.code_review" href="#tag-dev.code_review"&gt;Code Review&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Diffoscope&lt;/strong&gt; - (&lt;a href="https://salsa.debian.org/reproducible-builds/diffoscope" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://diffoscope.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://try.diffoscope.org/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://pypi.org/project/diffoscope" rel="nofollow"&gt;PyPI&lt;/a&gt;) Web-based deep comparison of files, archives, and directories, including support for diffing tarballs, ISO images, and PDFs. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Meld&lt;/strong&gt; - (&lt;a href="https://github.com/GNOME/meld"&gt;Repo&lt;/a&gt;, &lt;a href="http://meldmerge.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Visual diff and merge tool targeted at developers, providing two- and three-way comparison of both files and directories, and supports many version control systems including Git, Mercurial, Bazaar, and Subversion. &lt;code&gt;(linux, windows, mac, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Review Board&lt;/strong&gt; - (&lt;a href="https://github.com/reviewboard/reviewboard"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.reviewboard.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Extensible code review tool for projects and companies of all sizes. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rietveld&lt;/strong&gt; - (&lt;a href="https://github.com/rietveld-codereview/rietveld"&gt;Repo&lt;/a&gt;, &lt;a href="https://codereview.appspot.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Rietveld_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) Django-based collaborative code review tool for Subversion written by &lt;a href="https://en.wikipedia.org/wiki/Guido_van_Rossum" rel="nofollow"&gt;Guido van Rossum&lt;/a&gt; to run on &lt;a href="https://en.wikipedia.org/wiki/Google_App_Engine" rel="nofollow"&gt;Google AppEngine&lt;/a&gt;. The basis for &lt;a href="https://en.wikipedia.org/wiki/Gerrit_(software)" rel="nofollow"&gt;Gerrit&lt;/a&gt;. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-storage" class="anchor" aria-hidden="true" href="#storage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev.storage" href="#tag-dev.storage"&gt;Storage&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;B2&lt;/strong&gt; - (&lt;a href="https://github.com/Backblaze/B2_Command_Line_Tool"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.python.org/pypi/b2" rel="nofollow"&gt;PyPI&lt;/a&gt;) Command-line tool that gives easy access to all of the capabilities of Backblaze's &lt;a href="https://www.backblaze.com/b2/cloud-storage.html" rel="nofollow"&gt;B2 Cloud Storage&lt;/a&gt;. &lt;code&gt;(linux, windows, mac, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Barman&lt;/strong&gt; - (&lt;a href="https://github.com/2ndquadrant-it/barman"&gt;Repo&lt;/a&gt;) Remote backup and disaster recovery for PostgreSQL. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Datasette&lt;/strong&gt; - (&lt;a href="https://github.com/simonw/datasette"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/datasette" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://datasette.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) A tool for exploring and publishing data, backed by SQLite. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EdgeDB&lt;/strong&gt; - (&lt;a href="https://github.com/edgedb/edgedb"&gt;Repo&lt;/a&gt;, &lt;a href="https://edgedb.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://edgedb.com/docs" rel="nofollow"&gt;Docs&lt;/a&gt;) High-performance object-relational database built on top of PostgreSQL, featuring strict, strong typing, built-in migrations, and GraphQL support. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FreeNAS&lt;/strong&gt; - (&lt;a href="https://github.com/freenas/freenas"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.freenas.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.ixsystems.com/documentation/freenas" rel="nofollow"&gt;Docs&lt;/a&gt;) Operating system designed to be installed virtually any hardware platform, for sharing &lt;a href="https://en.wikipedia.org/wiki/ZFS" rel="nofollow"&gt;ZFS&lt;/a&gt;-based storage over a network, using SMB, NFS, AFP, FTP, and more. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gridsync&lt;/strong&gt; - (&lt;a href="https://github.com/gridsync/gridsync"&gt;Repo&lt;/a&gt;) Cross-platform GUI built to synchronize local directories with Tahoe-LAFS storage grids. &lt;code&gt;(productivity, linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;kinto&lt;/strong&gt; - (&lt;a href="https://github.com/Kinto/kinto"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.kinto-storage.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="http://docs.kinto-storage.org/" rel="nofollow"&gt;Docs&lt;/a&gt;) A generic JSON document store with sharing and synchronisation capabilities, supporting in-memory and PostgreSQL backends. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nuxeo Drive&lt;/strong&gt; - (&lt;a href="https://github.com/nuxeo/nuxeo-drive"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.nuxeo.com/products/drive-desktop-sync" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://doc.nuxeo.com/client-apps/nuxeo-drive" rel="nofollow"&gt;Docs&lt;/a&gt;) Cross-platform desktop synchronization client for the Nuxeo platform. &lt;code&gt;(productivity, linux, windows, mac, console, appimage, lgpl, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pgcli&lt;/strong&gt; - (&lt;a href="https://github.com/dbcli/pgcli"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.pgcli.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.python.org/pypi/pgcli" rel="nofollow"&gt;PyPI&lt;/a&gt;) Interactive PostgreSQL client that does auto-completion and syntax highlighting. &lt;code&gt;(linux, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;s3ql&lt;/strong&gt; - (&lt;a href="https://github.com/s3ql/s3ql"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.rath.org/s3ql-docs/index.html" rel="nofollow"&gt;Docs&lt;/a&gt;) A standards-conforming, full-featured UNIX filesystem for cloud-based storage services (S3, Google Storage, OpenStack), supporting compression, encryption, deduplication, snapshotting, and more. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seafile&lt;/strong&gt; - (&lt;a href="https://github.com/haiwen/seahub"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Seafile" rel="nofollow"&gt;WP&lt;/a&gt;) Cross-platform file hosting and synchronization system. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sqlmap&lt;/strong&gt; - (&lt;a href="https://github.com/sqlmapproject/sqlmap"&gt;Repo&lt;/a&gt;, &lt;a href="http://sqlmap.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://github.com/sqlmapproject/sqlmap/wiki"&gt;Docs&lt;/a&gt;) Automatic SQL injection and database takeover. &lt;code&gt;(security, console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TahoeLAFS&lt;/strong&gt; - (&lt;a href="https://github.com/tahoe-lafs/tahoe-lafs"&gt;Repo&lt;/a&gt;, &lt;a href="https://tahoe-lafs.org/trac/tahoe-lafs" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Tahoe-LAFS" rel="nofollow"&gt;WP&lt;/a&gt;) Decentralized cloud storage system for robust distributed data storage. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WAL-E&lt;/strong&gt; - (&lt;a href="https://github.com/wal-e/wal-e"&gt;Repo&lt;/a&gt;) Continuous archiving of PostgreSQL WAL files and base backups. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ZEO&lt;/strong&gt; - (&lt;a href="https://github.com/zopefoundation/ZEO"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/ZEO" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://zope.readthedocs.io/en/latest/zopebook/ZEO.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Server and client providing &lt;a href="http://www.zodb.org/" rel="nofollow"&gt;ZODB&lt;/a&gt;-based storage over the network. &lt;code&gt;(linux, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ZFSp&lt;/strong&gt; - (&lt;a href="https://github.com/alcarithemad/zfsp"&gt;Repo&lt;/a&gt;) A reverse-engineered &lt;a href="https://en.wikipedia.org/wiki/ZFS" rel="nofollow"&gt;ZFS&lt;/a&gt; implementation, written in Python, without reading the original C. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-ops" class="anchor" aria-hidden="true" href="#ops"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev.ops" href="#tag-dev.ops"&gt;Ops&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Airflow&lt;/strong&gt; - (&lt;a href="https://github.com/apache/airflow"&gt;Repo&lt;/a&gt;, &lt;a href="https://airflow.apache.org/" rel="nofollow"&gt;Docs&lt;/a&gt;) A platform to programmatically author, schedule and monitor workflows. &lt;code&gt;(linux, server, corp, flask)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ansible&lt;/strong&gt; - (&lt;a href="https://github.com/ansible/ansible"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.ansible.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://docs.ansible.com/ansible" rel="nofollow"&gt;Docs&lt;/a&gt;) Agentless, playbook-based automation. &lt;code&gt;(linux, mac, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;aws-cli&lt;/strong&gt; - (&lt;a href="https://github.com/aws/aws-cli"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/awscli" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://docs.aws.amazon.com/cli/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Official command-line interface for Amazon Web Services. &lt;code&gt;(console, py26)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beaker&lt;/strong&gt; - (&lt;a href="https://git.beaker-project.org/cgit/beaker" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://beaker-project.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://beaker-project.org/docs" rel="nofollow"&gt;Docs&lt;/a&gt;) Hardware integration testing system, used by RedHat to test compatiblity for RHEL and Fedora. &lt;code&gt;(server, flask)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cobbler&lt;/strong&gt; - (&lt;a href="https://github.com/Cobbler/Cobbler"&gt;Repo&lt;/a&gt;, &lt;a href="https://cobbler.github.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Cobbler_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) Linux installation server that allows for rapid setup of network installation environments. &lt;code&gt;(linux, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DCOS&lt;/strong&gt; - (&lt;a href="https://github.com/dcos/dcos"&gt;Repo&lt;/a&gt;, &lt;a href="https://dcos.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Mesosphere%2C_Inc.#Mesosphere_DC/OS" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://dcos.io/docs" rel="nofollow"&gt;Docs&lt;/a&gt;) Management platform for hardware and software resources in datacenters, built on &lt;a href="https://en.wikipedia.org/wiki/Apache_Mesos" rel="nofollow"&gt;Apache Mesos&lt;/a&gt;. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fail2ban&lt;/strong&gt; - (&lt;a href="https://github.com/fail2ban/fail2ban"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.fail2ban.org/wiki/index.php/Main_Page" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Fail2ban" rel="nofollow"&gt;WP&lt;/a&gt;) Daemon to ban hosts that cause multiple authentication errors on Linux servers. &lt;code&gt;(linux, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ganeti&lt;/strong&gt; - (&lt;a href="https://github.com/ganeti/ganeti"&gt;Repo&lt;/a&gt;) Virtual machine cluster management tool built on existing virtualization technologies such as &lt;a href="https://en.wikipedia.org/wiki/Xen" rel="nofollow"&gt;Xen&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine" rel="nofollow"&gt;KVM&lt;/a&gt;. &lt;code&gt;(linux, server, haskell)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Glances&lt;/strong&gt; - (&lt;a href="https://github.com/nicolargo/glances"&gt;Repo&lt;/a&gt;, &lt;a href="https://nicolargo.github.io/glances" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://glances.readthedocs.io/en/stable" rel="nofollow"&gt;Docs&lt;/a&gt;) A cross-platform top/htop alternative, providing an overview of system resources. &lt;code&gt;(productivity, linux, windows, mac, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gunicorn&lt;/strong&gt; - (&lt;a href="https://github.com/benoitc/gunicorn"&gt;Repo&lt;/a&gt;, &lt;a href="https://gunicorn.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.python.org/pypi/gunicorn" rel="nofollow"&gt;PyPI&lt;/a&gt;) Pluggable, pre-fork WSGI server, started as the counterpart to &lt;a href="https://en.wikipedia.org/wiki/Unicorn_(web_server)" rel="nofollow"&gt;Unicorn&lt;/a&gt;. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Healthchecks&lt;/strong&gt; - (&lt;a href="https://github.com/healthchecks/healthchecks"&gt;Repo&lt;/a&gt;, &lt;a href="https://healthchecks.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://healthchecks.io/docs" rel="nofollow"&gt;Docs&lt;/a&gt;) Web-based monitor for scheduled jobs (e.g., cron). &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Iris&lt;/strong&gt; - (&lt;a href="https://github.com/linkedin/iris"&gt;Repo&lt;/a&gt;, &lt;a href="https://iris.claims/" rel="nofollow"&gt;Home&lt;/a&gt;) Flexible automated incident paging system, developed by and used at LinkedIn. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nagstamon&lt;/strong&gt; - (&lt;a href="https://github.com/HenriWahl/Nagstamon"&gt;Repo&lt;/a&gt;, &lt;a href="https://nagstamon.ifw-dresden.de/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://nagstamon.ifw-dresden.de/docs" rel="nofollow"&gt;Docs&lt;/a&gt;) Status monitor for the desktop, with support for Nagios, Icinga, Opsview, and more. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NColony&lt;/strong&gt; - (&lt;a href="https://github.com/ncolony/ncolony"&gt;Repo&lt;/a&gt;, &lt;a href="http://ncolony.org/en/latest" rel="nofollow"&gt;Home&lt;/a&gt;) Process manager and monitor. &lt;code&gt;(linux, mac, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;netbox&lt;/strong&gt; - (&lt;a href="https://github.com/netbox-community/netbox"&gt;Repo&lt;/a&gt;, &lt;a href="https://netbox.readthedocs.io/en/stable" rel="nofollow"&gt;Docs&lt;/a&gt;) IP address management (IPAM) and data center infrastructure management (DCIM) tool, conceived at Digital Ocean. &lt;code&gt;(server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nsupdate.info&lt;/strong&gt; - (&lt;a href="https://github.com/nsupdate-info/nsupdate.info"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/nsupdate" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://nsupdateinfo.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Featureful dynamic DNS service, using the Dynamic DNS UPDATE protocol (&lt;a href="https://tools.ietf.org/html/rfc2136" rel="nofollow"&gt;RFC 2136&lt;/a&gt;) to update BIND and other major nameservers. &lt;code&gt;(internet, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Oncall&lt;/strong&gt; - (&lt;a href="https://github.com/linkedin/oncall"&gt;Repo&lt;/a&gt;, &lt;a href="https://oncall.tools/" rel="nofollow"&gt;Home&lt;/a&gt;) Calendar tool designed for on-call management and scheduling, developed by and used at LinkedIn. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenStack&lt;/strong&gt; - (&lt;a href="https://github.com/openstack/openstack"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.openstack.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://docs.openstack.org/" rel="nofollow"&gt;Docs&lt;/a&gt;) Cloud operating system that controls large pools of compute, storage, and networking resources throughout a datacenter, manageable through a web-based dashboard. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pulp&lt;/strong&gt; - (&lt;a href="https://github.com/pulp/pulp"&gt;Repo&lt;/a&gt;, &lt;a href="https://pulpproject.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://docs.pulpproject.org/en/3.0/nightly" rel="nofollow"&gt;Docs&lt;/a&gt;) Platform for managing repositories of software packages and making it available to a large numbers of consumers. Developed and used by Red Hat. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ralph&lt;/strong&gt; - (&lt;a href="https://github.com/allegro/ralph"&gt;Repo&lt;/a&gt;, &lt;a href="https://ralph.allegro.tech/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://ralph-ng.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Simple and powerful Asset Management, DCIM, and CMDB system for the data center and back office. &lt;code&gt;(server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Salt Stack&lt;/strong&gt; - (&lt;a href="https://github.com/saltstack/salt"&gt;Repo&lt;/a&gt;, &lt;a href="https://repo.saltstack.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Automation for the management and configuration of any infrastructure or application at scale. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shinken&lt;/strong&gt; - (&lt;a href="https://github.com/naparuba/shinken"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.shinken-monitoring.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Shinken is a modern, Nagios-compatible monitoring framework, designed to scale for large environments. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spinnaker&lt;/strong&gt; - (&lt;a href="https://github.com/spinnaker/spinnaker"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.spinnaker.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Spinnaker_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://www.spinnaker.io/concepts" rel="nofollow"&gt;Docs&lt;/a&gt;) Continuous delivery platform developed for Netflix's deployment and management of applications in cloud environments. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;StackStorm&lt;/strong&gt; - (&lt;a href="https://github.com/StackStorm/st2"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.stackstorm.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Rules- and event-driven operational automation for auto-remediation, security responses, troubleshooting, deployments, and more. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Supervisor&lt;/strong&gt; - (&lt;a href="https://github.com/Supervisor/supervisor"&gt;Repo&lt;/a&gt;, &lt;a href="http://supervisord.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Process manager and monitor. &lt;code&gt;(linux, mac, server)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-security" class="anchor" aria-hidden="true" href="#security"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev.security" href="#tag-dev.security"&gt;Security&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;BYOB (Build Your Own Botnet)&lt;/strong&gt; - (&lt;a href="https://github.com/malwaredllc/byob"&gt;Repo&lt;/a&gt;) Client-server framework (RAT and C2 server) for security researchers to build and operate basic botnets. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CAPE&lt;/strong&gt; - (&lt;a href="https://github.com/ctxis/CAPE"&gt;Repo&lt;/a&gt;, &lt;a href="https://cape.contextis.com/submit" rel="nofollow"&gt;Demo&lt;/a&gt;) Web application designed to automate malware analysis, with a goal of extracting payloads and configuration from uploaded artifacts. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cowrie&lt;/strong&gt; - (&lt;a href="https://github.com/cowrie/cowrie"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.cowrie.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Medium interaction SSH and Telnet honeypot designed to log brute force attacks and the shell interaction performed by the attacker. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GRR Rapid Response&lt;/strong&gt; - (&lt;a href="https://github.com/google/grr"&gt;Repo&lt;/a&gt;, &lt;a href="https://grr-doc.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Server-agent system focused on remote live forensics for quick, browser-based triage and analysis of attacks on fleets of machines, with agent support for Linux, Windows, and OS X. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hosts&lt;/strong&gt; - (&lt;a href="https://github.com/StevenBlack/hosts"&gt;Repo&lt;/a&gt;) Command-line application which merges reputable &lt;a href="https://en.wikipedia.org/wiki/Hosts_(file)" rel="nofollow"&gt;hosts files&lt;/a&gt; with deduplication for the purpose of blocking undesirable websites via DNS blackhole. &lt;code&gt;(internet, linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hubble&lt;/strong&gt; - (&lt;a href="https://github.com/hubblestack/hubble"&gt;Repo&lt;/a&gt;, &lt;a href="https://hubblestack.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Modular security compliance client, providing on-demand profile-based auditing, alerting, and reporting. Originally designed for Adobe. &lt;code&gt;(linux, windows, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Infection Monkey&lt;/strong&gt; - (&lt;a href="https://github.com/guardicore/monkey"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.guardicore.com/infectionmonkey" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://github.com/guardicore/monkey/wiki"&gt;Docs&lt;/a&gt;) Web-based tool for testing a datacenter's resiliency to perimeter breaches and internal server infection. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;King Phisher&lt;/strong&gt; - (&lt;a href="https://github.com/securestate/king-phisher"&gt;Repo&lt;/a&gt;, &lt;a href="https://king-phisher.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Server-based &lt;a href="https://en.wikipedia.org/wiki/Phishing" rel="nofollow"&gt;phishing&lt;/a&gt; campaign toolkit, used to simulate real-world phishing attacks, with GTK-powered client application. &lt;code&gt;(linux, windows, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LinOTP&lt;/strong&gt; - (&lt;a href="https://github.com/LinOTP/LinOTP"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.linotp.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/LinOTP" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://www.linotp.org/documentation.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Server supporting two-factor authentication with one-time passwords from several sources, from Yubikeys to SMS. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maltrail&lt;/strong&gt; - (&lt;a href="https://github.com/stamparm/maltrail"&gt;Repo&lt;/a&gt;) Malicious traffic detection system with web-based monitoring. &lt;code&gt;(linux, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MITMproxy&lt;/strong&gt; - (&lt;a href="https://github.com/mitmproxy/mitmproxy"&gt;Repo&lt;/a&gt;, &lt;a href="https://mitmproxy.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Interactive TLS-capable intercepting HTTP proxy for penetration testers and software developers. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MozDef&lt;/strong&gt; - (&lt;a href="https://github.com/mozilla/MozDef"&gt;Repo&lt;/a&gt;, &lt;a href="https://mozdef.readthedocs.io/en/latest?badge=latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Security incident automation with metrics and collaboration tools for defenders. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenSnitch&lt;/strong&gt; - (&lt;a href="https://github.com/evilsocket/opensnitch"&gt;Repo&lt;/a&gt;, &lt;a href="https://opensnitch.io/" rel="nofollow"&gt;Home&lt;/a&gt;) GNU/Linux port of the &lt;a href="https://en.wikipedia.org/wiki/Little_Snitch" rel="nofollow"&gt;Little Snitch&lt;/a&gt; application firewall. &lt;code&gt;(linux, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Passit&lt;/strong&gt; - (&lt;a href="https://gitlab.com/passit/passit-backend" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://passit.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://passit.io/documentation" rel="nofollow"&gt;Docs&lt;/a&gt;) Password management server, providing storage services and group access control list features. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;privacyIDEA&lt;/strong&gt; - (&lt;a href="https://github.com/privacyidea/privacyidea"&gt;Repo&lt;/a&gt;, &lt;a href="https://privacyidea.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/PrivacyIDEA" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://privacyidea.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) A multi factor authentication server running on premises, supporting many different token types and allowing authentication via REST API, RADIUS, PAM, Windows Credential Provider, SAML, OpenID Connect. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Psono&lt;/strong&gt; - (&lt;a href="https://gitlab.com/psono/psono-server" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://psono.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.psono.pw/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://doc.psono.com/" rel="nofollow"&gt;Docs&lt;/a&gt;) Server-based password manager, built for teams. &lt;code&gt;(productivity, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pupy&lt;/strong&gt; - (&lt;a href="https://github.com/n1nj4sec/pupy"&gt;Repo&lt;/a&gt;, &lt;a href="https://github.com/n1nj4sec/pupy/wiki/Installation"&gt;Docs&lt;/a&gt;) Remote administration tool and post-exploitation framework, supporting Windows, Linux, Mac OS X, and Android targets. &lt;code&gt;(linux, docker, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyEW&lt;/strong&gt; - (&lt;a href="https://github.com/joxeankoret/pyew"&gt;Repo&lt;/a&gt;, &lt;a href="https://github.com/joxeankoret/pyew/wiki"&gt;Docs&lt;/a&gt;) Malware analysis tool, with support for hexadecimal viewing, disassembly, PE and ELF formats, plugins, and more. &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Searx&lt;/strong&gt; - (&lt;a href="https://github.com/asciimoo/searx"&gt;Repo&lt;/a&gt;, &lt;a href="https://asciimoo.github.io/searx" rel="nofollow"&gt;Docs&lt;/a&gt;) Self-hosted metasearch engine, aggregating results from more than 70 services while avoiding tracking and profiling. &lt;code&gt;(internet, server, flask)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spiderfoot&lt;/strong&gt; - (&lt;a href="https://github.com/smicallef/spiderfoot"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.spiderfoot.net/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://www.spiderfoot.net/documentation" rel="nofollow"&gt;Docs&lt;/a&gt;) Reconnaissance tool that automatically queries over 100 public data sources to gather intelligence on IP addresses, domain names, e-mail addresses, names, and more. &lt;code&gt;(linux, windows, mac, docker, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sqlmap&lt;/strong&gt; - (&lt;a href="https://github.com/sqlmapproject/sqlmap"&gt;Repo&lt;/a&gt;, &lt;a href="http://sqlmap.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://github.com/sqlmapproject/sqlmap/wiki"&gt;Docs&lt;/a&gt;) Automatic SQL injection and database takeover. &lt;code&gt;(storage, console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sshuttle&lt;/strong&gt; - (&lt;a href="https://github.com/sshuttle/sshuttle"&gt;Repo&lt;/a&gt;, &lt;a href="https://sshuttle.readthedocs.org/" rel="nofollow"&gt;Docs&lt;/a&gt;) Transparent network proxy server that uses SSH to achieve VPN-like results, without requiring root access. &lt;code&gt;(linux, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Universal Radio Hacker (URH)&lt;/strong&gt; - (&lt;a href="https://github.com/jopohl/urh"&gt;Repo&lt;/a&gt;) Wireless protocol investigator, with a focus on analyzing proprietary IoT communication. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;XSStrike&lt;/strong&gt; - (&lt;a href="https://github.com/s0md3v/XSStrike"&gt;Repo&lt;/a&gt;, &lt;a href="https://somdev.me/XSStrike" rel="nofollow"&gt;Home&lt;/a&gt;) &lt;a href="https://en.wikipedia.org/wiki/Cross-site_scripting" rel="nofollow"&gt;Cross Site Scripting&lt;/a&gt; (XSS) detection suite equipped with multiple hand-written parsers, a payload generator, a fuzzing engine, and a performance-focused crawler. &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-docs" class="anchor" aria-hidden="true" href="#docs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev.docs" href="#tag-dev.docs"&gt;Docs&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;asciidoc&lt;/strong&gt; - (&lt;a href="https://github.com/asciidoc/asciidoc"&gt;Repo&lt;/a&gt;) Text document format for writing notes, documentation, articles, books, slideshows, man pages &amp;amp; blogs. &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;doc2dash&lt;/strong&gt; - (&lt;a href="https://github.com/hynek/doc2dash"&gt;Repo&lt;/a&gt;, &lt;a href="https://doc2dash.readthedocs.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/doc2dash" rel="nofollow"&gt;PyPI&lt;/a&gt;) Extensible CLI-based &lt;a href="https://developer.apple.com/library/archive/documentation/DeveloperTools/Conceptual/Documentation_Sets/010-Overview_of_Documentation_Sets/docset_overview.html#//apple_ref/doc/uid/TP40005266-CH13-SW6" rel="nofollow"&gt;Documentation Set&lt;/a&gt; generator intended for use with &lt;a href="https://kapeli.com/dash/" rel="nofollow"&gt;Dash.app&lt;/a&gt; and &lt;a href="https://velocity.silverlakesoftware.com/" rel="nofollow"&gt;other&lt;/a&gt; &lt;a href="https://github.com/dash-docs-el/helm-dash"&gt;compatible&lt;/a&gt; &lt;a href="https://zealdocs.org/" rel="nofollow"&gt;API browsers&lt;/a&gt;. &lt;code&gt;(linux, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gaphor&lt;/strong&gt; - (&lt;a href="https://github.com/gaphor/gaphor"&gt;Repo&lt;/a&gt;, &lt;a href="https://gaphor.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Simple &lt;a href="https://en.wikipedia.org/wiki/Unified_Modeling_Language" rel="nofollow"&gt;UML&lt;/a&gt; modeling tool designed for beginners. &lt;code&gt;(graphics, linux, windows, mac, flatpak, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kuma&lt;/strong&gt; - (&lt;a href="https://github.com/mozilla/kuma"&gt;Repo&lt;/a&gt;, &lt;a href="https://developer.mozilla.org/en-US" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://kuma.readthedocs.io/en/latest/installation.html" rel="nofollow"&gt;Docs&lt;/a&gt;) The platform powering the Mozilla Developer Network (MDN) &lt;code&gt;(server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mkdocs&lt;/strong&gt; - (&lt;a href="https://github.com/mkdocs/mkdocs"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.mkdocs.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Simple, customizable project documentation, with built-in dev server. &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;readthedocs.org&lt;/strong&gt; - (&lt;a href="https://github.com/readthedocs/readthedocs.org"&gt;Repo&lt;/a&gt;, &lt;a href="https://readthedocs.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://docs.readthedocs.io/en/stable" rel="nofollow"&gt;Docs&lt;/a&gt;) Continuous integration platform for building and hosting documentation. &lt;code&gt;(server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sphinx&lt;/strong&gt; - (&lt;a href="https://github.com/sphinx-doc/sphinx"&gt;Repo&lt;/a&gt;, &lt;a href="http://sphinx-doc.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Documentation tool for interconnected bodies of authorship, from code documentation to books. Used by &lt;a href="https://docs.python.org" rel="nofollow"&gt;the official Python docs&lt;/a&gt;, and many other projects (&lt;a href="https://varnish-cache.org/docs/" rel="nofollow"&gt;not all of them Python&lt;/a&gt;). &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-editor" class="anchor" aria-hidden="true" href="#editor"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev.editor" href="#tag-dev.editor"&gt;Editor&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Black&lt;/strong&gt; - (&lt;a href="https://github.com/ambv/black"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/black" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://black.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Uncompromising automatic formatter for Python code. &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eric IDE&lt;/strong&gt; - (&lt;a href="http://die-offenbachs.homelinux.org:48888/hg/eric" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://eric-ide.python-projects.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Python editor and IDE, based on Qt, integrating Scintilla editor control. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gedit&lt;/strong&gt; - (&lt;a href="https://gitlab.gnome.org/GNOME/gedit" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Gedit" rel="nofollow"&gt;WP&lt;/a&gt;) The default GNOME text editor makes extensive use of Python, in addition to C. &lt;code&gt;(linux, c, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Notebook&lt;/strong&gt; - (&lt;a href="https://github.com/jupyter/notebook"&gt;Repo&lt;/a&gt;, &lt;a href="https://jupyter.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Project_Jupyter#Jupyter_Notebook" rel="nofollow"&gt;WP&lt;/a&gt;) Web-based, extensible notebook environment for interactive computing. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Komodo Edit&lt;/strong&gt; - (&lt;a href="https://github.com/Komodo/KomodoEdit"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.activestate.com/products/komodo-edit" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Komodo_Edit" rel="nofollow"&gt;WP&lt;/a&gt;) Multi-language code editor, written in JS, Python, and C++, based on the Mozilla platform. &lt;code&gt;(linux, windows, mac, cpp, js)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leo Editor&lt;/strong&gt; - (&lt;a href="https://github.com/leo-editor/leo-editor"&gt;Repo&lt;/a&gt;, &lt;a href="http://leoeditor.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Leo_%28text_editor%29" rel="nofollow"&gt;WP&lt;/a&gt;) Personal Information Manager (PIM), IDE, and outliner with a holistic approach to programming and writing. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mu&lt;/strong&gt; - (&lt;a href="https://github.com/mu-editor/mu"&gt;Repo&lt;/a&gt;, &lt;a href="https://codewith.mu/en" rel="nofollow"&gt;Home&lt;/a&gt;) A small, simple editor designed for beginner Python programmers. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ninja IDE&lt;/strong&gt; - (&lt;a href="https://github.com/ninja-ide/ninja-ide"&gt;Repo&lt;/a&gt;, &lt;a href="http://ninja-ide.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Ninja-IDE" rel="nofollow"&gt;WP&lt;/a&gt;) Cross-platform Python IDE with project management, linting, extensions, and more. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pluma&lt;/strong&gt; - (&lt;a href="https://github.com/mate-desktop/pluma"&gt;Repo&lt;/a&gt;) Small and lightweight UTF-8 text editor for &lt;a href="http://mate-desktop.org/" rel="nofollow"&gt;the MATE environment&lt;/a&gt;. Based on gedit. &lt;code&gt;(linux, c, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ReText&lt;/strong&gt; - (&lt;a href="https://github.com/retext-project/retext"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/ReText" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://github.com/retext-project/retext/wiki"&gt;Docs&lt;/a&gt;) Simple but powerful editor for Markdown and reStructuredText markup languages. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spyder IDE&lt;/strong&gt; - (&lt;a href="https://github.com/spyder-ide/spyder"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.spyder-ide.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Spyder_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) Scientific editing and execution environment designed by and for scientists, engineers, and data analysts using Python. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Thonny&lt;/strong&gt; - (&lt;a href="https://github.com/thonny/thonny"&gt;Repo&lt;/a&gt;, &lt;a href="https://thonny.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Thonny" rel="nofollow"&gt;WP&lt;/a&gt;) Cross-platform Python IDE for beginners, designed for learning to code. &lt;code&gt;(linux, windows, mac, tk)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-package-managers" class="anchor" aria-hidden="true" href="#package-managers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev.pkg_mgr" href="#tag-dev.pkg_mgr"&gt;Package Managers&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Conan&lt;/strong&gt; - (&lt;a href="https://github.com/conan-io/conan"&gt;Repo&lt;/a&gt;, &lt;a href="https://conan.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://docs.conan.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Decentralized package manager for binary package management, targeted at C/C++ developers. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt; - (&lt;a href="https://github.com/conda/conda"&gt;Repo&lt;/a&gt;, &lt;a href="https://conda.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Conda_%28package_manager%29" rel="nofollow"&gt;WP&lt;/a&gt;) OS-agnostic, system-level binary package manager and ecosystem, with a focus on Python and high-performance scientific computing. &lt;code&gt;(linux, windows, mac, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dnf&lt;/strong&gt; - (&lt;a href="https://github.com/rpm-software-management/dnf"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/DNF_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://dnf.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Dandified YUM (DNF) is the successor to &lt;code&gt;yum&lt;/code&gt; and works everywhere yum worked. &lt;code&gt;(linux, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pip&lt;/strong&gt; - (&lt;a href="https://github.com/pypa/pip"&gt;Repo&lt;/a&gt;, &lt;a href="https://pip.pypa.io/en/stable" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Pip_%28package_manager%29" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://pypi.org/project/pip" rel="nofollow"&gt;PyPI&lt;/a&gt;) Python's go-to package manager, with a wide range of features and platform support. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pip-tools&lt;/strong&gt; - (&lt;a href="https://github.com/jazzband/pip-tools"&gt;Repo&lt;/a&gt;) A set of command line tools to help you keep your pip-based packages fresh, even when you've pinned them. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pipenv&lt;/strong&gt; - (&lt;a href="https://github.com/pypa/pipenv"&gt;Repo&lt;/a&gt;, &lt;a href="https://pipenv.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Wrapper around &lt;code&gt;pip&lt;/code&gt;, &lt;a href="https://github.com/pypa/virtualenv"&gt;&lt;code&gt;virtualenv&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/jazzband/pip-tools"&gt;&lt;code&gt;pip-tools&lt;/code&gt;&lt;/a&gt; for a more holistic package management workflow. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Poetry&lt;/strong&gt; - (&lt;a href="https://github.com/sdispater/poetry"&gt;Repo&lt;/a&gt;, &lt;a href="https://poetry.eustace.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://poetry.eustace.io/docs" rel="nofollow"&gt;Docs&lt;/a&gt;) An independent approach to Python dependency management and packaging. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Portage&lt;/strong&gt; - (&lt;a href="https://gitweb.gentoo.org/proj/portage.git" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Portage_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) Platform-agnostic Package management system created for and used by Gentoo Linux and also by Chrome OS, Sabayon, and Funtoo Linux. Inspired by FreeBSD ports. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Solaris IPS&lt;/strong&gt; - (&lt;a href="https://github.com/oracle/solaris-ips"&gt;Repo&lt;/a&gt;) Software delivery system backed by network repository, featuring safe execution for zones, use of ZFS for efficiency and rollback, preventing the introduction of invalid packages, and efficient use of bandwidth. &lt;code&gt;(linux, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;yum&lt;/strong&gt; - (&lt;a href="https://github.com/rpm-software-management/yum"&gt;Repo&lt;/a&gt;, &lt;a href="http://yum.baseurl.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Yum_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) Automatic updater and package installer/remover for RPM-based systems (Fedora, RHEL, etc.). &lt;code&gt;(linux, corp)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-package-repositories" class="anchor" aria-hidden="true" href="#package-repositories"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev.pkg_repo" href="#tag-dev.pkg_repo"&gt;Package Repositories&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Bandersnatch&lt;/strong&gt; - (&lt;a href="https://github.com/pypa/bandersnatch"&gt;Repo&lt;/a&gt;) PyPI mirror client complying with &lt;a href="http://www.python.org/dev/peps/pep-0381/" rel="nofollow"&gt;PEP 381&lt;/a&gt;. &lt;code&gt;(server, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;devpi&lt;/strong&gt; - (&lt;a href="https://github.com/devpi/devpi"&gt;Repo&lt;/a&gt;, &lt;a href="http://doc.devpi.net/" rel="nofollow"&gt;Docs&lt;/a&gt;) PyPI staging server, as well as a packaging, testing, release tool, complete with web and search interface. Like a local PyPI. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;distro-tracker&lt;/strong&gt; - (&lt;a href="https://salsa.debian.org/qa/distro-tracker" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://tracker.debian.org/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://qa.pages.debian.net/distro-tracker" rel="nofollow"&gt;Docs&lt;/a&gt;) Web application designed to follow the evolution of a Debian-based distribution with email updates and a comprehensive web interface. Powers the &lt;a href="https://tracker.debian.org/" rel="nofollow"&gt;Debian Package Tracker&lt;/a&gt;. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SweetTooth Web&lt;/strong&gt; - (&lt;a href="https://gitlab.gnome.org/Infrastructure/extensions-web" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://extensions.gnome.org/" rel="nofollow"&gt;Home&lt;/a&gt;) The web store for extensions to the &lt;a href="https://en.wikipedia.org/wiki/GNOME" rel="nofollow"&gt;GNOME&lt;/a&gt; desktop environment, supporting adding and updating extensions directly from the browser. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Warehouse&lt;/strong&gt; - (&lt;a href="https://github.com/pypa/warehouse"&gt;Repo&lt;/a&gt;, &lt;a href="https://psfmember.org/civicrm/contribute/transact?reset=1&amp;amp;id=13" rel="nofollow"&gt;Fund&lt;/a&gt;, &lt;a href="https://warehouse.pypa.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Server software that powers &lt;a href="https://pypi.org/" rel="nofollow"&gt;PyPI&lt;/a&gt;, where most Python libraries are downloaded from. &lt;code&gt;(server, fnd)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-build" class="anchor" aria-hidden="true" href="#build"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev.build" href="#tag-dev.build"&gt;Build&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;bitbake&lt;/strong&gt; - (&lt;a href="https://github.com/openembedded/bitbake"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/BitBake" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://www.yoctoproject.org/docs/current/bitbake-user-manual/bitbake-user-manual.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Generic task execution engine that allows shell and Python tasks to be run efficiently and in parallel while working within complex inter-task dependency constraints. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;buildbot&lt;/strong&gt; - (&lt;a href="https://github.com/buildbot/buildbot"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Buildbot" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://www.buildbot.net/" rel="nofollow"&gt;Docs&lt;/a&gt;) Job scheduling system tailored to the needs of continuous integration and software packaging. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Buildout&lt;/strong&gt; - (&lt;a href="https://github.com/buildout/buildout"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Buildout" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="http://docs.buildout.org/" rel="nofollow"&gt;Docs&lt;/a&gt;) Extensible deployment automation tool designed for application-centric assembly and deployment, as well as repeatable Python software builds. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;doit&lt;/strong&gt; - (&lt;a href="https://github.com/pydoit/doit"&gt;Repo&lt;/a&gt;, &lt;a href="https://pydoit.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://opencollective.com/doit" rel="nofollow"&gt;Fund&lt;/a&gt;, &lt;a href="https://pydoit.org/contents.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Command-line task management and automation tool, with directives written in Python. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GYP&lt;/strong&gt; - (&lt;a href="https://chromium.googlesource.com/external/gyp" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://gyp.gsrc.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/GYP_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) AKA 'Generate Your Projects', a build system that generates other build systems. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JHBuild&lt;/strong&gt; - (&lt;a href="https://gitlab.gnome.org/GNOME/jhbuild" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://wiki.gnome.org/Projects/Jhbuild" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://github.com/GNOME/jhbuild"&gt;gh&lt;/a&gt;, &lt;a href="https://developer.gnome.org/jhbuild/stable/getting-started.html.en" rel="nofollow"&gt;Docs&lt;/a&gt;) Tool designed to ease building collections of packages, originally written to build the GNOME desktop from sources. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Meson&lt;/strong&gt; - (&lt;a href="https://github.com/mesonbuild/meson"&gt;Repo&lt;/a&gt;, &lt;a href="http://mesonbuild.com/" rel="nofollow"&gt;Home&lt;/a&gt;) Build system designed for speed and user-friendliness. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pants&lt;/strong&gt; - (&lt;a href="https://github.com/pantsbuild/pants"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.pantsbuild.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Build system designed for monolithic repositories. &lt;code&gt;(linux, mac, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PlatformIO Core&lt;/strong&gt; - (&lt;a href="https://github.com/platformio/platformio-core"&gt;Repo&lt;/a&gt;, &lt;a href="https://platformio.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://platformio.org/donate?utm_source=github&amp;amp;utm_medium=core" rel="nofollow"&gt;Fund&lt;/a&gt;, &lt;a href="https://pypi.org/project/platformio" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://docs.platformio.org/en/latest?utm_source=github&amp;amp;utm_medium=core" rel="nofollow"&gt;Docs&lt;/a&gt;) Multiplatform CLI build system and library manager for IoT development. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;redo&lt;/strong&gt; - (&lt;a href="https://github.com/apenwarr/redo"&gt;Repo&lt;/a&gt;, &lt;a href="https://redo.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) A recursive, general-purpose build sytem, replacing &lt;code&gt;make&lt;/code&gt; with original design by &lt;a href="https://en.wikipedia.org/wiki/Daniel_J._Bernstein" rel="nofollow"&gt;DJB&lt;/a&gt;. &lt;code&gt;(linux, windows, mac, console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SCons&lt;/strong&gt; - (&lt;a href="https://github.com/SCons/scons"&gt;Repo&lt;/a&gt;, &lt;a href="http://scons.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/SCons" rel="nofollow"&gt;WP&lt;/a&gt;) Domain-specific language and build tool, designed to replace Make, autoconf, and ccache. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Snapcraft&lt;/strong&gt; - (&lt;a href="https://github.com/snapcore/snapcraft"&gt;Repo&lt;/a&gt;, &lt;a href="https://snapcraft.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://snapcraft.io/docs" rel="nofollow"&gt;Docs&lt;/a&gt;) A command-line tool to package, distribute, and update apps for Linux and IoT using containerization, developed by Canonical. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Waf&lt;/strong&gt; - (&lt;a href="https://gitlab.com/ita1024/waf" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://waf.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Waf" rel="nofollow"&gt;WP&lt;/a&gt;, &lt;a href="https://waf.io/book" rel="nofollow"&gt;Docs&lt;/a&gt;) Cross-platform build system designed to improve on SCons. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-shell" class="anchor" aria-hidden="true" href="#shell"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev.shell" href="#tag-dev.shell"&gt;Shell&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Ergonomica&lt;/strong&gt; - (&lt;a href="https://github.com/ergonomica/ergonomica"&gt;Repo&lt;/a&gt;, &lt;a href="http://ergonomica.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Cross-platform shell language based on &lt;a href="https://en.wikipedia.org/wiki/S-expression" rel="nofollow"&gt;S-expressions&lt;/a&gt; combined with traditional shell features. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Oil&lt;/strong&gt; - (&lt;a href="https://github.com/oilshell/oil"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.oilshell.org/" rel="nofollow"&gt;Home&lt;/a&gt;) A new &lt;a href="https://en.wikipedia.org/wiki/Bash_(Unix_shell)" rel="nofollow"&gt;bash&lt;/a&gt;- and &lt;a href="https://en.wikipedia.org/wiki/Almquist_shell#dash:_Ubuntu,_Debian_and_POSIX_compliance_of_Linux_distributions" rel="nofollow"&gt;dash&lt;/a&gt; backwards-compatible shell, with an improved language of its own. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xonsh&lt;/strong&gt; - (&lt;a href="https://github.com/xonsh/xonsh"&gt;Repo&lt;/a&gt;, &lt;a href="https://xon.sh/" rel="nofollow"&gt;Home&lt;/a&gt;) Cross-platform shell language and command prompt. The language is a superset of Python 3.4+ with additional shell primitives. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-other-dev-projects" class="anchor" aria-hidden="true" href="#other-dev-projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-dev-other" href="#tag-dev-other"&gt;Other Dev projects&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;asciinema&lt;/strong&gt; - (&lt;a href="https://github.com/asciinema/asciinema"&gt;Repo&lt;/a&gt;, &lt;a href="https://asciinema.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Terminal session recorder and replayer. &lt;code&gt;(linux, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;autojump&lt;/strong&gt; - (&lt;a href="https://github.com/wting/autojump"&gt;Repo&lt;/a&gt;) A &lt;code&gt;cd&lt;/code&gt; with many heuristics to speed up console filesystem navigation. &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;coala&lt;/strong&gt; - (&lt;a href="https://github.com/coala/coala"&gt;Repo&lt;/a&gt;, &lt;a href="https://coala.io/" rel="nofollow"&gt;Home&lt;/a&gt;) Unified command-line interface for linting and fixing code, regardless of programming language. &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cookiecutter&lt;/strong&gt; - (&lt;a href="https://github.com/audreyr/cookiecutter"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/cookiecutter" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://cookiecutter.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Utility for creating new projects from shareable templates. &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cython&lt;/strong&gt; - (&lt;a href="https://github.com/cython/cython"&gt;Repo&lt;/a&gt;, &lt;a href="https://cython.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/cython" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="http://docs.cython.org/" rel="nofollow"&gt;Docs&lt;/a&gt;) Language and compiler designed for high-performance Python and C interoperability. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;doitlive&lt;/strong&gt; - (&lt;a href="https://github.com/sloria/doitlive"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/doitlive" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://doitlive.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Tool for live presentations in the terminal. &lt;code&gt;(linux, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DrawBot&lt;/strong&gt; - (&lt;a href="https://github.com/typemytype/drawbot"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.drawbot.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/DrawBot" rel="nofollow"&gt;WP&lt;/a&gt;) A powerful programmatic 2D drawing application for MacOS X which generates graphics from Python scripts. &lt;code&gt;(graphics, education, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;gdbgui&lt;/strong&gt; - (&lt;a href="https://github.com/cs01/gdbgui"&gt;Repo&lt;/a&gt;, &lt;a href="https://gdbgui.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/gdbgui" rel="nofollow"&gt;PyPI&lt;/a&gt;) Browser-based frontend for &lt;a href="https://en.wikipedia.org/wiki/GNU_Debugger" rel="nofollow"&gt;gdb&lt;/a&gt;. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GNS3 GUI&lt;/strong&gt; - (&lt;a href="https://github.com/GNS3/gns3-gui"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.gns3.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/gns3-gui" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://docs.gns3.com/" rel="nofollow"&gt;Docs&lt;/a&gt;) Graphical Network Simulator used to emulate, configure, test and troubleshoot virtual and real networks. (Backed by server component &lt;a href="https://github.com/GNS3/gns3-server"&gt;here&lt;/a&gt;.) &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;howdoi&lt;/strong&gt; - (&lt;a href="https://github.com/gleitz/howdoi"&gt;Repo&lt;/a&gt;, &lt;a href="https://pypi.org/project/howdoi" rel="nofollow"&gt;PyPI&lt;/a&gt;) Instant coding answers from StackOverflow on your command line. &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;httpie&lt;/strong&gt; - (&lt;a href="https://github.com/jakubroztocil/httpie"&gt;Repo&lt;/a&gt;, &lt;a href="https://httpie.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/httpie" rel="nofollow"&gt;PyPI&lt;/a&gt;) Command-line HTTP client with JSON support, syntax highlighting, wget-like downloads, extensions, and more. &lt;code&gt;(internet, linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IPython&lt;/strong&gt; - (&lt;a href="https://github.com/ipython/ipython"&gt;Repo&lt;/a&gt;, &lt;a href="https://ipython.readthedocs.org/" rel="nofollow"&gt;Docs&lt;/a&gt;) Set of enhancements to Python, wrapping it for richer interactivity. &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LocalStack&lt;/strong&gt; - (&lt;a href="https://github.com/localstack/localstack"&gt;Repo&lt;/a&gt;, &lt;a href="https://localstack.cloud/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://pypi.org/project/localstack" rel="nofollow"&gt;PyPI&lt;/a&gt;) Self-hostable version of many AWS services, including S3, Route53, Lambda, Redshift, and much more, designed for testing cloud-centric code. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Locust&lt;/strong&gt; - (&lt;a href="https://github.com/locustio/locust"&gt;Repo&lt;/a&gt;, &lt;a href="https://locust.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://docs.locust.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Scalable user load testing tool for web sites, featuring an interactive web interface. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MLflow&lt;/strong&gt; - (&lt;a href="https://github.com/mlflow/mlflow"&gt;Repo&lt;/a&gt;, &lt;a href="https://mlflow.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://mlflow.org/docs/latest/index.html" rel="nofollow"&gt;Docs&lt;/a&gt;) Integrated command-line application and web service, supporting an end-to-end machine-learning workflow around tracking, packaging, and deploying. Developed by &lt;a href="https://docs.databricks.com/applications/mlflow/index.html" rel="nofollow"&gt;Databricks&lt;/a&gt;. &lt;code&gt;(organization, linux, mac, corp)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PathPicker&lt;/strong&gt; - (&lt;a href="https://github.com/facebook/PathPicker"&gt;Repo&lt;/a&gt;, &lt;a href="http://facebook.github.io/PathPicker" rel="nofollow"&gt;Home&lt;/a&gt;) Shell utility to interactively select paths from the output of other commands. &lt;code&gt;(linux, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PeachPy&lt;/strong&gt; - (&lt;a href="https://github.com/Maratyszcza/PeachPy"&gt;Repo&lt;/a&gt;) Highly portable assembler with unified syntax, sporting an extensive user list, including many cryptography libraries for Go. &lt;code&gt;(linux, windows)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PINCE&lt;/strong&gt; - (&lt;a href="https://github.com/korcankaraokcu/PINCE"&gt;Repo&lt;/a&gt;) Debugging frontend for GDB focused on reverse engineering video games. &lt;code&gt;(linux, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plinth&lt;/strong&gt; - (&lt;a href="https://salsa.debian.org/freedombox-team/plinth" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://freedombox.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://wiki.debian.org/FreedomBox/Plinth" rel="nofollow"&gt;Docs&lt;/a&gt;) The core functionality and web front-end of &lt;a href="https://freedombox.org/" rel="nofollow"&gt;FreedomBox&lt;/a&gt;, an easy-to-manage, privacy-oriented home server. &lt;code&gt;(linux, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Polyaxon&lt;/strong&gt; - (&lt;a href="https://github.com/polyaxon/polyaxon"&gt;Repo&lt;/a&gt;, &lt;a href="https://polyaxon.com/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://docs.polyaxon.com/" rel="nofollow"&gt;Docs&lt;/a&gt;) A web-based platform for reproducible and scalable machine learning experiment management and metrics-tracking, based on kubernetes, with support for TensorFlow, PyTorch, Keras, and many more. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PPCI&lt;/strong&gt; - (&lt;a href="https://bitbucket.org/windel/ppci" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://godbolt.org/g/eooaPP" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://github.com/windelbouwman/ppci-mirror"&gt;gh&lt;/a&gt;, &lt;a href="https://pypi.org/project/ppci" rel="nofollow"&gt;PyPI&lt;/a&gt;, &lt;a href="https://ppci.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) The Pure Python Compiler Infrastructure is a compiler written entirely in Python, containing front-ends for various programming languages (C, c3, WebAssembly, and others) as well as machine code generation backends for various CPUs (6500, arm, avr, x86_64, openrisc, among others). &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RedHat Anaconda&lt;/strong&gt; - (&lt;a href="https://github.com/rhinstaller/anaconda"&gt;Repo&lt;/a&gt;, &lt;a href="https://anaconda-installer.readthedocs.io/en/latest" rel="nofollow"&gt;Docs&lt;/a&gt;) Installation program used by Fedora, Red Hat Enterprise Linux, and other Linux distributions. &lt;code&gt;(linux, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Robot Framework&lt;/strong&gt; - (&lt;a href="https://github.com/robotframework/robotframework"&gt;Repo&lt;/a&gt;, &lt;a href="http://robotframework.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Generic, cross-platform, and language-independent automation framework for acceptance testing, acceptance test driven development (ATDD), and robotic process automation (RPA). Extensible in Python and Java. &lt;code&gt;(console)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ScratchABit&lt;/strong&gt; - (&lt;a href="https://github.com/pfalcon/ScratchABit"&gt;Repo&lt;/a&gt;) Easily retargetable and hackable interactive disassembler with IDAPython-compatible plugin API. &lt;code&gt;(linux, tui)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sentry&lt;/strong&gt; - (&lt;a href="https://github.com/getsentry/sentry"&gt;Repo&lt;/a&gt;, &lt;a href="https://sentry.io/" rel="nofollow"&gt;Home&lt;/a&gt;) Web service and frontend for cross-platform application monitoring, with a focus on error reporting. &lt;code&gt;(server, corp, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Socorro&lt;/strong&gt; - (&lt;a href="https://github.com/mozilla-services/socorro"&gt;Repo&lt;/a&gt;, &lt;a href="https://wiki.mozilla.org/Socorro" rel="nofollow"&gt;Docs&lt;/a&gt;) Web service for collecting crash statistics from Mozilla products, including Firefox, Thunderbird, and &lt;a href="https://crash-stats.mozilla.org/" rel="nofollow"&gt;others&lt;/a&gt;. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Taiga&lt;/strong&gt; - (&lt;a href="https://github.com/taigaio/taiga-back"&gt;Repo&lt;/a&gt;, &lt;a href="https://taiga.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="http://taigaio.github.io/taiga-doc/dist" rel="nofollow"&gt;Docs&lt;/a&gt;) Web application built for managing projects with agile development processes. &lt;code&gt;(organization, server, django)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Thumbor&lt;/strong&gt; - (&lt;a href="https://github.com/thumbor/thumbor"&gt;Repo&lt;/a&gt;, &lt;a href="http://thumbor.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://thumbor.readthedocs.io/" rel="nofollow"&gt;Docs&lt;/a&gt;) Photo thumbnail service with resizing, flipping, and smart cropping of images. &lt;code&gt;(graphics, server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ubiquity&lt;/strong&gt; - (&lt;a href="https://code.launchpad.net/ubiquity" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Ubiquity_%28software%29" rel="nofollow"&gt;WP&lt;/a&gt;) The default installer for Ubuntu and its derivatives, designed to be run from Live CD or USB. &lt;code&gt;(linux, gtk, qt)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Voltron&lt;/strong&gt; - (&lt;a href="https://github.com/snare/voltron"&gt;Repo&lt;/a&gt;) Extensible debugger wrapper aiming to improve the user experience of various debuggers, such as &lt;a href="https://lldb.llvm.org/" rel="nofollow"&gt;LLDB&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/GNU_Debugger" rel="nofollow"&gt;GDB&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/WinDbg" rel="nofollow"&gt;WinDbg&lt;/a&gt;. &lt;code&gt;(linux, windows, mac)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;YunoHost&lt;/strong&gt; - (&lt;a href="https://github.com/YunoHost/yunohost"&gt;Repo&lt;/a&gt;, &lt;a href="https://yunohost.org/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://yunohost.org/#/docs" rel="nofollow"&gt;Docs&lt;/a&gt;) Server operating system based on Debian Linux aiming to make self-hosting accessible to as many people as possible, with support for several types of hardware. &lt;code&gt;(linux, server)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-misc" class="anchor" aria-hidden="true" href="#misc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a id="user-content-tag-misc" href="#tag-misc"&gt;Misc&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Guake&lt;/strong&gt; - (&lt;a href="https://github.com/Guake/guake"&gt;Repo&lt;/a&gt;, &lt;a href="http://guake-project.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Drop-down terminal for GNOME, reminiscent of first-person game command consoles. &lt;code&gt;(linux, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Home Assistant&lt;/strong&gt; - (&lt;a href="https://github.com/home-assistant/home-assistant"&gt;Repo&lt;/a&gt;, &lt;a href="https://www.home-assistant.io/" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://demo.home-assistant.io/" rel="nofollow"&gt;Demo&lt;/a&gt;, &lt;a href="https://www.home-assistant.io/docs" rel="nofollow"&gt;Docs&lt;/a&gt;) Home automation platform that puts local control and privacy first. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JARVIS on Messenger&lt;/strong&gt; - (&lt;a href="https://github.com/swapagarwal/JARVIS-on-Messenger"&gt;Repo&lt;/a&gt;, &lt;a href="https://m.me/J.A.R.V.I.S.on.Messenger" rel="nofollow"&gt;Home&lt;/a&gt;) Facebook Messenger bot with a wide assortment of features. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NFO Viewer&lt;/strong&gt; - (&lt;a href="https://github.com/otsaloma/nfoview"&gt;Repo&lt;/a&gt;, &lt;a href="https://otsaloma.io/nfoview" rel="nofollow"&gt;Home&lt;/a&gt;) A simple viewer for NFO files and the ASCII art therein, with preset fonts, encodings, automatic window sizing, and clickable hyperlinks. &lt;code&gt;(graphics, linux, windows)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nicotine+&lt;/strong&gt; - (&lt;a href="https://github.com/Nicotine-Plus/nicotine-plus"&gt;Repo&lt;/a&gt;) Graphical desktop client for the &lt;a href="https://en.wikipedia.org/wiki/Soulseek" rel="nofollow"&gt;Soulseek&lt;/a&gt; peer-to-peer system. &lt;code&gt;(linux, windows, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nimbus&lt;/strong&gt; - (&lt;a href="https://github.com/nimbusproject/nimbus"&gt;Repo&lt;/a&gt;, &lt;a href="http://www.nimbusproject.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Infrastructure-as-a-Service platform geared toward scientific cloud computing. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenLP&lt;/strong&gt; - (&lt;a href="https://code.launchpad.net/openlp" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="https://openlp.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Presentation software geared toward church usage. &lt;code&gt;(linux, windows, mac, qt5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;qtile&lt;/strong&gt; - (&lt;a href="https://github.com/qtile/qtile"&gt;Repo&lt;/a&gt;, &lt;a href="http://qtile.org/" rel="nofollow"&gt;Home&lt;/a&gt;) A small, flexible, scriptable tiling window manager. &lt;code&gt;(linux)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;uMap&lt;/strong&gt; - (&lt;a href="https://github.com/umap-project/umap"&gt;Repo&lt;/a&gt;, &lt;a href="https://wiki.openstreetmap.org/wiki/UMap" rel="nofollow"&gt;Docs&lt;/a&gt;) Web application allowing users to create maps with OpenStreetMap layers and embed it on other sites. &lt;code&gt;(server)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wammu&lt;/strong&gt; - (&lt;a href="https://github.com/gammu/wammu"&gt;Repo&lt;/a&gt;, &lt;a href="https://wammu.eu/wammu" rel="nofollow"&gt;Home&lt;/a&gt;) GUI phone manager with read/write support for contacts, todo, calendar, SMS, and more, primarily designed for Nokia and AT-compatible phones. &lt;code&gt;(linux, windows)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wicd&lt;/strong&gt; - (&lt;a href="https://code.launchpad.net/wicd" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="http://wicd.sourceforge.net/download.php" rel="nofollow"&gt;Home&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Wicd" rel="nofollow"&gt;WP&lt;/a&gt;) Graphical utility for managing wired and wireless connections on Linux. &lt;code&gt;(linux, gtk)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xpra&lt;/strong&gt; - (&lt;a href="https://xpra.org/svn/Xpra/trunk" rel="nofollow"&gt;Repo&lt;/a&gt;, &lt;a href="http://xpra.org/" rel="nofollow"&gt;Home&lt;/a&gt;) Cross-platform remote display server and client for forwarding applications and desktop screens. &lt;code&gt;(linux, windows)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;&lt;a id="user-content-conclusion" class="anchor" aria-hidden="true" href="#conclusion"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;If you have a project to add, &lt;a href="https://github.com/mahmoud/awesome-python-applications/issues"&gt;please let us know&lt;/a&gt;!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>mahmoud</author><guid isPermaLink="false">https://github.com/mahmoud/awesome-python-applications</guid><pubDate>Tue, 19 Nov 2019 00:25:00 GMT</pubDate></item></channel></rss>