<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Jupyter Notebook, This month</title><link>https://github.com/trending/jupyter-notebook?since=monthly</link><description>The top repositories on GitHub for jupyter-notebook, measured monthly</description><pubDate>Fri, 03 Jan 2020 01:04:05 GMT</pubDate><lastBuildDate>Fri, 03 Jan 2020 01:04:05 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>lmoroney/dlaicourse #1 in Jupyter Notebook, This month</title><link>https://github.com/lmoroney/dlaicourse</link><description>&lt;p&gt;&lt;i&gt;Notebooks for learning deep learning&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;This repo does not have a README.&lt;/i&gt;&lt;/p&gt;</description><author>lmoroney</author><guid isPermaLink="false">https://github.com/lmoroney/dlaicourse</guid><pubDate>Fri, 03 Jan 2020 00:01:00 GMT</pubDate></item><item><title>microsoft/ai-edu #2 in Jupyter Notebook, This month</title><link>https://github.com/microsoft/ai-edu</link><description>&lt;p&gt;&lt;i&gt;AI education materials for Chinese students, teachers and IT professionals.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-微软人工智能教育与学习共建社区" class="anchor" aria-hidden="true" href="#微软人工智能教育与学习共建社区"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;微软人工智能教育与学习共建社区&lt;/h1&gt;
&lt;p&gt;本社区是微软亚洲研究院（Microsoft Research Asia，简称MSRA）人工智能教育团队创立的人工智能教育与学习共建社区.&lt;/p&gt;
&lt;p&gt;在教育部指导下，依托于新一代人工智能开放科研教育平台，微软亚洲研究院研发团队和学术合作部将为本社区提供全面支持。我们将在此提供人工智能应用开发的真实案例，以及配套的教程、工具等学习资源，人工智能领域的一线教师及学习者也将分享他们的资源与经验。&lt;/p&gt;
&lt;p&gt;正如微软的使命“予力全球每一人、每一组织，成就不凡”所指出的，期待借由本社区的建立，能以开源的方式，与广大师生、开发者一起学习、一起贡献，共同丰富、完善本社区，既而为中国人工智能的发展添砖加瓦。&lt;/p&gt;
&lt;p&gt;本社区注明版权出处的内容适用于&lt;a href="./LICENSE.md"&gt;License&lt;/a&gt;版权许可。&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-新闻" class="anchor" aria-hidden="true" href="#新闻"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;新闻&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;2019-11-20:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;首页改版啦！新版本的首页，将社区资源进一步系统化，按认识AI（初级），理解AI（中级）,研究AI（高级）的结构分级编写了学习路径，并给出学习时长参考，先修知识资源参考，循序渐进，旨在帮助广大学习者更最高效地学习AI，赶快学起来吧！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2019-11-19:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;更新&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B13-AI%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A1%88%E4%BE%8B"&gt;智能对联&lt;/a&gt;案例，案例更加简洁、清晰，方便上手！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2019-11-15:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://aka.ms/beginnerAI" rel="nofollow"&gt;神经网络基本原理简明教程&lt;/a&gt;暨&lt;strong&gt;9步学习神经网络&lt;/strong&gt;全部内容完成！&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-学习资源介绍" class="anchor" aria-hidden="true" href="#学习资源介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;学习资源介绍&lt;/h1&gt;
&lt;p&gt;介绍：&lt;/p&gt;
&lt;p&gt;本社区的学习资源优质且免费，绝大部分为原创内容，核心学习资源包括&lt;strong&gt;实战篇&lt;/strong&gt;和&lt;strong&gt;理论篇&lt;/strong&gt;两大部分，辅以参考学习路径和先修知识参考资源，让广大学习者可以清晰地选择适合自己的学习路径，高效地学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 实战篇&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;以“做中学“的理念为核心，从人工智能真实的应用场景与案例出发，先讲生动的案例，配合详实的实际操作说明，然后在动手实现场景的基础上，逐步引入人工智能学习中的相关理论知识，以递进学习的新颖方式层层剖析人工智能开发的主流场景，让大家在不需要大量时间学习庞大的理论基础的情况下，也可以真正动手开始进行人工智能应用的开发，提高实际动手的能力.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.1 初级实战案例&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;适用人群：AI小白，想简单认识AI，简单了解人工智能应用开发过程，或者想快速利用成熟的人工智能API给自己开发的应用添加人工智能元素。非计算机专业的学生也可以适用。&lt;/li&gt;
&lt;li&gt;内容：基础人工智能案例资源，覆盖计算机视觉，自然语言处理，语音识别等领域。&lt;/li&gt;
&lt;li&gt;先修知识：了解C#基本语法。&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/%E5%85%88%E4%BF%AE%E7%9F%A5%E8%AF%86%E5%8F%82%E8%80%83%E9%98%85%E8%AF%BB.md#C#"&gt;点击这里查看参考资料&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;学习时长：10~16 小时&lt;/li&gt;
&lt;li&gt;学习目标：认识AI,初步了解AI应用开发过程，通过人工智能API使用已有模型，开发人工智能应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;初级实战案例学习路径参考&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;序号&lt;/th&gt;
&lt;th&gt;内容&lt;/th&gt;
&lt;th&gt;知识点&lt;/th&gt;
&lt;th align="center"&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;1&lt;/td&gt;
&lt;td&gt;&lt;a href="./A-%E6%95%99%E5%AD%A6%E8%AF%BE%E7%A8%8B/%E5%8C%97%E4%BA%AC%E5%A4%A7%E5%AD%A6%E5%BE%AE%E8%BD%AF%E4%BA%9A%E6%B4%B2%E7%A0%94%E7%A9%B6%E9%99%A2%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E4%BF%A1%E6%81%AF%E7%A4%BE%E4%BC%9A%E5%A4%A7%E7%BA%B2%EF%BC%88MOOC%EF%BC%89-%E5%8C%97%E5%A4%A7%E9%99%88%E6%96%8C.md"&gt;概论：人工智能与信息社会&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;人工智能技术的基本概念、发展历史、经典算法、应用领域和对人类社会的深远影响，展示信息社会各领域中人工智能的应用发展前景，人工智能的入门基础。&lt;/td&gt;
&lt;td align="center"&gt;可选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B12-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E9%81%93%E5%BE%B7%E4%B8%8E%E4%BC%A6%E7%90%86/7_AI_Ethics.md"&gt;人工智能道德与伦理&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;人工智能道德与伦理面临的挑战，AI从业者如何正确处理AI伦理与道德的问题&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/microsoft/ai-edu/tree/master/A-%E6%95%99%E5%AD%A6%E8%AF%BE%E7%A8%8B/math_intro"&gt;数学基础&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;代数基础，微积分基础，线性代数基础，概率统计基础&lt;/td&gt;
&lt;td align="center"&gt;可选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/microsoft/ai-edu/tree/master/A-%E6%95%99%E5%AD%A6%E8%AF%BE%E7%A8%8B/py_intro"&gt;Python语言导论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;编程环境介绍，变量和数据类型，函数和库，Numpy，绘图&lt;/td&gt;
&lt;td align="center"&gt;可选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;5&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B1-%E9%A2%84%E6%9E%84%E5%BB%BA%EF%BC%8DOCR%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%EF%BC%8D%E6%BC%AB%E7%94%BB%E7%BF%BB%E8%AF%91/README.md"&gt;计算机视觉初级案例-漫画翻译&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;数据预处理，小型客户端软件的界面设计与实现，商业应用软件的架构设计，REST API ，OCR应用开发&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;6&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/%E4%B8%9C%E5%8C%97%E5%A4%A7%E5%AD%A6%E8%A7%86%E9%A2%91%E8%A7%81%E8%A7%A3%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%E4%B9%8B%E8%A7%86%E9%A2%91%E6%A0%87%E7%AD%BE%E6%8F%90%E5%8F%96/README.md"&gt;计算机视觉初级案例-视频标签提取&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;利用 Video Indexer API，针对视频影像内容进行分析，提取见解和信息&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;7&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B3-%E5%AE%9A%E5%88%B6%E5%8C%96%EF%BC%8D%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%EF%BC%8D%E7%9C%8B%E5%9B%BE%E8%AF%86%E7%86%8A/README.md"&gt;计算机视觉初级案例-看图识熊&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;数据标记,数据预处理,离线模型推理应用,基于定制化视觉服务的应用开发&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;8&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B2-%E5%AE%9A%E5%88%B6%E5%8C%96%EF%BC%8D%E6%96%87%E5%AD%97%E7%90%86%E8%A7%A3%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B-%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%92%8C%E5%AF%B9%E8%AF%9D%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%9C%8D%E5%8A%A1/README.md"&gt;自然语言理解初级案例-问答机器人&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;微软认知服务中知识库服务（QnA maker）的申请与搭建;使用REST API访问知识库数据；机器人服务（Bot Service）的申请与搭建；集体成知识库服务与机器人服务&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;9&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B4-%E5%AE%9A%E5%88%B6%E5%8C%96%EF%BC%8D%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%EF%BC%8D%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/README.md"&gt;自然语言理解初级案例-智能家居&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;数据标记；数据预处理；在线定制语言理解服务；智能家居应用开发&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;10&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B16-%E5%AE%9A%E5%88%B6%E5%8C%96%20-%20%E8%AF%AD%E9%9F%B3%E6%9C%8D%E5%8A%A1%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%20-%20%E6%96%87%E6%9C%AC%E6%9C%97%E8%AF%BB%E5%BA%94%E7%94%A8/README.md"&gt;语音识别初级案例-智能听书&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TTS API的申请和使用；TTS应用的构建；智能听书功能实现&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;11&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B5-%E6%89%A9%E5%B1%95%E9%98%85%E8%AF%BB-%E6%90%AD%E5%BB%BA%E4%B8%AD%E9%97%B4%E6%9C%8D%E5%8A%A1%E5%B1%82/README.md"&gt;扩展阅读-搭建中间服务层&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;商业应用软件的架构设计&lt;/td&gt;
&lt;td align="center"&gt;可选&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;本部分内容也可以结合 &lt;strong&gt;2.1 神经网络初探&lt;/strong&gt; 的理论知识完成理论加实践的AI入门学习&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.2 中级实战案例 &lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;适用人群：具有人工智能一定理论基础/编程经验的的学生、程序员。有一定的AI 入门知识，想进一步理解AI，并真正可以动手开发人工智能应用。&lt;/li&gt;
&lt;li&gt;内容：基础人工智能案例资源，覆盖计算机视觉，自然语言处理，语音识别等领域。&lt;/li&gt;
&lt;li&gt;先修知识：Python;CNN;RNN;主流深度学习框架&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/%E5%85%88%E4%BF%AE%E7%9F%A5%E8%AF%86%E5%8F%82%E8%80%83%E9%98%85%E8%AF%BB.md#Python"&gt;点击这里查看参考资料&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;学习时长：20~30小时&lt;/li&gt;
&lt;li&gt;学习目标：理解AI,学会基于常用工具、主流Framework 搭建AI模型，开发人工智能应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;中级实战案例学习路径参考&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;序号&lt;/th&gt;
&lt;th&gt;内容&lt;/th&gt;
&lt;th&gt;知识点&lt;/th&gt;
&lt;th align="center"&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;1&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B9-%E8%87%AA%E6%9E%84%E5%BB%BA%EF%BC%8D%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B-%E6%89%8B%E5%86%99%E7%AE%97%E5%BC%8F%E8%AE%A1%E7%AE%97%E5%99%A8/README.md"&gt;计算机视觉中级案例-手写算式计算器&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TensorFlow模型到 .Net Framework应用环境的快速集成；基于本地MNIST模型的手写数字识别应用开发&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B13-AI%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A1%88%E4%BE%8B/README.md"&gt;自然语言理解中级案例-智能对联&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;数据预处理；机器翻译编码-解码过程剖析；模型训练与调参；模型推理；在线服务的搭建；基于微信的AI小程序开发）&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B8-%E8%87%AA%E6%9E%84%E5%BB%BA%EF%BC%8DAI%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91%E6%A1%88%E4%BE%8B%EF%BC%8D%E9%BB%84%E9%87%91%E7%82%B9%E6%B8%B8%E6%88%8F/README.md"&gt;游戏AI/强化学习中级案例-黄金点&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;游戏AI策略；多个游戏AI比赛的实现&lt;/td&gt;
&lt;td align="center"&gt;可选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B11-%E9%87%8F%E5%8C%96%E4%BA%A4%E6%98%93%E6%A1%88%E4%BE%8B/README.md"&gt;中级案例-预测股票走势&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;量化交易的经典过程；股价预测的建模理念；特征工程的一般方法；梯度提升决策树模块（lightbgm）的使用方法；深度学习神经网络框架（Keras）的使用 &lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;5&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B15-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/README.md"&gt;中级案例-生成对抗网络GAN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GAN简介；动手实现并训练生成对抗网络&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;6&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/microsoft/ai-edu/tree/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/%E5%A4%8D%E6%97%A6%E5%A4%A7%E5%AD%A6%E5%9F%8E%E5%B8%82%E5%A3%B0%E9%9F%B3%E5%88%86%E7%B1%BB-%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87"&gt;计算机视觉中级案例-图像超分辨率&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;数据预处理；使用GAN、CNN和ResNet的组合构建超分辨率模型&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;7&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/microsoft/ai-edu/tree/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/%E5%A4%8D%E6%97%A6%E5%A4%A7%E5%AD%A6%E5%9F%8E%E5%B8%82%E5%A3%B0%E9%9F%B3%E5%88%86%E7%B1%BB-%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87"&gt;中级案例-智慧城市之声音分类&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;数据分析；特征工程；TensorFlow 框架下构建多种深度学习模型（多层感知机、LSTM、GRU 和 CNN 等）&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;8&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B10-%E6%89%A9%E5%B1%95%E9%98%85%E8%AF%BB%EF%BC%8D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%8F%B0%E5%BB%BA%E8%AE%BE/readme.md"&gt;扩展阅读-机器学习平台建设&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;机器学习平台的架构；机器学习平台的功能；微软开源机器学习平台OpenPAI&lt;/td&gt;
&lt;td align="center"&gt;可选&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;本部分内容也可以结合 &lt;strong&gt;2.2 神经网络进阶&lt;/strong&gt; 以及&lt;strong&gt;2.3 深度网络基础&lt;/strong&gt; 的理论知识完成理论加实践的AI进阶学习&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.3 高级实战案例&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;适用人群：已经深刻理解AI原理，想通过学习或者共建微软亚洲研究院的开源案例/项目，进行AI领域前沿性研究。&lt;/li&gt;
&lt;li&gt;内容：微软亚洲研究院前沿性研究的开源项目/案例。&lt;/li&gt;
&lt;li&gt;学习目标：进行AI领域前沿性研究。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;高级实战案例学习路径参考&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;序号&lt;/th&gt;
&lt;th&gt;内容&lt;/th&gt;
&lt;th&gt;知识点&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;1&lt;/td&gt;
&lt;td&gt;&lt;a href="./B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B14-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BB%A3%E7%A0%81%E6%90%9C%E7%B4%A2%E6%A1%88%E4%BE%8B/README.md"&gt;高级实战案例-基于深度学习的代码搜索案例&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;理解自然语言并按要求产生可用的代码&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Microsoft/nni"&gt;高级实战项目-NNI (Neural Network Intelligence)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;微软开源的自动机器学习工具;调参器算法的实现，评估器算法的实现&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Microsoft/pai"&gt;高级实战项目-Open Platform for AI (OpenPAI)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;微软开源GPU管理利器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Microsoft/LightGBM"&gt;高级实战项目-LightGBM&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;boosting框架&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;2. 理论篇&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;理论篇的内容又称作“&lt;a href="https://aka.ms/beginnerAI" rel="nofollow"&gt;9步学习神经网络&lt;/a&gt;”,为微软亚洲研究院研发团队原创内容，着重讲述偏理论的知识，同样以“做中学”为核心概念，但是独特地以化繁为简，深入浅出为特点，提供通俗易懂的理论讲解，清晰工整的代码，准确无误的内容，完整的作业体系，不但有理论，还有大量实践动手环节，帮助读者不但迅速掌握“深度学习”的基础知识，更好地理解并使用现有框架，而且可以助力读者快速学习最新出现的各种神经网络的扩展或者变型，跟上快速发展的AI浪潮,使学习者从新的角度快速上手神经网络的学习，做到真正的从入门到精通。该部分内容在针对合作伙伴线下的培训中，受到广大学习者的广泛好评。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.1 神经网络初探&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;适用人群：希望了解最基本的神经网络知识，有一定代码能力。&lt;/li&gt;
&lt;li&gt;内容：神经网络基础知识，线性回归，线性分类。&lt;/li&gt;
&lt;li&gt;先修知识：微分知识和线性代数;&lt;a href="https://docs.python.org/zh-cn/3/tutorial/index.html" rel="nofollow"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;学习时长：6~8小时&lt;/li&gt;
&lt;li&gt;学习目标：理解神经网络基础概念。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;神经网络初探学习路径参考&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;序号&lt;/th&gt;
&lt;th&gt;内容&lt;/th&gt;
&lt;th&gt;知识点&lt;/th&gt;
&lt;th align="center"&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;1&lt;/td&gt;
&lt;td&gt;&lt;a href="https://aka.ms/step1" rel="nofollow"&gt;第一步：神经网络概论与基本概念&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;神经网络基本工作原理；反向传播；梯度下降；损失函数&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://aka.ms/step2" rel="nofollow"&gt;第二步：线性回归&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;单入单出的单层神经网络；多入单出的单层神经网络&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td&gt;&lt;a href="https://aka.ms/step3" rel="nofollow"&gt;第三步：线性分类&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;多入单出的单层神经网路；多入单出的单层神经网路&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;2.2 神经网络进阶&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;适用人群：具有一定的神经网络学习基础和代码能力的学习者。&lt;/li&gt;
&lt;li&gt;内容：非线性回归，非线性分类，模型推理。&lt;/li&gt;
&lt;li&gt;先修知识：微分知识和线性代数;&lt;a href="https://docs.python.org/zh-cn/3/tutorial/index.html" rel="nofollow"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;学习时长：8~12 小时&lt;/li&gt;
&lt;li&gt;学习目标：更好地理解并使用现有神经网络框架。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;神经网络进阶学习路径参考&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;序号&lt;/th&gt;
&lt;th&gt;内容&lt;/th&gt;
&lt;th&gt;知识点&lt;/th&gt;
&lt;th align="center"&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;1&lt;/td&gt;
&lt;td&gt;&lt;a href="https://aka.ms/step4" rel="nofollow"&gt;第四步：非线性回归&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;激活函数；单入单出的双层神经网络&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://aka.ms/step5" rel="nofollow"&gt;第五步：非线性分类&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;多入单出的双层神经网络；多入多出的双层神经网络；多入多出的三层神经网络&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td&gt;&lt;a href="https://aka.ms/step6" rel="nofollow"&gt;第六步：模型的推理与部署&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;模型文件概述；ONNX模型文件；Windows中模型的部署&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;2.3 深度网络基础&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;适用人群：有较好的神经网络理论基础，想通过学习深度网络进行更多地扩展性研究或者开发。&lt;/li&gt;
&lt;li&gt;内容：DNN，CNN，RNN。&lt;/li&gt;
&lt;li&gt;先修知识：微分知识和线性代数;&lt;a href="https://docs.python.org/zh-cn/3/tutorial/index.html" rel="nofollow"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;学习时长：16~24 小时&lt;/li&gt;
&lt;li&gt;学习目标：有能力快速学习最新出现的各种神经网络的扩展或者变型，或进行前沿性研究。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;深度网络基础学习路径参考&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;序号&lt;/th&gt;
&lt;th&gt;内容&lt;/th&gt;
&lt;th&gt;知识点&lt;/th&gt;
&lt;th align="center"&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;1&lt;/td&gt;
&lt;td&gt;&lt;a href="https://aka.ms/step7" rel="nofollow"&gt;第七步：深度神经网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;搭建深度神经网络框架；网络优化；正则化&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://aka.ms/step8" rel="nofollow"&gt;第八步：卷积神经网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;卷积神经网络原理；卷积神经网络应用&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td&gt;&lt;a href="https://aka.ms/step9" rel="nofollow"&gt;第九步：循环神经网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;普通循环神经网络；高级循环神经网络&lt;/td&gt;
&lt;td align="center"&gt;必选&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-ai-前沿精选" class="anchor" aria-hidden="true" href="#ai-前沿精选"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AI 前沿精选&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://www.msra.cn/zh-cn/news/features/emnlp-2019-exploiting-monolingual-data-at-scale-for-nmt" rel="nofollow"&gt;大规模利用单语数据提升神经机器翻译&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.msra.cn/zh-cn/news/features/emnlp-2019-rmg" rel="nofollow"&gt;基于层次化注意力图网络和多视角学习的商品推荐&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.msra.cn/zh-cn/news/features/ai-detect-fake-face" rel="nofollow"&gt;AI换脸鉴别率超99.6%，微软用技术应对虚假信息&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.msra.cn/zh-cn/news/features/emnlp-2019" rel="nofollow"&gt;微软亚洲研究院精选论文解读&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.msra.cn/zh-cn/news?wd&amp;amp;content-type=posts" rel="nofollow"&gt;查看更多...&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-等你来战" class="anchor" aria-hidden="true" href="#等你来战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;等你来战&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="./E-Challenge/GoldenNumberGame"&gt;挑战黄金点&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="./E-Challenge/BeihangUniversity2019Spring"&gt;北京航空航天大学2019春季&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="./E-Challenge/ShandongUniversity2019Spring"&gt;山东大学2019春季&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="./E-Challenge/CodeSearch"&gt;Code Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="./E-Challenge/2019studentclub"&gt;2019实践空间站&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="./README_1.0.md"&gt;访问旧版主页 (Version 1.0)&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>microsoft</author><guid isPermaLink="false">https://github.com/microsoft/ai-edu</guid><pubDate>Fri, 03 Jan 2020 00:02:00 GMT</pubDate></item><item><title>Pierian-Data/Complete-Python-3-Bootcamp #3 in Jupyter Notebook, This month</title><link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link><description>&lt;p&gt;&lt;i&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-complete-python-3-bootcamp" class="anchor" aria-hidden="true" href="#complete-python-3-bootcamp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete-Python-3-Bootcamp&lt;/h1&gt;
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;
&lt;p&gt;Get it now for 95% off with the link:
&lt;a href="https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB" rel="nofollow"&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pierian-Data</author><guid isPermaLink="false">https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</guid><pubDate>Fri, 03 Jan 2020 00:03:00 GMT</pubDate></item><item><title>practicalAI/practicalAI #4 in Jupyter Notebook, This month</title><link>https://github.com/practicalAI/practicalAI</link><description>&lt;p&gt;&lt;i&gt;📚 A practical approach to machine learning to enable everyone to learn, explore and build.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
&lt;a href="https://practicalai.me" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/practicalAI/images/master/images/logo.png" width="200" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;p&gt;A &lt;i&gt;&lt;b&gt;practical&lt;/b&gt;&lt;/i&gt; approach to machine learning.&lt;/p&gt;
&lt;a href="https://github.com/practicalAI/practicalAI"&gt;
&lt;img src="https://camo.githubusercontent.com/c1b6c20adc52e06a1c58218665169097a63bd549/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f70726163746963616c41492f70726163746963616c41492e7376673f7374796c653d736f6369616c266c6162656c3d53746172" data-canonical-src="https://img.shields.io/github/stars/practicalAI/practicalAI.svg?style=social&amp;amp;label=Star" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://www.linkedin.com/company/practicalai-me" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/19c0cf9ba93aa446aa855a0203c46ee39841cba9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374796c652d2d3565626130302e7376673f6c6162656c3d4c696e6b6564496e266c6f676f3d6c696e6b6564696e267374796c653d736f6369616c" data-canonical-src="https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&amp;amp;logo=linkedin&amp;amp;style=social" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://twitter.com/practicalAIme" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/1a44bef694d0cd085f8365eac5ff9b5f85568043/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f70726163746963616c41496d652e7376673f6c6162656c3d466f6c6c6f77267374796c653d736f6369616c" data-canonical-src="https://img.shields.io/twitter/follow/practicalAIme.svg?label=Follow&amp;amp;style=social" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;p&gt;&lt;sub&gt;Created by
&lt;a href="https://goku.me" rel="nofollow"&gt;Goku Mohandas&lt;/a&gt; and
&lt;a href="https://github.com/practicalAI/practicalAI/graphs/contributors"&gt;
contributors
&lt;/a&gt;
&lt;/sub&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-notebooks" class="anchor" aria-hidden="true" href="#notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Notebooks&lt;/h2&gt;
&lt;ul&gt;
    &lt;li&gt;
        &lt;g-emoji class="g-emoji" alias="earth_americas" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30e.png"&gt;🌎&lt;/g-emoji&gt; → &lt;a href="https://practicalai.me" rel="nofollow"&gt;https://practicalai.me&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
        &lt;g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png"&gt;📚&lt;/g-emoji&gt; Illustrative ML notebooks in &lt;a href="https://tensorflow.org" rel="nofollow"&gt;TensorFlow 2.0 + Keras&lt;/a&gt;.
    &lt;/li&gt;
    &lt;li&gt;
        &lt;g-emoji class="g-emoji" alias="hammer_and_pick" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2692.png"&gt;⚒️&lt;/g-emoji&gt; Build robust models using the functional API w/ custom components
    &lt;/li&gt;
    &lt;li&gt;
        &lt;g-emoji class="g-emoji" alias="package" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e6.png"&gt;📦&lt;/g-emoji&gt; Train using simple yet highly customizable loops to build products fast
    &lt;/li&gt;
    &lt;li&gt;
        If you prefer Jupyter Notebooks or want to add/fix content, check out the &lt;a href="https://github.com/practicalAI/practicalAI/tree/master/notebooks"&gt;notebooks&lt;/a&gt; directory.
    &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;
        &lt;h4 align="center"&gt;&lt;a id="user-content-basic-ml" class="anchor" aria-hidden="true" href="#basic-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic ML&lt;/h4&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Basics&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Machine Learning&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Tools&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Deep Learning&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="4"&gt;
        &lt;ul&gt;
            &lt;li&gt;Learn Python basics with notebooks.&lt;/li&gt;
            &lt;li&gt;Use data science libraries like &lt;a href="https://www.numpy.org/" rel="nofollow"&gt;NumPy&lt;/a&gt; and &lt;a href="https://pandas.pydata.org/" rel="nofollow"&gt;Pandas&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Implement basic ML models in &lt;a href="https://www.tensorflow.org/overview/" rel="nofollow"&gt;TensorFlow 2.0 + Keras&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Create deep learning models for improved performance.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/00_Notebooks.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="notebook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d3.png"&gt;📓&lt;/g-emoji&gt; Notebooks&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/04_Linear_Regression.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="chart_with_upwards_trend" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c8.png"&gt;📈&lt;/g-emoji&gt; Linear Regression&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/07_Data_and_Models.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="mag_right" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f50e.png"&gt;🔎&lt;/g-emoji&gt; Data &amp;amp; Models&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/10_Convolutional_Neural_Networks.ipynb" rel="nofollow"&gt;️&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;🖼&lt;/g-emoji&gt; Convolutional Neural Networks&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/01_Python.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt; Python&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/05_Logistic_Regression.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="bar_chart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png"&gt;📊&lt;/g-emoji&gt; Logistic Regression&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/08_Utilities.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="hammer_and_wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6e0.png"&gt;🛠&lt;/g-emoji&gt; Utilities&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/11_Embeddings.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="crown" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f451.png"&gt;👑&lt;/g-emoji&gt; Embeddings&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/02_NumPy.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="1234" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f522.png"&gt;🔢&lt;/g-emoji&gt; NumPy&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/06_Multilayer_Perceptrons.ipynb" rel="nofollow"&gt;️&lt;g-emoji class="g-emoji" alias="control_knobs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f39b.png"&gt;🎛&lt;/g-emoji&gt; Multilayer Perceptrons&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/09_Preprocessing.ipynb" rel="nofollow"&gt;️&lt;g-emoji class="g-emoji" alias="scissors" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2702.png"&gt;✂️&lt;/g-emoji&gt; Preprocessing&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/12_Recurrent_Neural_Networks.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;📗&lt;/g-emoji&gt; Recurrent Neural Networks&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/03_Pandas.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="panda_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f43c.png"&gt;🐼&lt;/g-emoji&gt; Pandas&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-production-ml" class="anchor" aria-hidden="true" href="#production-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Production ML&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Local&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Applications&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Scale&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="3"&gt;
        &lt;ul&gt;
            &lt;li&gt;Setup your local environment for ML.&lt;/li&gt;
            &lt;li&gt;Wrap your ML in RESTful APIs using &lt;a href="http://flask.pocoo.org/" rel="nofollow"&gt;Flask&lt;/a&gt; to create applications.&lt;/li&gt;
            &lt;li&gt;Standardize and scale your ML applications with &lt;a href="https://www.docker.com/" rel="nofollow"&gt;Docker&lt;/a&gt; and &lt;a href="https://kubernetes.io/" rel="nofollow"&gt;Kubernetes&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Deploy simple and scalable ML workflows using &lt;a href="https://www.kubeflow.org/" rel="nofollow"&gt;Kubeflow&lt;/a&gt;.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;💻&lt;/g-emoji&gt; Local Setup&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="evergreen_tree" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f332.png"&gt;🌲&lt;/g-emoji&gt; Logging&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="whale" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f433.png"&gt;🐳&lt;/g-emoji&gt; Docker&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="handshake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f91d.png"&gt;🤝&lt;/g-emoji&gt; Distributed Training&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt; ML Scripts&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="funeral_urn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26b1.png"&gt;⚱️&lt;/g-emoji&gt; Flask Applications&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="ship" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a2.png"&gt;🚢&lt;/g-emoji&gt; Kubernetes&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="battery" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f50b.png"&gt;🔋&lt;/g-emoji&gt; Databases&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png"&gt;✅&lt;/g-emoji&gt; Unit Tests&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="ocean" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30a.png"&gt;🌊&lt;/g-emoji&gt; Kubeflow&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="closed_lock_with_key" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f510.png"&gt;🔐&lt;/g-emoji&gt; Authentication&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-advanced-ml" class="anchor" aria-hidden="true" href="#advanced-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advanced ML&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;General&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Sequential&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Popular&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="3"&gt;
        &lt;ul&gt;
            &lt;li&gt;Dive into architectural and interpretable advancements in neural networks.&lt;/li&gt;
            &lt;li&gt;Implement state-of-the-art NLP techniques.&lt;/li&gt;
            &lt;li&gt;Learn about popular deep learning algorithms used for generation, time-series, etc.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;🧐 Attention&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="bee" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f41d.png"&gt;🐝&lt;/g-emoji&gt; Transformers&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="performing_arts" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ad.png"&gt;🎭&lt;/g-emoji&gt; Generative Adversarial Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="crystal_ball" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f52e.png"&gt;🔮&lt;/g-emoji&gt; Autoencoders&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="racing_car" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ce.png"&gt;🏎️&lt;/g-emoji&gt; Highway Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="japanese_ogre" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f479.png"&gt;👹&lt;/g-emoji&gt; BERT, GPT2, XLNet&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="8ball" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b1.png"&gt;🎱&lt;/g-emoji&gt; Bayesian Deep Learning&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="spider" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f577.png"&gt;🕷️&lt;/g-emoji&gt; Graph Neural Networks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="droplet" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a7.png"&gt;💧&lt;/g-emoji&gt; Residual Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="clock9" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f558.png"&gt;🕘&lt;/g-emoji&gt; Temporal CNNs&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="cherries" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f352.png"&gt;🍒&lt;/g-emoji&gt; Reinforcement Learning&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-topics" class="anchor" aria-hidden="true" href="#topics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Topics&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Computer Vision&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Natural Language&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Unsupervised Learning&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="4"&gt;
        &lt;ul&gt;
            &lt;li&gt;Learn how to use deep learning for computer vision tasks.&lt;/li&gt;
            &lt;li&gt;Implement techniques for natural language tasks.&lt;/li&gt;
            &lt;li&gt;Derive insights from unlabeled data using unsupervised learning.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="camera_flash" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f8.png"&gt;📸&lt;/g-emoji&gt; Image Recognition&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;📖&lt;/g-emoji&gt; Text classification&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="dango" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f361.png"&gt;🍡&lt;/g-emoji&gt; Clustering&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="alarm_clock" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/23f0.png"&gt;⏰&lt;/g-emoji&gt; Time-series Analysis&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;🖼️&lt;/g-emoji&gt; Image Segmentation&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="speech_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png"&gt;💬&lt;/g-emoji&gt; Named Entity Recognition&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="houses" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3d8.png"&gt;🏘️&lt;/g-emoji&gt; Topic Modeling&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="shopping_cart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6d2.png"&gt;🛒&lt;/g-emoji&gt; Recommendation Systems&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="art" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a8.png"&gt;🎨&lt;/g-emoji&gt; Image Generation&lt;/td&gt;
        &lt;td&gt;🧠 Knowledge Graphs&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="dart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png"&gt;🎯&lt;/g-emoji&gt; One-shot Learning&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="card_file_box" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5c3.png"&gt;🗃️&lt;/g-emoji&gt; Interpretability&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://practicalai.me/#newsletter" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="mailbox_with_mail" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ec.png"&gt;📬&lt;/g-emoji&gt; Newsletter&lt;/a&gt; - Subscribe to get updates on new content.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>practicalAI</author><guid isPermaLink="false">https://github.com/practicalAI/practicalAI</guid><pubDate>Fri, 03 Jan 2020 00:04:00 GMT</pubDate></item><item><title>rasbt/python-machine-learning-book-2nd-edition #5 in Jupyter Notebook, This month</title><link>https://github.com/rasbt/python-machine-learning-book-2nd-edition</link><description>&lt;p&gt;&lt;i&gt;The "Python Machine Learning (2nd edition)" book code repository and info resource&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-python-machine-learning-2nd-ed-code-repository" class="anchor" aria-hidden="true" href="#python-machine-learning-2nd-ed-code-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Machine Learning (2nd Ed.) Code Repository&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://travis-ci.com/rasbt/python-machine-learning-book-2nd-edition" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8e6f14639b96b26916fac047dc7b175e928ac608/68747470733a2f2f7472617669732d63692e636f6d2f72617362742f707974686f6e2d6d616368696e652d6c6561726e696e672d626f6f6b2d326e642d65646974696f6e2e7376673f746f6b656e3d7a7653734a564c4a464b7a4232797161654b4e31266272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/rasbt/python-machine-learning-book-2nd-edition.svg?token=zvSsJVLJFKzB2yqaeKN1&amp;amp;branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/4c4736c9a7f8ddf0812e861869d9b1534557b4cb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e362d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/4c4736c9a7f8ddf0812e861869d9b1534557b4cb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e362d626c75652e737667" alt="Python 3.6" data-canonical-src="https://img.shields.io/badge/Python-3.6-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/a0e2e02654c03ef5b20640e5d052b0b448e59313/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64652532304c6963656e73652d4d49542d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/a0e2e02654c03ef5b20640e5d052b0b448e59313/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64652532304c6963656e73652d4d49542d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/Code%20License-MIT-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Please note that a new edition (3rd edition) is now available as of December 2019. The code repository link for the 3rd edition is &lt;a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition"&gt;https://github.com/rasbt/python-machine-learning-book-3rd-edition&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Python Machine Learning, 2nd Ed.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;published September 20th, 2017&lt;/p&gt;
&lt;p&gt;Paperback: 622 pages&lt;br&gt;
Publisher: Packt Publishing&lt;br&gt;
Language: English&lt;/p&gt;
&lt;p&gt;ISBN-10: 1787125939&lt;br&gt;
ISBN-13: 978-1787125933&lt;br&gt;
Kindle ASIN: B0742K7HYF&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939" rel="nofollow"&gt;&lt;img src="./images/cover_1.jpg" width="348" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939" rel="nofollow"&gt;Amazon Page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning-second-edition" rel="nofollow"&gt;Packt Page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents-and-code-notebooks" class="anchor" aria-hidden="true" href="#table-of-contents-and-code-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents and Code Notebooks&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Helpful installation and setup instructions can be found in the &lt;a href="code/ch01/README.md"&gt;README.md file of Chapter 1&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To access the code materials for a given chapter, simply click on the &lt;code&gt;open dir&lt;/code&gt; links next to the chapter headlines to navigate to the chapter subdirectories located in the &lt;a href="code/"&gt;code/&lt;/a&gt; subdirectory. You can also click on the &lt;code&gt;ipynb&lt;/code&gt; links below to open and view the Jupyter notebook of each chapter directly on GitHub.&lt;/p&gt;
&lt;p&gt;In addition, the &lt;a href="code/"&gt;code/&lt;/a&gt; subdirectories also contain .py script files, which were created from the Jupyter Notebooks. However, I highly recommend working with the Jupyter notebook if possible in your computing environment. Not only do the Jupyter notebooks contain the images and section headings for easier navigation, but they also allow for a stepwise execution of individual code snippets, which -- in my opinion -- provide a better learning experience.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please note that these are just the code examples accompanying the book, which I uploaded for your convenience; be aware that these notebooks may not be useful without the formulae and descriptive text.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Machine Learning - Giving Computers the Ability to Learn from Data [&lt;a href="./code/ch01"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch01/ch01.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Training Machine Learning Algorithms for Classification [&lt;a href="./code/ch02"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch02/ch02.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A Tour of Machine Learning Classifiers Using Scikit-Learn [&lt;a href="./code/ch03"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch03/ch03.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Building Good Training Sets – Data Pre-Processing [&lt;a href="./code/ch04"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch04/ch04.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Compressing Data via Dimensionality Reduction [&lt;a href="./code/ch05"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch05/ch05.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning Best Practices for Model Evaluation and Hyperparameter Optimization [&lt;a href="./code/ch06"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch06/ch06.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Combining Different Models for Ensemble Learning [&lt;a href="./code/ch07"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch07/ch07.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Applying Machine Learning to Sentiment Analysis [&lt;a href="./code/ch08"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch08/ch08.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Embedding a Machine Learning Model into a Web Application [&lt;a href="./code/ch09"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch09/ch09.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting Continuous Target Variables with Regression Analysis [&lt;a href="./code/ch10"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch10/ch10.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Working with Unlabeled Data – Clustering Analysis [&lt;a href="./code/ch11"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch11/ch11.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Implementing a Multi-layer Artificial Neural Network from Scratch [&lt;a href="./code/ch12"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch12/ch12.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Parallelizing Neural Network Training with TensorFlow [&lt;a href="./code/ch13"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch13/ch13.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Going Deeper: The Mechanics of TensorFlow [&lt;a href="./code/ch14"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch14/ch14.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Classifying Images with Deep Convolutional Neural Networks [&lt;a href="./code/ch15"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch15/ch15.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Modeling Sequential Data Using Recurrent Neural Networks [&lt;a href="./code/ch16"&gt;open dir&lt;/a&gt;] [&lt;a href="./code/ch16/ch16.ipynb"&gt;ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-whats-new-in-the-second-edition-from-the-first-edition" class="anchor" aria-hidden="true" href="#whats-new-in-the-second-edition-from-the-first-edition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What’s new in the second edition from the first edition?&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Oh, there are so many things that we improved or added; where should I start!? The one issue on top of my priority list was to fix all the nasty typos that were introduced during the layout stage or my oversight. I really appreciated all the helpful feedback from readers in this manner! Furthermore, I addressed all the feedback about sections that may have been confusing or a bit unclear, reworded paragraphs, and added additional explanations. Also, special thanks go to the excellent editors of the second edition, who helped a lot along the way!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Also, the figures and plots became much prettier. While readers liked the graphic content a lot, some people criticized the PowerPoint-esque style and layout. Thus, I decided to overhaul every little figure with a hopefully more pleasing choice of fonts and colors. Also, the data plots look much nicer now, thanks to the matplotlib team who put a lot of work in matplotlib 2.0 and its new styling theme.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Beyond all these cosmetic fixes, new sections were added here and there. Among these is, for example, is a section on dealing with imbalanced datasets, which several readers were missing in the first edition and short section on Latent Dirichlet Allocation among others.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;As time and the software world moved on after the first edition was released in September 2015, we decided to replace the introduction to deep learning via Theano. No worries, we didn't remove it but it got a substantial overhaul and is now based on TensorFlow, which has become a major player in my research toolbox since its open source release by Google in November 2015.
Along with the new introduction to deep learning using TensorFlow, the biggest additions to this new edition are three brand new chapters focussing on deep learning applications: A more detailed overview of the TensorFlow mechanics, an introduction to convolutional neural networks for image classification, and an introduction to recurrent neural networks for natural language processing. Of course, and in a similar vein as the rest of the book, these new chapters do not only provide readers with practical instructions and examples but also introduce the fundamental mathematics behind those concepts, which are an essential building block for understanding how deep learning works.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;[ &lt;a href="https://www.packtpub.com/books/content/machine-learning-useful-every-problem-domain-interview-sebastian-raschka/" rel="nofollow"&gt;Excerpt from "Machine Learning can be useful in almost every problem domain:" An interview with Sebastian Raschka&lt;/a&gt; ]&lt;/p&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;Raschka, Sebastian, and Vahid Mirjalili. &lt;em&gt;Python Machine Learning, 2nd Ed&lt;/em&gt;. Packt Publishing, 2017.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@book{RaschkaMirjalili2017,  
address = {Birmingham, UK},  
author = {Raschka, Sebastian and Mirjalili, Vahid},  
edition = {2},  
isbn = {978-1787125933},  
keywords = {Clustering,Data Science,Deep Learning,  
            Machine Learning,Neural Networks,Programming,  
            Supervised Learning},  
publisher = {Packt Publishing},  
title = {{Python Machine Learning, 2nd Ed.}},  
year = {2017}  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-german" class="anchor" aria-hidden="true" href="#german"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;German&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ISBN-10: 3958457339&lt;/li&gt;
&lt;li&gt;ISBN-13: 978-3958457331&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.de/Machine-Learning-Python-Scikit-Learn-TensorFlow/dp/3958457339/ref=tmm_pap_swatch_0?_encoding=UTF8&amp;amp;qid=1513601461&amp;amp;sr=8-5" rel="nofollow"&gt;Amazon.de link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mitp.de/IT-WEB/Programmierung/Machine-Learning-mit-Python-oxid.html" rel="nofollow"&gt;Publisher link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/cover-german.jpg"&gt;&lt;img src="images/cover-german.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-japanese" class="anchor" aria-hidden="true" href="#japanese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Japanese&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ISBN-10: 4295003379&lt;/li&gt;
&lt;li&gt;ISBN-13: 978-4295003373&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.co.jp/Python-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E9%81%94%E4%BA%BA%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%83%86%E3%82%A3%E3%82%B9%E3%83%88%E3%81%AB%E3%82%88%E3%82%8B%E7%90%86%E8%AB%96%E3%81%A8%E5%AE%9F%E8%B7%B5-impress-gear/dp/4295003379/ref=tmm_pap_swatch_0" rel="nofollow"&gt;Amazon.co.jp link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/cover-japanese.jpg"&gt;&lt;img src="images/cover-japanese.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rasbt</author><guid isPermaLink="false">https://github.com/rasbt/python-machine-learning-book-2nd-edition</guid><pubDate>Fri, 03 Jan 2020 00:05:00 GMT</pubDate></item><item><title>NLP-LOVE/ML-NLP #6 in Jupyter Notebook, This month</title><link>https://github.com/NLP-LOVE/ML-NLP</link><description>&lt;p&gt;&lt;i&gt;此项目是机器学习(Machine Learning)、深度学习(Deep Learning)、NLP面试中常考到的知识点和代码实现，也是作为一个算法工程师必会的理论基础知识。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-项目介绍" class="anchor" aria-hidden="true" href="#项目介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目介绍&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;此项目是&lt;strong&gt;机器学习、NLP面试&lt;/strong&gt;中常考到的&lt;strong&gt;知识点和代码实现&lt;/strong&gt;，也是作为一个算法工程师必会的理论基础知识。&lt;/li&gt;
&lt;li&gt;既然是以面试为主要目的，亦不可以篇概全，请谅解，有问题可提出。&lt;/li&gt;
&lt;li&gt;此项目以各个模块为切入点，让大家有一个清晰的知识体系。&lt;/li&gt;
&lt;li&gt;此项目亦可拿来常读、常记以及面试时复习之用。&lt;/li&gt;
&lt;li&gt;每一章里的问题都是面试时有可能问到的知识点，如有遗漏可联系我进行补充，结尾处都有算法的&lt;strong&gt;实战代码案例&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;有意向一起完成此项目或者有问题、有补充的可以加入&lt;del&gt;NLP学习QQ群【541954936】&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1群已加满，请加2群，NLP学习QQ2群【207576902】&lt;/strong&gt;&lt;a href="http://shang.qq.com/wpa/qunwpa?idkey=1defd70810d9e67ca6ab3a30e1425a8a358139315a186dd2192d82a4c0ca1ce9" rel="nofollow"&gt;&lt;img border="0" src="https://camo.githubusercontent.com/615c9901677f501582b6057efc9396b3ed27dc29/687474703a2f2f7075622e69647171696d672e636f6d2f7770612f696d616765732f67726f75702e706e67" alt="NLP学习群②" title="NLP学习群②" data-canonical-src="http://pub.idqqimg.com/wpa/images/group.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;项目持续更新中......&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模块&lt;/th&gt;
&lt;th&gt;章节&lt;/th&gt;
&lt;th&gt;负责人(GitHub)&lt;/th&gt;
&lt;th&gt;联系QQ&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/Liner%20Regression/1.Liner%20Regression.md"&gt;1. 线性回归(Liner Regression)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/2.Logistics%20Regression/2.Logistics%20Regression.md"&gt;2. 逻辑回归(Logistics Regression)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.Desition%20Tree/Desition%20Tree.md"&gt;3. 决策树(Desision Tree)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.1%20Random%20Forest/3.1%20Random%20Forest.md"&gt;3.1 随机森林(Random Forest)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.2%20GBDT/3.2%20GBDT.md"&gt;3.2 梯度提升决策树(GBDT)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.3%20XGBoost/3.3%20XGBoost.md"&gt;3.3 XGBoost&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.4%20LightGBM/3.4%20LightGBM.md"&gt;3.4 LightGBM&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/4.%20SVM/4.%20SVM.md"&gt;4. 支持向量机(SVM)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;5. 概率图模型(Probabilistic Graphical Model)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/5.1%20Bayes%20Network/5.1%20Bayes%20Network.md"&gt;5.1 贝叶斯网络(Bayesian Network)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/5.2%20Markov/5.2%20Markov.md"&gt;5.2 马尔科夫(Markov)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/5.3%20Topic%20Model"&gt;5.3 主题模型(Topic Model)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/6.%20EM"&gt;6.最大期望算法(EM)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/7.%20Clustering"&gt;7.聚类(Clustering)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/8.%20ML%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E5%92%8C%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"&gt;8.ML特征工程和优化方法&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/9.%20KNN"&gt;9.K近邻算法(KNN)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;深度学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/10.%20Neural%20Network"&gt;10.神经网络(Neural Network)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;深度学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/11.%20CNN"&gt;11. 卷积神经网络(CNN)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;深度学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/12.%20RNN"&gt;12. 循环神经网络(RNN)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;深度学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/12.1%20GRU"&gt;12.1 门控循环单元(GRU)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;深度学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/12.2%20LSTM"&gt;12.2 长短期记忆(LSTM)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;深度学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/13.%20Transfer%20Learning"&gt;13.迁移学习(Transfer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;深度学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/14.%20Reinforcement%20Learning"&gt;14.强化学习(Reinforcement) &amp;amp; 多任务&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;深度学习&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/15.%20DL%20Optimizer"&gt;15. 深度学习的优化方法&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.%20NLP"&gt;16. 自然语言处理(NLP)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.1%20Word%20Embedding"&gt;16.1 词嵌入(Word2Vec)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.2%20fastText"&gt;16.2 子词嵌入(fastText)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.3%20GloVe"&gt;16.3 全局向量词嵌入(GloVe)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.4%20textRNN%20%26%20textCNN"&gt;16.4 textRNN &amp;amp; textCNN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.5%20seq2seq"&gt;16.5 序列到序列模型(seq2seq)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.6%20Attention"&gt;16.6 注意力机制(Attention Mechanism)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.7%20Transformer"&gt;16.7 Transformer模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.8%20BERT"&gt;16.8 BERT模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.9%20XLNet"&gt;16.9 XLNet模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;项目&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Project/17.%20Recommendation%20System"&gt;17. 推荐系统(Recommendation System)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;项目&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Project/18.%20Intelligent%20Customer%20Service"&gt;18. 智能客服(Intelligent Customer Service)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/NLP-LOVE"&gt;@mantchs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;448966528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;项目&lt;/td&gt;
&lt;td&gt;19. 知识图谱(Knowledge Graph)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;项目&lt;/td&gt;
&lt;td&gt;20. 评论分析&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;欢迎大家加入！共同完善此项目！NLP学习QQ2群【207576902】&lt;a href="http://shang.qq.com/wpa/qunwpa?idkey=1defd70810d9e67ca6ab3a30e1425a8a358139315a186dd2192d82a4c0ca1ce9" rel="nofollow"&gt;&lt;img border="0" src="https://camo.githubusercontent.com/615c9901677f501582b6057efc9396b3ed27dc29/687474703a2f2f7075622e69647171696d672e636f6d2f7770612f696d616765732f67726f75702e706e67" alt="NLP学习群②" title="NLP学习群②" data-canonical-src="http://pub.idqqimg.com/wpa/images/group.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NLP-LOVE</author><guid isPermaLink="false">https://github.com/NLP-LOVE/ML-NLP</guid><pubDate>Fri, 03 Jan 2020 00:06:00 GMT</pubDate></item><item><title>ageron/handson-ml2 #7 in Jupyter Notebook, This month</title><link>https://github.com/ageron/handson-ml2</link><description>&lt;p&gt;&lt;i&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-notebooks" class="anchor" aria-hidden="true" href="#machine-learning-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Notebooks&lt;/h1&gt;
&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in
python. It contains the example code and solutions to the exercises in the second edition of my O'Reilly book &lt;a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="nofollow"&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/bdec1a5ed5a56e2ab3fc0c4decda7081bd62d662/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f353161715963315179724c2e5f53583337395f424f312c3230342c3230332c3230305f2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/bdec1a5ed5a56e2ab3fc0c4decda7081bd62d662/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f353161715963315179724c2e5f53583337395f424f312c3230342c3230332c3230305f2e6a7067" title="book" width="150" data-canonical-src="https://images-na.ssl-images-amazon.com/images/I/51aqYc1QyrL._SX379_BO1,204,203,200_.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the first edition notebooks, check out &lt;a href="https://github.com/ageron/handson-ml"&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-play-with-these-notebooks-online-without-having-to-install-anything" class="anchor" aria-hidden="true" href="#want-to-play-with-these-notebooks-online-without-having-to-install-anything"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt;
&lt;p&gt;Use any of the following services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Recommended&lt;/strong&gt;: open this repository in &lt;a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/" rel="nofollow"&gt;Colaboratory&lt;/a&gt;:
&lt;a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e69988217d15707bdd8b6b27f1d7d53a0dd00af7/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f696d672f636f6c61625f66617669636f6e2e69636f" width="90" data-canonical-src="https://colab.research.google.com/img/colab_favicon.ico" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or open it in &lt;a href="https://mybinder.org/v2/gh/ageron/handson-ml2/master" rel="nofollow"&gt;Binder&lt;/a&gt;:
&lt;a href="https://mybinder.org/v2/gh/ageron/handson-ml2/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/69ea8abed4df43bca4c671b965aeffef2c4f897a/68747470733a2f2f6d61747468696173627573736f6e6e6965722e636f6d2f706f7374732f696d672f62696e6465725f6c6f676f5f313238783132382e706e67" width="90" data-canonical-src="https://matthiasbussonnier.com/posts/img/binder_logo_128x128.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Note&lt;/em&gt;: Most of the time, Binder starts up quickly and works great, but when handson-ml2 is updated, Binder creates a new environment from scratch, and this can take quite some time.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or open it in &lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;Deepnote&lt;/a&gt;:
&lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3fae03be31b768100aa2a800d2cc3b6650c6cd48/68747470733a2f2f7777772e646565706e6f74652e636f6d2f7374617469632f696c6c757374726174696f6e2e706e67" width="150" data-canonical-src="https://www.deepnote.com/static/illustration.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-just-want-to-quickly-look-at-some-notebooks-without-executing-any-code" class="anchor" aria-hidden="true" href="#just-want-to-quickly-look-at-some-notebooks-without-executing-any-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt;
&lt;p&gt;Browse this repository using &lt;a href="https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;jupyter.org's notebook viewer&lt;/a&gt;:
&lt;a href="https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/079030b4c39b76eafa0c6c3a5bd18112aafe42dd/68747470733a2f2f6a7570797465722e6f72672f6173736574732f6e61765f6c6f676f2e737667" width="150" data-canonical-src="https://jupyter.org/assets/nav_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: &lt;a href="index.ipynb"&gt;github.com's notebook viewer&lt;/a&gt; also works but it is slower and the math equations are not always displayed correctly.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-run-this-project-using-a-docker-image" class="anchor" aria-hidden="true" href="#want-to-run-this-project-using-a-docker-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to run this project using a Docker image?&lt;/h3&gt;
&lt;p&gt;Read the &lt;a href="https://github.com/ageron/handson-ml2/tree/master/docker"&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-install-this-project-on-your-own-machine" class="anchor" aria-hidden="true" href="#want-to-install-this-project-on-your-own-machine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to install this project on your own machine?&lt;/h3&gt;
&lt;p&gt;Start by installing &lt;a href="https://www.anaconda.com/distribution/" rel="nofollow"&gt;Anaconda&lt;/a&gt; (or &lt;a href="https://docs.conda.io/en/latest/miniconda.html" rel="nofollow"&gt;Miniconda&lt;/a&gt;), &lt;a href="https://git-scm.com/downloads" rel="nofollow"&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href="https://www.nvidia.com/Download/index.aspx" rel="nofollow"&gt;GPU driver&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml2.git
$ cd handson-ml2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to use a GPU, then edit &lt;code&gt;environment.yml&lt;/code&gt; (or &lt;code&gt;environment-windows.yml&lt;/code&gt; on Windows) and replace &lt;code&gt;tensorflow=2.0.0&lt;/code&gt; with &lt;code&gt;tensorflow-gpu=2.0.0&lt;/code&gt;. Also replace &lt;code&gt;tensorflow-serving-api==2.0.0&lt;/code&gt; with &lt;code&gt;tensorflow-serving-api-gpu==2.0.0&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, run the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml # or environment-windows.yml on Windows
$ conda activate tf2
$ python -m ipykernel install --user --name=python3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then if you're on Windows, run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, start Jupyter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you need further instructions, read the &lt;a href="INSTALL.md"&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;I would like to thank everyone who contributed to this project, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park who helped on some of the exercise solutions, and to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ageron</author><guid isPermaLink="false">https://github.com/ageron/handson-ml2</guid><pubDate>Fri, 03 Jan 2020 00:07:00 GMT</pubDate></item><item><title>lexfridman/mit-deep-learning #8 in Jupyter Notebook, This month</title><link>https://github.com/lexfridman/mit-deep-learning</link><description>&lt;p&gt;&lt;i&gt;Tutorials, assignments, and competitions for MIT Deep Learning related courses.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-mit-deep-learning" class="anchor" aria-hidden="true" href="#mit-deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MIT Deep Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://deeplearning.mit.edu/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/42d3d635cdb0e88dee786c6edc1f41e0902faa2b/68747470733a2f2f646565706c6561726e696e672e6d69742e6564752f66696c65732f696d616765732f6d69745f646565705f6c6561726e696e672e706e67" data-canonical-src="https://deeplearning.mit.edu/files/images/mit_deep_learning.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository is a collection of tutorials for &lt;a href="https://deeplearning.mit.edu/" rel="nofollow"&gt;MIT Deep Learning&lt;/a&gt; courses. More added as courses progress.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tutorial-deep-learning-basics" class="anchor" aria-hidden="true" href="#tutorial-deep-learning-basics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorial: Deep Learning Basics&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb"&gt;&lt;img src="https://camo.githubusercontent.com/0d2d7920619e3df257f1c677431968ff491a5e80/68747470733a2f2f692e696d6775722e636f6d2f6a3446714275522e676966" data-canonical-src="https://i.imgur.com/j4FqBuR.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tutorial accompanies the &lt;a href="https://www.youtube.com/watch?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf&amp;amp;v=O5xeyoRL95U" rel="nofollow"&gt;lecture on Deep Learning Basics&lt;/a&gt;. It presents several concepts in deep learning, demonstrating the first two (feed forward and convolutional neural networks) and providing pointers to tutorials on the others. This is a good place to start.&lt;/p&gt;
&lt;p&gt;Links: [ &lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb"&gt;Jupyter Notebook&lt;/a&gt; ]
[ &lt;a href="https://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt; ]
[ &lt;a href="https://medium.com/tensorflow/mit-deep-learning-basics-introduction-and-overview-with-tensorflow-355bcd26baf0" rel="nofollow"&gt;Blog Post&lt;/a&gt; ]
[ &lt;a href="https://www.youtube.com/watch?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf&amp;amp;v=O5xeyoRL95U" rel="nofollow"&gt;Lecture Video&lt;/a&gt; ]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tutorial-driving-scene-segmentation" class="anchor" aria-hidden="true" href="#tutorial-driving-scene-segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorial: Driving Scene Segmentation&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb"&gt;&lt;img src="images/thumb_driving_scene_segmentation.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tutorial demostrates semantic segmentation with a state-of-the-art model (DeepLab) on a sample video from the MIT Driving Scene Segmentation Dataset.&lt;/p&gt;
&lt;p&gt;Links: [ &lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb"&gt;Jupyter Notebook&lt;/a&gt; ]
[ &lt;a href="https://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt; ]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tutorial-generative-adversarial-networks-gans" class="anchor" aria-hidden="true" href="#tutorial-generative-adversarial-networks-gans"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorial: Generative Adversarial Networks (GANs)&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_gans/tutorial_gans.ipynb"&gt;&lt;img src="images/thumb_mushroom_biggan.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tutorial explores generative adversarial networks (GANs) starting with BigGAN, the state-of-the-art conditional GAN.&lt;/p&gt;
&lt;p&gt;Links: [ &lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_gans/tutorial_gans.ipynb"&gt;Jupyter Notebook&lt;/a&gt; ]
[ &lt;a href="https://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_gans/tutorial_gans.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt; ]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deeptraffic-deep-reinforcement-learning-competition" class="anchor" aria-hidden="true" href="#deeptraffic-deep-reinforcement-learning-competition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepTraffic Deep Reinforcement Learning Competition&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://selfdrivingcars.mit.edu/deeptraffic" rel="nofollow"&gt;&lt;img src="images/thumb_deeptraffic.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;DeepTraffic is a deep reinforcement learning competition. The goal is to create a neural network that drives a vehicle (or multiple vehicles) as fast as possible through dense highway traffic.&lt;/p&gt;
&lt;p&gt;Links: [ &lt;a href="https://github.com/lexfridman/deeptraffic"&gt;GitHub&lt;/a&gt; ] [ &lt;a href="https://selfdrivingcars.mit.edu/deeptraffic" rel="nofollow"&gt;Website&lt;/a&gt; ] [ &lt;a href="https://arxiv.org/abs/1801.02805" rel="nofollow"&gt;Paper&lt;/a&gt; ]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-team" class="anchor" aria-hidden="true" href="#team"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Team&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://lexfridman.com" rel="nofollow"&gt;Lex Fridman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~liding/" rel="nofollow"&gt;Li Ding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~jterwill/" rel="nofollow"&gt;Jack Terwilliger&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~glazermi/" rel="nofollow"&gt;Michael Glazer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~patsekin/" rel="nofollow"&gt;Aleksandr Patsekin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~aishni/" rel="nofollow"&gt;Aishni Parab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~aladawy/" rel="nofollow"&gt;Dina AlAdawy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~henris/" rel="nofollow"&gt;Henri Schmidt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>lexfridman</author><guid isPermaLink="false">https://github.com/lexfridman/mit-deep-learning</guid><pubDate>Fri, 03 Jan 2020 00:08:00 GMT</pubDate></item><item><title>trekhleb/homemade-machine-learning #9 in Jupyter Notebook, This month</title><link>https://github.com/trekhleb/homemade-machine-learning</link><description>&lt;p&gt;&lt;i&gt;🤖 Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-homemade-machine-learning" class="anchor" aria-hidden="true" href="#homemade-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Homemade Machine Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://mybinder.org/v2/gh/trekhleb/homemade-machine-learning/master?filepath=notebooks" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/trekhleb/homemade-machine-learning" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ec56d7f6b55050cd6e8ee599c29b1436223ebebc/68747470733a2f2f7472617669732d63692e6f72672f7472656b686c65622f686f6d656d6164652d6d616368696e652d6c6561726e696e672e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/trekhleb/homemade-machine-learning.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For Octave/MatLab version of this repository please check &lt;a href="https://github.com/trekhleb/machine-learning-octave"&gt;machine-learning-octave&lt;/a&gt; project.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This repository contains examples of popular machine learning algorithms implemented in &lt;strong&gt;Python&lt;/strong&gt; with mathematics behind them being explained. Each algorithm has interactive &lt;strong&gt;Jupyter Notebook&lt;/strong&gt; demo that allows you to play with training data, algorithms configurations and immediately see the results, charts and predictions &lt;strong&gt;right in your browser&lt;/strong&gt;. In most cases the explanations are based on &lt;a href="https://www.coursera.org/learn/machine-learning" rel="nofollow"&gt;this great machine learning course&lt;/a&gt; by Andrew Ng.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The purpose of this repository is &lt;em&gt;not&lt;/em&gt; to implement machine learning algorithms by using 3&lt;sup&gt;rd&lt;/sup&gt; party library one-liners &lt;em&gt;but&lt;/em&gt; rather to practice implementing these algorithms from scratch and get better understanding of the mathematics behind each algorithm. That's why all algorithms implementations are called "homemade" and not intended to be used for production.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-supervised-learning" class="anchor" aria-hidden="true" href="#supervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supervised Learning&lt;/h2&gt;
&lt;p&gt;In supervised learning we have a set of training data as an input and a set of labels or "correct answers" for each training set as an output. Then we're training our model (machine learning algorithm parameters) to map the input to the output correctly (to do correct prediction). The ultimate purpose is to find such model parameters that will successfully continue correct &lt;em&gt;input→output&lt;/em&gt; mapping (predictions) even for new input examples.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-regression" class="anchor" aria-hidden="true" href="#regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Regression&lt;/h3&gt;
&lt;p&gt;In regression problems we do real value predictions. Basically we try to draw a line/plane/n-dimensional plane along the training examples.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Usage examples: stock price forecast, sales analysis, dependency of any number, etc.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content--linear-regression" class="anchor" aria-hidden="true" href="#-linear-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;🤖&lt;/g-emoji&gt; Linear Regression&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;📗&lt;/g-emoji&gt; &lt;a href="homemade/linear_regression"&gt;Math | Linear Regression&lt;/a&gt; - theory and links for further readings&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;⚙️&lt;/g-emoji&gt; &lt;a href="homemade/linear_regression/linear_regression.py"&gt;Code | Linear Regression&lt;/a&gt; - implementation example&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;▶️&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/univariate_linear_regression_demo.ipynb" rel="nofollow"&gt;Demo | Univariate Linear Regression&lt;/a&gt; - predict &lt;code&gt;country happiness&lt;/code&gt; score by &lt;code&gt;economy GDP&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;▶️&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/multivariate_linear_regression_demo.ipynb" rel="nofollow"&gt;Demo | Multivariate Linear Regression&lt;/a&gt; - predict &lt;code&gt;country happiness&lt;/code&gt; score by &lt;code&gt;economy GDP&lt;/code&gt; and &lt;code&gt;freedom index&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;▶️&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/non_linear_regression_demo.ipynb" rel="nofollow"&gt;Demo | Non-linear Regression&lt;/a&gt; - use linear regression with &lt;em&gt;polynomial&lt;/em&gt; and &lt;em&gt;sinusoid&lt;/em&gt; features to predict non-linear dependencies&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-classification" class="anchor" aria-hidden="true" href="#classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Classification&lt;/h3&gt;
&lt;p&gt;In classification problems we split input examples by certain characteristic.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Usage examples: spam-filters, language detection, finding similar documents, handwritten letters recognition, etc.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content--logistic-regression" class="anchor" aria-hidden="true" href="#-logistic-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;🤖&lt;/g-emoji&gt; Logistic Regression&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;📗&lt;/g-emoji&gt; &lt;a href="homemade/logistic_regression"&gt;Math | Logistic Regression&lt;/a&gt; - theory and links for further readings&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;⚙️&lt;/g-emoji&gt; &lt;a href="homemade/logistic_regression/logistic_regression.py"&gt;Code | Logistic Regression&lt;/a&gt; - implementation example&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;▶️&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/logistic_regression_with_linear_boundary_demo.ipynb" rel="nofollow"&gt;Demo | Logistic Regression (Linear Boundary)&lt;/a&gt; - predict Iris flower &lt;code&gt;class&lt;/code&gt; based on &lt;code&gt;petal_length&lt;/code&gt; and &lt;code&gt;petal_width&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;▶️&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/logistic_regression_with_non_linear_boundary_demo.ipynb" rel="nofollow"&gt;Demo | Logistic Regression (Non-Linear Boundary)&lt;/a&gt; - predict microchip &lt;code&gt;validity&lt;/code&gt; based on &lt;code&gt;param_1&lt;/code&gt; and &lt;code&gt;param_2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;▶️&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/multivariate_logistic_regression_demo.ipynb" rel="nofollow"&gt;Demo | Multivariate Logistic Regression | MNIST&lt;/a&gt; - recognize handwritten digits from &lt;code&gt;28x28&lt;/code&gt; pixel images&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;▶️&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/multivariate_logistic_regression_fashion_demo.ipynb" rel="nofollow"&gt;Demo | Multivariate Logistic Regression | Fashion MNIST&lt;/a&gt; - recognize clothes types from &lt;code&gt;28x28&lt;/code&gt; pixel images&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-unsupervised-learning" class="anchor" aria-hidden="true" href="#unsupervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised Learning&lt;/h2&gt;
&lt;p&gt;Unsupervised learning is a branch of machine learning that learns from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-clustering" class="anchor" aria-hidden="true" href="#clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Clustering&lt;/h3&gt;
&lt;p&gt;In clustering problems we split the training examples by unknown characteristics. The algorithm itself decides what characteristic to use for splitting.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Usage examples: market segmentation, social networks analysis, organize computing clusters, astronomical data analysis, image compression, etc.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content--k-means-algorithm" class="anchor" aria-hidden="true" href="#-k-means-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;🤖&lt;/g-emoji&gt; K-means Algorithm&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;📗&lt;/g-emoji&gt; &lt;a href="homemade/k_means"&gt;Math | K-means Algorithm&lt;/a&gt; - theory and links for further readings&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;⚙️&lt;/g-emoji&gt; &lt;a href="homemade/k_means/k_means.py"&gt;Code | K-means Algorithm&lt;/a&gt; - implementation example&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;▶️&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/k_means/k_means_demo.ipynb" rel="nofollow"&gt;Demo | K-means Algorithm&lt;/a&gt; - split Iris flowers into clusters based on &lt;code&gt;petal_length&lt;/code&gt; and &lt;code&gt;petal_width&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-anomaly-detection" class="anchor" aria-hidden="true" href="#anomaly-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Anomaly Detection&lt;/h3&gt;
&lt;p&gt;Anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Usage examples: intrusion detection, fraud detection, system health monitoring, removing anomalous data from the dataset etc.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content--anomaly-detection-using-gaussian-distribution" class="anchor" aria-hidden="true" href="#-anomaly-detection-using-gaussian-distribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;🤖&lt;/g-emoji&gt; Anomaly Detection using Gaussian Distribution&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;📗&lt;/g-emoji&gt; &lt;a href="homemade/anomaly_detection"&gt;Math | Anomaly Detection using Gaussian Distribution&lt;/a&gt; - theory and links for further readings&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;⚙️&lt;/g-emoji&gt; &lt;a href="homemade/anomaly_detection/gaussian_anomaly_detection.py"&gt;Code | Anomaly Detection using Gaussian Distribution&lt;/a&gt; - implementation example&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;▶️&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/anomaly_detection/anomaly_detection_gaussian_demo.ipynb" rel="nofollow"&gt;Demo | Anomaly Detection&lt;/a&gt; - find anomalies in server operational parameters like &lt;code&gt;latency&lt;/code&gt; and &lt;code&gt;threshold&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-neural-network-nn" class="anchor" aria-hidden="true" href="#neural-network-nn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Neural Network (NN)&lt;/h2&gt;
&lt;p&gt;The neural network itself isn't an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Usage examples: as a substitute of all other algorithms in general, image recognition, voice recognition, image processing (applying specific style), language translation, etc.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content--multilayer-perceptron-mlp" class="anchor" aria-hidden="true" href="#-multilayer-perceptron-mlp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;🤖&lt;/g-emoji&gt; Multilayer Perceptron (MLP)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;📗&lt;/g-emoji&gt; &lt;a href="homemade/neural_network"&gt;Math | Multilayer Perceptron&lt;/a&gt; - theory and links for further readings&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;⚙️&lt;/g-emoji&gt; &lt;a href="homemade/neural_network/multilayer_perceptron.py"&gt;Code | Multilayer Perceptron&lt;/a&gt; - implementation example&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;▶️&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/neural_network/multilayer_perceptron_demo.ipynb" rel="nofollow"&gt;Demo | Multilayer Perceptron | MNIST&lt;/a&gt; - recognize handwritten digits from &lt;code&gt;28x28&lt;/code&gt; pixel images&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="arrow_forward" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/25b6.png"&gt;▶️&lt;/g-emoji&gt; &lt;a href="https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/neural_network/multilayer_perceptron_fashion_demo.ipynb" rel="nofollow"&gt;Demo | Multilayer Perceptron | Fashion MNIST&lt;/a&gt; - recognize the type of clothes from &lt;code&gt;28x28&lt;/code&gt; pixel images&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-machine-learning-map" class="anchor" aria-hidden="true" href="#machine-learning-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Map&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/machine-learning-map.png"&gt;&lt;img src="images/machine-learning-map.png" alt="Machine Learning Map" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The source of the following machine learning topics map is &lt;a href="https://vas3k.ru/blog/machine_learning/" rel="nofollow"&gt;this wonderful blog post&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-installing-python" class="anchor" aria-hidden="true" href="#installing-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing Python&lt;/h4&gt;
&lt;p&gt;Make sure that you have &lt;a href="https://realpython.com/installing-python/" rel="nofollow"&gt;Python installed&lt;/a&gt; on your machine.&lt;/p&gt;
&lt;p&gt;You might want to use &lt;a href="https://docs.python.org/3/library/venv.html" rel="nofollow"&gt;venv&lt;/a&gt; standard Python library
to create virtual environments and have Python, &lt;code&gt;pip&lt;/code&gt; and all dependent packages to be installed and
served from the local project directory to avoid messing with system wide packages and their
versions.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-installing-dependencies" class="anchor" aria-hidden="true" href="#installing-dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing Dependencies&lt;/h4&gt;
&lt;p&gt;Install all dependencies that are required for the project by running:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-launching-jupyter-locally" class="anchor" aria-hidden="true" href="#launching-jupyter-locally"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Launching Jupyter Locally&lt;/h4&gt;
&lt;p&gt;All demos in the project may be run directly in your browser without installing Jupyter locally. But if you want to launch &lt;a href="http://jupyter.org/" rel="nofollow"&gt;Jupyter Notebook&lt;/a&gt; locally you may do it by running the following command from the root folder of the project:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;jupyter notebook&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After this Jupyter Notebook will be accessible by &lt;code&gt;http://localhost:8888&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-launching-jupyter-remotely" class="anchor" aria-hidden="true" href="#launching-jupyter-remotely"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Launching Jupyter Remotely&lt;/h4&gt;
&lt;p&gt;Each algorithm section contains demo links to &lt;a href="http://nbviewer.jupyter.org/" rel="nofollow"&gt;Jupyter NBViewer&lt;/a&gt;. This is fast online previewer for Jupyter notebooks where you may see demo code, charts and data right in your browser without installing anything locally. In case if you want to &lt;em&gt;change&lt;/em&gt; the code and &lt;em&gt;experiment&lt;/em&gt; with demo notebook you need to launch the notebook in &lt;a href="https://mybinder.org/" rel="nofollow"&gt;Binder&lt;/a&gt;. You may do it by simply clicking the &lt;em&gt;"Execute on Binder"&lt;/em&gt; link in top right corner of the NBViewer.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/binder-button-place.png"&gt;&lt;img src="./images/binder-button-place.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h2&gt;
&lt;p&gt;The list of datasets that is being used for Jupyter Notebook demos may be found in &lt;a href="data"&gt;data folder&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>trekhleb</author><guid isPermaLink="false">https://github.com/trekhleb/homemade-machine-learning</guid><pubDate>Fri, 03 Jan 2020 00:09:00 GMT</pubDate></item><item><title>huseinzol05/Stock-Prediction-Models #10 in Jupyter Notebook, This month</title><link>https://github.com/huseinzol05/Stock-Prediction-Models</link><description>&lt;p&gt;&lt;i&gt;Gathers machine learning and deep learning models for Stock forecasting including trading bots and simulations&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;a href="#readme"&gt;
        &lt;img alt="logo" width="50%" src="output/evolution-strategy.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models/blob/master/LICENSE"&gt;&lt;img alt="MIT License" src="https://camo.githubusercontent.com/65c6912b68aeea16375c94041820c27b3d8d9c8b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368652d2d4c6963656e73652d2d322e302d79656c6c6f772e737667" data-canonical-src="https://img.shields.io/badge/License-Apache--License--2.0-yellow.svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="#"&gt;&lt;img src="https://camo.githubusercontent.com/a109e6ff3099b0fe22cf4964a86453b3992990d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646565706c6561726e696e672d33302d2d6d6f64656c732d737563636573732e737667" data-canonical-src="https://img.shields.io/badge/deeplearning-30--models-success.svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="#"&gt;&lt;img src="https://camo.githubusercontent.com/49a8b81e45fcd97bb447147c89e1dc8d7ba5db83/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6167656e742d32332d2d6d6f64656c732d737563636573732e737667" data-canonical-src="https://img.shields.io/badge/agent-23--models-success.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Stock-Prediction-Models&lt;/strong&gt;, Gathers machine learning and deep learning models for Stock forecasting, included trading bots and simulations.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#models"&gt;Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#agents"&gt;Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="realtime-agent"&gt;Realtime Agent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#data-explorations"&gt;Data Explorations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#simulations"&gt;Simulations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#tensorflow-js"&gt;Tensorflow-js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#misc"&gt;Misc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#results"&gt;Results&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#results-agent"&gt;Results Agent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#results-signal-prediction"&gt;Results signal prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#results-analysis"&gt;Results analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#results-simulation"&gt;Results simulation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-models" class="anchor" aria-hidden="true" href="#models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-deep-learning-models" class="anchor" aria-hidden="true" href="#deep-learning-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="deep-learning"&gt;Deep-learning models&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;LSTM Bidirectional&lt;/li&gt;
&lt;li&gt;LSTM 2-Path&lt;/li&gt;
&lt;li&gt;GRU&lt;/li&gt;
&lt;li&gt;GRU Bidirectional&lt;/li&gt;
&lt;li&gt;GRU 2-Path&lt;/li&gt;
&lt;li&gt;Vanilla&lt;/li&gt;
&lt;li&gt;Vanilla Bidirectional&lt;/li&gt;
&lt;li&gt;Vanilla 2-Path&lt;/li&gt;
&lt;li&gt;LSTM Seq2seq&lt;/li&gt;
&lt;li&gt;LSTM Bidirectional Seq2seq&lt;/li&gt;
&lt;li&gt;LSTM Seq2seq VAE&lt;/li&gt;
&lt;li&gt;GRU Seq2seq&lt;/li&gt;
&lt;li&gt;GRU Bidirectional Seq2seq&lt;/li&gt;
&lt;li&gt;GRU Seq2seq VAE&lt;/li&gt;
&lt;li&gt;Attention-is-all-you-Need&lt;/li&gt;
&lt;li&gt;CNN-Seq2seq&lt;/li&gt;
&lt;li&gt;Dilated-CNN-Seq2seq&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How to use one of the model to forecast &lt;code&gt;t + N&lt;/code&gt;, &lt;a href="deep-learning/how-to-forecast.ipynb"&gt;how-to-forecast.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consensus, how to use sentiment data to forecast &lt;code&gt;t + N&lt;/code&gt;, &lt;a href="deep-learning/sentiment-consensus.ipynb"&gt;sentiment-consensus.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-stacking-models" class="anchor" aria-hidden="true" href="#stacking-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="stacking"&gt;Stacking models&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Deep Feed-forward Auto-Encoder Neural Network to reduce dimension + Deep Recurrent Neural Network + ARIMA + Extreme Boosting Gradient Regressor&lt;/li&gt;
&lt;li&gt;Adaboost + Bagging + Extra Trees + Gradient Boosting + Random Forest + XGB&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-agents" class="anchor" aria-hidden="true" href="#agents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="agent"&gt;Agents&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Turtle-trading agent&lt;/li&gt;
&lt;li&gt;Moving-average agent&lt;/li&gt;
&lt;li&gt;Signal rolling agent&lt;/li&gt;
&lt;li&gt;Policy-gradient agent&lt;/li&gt;
&lt;li&gt;Q-learning agent&lt;/li&gt;
&lt;li&gt;Evolution-strategy agent&lt;/li&gt;
&lt;li&gt;Double Q-learning agent&lt;/li&gt;
&lt;li&gt;Recurrent Q-learning agent&lt;/li&gt;
&lt;li&gt;Double Recurrent Q-learning agent&lt;/li&gt;
&lt;li&gt;Duel Q-learning agent&lt;/li&gt;
&lt;li&gt;Double Duel Q-learning agent&lt;/li&gt;
&lt;li&gt;Duel Recurrent Q-learning agent&lt;/li&gt;
&lt;li&gt;Double Duel Recurrent Q-learning agent&lt;/li&gt;
&lt;li&gt;Actor-critic agent&lt;/li&gt;
&lt;li&gt;Actor-critic Duel agent&lt;/li&gt;
&lt;li&gt;Actor-critic Recurrent agent&lt;/li&gt;
&lt;li&gt;Actor-critic Duel Recurrent agent&lt;/li&gt;
&lt;li&gt;Curiosity Q-learning agent&lt;/li&gt;
&lt;li&gt;Recurrent Curiosity Q-learning agent&lt;/li&gt;
&lt;li&gt;Duel Curiosity Q-learning agent&lt;/li&gt;
&lt;li&gt;Neuro-evolution agent&lt;/li&gt;
&lt;li&gt;Neuro-evolution with Novelty search agent&lt;/li&gt;
&lt;li&gt;ABCD strategy agent&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-data-explorations" class="anchor" aria-hidden="true" href="#data-explorations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="misc"&gt;Data Explorations&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;stock market study on TESLA stock, &lt;a href="misc/tesla-study.ipynb"&gt;tesla-study.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Outliers study using K-means, SVM, and Gaussian on TESLA stock, &lt;a href="misc/outliers.ipynb"&gt;outliers.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Overbought-Oversold study on TESLA stock, &lt;a href="misc/overbought-oversold.ipynb"&gt;overbought-oversold.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Which stock you need to buy? &lt;a href="misc/which-stock.ipynb"&gt;which-stock.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-simulations" class="anchor" aria-hidden="true" href="#simulations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="simulation"&gt;Simulations&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Simple Monte Carlo, &lt;a href="simulation/monte-carlo-drift.ipynb"&gt;monte-carlo-drift.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dynamic volatility Monte Carlo, &lt;a href="simulation/monte-carlo-dynamic-volatility.ipynb"&gt;monte-carlo-dynamic-volatility.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Drift Monte Carlo, &lt;a href="simulation/monte-carlo-drift.ipynb"&gt;monte-carlo-drift.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Multivariate Drift Monte Carlo BTC/USDT with Bitcurate sentiment, &lt;a href="simulation/multivariate-drift-monte-carlo.ipynb"&gt;multivariate-drift-monte-carlo.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Portfolio optimization, &lt;a href="simulation/portfolio-optimization.ipynb"&gt;portfolio-optimization.ipynb&lt;/a&gt;, inspired from &lt;a href="https://pythonforfinance.net/2017/01/21/investment-portfolio-optimisation-with-python/" rel="nofollow"&gt;https://pythonforfinance.net/2017/01/21/investment-portfolio-optimisation-with-python/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-tensorflow-js" class="anchor" aria-hidden="true" href="#tensorflow-js"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="stock-forecasting-js"&gt;Tensorflow-js&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I code &lt;a href="deep-learning/1.lstm.ipynb"&gt;LSTM Recurrent Neural Network&lt;/a&gt; and &lt;a href="agent/simple-agent.ipynb"&gt;Simple signal rolling agent&lt;/a&gt; inside Tensorflow JS, you can try it here, &lt;a href="https://huseinhouse.com/stock-forecasting-js/" rel="nofollow"&gt;huseinhouse.com/stock-forecasting-js&lt;/a&gt;, you can download any historical CSV and upload dynamically.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-misc" class="anchor" aria-hidden="true" href="#misc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="misc"&gt;Misc&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;fashion trending prediction with cross-validation, &lt;a href="misc/fashion-forecasting.ipynb"&gt;fashion-forecasting.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bitcoin analysis with LSTM prediction, &lt;a href="misc/bitcoin-analysis-lstm.ipynb"&gt;bitcoin-analysis-lstm.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kijang Emas Bank Negara, &lt;a href="misc/kijang-emas-bank-negara.ipynb"&gt;kijang-emas-bank-negara.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-results" class="anchor" aria-hidden="true" href="#results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-results-agent" class="anchor" aria-hidden="true" href="#results-agent"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results Agent&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;This agent only able to buy or sell 1 unit per transaction.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Turtle-trading agent, &lt;a href="agent/1.turtle-agent.ipynb"&gt;turtle-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/turtle-agent.png"&gt;&lt;img src="output-agent/turtle-agent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Moving-average agent, &lt;a href="agent/2.moving-average-agent.ipynb"&gt;moving-average-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/moving-average-agent.png"&gt;&lt;img src="output-agent/moving-average-agent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Signal rolling agent, &lt;a href="agent/3.signal-rolling-agent.ipynb"&gt;signal-rolling-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/signal-rolling-agent.png"&gt;&lt;img src="output-agent/signal-rolling-agent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Policy-gradient agent, &lt;a href="agent/4.policy-gradient-agent.ipynb"&gt;policy-gradient-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/policy-gradient-agent.png"&gt;&lt;img src="output-agent/policy-gradient-agent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Q-learning agent, &lt;a href="agent/5.q-learning-agent.ipynb"&gt;q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/q-learning-agent.png"&gt;&lt;img src="output-agent/q-learning-agent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="6"&gt;
&lt;li&gt;Evolution-strategy agent, &lt;a href="agent/6.evolution-strategy-agent.ipynb"&gt;evolution-strategy-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/evolution-strategy-agent.png"&gt;&lt;img src="output-agent/evolution-strategy-agent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="7"&gt;
&lt;li&gt;Double Q-learning agent, &lt;a href="agent/7.double-q-learning-agent.ipynb"&gt;double-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/double-q-learning.png"&gt;&lt;img src="output-agent/double-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="8"&gt;
&lt;li&gt;Recurrent Q-learning agent, &lt;a href="agent/8.recurrent-q-learning-agent.ipynb"&gt;recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/recurrent-q-learning.png"&gt;&lt;img src="output-agent/recurrent-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="9"&gt;
&lt;li&gt;Double Recurrent Q-learning agent, &lt;a href="agent/9.double-recurrent-q-learning-agent.ipynb"&gt;double-recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/double-recurrent-q-learning.png"&gt;&lt;img src="output-agent/double-recurrent-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="10"&gt;
&lt;li&gt;Duel Q-learning agent, &lt;a href="agent/10.duel-q-learning-agent.ipynb"&gt;duel-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/double-q-learning.png"&gt;&lt;img src="output-agent/double-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="11"&gt;
&lt;li&gt;Double Duel Q-learning agent, &lt;a href="agent/11.double-duel-q-learning-agent.ipynb"&gt;double-duel-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/double-duel-q-learning.png"&gt;&lt;img src="output-agent/double-duel-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="12"&gt;
&lt;li&gt;Duel Recurrent Q-learning agent, &lt;a href="agent/12.duel-recurrent-q-learning-agent.ipynb"&gt;duel-recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/duel-recurrent-q-learning.png"&gt;&lt;img src="output-agent/duel-recurrent-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="13"&gt;
&lt;li&gt;Double Duel Recurrent Q-learning agent, &lt;a href="agent/13.double-duel-recurrent-q-learning-agent.ipynb"&gt;double-duel-recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/double-duel-recurrent-q-learning.png"&gt;&lt;img src="output-agent/double-duel-recurrent-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="14"&gt;
&lt;li&gt;Actor-critic agent, &lt;a href="agent/14.actor-critic-agent.ipynb"&gt;actor-critic-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/actor-critic.png"&gt;&lt;img src="output-agent/actor-critic.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="15"&gt;
&lt;li&gt;Actor-critic Duel agent, &lt;a href="agent/14.actor-critic-duel-agent.ipynb"&gt;actor-critic-duel-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/actor-critic-duel.png"&gt;&lt;img src="output-agent/actor-critic-duel.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="16"&gt;
&lt;li&gt;Actor-critic Recurrent agent, &lt;a href="agent/16.actor-critic-recurrent-agent.ipynb"&gt;actor-critic-recurrent-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/actor-critic-recurrent.png"&gt;&lt;img src="output-agent/actor-critic-recurrent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="17"&gt;
&lt;li&gt;Actor-critic Duel Recurrent agent, &lt;a href="agent/17.actor-critic-duel-recurrent-agent.ipynb"&gt;actor-critic-duel-recurrent-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/actor-critic-duel-recurrent.png"&gt;&lt;img src="output-agent/actor-critic-duel-recurrent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="18"&gt;
&lt;li&gt;Curiosity Q-learning agent, &lt;a href="agent/18.curiosity-q-learning-agent.ipynb"&gt;curiosity-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/curiosity-q-learning.png"&gt;&lt;img src="output-agent/curiosity-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="19"&gt;
&lt;li&gt;Recurrent Curiosity Q-learning agent, &lt;a href="agent/19.recurrent-curiosity-q-learning-agent.ipynb"&gt;recurrent-curiosity-q-learning.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/recurrent-curiosity-q-learning.png"&gt;&lt;img src="output-agent/recurrent-curiosity-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="20"&gt;
&lt;li&gt;Duel Curiosity Q-learning agent, &lt;a href="agent/20.duel-curiosity-q-learning-agent.ipynb"&gt;duel-curiosity-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/duel-curiosity-q-learning.png"&gt;&lt;img src="output-agent/duel-curiosity-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="21"&gt;
&lt;li&gt;Neuro-evolution agent, &lt;a href="agent/21.neuro-evolution-agent.ipynb"&gt;neuro-evolution.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/neuro-evolution.png"&gt;&lt;img src="output-agent/neuro-evolution.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="22"&gt;
&lt;li&gt;Neuro-evolution with Novelty search agent, &lt;a href="agent/22.neuro-evolution-novelty-search-agent.ipynb"&gt;neuro-evolution-novelty-search.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/neuro-evolution-novelty-search.png"&gt;&lt;img src="output-agent/neuro-evolution-novelty-search.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="23"&gt;
&lt;li&gt;ABCD strategy agent, &lt;a href="agent/23.abcd-strategy-agent.ipynb"&gt;abcd-strategy.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/abcd-strategy.png"&gt;&lt;img src="output-agent/abcd-strategy.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-results-signal-prediction" class="anchor" aria-hidden="true" href="#results-signal-prediction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results signal prediction&lt;/h3&gt;
&lt;p&gt;I will cut the dataset to train and test datasets,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Train dataset derived from starting timestamp until last 30 days&lt;/li&gt;
&lt;li&gt;Test dataset derived from last 30 days until end of the dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So we will let the model do forecasting based on last 30 days, and we will going to repeat the experiment for 10 times. You can increase it locally if you want, and tuning parameters will help you by a lot.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;LSTM, accuracy 95.693%, time taken for 1 epoch 01:09&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/lstm.png"&gt;&lt;img src="output/lstm.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;LSTM Bidirectional, accuracy 93.8%, time taken for 1 epoch 01:40&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/bidirectional-lstm.png"&gt;&lt;img src="output/bidirectional-lstm.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;LSTM 2-Path, accuracy 94.63%, time taken for 1 epoch 01:39&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/lstm-2path.png"&gt;&lt;img src="output/lstm-2path.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;GRU, accuracy 94.63%, time taken for 1 epoch 02:10&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/gru.png"&gt;&lt;img src="output/gru.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="5"&gt;
&lt;li&gt;GRU Bidirectional, accuracy 92.5673%, time taken for 1 epoch 01:40&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/bidirectional-gru.png"&gt;&lt;img src="output/bidirectional-gru.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="6"&gt;
&lt;li&gt;GRU 2-Path, accuracy 93.2117%, time taken for 1 epoch 01:39&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/gru-2path.png"&gt;&lt;img src="output/gru-2path.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="7"&gt;
&lt;li&gt;Vanilla, accuracy 91.4686%, time taken for 1 epoch 00:52&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/vanilla.png"&gt;&lt;img src="output/vanilla.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="8"&gt;
&lt;li&gt;Vanilla Bidirectional, accuracy 88.9927%, time taken for 1 epoch 01:06&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/bidirectional-vanilla.png"&gt;&lt;img src="output/bidirectional-vanilla.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="9"&gt;
&lt;li&gt;Vanilla 2-Path, accuracy 91.5406%, time taken for 1 epoch 01:08&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/vanilla-2path.png"&gt;&lt;img src="output/vanilla-2path.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="10"&gt;
&lt;li&gt;LSTM Seq2seq, accuracy 94.9817%, time taken for 1 epoch 01:36&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/lstm-seq2seq.png"&gt;&lt;img src="output/lstm-seq2seq.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="11"&gt;
&lt;li&gt;LSTM Bidirectional Seq2seq, accuracy 94.517%, time taken for 1 epoch 02:30&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/bidirectional-lstm-seq2seq.png"&gt;&lt;img src="output/bidirectional-lstm-seq2seq.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="12"&gt;
&lt;li&gt;LSTM Seq2seq VAE, accuracy 95.4190%, time taken for 1 epoch 01:48&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/lstm-seq2seq-vae.png"&gt;&lt;img src="output/lstm-seq2seq-vae.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="13"&gt;
&lt;li&gt;GRU Seq2seq, accuracy 90.8854%, time taken for 1 epoch 01:34&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/gru-seq2seq.png"&gt;&lt;img src="output/gru-seq2seq.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="14"&gt;
&lt;li&gt;GRU Bidirectional Seq2seq, accuracy 67.9915%, time taken for 1 epoch 02:30&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/bidirectional-gru-seq2seq.png"&gt;&lt;img src="output/bidirectional-gru-seq2seq.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="15"&gt;
&lt;li&gt;GRU Seq2seq VAE, accuracy 89.1321%, time taken for 1 epoch 01:48&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/gru-seq2seq-vae.png"&gt;&lt;img src="output/gru-seq2seq-vae.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="16"&gt;
&lt;li&gt;Attention-is-all-you-Need, accuracy 94.2482%, time taken for 1 epoch 01:41&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/attention-is-all-you-need.png"&gt;&lt;img src="output/attention-is-all-you-need.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="17"&gt;
&lt;li&gt;CNN-Seq2seq, accuracy 90.74%, time taken for 1 epoch 00:43&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/cnn-seq2seq.png"&gt;&lt;img src="output/cnn-seq2seq.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="18"&gt;
&lt;li&gt;Dilated-CNN-Seq2seq, accuracy 95.86%, time taken for 1 epoch 00:14&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/dilated-cnn-seq2seq.png"&gt;&lt;img src="output/dilated-cnn-seq2seq.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How to forecast,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/how-to-forecast.png"&gt;&lt;img src="output/how-to-forecast.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Sentiment consensus,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/sentiment-consensus.png"&gt;&lt;img src="output/sentiment-consensus.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-results-analysis" class="anchor" aria-hidden="true" href="#results-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results analysis&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Outliers study using K-means, SVM, and Gaussian on TESLA stock&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="misc/outliers.png"&gt;&lt;img src="misc/outliers.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Overbought-Oversold study on TESLA stock&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="misc/overbought-oversold.png"&gt;&lt;img src="misc/overbought-oversold.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Which stock you need to buy?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="misc/which-stock.png"&gt;&lt;img src="misc/which-stock.png" width="40%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-results-simulation" class="anchor" aria-hidden="true" href="#results-simulation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results simulation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Simple Monte Carlo&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="simulation/monte-carlo-simple.png"&gt;&lt;img src="simulation/monte-carlo-simple.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Dynamic volatity Monte Carlo&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="simulation/monte-carlo-dynamic-volatility.png"&gt;&lt;img src="simulation/monte-carlo-dynamic-volatility.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Drift Monte Carlo&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="simulation/monte-carlo-drift.png"&gt;&lt;img src="simulation/monte-carlo-drift.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Multivariate Drift Monte Carlo BTC/USDT with Bitcurate sentiment&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="simulation/multivariate-drift-monte-carlo.png"&gt;&lt;img src="simulation/multivariate-drift-monte-carlo.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Portfolio optimization&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="simulation/portfolio-optimization.png"&gt;&lt;img src="simulation/portfolio-optimization.png" width="40%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>huseinzol05</author><guid isPermaLink="false">https://github.com/huseinzol05/Stock-Prediction-Models</guid><pubDate>Fri, 03 Jan 2020 00:10:00 GMT</pubDate></item><item><title>iamtrask/Grokking-Deep-Learning #11 in Jupyter Notebook, This month</title><link>https://github.com/iamtrask/Grokking-Deep-Learning</link><description>&lt;p&gt;&lt;i&gt;this repository accompanies the book "Grokking Deep Learning"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-grokking-deep-learning" class="anchor" aria-hidden="true" href="#grokking-deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grokking-Deep-Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://floydhub.com/run" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2208430861b539afa7c55a34278f45d4aed96d0b/68747470733a2f2f7374617469632e666c6f79646875622e636f6d2f627574746f6e2f627574746f6e2d736d616c6c2e737667" alt="Run on FloydHub" data-canonical-src="https://static.floydhub.com/button/button-small.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository accompanies the book "Grokking Deep Learning", &lt;a href="https://manning.com/books/grokking-deep-learning?a_aid=grokkingdl&amp;amp;a_bid=32715258" title="Grokking Deep Learning" rel="nofollow"&gt;available here&lt;/a&gt;. Also, the coupon code "trask40" is good for a 40% discount.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter3%20-%20%20Forward%20Propagation%20-%20Intro%20to%20Neural%20Prediction.ipynb"&gt;Chapter 3 - Forward Propagation - Intro to Neural Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter4%20-%20Gradient%20Descent%20-%20Intro%20to%20Neural%20Learning.ipynb"&gt;Chapter 4 - Gradient Descent - Into to Neural Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter5%20-%20Generalizing%20Gradient%20Descent%20-%20Learning%20Multiple%20Weights%20at%20a%20Time.ipynb"&gt;Chapter 5 - Generalizing Gradient Descent - Learning Multiple Weights at a Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter6%20-%20Intro%20to%20Backpropagation%20-%20Building%20Your%20First%20DEEP%20Neural%20Network.ipynb"&gt;Chapter 6 - Intro to Backpropagation - Building your first DEEP Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter8%20-%20Intro%20to%20Regularization%20-%20Learning%20Signal%20and%20Ignoring%20Noise.ipynb"&gt;Chapter 8 - Intro to Regularization - Learning Signal and Ignoring Noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter9%20-%20Intro%20to%20Activation%20Functions%20-%20Modeling%20Probabilities.ipynb"&gt;Chapter 9 - Intro to Activation Functions - Learning to Model Probabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter10%20-%20Intro%20to%20Convolutional%20Neural%20Networks%20-%20Learning%20Edges%20and%20Corners.ipynb"&gt;Chapter 10 - Intro to Convolutional Neural Networks - Learning Edges and Corners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter11%20-%20Intro%20to%20Word%20Embeddings%20-%20Neural%20Networks%20that%20Understand%20Language.ipynb"&gt;Chapter 11 - Intro to Word Embeddings - Neural Networks which Understand Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter12%20-%20Intro%20to%20Recurrence%20-%20Predicting%20the%20Next%20Word.ipynb"&gt;Chapter 12 - Intro to Recurrence (RNNs) - Predicting the Next Word&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter13%20-%20Intro%20to%20Automatic%20Differentiation%20-%20Let's%20Build%20A%20Deep%20Learning%20Framework.ipynb"&gt;Chapter 13 - Intro to Automatic Differentiation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter14%20-%20Exploding%20Gradients%20Examples.ipynb"&gt;Chapter 14 - Exploding Gradients Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter14%20-%20Intro%20to%20LSTMs%20-%20Learn%20to%20Write%20Like%20Shakespeare.ipynb"&gt;Chapter 14 - Intro to LSTMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter14%20-%20Intro%20to%20LSTMs%20-%20Part%202%20-%20Learn%20to%20Write%20Like%20Shakespeare.ipynb"&gt;Chapter 14 - Intro to LSTMs - Part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/iamtrask/Grokking-Deep-Learning/blob/master/Chapter15%20-%20Intro%20to%20Federated%20Learning%20-%20Deep%20Learning%20on%20Unseen%20Data.ipynb"&gt;Chapter 15 - Intro to Federated Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>iamtrask</author><guid isPermaLink="false">https://github.com/iamtrask/Grokking-Deep-Learning</guid><pubDate>Fri, 03 Jan 2020 00:11:00 GMT</pubDate></item><item><title>virgili0/Virgilio #12 in Jupyter Notebook, This month</title><link>https://github.com/virgili0/Virgilio</link><description>&lt;p&gt;&lt;i&gt;Your new Mentor for Data Science E-Learning.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-virgilio" class="anchor" aria-hidden="true" href="#virgilio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;Virgilio&lt;/em&gt;&lt;/h1&gt;
&lt;h4&gt;&lt;a id="user-content-your-new-mentor-for-data-science-e-learning" class="anchor" aria-hidden="true" href="#your-new-mentor-for-data-science-e-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Your new Mentor for Data Science E-Learning.&lt;/h4&gt;
&lt;p&gt;Join our community:   &lt;a href="https://www.facebook.com/groups/mathfordatascience/?notif_id=1576071669338330" rel="nofollow"&gt;&lt;img height="24px" src="https://camo.githubusercontent.com/707de57448c727d1086eef95ab883c780b749931/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f66616365626f6f6b2d34354b25323067726f75702532306d656d626572732d626c75652e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/facebook-45K%20group%20members-blue.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/UpQ8bb7" rel="nofollow"&gt;&lt;img height="24px" src="https://camo.githubusercontent.com/9b0e0cc54a5a222ada2528d8b05b933153c66c7b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646973636f72642d35302532306f6e6c696e6525323075736572732d677265656e2e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/discord-50%20online%20users-green.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSeVJ9N7ae8Wr07tfSuHkP5i_5Fa-4Lp5V4fBevsinWyx6t17g/viewform" rel="nofollow"&gt;&lt;img height="24px" src="https://camo.githubusercontent.com/b10f25e6bbf851f7ea83234d92f249103ee6dfae/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e6577736c65747465722d35303025323073756273637269626572732d79656c6c6f772e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/newsletter-500%20subscribers-yellow.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;blockquote&gt;
&lt;p&gt;Virgilio is an open source initiative, aiming to mentor and guide anyone in the world of Data Science and Machine Learning. Our vision is to give &lt;em&gt;everyone&lt;/em&gt; the chance to get involved in this field, get self-started as a practitioner, gain new cutting edge practical skills and learn to navigate through the infinite web of resources and find the ones useful for &lt;em&gt;you&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e652d41e2104dadcc8c6b2d209e00ee6f1caa190/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f632f63652f56697267696c5f2e6a7067"&gt;&lt;img width="480px" src="https://camo.githubusercontent.com/e652d41e2104dadcc8c6b2d209e00ee6f1caa190/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f632f63652f56697267696c5f2e6a7067" data-canonical-src="https://upload.wikimedia.org/wikipedia/commons/c/ce/Virgil_.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
&lt;h3&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-is-virgilio"&gt;&lt;em&gt;What&lt;/em&gt; is Virgilio&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#structure"&gt;Structure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#paradiso"&gt;Paradiso&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#purgatorio"&gt;Purgatorio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#inferno"&gt;Inferno&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#complete-learning-paths"&gt;Complete Learning Paths&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#About"&gt;About&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contribute"&gt;Contribute&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-what-is-virgilio" class="anchor" aria-hidden="true" href="#what-is-virgilio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is Virgilio?&lt;/h1&gt;
&lt;p&gt;Studying and reading through the Internet means swimming in an infinite jungle of chaotic information, even more so in rapidly changing innovative fields.&lt;/p&gt;
&lt;p&gt;Have you ever felt overwhelmed when trying to approach Data Science without a real “path” to follow?
Are you tired of clicking “Run”, “Run”, “Run”.. on a Jupyter Notebook, with that false sense of confidence given by the comfort zone of the work of others?&lt;/p&gt;
&lt;p&gt;Have you ever got confused because of the several and contradicting names for the same algorithm or approach, from different websites and fragmented tutorials?&lt;/p&gt;
&lt;p&gt;We will address these critical issues for free, for everyone.&lt;/p&gt;
&lt;p&gt;Hi, I'm &lt;a href="https://en.wikipedia.org/wiki/Virgil" rel="nofollow"&gt;Virgilio&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Like I did with &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Dante_Alighieri" rel="nofollow"&gt;Dante&lt;/a&gt;&lt;/em&gt;, just some centuries ago, I'll be your mentor and reference point during your journey through this &lt;em&gt;selva oscura&lt;/em&gt;, providing you complete and organic learning paths for several fields, tools, skills and more.&lt;/p&gt;
&lt;p&gt;The vision of Virgilio is to give everyone the possibility to get into the incredible world of the Data Science and Machine Learning and the business and creative possibilities that they offer, to get self-started as a practitioner, gain new cutting edge practical skills or just learn to discriminate good information from poor information.&lt;/p&gt;
&lt;p&gt;We are doing this by providing only high-quality and coherent content, with clear step-by-step paths and a consistent naming system.&lt;/p&gt;
&lt;p&gt;Imagine Virgilio as an E-Mentor who will tell you what do to get the next step, the next skill, or to apply them in practice to create value.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But what does it mean in practice?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In other words, what's the &lt;strong&gt;target&lt;/strong&gt; of the Virgilio project?&lt;/p&gt;
&lt;p&gt;There are different scenarios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;student&lt;/em&gt; from a different field who wants to explore the intersections and the possibilities offered by the Machine Learning and Statistical methods.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;curious individual&lt;/em&gt; who came in touch with one of the buzzwords related to these fields and wants to discriminate between reliable and unreliable information.&lt;/li&gt;
&lt;li&gt;An &lt;em&gt;experienced practitioner&lt;/em&gt; who wants to have a reference point for the latest techniques, papers and best practices.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;manager&lt;/em&gt; who wants to understand the possibilities of ML applied to their actual problems, like integration with production systems or new solutions from scratch.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;businessman&lt;/em&gt; who wants to understand if his data are suitable for an ML project, and what could be the real business value.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;salesman&lt;/em&gt; who needs to stay up-to-date with the latest technologies and Jargon.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-structure" class="anchor" aria-hidden="true" href="#structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Structure&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;WIP notice: we are in the progress of migrating the content from the old conceptual organisation to the new one. Please be patient while we make Virgilio more awesome!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="map.PNG"&gt;&lt;img src="map.PNG" alt="Figure 1" title="1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As you can imagine it’s not easy to intercept all these different needs, so our solution ended up in a hierarchical structure which distinguish the content based on different levels of abstraction.&lt;/p&gt;
&lt;p&gt;To do this, we got our inspiration from Dante’s amazing masterpiece &lt;a href="https://en.wikipedia.org/wiki/Divina_Commedia" rel="nofollow"&gt;“La Divina Commedia”&lt;/a&gt;, written in over 20 years of work and published for the first time in 1323 d.c.&lt;/p&gt;
&lt;p&gt;Dante’s journey wouldn’t have been possible without his companion and guide, Virgilio, a roman famous poet (70 a.c.) who inspired generations of artists since the Roman hegemony in Europe.&lt;/p&gt;
&lt;p&gt;In his journey, Dante travel across the different levels of the catholic conception of the divine world at that time, starting from the Inferno (the prison of the damned), passing through the Purgatorio and reaching eventually the Paradiso (you can call it Valhalla or Nirvana, as you prefer :-) ).&lt;/p&gt;
&lt;p&gt;In your journey, you will start from scratch and eventually reach the theoretical knowledge and solid expendable skill.&lt;/p&gt;
&lt;p&gt;The parallelism is natural:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="order.png"&gt;&lt;img src="order.png" alt="Figure 2" title="1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Going from top to bottom increases the level of detail and decreases the level of abstraction&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;Paradiso&lt;/strong&gt; you won’t find a single line of code or a math formula, just plain English.
Here’s the place for introductions, simple explanations, demystifications, and meta-guides (for example a guide about the best way to use Virgilio).
It’s the best place for the not-techies, beginners and literally everyone who wants to get in touch with Data Science and Machine Learning without getting bored into technical details.
Do you want to communicate these innovative fields? Pass from the Paradiso!&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;Purgatorio&lt;/strong&gt; you can find technical guides for beginners (in the field or in general IT).
For example, you’ll find guides about Python, maths, and statistics.
You will find guides about study techniques, soft skills and you’ll learn how to develop an analytical mindset too.
It’s an obliged step before the Hell.
Depending on your starting skill, you’ll probably spend here most of the time, learning to code, understand math concepts and more!
&lt;em&gt;If you’re a complete beginner&lt;/em&gt;, follow the track we proposed, starting from the Fundamentals.&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;Inferno&lt;/strong&gt; you won’t find gentle introductions or generic explanations, but a lot of different detailed guides, topics, hands-on tutorials and more!
You’ll find an entire section dedicated to research and daily updates from the field! You’ll find guides like “how to train a massive neural network over hundreds of GPUs efficiently” or “How to deal with huge datasets”, or “how to fine-tune a preprocessing pipeline”. Think about Inferno as the place where you will pick up the sub-field you prefer and dive into that.
It’s impossible to learn everything at once! One of the Virgilio’s most important learning strategies is “One enemy at time”, "Divide and conquer!".
The three specializations that we provide are Computer Vision, Natural Language Processing, and Agent-based and Reinforcement Learning.
These learning paths are the “final bosses” of the Virgilio’s experience: once you’ll have completed them you will hopefully be skilled enough to land an internship or tackle real business problems!&lt;/p&gt;
&lt;p&gt;Above these you’ll find a plenty of other useful zones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Business Skills: these are necessary to unlock the business value in the real world, and they are probably the most valuable asset for a “data guy” to have in his pocket.&lt;/li&gt;
&lt;li&gt;Tools: here you find guides about useful tools for programming, or scientific computing in general.&lt;/li&gt;
&lt;li&gt;Research: here you will learn how to find the right papers and digest them. In addition, you’ll discover which teams to follow for your interests.&lt;/li&gt;
&lt;li&gt;Massive computation: here you’ll find hints and resources for computing on clusters, optimize your system and other advanced topics.&lt;/li&gt;
&lt;li&gt;ML for Business applications: here we provide additional resources to select a sector and see what are the today’s available techniques for the problems of your interest.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-paradiso" class="anchor" aria-hidden="true" href="#paradiso"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Paradiso&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/paradiso/demystification-ai-ml-dl/demystification-ai-ml-dl.md"&gt;Demystification of the key concepts of Artificial Intelligence and Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/paradiso/what-do-you-need-for-ml/what-do-you-need-for-ml.md"&gt;What do you need for ML? &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/paradiso/do-you-really-need-ml/do-you-really-need-ml.md"&gt;Do you really need ML?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/paradiso/use-cases/use-cases.md"&gt;ML use cases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/paradiso/virgilio-teaching-strategy/virgilio-teaching-strategy.md"&gt;Virgilio's Teaching Strategy - Learning to Learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/paradiso/introduction-to-ml/introduction-to-ml.md"&gt;Introduction to ML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-purgatorio" class="anchor" aria-hidden="true" href="#purgatorio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Purgatorio&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fundamentals
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/fundamentals/math-fundamentals/math-fundamentals.md"&gt;Math Fundamentals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/fundamentals/statistics-fundamentals/statistics-fundamentals.md"&gt;Statistics Fundamentals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/fundamentals/python-fundamentals/python-fundamentals.md"&gt;Python Fundamentals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/fundamentals/jupyter-notebook/jupyter-notebook.md"&gt;Jupyter Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/fundamentals/the-data-science-process/the-data-science-process.md"&gt;The Data Science Process&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Define The Scope and Ask Questions
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/define-the-scope-and-ask-questions/frame-the-problem/frame-the-problem.md"&gt;Frame The Problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/define-the-scope-and-ask-questions/usage-and-integration/usage-and-integration.md"&gt;Usage and Integration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/define-the-scope-and-ask-questions/starting-a-data-project/starting-a-data-project.md"&gt;Starting a Data Project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/define-the-scope-and-ask-questions/workspace-setup-and-cloud-computing/workspace-setup-and-cloud-computing.md"&gt;WorkSpace Setup and Cloud Computing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Collect and Prepare Data
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/collect-and-prepare-data/data-preparation/data-preparation.md"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/collect-and-prepare-data/data-visualization/data-visualization.md"&gt;Data Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Select and Train Machine Learning Models
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/purgatorio/select-and-train-machine-learning-models/machine-learning/machine-learning.md"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Evaluate and Fine-Tune&lt;/li&gt;
&lt;li&gt;Launch and Mantain the System&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-inferno" class="anchor" aria-hidden="true" href="#inferno"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inferno&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Computer Vision
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/inferno/computer-vision/introduction-to-computer-vision/introduction-to-computer-vision.ipynb"&gt;Introduction to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/inferno/computer-vision/object-instance-segmentation/object-instance-segmentation.ipynb"&gt;Object Instance Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/inferno/computer-vision/object-tracking/object-tracking.ipynb"&gt;Object Tracking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Natural Language Processing&lt;/li&gt;
&lt;li&gt;Virtual Assistants
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/inferno/virtual-assistants/dialogflow-chatbot/dialogflow-chatbot.md"&gt;Build a Virtual Assistant with DialogFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Soft Skills
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/inferno/soft-skills/impactful-presentations/impactful-presentations.md"&gt;Impactful Presentations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tools
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/inferno/tools/geo-gebra/geo-gebra.md"&gt;Geo Gebra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/inferno/tools/latex/latex.md"&gt;Latex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/inferno/tools/regex/regex.ipynb"&gt;Regex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/inferno/tools/wolfram-alpha/wolfram-alpha.md"&gt;Wolfram Alpha&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Research
&lt;ul&gt;
&lt;li&gt;&lt;a href="serving/inferno/research/zotero/zotero.md"&gt;Zotero&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="serving/inferno/research/sota-papers/sota-papers.md"&gt;State-of-Art Papers Explained&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Massive Computation&lt;/li&gt;
&lt;li&gt;Machine Learning for Business Applications&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-complete-learning-paths" class="anchor" aria-hidden="true" href="#complete-learning-paths"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete Learning Paths&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="LearningPaths/Machine%20Learning%20Engineer%20Career%20Path"&gt;Machine Learning Study Path&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;Virgilio is developed and mantained by &lt;a href="docs/contributors.md"&gt;these awesome people&lt;/a&gt;.
You can email us &lt;code&gt;virgilio.datascience (at) gmail.com&lt;/code&gt; or join the &lt;a href="https://discord.gg/UpQ8bb7" rel="nofollow"&gt;Discord chat&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contribute" class="anchor" aria-hidden="true" href="#contribute"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribute&lt;/h3&gt;
&lt;p&gt;That's awesome! Check the &lt;a href="https://github.com/virgili0/Virgilio/blob/master/docs/contributing.md"&gt;contribution guidelines&lt;/a&gt; and get involved in our project!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;The project is licensed under the &lt;a href="LICENSE.md"&gt;GPLv3 terms&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>virgili0</author><guid isPermaLink="false">https://github.com/virgili0/Virgilio</guid><pubDate>Fri, 03 Jan 2020 00:12:00 GMT</pubDate></item><item><title>udacity/deep-learning-v2-pytorch #13 in Jupyter Notebook, This month</title><link>https://github.com/udacity/deep-learning-v2-pytorch</link><description>&lt;p&gt;&lt;i&gt;Projects and exercises for the latest Deep Learning ND program https://www.udacity.com/course/deep-learning-nanodegree--nd101&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-pytorch" class="anchor" aria-hidden="true" href="#deep-learning-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning (PyTorch)&lt;/h1&gt;
&lt;p&gt;This repository contains material related to Udacity's &lt;a href="https://www.udacity.com/course/deep-learning-nanodegree--nd101" rel="nofollow"&gt;Deep Learning Nanodegree program&lt;/a&gt;. It consists of a bunch of tutorial notebooks for various deep learning topics. In most cases, the notebooks lead you through implementing models such as convolutional networks, recurrent networks, and GANs. There are other topics covered such as weight initialization and batch normalization.&lt;/p&gt;
&lt;p&gt;There are also notebooks used as projects for the Nanodegree program. In the program itself, the projects are reviewed by real people (Udacity reviewers), but the starting code is available here, as well.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table Of Contents&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-neural-networks" class="anchor" aria-hidden="true" href="#introduction-to-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-neural-networks"&gt;Introduction to Neural Networks&lt;/a&gt;: Learn how to implement gradient descent and apply it to predicting patterns in student admissions data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-analysis-network"&gt;Sentiment Analysis with NumPy&lt;/a&gt;: &lt;a href="http://iamtrask.github.io/" rel="nofollow"&gt;Andrew Trask&lt;/a&gt; leads you through building a sentiment analysis model, predicting if some text is positive or negative.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch"&gt;Introduction to PyTorch&lt;/a&gt;: Learn how to build neural networks in PyTorch and use pre-trained networks for state-of-the-art image classifiers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/convolutional-neural-networks"&gt;Convolutional Neural Networks&lt;/a&gt;: Visualize the output of layers that make up a CNN. Learn how to define and train a CNN for classifying &lt;a href="https://en.wikipedia.org/wiki/MNIST_database" rel="nofollow"&gt;MNIST data&lt;/a&gt;, a handwritten digit database that is notorious in the fields of machine and deep learning. Also, define and train a CNN for classifying images in the &lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow"&gt;CIFAR10 dataset&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/transfer-learning"&gt;Transfer Learning&lt;/a&gt;. In practice, most people don't train their own networks on huge datasets; they use &lt;strong&gt;pre-trained&lt;/strong&gt; networks such as VGGnet. Here you'll use VGGnet to help classify images of flowers without training an end-to-end network from scratch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/weight-initialization"&gt;Weight Initialization&lt;/a&gt;: Explore how initializing network weights affects performance.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/autoencoder"&gt;Autoencoders&lt;/a&gt;: Build models for image compression and de-noising, using feedforward and convolutional networks in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/style-transfer"&gt;Style Transfer&lt;/a&gt;: Extract style and content features from images, using a pre-trained network. Implement style transfer according to the paper, &lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="nofollow"&gt;Image Style Transfer Using Convolutional Neural Networks&lt;/a&gt; by Gatys et. al. Define appropriate losses for iteratively creating a target, style-transferred image of your own design!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-recurrent-neural-networks" class="anchor" aria-hidden="true" href="#recurrent-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/recurrent-neural-networks"&gt;Intro to Recurrent Networks (Time series &amp;amp; Character-level RNN)&lt;/a&gt;: Recurrent neural networks are able to use information about the sequence of data, such as the sequence of characters in text; learn how to implement these in PyTorch for a variety of tasks.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/word2vec-embeddings"&gt;Embeddings (Word2Vec)&lt;/a&gt;: Implement the Word2Vec model to find semantic representations of words for use in natural language processing.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-rnn"&gt;Sentiment Analysis RNN&lt;/a&gt;: Implement a recurrent neural network that can predict if the text of a moview review is positive or negative.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/attention"&gt;Attention&lt;/a&gt;: Implement attention and apply it to annotation vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-generative-adversarial-networks" class="anchor" aria-hidden="true" href="#generative-adversarial-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/gan-mnist"&gt;Generative Adversarial Network on MNIST&lt;/a&gt;: Train a simple generative adversarial network on the MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/batch-norm"&gt;Batch Normalization&lt;/a&gt;: Learn how to improve training rates and network stability with batch normalizations.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/dcgan-svhn"&gt;Deep Convolutional GAN (DCGAN)&lt;/a&gt;: Implement a DCGAN to generate new images based on the Street View House Numbers (SVHN) dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/cycle-gan"&gt;CycleGAN&lt;/a&gt;: Implement a CycleGAN that is designed to learn from unpaired and unlabeled data; use trained generators to transform images from summer to winter and vice versa.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deploying-a-model-with-aws-sagemaker" class="anchor" aria-hidden="true" href="#deploying-a-model-with-aws-sagemaker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deploying a Model (with AWS SageMaker)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/sagemaker-deployment"&gt;All exercise and project notebooks&lt;/a&gt; for the lessons on model deployment can be found in the linked, Github repo. Learn to deploy pre-trained models using AWS SageMaker.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-projects" class="anchor" aria-hidden="true" href="#projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-bikesharing"&gt;Predicting Bike-Sharing Patterns&lt;/a&gt;: Implement a neural network in NumPy to predict bike rentals.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-dog-classification"&gt;Dog Breed Classifier&lt;/a&gt;: Build a convolutional neural network with PyTorch to classify any image (even an image of a face) as a specific dog breed.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-tv-script-generation"&gt;TV Script Generation&lt;/a&gt;: Train a recurrent neural network to generate scripts in the style of dialogue from Seinfeld.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-face-generation"&gt;Face Generation&lt;/a&gt;: Use a DCGAN on the CelebA dataset to generate images of new and realistic human faces.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-elective-material" class="anchor" aria-hidden="true" href="#elective-material"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Elective Material&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/tensorflow/intro-to-tensorflow"&gt;Intro to TensorFlow&lt;/a&gt;: Starting building neural networks with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/keras"&gt;Keras&lt;/a&gt;: Learn to build neural networks and convolutional neural networks with Keras.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-configure-and-manage-your-environment-with-anaconda" class="anchor" aria-hidden="true" href="#configure-and-manage-your-environment-with-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configure and Manage Your Environment with Anaconda&lt;/h2&gt;
&lt;p&gt;Per the Anaconda &lt;a href="http://conda.pydata.org/docs" rel="nofollow"&gt;docs&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Conda is an open source package management system and environment management system
for installing multiple versions of software packages and their dependencies and
switching easily between them. It works on Linux, OS X and Windows, and was created
for Python programs but can package and distribute any software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h2&gt;
&lt;p&gt;Using Anaconda consists of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt; on your computer, by selecting the latest Python version for your operating system. If you already have &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;miniconda&lt;/code&gt; installed, you should be able to skip this step and move on to step 2.&lt;/li&gt;
&lt;li&gt;Create and activate * a new &lt;code&gt;conda&lt;/code&gt; &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;environment&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;* Each time you wish to work on any exercises, activate your &lt;code&gt;conda&lt;/code&gt; environment!&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-1-installation" class="anchor" aria-hidden="true" href="#1-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; the latest version of &lt;code&gt;miniconda&lt;/code&gt; that matches your system.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Linux&lt;/th&gt;
&lt;th&gt;Mac&lt;/th&gt;
&lt;th&gt;Windows&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;64-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86_64.exe" rel="nofollow"&gt;64-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;32-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86.sh" rel="nofollow"&gt;32-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86.exe" rel="nofollow"&gt;32-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt; &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;miniconda&lt;/a&gt; on your machine. Detailed instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-2-create-and-activate-the-environment" class="anchor" aria-hidden="true" href="#2-create-and-activate-the-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Create and Activate the Environment&lt;/h2&gt;
&lt;p&gt;For Windows users, these following commands need to be executed from the &lt;strong&gt;Anaconda prompt&lt;/strong&gt; as opposed to a Windows terminal window. For Mac, a normal terminal window will work.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-git-and-version-control" class="anchor" aria-hidden="true" href="#git-and-version-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Git and version control&lt;/h4&gt;
&lt;p&gt;These instructions also assume you have &lt;code&gt;git&lt;/code&gt; installed for working with Github from a terminal window, but if you do not, you can download that first with the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you'd like to learn more about version control and using &lt;code&gt;git&lt;/code&gt; from the command line, take a look at our &lt;a href="https://www.udacity.com/course/version-control-with-git--ud123" rel="nofollow"&gt;free course: Version Control with Git&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now, we're ready to create our local environment!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clone the repository, and navigate to the downloaded folder. This may take a minute or two to clone due to the included image data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/udacity/deep-learning-v2-pytorch.git
cd deep-learning-v2-pytorch
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;
&lt;p&gt;Create (and activate) a new environment, named &lt;code&gt;deep-learning&lt;/code&gt; with Python 3.6. If prompted to proceed with the install &lt;code&gt;(Proceed [y]/n)&lt;/code&gt; type y.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create -n deep-learning python=3.6
source activate deep-learning
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create --name deep-learning python=3.6
activate deep-learning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point your command line should look something like: &lt;code&gt;(deep-learning) &amp;lt;User&amp;gt;:deep-learning-v2-pytorch &amp;lt;user&amp;gt;$&lt;/code&gt;. The &lt;code&gt;(deep-learning)&lt;/code&gt; indicates that your environment has been activated, and you can proceed with further package installations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install PyTorch and torchvision; this should install the latest version of PyTorch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision -c pytorch 
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch -c pytorch
pip install torchvision
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install a few required pip packages, which are specified in the requirements text file (including OpenCV).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="7"&gt;
&lt;li&gt;That's it!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now most of the &lt;code&gt;deep-learning&lt;/code&gt; libraries are available to you. Very occasionally, you will see a repository with an addition requirements file, which exists should you want to use TensorFlow and Keras, for example. In this case, you're encouraged to install another library to your existing environment, or create a new environment for a specific project.&lt;/p&gt;
&lt;p&gt;Now, assuming your &lt;code&gt;deep-learning&lt;/code&gt; environment is still activated, you can navigate to the main repo and start looking at the notebooks:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd
cd deep-learning-v2-pytorch
jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To exit the environment when you have completed your work session, simply close the terminal window.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>udacity</author><guid isPermaLink="false">https://github.com/udacity/deep-learning-v2-pytorch</guid><pubDate>Fri, 03 Jan 2020 00:13:00 GMT</pubDate></item><item><title>rasbt/deeplearning-models #14 in Jupyter Notebook, This month</title><link>https://github.com/rasbt/deeplearning-models</link><description>&lt;p&gt;&lt;i&gt;A collection of various deep learning architectures, models, and tips&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667" alt="Python 3.7" data-canonical-src="https://img.shields.io/badge/Python-3.7-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-deep-learning-models" class="anchor" aria-hidden="true" href="#deep-learning-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning Models&lt;/h1&gt;
&lt;p&gt;A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-traditional-machine-learning" class="anchor" aria-hidden="true" href="#traditional-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Traditional Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Perceptron&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/perceptron.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/perceptron.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/perceptron.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/perceptron.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Logistic Regression&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/logistic-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/logistic-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/logistic-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/logistic-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Softmax Regression (Multinomial Logistic Regression)&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/softmax-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/softmax-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/softmax-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/softmax-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Softmax Regression with MLxtend's plot_decision_regions on Iris&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/softmax-regression-mlxtend-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/softmax-regression-mlxtend-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-multilayer-perceptrons" class="anchor" aria-hidden="true" href="#multilayer-perceptrons"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multilayer Perceptrons&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Multilayer Perceptron&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Dropout&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-dropout.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-dropout.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-dropout.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-dropout.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Batch Normalization&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Backpropagation from Scratch&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-lowlevel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-lowlevel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-fromscratch__sigmoid-mse.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-fromscratch__sigmoid-mse.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-basic" class="anchor" aria-hidden="true" href="#basic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/cnn/cnn-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/cnn/cnn-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network with He Initialization&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-he-init.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-he-init.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-concepts" class="anchor" aria-hidden="true" href="#concepts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Concepts&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Replacing Fully-Connnected by Equivalent Convolutional Layers&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/fc-to-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/fc-to-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-fully-convolutional" class="anchor" aria-hidden="true" href="#fully-convolutional"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fully Convolutional&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Fully Convolutional Neural Network&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-allconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-allconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-lenet" class="anchor" aria-hidden="true" href="#lenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LeNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;LeNet-5 on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LeNet-5 on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LeNet-5 on QuickDraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-alexnet" class="anchor" aria-hidden="true" href="#alexnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AlexNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;AlexNet on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-vgg" class="anchor" aria-hidden="true" href="#vgg"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VGG&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network VGG-16&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/cnn/cnn-vgg16.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/cnn/cnn-vgg16.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;VGG-16 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network VGG-19&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg19.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg19.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-densenet" class="anchor" aria-hidden="true" href="#densenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DenseNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DenseNet-121 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-densenet121-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-densenet121-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;DenseNet-121 Image Classifier Trained on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-densenet121-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-densenet121-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-resnet" class="anchor" aria-hidden="true" href="#resnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ResNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ResNet and Residual Blocks&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/resnet-ex-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/resnet-ex-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-18 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-18 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Object Classifier Trained on QuickDraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-50 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet50-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet50-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-50 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-101 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet101-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-101 Trained on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-152 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet152-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet152-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-network-in-network" class="anchor" aria-hidden="true" href="#network-in-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network in Network&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Network in Network CIFAR-10 Classifier&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-normalization-layers" class="anchor" aria-hidden="true" href="#normalization-layers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Normalization Layers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;BatchNorm before and after Activation for Network-in-Network CIFAR-10 Classifier&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10_batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10_batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Filter Response Normalization for Network-in-Network CIFAR-10 Classifier&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10_filter-response-norm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10_filter-response-norm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-metric-learning" class="anchor" aria-hidden="true" href="#metric-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Metric Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Siamese Network with Multilayer Perceptrons&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/metric/siamese-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/metric/siamese-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-autoencoders" class="anchor" aria-hidden="true" href="#autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Autoencoders&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-fully-connected-autoencoders" class="anchor" aria-hidden="true" href="#fully-connected-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fully-connected Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Autoencoder (MNIST)&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Autoencoder (MNIST) + Scikit-Learn Random Forest Classifier&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-basic-with-rf.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-basic-with-rf.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-convolutional-autoencoders" class="anchor" aria-hidden="true" href="#convolutional-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions / Transposed Convolutions&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-deconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-deconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions and Continuous Jaccard Distance&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv-jaccard.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv-jaccard.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions (without pooling operations)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv-nopool.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv-nopool.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-conv-nneighbor.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-conv-nneighbor.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation -- Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation -- Trained on Quickdraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor-quickdraw-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor-quickdraw-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-variational-autoencoders" class="anchor" aria-hidden="true" href="#variational-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Variational Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Variational Autoencoder&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-var.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-var.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Variational Autoencoder&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-var.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-var.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-conditional-variational-autoencoders" class="anchor" aria-hidden="true" href="#conditional-variational-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conditional Variational Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Conditional Variational Autoencoder (with labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cvae.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cvae.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Conditional Variational Autoencoder (without labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cvae_no-out-concat.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cvae_no-out-concat.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Conditional Variational Autoencoder (with labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cnn-cvae.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cnn-cvae.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Conditional Variational Autoencoder (without labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cnn-cvae_no-out-concat.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cnn-cvae_no-out-concat.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-generative-adversarial-networks-gans" class="anchor" aria-hidden="true" href="#generative-adversarial-networks-gans"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks (GANs)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fully Connected GAN on MNIST&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Fully Connected Wasserstein GAN on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/wgan-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/wgan-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional GAN on MNIST&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional GAN on MNIST with Label Smoothing&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan-conv-smoothing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan-conv-smoothing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan-conv-smoothing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan-conv-smoothing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Wasserstein GAN on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/dc-wgan-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/dc-wgan-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-graph-neural-networks-gnns" class="anchor" aria-hidden="true" href="#graph-neural-networks-gnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Graph Neural Networks (GNNs)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most Basic Graph Neural Network with Gaussian Filter on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Basic Graph Neural Network with Edge Prediction on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-edge-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-edge-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Basic Graph Neural Network with Spectral Graph Convolution on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-graph-spectral-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-graph-spectral-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-recurrent-neural-networks-rnns" class="anchor" aria-hidden="true" href="#recurrent-neural-networks-rnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks (RNNs)&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-many-to-one-sentiment-analysis--classification" class="anchor" aria-hidden="true" href="#many-to-one-sentiment-analysis--classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Many-to-one: Sentiment Analysis / Classification&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple single-layer RNN (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_simple_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_simple_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A simple single-layer RNN with packed sequences to ignore padding characters (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_simple_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_simple_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells (IMDB) and pre-trained GloVe word vectors&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells and Own Dataset in CSV Format (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with GRU cells (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer bi-directional RNN (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (AG News)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_agnews.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_agnews.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (Yelp Review Polarity)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_yelp-polarity.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_yelp-polarity.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (Amazon Review Polarity)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_amazon-polarity.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_amazon-polarity.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-many-to-many--sequence-to-sequence" class="anchor" aria-hidden="true" href="#many-to-many--sequence-to-sequence"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Many-to-Many / Sequence-to-Sequence&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple character RNN to generate new text (Charles Dickens)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ordinal-regression" class="anchor" aria-hidden="true" href="#ordinal-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ordinal Regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ordinal Regression CNN -- CORAL w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-coral-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-coral-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ordinal Regression CNN -- Niu et al. 2016 w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-niu-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-niu-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ordinal Regression CNN -- Beckham and Pal 2016 w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-beckham2016-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-beckham2016-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tips-and-tricks" class="anchor" aria-hidden="true" href="#tips-and-tricks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tips and Tricks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cyclical Learning Rate&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/cyclical-learning-rate.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/cyclical-learning-rate.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Annealing with Increasing the Batch Size (w. CIFAR-10 &amp;amp; AlexNet)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/cnn-alexnet-cifar10-batchincrease.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/cnn-alexnet-cifar10-batchincrease.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Gradient Clipping (w. MLP on MNIST)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/gradclipping_mlp.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/gradclipping_mlp.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-transfer-learning" class="anchor" aria-hidden="true" href="#transfer-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transfer Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Transfer Learning Example (VGG16 pre-trained on ImageNet for Cifar-10)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;   [PyTorch: &lt;a href="pytorch_ipynb/transfer/transferlearning-vgg16-cifar10-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/transfer/transferlearning-vgg16-cifar10-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pytorch-workflows-and-mechanics" class="anchor" aria-hidden="true" href="#pytorch-workflows-and-mechanics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Workflows and Mechanics&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-custom-datasets" class="anchor" aria-hidden="true" href="#custom-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Custom Data Loader Example for PNG Files&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-dataloader-png/custom-dataloader-example.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-dataloader-png/custom-dataloader-example.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- CSV files converted to HDF5&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Face Images from CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from Quickdraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from the Street View House Number (SVHN) Dataset&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Asian Face Dataset (AFAD)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Dating Historical Color Images&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-and-preprocessing" class="anchor" aria-hidden="true" href="#training-and-preprocessing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Preprocessing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Generating Validation Set Splits&lt;br&gt;
[PyTorch]: &lt;a href="pytorch_ipynb/mechanics/validation-splits.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/validation-splits.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Dataloading with Pinned Memory&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-cifar10-pinmem.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-cifar10-pinmem.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Standardizing Images&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-standardized.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-standardized.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Image Transformation Examples&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/torchvision-transform-examples.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/torchvision-transform-examples.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Char-RNN with Own Text File&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sentiment Classification RNN with Own CSV File&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-parallel-computing" class="anchor" aria-hidden="true" href="#parallel-computing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Parallel Computing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Using Multiple GPUs with DataParallel -- VGG-16 Gender Classifier on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-other" class="anchor" aria-hidden="true" href="#other"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Sequential API and hooks&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/mlp-sequential.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/mlp-sequential.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Weight Sharing Within a Layer&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/cnn-weight-sharing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/cnn-weight-sharing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Plotting Live Training Performance in Jupyter Notebooks with just Matplotlib&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/plot-jupyter-matplotlib.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/plot-jupyter-matplotlib.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-autograd" class="anchor" aria-hidden="true" href="#autograd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Autograd&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Getting Gradients of an Intermediate Variable in PyTorch&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/manual-gradients.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/manual-gradients.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tensorflow-workflows-and-mechanics" class="anchor" aria-hidden="true" href="#tensorflow-workflows-and-mechanics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Workflows and Mechanics&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-custom-datasets-1" class="anchor" aria-hidden="true" href="#custom-datasets-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Chunking an Image Dataset for Minibatch Training using NumPy NPZ Archives&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Storing an Image Dataset for Minibatch Training using HDF5&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using Input Pipelines to Read Data from TFRecords Files&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/tfrecords.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/tfrecords.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using Queue Runners to Feed Images Directly from Disk&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/file-queues.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/file-queues.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using TensorFlow's Dataset API&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/dataset-api.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/dataset-api.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-and-preprocessing-1" class="anchor" aria-hidden="true" href="#training-and-preprocessing-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Preprocessing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Saving and Loading Trained Models -- from TensorFlow Checkpoint Files and NumPy NPZ Archives&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/saving-and-reloading-models.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/saving-and-reloading-models.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rasbt</author><guid isPermaLink="false">https://github.com/rasbt/deeplearning-models</guid><pubDate>Fri, 03 Jan 2020 00:14:00 GMT</pubDate></item><item><title>chenyuntc/pytorch-book #15 in Jupyter Notebook, This month</title><link>https://github.com/chenyuntc/pytorch-book</link><description>&lt;p&gt;&lt;i&gt;PyTorch tutorials and fun projects including neural talk, neural style, poem writing, anime generation (《深度学习框架PyTorch：入门与实战》)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-english-version" class="anchor" aria-hidden="true" href="#english-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="README_EN.md"&gt;English Version&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;这是书籍《深度学习框架PyTorch：入门与实践》的对应代码，但是也可以作为一个独立的PyTorch入门指南和教程。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-更新说明" class="anchor" aria-hidden="true" href="#更新说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新说明&lt;/h2&gt;
&lt;p&gt;Working on migration to Pytorch 1.0, stay tuned!&lt;/p&gt;
&lt;p&gt;当前版本的代码是基于pytorch 1.0.1， 如果想使用旧版的 请 &lt;code&gt;git checkout v0.4&lt;/code&gt; 或者 &lt;code&gt;git checkout v0.3&lt;/code&gt;。旧版代码有更好的python2/python3 兼容，CPU/GPU兼容测试。 新版的代码未经过完整测试，已在GPU和python3 下测试通过。但是理论上在python2和CPU上不应该有太多的问题。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-内容" class="anchor" aria-hidden="true" href="#内容"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;内容&lt;/h2&gt;
&lt;p&gt;该书（教程/仓库）的内容如图所示：
&lt;a target="_blank" rel="noopener noreferrer" href="imgs/mindmap.png"&gt;&lt;img src="imgs/mindmap.png" alt="思维导图" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;可以看出本教程可以分为两部分：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基础部分&lt;/strong&gt;（前五章）讲解PyTorch内容，这部份介绍了PyTorch中主要的的模块，和深度学习中常用的一些工具。对于这部分内容，这里利用Jupyter Notebook作为教学工具，读者可以结合notebook修改运行，反复实验。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第二章介绍如何安装PyTorch和配置学习环境。同时提供了一个快速入门教程，基于官方的教程简化并更新内容，读者可以花费大约1到2小时的时间快速完成入门任务，而后根据需求再选择深入阅读后续相关章节的内容。&lt;/li&gt;
&lt;li&gt;第三章介绍了PyTorch中多维数组Tensor和动态图autograd/Variable的使用，并配以例子，让读者分别使用Tensor和autograd实现线性回归，比较二者的不同点。除了介绍这二者的基础使用之外，本章还对Tensor的底层设计，以及autograd的计算图原理进行比较深入分析，希望能使得读者能对这些底层知识有更全面的掌握。&lt;/li&gt;
&lt;li&gt;第四章介绍了PyTorch中神经网络模块nn的基础用法，同时讲解了神经网络中“层”，“损失函数”，“优化器”等，最后带领读者用不到50行的代码搭建出曾夺得ImageNet冠军的ResNet。&lt;/li&gt;
&lt;li&gt;第五章介绍了PyTorch中数据加载，GPU加速，持久化和可视化等相关工具。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实战部分&lt;/strong&gt;（第六到十章）利用PyTorch实现了几个酷炫有趣的应用，对于这部分的内容，本仓库给出完整的实现代码，并提供预训练好的模型作为demo，供读者测试。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第六章是承上启下的一章，这一章的目标不是教会读者新函数，新知识，而是结合Kaggle中一个经典的比赛，实现一个深度学习中比较简单的图像二分类问题。在实现过程中，带领读者复习前五章的知识，并提出代码规范以合理的组织程序，代码，使得程序更加可读，可维护。第六章还介绍了在PyTorch中如何进行debug。&lt;/li&gt;
&lt;li&gt;第七章为读者讲解了当前最火爆的生成对抗网络（GAN），带领读者从头实现一个动漫头像生成器，能够利用GAN生成风格多变的动漫头像。&lt;/li&gt;
&lt;li&gt;第八章为读者讲解了风格迁移的相关知识，并带领读者实现风格迁移网络，将自己的照片变成高大上的名画。&lt;/li&gt;
&lt;li&gt;第九章为读者讲解了一些自然语言处理的基础知识，并讲解了CharRNN的原理。而后利用收集了几万首唐诗，训练出了一个可以自动写诗歌的小程序。这个小程序可以控制生成诗歌的&lt;strong&gt;格式&lt;/strong&gt;，&lt;strong&gt;意境&lt;/strong&gt;，还能生成&lt;strong&gt;藏头诗&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;第十章为读者介绍了图像描述任务，并以最新的AI Challenger比赛的数据为例，带领读者实现了一个可以进行简单图像描述的的小程序。&lt;/li&gt;
&lt;li&gt;第十一章（&lt;strong&gt;新增，实验性&lt;/strong&gt;） 由&lt;a href="https://github.com/Diamondfan"&gt;Diamondfan&lt;/a&gt; 编写的语音识别。完善了本项目（本项目已囊括图像，文本，语音三大领域的例子）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Notebook中的文字描述内容属于本书的初稿，有描述不通顺，错别字之处还请谅解&lt;/strong&gt;。本打算删除notebook中描述的内容，只留下代码，但为了方便读者阅读学习，最终还是决定留下。 我会抽空根据书中内容逐字校对这部分内容，但并不对此并不提供具体时间点。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-是否需要买书" class="anchor" aria-hidden="true" href="#是否需要买书"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;是否需要买书&lt;/h2&gt;
&lt;p&gt;书&lt;strong&gt;不是必要的&lt;/strong&gt;，这个仓库包含书中50%以上的文字内容，90%以上的代码，尤其是前几章入门内容，几乎是完全保留了书中的讲解内容。读者即使不买书也能正常使用本教程。&lt;/p&gt;
&lt;p&gt;&lt;del&gt;如果你觉得纸质书的优势吸引你，不妨小破费一笔，支持一下作者这大半年来的工作。同时为了尽可能的方便读者，笔者还专门开通腾讯云的服务，用以保存教程中用到的部分模型，预处理的数据和部分大文件。&lt;/del&gt;
书中的部分内容已经过时，以此仓库内容为准。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-代码说明" class="anchor" aria-hidden="true" href="#代码说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;代码说明&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;代码主要在python3下测试得到最终结果，python2暂未测试。v0.2和v0.3 分支的代码同时经过严格测试支持python2/python3&lt;/li&gt;
&lt;li&gt;实战部分代码同时在GPU和CPU环境下测试通过&lt;/li&gt;
&lt;li&gt;代码已更新兼容到PyTorch &lt;code&gt;0.4.1&lt;/code&gt;, 后续会考虑兼容 &lt;code&gt;v1.0&lt;/code&gt;，但暂无确切时间点。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果你想在PyTorch 0.2.0或0.3下运行,请&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git checkout v0.2 # v0.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果有任何不当，或者有待改进的地方，欢迎读者开issue讨论，或者提交pull request。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-环境配置" class="anchor" aria-hidden="true" href="#环境配置"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;环境配置&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;安装&lt;a href="http://pytorch.org" rel="nofollow"&gt;PyTorch&lt;/a&gt;，请从官网选择指定的版本安装即可，一键安装（即使你使用anaconda，也建议使用pip）。更多的安装方式请参阅书中说明。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;克隆仓库&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;git clone https:&lt;span class="pl-k"&gt;//&lt;/span&gt;github.com&lt;span class="pl-k"&gt;/&lt;/span&gt;chenyuntc&lt;span class="pl-k"&gt;/&lt;/span&gt;PyTorch&lt;span class="pl-k"&gt;-&lt;/span&gt;book.git&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装第三方依赖包&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;cd pytorch&lt;span class="pl-k"&gt;-&lt;/span&gt;book &lt;span class="pl-ii"&gt;&amp;amp;&amp;amp;&lt;/span&gt; pip install &lt;span class="pl-k"&gt;-&lt;/span&gt;r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-visdom打不开及其解决方案" class="anchor" aria-hidden="true" href="#visdom打不开及其解决方案"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visdom打不开及其解决方案&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;新版的visdom已经解决了这个问题,只需要升级即可&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install --upgrade visdom
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;之前的&lt;a href="https://github.com/chenyuntc/pytorch-book/blob/2c8366137b691aaa8fbeeea478cc1611c09e15f5/README.md#visdom%E6%89%93%E4%B8%8D%E5%BC%80%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"&gt;解决方案&lt;/a&gt; 不再需要，已删除。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-_" class="anchor" aria-hidden="true" href="#_"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;^_^&lt;/h2&gt;
&lt;p&gt;有任何bug，解释不清楚的地方或者是困惑，欢迎开issue&lt;/p&gt;
&lt;p&gt;欢迎pull requests&lt;/p&gt;
&lt;p&gt;Happy Coding!&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0376580818bbc47cd4b2f29ab6ca684122ba6e9f/687474703a2f2f696d6731342e333630627579696d672e636f6d2f6e312f6a66732f7431333333392f33322f323436333733303139382f3231373438332f65383134386336622f35613431323737644e62643134373063312e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/0376580818bbc47cd4b2f29ab6ca684122ba6e9f/687474703a2f2f696d6731342e333630627579696d672e636f6d2f6e312f6a66732f7431333333392f33322f323436333733303139382f3231373438332f65383134386336622f35613431323737644e62643134373063312e6a7067" alt="" data-canonical-src="http://img14.360buyimg.com/n1/jfs/t13339/32/2463730198/217483/e8148c6b/5a41277dNbd1470c1.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://search.jd.com/Search?keyword=pytorch%20%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5&amp;amp;enc=utf-8&amp;amp;wq=pytorch%20%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5&amp;amp;pvid=8b0d91d7108845ad8cbaf596326f3eb3" rel="nofollow"&gt;京东购买链接&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://search.dangdang.com/?key=pytorch%20%C8%EB%C3%C5%D3%EB%CA%B5%BC%F9&amp;amp;act=input" rel="nofollow"&gt;当当购买链接&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>chenyuntc</author><guid isPermaLink="false">https://github.com/chenyuntc/pytorch-book</guid><pubDate>Fri, 03 Jan 2020 00:15:00 GMT</pubDate></item><item><title>wesm/pydata-book #16 in Jupyter Notebook, This month</title><link>https://github.com/wesm/pydata-book</link><description>&lt;p&gt;&lt;i&gt;Materials and IPython notebooks for "Python for Data Analysis" by Wes McKinney, published by O'Reilly Media&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python-for-data-analysis-2nd-edition" class="anchor" aria-hidden="true" href="#python-for-data-analysis-2nd-edition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python for Data Analysis, 2nd Edition&lt;/h1&gt;
&lt;p&gt;Materials and IPython notebooks for "Python for Data Analysis" by Wes McKinney,
published by O'Reilly Media&lt;/p&gt;
&lt;p&gt;&lt;a href="http://amzn.to/2vvBijB" rel="nofollow"&gt;Buy the book on Amazon&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notebooks.azure.com/import/gh/wesm/pydata-book" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c33d8af3d101ffcd6ea73a8d02290b8d829ac52/68747470733a2f2f6e6f7465626f6f6b732e617a7572652e636f6d2f6c61756e63682e706e67" data-canonical-src="https://notebooks.azure.com/launch.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Follow Wes on Twitter: &lt;a href="https://twitter.com/wesmckinn" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e5ee948ef48fbfa31f868c9dbd301bbd5cf38097/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f7765736d636b696e6e2e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/wesmckinn.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-1st-edition-readers" class="anchor" aria-hidden="true" href="#1st-edition-readers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1st Edition Readers&lt;/h1&gt;
&lt;p&gt;If you are reading the &lt;a href="http://amzn.to/2vvBijB" rel="nofollow"&gt;1st Edition&lt;/a&gt; (published in 2012), please find the
reorganized book materials on the &lt;a href="https://github.com/wesm/pydata-book/tree/1st-edition"&gt;&lt;code&gt;1st-edition&lt;/code&gt; branch&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/BrambleXu/pydata-notebook"&gt;Chinese&lt;/a&gt; by Xu Liang&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ipython-notebooks" class="anchor" aria-hidden="true" href="#ipython-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;IPython Notebooks:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch02.ipynb" rel="nofollow"&gt;Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch03.ipynb" rel="nofollow"&gt;Chapter 3: Built-in Data Structures, Functions, and Files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch04.ipynb" rel="nofollow"&gt;Chapter 4: NumPy Basics: Arrays and Vectorized Computation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch05.ipynb" rel="nofollow"&gt;Chapter 5: Getting Started with pandas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch06.ipynb" rel="nofollow"&gt;Chapter 6: Data Loading, Storage, and File Formats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch07.ipynb" rel="nofollow"&gt;Chapter 7: Data Cleaning and Preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch08.ipynb" rel="nofollow"&gt;Chapter 8: Data Wrangling: Join, Combine, and Reshape&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch09.ipynb" rel="nofollow"&gt;Chapter 9: Plotting and Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch10.ipynb" rel="nofollow"&gt;Chapter 10: Data Aggregation and Group Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch11.ipynb" rel="nofollow"&gt;Chapter 11: Time Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch12.ipynb" rel="nofollow"&gt;Chapter 12: Advanced pandas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch13.ipynb" rel="nofollow"&gt;Chapter 13: Introduction to Modeling Libraries in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch14.ipynb" rel="nofollow"&gt;Chapter 14: Data Analysis Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/appa.ipynb" rel="nofollow"&gt;Appendix A: Advanced NumPy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-code" class="anchor" aria-hidden="true" href="#code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code&lt;/h3&gt;
&lt;p&gt;The code in this repository, including all code samples in the notebooks listed
above, is released under the &lt;a href="LICENSE-CODE"&gt;MIT license&lt;/a&gt;. Read more at the
&lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>wesm</author><guid isPermaLink="false">https://github.com/wesm/pydata-book</guid><pubDate>Fri, 03 Jan 2020 00:16:00 GMT</pubDate></item><item><title>xianhu/LearnPython #17 in Jupyter Notebook, This month</title><link>https://github.com/xianhu/LearnPython</link><description>&lt;p&gt;&lt;i&gt;以撸代码的形式学习Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-learnpython" class="anchor" aria-hidden="true" href="#learnpython"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LearnPython&lt;/h1&gt;
&lt;p&gt;以撸代码的形式学习Python, 具体说明在&lt;a href="https://zhuanlan.zhihu.com/pythoner" rel="nofollow"&gt;知乎专栏-撸代码,学知识&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;===================================================================================================&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python_basepy-千行代码入门python" class="anchor" aria-hidden="true" href="#python_basepy-千行代码入门python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_base.py: 千行代码入门Python&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_visualpy-15张图入门matplotlib" class="anchor" aria-hidden="true" href="#python_visualpy-15张图入门matplotlib"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_visual.py: 15张图入门Matplotlib&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_visual_animationpy-使用matplotlib画动态图实例" class="anchor" aria-hidden="true" href="#python_visual_animationpy-使用matplotlib画动态图实例"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_visual_animation.py: 使用Matplotlib画动态图实例&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_spiderpy-一个很水的python爬虫入门代码文件" class="anchor" aria-hidden="true" href="#python_spiderpy-一个很水的python爬虫入门代码文件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_spider.py: 一个很“水”的Python爬虫入门代码文件&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_weibopy-史上最详细的python模拟登录新浪微博流程" class="anchor" aria-hidden="true" href="#python_weibopy-史上最详细的python模拟登录新浪微博流程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_weibo.py: “史上最详细”的Python模拟登录新浪微博流程&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_ldapy-玩点高级的--带你入门topic模型lda小改进附源码" class="anchor" aria-hidden="true" href="#python_ldapy-玩点高级的--带你入门topic模型lda小改进附源码"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_lda.py: 玩点高级的--带你入门Topic模型LDA（小改进+附源码）&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_sqlalchemypy-作为一个pythoner-不会sqlalchemy都不好意思跟同行打招呼" class="anchor" aria-hidden="true" href="#python_sqlalchemypy-作为一个pythoner-不会sqlalchemy都不好意思跟同行打招呼"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_sqlalchemy.py: 作为一个Pythoner, 不会SQLAlchemy都不好意思跟同行打招呼！&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_onelinepy-几个小例子告诉你-一行python代码能干哪些事" class="anchor" aria-hidden="true" href="#python_onelinepy-几个小例子告诉你-一行python代码能干哪些事"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_oneline.py: 几个小例子告诉你, 一行Python代码能干哪些事&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_requestspy-python中最好用的爬虫库requests代码实例" class="anchor" aria-hidden="true" href="#python_requestspy-python中最好用的爬虫库requests代码实例"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_requests.py: Python中最好用的爬虫库Requests代码实例&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_functionalpy-python进阶-函数式编程实例附代码" class="anchor" aria-hidden="true" href="#python_functionalpy-python进阶-函数式编程实例附代码"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_functional.py: Python进阶: 函数式编程实例（附代码）&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_decoratorpy-python进阶-通过实例详解装饰器附代码" class="anchor" aria-hidden="true" href="#python_decoratorpy-python进阶-通过实例详解装饰器附代码"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_decorator.py: Python进阶: 通过实例详解装饰器（附代码）&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_datetimepy-你真的了解python中的日期时间处理吗" class="anchor" aria-hidden="true" href="#python_datetimepy-你真的了解python中的日期时间处理吗"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_datetime.py: 你真的了解Python中的日期时间处理吗？&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_metaclasspy-python进阶-一步步理解python中的元类metaclass" class="anchor" aria-hidden="true" href="#python_metaclasspy-python进阶-一步步理解python中的元类metaclass"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_metaclass.py: Python进阶: 一步步理解Python中的元类metaclass&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_coroutinepy-python进阶-理解python中的异步io和协程coroutine-并应用在爬虫中" class="anchor" aria-hidden="true" href="#python_coroutinepy-python进阶-理解python中的异步io和协程coroutine-并应用在爬虫中"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_coroutine.py: Python进阶: 理解Python中的异步IO和协程(Coroutine), 并应用在爬虫中&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_aiohttppy-python中最好用的异步爬虫库aiohttp代码实例" class="anchor" aria-hidden="true" href="#python_aiohttppy-python中最好用的异步爬虫库aiohttp代码实例"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_aiohttp.py: Python中最好用的异步爬虫库Aiohttp代码实例&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_thread_multiprocesspy-python进阶-聊聊io密集型任务计算密集型任务以及多线程多进程" class="anchor" aria-hidden="true" href="#python_thread_multiprocesspy-python进阶-聊聊io密集型任务计算密集型任务以及多线程多进程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_thread_multiprocess.py: Python进阶: 聊聊IO密集型任务、计算密集型任务，以及多线程、多进程&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_version36py-python36正式版要来了-你期待哪些新特性" class="anchor" aria-hidden="true" href="#python_version36py-python36正式版要来了-你期待哪些新特性"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_version36.py: Python3.6正式版要来了, 你期待哪些新特性？&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_magic_methods-python进阶-实例讲解python中的魔法函数magic-methods" class="anchor" aria-hidden="true" href="#python_magic_methods-python进阶-实例讲解python中的魔法函数magic-methods"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_magic_methods: Python进阶: 实例讲解Python中的魔法函数(Magic Methods)&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_restful_apipy-利用python和flask快速开发restful-api" class="anchor" aria-hidden="true" href="#python_restful_apipy-利用python和flask快速开发restful-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_restful_api.py: 利用Python和Flask快速开发RESTful API&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_restful_apipy-restful-api进阶-连接数据库添加参数token认证返回代码说明等" class="anchor" aria-hidden="true" href="#python_restful_apipy-restful-api进阶-连接数据库添加参数token认证返回代码说明等"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_restful_api.py: RESTful API进阶: 连接数据库、添加参数、Token认证、返回代码说明等&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_contextpy-with语句和上下文管理器contextmanager" class="anchor" aria-hidden="true" href="#python_contextpy-with语句和上下文管理器contextmanager"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_context.py: With语句和上下文管理器ContextManager&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_flaskpy-flask相关说明" class="anchor" aria-hidden="true" href="#python_flaskpy-flask相关说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_flask.py: Flask相关说明&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-myshow-玩点好玩的--知乎全部话题关系可视化" class="anchor" aria-hidden="true" href="#myshow-玩点好玩的--知乎全部话题关系可视化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MyShow: 玩点好玩的--知乎全部话题关系可视化&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_markov_chainpy-玩点好玩的--使用马尔可夫模型自动生成文章" class="anchor" aria-hidden="true" href="#python_markov_chainpy-玩点好玩的--使用马尔可夫模型自动生成文章"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_markov_chain.py: 玩点好玩的--使用马尔可夫模型自动生成文章&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_wechatpy-玩点好玩的--自己写一个微信小助手" class="anchor" aria-hidden="true" href="#python_wechatpy-玩点好玩的--自己写一个微信小助手"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_wechat.py: 玩点好玩的--自己写一个微信小助手&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_csvpy-python中csv文件的简单读写" class="anchor" aria-hidden="true" href="#python_csvpy-python中csv文件的简单读写"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_csv.py: Python中CSV文件的简单读写&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_numpypy-使用numpy进行矩阵操作" class="anchor" aria-hidden="true" href="#python_numpypy-使用numpy进行矩阵操作"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_numpy.py: 使用numpy进行矩阵操作&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_mailpy-使用python自动发送邮件包括发送html以及图片附件等" class="anchor" aria-hidden="true" href="#python_mailpy-使用python自动发送邮件包括发送html以及图片附件等"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_mail.py: 使用Python自动发送邮件，包括发送HTML以及图片、附件等&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_redispy-python操作redis实现消息的发布与订阅" class="anchor" aria-hidden="true" href="#python_redispy-python操作redis实现消息的发布与订阅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_redis.py: Python操作Redis实现消息的发布与订阅&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_schedulepy-python进行调度开发" class="anchor" aria-hidden="true" href="#python_schedulepy-python进行调度开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_schedule.py: Python进行调度开发&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-python_socketpy-python的socket开发实例" class="anchor" aria-hidden="true" href="#python_socketpy-python的socket开发实例"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python_socket.py: Python的socket开发实例&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-plotly目录-一些plotly画图的实例使用jupyter-notebook编写" class="anchor" aria-hidden="true" href="#plotly目录-一些plotly画图的实例使用jupyter-notebook编写"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Plotly目录: 一些plotly画图的实例，使用jupyter notebook编写&lt;/h3&gt;
&lt;p&gt;===================================================================================================&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-您可以fork该项目-并在修改后提交pull-request-看到后会尽量进行代码合并" class="anchor" aria-hidden="true" href="#您可以fork该项目-并在修改后提交pull-request-看到后会尽量进行代码合并"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;您可以fork该项目, 并在修改后提交Pull request, 看到后会尽量进行代码合并&lt;/h3&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>xianhu</author><guid isPermaLink="false">https://github.com/xianhu/LearnPython</guid><pubDate>Fri, 03 Jan 2020 00:17:00 GMT</pubDate></item><item><title>fastai/course-nlp #18 in Jupyter Notebook, This month</title><link>https://github.com/fastai/course-nlp</link><description>&lt;p&gt;&lt;i&gt;A Code-First Introduction to NLP course&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-a-code-first-intro-to-natural-language-processing" class="anchor" aria-hidden="true" href="#a-code-first-intro-to-natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A Code-First Intro to Natural Language Processing&lt;/h1&gt;
&lt;p&gt;You can find out about the course in &lt;a href="https://www.fast.ai/2019/07/08/fastai-nlp/" rel="nofollow"&gt;this blog post&lt;/a&gt; and all &lt;a href="https://www.youtube.com/playlist?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9" rel="nofollow"&gt;lecture videos are available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This course was originally taught in the &lt;a href="https://www.usfca.edu/arts-sciences/graduate-programs/analytics" rel="nofollow"&gt;University of San Francisco's Masters of Science in Data Science&lt;/a&gt; program, summer 2019.  The course is taught in Python with Jupyter Notebooks, using libraries such as sklearn, nltk, pytorch, and fastai.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;p&gt;The following topics will be covered:&lt;/p&gt;
&lt;p&gt;1. What is NLP?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A changing field&lt;/li&gt;
&lt;li&gt;Resources&lt;/li&gt;
&lt;li&gt;Tools&lt;/li&gt;
&lt;li&gt;Python libraries&lt;/li&gt;
&lt;li&gt;Example applications&lt;/li&gt;
&lt;li&gt;Ethics issues&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2. Topic Modeling with NMF and SVD&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stop words, stemming, &amp;amp; lemmatization&lt;/li&gt;
&lt;li&gt;Term-document matrix&lt;/li&gt;
&lt;li&gt;Topic Frequency-Inverse Document Frequency (TF-IDF)&lt;/li&gt;
&lt;li&gt;Singular Value Decomposition (SVD)&lt;/li&gt;
&lt;li&gt;Non-negative Matrix Factorization (NMF)&lt;/li&gt;
&lt;li&gt;Truncated SVD, Randomized SVD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3. Sentiment classification with Naive Bayes, Logistic regression, and ngrams&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sparse matrix storage&lt;/li&gt;
&lt;li&gt;Counters&lt;/li&gt;
&lt;li&gt;the fastai library&lt;/li&gt;
&lt;li&gt;Naive Bayes&lt;/li&gt;
&lt;li&gt;Logistic regression&lt;/li&gt;
&lt;li&gt;Ngrams&lt;/li&gt;
&lt;li&gt;Logistic regression with Naive Bayes features, with trigrams&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;4. Regex (and re-visiting tokenization)&lt;/p&gt;
&lt;p&gt;5. Language modeling &amp;amp; sentiment classification with deep learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Language model&lt;/li&gt;
&lt;li&gt;Transfer learning&lt;/li&gt;
&lt;li&gt;Sentiment classification&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;6. Translation with RNNs&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Review Embeddings&lt;/li&gt;
&lt;li&gt;Bleu metric&lt;/li&gt;
&lt;li&gt;Teacher Forcing&lt;/li&gt;
&lt;li&gt;Bidirectional&lt;/li&gt;
&lt;li&gt;Attention&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;7. Translation with the Transformer architecture&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer Model&lt;/li&gt;
&lt;li&gt;Multi-head attention&lt;/li&gt;
&lt;li&gt;Masking&lt;/li&gt;
&lt;li&gt;Label smoothing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;8. Bias &amp;amp; ethics in NLP&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bias in word embeddings&lt;/li&gt;
&lt;li&gt;types of bias&lt;/li&gt;
&lt;li&gt;attention economy&lt;/li&gt;
&lt;li&gt;drowning in fraudulent/fake info&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-why-is-this-course-taught-in-a-weird-order" class="anchor" aria-hidden="true" href="#why-is-this-course-taught-in-a-weird-order"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why is this course taught in a weird order?&lt;/h2&gt;
&lt;p&gt;This course is structured with a &lt;em&gt;top-down&lt;/em&gt; teaching method, which is different from how most math courses operate.  Typically, in a &lt;em&gt;bottom-up&lt;/em&gt; approach, you first learn all the separate components you will be using, and then you gradually build them up into more complex structures.  The problems with this are that students often lose motivation, don't have a sense of the "big picture", and don't know what they'll need.&lt;/p&gt;
&lt;p&gt;Harvard Professor David Perkins has a book, &lt;a href="https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719" rel="nofollow"&gt;Making Learning Whole&lt;/a&gt; in which he uses baseball as an analogy.  We don't require kids to memorize all the rules of baseball and understand all the technical details before we let them play the game.  Rather, they start playing with a just general sense of it, and then gradually learn more rules/details as time goes on.&lt;/p&gt;
&lt;p&gt;If you took the fast.ai deep learning course, that is what we used.  You can hear more about my teaching philosophy &lt;a href="http://www.fast.ai/2016/10/08/teaching-philosophy/" rel="nofollow"&gt;in this blog post&lt;/a&gt; or &lt;a href="https://vimeo.com/214233053" rel="nofollow"&gt;this talk I gave at the San Francisco Machine Learning meetup&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All that to say, don't worry if you don't understand everything at first!  You're not supposed to.  We will start using some "black boxes" and then we'll dig into the lower level details later.&lt;/p&gt;
&lt;p&gt;To start, focus on what things DO, not what they ARE.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fastai</author><guid isPermaLink="false">https://github.com/fastai/course-nlp</guid><pubDate>Fri, 03 Jan 2020 00:18:00 GMT</pubDate></item><item><title>AtsushiSakai/PythonRobotics #19 in Jupyter Notebook, This month</title><link>https://github.com/AtsushiSakai/PythonRobotics</link><description>&lt;p&gt;&lt;i&gt;Python sample codes for robotics algorithms.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true" align="right" width="300" alt="header pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pythonrobotics" class="anchor" aria-hidden="true" href="#pythonrobotics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PythonRobotics&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/AtsushiSakai/PythonRobotics" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/58f87d5d3604646322c28abd8c5a9b2faa05fa51/68747470733a2f2f7472617669732d63692e6f72672f4174737573686953616b61692f507974686f6e526f626f746963732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/AtsushiSakai/PythonRobotics.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pythonrobotics.readthedocs.io/en/latest/?badge=latest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a60f894ef011c8a7e648348c16aabfdfb603613a/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f707974686f6e726f626f746963732f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/pythonrobotics/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2e66a00c9dcf7ecc1f24189c6055aa7e6da233dc/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f73623237396b787576316265333931673f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/AtsushiSakai/PythonRobotics?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c26144817eba34b4ee9f9a6aee913e6b466218b/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4174737573686953616b61692f507974686f6e526f626f746963732f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/AtsushiSakai/PythonRobotics/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://lgtm.com/projects/g/AtsushiSakai/PythonRobotics/context:python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4c3af4cd47bb2ea2c71cac274f1f7dd392eea893/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f672f4174737573686953616b61692f507974686f6e526f626f746963732e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Language grade: Python" data-canonical-src="https://img.shields.io/lgtm/grade/python/g/AtsushiSakai/PythonRobotics.svg?logo=lgtm&amp;amp;logoWidth=18" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/overview/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c3cd55e61ef2e22ff00427b50b9e7f1c3547de91/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6174737573686973616b61692f707974686f6e726f626f746963732f62616467652f6d6173746572" alt="CodeFactor" data-canonical-src="https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/badge/master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/AtsushiSakai/PythonRobotics"&gt;&lt;img src="https://camo.githubusercontent.com/230f0a1eaa529fa727cad2c9d3c1ace4738bd25d/68747470733a2f2f746f6b65692e72732f62312f6769746875622f4174737573686953616b61692f507974686f6e526f626f74696373" alt="tokei" data-canonical-src="https://tokei.rs/b1/github/AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Python codes for robotics algorithm.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-is-this"&gt;What is this?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#requirements"&gt;Requirements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-use"&gt;How to use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#localization"&gt;Localization&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#extended-kalman-filter-localization"&gt;Extended Kalman Filter localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#particle-filter-localization"&gt;Particle filter localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#histogram-filter-localization"&gt;Histogram filter localization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#mapping"&gt;Mapping&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#gaussian-grid-map"&gt;Gaussian grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ray-casting-grid-map"&gt;Ray casting grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lidar-to-grid-map"&gt;Lidar to grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#k-means-object-clustering"&gt;k-means object clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rectangle-fitting"&gt;Rectangle fitting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#slam"&gt;SLAM&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#iterative-closest-point-icp-matching"&gt;Iterative Closest Point (ICP) Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#fastslam-10"&gt;FastSLAM 1.0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#path-planning"&gt;Path Planning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#dynamic-window-approach"&gt;Dynamic Window Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grid-based-search"&gt;Grid based search&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#dijkstra-algorithm"&gt;Dijkstra algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#a-algorithm"&gt;A* algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#potential-field-algorithm"&gt;Potential Field algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grid-based-coverage-path-planning"&gt;Grid based coverage path planning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#state-lattice-planning"&gt;State Lattice Planning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#biased-polar-sampling"&gt;Biased polar sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lane-sampling"&gt;Lane sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#probabilistic-road-map-prm-planning"&gt;Probabilistic Road-Map (PRM) planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rapidly-exploring-random-trees-rrt"&gt;Rapidly-Exploring Random Trees (RRT)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#rrt"&gt;RRT*&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rrt-with-reeds-shepp-path"&gt;RRT* with reeds-shepp path&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lqr-rrt"&gt;LQR-RRT*&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#quintic-polynomials-planning"&gt;Quintic polynomials planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reeds-shepp-planning"&gt;Reeds Shepp planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lqr-based-path-planning"&gt;LQR based path planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimal-trajectory-in-a-frenet-frame"&gt;Optimal Trajectory in a Frenet Frame&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#path-tracking"&gt;Path Tracking&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#move-to-a-pose-control"&gt;move to a pose control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stanley-control"&gt;Stanley control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rear-wheel-feedback-control"&gt;Rear wheel feedback control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#linearquadratic-regulator-lqr-speed-and-steering-control"&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#model-predictive-speed-and-steering-control"&gt;Model predictive speed and steering control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#nonlinear-model-predictive-control-with-c-gmres"&gt;Nonlinear Model predictive control with C-GMRES&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#arm-navigation"&gt;Arm Navigation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#n-joint-arm-to-point-control"&gt;N joint arm to point control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#arm-navigation-with-obstacle-avoidance"&gt;Arm navigation with obstacle avoidance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#aerial-navigation"&gt;Aerial Navigation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#drone-3d-trajectory-following"&gt;drone 3d trajectory following&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rocket-powered-landing"&gt;rocket powered landing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#bipedal"&gt;Bipedal&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#bipedal-planner-with-inverted-pendulum"&gt;bipedal planner with inverted pendulum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#use-case"&gt;Use-case&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citing"&gt;Citing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#support"&gt;Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#authors"&gt;Authors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-what-is-this" class="anchor" aria-hidden="true" href="#what-is-this"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is this?&lt;/h1&gt;
&lt;p&gt;This is a Python code collection of robotics algorithms, especially for autonomous navigation.&lt;/p&gt;
&lt;p&gt;Features:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Easy to read for understanding each algorithm's basic idea.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Widely used and practical algorithms are selected.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Minimum dependency.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;See this paper for more details:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1808.10703" rel="nofollow"&gt;[1808.10703] PythonRobotics: a Python code collection of robotics algorithms&lt;/a&gt; (&lt;a href="https://github.com/AtsushiSakai/PythonRoboticsPaper/blob/master/python_robotics.bib"&gt;BibTeX&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Python 3.7.x (2.7 is not supported)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;numpy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;scipy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;matplotlib&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;pandas&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.cvxpy.org/index.html" rel="nofollow"&gt;cvxpy&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h1&gt;
&lt;p&gt;This README only shows some examples of this project.&lt;/p&gt;
&lt;p&gt;If you are interested in other examples or mathematical backgrounds of each algorithm,&lt;/p&gt;
&lt;p&gt;You can check the full documentation online: &lt;a href="https://pythonrobotics.readthedocs.io/" rel="nofollow"&gt;https://pythonrobotics.readthedocs.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All animation gifs are stored here: &lt;a href="https://github.com/AtsushiSakai/PythonRoboticsGifs"&gt;AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Clone this repo.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;git clone &lt;a href="https://github.com/AtsushiSakai/PythonRobotics.git"&gt;https://github.com/AtsushiSakai/PythonRobotics.git&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;cd PythonRobotics/&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Install the required libraries. You can use environment.yml with conda command.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;conda env create -f environment.yml&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start="3"&gt;
&lt;li&gt;
&lt;p&gt;Execute python script in each directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add star to this repo if you like it &lt;g-emoji class="g-emoji" alias="smiley" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f603.png"&gt;😃&lt;/g-emoji&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-localization" class="anchor" aria-hidden="true" href="#localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Localization&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-extended-kalman-filter-localization" class="anchor" aria-hidden="true" href="#extended-kalman-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Extended Kalman Filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif" width="640" alt="EKF pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Documentation: &lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/Localization/extended_kalman_filter/extended_kalman_filter_localization.ipynb"&gt;Notebook&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-particle-filter-localization" class="anchor" aria-hidden="true" href="#particle-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Particle filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a sensor fusion localization with Particle Filter(PF).&lt;/p&gt;
&lt;p&gt;The blue line is true trajectory, the black line is dead reckoning trajectory,&lt;/p&gt;
&lt;p&gt;and the red line is estimated trajectory with PF.&lt;/p&gt;
&lt;p&gt;It is assumed that the robot can measure a distance from landmarks (RFID).&lt;/p&gt;
&lt;p&gt;This measurements are used for PF localization.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-histogram-filter-localization" class="anchor" aria-hidden="true" href="#histogram-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Histogram filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a 2D localization example with Histogram filter.&lt;/p&gt;
&lt;p&gt;The red cross is true position, black points are RFID positions.&lt;/p&gt;
&lt;p&gt;The blue grid shows a position probability of histogram filter.&lt;/p&gt;
&lt;p&gt;In this simulation, x,y are unknown, yaw is known.&lt;/p&gt;
&lt;p&gt;The filter integrates speed input and range observations from RFID for localization.&lt;/p&gt;
&lt;p&gt;Initial position is not needed.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-mapping" class="anchor" aria-hidden="true" href="#mapping"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mapping&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-gaussian-grid-map" class="anchor" aria-hidden="true" href="#gaussian-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gaussian grid map&lt;/h2&gt;
&lt;p&gt;This is a 2D Gaussian grid mapping example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ray-casting-grid-map" class="anchor" aria-hidden="true" href="#ray-casting-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ray casting grid map&lt;/h2&gt;
&lt;p&gt;This is a 2D ray casting grid mapping example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-lidar-to-grid-map" class="anchor" aria-hidden="true" href="#lidar-to-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lidar to grid map&lt;/h2&gt;
&lt;p&gt;This example shows how to convert a 2D range measurement to a grid map.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="Mapping/lidar_to_grid_map/animation.gif"&gt;&lt;img src="Mapping/lidar_to_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-k-means-object-clustering" class="anchor" aria-hidden="true" href="#k-means-object-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;k-means object clustering&lt;/h2&gt;
&lt;p&gt;This is a 2D object clustering with k-means algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rectangle-fitting" class="anchor" aria-hidden="true" href="#rectangle-fitting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rectangle fitting&lt;/h2&gt;
&lt;p&gt;This is a 2D rectangle fitting for vehicle detection.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-slam" class="anchor" aria-hidden="true" href="#slam"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SLAM&lt;/h1&gt;
&lt;p&gt;Simultaneous Localization and Mapping(SLAM) examples&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-iterative-closest-point-icp-matching" class="anchor" aria-hidden="true" href="#iterative-closest-point-icp-matching"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Iterative Closest Point (ICP) Matching&lt;/h2&gt;
&lt;p&gt;This is a 2D ICP matching example with singular value decomposition.&lt;/p&gt;
&lt;p&gt;It can calculate a rotation matrix and a translation vector between points to points.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf" rel="nofollow"&gt;Introduction to Mobile Robotics: Iterative Closest Point Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-fastslam-10" class="anchor" aria-hidden="true" href="#fastslam-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FastSLAM 1.0&lt;/h2&gt;
&lt;p&gt;This is a feature based SLAM example using FastSLAM 1.0.&lt;/p&gt;
&lt;p&gt;The blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.&lt;/p&gt;
&lt;p&gt;The red points are particles of FastSLAM.&lt;/p&gt;
&lt;p&gt;Black points are landmarks, blue crosses are estimated landmark positions by FastSLAM.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm" rel="nofollow"&gt;SLAM simulations by Tim Bailey&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-path-planning" class="anchor" aria-hidden="true" href="#path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Path Planning&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-dynamic-window-approach" class="anchor" aria-hidden="true" href="#dynamic-window-approach"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dynamic Window Approach&lt;/h2&gt;
&lt;p&gt;This is a 2D navigation sample code with Dynamic Window Approach.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf" rel="nofollow"&gt;The Dynamic Window Approach to Collision Avoidance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-grid-based-search" class="anchor" aria-hidden="true" href="#grid-based-search"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grid based search&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-dijkstra-algorithm" class="anchor" aria-hidden="true" href="#dijkstra-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dijkstra algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based shortest path planning with Dijkstra's algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-a-algorithm" class="anchor" aria-hidden="true" href="#a-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A* algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based shortest path planning with A star algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt;
&lt;p&gt;Its heuristic is 2D Euclid distance.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-potential-field-algorithm" class="anchor" aria-hidden="true" href="#potential-field-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Potential Field algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based path planning with Potential Field algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif" alt="PotentialField" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, the blue heat map shows potential value on each grid.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf" rel="nofollow"&gt;Robotic Motion Planning:Potential Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-grid-based-coverage-path-planning" class="anchor" aria-hidden="true" href="#grid-based-coverage-path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grid based coverage path planning&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based coverage path planning simulation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif" alt="PotentialField" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-state-lattice-planning" class="anchor" aria-hidden="true" href="#state-lattice-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;State Lattice Planning&lt;/h2&gt;
&lt;p&gt;This script is a path planning code with state lattice planning.&lt;/p&gt;
&lt;p&gt;This code uses the model predictive trajectory generator to solve boundary problem.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://journals.sagepub.com/doi/pdf/10.1177/0278364906075328" rel="nofollow"&gt;Optimal rough terrain trajectory generation for wheeled mobile robots&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.frc.ri.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf" rel="nofollow"&gt;State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-biased-polar-sampling" class="anchor" aria-hidden="true" href="#biased-polar-sampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Biased polar sampling&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-lane-sampling" class="anchor" aria-hidden="true" href="#lane-sampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lane sampling&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-probabilistic-road-map-prm-planning" class="anchor" aria-hidden="true" href="#probabilistic-road-map-prm-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Probabilistic Road-Map (PRM) planning&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif" alt="PRM" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This PRM planner uses Dijkstra method for graph search.&lt;/p&gt;
&lt;p&gt;In the animation, blue points are sampled points,&lt;/p&gt;
&lt;p&gt;Cyan crosses means searched points with Dijkstra method,&lt;/p&gt;
&lt;p&gt;The red line is the final path of PRM.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Probabilistic_roadmap" rel="nofollow"&gt;Probabilistic roadmap - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;　　&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rapidly-exploring-random-trees-rrt" class="anchor" aria-hidden="true" href="#rapidly-exploring-random-trees-rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rapidly-Exploring Random Trees (RRT)&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-rrt" class="anchor" aria-hidden="true" href="#rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RRT*&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a path planning code with RRT*&lt;/p&gt;
&lt;p&gt;Black circles are obstacles, green line is a searched tree, red crosses are start and goal positions.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1005.0416" rel="nofollow"&gt;Incremental Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.419.5503&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-rrt-with-reeds-shepp-path" class="anchor" aria-hidden="true" href="#rrt-with-reeds-shepp-path"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RRT* with reeds-shepp path&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif" alt="Robotics/animation.gif at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Path planning for a car robot with RRT* and reeds shepp path planner.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-lqr-rrt" class="anchor" aria-hidden="true" href="#lqr-rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LQR-RRT*&lt;/h3&gt;
&lt;p&gt;This is a path planning simulation with LQR-RRT*.&lt;/p&gt;
&lt;p&gt;A double integrator motion model is used for LQR local planner.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif" alt="LQRRRT" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://lis.csail.mit.edu/pubs/perez-icra12.pdf" rel="nofollow"&gt;LQR-RRT*: Optimal Sampling-Based Motion Planning with Automatically Derived Extension Heuristics&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/MahanFathi/LQR-RRTstar"&gt;MahanFathi/LQR-RRTstar: LQR-RRT* method is used for random motion planning of a simple pendulum in its phase plot&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-quintic-polynomials-planning" class="anchor" aria-hidden="true" href="#quintic-polynomials-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quintic polynomials planning&lt;/h2&gt;
&lt;p&gt;Motion planning with quintic polynomials.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It can calculate 2D path, velocity, and acceleration profile based on quintic polynomials.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ieeexplore.ieee.org/document/637936/" rel="nofollow"&gt;Local Path Planning And Motion Control For Agv In Positioning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-reeds-shepp-planning" class="anchor" aria-hidden="true" href="#reeds-shepp-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reeds Shepp planning&lt;/h2&gt;
&lt;p&gt;A sample code with Reeds Shepp path planning.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true" alt="RSPlanning" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://planning.cs.uiuc.edu/node822.html" rel="nofollow"&gt;15.3.2 Reeds-Shepp Curves&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf" rel="nofollow"&gt;optimal paths for a car that goes both forwards and backwards&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/ghliu/pyReedsShepp"&gt;ghliu/pyReedsShepp: Implementation of Reeds Shepp curve.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-lqr-based-path-planning" class="anchor" aria-hidden="true" href="#lqr-based-path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LQR based path planning&lt;/h2&gt;
&lt;p&gt;A sample code using LQR based path planning for double integrator model.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true" alt="RSPlanning" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-optimal-trajectory-in-a-frenet-frame" class="anchor" aria-hidden="true" href="#optimal-trajectory-in-a-frenet-frame"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimal Trajectory in a Frenet Frame&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is optimal trajectory generation in a Frenet Frame.&lt;/p&gt;
&lt;p&gt;The cyan line is the target course and black crosses are obstacles.&lt;/p&gt;
&lt;p&gt;The red line is predicted path.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf" rel="nofollow"&gt;Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Cj6tAQe7UCY" rel="nofollow"&gt;Optimal trajectory generation for dynamic street scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-path-tracking" class="anchor" aria-hidden="true" href="#path-tracking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Path Tracking&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-move-to-a-pose-control" class="anchor" aria-hidden="true" href="#move-to-a-pose-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;move to a pose control&lt;/h2&gt;
&lt;p&gt;This is a simulation of moving to a pose control&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://link.springer.com/book/10.1007/978-3-642-20144-8" rel="nofollow"&gt;P. I. Corke, "Robotics, Vision and Control" | SpringerLink p102&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-stanley-control" class="anchor" aria-hidden="true" href="#stanley-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stanley control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with Stanley steering control and PID speed control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://robots.stanford.edu/papers/thrun.stanley05.pdf" rel="nofollow"&gt;Stanley: The robot that won the DARPA grand challenge&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf" rel="nofollow"&gt;Automatic Steering Methods for Autonomous Automobile Path Tracking&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-rear-wheel-feedback-control" class="anchor" aria-hidden="true" href="#rear-wheel-feedback-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rear wheel feedback control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with rear wheel feedback steering control and PID speed control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1604.07446" rel="nofollow"&gt;A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-linearquadratic-regulator-lqr-speed-and-steering-control" class="anchor" aria-hidden="true" href="#linearquadratic-regulator-lqr-speed-and-steering-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with LQR speed and steering control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ieeexplore.ieee.org/document/5940562/" rel="nofollow"&gt;Towards fully autonomous driving: Systems and algorithms - IEEE Conference Publication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-model-predictive-speed-and-steering-control" class="anchor" aria-hidden="true" href="#model-predictive-speed-and-steering-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model predictive speed and steering control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with iterative linear model predictive speed and steering control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif" width="640" alt="MPC pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/model_predictive_speed_and_steer_control/Model_predictive_speed_and_steering_control.ipynb"&gt;notebook&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://grauonline.de/wordpress/?page_id=3244" rel="nofollow"&gt;Real-time Model Predictive Control (MPC), ACADO, Python | Work-is-Playing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-nonlinear-model-predictive-control-with-c-gmres" class="anchor" aria-hidden="true" href="#nonlinear-model-predictive-control-with-c-gmres"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Nonlinear Model predictive control with C-GMRES&lt;/h2&gt;
&lt;p&gt;A motion planning and path tracking simulation with NMPC of C-GMRES&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/cgmres_nmpc/cgmres_nmpc.ipynb"&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-arm-navigation" class="anchor" aria-hidden="true" href="#arm-navigation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arm Navigation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-n-joint-arm-to-point-control" class="anchor" aria-hidden="true" href="#n-joint-arm-to-point-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;N joint arm to point control&lt;/h2&gt;
&lt;p&gt;N joint arm to a point control simulation.&lt;/p&gt;
&lt;p&gt;This is a interactive simulation.&lt;/p&gt;
&lt;p&gt;You can set the goal position of the end effector with left-click on the ploting area.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this simulation N = 10, however, you can change it.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-arm-navigation-with-obstacle-avoidance" class="anchor" aria-hidden="true" href="#arm-navigation-with-obstacle-avoidance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arm navigation with obstacle avoidance&lt;/h2&gt;
&lt;p&gt;Arm navigation with obstacle avoidance simulation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-aerial-navigation" class="anchor" aria-hidden="true" href="#aerial-navigation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Aerial Navigation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-drone-3d-trajectory-following" class="anchor" aria-hidden="true" href="#drone-3d-trajectory-following"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;drone 3d trajectory following&lt;/h2&gt;
&lt;p&gt;This is a 3d trajectory following simulation for a quadrotor.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rocket-powered-landing" class="anchor" aria-hidden="true" href="#rocket-powered-landing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;rocket powered landing&lt;/h2&gt;
&lt;p&gt;This is a 3d trajectory generation simulation for a rocket powered landing.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/AerialNavigation/rocket_powered_landing/rocket_powered_landing.ipynb"&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-bipedal" class="anchor" aria-hidden="true" href="#bipedal"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bipedal&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-bipedal-planner-with-inverted-pendulum" class="anchor" aria-hidden="true" href="#bipedal-planner-with-inverted-pendulum"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;bipedal planner with inverted pendulum&lt;/h2&gt;
&lt;p&gt;This is a bipedal planner for modifying footsteps with inverted pendulum.&lt;/p&gt;
&lt;p&gt;You can set the footsteps and the planner will modify those automatically.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-use-case" class="anchor" aria-hidden="true" href="#use-case"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use-case&lt;/h1&gt;
&lt;p&gt;If this project helps your robotics project, please let me know with &lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Your robot's video, which is using PythonRobotics, is very welcome!!&lt;/p&gt;
&lt;p&gt;This is a list of other user's comment and references:&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md"&gt;users_comments&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h1&gt;
&lt;p&gt;A small PR like bug fix is welcome.&lt;/p&gt;
&lt;p&gt;If your PR is merged multiple times, I will add your account to the author list.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h1&gt;
&lt;p&gt;If you use this project's code for your academic work, we encourage you to cite &lt;a href="https://arxiv.org/abs/1808.10703" rel="nofollow"&gt;our papers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you use this project's code in industry, we'd love to hear from you as well; feel free to reach out to the developers directly.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h1&gt;
&lt;p&gt;If you or your company would like to support this project, please consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.patreon.com/myenigma" rel="nofollow"&gt;Become a backer or sponsor on Patreon&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.paypal.me/myenigmapay/" rel="nofollow"&gt;One-time donation via PayPal&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can add your name or your company logo in README if you are a patron.&lt;/p&gt;
&lt;p&gt;E-mail consultant is also available.&lt;/p&gt;
&lt;p&gt;　&lt;/p&gt;
&lt;p&gt;Your comment using &lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt; is also welcome.&lt;/p&gt;
&lt;p&gt;This is a list: &lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md"&gt;Users comments&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AtsushiSakai/"&gt;Atsushi Sakai&lt;/a&gt; (&lt;a href="https://twitter.com/Atsushi_twi" rel="nofollow"&gt;@Atsushi_twi&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/daniel-s-ingram"&gt;Daniel Ingram&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/jwdinius"&gt;Joe Dinius&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/karanchawla"&gt;Karan Chawla&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/araffin"&gt;Antonin RAFFIN&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AlexisTM"&gt;Alexis Paques&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rsasaki0109"&gt;Ryohei Sasaki&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/goktug97"&gt;Göktuğ Karakaşlı&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>AtsushiSakai</author><guid isPermaLink="false">https://github.com/AtsushiSakai/PythonRobotics</guid><pubDate>Fri, 03 Jan 2020 00:19:00 GMT</pubDate></item><item><title>spmallick/learnopencv #20 in Jupyter Notebook, This month</title><link>https://github.com/spmallick/learnopencv</link><description>&lt;p&gt;&lt;i&gt;Learn OpenCV  : C++ and Python Examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-learnopencv" class="anchor" aria-hidden="true" href="#learnopencv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;learnopencv&lt;/h1&gt;
&lt;p&gt;Learn OpenCV  : C++ and Python Examples. You can find the details at &lt;a href="https://www.LearnOpenCV.com" rel="nofollow"&gt;LearnOpenCV.com&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-list-of-blog-posts" class="anchor" aria-hidden="true" href="#list-of-blog-posts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;List of Blog Posts&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Blog Post&lt;/th&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/applications-of-foreground-background-separation-with-semantic-segmentation/" rel="nofollow"&gt;Applications of Foreground-Background separation with Semantic Segmentation&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/app-seperation-semseg"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/efficientnet-theory-code" rel="nofollow"&gt;EfficientNet: Theory + Code&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/EfficientNet"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/mask-r-cnn-instance-segmentation-with-pytorch/" rel="nofollow"&gt;PyTorch for Beginners: Mask R-CNN Instance Segmentation with PyTorch&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="./PyTorch-Mask-RCNN"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch" rel="nofollow"&gt;PyTorch for Beginners: Faster R-CNN Object Detection with PyTorch&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/PyTorch-faster-RCNN"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/" rel="nofollow"&gt;PyTorch for Beginners: Semantic Segmentation using torchvision&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/PyTorch-Segmentation-torchvision"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/image-classification-using-pre-trained-models-using-pytorch/" rel="nofollow"&gt;PyTorch for Beginners: Comparison of pre-trained models for Image Classification&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Image-classification-pre-trained-models/Image_Classification_using_pre_trained_models.ipynb"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/pytorch-for-beginners-basics/" rel="nofollow"&gt;PyTorch for Beginners: Basics&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/PyTorch-for-Beginners/PyTorch_for_Beginners.ipynb"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/pytorch-model-inference-using-onnx-and-caffe2/" rel="nofollow"&gt;PyTorch Model Inference using ONNX and Caffe2&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Inference-for-PyTorch-Models/ONNX-Caffe2"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/" rel="nofollow"&gt;Image Classification Using Transfer Learning in PyTorch&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Image-Classification-in-PyTorch"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/hangman-creating-games-in-opencv/" rel="nofollow"&gt;Hangman: Creating games in OpenCV&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Hangman"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/image-inpainting-with-opencv-c-python/" rel="nofollow"&gt;Image Inpainting with OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Image-Inpainting"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/hough-transform-with-opencv-c-python/" rel="nofollow"&gt;Hough Transform with OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Hough-Transform"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/" rel="nofollow"&gt;Xeus-Cling: Run C++ code in Jupyter Notebook&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/XeusCling"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/" rel="nofollow"&gt;Gender &amp;amp; Age Classification using OpenCV Deep Learning ( C++/Python )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/AgeGender"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/invisibility-cloak-using-color-detection-and-segmentation-with-opencv/" rel="nofollow"&gt;Invisibility Cloak using Color Detection and Segmentation with OpenCV&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/InvisibilityCloak"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/fast-image-downloader-for-open-images-v4/" rel="nofollow"&gt;Fast Image Downloader for Open Images V4 (Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/downloadOpenImages"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/deep-learning-based-text-detection-using-opencv-c-python/" rel="nofollow"&gt;Deep Learning based Text Detection Using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/TextDetectionEAST"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/" rel="nofollow"&gt;Video Stabilization Using Point Feature Matching in OpenCV&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/VideoStabilization"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/training-yolov3-deep-learning-based-custom-object-detector/" rel="nofollow"&gt;Training YOLOv3 : Deep Learning based Custom Object Detector&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/YOLOv3-Training-Snowman-Detector"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/using-openvino-with-opencv/" rel="nofollow"&gt;Using OpenVINO with OpenCV&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/OpenVINO-OpenCV"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/duplicate-search-on-quora-dataset/" rel="nofollow"&gt;Duplicate Search on Quora Dataset&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Quora-Dataset-Duplicate-Search"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/shape-matching-using-hu-moments-c-python/" rel="nofollow"&gt;Shape Matching using Hu Moments (C++/Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/HuMoments"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-4-on-centos-7/" rel="nofollow"&gt;Install OpenCV 4 on CentOS (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-centos.sh"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-3-4-4-on-centos-7/" rel="nofollow"&gt;Install OpenCV 3.4.4 on CentOS (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-centos.sh"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-3-4-4-on-red-hat/" rel="nofollow"&gt;Install OpenCV 3.4.4 on Red Hat (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-red-hat.sh"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-4-on-red-hat/" rel="nofollow"&gt;Install OpenCV 4 on Red Hat (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-red-hat.sh"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-4-on-macos/" rel="nofollow"&gt;Install OpenCV 4 on macOS (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/InstallScripts/installOpenCV-4-macos.sh"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-3-4-4-on-raspberry-pi/" rel="nofollow"&gt;Install OpenCV 3.4.4 on Raspberry Pi&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-raspberry-pi.sh"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-3-4-4-on-macos/" rel="nofollow"&gt;Install OpenCV 3.4.4 on macOS (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-macos.sh"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/opencv-qr-code-scanner-c-and-python/" rel="nofollow"&gt;OpenCV QR Code Scanner (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/QRCode-OpenCV"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-3-4-4-on-windows/" rel="nofollow"&gt;Install OpenCV 3.4.4 on Windows (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-3"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-16-04/" rel="nofollow"&gt;Install OpenCV 3.4.4 on Ubuntu 16.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-Ubuntu-16-04.sh"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-18-04/" rel="nofollow"&gt;Install OpenCV 3.4.4 on Ubuntu 18.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-Ubuntu-18-04.sh"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/universal-sentence-encoder" rel="nofollow"&gt;Universal Sentence Encoder&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/blob/master/Universal-Sentence-Encoder"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-4-on-raspberry-pi/" rel="nofollow"&gt;Install OpenCV 4 on Raspberry Pi&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-raspberry-pi.sh"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-4-on-windows/" rel="nofollow"&gt;Install OpenCV 4 on Windows (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-4"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/hand-keypoint-detection-using-deep-learning-and-opencv/" rel="nofollow"&gt;Hand Keypoint Detection using Deep Learning and OpenCV&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/HandPose"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/deep-learning-based-object-detection-and-instance-segmentation-using-mask-r-cnn-in-opencv-python-c/" rel="nofollow"&gt;Deep learning based Object Detection and Instance Segmentation using Mask R-CNN in OpenCV (Python / C++)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Mask-RCNN"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/" rel="nofollow"&gt;Install OpenCV 4 on Ubuntu 18.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-Ubuntu-18-04.sh"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-4-on-ubuntu-16-04/" rel="nofollow"&gt;Install OpenCV 4 on Ubuntu 16.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-Ubuntu-16-04.sh"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/" rel="nofollow"&gt;Multi-Person Pose Estimation in OpenCV using OpenPose&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/OpenPose-Multi-Person"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/heatmap-for-logo-detection-using-opencv-python/" rel="nofollow"&gt;Heatmap for Logo Detection using OpenCV (Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/heatmap"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/" rel="nofollow"&gt;Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/ObjectDetection-YOLO"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/" rel="nofollow"&gt;Convex Hull using OpenCV in Python and C++&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/ConvexHull"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/multitracker-multiple-object-tracking-using-opencv-c-python/" rel="nofollow"&gt;MultiTracker : Multiple Object Tracking using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/MultiObjectTracker"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/convolutional-neural-network-based-image-colorization-using-opencv/" rel="nofollow"&gt;Convolutional Neural Network based Image Colorization using OpenCV&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Colorization"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/svm-using-scikit-learn-in-python/" rel="nofollow"&gt;SVM using scikit-learn&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/" rel="nofollow"&gt;GOTURN: Deep Learning based Object Tracking&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/GOTURN"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/" rel="nofollow"&gt;Find the Center of a Blob (Centroid) using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/CenterofBlob"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/support-vector-machines-svm/" rel="nofollow"&gt;Support Vector Machines (SVM)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/batch-normalization-in-deep-networks/" rel="nofollow"&gt;Batch Normalization in Deep Networks&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/BatchNormalization"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/deep-learning-character-classification-using-synthetic-dataset/" rel="nofollow"&gt;Deep Learning based Character Classification using Synthetic Dataset&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/CharClassification"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/image-quality-assessment-brisque/" rel="nofollow"&gt;Image Quality Assessment : BRISQUE&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/ImageMetrics"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/understanding-alexnet/" rel="nofollow"&gt;Understanding AlexNet&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/deep-learning-based-text-recognition-ocr-using-tesseract-and-opencv/" rel="nofollow"&gt;Deep Learning based Text Recognition (OCR) using Tesseract and OpenCV&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/OCR"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/" rel="nofollow"&gt;Deep Learning based Human Pose Estimation using OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/OpenPose"&gt; Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/" rel="nofollow"&gt;Number of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/how-to-convert-your-opencv-c-code-into-a-python-module/" rel="nofollow"&gt;How to convert your OpenCV C++ code into a Python module&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/pymodule"&gt; Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/cv4faces-best-project-award-2018/" rel="nofollow"&gt;CV4Faces : Best Project Award 2018&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/facemark-facial-landmark-detection-using-opencv/" rel="nofollow"&gt;Facemark : Facial Landmark Detection using OpenCV&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/FacialLandmarkDetection"&gt; Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/" rel="nofollow"&gt;Image Alignment (Feature Based) using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/ImageAlignment-FeatureBased"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/barcode-and-qr-code-scanner-using-zbar-and-opencv/" rel="nofollow"&gt;Barcode and QR code Scanner using ZBar and OpenCV&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/barcode-QRcodeScanner"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/" rel="nofollow"&gt;Keras Tutorial : Fine-tuning using pre-trained models&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Keras-Fine-Tuning"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/opencv-transparent-api/" rel="nofollow"&gt;OpenCV Transparent API&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/face-reconstruction-using-eigenfaces-cpp-python/" rel="nofollow"&gt;Face Reconstruction using EigenFaces (C++/Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/ReconstructFaceUsingEigenFaces"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/eigenface-using-opencv-c-python/" rel="nofollow"&gt;Eigenface using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/EigenFace"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/principal-component-analysis/" rel="nofollow"&gt;Principal Component Analysis&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/" rel="nofollow"&gt;Keras Tutorial : Transfer Learning using pre-trained models&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Keras-Transfer-Learning"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/" rel="nofollow"&gt;Keras Tutorial : Using pre-trained Imagenet models&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Keras-ImageNet-Models"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/technical-aspects-of-a-digital-slr/" rel="nofollow"&gt;Technical Aspects of a Digital SLR&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/using-harry-potter-interactive-wand-with-opencv-to-create-magic/" rel="nofollow"&gt;Using Harry Potter interactive wand with OpenCV to create magic&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/install-opencv-3-and-dlib-on-windows-python-only/" rel="nofollow"&gt;Install OpenCV 3 and Dlib on Windows ( Python only )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras" rel="nofollow"&gt;Image Classification using Convolutional Neural Networks in Keras&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/KerasCNN-CIFAR"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/" rel="nofollow"&gt;Understanding Autoencoders using Tensorflow (Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/DenoisingAutoencoder"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/best-project-award-computer-vision-for-faces/" rel="nofollow"&gt;Best Project Award : Computer Vision for Faces&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/" rel="nofollow"&gt;Understanding Activation Functions in Deep Learning&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/" rel="nofollow"&gt;Image Classification using Feedforward Neural Network in Keras&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/KerasMLP-MNIST"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/exposure-fusion-using-opencv-cpp-python/" rel="nofollow"&gt;Exposure Fusion using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/ExposureFusion"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.learnopencv.com/understanding-feedforward-neural-networks/" rel="nofollow"&gt;Understanding Feedforward Neural Networks&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/high-dynamic-range-hdr-imaging-using-opencv-cpp-python" rel="nofollow"&gt;High Dynamic Range (HDR) Imaging using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/hdr"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/deep-learning-using-keras-the-basics" rel="nofollow"&gt;Deep learning using Keras – The Basics&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/keras-linear-regression"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/selective-search-for-object-detection-cpp-python/" rel="nofollow"&gt;Selective Search for Object Detection (C++ / Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/SelectiveSearch"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/" rel="nofollow"&gt;Installing Deep Learning Frameworks on Ubuntu with CUDA support&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/parallel-pixel-access-in-opencv-using-foreach/" rel="nofollow"&gt;Parallel Pixel Access in OpenCV using forEach&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/forEach"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/cvui-gui-lib-built-on-top-of-opencv-drawing-primitives/" rel="nofollow"&gt;cvui: A GUI lib built on top of OpenCV drawing primitives&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/UI-cvui"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/install-dlib-on-windows/" rel="nofollow"&gt;Install Dlib on Windows&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/install-dlib-on-ubuntu/" rel="nofollow"&gt;Install Dlib on Ubuntu&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/install-opencv3-on-ubuntu/" rel="nofollow"&gt;Install OpenCV3 on Ubuntu&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/" rel="nofollow"&gt;Read, Write and Display a video using OpenCV ( C++/ Python )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/VideoReadWriteDisplay"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/install-dlib-on-macos/" rel="nofollow"&gt;Install Dlib on MacOS&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/install-opencv3-on-macos/" rel="nofollow"&gt;Install OpenCV 3 on MacOS&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/install-opencv3-on-windows/" rel="nofollow"&gt;Install OpenCV 3 on Windows&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/get-opencv-build-information-getbuildinformation/" rel="nofollow"&gt;Get OpenCV Build Information ( getBuildInformation )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/color-spaces-in-opencv-cpp-python/" rel="nofollow"&gt;Color spaces in OpenCV (C++ / Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/ColorSpaces"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/" rel="nofollow"&gt;Neural Networks : A 30,000 Feet View for Beginners&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/alpha-blending-using-opencv-cpp-python/" rel="nofollow"&gt;Alpha Blending using OpenCV (C++ / Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/AlphaBlending"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/user-stories-how-readers-of-this-blog-are-applying-their-knowledge-to-build-applications/" rel="nofollow"&gt;User stories : How readers of this blog are applying their knowledge to build applications&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/how-to-select-a-bounding-box-roi-in-opencv-cpp-python/" rel="nofollow"&gt;How to select a bounding box ( ROI ) in OpenCV (C++/Python) ?&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/automatic-red-eye-remover-using-opencv-cpp-python/" rel="nofollow"&gt;Automatic Red Eye Remover using OpenCV (C++ / Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/RedEyeRemover"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/" rel="nofollow"&gt;Bias-Variance Tradeoff in Machine Learning&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/embedded-computer-vision-which-device-should-you-choose/" rel="nofollow"&gt;Embedded Computer Vision: Which device should you choose?&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/object-tracking-using-opencv-cpp-python/" rel="nofollow"&gt;Object Tracking using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/tracking"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/" rel="nofollow"&gt;Handwritten Digits Classification : An OpenCV ( C++ / Python ) Tutorial&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/digits-classification"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/training-better-haar-lbp-cascade-eye-detector-opencv/" rel="nofollow"&gt;Training a better Haar and LBP cascade based Eye Detector using OpenCV&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/deep-learning-book-gift-recipients/" rel="nofollow"&gt;Deep Learning Book Gift Recipients&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/minified-opencv-haar-and-lbp-cascades/" rel="nofollow"&gt;Minified OpenCV Haar and LBP Cascades&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/ninjaEyeDetector"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/deep-learning-book-gift/" rel="nofollow"&gt;Deep Learning Book Gift&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/histogram-of-oriented-gradients/" rel="nofollow"&gt;Histogram of Oriented Gradients&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/image-recognition-and-object-detection-part1/" rel="nofollow"&gt;Image Recognition and Object Detection : Part 1&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/" rel="nofollow"&gt;Head Pose Estimation using OpenCV and Dlib&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/HeadPose"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/live-cv/" rel="nofollow"&gt;Live CV : A Computer Vision Coding Application&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/" rel="nofollow"&gt;Approximate Focal Length for Webcams and Cell Phone Cameras&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/configuring-qt-for-opencv-on-osx/" rel="nofollow"&gt;Configuring Qt for OpenCV on OSX&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/qt-test"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/rotation-matrix-to-euler-angles/" rel="nofollow"&gt;Rotation Matrix To Euler Angles&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/RotationMatrixToEulerAngles"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/speeding-up-dlib-facial-landmark-detector/" rel="nofollow"&gt;Speeding up Dlib’s Facial Landmark Detector&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/warp-one-triangle-to-another-using-opencv-c-python/" rel="nofollow"&gt;Warp one triangle to another using OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/WarpTriangle"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/average-face-opencv-c-python-tutorial/" rel="nofollow"&gt;Average Face : OpenCV ( C++ / Python ) Tutorial&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/FaceAverage"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/face-swap-using-opencv-c-python/" rel="nofollow"&gt;Face Swap using OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/FaceSwap"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/face-morph-using-opencv-cpp-python/" rel="nofollow"&gt;Face Morph Using OpenCV — C++ / Python&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/FaceMorph"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/deep-learning-example-using-nvidia-digits-3-on-ec2/" rel="nofollow"&gt;Deep Learning Example using NVIDIA DIGITS 3 on EC2&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/nvidia-digits-3-on-ec2/" rel="nofollow"&gt;NVIDIA DIGITS 3 on EC2&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/homography-examples-using-opencv-python-c/" rel="nofollow"&gt;Homography Examples using OpenCV ( Python / C ++ )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Homography"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/filling-holes-in-an-image-using-opencv-python-c/" rel="nofollow"&gt;Filling holes in an image using OpenCV ( Python / C++ )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Holes"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/how-to-find-frame-rate-or-frames-per-second-fps-in-opencv-python-cpp/" rel="nofollow"&gt;How to find frame rate or frames per second (fps) in OpenCV ( Python / C++ ) ?&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/FPS"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/delaunay-triangulation-and-voronoi-diagram-using-opencv-c-python/" rel="nofollow"&gt;Delaunay Triangulation and Voronoi Diagram using OpenCV ( C++ / Python) &lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Delaunay"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/opencv-c-vs-python-vs-matlab-for-computer-vision/" rel="nofollow"&gt;OpenCV (C++ vs Python) vs MATLAB for Computer Vision&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/facial-landmark-detection/" rel="nofollow"&gt;Facial Landmark Detection&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/why-does-opencv-use-bgr-color-format/" rel="nofollow"&gt;Why does OpenCV use BGR color format ?&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/computer-vision-for-predicting-facial-attractiveness/" rel="nofollow"&gt;Computer Vision for Predicting Facial Attractiveness&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/FacialAttractiveness"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/applycolormap-for-pseudocoloring-in-opencv-c-python/" rel="nofollow"&gt;applyColorMap for pseudocoloring in OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Colormap"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/image-alignment-ecc-in-opencv-c-python/" rel="nofollow"&gt;Image Alignment (ECC) in OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/ImageAlignment"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/how-to-find-opencv-version-python-cpp/" rel="nofollow"&gt;How to find OpenCV version in Python and C++ ?&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/baidu-banned-from-ilsvrc-2015/" rel="nofollow"&gt;Baidu banned from ILSVRC 2015&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/opencv-transparent-api/" rel="nofollow"&gt;OpenCV Transparent API&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/how-computer-vision-solved-the-greatest-soccer-mystery-of-all-times/" rel="nofollow"&gt;How Computer Vision Solved the Greatest Soccer Mystery of All Time&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/embedded-vision-summit-2015/" rel="nofollow"&gt;Embedded Vision Summit 2015&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/read-an-image-in-opencv-python-cpp/" rel="nofollow"&gt;Read an Image in OpenCV ( Python, C++ )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/imread"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/" rel="nofollow"&gt;Non-Photorealistic Rendering using OpenCV ( Python, C++ )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/NonPhotorealisticRendering"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp/" rel="nofollow"&gt;Seamless Cloning using OpenCV ( Python , C++ )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/SeamlessCloning"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/opencv-threshold-python-cpp/" rel="nofollow"&gt;OpenCV Threshold ( Python , C++ )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/Threshold"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/blob-detection-using-opencv-python-c/" rel="nofollow"&gt;Blob Detection Using OpenCV ( Python, C++ )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/spmallick/learnopencv/tree/master/BlobDetector"&gt;Code&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/turn-your-opencv-Code-into-a-web-api-in-under-10-minutes-part-1/" rel="nofollow"&gt;Turn your OpenCV Code into a Web API in under 10 minutes — Part 1&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/how-to-compile-opencv-sample-Code/" rel="nofollow"&gt;How to compile OpenCV sample Code ?&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.learnopencv.com/install-opencv-3-on-yosemite-osx-10-10-x/" rel="nofollow"&gt;Install OpenCV 3 on Yosemite ( OSX 10.10.x )&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>spmallick</author><guid isPermaLink="false">https://github.com/spmallick/learnopencv</guid><pubDate>Fri, 03 Jan 2020 00:20:00 GMT</pubDate></item><item><title>jessevig/bertviz #21 in Jupyter Notebook, This month</title><link>https://github.com/jessevig/bertviz</link><description>&lt;p&gt;&lt;i&gt;Tool for visualizing attention in the Transformer model (BERT, GPT-2, Albert, XLNet, RoBERTa, CTRL, etc.)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-bertviz" class="anchor" aria-hidden="true" href="#bertviz"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BertViz&lt;/h1&gt;
&lt;p&gt;BertViz is a tool for visualizing attention in the Transformer model, supporting all models from the &lt;a href="https://github.com/huggingface/transformers"&gt;transformers&lt;/a&gt; library (BERT, GPT-2, XLNet, RoBERTa, XLM, CTRL, etc.). It extends the &lt;a href="https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization"&gt;Tensor2Tensor visualization tool&lt;/a&gt; by &lt;a href="https://medium.com/@llionj" rel="nofollow"&gt;Llion Jones&lt;/a&gt; and the &lt;a href="https://github.com/huggingface/transformers"&gt;transformers&lt;/a&gt; library from &lt;a href="https://github.com/huggingface"&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Blog posts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1" rel="nofollow"&gt;Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8" rel="nofollow"&gt;OpenAI GPT-2: Understanding Language Generation through Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77" rel="nofollow"&gt;Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1906.05714.pdf" rel="nofollow"&gt;A Multiscale Visualization of Attention in the Transformer Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-attention-head-view" class="anchor" aria-hidden="true" href="#attention-head-view"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Attention-head view&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;attention-head view&lt;/em&gt; visualizes the attention patterns produced by one or more attention heads in a given transformer layer.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jessevig/bertviz/master/images/head_thumbnail_left.png"&gt;&lt;img src="https://raw.githubusercontent.com/jessevig/bertviz/master/images/head_thumbnail_left.png" alt="Attention-head view" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jessevig/bertviz/master/images/head_thumbnail_right.gif"&gt;&lt;img src="https://raw.githubusercontent.com/jessevig/bertviz/master/images/head_thumbnail_right.gif" alt="Attention-head view animated" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The attention view supports all models from the Transformers library, including:&lt;br&gt;
BERT:
&lt;a href="https://github.com/jessevig/bertviz/blob/master/head_view_bert.ipynb"&gt;[Notebook]&lt;/a&gt;
&lt;a href="https://colab.research.google.com/drive/1PEHWRHrvxQvYr9NFRC-E_fr3xDq1htCj" rel="nofollow"&gt;[Colab]&lt;/a&gt;&lt;br&gt;
GPT-2:
&lt;a href="https://github.com/jessevig/bertviz/blob/master/head_view_gpt2.ipynb"&gt;[Notebook]&lt;/a&gt;
&lt;a href="https://colab.research.google.com/drive/1c9kBsbvSqpKkmd62u7nfqVhvWr0W8_Lx" rel="nofollow"&gt;[Colab]&lt;/a&gt;&lt;br&gt;
XLNet: &lt;a href="https://github.com/jessevig/bertviz/blob/master/head_view_xlnet.ipynb"&gt;[Notebook]&lt;/a&gt;&lt;br&gt;
RoBERTa: &lt;a href="https://github.com/jessevig/bertviz/blob/master/head_view_roberta.ipynb"&gt;[Notebook]&lt;/a&gt;&lt;br&gt;
XLM: &lt;a href="https://github.com/jessevig/bertviz/blob/master/head_view_xlm.ipynb"&gt;[Notebook]&lt;/a&gt;&lt;br&gt;
Albert: &lt;a href="https://github.com/jessevig/bertviz/blob/master/head_view_albert.ipynb"&gt;[Notebook]&lt;/a&gt;&lt;br&gt;
DistilBert: &lt;a href="https://github.com/jessevig/bertviz/blob/master/head_view_distilbert.ipynb"&gt;[Notebook]&lt;/a&gt;&lt;br&gt;
(and others)&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-view" class="anchor" aria-hidden="true" href="#model-view"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model view&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;model view&lt;/em&gt; provides a birds-eye view of attention across all of the model’s layers  and heads.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jessevig/bertviz/master/images/model_thumbnail.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/jessevig/bertviz/master/images/model_thumbnail.jpg" alt="Model view" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The model view supports all models from the Transformers library, including:&lt;br&gt;
BERT: &lt;a href="https://github.com/jessevig/bertviz/blob/master/model_view_bert.ipynb"&gt;[Notebook]&lt;/a&gt;
&lt;a href="https://colab.research.google.com/drive/1c73DtKNdl66B0_HF7QXuPenraDp0jHRS" rel="nofollow"&gt;[Colab]&lt;/a&gt;&lt;br&gt;
GPT2: &lt;a href="https://github.com/jessevig/bertviz/blob/master/model_view_gpt2.ipynb"&gt;[Notebook]&lt;/a&gt;
&lt;a href="https://colab.research.google.com/drive/1y-wfC95Z0aASawYqA34LQeV0_qC9mOto" rel="nofollow"&gt;[Colab]&lt;/a&gt;&lt;br&gt;
XLNet: &lt;a href="https://github.com/jessevig/bertviz/blob/master/model_view_xlnet.ipynb"&gt;[Notebook]&lt;/a&gt;&lt;br&gt;
RoBERTa: &lt;a href="https://github.com/jessevig/bertviz/blob/master/model_view_roberta.ipynb"&gt;[Notebook]&lt;/a&gt;&lt;br&gt;
XLM: &lt;a href="https://github.com/jessevig/bertviz/blob/master/model_view_xlm.ipynb"&gt;[Notebook]&lt;/a&gt;&lt;br&gt;
Albert: &lt;a href="https://github.com/jessevig/bertviz/blob/master/model_view_albert.ipynb"&gt;[Notebook]&lt;/a&gt;&lt;br&gt;
DistilBert: &lt;a href="https://github.com/jessevig/bertviz/blob/master/model_view_distilbert.ipynb"&gt;[Notebook]&lt;/a&gt;&lt;br&gt;
(and others)&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-neuron-view" class="anchor" aria-hidden="true" href="#neuron-view"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Neuron view&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;neuron view&lt;/em&gt; visualizes the individual neurons in the query and key vectors and shows how they are used to compute attention.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jessevig/bertviz/master/images/neuron_thumbnail.png"&gt;&lt;img src="https://raw.githubusercontent.com/jessevig/bertviz/master/images/neuron_thumbnail.png" alt="Neuron view" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The neuron view supports the following three models:&lt;br&gt;
BERT: &lt;a href="https://github.com/jessevig/bertviz/blob/master/neuron_view_bert.ipynb"&gt;[Notebook]&lt;/a&gt;
&lt;a href="https://colab.research.google.com/drive/1m37iotFeubMrp9qIf9yscXEL1zhxTN2b" rel="nofollow"&gt;[Colab]&lt;/a&gt;&lt;br&gt;
GPT-2
&lt;a href="https://github.com/jessevig/bertviz/blob/master/neuron_view_gpt2.ipynb"&gt;[Notebook]&lt;/a&gt;
&lt;a href="https://colab.research.google.com/drive/1s8XCCyxsKvNRWNzjWi5Nl8ZAYZ5YkLm_" rel="nofollow"&gt;[Colab]&lt;/a&gt;&lt;br&gt;
RoBERTa
&lt;a href="https://github.com/jessevig/bertviz/blob/master/neuron_view_roberta.ipynb"&gt;[Notebook]&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/transformers/" rel="nofollow"&gt;Transformers&lt;/a&gt; (version required depends on models used)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/" rel="nofollow"&gt;PyTorch&lt;/a&gt; &amp;gt;=1.0.0&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jupyter.org/install" rel="nofollow"&gt;Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/tqdm/" rel="nofollow"&gt;tqdm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/boto3/" rel="nofollow"&gt;boto3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/ipython/" rel="nofollow"&gt;IPython&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/requests/" rel="nofollow"&gt;requests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/regex/" rel="nofollow"&gt;regex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/sentencepiece/" rel="nofollow"&gt;sentencepiece&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(See &lt;a href="https://github.com/jessevig/bertviz/blob/master/requirements.txt"&gt;requirements.txt&lt;/a&gt;)&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-execution" class="anchor" aria-hidden="true" href="#execution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Execution&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/jessevig/bertviz.git
cd bertviz
jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: If you wish to run BertViz using Colab, please see the example Colab scripts above, as they differ slightly from the Jupyter notebook versions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://twitter.com/jesse_vig" rel="nofollow"&gt;Jesse Vig&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;When referencing BertViz, please cite &lt;a href="https://arxiv.org/abs/1906.05714" rel="nofollow"&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{vig2019transformervis,
  author    = {Jesse Vig},
  title     = {A Multiscale Visualization of Attention in the Transformer Model},
  journal   = {arXiv preprint arXiv:1906.05714},
  year      = {2019},
  url       = {https://arxiv.org/abs/1906.05714}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="LICENSE"&gt;LICENSE&lt;/a&gt; file for details&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgments" class="anchor" aria-hidden="true" href="#acknowledgments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;This project incorporates code from the following repos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/tensor2tensor"&gt;https://github.com/tensorflow/tensor2tensor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;https://github.com/huggingface/pytorch-pretrained-BERT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jessevig</author><guid isPermaLink="false">https://github.com/jessevig/bertviz</guid><pubDate>Fri, 03 Jan 2020 00:21:00 GMT</pubDate></item><item><title>rasbt/python-machine-learning-book-3rd-edition #22 in Jupyter Notebook, This month</title><link>https://github.com/rasbt/python-machine-learning-book-3rd-edition</link><description>&lt;p&gt;&lt;i&gt;The "Python Machine Learning (3rd edition)" book code repository&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-python-machine-learning-3rd-ed-code-repository" class="anchor" aria-hidden="true" href="#python-machine-learning-3rd-ed-code-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Machine Learning (3rd Ed.) Code Repository&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667" alt="Python 3.6" data-canonical-src="https://img.shields.io/badge/Python-3.7-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/a0e2e02654c03ef5b20640e5d052b0b448e59313/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64652532304c6963656e73652d4d49542d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/a0e2e02654c03ef5b20640e5d052b0b448e59313/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64652532304c6963656e73652d4d49542d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/Code%20License-MIT-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code repositories for the 1st and 2nd edition are available at&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/rasbt/python-machine-learning-book"&gt;https://github.com/rasbt/python-machine-learning-book&lt;/a&gt; and&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rasbt/python-machine-learning-book-2nd-edition"&gt;https://github.com/rasbt/python-machine-learning-book-2nd-edition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Python Machine Learning, 3rd Ed.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;to be published December 12th, 2019&lt;/p&gt;
&lt;p&gt;Paperback: 770 pages&lt;br&gt;
Publisher: Packt Publishing&lt;br&gt;
Language: English&lt;/p&gt;
&lt;p&gt;ISBN-10: 1789955750&lt;br&gt;
ISBN-13: 978-1789955750&lt;br&gt;
Kindle ASIN: B07VBLX2W7&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/" rel="nofollow"&gt;&lt;img src="./.other/cover_1.jpg" width="248" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/" rel="nofollow"&gt;Amazon Page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.packtpub.com/data/python-machine-learning-third-edition" rel="nofollow"&gt;Packt Page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents-and-code-notebooks" class="anchor" aria-hidden="true" href="#table-of-contents-and-code-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents and Code Notebooks&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Helpful installation and setup instructions can be found in the &lt;a href="ch01/README.md"&gt;README.md file of Chapter 1&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please note that these are just the code examples accompanying the book, which we uploaded for your convenience; be aware that these notebooks may not be useful without the formulae and descriptive text.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Machine Learning - Giving Computers the Ability to Learn from Data [&lt;a href="ch01"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Training Machine Learning Algorithms for Classification [&lt;a href="ch02"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A Tour of Machine Learning Classifiers Using Scikit-Learn [&lt;a href="ch03"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Building Good Training Sets – Data Pre-Processing [&lt;a href="ch04"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Compressing Data via Dimensionality Reduction [&lt;a href="ch05"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning Best Practices for Model Evaluation and Hyperparameter Optimization [&lt;a href="ch06"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Combining Different Models for Ensemble Learning [&lt;a href="ch07"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Applying Machine Learning to Sentiment Analysis [&lt;a href="ch08"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Embedding a Machine Learning Model into a Web Application [&lt;a href="ch09"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting Continuous Target Variables with Regression Analysis [&lt;a href="ch10"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Working with Unlabeled Data – Clustering Analysis [&lt;a href="ch11"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Implementing a Multi-layer Artificial Neural Network from Scratch [&lt;a href="ch12"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Parallelizing Neural Network Training with TensorFlow [&lt;a href="ch13"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Going Deeper: The Mechanics of TensorFlow [&lt;a href="ch14"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Classifying Images with Deep Convolutional Neural Networks [&lt;a href="ch15"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Modeling Sequential Data Using Recurrent Neural Networks [&lt;a href="ch16"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative Adversarial Networks for Synthesizing New Data [&lt;a href="ch17"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Reinforcement Learning for Decision Making in Complex Environments [&lt;a href="ch18"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;Raschka, Sebastian, and Vahid Mirjalili. &lt;em&gt;Python Machine Learning, 3rd Ed&lt;/em&gt;. Packt Publishing, 2019.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@book{RaschkaMirjalili2019,  
address = {Birmingham, UK},  
author = {Raschka, Sebastian and Mirjalili, Vahid},  
edition = {3},  
isbn = {978-1789955750},   
publisher = {Packt Publishing},  
title = {{Python Machine Learning, 3rd Ed.}},  
year = {2019}  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rasbt</author><guid isPermaLink="false">https://github.com/rasbt/python-machine-learning-book-3rd-edition</guid><pubDate>Fri, 03 Jan 2020 00:22:00 GMT</pubDate></item><item><title>yuanxiaosc/Google-Machine-learning-crash-course #23 in Jupyter Notebook, This month</title><link>https://github.com/yuanxiaosc/Google-Machine-learning-crash-course</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;p&gt;No README was found for this project.&lt;/p&gt;</description><author>yuanxiaosc</author><guid isPermaLink="false">https://github.com/yuanxiaosc/Google-Machine-learning-crash-course</guid><pubDate>Fri, 03 Jan 2020 00:23:00 GMT</pubDate></item><item><title>randerson112358/Python #24 in Jupyter Notebook, This month</title><link>https://github.com/randerson112358/Python</link><description>&lt;p&gt;&lt;i&gt;:snake: Python Programs&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python" class="anchor" aria-hidden="true" href="#python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python&lt;/h1&gt;
&lt;p&gt;This is a repository that holds my Python programs&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2cffe57e3c8276001c970391a67d67cf3a02f165/68747470733a2f2f7777772e707974686f6e2e6f72672f7374617469632f636f6d6d756e6974795f6c6f676f732f707974686f6e2d6c6f676f2d696e6b73636170652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/2cffe57e3c8276001c970391a67d67cf3a02f165/68747470733a2f2f7777772e707974686f6e2e6f72672f7374617469632f636f6d6d756e6974795f6c6f676f732f707974686f6e2d6c6f676f2d696e6b73636170652e737667" width="400" data-canonical-src="https://www.python.org/static/community_logos/python-logo-inkscape.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
To see me programming in Python checkout the YouTube channel: &lt;a href="https://www.youtube.com/playlist?list=PLBhJnyA0V0uIP6tScPs01FW5WtSpJdmcv" rel="nofollow"&gt;Go To YouTube Channel&lt;/a&gt;
&lt;h1&gt;&lt;a id="user-content-relavent-books-on-amazon" class="anchor" aria-hidden="true" href="#relavent-books-on-amazon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Relavent Books On Amazon&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/gp/product/1449355730/ref=as_li_tl?ie=UTF8&amp;amp;tag=github01d-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=1449355730&amp;amp;linkId=95e6eaf8c12b9fcd483dd06c1dd53e48" rel="nofollow"&gt;Learning Python, 5th Edition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/gp/product/1491962291/ref=as_li_tl?ie=UTF8&amp;amp;tag=github01d-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=1491962291&amp;amp;linkId=9dec6584d63a7cfcbc32af1ff9737bbf" rel="nofollow"&gt;Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/gp/product/1491912057/ref=as_li_tl?ie=UTF8&amp;amp;tag=github01d-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=1491912057&amp;amp;linkId=af650651a6d71fdea49cd5aa95653e1c" rel="nofollow"&gt;Python Data Science Handbook: Essential Tools for Working with Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&amp;amp;tag=github01d-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=1449369413&amp;amp;linkId=7b6ad9375121575c83af505f2a3ed6f3" rel="nofollow"&gt;Introduction to Machine Learning with Python: A Guide for Data Scientists&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-python-data-cleaning-programs" class="anchor" aria-hidden="true" href="#python-data-cleaning-programs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Data Cleaning Programs&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Program Name&lt;/th&gt;
&lt;th&gt;Algorithm Name&lt;/th&gt;
&lt;th&gt;Link to Program&lt;/th&gt;
&lt;th&gt;Blog&lt;/th&gt;
&lt;th&gt;YouTube&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;concatenate_file.py&lt;/td&gt;
&lt;td&gt;Concatenate Multiple CSV files&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/concatenate_file.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://everythingcomputerscience.com/" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.youtube.com/channel/UCbmb5IoBtHZTpYZCDBOC1CA" rel="nofollow"&gt;YouTubeX&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;remove_empty_row.py&lt;/td&gt;
&lt;td&gt;Removes Empty Rows&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/remove_empty_row.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://everythingcomputerscience.com/" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.youtube.com/channel/UCbmb5IoBtHZTpYZCDBOC1CA" rel="nofollow"&gt;YouTubeX&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;replace_strings_with_numbers.py&lt;/td&gt;
&lt;td&gt;Changes Strings in CSV to Numbers&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Replace_Strings_With_Numbers/replace_strings_with_numbers.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://everythingcomputerscience.com/" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/zv_fzW2iA_U" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-web-scraping" class="anchor" aria-hidden="true" href="#web-scraping"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web Scraping&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Program Name&lt;/th&gt;
&lt;th&gt;Algorithm Name&lt;/th&gt;
&lt;th&gt;Link to Program&lt;/th&gt;
&lt;th&gt;Blog&lt;/th&gt;
&lt;th&gt;YouTube&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;scrape.py&lt;/td&gt;
&lt;td&gt;Scrape Website Links&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/scrape.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/scrape-website-using-python-90619cac7c97" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/LGZEn1OYUTk" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;News_Article.py&lt;/td&gt;
&lt;td&gt;Scrape &amp;amp; Summarize Article&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/News_Article.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://everythingcomputerscience.com/" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/YzMA2O_v5co" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-machine-learning-projects--programs" class="anchor" aria-hidden="true" href="#machine-learning-projects--programs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Projects &amp;amp; Programs&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Project Name&lt;/th&gt;
&lt;th&gt;Program Name&lt;/th&gt;
&lt;th&gt;Algorithm Name&lt;/th&gt;
&lt;th&gt;Link to Program&lt;/th&gt;
&lt;th&gt;Blog&lt;/th&gt;
&lt;th&gt;YouTube&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sentiment Analysis&lt;/td&gt;
&lt;td&gt;sentiment.py&lt;/td&gt;
&lt;td&gt;Sentiment Analysis&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/sentiment.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/sentiment-analysis-e2e4442bac13" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/1VHhDSOwJPw" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Simple Linear Regression Ex&lt;/td&gt;
&lt;td&gt;LinearRegression.py&lt;/td&gt;
&lt;td&gt;Linear Regression&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/LinearRegression.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/a-simple-machine-learning-python-program-bf5d156d2cda" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/z7jEJY8FbA8" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Car Classification&lt;/td&gt;
&lt;td&gt;decisionTree.py&lt;/td&gt;
&lt;td&gt;Decision Tree&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/DecisionTree/decisionTree.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/car-classification-89ad60204acf" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/U-Jm8ugN0Ps" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Golf Predictions&lt;/td&gt;
&lt;td&gt;Golf_Predictions.ipynb&lt;/td&gt;
&lt;td&gt;Decision Tree&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Golf_Predictions.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/python-decision-tree-classifier-example-d73bc3aeca6" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/bT-43kgYI3o" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict Boston House Price&lt;/td&gt;
&lt;td&gt;Predict_Boston_Housing_Price.ipynb&lt;/td&gt;
&lt;td&gt;Linear Regression&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Predict_Boston_Housing_Price.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/predict-boston-house-prices-using-python-linear-regression-90469e0a341" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/gOXoFDrseis" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict Stock Price&lt;/td&gt;
&lt;td&gt;stock.ipynb&lt;/td&gt;
&lt;td&gt;Linear Regression &amp;amp; SVR&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/stock.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/predict-stock-prices-using-python-machine-learning-53aa024da20a" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/EYnC4ACIt2g" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Classify Iris Species&lt;/td&gt;
&lt;td&gt;Logistic_Regression.ipynb&lt;/td&gt;
&lt;td&gt;Logistic Regression&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Logistic_Regression.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/python-logistic-regression-program-5e1b32f964db" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/ACdBKML9l4s" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict Median House Price&lt;/td&gt;
&lt;td&gt;Neural_Networks.ipynb&lt;/td&gt;
&lt;td&gt;Deep Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Neural_Networks/Neural_Networks.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/predict-house-median-prices-5f1a768dd256?postPublishedType=repub" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/vSzou5zRwNQ" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Classify Handwritten Digits&lt;/td&gt;
&lt;td&gt;MNIST_ANN.ipynb&lt;/td&gt;
&lt;td&gt;Artificial Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/MNIST_ANN.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/classify-hand-written-digits-5fdbe5d99ee7" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/kOFUQB7u5Ck" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cluster NBA Basketball Players&lt;/td&gt;
&lt;td&gt;Basketball_Data_Exploration.ipynb&lt;/td&gt;
&lt;td&gt;KMeans&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/NBA_Basketball_Exploration/Basketball_Data_Exploration.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/nba-data-analysis-exploration-9293f311e0e8" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/2Pmf6Kqak3w" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict FB Stock Price&lt;/td&gt;
&lt;td&gt;SVM.ipynb&lt;/td&gt;
&lt;td&gt;Support Vector Regression (SVR)&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/SVM_Stock/SVM.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/facebook-stock-prediction-bcfc676bc611" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/tMPfZV_ipOg" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Breast Cancer Detection&lt;/td&gt;
&lt;td&gt;Breast_Cancer_Detection.ipynb&lt;/td&gt;
&lt;td&gt;Random Forest Classifier &amp;amp; Gaussian Naive Bayes &amp;amp; Logistic Regression &amp;amp; Decision Tree Classifier &amp;amp; SVC&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/breast_cancer_detection/Breast_Cancer_Detection.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/breast-cancer-detection-using-machine-learning-38820fe98982" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/NSSOyhJBmWY" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face Detection&lt;/td&gt;
&lt;td&gt;face_detection.py&lt;/td&gt;
&lt;td&gt;Open CV &amp;amp; Adaboost&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/face_detection/face_detection.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/face-detection-using-python-open-cv-d51e27266f7f" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/6klXqQMctPk" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image Classification&lt;/td&gt;
&lt;td&gt;cnn.ipynb&lt;/td&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Classify_Images/cnn.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/classify-images-using-convolutional-neural-networks-python-a89cecc8c679" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/mB7fdy67eFw" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Classify Handwritten Digits CNN&lt;/td&gt;
&lt;td&gt;mnist_cnn.ipynb&lt;/td&gt;
&lt;td&gt;Convolutional Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/mnist_cnn.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/classify-hand-written-digits-using-python-and-convolutional-neural-networks-26ccfc06b95c" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/V4dd2Bt9OHY" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Spam Detection&lt;/td&gt;
&lt;td&gt;Email_Spam_Detection.ipynb&lt;/td&gt;
&lt;td&gt;Naive Bayes&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Email_Spam_Detection/Email_Spam_Detection.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/email-spam-detection-using-python-machine-learning-abe38c889855" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/cNLPt02RwF0" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pima-Indians Diabetes&lt;/td&gt;
&lt;td&gt;Diabetes.ipynb&lt;/td&gt;
&lt;td&gt;Artificial Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Diabetes/Diabetes.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/build-your-own-artificial-neural-network-using-python-f37d16be06bf" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=S2sZNlr-4_4&amp;amp;list=PLBhJnyA0V0uIP6tScPs01FW5WtSpJdmcv&amp;amp;index=28&amp;amp;t=0s" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Movie Recommendation Engine&lt;/td&gt;
&lt;td&gt;Movie_Recommendation.ipynb&lt;/td&gt;
&lt;td&gt;Cosine Similarity&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Movie_Recommender/Movie_Recommendation.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/build-a-movie-recommendation-engine-using-python-scikit-learn-machine-learning-e68ba297e163" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/umSM8rFtVMs" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Article Text To Speech&lt;/td&gt;
&lt;td&gt;Article_Text_To_Speech.py&lt;/td&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Article_Text_To_Speech.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/build-a-text-to-speech-program-using-python-b70de7105383" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/uPSIUjo_Fhw" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AI Smart Dr.Chat Bot&lt;/td&gt;
&lt;td&gt;smartbot.py&lt;/td&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/smartbot.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/build-your-own-ai-chat-bot-using-python-machine-learning-682ddd8acc29" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/QpMsT0WuIuI" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Neural Network Stock Prediction&lt;/td&gt;
&lt;td&gt;lstm2.py&lt;/td&gt;
&lt;td&gt;LSTM&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/LSTM_Stock/lstm2.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/stock-price-prediction-using-python-machine-learning-e82a039ac2bb" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/QIUxPv5PJOY" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>randerson112358</author><guid isPermaLink="false">https://github.com/randerson112358/Python</guid><pubDate>Fri, 03 Jan 2020 00:24:00 GMT</pubDate></item><item><title>nianticlabs/monodepth2 #25 in Jupyter Notebook, This month</title><link>https://github.com/nianticlabs/monodepth2</link><description>&lt;p&gt;&lt;i&gt;Monocular depth estimation from a single image&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-monodepth2" class="anchor" aria-hidden="true" href="#monodepth2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Monodepth2&lt;/h1&gt;
&lt;p&gt;This is the reference PyTorch implementation for training and testing depth estimation models using the method described in&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Digging into Self-Supervised Monocular Depth Prediction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/C.Godard/" rel="nofollow"&gt;Clément Godard&lt;/a&gt;, &lt;a href="http://vision.caltech.edu/~macaodha/" rel="nofollow"&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href="http://www.michaelfirman.co.uk" rel="nofollow"&gt;Michael Firman&lt;/a&gt; and &lt;a href="http://www0.cs.ucl.ac.uk/staff/g.brostow/" rel="nofollow"&gt;Gabriel J. Brostow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1806.01260" rel="nofollow"&gt;ICCV 2019&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="assets/teaser.gif"&gt;&lt;img src="assets/teaser.gif" alt="example input output gif" width="600" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;This code is for non-commercial use; please see the &lt;a href="LICENSE"&gt;license file&lt;/a&gt; for terms.&lt;/p&gt;
&lt;p&gt;If you find our work useful in your research please consider citing our paper:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{monodepth2,
  title     = {Digging into Self-Supervised Monocular Depth Prediction},
  author    = {Cl{\'{e}}ment Godard and
               Oisin {Mac Aodha} and
               Michael Firman and
               Gabriel J. Brostow},
  booktitle = {The International Conference on Computer Vision (ICCV)},
  month = {October},
year = {2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-️-setup" class="anchor" aria-hidden="true" href="#️-setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;⚙️&lt;/g-emoji&gt; Setup&lt;/h2&gt;
&lt;p&gt;Assuming a fresh &lt;a href="https://www.anaconda.com/download/" rel="nofollow"&gt;Anaconda&lt;/a&gt; distribution, you can install the dependencies with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install pytorch=0.4.1 torchvision=0.2.1 -c pytorch
pip install tensorboardX==1.4
conda install opencv=3.3.1   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; just needed for evaluation&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We ran our experiments with PyTorch 0.4.1, CUDA 9.1, Python 3.6.6 and Ubuntu 18.04.
We have also successfully trained models with PyTorch 1.0, and our code is compatible with Python 2.7. You may have issues installing OpenCV version 3.3.1 if you use Python 3.7, we recommend to create a virtual environment with Python 3.6.6 &lt;code&gt;conda create -n monodepth2 python=3.6.6 anaconda &lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;&lt;a id="user-content-️-prediction-for-a-single-image" class="anchor" aria-hidden="true" href="#️-prediction-for-a-single-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;🖼️&lt;/g-emoji&gt; Prediction for a single image&lt;/h2&gt;
&lt;p&gt;You can predict depth for a single image with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python test_simple.py --image_path assets/test_image.jpg --model_name mono+stereo_640x192&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On its first run this will download the &lt;code&gt;mono+stereo_640x192&lt;/code&gt; pretrained model (99MB) into the &lt;code&gt;models/&lt;/code&gt; folder.
We provide the following  options for &lt;code&gt;--model_name&lt;/code&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;--model_name&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;Training modality&lt;/th&gt;
&lt;th&gt;Imagenet pretrained?&lt;/th&gt;
&lt;th&gt;Model resolution&lt;/th&gt;
&lt;th&gt;KITTI abs. rel. error&lt;/th&gt;
&lt;th&gt;delta &amp;lt; 1.25&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.115&lt;/td&gt;
&lt;td&gt;0.877&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip" rel="nofollow"&gt;&lt;code&gt;stereo_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.109&lt;/td&gt;
&lt;td&gt;0.864&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono+stereo_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.106&lt;/td&gt;
&lt;td&gt;0.874&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip" rel="nofollow"&gt;&lt;code&gt;mono_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;0.115&lt;/td&gt;
&lt;td&gt;0.879&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip" rel="nofollow"&gt;&lt;code&gt;stereo_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;0.107&lt;/td&gt;
&lt;td&gt;0.874&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip" rel="nofollow"&gt;&lt;code&gt;mono+stereo_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;0.106&lt;/td&gt;
&lt;td&gt;0.876&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.132&lt;/td&gt;
&lt;td&gt;0.845&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip" rel="nofollow"&gt;&lt;code&gt;stereo_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.130&lt;/td&gt;
&lt;td&gt;0.831&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono+stereo_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.127&lt;/td&gt;
&lt;td&gt;0.836&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can also download models trained on the odometry split with &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_odom_640x192.zip" rel="nofollow"&gt;monocular&lt;/a&gt; and &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_odom_640x192.zip" rel="nofollow"&gt;mono+stereo&lt;/a&gt; training modalities.
Finally, we provide resnet 50 depth estimation models trained with &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_640x192.zip" rel="nofollow"&gt;ImageNet pretrained weights&lt;/a&gt; and &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_no_pt_640x192.zip" rel="nofollow"&gt;trained from scratch&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--kitti-training-data" class="anchor" aria-hidden="true" href="#-kitti-training-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="floppy_disk" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png"&gt;💾&lt;/g-emoji&gt; KITTI training data&lt;/h2&gt;
&lt;p&gt;You can download the entire &lt;a href="http://www.cvlibs.net/datasets/kitti/raw_data.php" rel="nofollow"&gt;raw KITTI dataset&lt;/a&gt; by running:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;wget -i splits/kitti_archives_to_download.txt -P kitti_data/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then unzip with&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; kitti_data
unzip &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;*.zip&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; ..&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; it weighs about &lt;strong&gt;175GB&lt;/strong&gt;, so make sure you have enough space to unzip too!&lt;/p&gt;
&lt;p&gt;Our default settings expect that you have converted the png images to jpeg with this command, &lt;strong&gt;which also deletes the raw KITTI &lt;code&gt;.png&lt;/code&gt; files&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;find kitti_data/ -name &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;*.png&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;|&lt;/span&gt; parallel &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg &amp;amp;&amp;amp; rm {}&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;or&lt;/strong&gt; you can skip this conversion step and train from raw png files by adding the flag &lt;code&gt;--png&lt;/code&gt; when training, at the expense of slower load times.&lt;/p&gt;
&lt;p&gt;The above conversion command creates images which match our experiments, where KITTI &lt;code&gt;.png&lt;/code&gt; images were converted to &lt;code&gt;.jpg&lt;/code&gt; on Ubuntu 16.04 with default chroma subsampling &lt;code&gt;2x2,1x1,1x1&lt;/code&gt;.
We found that Ubuntu 18.04 defaults to &lt;code&gt;2x2,2x2,2x2&lt;/code&gt;, which gives different results, hence the explicit parameter in the conversion command.&lt;/p&gt;
&lt;p&gt;You can also place the KITTI dataset wherever you like and point towards it with the &lt;code&gt;--data_path&lt;/code&gt; flag during training and evaluation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Splits&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The train/test/validation splits are defined in the &lt;code&gt;splits/&lt;/code&gt; folder.
By default, the code will train a depth model using &lt;a href="https://github.com/tinghuiz/SfMLearner"&gt;Zhou's subset&lt;/a&gt; of the standard Eigen split of KITTI, which is designed for monocular training.
You can also train a model using the new &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="nofollow"&gt;benchmark split&lt;/a&gt; or the &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_odometry.php" rel="nofollow"&gt;odometry split&lt;/a&gt; by setting the &lt;code&gt;--split&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custom dataset&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can train on a custom monocular or stereo dataset by writing a new dataloader class which inherits from &lt;code&gt;MonoDataset&lt;/code&gt; – see the &lt;code&gt;KITTIDataset&lt;/code&gt; class in &lt;code&gt;datasets/kitti_dataset.py&lt;/code&gt; for an example.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--training" class="anchor" aria-hidden="true" href="#-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="hourglass_flowing_sand" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/23f3.png"&gt;⏳&lt;/g-emoji&gt; Training&lt;/h2&gt;
&lt;p&gt;By default models and tensorboard event files are saved to &lt;code&gt;~/tmp/&amp;lt;model_name&amp;gt;&lt;/code&gt;.
This can be changed with the &lt;code&gt;--log_dir&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Monocular training:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name mono_model&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Stereo training:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our code defaults to using Zhou's subsampled Eigen training data. For stereo-only training we have to specify that we want to use the full Eigen training set – see paper for details.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name stereo_model \
  --frame_ids 0 --use_stereo --split eigen_full&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Monocular + stereo training:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name mono+stereo_model \
  --frame_ids 0 -1 1 --use_stereo&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-gpus" class="anchor" aria-hidden="true" href="#gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GPUs&lt;/h3&gt;
&lt;p&gt;The code can only be run on a single GPU.
You can specify which GPU to use with the &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; environment variable:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;CUDA_VISIBLE_DEVICES=2 python train.py --model_name mono_model&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All our experiments were performed on a single NVIDIA Titan Xp.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Training modality&lt;/th&gt;
&lt;th&gt;Approximate GPU memory&lt;/th&gt;
&lt;th&gt;Approximate training time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;9GB&lt;/td&gt;
&lt;td&gt;12 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;6GB&lt;/td&gt;
&lt;td&gt;8 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;11GB&lt;/td&gt;
&lt;td&gt;15 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content--finetuning-a-pretrained-model" class="anchor" aria-hidden="true" href="#-finetuning-a-pretrained-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="minidisc" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bd.png"&gt;💽&lt;/g-emoji&gt; Finetuning a pretrained model&lt;/h3&gt;
&lt;p&gt;Add the following to the training command to load an existing model for finetuning:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name finetuned_mono --load_weights_folder &lt;span class="pl-k"&gt;~&lt;/span&gt;/tmp/mono_model/models/weights_19&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content--other-training-options" class="anchor" aria-hidden="true" href="#-other-training-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f527.png"&gt;🔧&lt;/g-emoji&gt; Other training options&lt;/h3&gt;
&lt;p&gt;Run &lt;code&gt;python train.py -h&lt;/code&gt; (or look at &lt;code&gt;options.py&lt;/code&gt;) to see the range of other training options, such as learning rates and ablation settings.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--kitti-evaluation" class="anchor" aria-hidden="true" href="#-kitti-evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="bar_chart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png"&gt;📊&lt;/g-emoji&gt; KITTI evaluation&lt;/h2&gt;
&lt;p&gt;To prepare the ground truth depth maps run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python export_gt_depth.py --data_path kitti_data --split eigen
python export_gt_depth.py --data_path kitti_data --split eigen_benchmark&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;...assuming that you have placed the KITTI dataset in the default location of &lt;code&gt;./kitti_data/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The following example command evaluates the epoch 19 weights of a model named &lt;code&gt;mono_model&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_depth.py --load_weights_folder &lt;span class="pl-k"&gt;~&lt;/span&gt;/tmp/mono_model/models/weights_19/ --eval_mono&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For stereo models, you must use the &lt;code&gt;--eval_stereo&lt;/code&gt; flag (see note below):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_depth.py --load_weights_folder &lt;span class="pl-k"&gt;~&lt;/span&gt;/tmp/stereo_model/models/weights_19/ --eval_stereo&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you train your own model with our code you are likely to see slight differences to the publication results due to randomization in the weights initialization and data loading.&lt;/p&gt;
&lt;p&gt;An additional parameter &lt;code&gt;--eval_split&lt;/code&gt; can be set.
The three different values possible for &lt;code&gt;eval_split&lt;/code&gt; are explained here:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;--eval_split&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;Test set size&lt;/th&gt;
&lt;th&gt;For models trained with...&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;eigen&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;697&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--split eigen_zhou&lt;/code&gt; (default) or &lt;code&gt;--split eigen_full&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The standard Eigen test files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;eigen_benchmark&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;652&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--split eigen_zhou&lt;/code&gt; (default) or &lt;code&gt;--split eigen_full&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Evaluate with the improved ground truth from the &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="nofollow"&gt;new KITTI depth benchmark&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;benchmark&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--split benchmark&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="nofollow"&gt;new KITTI depth benchmark&lt;/a&gt; test files.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Because no ground truth is available for the new KITTI depth benchmark, no scores will be reported  when &lt;code&gt;--eval_split benchmark&lt;/code&gt; is set.
Instead, a set of &lt;code&gt;.png&lt;/code&gt; images will be saved to disk ready for upload to the evaluation server.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;External disparities evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Finally you can also use &lt;code&gt;evaluate_depth.py&lt;/code&gt; to evaluate raw disparities (or inverse depth) from other methods by using the &lt;code&gt;--ext_disp_to_eval&lt;/code&gt; flag:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_depth.py --ext_disp_to_eval &lt;span class="pl-k"&gt;~&lt;/span&gt;/other_method_disp.npy&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;g-emoji class="g-emoji" alias="camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f7.png"&gt;📷&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f7.png"&gt;📷&lt;/g-emoji&gt; Note on stereo evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our stereo models are trained with an effective baseline of &lt;code&gt;0.1&lt;/code&gt; units, while the actual KITTI stereo rig has a baseline of &lt;code&gt;0.54m&lt;/code&gt;. This means a scaling of &lt;code&gt;5.4&lt;/code&gt; must be applied for evaluation.
In addition, for models trained with stereo supervision we disable median scaling.
Setting the &lt;code&gt;--eval_stereo&lt;/code&gt; flag when evaluating will automatically disable median scaling and scale predicted depths by &lt;code&gt;5.4&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;g-emoji class="g-emoji" alias="arrow_heading_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2934.png"&gt;⤴️&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="arrow_heading_down" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2935.png"&gt;⤵️&lt;/g-emoji&gt; Odometry evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We include code for evaluating poses predicted by models trained with &lt;code&gt;--split odom --dataset kitti_odom --data_path /path/to/kitti/odometry/dataset&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this evaluation, the &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_odometry.php" rel="nofollow"&gt;KITTI odometry dataset&lt;/a&gt; &lt;strong&gt;(color, 65GB)&lt;/strong&gt; and &lt;strong&gt;ground truth poses&lt;/strong&gt; zip files must be downloaded.
As above, we assume that the pngs have been converted to jpgs.&lt;/p&gt;
&lt;p&gt;If this data has been unzipped to folder &lt;code&gt;kitti_odom&lt;/code&gt;, a model can be evaluated with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_pose.py --eval_split odom_9 --load_weights_folder ./odom_split.M/models/weights_29 --data_path kitti_odom/
python evaluate_pose.py --eval_split odom_10 --load_weights_folder ./odom_split.M/models/weights_29 --data_path kitti_odom/&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content--precomputed-results" class="anchor" aria-hidden="true" href="#-precomputed-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="package" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e6.png"&gt;📦&lt;/g-emoji&gt; Precomputed results&lt;/h2&gt;
&lt;p&gt;You can download our precomputed disparity predictions from the following links:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Training modality&lt;/th&gt;
&lt;th&gt;Input size&lt;/th&gt;
&lt;th&gt;&lt;code&gt;.npy&lt;/code&gt; filesize&lt;/th&gt;
&lt;th&gt;Eigen disparities&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;343 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;343 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;343 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;914 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;914 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;914 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-️-license" class="anchor" aria-hidden="true" href="#️-license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="woman_judge" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f469-2696.png"&gt;👩‍⚖️&lt;/g-emoji&gt; License&lt;/h2&gt;
&lt;p&gt;Copyright © Niantic, Inc. 2019. Patent Pending.
All rights reserved.
Please see the &lt;a href="LICENSE"&gt;license file&lt;/a&gt; for terms.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>nianticlabs</author><guid isPermaLink="false">https://github.com/nianticlabs/monodepth2</guid><pubDate>Fri, 03 Jan 2020 00:25:00 GMT</pubDate></item></channel></rss>