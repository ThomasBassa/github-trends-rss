<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: TeX, This month</title><link>https://github.com/trending/tex?since=monthly</link><description>The top repositories on GitHub for tex, measured monthly</description><pubDate>Wed, 30 Oct 2019 00:04:39 GMT</pubDate><lastBuildDate>Wed, 30 Oct 2019 00:04:39 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>1400</ttl><item><title>jacobeisenstein/gt-nlp-class #1 in TeX, This month</title><link>https://github.com/jacobeisenstein/gt-nlp-class</link><description>&lt;p&gt;&lt;i&gt;Course materials for Georgia Tech CS 4650 and 7650, "Natural Language"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cs-4650-and-7650" class="anchor" aria-hidden="true" href="#cs-4650-and-7650"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CS 4650 and 7650&lt;/h1&gt;
&lt;p&gt;(&lt;strong&gt;Note about registration&lt;/strong&gt;: registration is currently restricted to students pursuing CS degrees for which this course is an essential requirement. Unfortunately, the enrollment is already at the limit of the classroom space, so this restriction is unlikely to be lifted.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Course&lt;/strong&gt;: Natural Language Understanding&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instructor&lt;/strong&gt;: Jacob Eisenstein&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Semester&lt;/strong&gt;: Spring 2018&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time&lt;/strong&gt;: Mondays and Wednesdays, 3:00-4:15pm&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TAs&lt;/strong&gt;: Murali Raghu Babu, James Mullenbach, Yuval Pinter, Zhewei Sun&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1BuvRjPhfHmy7XAfpc5KoygdfqI3Cue3bbmiO6yYuX_E/edit?usp=sharing" rel="nofollow"&gt;Schedule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/document/d/1loefqZhmOaF2mP8yQPEx91jZ7BHylWixVtYlFhpIlGM/edit?usp=sharing" rel="nofollow"&gt;Recaps&lt;/a&gt; from previous classes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This course gives an overview of modern data-driven techniques for natural language processing. The course moves from shallow bag-of-words models to richer structural representations of how words interact to create meaning. At each level, we will discuss the salient linguistic phemonena and most successful computational models. Along the way we will cover machine learning techniques which
are especially relevant to natural language processing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#readings"&gt;Readings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grading"&gt;Grading&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#help"&gt;Help&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#policies"&gt;Policies&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-learning-goals" class="anchor" aria-hidden="true" href="#learning-goals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learning goals&lt;/h1&gt;
&lt;a name="user-content-learning"&gt;
&lt;ul&gt;
&lt;li&gt;Acquire the fundamental linguistic concepts that are relevant to language technology. This goal will be assessed in the short homework assignments and the exams.&lt;/li&gt;
&lt;li&gt;Analyze and understand state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the exams and the assigned projects.&lt;/li&gt;
&lt;li&gt;Implement state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the assigned projects.&lt;/li&gt;
&lt;li&gt;Adapt and apply state-of-the-art language technology to new problems and settings. This goal will be assessed in assigned projects.&lt;/li&gt;
&lt;li&gt;(7650 only) Read and understand current research on natural language processing. This goal will be assessed in assigned projects.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-readings" class="anchor" aria-hidden="true" href="#readings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Readings&lt;/h1&gt;
&lt;/a&gt;&lt;a name="user-content-readings"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-readings"&gt;Readings will be drawn mainly from my &lt;/a&gt;&lt;a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes"&gt;notes&lt;/a&gt;. Additional readings may be assigned from published papers, blogposts, and tutorials.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-supplemental-textbooks" class="anchor" aria-hidden="true" href="#supplemental-textbooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supplemental textbooks&lt;/h2&gt;
&lt;p&gt;These are completely optional, but might deepen your understanding of the material.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210/" rel="nofollow"&gt;Speech and Language Processing&lt;/a&gt; is the textbook most often used in NLP courses. It's a great reference for both the linguistics and algorithms we'll encounter in this course. Several chapters from the upcoming &lt;a href="https://web.stanford.edu/~jurafsky/slp3/" rel="nofollow"&gt;third edition&lt;/a&gt; are free online.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.amazon.com/Natural-Language-Processing-Python-Steven/dp/0596516495" rel="nofollow"&gt;Natural Language Processing with Python&lt;/a&gt;
shows how to do hands-on work with Python's Natural Language Toolkit (NLTK), and also brings a strong linguistic perspective.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.amazon.com/Schaums-Outline-Probability-Statistics-Edition/dp/007179557X/ref=pd_sim_b_1?ie=UTF8&amp;amp;refRID=1R57HWNCW6EEWD1ZRH4C" rel="nofollow"&gt;Schaum's Outline of Probability and Statistics&lt;/a&gt; can help you review the probability and statistics that we use in this course.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-grading" class="anchor" aria-hidden="true" href="#grading"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grading&lt;/h1&gt;
&lt;a name="user-content-grading"&gt;
&lt;p&gt;The graded material for the course will consist of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Seven short homework assignments, of which you must do six. Most of these involve performing linguistic annotation on some text of your choice. The purpose is to get a basic understanding of key linguistic concepts. Each assignment should take less than an hour. Each homework is worth 2 points (12 total). (Many of these homeworks are implemented at &lt;strong&gt;quizzes&lt;/strong&gt; on Canvas.)&lt;/li&gt;
&lt;li&gt;Four assigned problem sets. These involve building and using NLP techniques which are at or near the state-of-the-art. The purpose is to learn how to implement natural language processing software, and to have fun. These assignments must be done individually. Each problem set is worth ten points (48 total). Students enrolled in CS 7650 will have an additional, research-oriented component to the problem sets.&lt;/li&gt;
&lt;li&gt;An in-class midterm exam, worth 20 points, and a final exam, worth 20 points. The purpose of these exams is to assess understanding of the core theoretical concepts, and to encourage you to review and synthesize your understanding of these concepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-grading"&gt;Barring a personal emergency or an institute-approved absence, you must take each exam on the day indicated in the schedule. Job interviews and travel plans are generally not a reason for an institute-approved absence. See &lt;/a&gt;&lt;a href="https://registrar.gatech.edu/info/institute-approved-absence-form-for-students" rel="nofollow"&gt;here&lt;/a&gt; for more information on GT policy about absences.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-late-policy" class="anchor" aria-hidden="true" href="#late-policy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Late policy&lt;/h2&gt;
&lt;p&gt;Problem sets will be accepted up to 72 hours late, at a penalty of 2 points per 24 hours. (Maximum score after missing the deadline: 10/12; maximum score 24 hours after the deadline: 8/12, etc.)  It is usually best just to turn in what you have at the due date. Late homeworks will not be accepted. This late policy is intended to ensure fair and timely evaluation.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-getting-help" class="anchor" aria-hidden="true" href="#getting-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting help&lt;/h1&gt;
&lt;a name="user-content-help"&gt;
&lt;h2&gt;&lt;a id="user-content-office-hours" class="anchor" aria-hidden="true" href="#office-hours"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Office hours&lt;/h2&gt;
&lt;p&gt;My office hours follow Wednesday classes (4:15-5:15PM) and take place in class when available.&lt;/p&gt;
&lt;p&gt;TA office hours are in CCB commons (1st floor) unless otherwise announced on Piazza.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Murali: Friday   10AM-11AM&lt;/li&gt;
&lt;li&gt;James:  Thursday 11AM-12PM&lt;/li&gt;
&lt;li&gt;Yuval:  Tuesday  3PM-4PM&lt;/li&gt;
&lt;li&gt;Zhewei: Monday   1PM-2PM&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-online-help" class="anchor" aria-hidden="true" href="#online-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online help&lt;/h2&gt;
&lt;p&gt;Please use Piazza rather than personal email to ask questions. This helps other students, who may have the same question. Personal emails may not be answered. If you cannot make it to office hours, please use Piazza to make an appointment. It is unlikely that I will be able to chat if you make an unscheduled visit to my office. The same is true for the TAs.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-class-policies" class="anchor" aria-hidden="true" href="#class-policies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Class policies&lt;/h1&gt;
&lt;/a&gt;&lt;a name="user-content-policies"&gt;
&lt;p&gt;Attendance will not be taken, but &lt;strong&gt;you are responsible for knowing what happens in every class&lt;/strong&gt;. If you cannot attend class, make sure you check up with someone who was there.&lt;/p&gt;
&lt;p&gt;Respect your classmates and your instructor by preventing distractions. This means be on time, turn off your cellphone, and save side conversations for after class. If you can't read something I wrote on the board, or if you think I made a mistake in a derivation, please raise your hand and tell me!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using a laptop in class is likely to reduce your education attainment&lt;/strong&gt;. This has been documented by multiple studies, which are nicely summarized in the following article:&lt;/p&gt;
&lt;/a&gt;&lt;ul&gt;&lt;a name="user-content-policies"&gt;
&lt;/a&gt;&lt;li&gt;&lt;a name="user-content-policies"&gt;&lt;/a&gt;&lt;a href="https://www.nytimes.com/2017/11/22/business/laptops-not-during-lecture-or-meeting.html" rel="nofollow"&gt;https://www.nytimes.com/2017/11/22/business/laptops-not-during-lecture-or-meeting.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I am not going to ban laptops, as long as they are not a distraction to anyone but the user. But I suggest you try pen and paper for a few weeks, and see if it helps.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h2&gt;
&lt;a name="user-content-prerequisites"&gt;
&lt;p&gt;The official prerequisite for CS 4650 is CS 3510/3511, "Design and Analysis of Algorithms." This prerequisite is essential because understanding natural language processing algorithms requires familiarity with dynamic programming, as well as automata and formal language theory: finite-state and context-free languages, NP-completeness, etc. While course prerequisites are not enforced for graduate students, prior exposure to analysis of algorithms is very strongly recommended.&lt;/p&gt;
&lt;p&gt;Furthermore, this course assumes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Good coding ability, corresponding to at least a third or fourth-year undergraduate CS major. Assignments will be in Python.&lt;/li&gt;
&lt;li&gt;Background in basic probability, linear algebra, and calculus.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;People sometimes want to take the course without having all of these
prerequisites. Frequent cases are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Junior CS students with strong programming skills but limited theoretical and mathematical background,&lt;/li&gt;
&lt;li&gt;Non-CS students with strong mathematical background but limited programming experience.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Students in the first group suffer in the exam and don't understand the lectures, and students in the second group suffer in the problem sets. My advice is to get the background material first, and
then take this course.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-collaboration-policy" class="anchor" aria-hidden="true" href="#collaboration-policy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Collaboration policy&lt;/h2&gt;
&lt;p&gt;One of the goals of the assigned work is to assess your individual progress in meeting the learning objectives of the course. You may discuss the homework and projects with other students, but your work must be your own -- particularly all coding and writing. For example:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examples-of-acceptable-collaboration" class="anchor" aria-hidden="true" href="#examples-of-acceptable-collaboration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples of acceptable collaboration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Alice and Bob discuss alternatives for storing large, sparse vectors of feature counts, as required by a problem set.&lt;/li&gt;
&lt;li&gt;Bob is confused about how to implement the Viterbi algorithm, and asks Alice for a conceptual description of her strategy.&lt;/li&gt;
&lt;li&gt;Alice asks Bob if he encountered a failure condition at a "sanity check" in a coding assignment, and Bob explains at a conceptual level how he overcame that failure condition.&lt;/li&gt;
&lt;li&gt;Alice is having trouble getting adequate performance from her part-of-speech tagger. She finds a blog page or research paper that gives her some new ideas, which she implements.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-examples-of-unacceptable-collaboration" class="anchor" aria-hidden="true" href="#examples-of-unacceptable-collaboration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples of unacceptable collaboration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Alice and Bob work together to write code for storing feature counts.&lt;/li&gt;
&lt;li&gt;Alice and Bob divide the assignment into parts, and each write the code for their part, and then share their solutions with each other to complete the assignment.&lt;/li&gt;
&lt;li&gt;Alice or Bob obtain a solution to a previous year's assignment or to a related assignment in another class, and use it as the starting point for their own solutions.&lt;/li&gt;
&lt;li&gt;Bob is having trouble getting adequate performance from his part-of-speech tagger. He finds source code online, and copies it into his own submission.&lt;/li&gt;
&lt;li&gt;Alice wants to win the Kaggle competition for a problem set. She finds the test set online, and customizes her submission to do well on it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some assignments will involve written responses. Using other people’s text or figures without attribution is plagiarism, and is never acceptable.&lt;/p&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-prerequisites"&gt;Suspected cases of academic misconduct will be (and have been!) referred to the Honor Advisory Council. For any questions involving these or any other Academic Honor Code issues, please consult me, my teaching assistants, or &lt;/a&gt;&lt;a href="http://www.honor.gatech.edu" rel="nofollow"&gt;http://www.honor.gatech.edu&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jacobeisenstein</author><guid isPermaLink="false">https://github.com/jacobeisenstein/gt-nlp-class</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>HarisIqbal88/PlotNeuralNet #2 in TeX, This month</title><link>https://github.com/HarisIqbal88/PlotNeuralNet</link><description>&lt;p&gt;&lt;i&gt;Latex code for making neural networks diagrams&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-plotneuralnet" class="anchor" aria-hidden="true" href="#plotneuralnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PlotNeuralNet&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.5281/zenodo.2526396" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/89c8c312f40c2d237b2319aececd5740a147b11c/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e323532363339362e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.2526396.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Latex code for drawing neural networks for reports and presentation. Have a look into examples to see how they are made. Additionally, lets consolidate any improvements that you make and fix any bugs to help more people with this code.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Install the following packages on Ubuntu.
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ubuntu 16.04&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install texlive-latex-extra
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ubuntu 18.04.2
Base on this &lt;a href="https://gist.github.com/rain1024/98dd5e2c6c8c28f9ea9d"&gt;website&lt;/a&gt;, please install the following packages.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install texlive-latex-base
sudo apt-get install texlive-fonts-recommended
sudo apt-get install texlive-fonts-extra
sudo apt-get install texlive-latex-extra
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Execute the example as followed.
&lt;pre&gt;&lt;code&gt;cd pyexamples/
bash ../tikzmake.sh test_simple
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Python interface&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add easy legend functionality&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add more layer shapes like TruncatedPyramid, 2DSheet etc&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add examples for RNN and likes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-latex-usage" class="anchor" aria-hidden="true" href="#latex-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Latex usage&lt;/h2&gt;
&lt;p&gt;See &lt;a href="examples"&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory for usage.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-python-usage" class="anchor" aria-hidden="true" href="#python-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python usage&lt;/h2&gt;
&lt;p&gt;First, create a new directory and a new Python file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ mkdir my_project
$ cd my_project
vim my_arch.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the following code to your new file:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; sys
sys.path.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;../&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;from&lt;/span&gt; pycore.tikzeng &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; defined your arch&lt;/span&gt;
arch &lt;span class="pl-k"&gt;=&lt;/span&gt; [
    to_head( &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;..&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; ),
    to_cor(),
    to_begin(),
    to_Conv(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conv1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;512&lt;/span&gt;, &lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;depth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt; ),
    to_Pool(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(conv1-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
    to_Conv(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conv2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(1,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(pool1-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;, &lt;span class="pl-v"&gt;depth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;, &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt; ),
    to_connection( &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conv2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
    to_Pool(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(conv2-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;28&lt;/span&gt;, &lt;span class="pl-v"&gt;depth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;28&lt;/span&gt;, &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;),
    to_SoftMax(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;soft1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;10&lt;/span&gt; ,&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(3,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(pool1-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;caption&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;SOFT&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;  ),
    to_connection(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;soft1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
    to_end()
    ]

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;main&lt;/span&gt;():
    namefile &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;str&lt;/span&gt;(sys.argv[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]).split(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;.&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]
    to_generate(arch, namefile &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;.tex&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; )

&lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;__name__&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;__main__&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:
    main()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, run the program as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bash ../tikzmake.sh my_arch
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;p&gt;Following are some network representations:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/17570785/50308846-c2231880-049c-11e9-8763-3daa1024de78.png"&gt;&lt;img src="https://user-images.githubusercontent.com/17570785/50308846-c2231880-049c-11e9-8763-3daa1024de78.png" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-fcn-8" class="anchor" aria-hidden="true" href="#fcn-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FCN-8&lt;/h6&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/17570785/50308873-e2eb6e00-049c-11e9-9587-9da6bdec011b.png"&gt;&lt;img src="https://user-images.githubusercontent.com/17570785/50308873-e2eb6e00-049c-11e9-9587-9da6bdec011b.png" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-fcn-32" class="anchor" aria-hidden="true" href="#fcn-32"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FCN-32&lt;/h6&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/17570785/50308911-03b3c380-049d-11e9-92d9-ce15669017ad.png"&gt;&lt;img src="https://user-images.githubusercontent.com/17570785/50308911-03b3c380-049d-11e9-92d9-ce15669017ad.png" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-holistically-nested-edge-detection" class="anchor" aria-hidden="true" href="#holistically-nested-edge-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Holistically-Nested Edge Detection&lt;/h6&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>HarisIqbal88</author><guid isPermaLink="false">https://github.com/HarisIqbal88/PlotNeuralNet</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>vdumoulin/conv_arithmetic #3 in TeX, This month</title><link>https://github.com/vdumoulin/conv_arithmetic</link><description>&lt;p&gt;&lt;i&gt;A technical report on convolution arithmetic in the context of deep learning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-convolution-arithmetic" class="anchor" aria-hidden="true" href="#convolution-arithmetic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolution arithmetic&lt;/h1&gt;
&lt;p&gt;A technical report on convolution arithmetic in the context of deep learning.&lt;/p&gt;
&lt;p&gt;The code and the images of this tutorial are free to use as regulated by the
licence and subject to proper attribution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[1] Vincent Dumoulin, Francesco Visin - &lt;a href="https://arxiv.org/abs/1603.07285" rel="nofollow"&gt;A guide to convolution arithmetic
for deep learning&lt;/a&gt;
(&lt;a href="https://gist.github.com/fvisin/165ca9935392fa9600a6c94664a01214"&gt;BibTeX&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-convolution-animations" class="anchor" aria-hidden="true" href="#convolution-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/no_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/arbitrary_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/arbitrary_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/same_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/same_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/full_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/full_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, no strides&lt;/td&gt;
    &lt;td&gt;Arbitrary padding, no strides&lt;/td&gt;
    &lt;td&gt;Half padding, no strides&lt;/td&gt;
    &lt;td&gt;Full padding, no strides&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_strides.gif"&gt;&lt;img width="150px" src="gif/no_padding_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides.gif"&gt;&lt;img width="150px" src="gif/padding_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides_odd.gif"&gt;&lt;img width="150px" src="gif/padding_strides_odd.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, strides&lt;/td&gt;
    &lt;td&gt;Padding, strides&lt;/td&gt;
    &lt;td&gt;Padding, strides (odd)&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-transposed-convolution-animations" class="anchor" aria-hidden="true" href="#transposed-convolution-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transposed convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/no_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/arbitrary_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/arbitrary_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/same_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/same_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/full_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/full_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, no strides, transposed&lt;/td&gt;
    &lt;td&gt;Arbitrary padding, no strides, transposed&lt;/td&gt;
    &lt;td&gt;Half padding, no strides, transposed&lt;/td&gt;
    &lt;td&gt;Full padding, no strides, transposed&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/no_padding_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/padding_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides_odd_transposed.gif"&gt;&lt;img width="150px" src="gif/padding_strides_odd_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, strides, transposed&lt;/td&gt;
    &lt;td&gt;Padding, strides, transposed&lt;/td&gt;
    &lt;td&gt;Padding, strides, transposed (odd)&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-dilated-convolution-animations" class="anchor" aria-hidden="true" href="#dilated-convolution-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dilated convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/dilation.gif"&gt;&lt;img width="150px" src="gif/dilation.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, no stride, dilation&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-generating-the-makefile" class="anchor" aria-hidden="true" href="#generating-the-makefile"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating the Makefile&lt;/h2&gt;
&lt;p&gt;From the repository's root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ ./bin/generate_makefile&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-generating-the-animations" class="anchor" aria-hidden="true" href="#generating-the-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating the animations&lt;/h2&gt;
&lt;p&gt;From the repository's root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make all_animations&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The animations will be output to the &lt;code&gt;gif&lt;/code&gt; directory. Individual animation steps
will be output in PDF format to the &lt;code&gt;pdf&lt;/code&gt; directory and in PNG format to the
&lt;code&gt;png&lt;/code&gt; directory.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-compiling-the-document" class="anchor" aria-hidden="true" href="#compiling-the-document"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compiling the document&lt;/h2&gt;
&lt;p&gt;From the repository's root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>vdumoulin</author><guid isPermaLink="false">https://github.com/vdumoulin/conv_arithmetic</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>matze/mtheme #4 in TeX, This month</title><link>https://github.com/matze/mtheme</link><description>&lt;p&gt;&lt;i&gt;A modern LaTeX Beamer theme&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-metropolis" class="anchor" aria-hidden="true" href="#metropolis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Metropolis&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;IMPORTANT NOTICES FOR VERSION 1.0&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The package and theme name changed from &lt;em&gt;m&lt;/em&gt; to &lt;em&gt;metropolis&lt;/em&gt;!&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;title format&lt;/code&gt; values have been restructured. Please refer to the
&lt;a href="http://mirrors.ctan.org/macros/latex/contrib/beamer-contrib/themes/metropolis/doc/metropolistheme.pdf" rel="nofollow"&gt;manual&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Metropolis is a simple, modern Beamer theme suitable for anyone to use. It tries
to minimize noise and maximize space for content; the only visual flourish it
offers is an (optional) progress bar added to each slide. The core design
principles of the theme were described in a blog post
&lt;a href="http://bloerg.net/2014/09/20/a-modern-beamer-theme.html" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Not convinced? Have a look at the &lt;a href="http://mirrors.ctan.org/macros/latex/contrib/beamer-contrib/themes/metropolis/demo/demo.pdf" rel="nofollow"&gt;demo slides&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/da754cd10b8a0c20144d0fa848a347698ff7a197/687474703a2f2f692e696d6775722e636f6d2f4278753532667a2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/da754cd10b8a0c20144d0fa848a347698ff7a197/687474703a2f2f692e696d6775722e636f6d2f4278753532667a2e706e67" alt="Sample" data-canonical-src="http://i.imgur.com/Bxu52fz.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;To install a stable version of this theme, please refer to update instructions
of your TeX distribution. Metropolis is on &lt;a href="http://ctan.org/pkg/beamertheme-metropolis" rel="nofollow"&gt;CTAN&lt;/a&gt; since December
2014 thus it is part of MikTeX and will be part of TeX Live 2016.&lt;/p&gt;
&lt;p&gt;Installing Metropolis from source, like any Beamer theme, involves four easy
steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Download the source&lt;/strong&gt; with a &lt;code&gt;git clone&lt;/code&gt; of the &lt;a href="https://github.com/matze/mtheme"&gt;Metropolis repository&lt;/a&gt;
or as a &lt;a href="https://github.com/matze/mtheme/archive/master.zip"&gt;zip archive&lt;/a&gt; of
the latest development version.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compile the style files&lt;/strong&gt; by running &lt;code&gt;make sty&lt;/code&gt; inside the downloaded
directory. (Or run LaTeX directly on &lt;code&gt;source/metropolistheme.ins&lt;/code&gt;.)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Move the resulting &lt;code&gt;*.sty&lt;/code&gt; files&lt;/strong&gt; to the folder containing your
presentation. To use Metropolis with many presentations, run &lt;code&gt;make install&lt;/code&gt;
or move the &lt;code&gt;*.sty&lt;/code&gt; files to a folder in your TeX path instead (might require
&lt;code&gt;sudo&lt;/code&gt; rights).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use the theme for your presentation&lt;/strong&gt; by declaring &lt;code&gt;\usetheme{metropolis}&lt;/code&gt; in
the preamble of your Beamer document.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For best results&lt;/strong&gt; install Mozilla's &lt;a href="https://github.com/bBoxType/FiraSans"&gt;Fira Sans&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;The following code shows a minimal example of a Beamer presentation using
Metropolis.&lt;/p&gt;
&lt;div class="highlight highlight-text-tex-latex"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;\documentclass&lt;/span&gt;{&lt;span class="pl-c1"&gt;beamer&lt;/span&gt;}
&lt;span class="pl-c1"&gt;\usetheme&lt;/span&gt;{metropolis}           &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt; Use metropolis theme&lt;/span&gt;
&lt;span class="pl-c1"&gt;\title&lt;/span&gt;{A minimal example}
&lt;span class="pl-c1"&gt;\date&lt;/span&gt;{&lt;span class="pl-c1"&gt;\today&lt;/span&gt;}
&lt;span class="pl-c1"&gt;\author&lt;/span&gt;{Matthias Vogelgesang}
&lt;span class="pl-c1"&gt;\institute&lt;/span&gt;{Centre for Modern Beamer Themes}
&lt;span class="pl-c1"&gt;\begin&lt;/span&gt;{&lt;span class="pl-smi"&gt;document&lt;/span&gt;}
  &lt;span class="pl-c1"&gt;\maketitle&lt;/span&gt;
  &lt;span class="pl-c1"&gt;\section&lt;/span&gt;{&lt;span class="pl-en"&gt;First Section&lt;/span&gt;}
  &lt;span class="pl-c1"&gt;\begin&lt;/span&gt;{&lt;span class="pl-smi"&gt;frame&lt;/span&gt;}{First Frame}
    Hello, world!
  &lt;span class="pl-c1"&gt;\end&lt;/span&gt;{&lt;span class="pl-smi"&gt;frame&lt;/span&gt;}
&lt;span class="pl-c1"&gt;\end&lt;/span&gt;{&lt;span class="pl-smi"&gt;document&lt;/span&gt;}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Detailed information on using Metropolis can be found in the &lt;a href="http://mirrors.ctan.org/macros/latex/contrib/beamer-contrib/themes/metropolis/doc/metropolistheme.pdf" rel="nofollow"&gt;manual&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For an alternative dark color theme, please have a look at Ross Churchley's
excellent &lt;a href="https://github.com/rchurchley/beamercolortheme-owl"&gt;owl&lt;/a&gt; theme.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;The theme itself is licensed under a &lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-ShareAlike
4.0 International License&lt;/a&gt;. This
means that if you change the theme and re-distribute it, you &lt;em&gt;must&lt;/em&gt; retain the
copyright notice header and license it under the same CC-BY-SA license. This
does not affect the presentation that you create with the theme.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>matze</author><guid isPermaLink="false">https://github.com/matze/mtheme</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>rstudio/cheatsheets #5 in TeX, This month</title><link>https://github.com/rstudio/cheatsheets</link><description>&lt;p&gt;&lt;i&gt;RStudio Cheat Sheets&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-rstudio-cheat-sheets" class="anchor" aria-hidden="true" href="#rstudio-cheat-sheets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RStudio Cheat Sheets&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="pngs/rstudio-ide.png"&gt;&lt;img src="pngs/rstudio-ide.png" width="364" height="288" align="right" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The cheat sheets make it easy to learn about and use some of our favorite packages. They are published in their respective PDF versions here: &lt;a href="https://www.rstudio.com/resources/cheatsheets/" rel="nofollow"&gt;https://www.rstudio.com/resources/cheatsheets/&lt;/a&gt;, some are also available in the RStudio IDE under Help-Cheatsheets.&lt;/p&gt;
&lt;p&gt;This repository contains the source Apple Keynote files or the current, archived and translated versions.&lt;/p&gt;
&lt;p&gt;The cheat sheets use the creative commons copyright. Please see the LICENSE document for more details.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h2&gt;
&lt;p&gt;If you wish to contribute to this effort by translating a cheat sheet, please feel free to use the source Keynote file. To submit a translation, please use a Pull Request via GitHub or email it to us at &lt;a href="mailto:info@rstudio.com"&gt;info@rstudio.com&lt;/a&gt; with the subject "Translated Cheatsheet".&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tips-for-making-a-new-cheat-sheet" class="anchor" aria-hidden="true" href="#tips-for-making-a-new-cheat-sheet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tips for making a new cheat sheet&lt;/h2&gt;
&lt;p&gt;Keep these tips in mind when creating a new cheat sheet:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;RStudio cheat sheets are hosted at &lt;a href="https://github.com/rstudio/cheatsheets"&gt;https://github.com/rstudio/cheatsheets&lt;/a&gt;. You can submit new cheat sheets to the repository with a pull request.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The files &lt;code&gt;keynotes/0-template.key&lt;/code&gt; and &lt;code&gt;powerpoints/0-template.ppt&lt;/code&gt; are official templates that contain some helpful tips.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tip. You may find it easier to create a new cheat sheet by duplicating the most recent Keynote / Powerpoint cheat sheet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The cheat sheets are not meant to be text documents. Ideally, they are scannable visual aids that use layout and visual mnemonics to help people zoom into the functions they need. As an analogy, think of a cheat sheet as more like a well organized computer menu bar that leads you to a command than a manual that documents each command.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The cheat sheets use a cohesive, black and white printer friendly theme (which is what you see in the sheets), so please stay close to the appearance of the existing sheets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It's already baked into every cheat sheet and the template, but you should include a &lt;a href="https://creativecommons.org/" rel="nofollow"&gt;Creative Commons&lt;/a&gt; Copyright on each side of the sheet to make them easy to repurpose.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Budget more time than you expect to make the sheets. So far, I've found this process to be the least time consuming:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the package web page and any vignettes to identify which functions to include (I try to include anything that doesn't seem trivial.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Organize the functions into meaningful, self-explanatory groups.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Think about how to visualize the purpose of each function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Think about what key mental models, definitions, or explanations the cheat sheet should contain in addition to the functions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sketch out several possible layouts for the sheet. Take care to put the more basic and/or pre-requisite content above and to the left of other content. Try to keep related content on the same side of the page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Type out all of the explanations and function definitions. Lay them out. Verify that everything fits. White space is very important. Use it to make the sheet scannable, even if it means smaller text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Making the visuals is the most time consuming part, so I usually save them for last.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tweak until happy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pay attention to the details!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Final tip: Edit the text to be very concise - rely on diagrams where possible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rstudio</author><guid isPermaLink="false">https://github.com/rstudio/cheatsheets</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>sb2nov/resume #6 in TeX, This month</title><link>https://github.com/sb2nov/resume</link><description>&lt;p&gt;&lt;i&gt;Software developer resume in Latex&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;A single-page, one-column resume for software developers. It uses the base latex templates and fonts to provide ease of use and installation when trying to update the resume. The different sections are clearly documented and custom commands are used to provide consistent formatting. The three main sections in the resume are education, experience, and projects.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-motivation" class="anchor" aria-hidden="true" href="#motivation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Motivation&lt;/h3&gt;
&lt;p&gt;I created this template as managing a resume on Google Docs was hard and changing any formatting was too difficult since it had to be applied in multiple places. Most currently available templates either focus on two columns, or are multiple pages long. I personally found the two-column templates hard to focus while multiple-page resumes were just too long to be used in career fairs.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/resume_preview.png"&gt;&lt;img src="/resume_preview.png" alt="Resume Screenshot" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;Format is MIT but all the data is owned by Sourabh Bajaj.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sb2nov</author><guid isPermaLink="false">https://github.com/sb2nov/resume</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>thunlp/NRLPapers #7 in TeX, This month</title><link>https://github.com/thunlp/NRLPapers</link><description>&lt;p&gt;&lt;i&gt;Must-read papers on network representation learning (NRL) / network embedding (NE)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-must-read-papers-on-nrlne" class="anchor" aria-hidden="true" href="#must-read-papers-on-nrlne"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Must-read papers on NRL/NE.&lt;/h2&gt;
&lt;p&gt;NRL: network representation learning. NE: network embedding.&lt;/p&gt;
&lt;p&gt;Contributed by &lt;a href="http://thunlp.org/~tcc/" rel="nofollow"&gt;Cunchao Tu&lt;/a&gt;, Yuan Yao, Zhengyan Zhang, GanquCui, Hao Wang (BUPT), Changxin Tian (BUPT), Jie Zhou and Cheng Yang (BUPT).&lt;/p&gt;
&lt;p&gt;We release &lt;a href="https://github.com/thunlp/openne"&gt;OpenNE&lt;/a&gt;, an open source toolkit for NE/NRL. This repository provides a standard NE/NRL(Network Representation Learning）training and testing framework. Currently, the implemented models in OpenNE include DeepWalk, LINE, node2vec, GraRep, TADW and GCN.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-content" class="anchor" aria-hidden="true" href="#content"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Content&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#survey-papers"&gt;Survey Papers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#models"&gt;Models&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#bacis-models"&gt;Bacis Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#attributed-network"&gt;Attributed Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dynamic-network"&gt;Dynamic Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#heterogeneous-information-network"&gt;Heterogeneous Information Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bipartite-network"&gt;Bipartite Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#directed-network"&gt;Directed Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-models"&gt;Other Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#applications"&gt;Applications&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#natural-language-processing"&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#knowledge-graph"&gt;Knowledge Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#social-network"&gt;Social Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#graph-clustering"&gt;Graph Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#community-detection"&gt;Community Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#recommendation"&gt;Recommendation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-applications"&gt;Other Applications&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-survey-papers" class="anchor" aria-hidden="true" href="#survey-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Survey Papers&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Representation Learning on Graphs: Methods and Applications.&lt;/strong&gt;
&lt;em&gt;William L. Hamilton, Rex Ying, Jure Leskovec.&lt;/em&gt; IEEE Data(base) Engineering Bulletin 2017. &lt;a href="https://arxiv.org/pdf/1709.05584.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Embedding Techniques, Applications, and Performance: A Survey.&lt;/strong&gt;
&lt;em&gt;Palash Goyal, Emilio Ferrara.&lt;/em&gt; Knowledge Based Systems 2017. &lt;a href="https://arxiv.org/pdf/1705.02801.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications.&lt;/strong&gt;
&lt;em&gt;Hongyun Cai, Vincent W. Zheng, Kevin Chen-Chuan Chang.&lt;/em&gt; TKDE 2017. &lt;a href="https://arxiv.org/pdf/1709.07604.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Representation Learning: A Survey.&lt;/strong&gt;
&lt;em&gt;Daokun Zhang, Jie Yin, Xingquan Zhu, Chengqi Zhang.&lt;/em&gt; IEEE Transactions on Big Data 2018. &lt;a href="https://arxiv.org/pdf/1801.05852.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Tutorial on Network Embeddings.&lt;/strong&gt;
&lt;em&gt;Haochen Chen, Bryan Perozzi, Rami Al-Rfou, Steven Skiena.&lt;/em&gt; arxiv 2018. &lt;a href="https://arxiv.org/pdf/1808.02590.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Representation Learning: An Overview.(In Chinese)&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Cheng Yang, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; 2017. &lt;a href="http://engine.scichina.com/publisher/scp/journal/SSI/47/8/10.1360/N112017-00145" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational inductive biases, deep learning, and graph networks.&lt;/strong&gt;
&lt;em&gt;Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu.&lt;/em&gt; arxiv 2018. &lt;a href="https://arxiv.org/pdf/1806.01261.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-models" class="anchor" aria-hidden="true" href="#models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Models&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-bacis-models" class="anchor" aria-hidden="true" href="#bacis-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Bacis Models&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SepNE: Bringing Separability to Network Embedding.&lt;/strong&gt;
&lt;em&gt;Ziyao Li, Liang Zhang, Guojie Song.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4333" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robust Negative Sampling for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Mohammadreza Armandpour, Patrick Ding, Jianhua Huang, Xia Hu.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.stat.tamu.edu/~armand/R-NS.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Structure and Transfer Behaviors Embedding via Deep Prediction Model.&lt;/strong&gt;
&lt;em&gt;Xin Sun, Zenghui Song, Junyu Dong, Yongbo Yu, Claudia Plant, Christian Böhm.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4436" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Simplifying Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Weinberger.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/wu19e/wu19e.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GMNN: Graph Markov Neural Networks.&lt;/strong&gt;
&lt;em&gt;Meng Qu, Yoshua Bengio, Jian Tang.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/qu19a/qu19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stochastic Blockmodels meet Graph Neural Networks.&lt;/strong&gt;
&lt;em&gt;Nikhil Mehta, Lawrence Carin Duke, Piyush Rai.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/mehta19a/mehta19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Disentangled Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, Wenwu Zhu.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/ma19a/ma19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Position-aware Graph Neural Networks.&lt;/strong&gt;
&lt;em&gt;Jiaxuan You, Rex Ying, Jure Leskovec.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/you19b/you19b.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing.&lt;/strong&gt;
&lt;em&gt;Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Harutyunyan, Greg Ver Steeg, Aram Galstyan.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/abu-el-haija19a/abu-el-haija19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph U-Nets.&lt;/strong&gt;
&lt;em&gt;Hongyang Gao, Shuiwang Ji.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/gao19a/gao19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Attention Graph Pooling.&lt;/strong&gt;
&lt;em&gt;Junhyun Lee, Inyeop Lee, Jaewoo Kang.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/lee19c/lee19c.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking.&lt;/strong&gt;
&lt;em&gt;Aleksandar Bojchevski, Stephan Günnemann.&lt;/em&gt; ICLR 2018. &lt;a href="https://arxiv.org/pdf/1707.03815.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling.&lt;/strong&gt;
&lt;em&gt;Jie Chen, Tengfei Ma, Cao Xiao.&lt;/em&gt; ICLR 2018. &lt;a href="https://arxiv.org/pdf/1801.10247.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Attention Networks.&lt;/strong&gt;
&lt;em&gt;Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio.&lt;/em&gt; ICLR 2018. &lt;a href="https://arxiv.org/pdf/1710.10903.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stochastic Training of Graph Convolutional Networks with Variance Reduction.&lt;/strong&gt;
&lt;em&gt;Jianfei Chen, Jun Zhu, Le Song.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1710.10568.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarially Regularized Graph Autoencoder for Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, Chengqi Zhang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0362.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discrete Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiaobo Shen, Shirui Pan, Weiwei Liu, Yew-Soon Ong, Quan-Sen Sun.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0493.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Hashing for Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Qixiang Wang, Shanfeng Wang, Maoguo Gong, Yue Wu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0390.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Inductive Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Ryan A. Rossi, Rong Zhou, Nesreen K. Ahmed.&lt;/em&gt; WWW 2018. &lt;a href="http://ryanrossi.com/pubs/rossi-et-al-WWW18-BigNet.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Active Discriminative Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Li Gao, Hong Yang, Chuan Zhou, Jia Wu, Shirui Pan, Yue Hu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0296.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MILE: A Multi-Level Framework for Scalable Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Jiongqian Liang, Saket Gurukar, Srinivasan Parthasarathy.&lt;/em&gt; arxiv 2018. &lt;a href="https://arxiv.org/pdf/1802.09612.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Out-of-sample extension of graph adjacency spectral embedding.&lt;/strong&gt;
&lt;em&gt;Keith Levin, Farbod Roosta-Khorasani, Michael W. Mahoney, Carey E. Priebe.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1802.06307.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DeepWalk: Online Learning of Social Representations.&lt;/strong&gt;
&lt;em&gt;Bryan Perozzi, Rami Al-Rfou, Steven Skiena.&lt;/em&gt; KDD 2014. &lt;a href="https://arxiv.org/pdf/1403.6652" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/phanein/deepwalk"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non-transitive Hashing with Latent Similarity Componets.&lt;/strong&gt;
&lt;em&gt;Mingdong Ou, Peng Cui, Fei Wang, Jun Wang, Wenwu Zhu.&lt;/em&gt; KDD 2015. &lt;a href="http://cuip.thumedialab.com/papers/KDD-NonTransitiveHashing.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GraRep: Learning Graph Representations with Global Structural Information.&lt;/strong&gt;
&lt;em&gt;Shaosheng Cao, Wei Lu, Qiongkai Xu.&lt;/em&gt; CIKM 2015. &lt;a href="https://www.researchgate.net/profile/Qiongkai_Xu/publication/301417811_GraRep/links/5847ecdb08ae8e63e633b5f2/GraRep.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/ShelsonCao/GraRep"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LINE: Large-scale Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Me.&lt;/em&gt; WWW 2015. &lt;a href="https://arxiv.org/pdf/1503.03578.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/tangjianpku/LINE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Neural Networks for Learning Graph Representations.&lt;/strong&gt;
&lt;em&gt;Shaosheng Cao, Wei Lu, Xiongkai Xu.&lt;/em&gt; AAAI 2016. &lt;a href="https://pdfs.semanticscholar.org/1a37/f07606d60df365d74752857e8ce909f700b3.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/ShelsonCao/DNGR"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Revisiting Semi-supervised Learning with Graph Embeddings.&lt;/strong&gt;
&lt;em&gt;Zhilin Yang, William W. Cohen, Ruslan Salakhutdinov.&lt;/em&gt; ICML 2016. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/yanga16.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Max-Margin DeepWalk: Discriminative Learning of Network Representation.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; IJCAI 2016. &lt;a href="http://thunlp.org/~tcc/publications/ijcai2016_mmdw.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/mmdw"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discriminative Deep RandomWalk for Network Classification.&lt;/strong&gt;
&lt;em&gt;Juzheng Li, Jun Zhu, Bo Zhang.&lt;/em&gt; ACL 2016. &lt;a href="http://www.aclweb.org/anthology/P16-1095" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structural Deep Network Embedding.&lt;/strong&gt;
&lt;em&gt;Daixin Wang, Peng Cui, Wenwu Zhu.&lt;/em&gt; KDD 2016. &lt;a href="http://cuip.thumedialab.com/papers/SDNE.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structural Neighborhood Based Classification of Nodes in a Network.&lt;/strong&gt;
&lt;em&gt;Sharad Nandanwar, M. N. Murty.&lt;/em&gt; KDD 2016. &lt;a href="http://www.kdd.org/kdd2016/papers/files/Paper_679.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Community Preserving Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, Shiqiang Yang.&lt;/em&gt; AAAI 2017. &lt;a href="http://cuip.thumedialab.com/papers/NE-Community.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised Classification with Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Thomas N. Kipf, Max Welling.&lt;/em&gt; ICLR 2017. &lt;a href="https://arxiv.org/pdf/1609.02907.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/tkipf/gcn"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fast Network Embedding Enhancement via High Order Proximity Approximation.&lt;/strong&gt;
&lt;em&gt;Cheng Yang, Maosong Sun, Zhiyuan Liu, Cunchao Tu.&lt;/em&gt; IJCAI 2017. &lt;a href="http://thunlp.org/~tcc/publications/ijcai2017_neu.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/neu"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CANE: Context-Aware Network Embedding for Relation Modeling.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Han Liu, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; ACL 2017. &lt;a href="http://thunlp.org/~tcc/publications/acl2017_cane.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/cane"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A General View for Network Embedding as Matrix Factorization.&lt;/strong&gt;
&lt;em&gt;Xin Liu, Tsuyoshi Murata, Kyoung-Sook Kim, Chatchawan Kotarasu, Chenyi Zhuang.&lt;/em&gt; WSDM 2019. &lt;a href="https://dl.acm.org/citation.cfm?doid=3289600.3291029" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Co-Embedding Attributed Networks.&lt;/strong&gt;
&lt;em&gt;Zaiqiao Meng, Shangsong Liang, Xiangliang Zhang, Hongyan Bao.&lt;/em&gt; WSDM 2019. &lt;a href="https://mine.kaust.edu.sa/Documents/papers/WSDM19attribute.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enhanced Network Embeddings via Exploiting Edge Labels.&lt;/strong&gt;
&lt;em&gt;Haochen Chen, Xiaofei Sun, Yingtao Tian, Bryan Perozzi, Muhao Chen, Steven Skiena.&lt;/em&gt; CIKM 2018. &lt;a href="https://arxiv.org/pdf/1809.05124.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Improve Network Embeddings with Regularization.&lt;/strong&gt;
&lt;em&gt;Yi Zhang, Jianguo Lu, Ofer Shai.&lt;/em&gt; CIKM 2018. &lt;a href="https://jlu.myweb.cs.uwindsor.ca/n2v.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Modeling Multi-way Relations with Hypergraph Embedding.&lt;/strong&gt;
&lt;em&gt;Chia-An Yu, Ching-Lun Tai, Tak-Shing Chan, Yi-Hsuan Yang.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3269274" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/chia-an/HGE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;REGAL: Representation Learning-based Graph Alignment.&lt;/strong&gt;
&lt;em&gt;Mark Heimann, Haoming Shen, Tara Safavi, Danai Koutra.&lt;/em&gt; CIKM 2018. &lt;a href="https://arxiv.org/pdf/1802.06257.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Network Embedding.&lt;/strong&gt;
&lt;em&gt;Quanyu Dai, Qiang Li, Jian Tang, Dan Wang.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.07838.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/sachinbiradar9/Adverserial-Inductive-Deep-Walk"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bernoulli Embeddings for Graphs.&lt;/strong&gt;
&lt;em&gt;Vinith Misra, Sumit Bhatia.&lt;/em&gt; AAAI 2018. &lt;a href="http://sumitbhatia.net/papers/aaai18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GraphGAN: Graph Representation Learning with Generative Adversarial Nets.&lt;/strong&gt;
&lt;em&gt;Hongwei Wang, jia Wang, jialin Wang, MIAO ZHAO, Weinan Zhang, Fuzheng Zhang, Xie Xing, Minyi Guo.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.08267.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HARP: Hierarchical Representation Learning for Networks.&lt;/strong&gt;
&lt;em&gt;Haochen Chen, Bryan Perozzi, Yifan Hu, Steven Skiena.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1706.07845.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/GTmac/HARP"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Social Rank Regulated Large-scale Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yupeng Gu, Yizhou Sun, Yanen Li, Yang Yang.&lt;/em&gt; WWW 2018. &lt;a href="http://yangy.org/works/ge/rare.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Latent Network Summarization: Bridging Network Embedding and Summarization.&lt;/strong&gt;
&lt;em&gt;Di Jin,Ryan Rossi,Danai Koutra,Eunyee Koh,Sungchul Kim,Anup Rao&lt;/em&gt; KDD 2019. &lt;a href="https://arxiv.org/pdf/1811.04461.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching.&lt;/strong&gt;
&lt;em&gt;Dingqi Yang,Paolo Rosso,Bin Li,Philippe Cudre-Mauroux.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330951/p1162-yang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ProGAN: Network Embedding via Proximity Generative Adversarial Network.&lt;/strong&gt;
&lt;em&gt;Hongchang Gao,Jian Pei,Heng Huang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330866/p1308-gao.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Global Alignment Graph Kernel Using Random Features: From Node Embedding to Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Lingfei Wu,Ian En-Hsu Yen,Zhen Zhang,Kun Xu,Liang Zhao,Xi Peng,Yinglong Xia,Charu Aggarwal.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330918/p1418-wu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Graph Embeddings via Sparse Transpose Proximities.&lt;/strong&gt;
&lt;em&gt;Yuan Yin,Zhewei Wei.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330860/p1429-yin.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AutoNRL: Hyperparameter Optimization for Massive Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Ke Tu,Jianxin Ma,Peng Cui,Jian Pei,Wenwu Zhu.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330848/p216-tu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Representation Learning via Hard and Channel-Wise Attention Networks.&lt;/strong&gt;
&lt;em&gt;Hongyang Gao,Shuiwang Ji.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330897/p741-gao.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Training Methods for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Quanyu Dai,Xiao Shen,Liang Zhang,Qiang Li,Dan Wang.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1908.11514.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-relational Network Embeddings with Relational Proximity and Node Attributes.&lt;/strong&gt;
&lt;em&gt;Ming-Han Feng,Chin-Chi Hsu,Cheng-Te Li,Mi-Yen Yeh,Shou-De Lin.&lt;/em&gt; WWW 2019. &lt;a href="https://pdfs.semanticscholar.org/6274/3cbebc142897c6c005f3c12c00b9202ca43f.pdf?_ga=2.108748866.1527570260.1569422306-1231101604.1568798295" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sampled in Pairs and Driven by Text: A New Graph Embedding Framework.&lt;/strong&gt;
&lt;em&gt;Liheng Chen,Yanru Qu,Zhenghui Wang,Weinan Zhang,Ken Chen,Shaodian Zhang,Yong Yu.&lt;/em&gt; WWW 2019. &lt;a href="https://sci-hub.tw/10.1145/3308558.3313520#" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DDGK: Learning Graph Representations via Deep Divergence Graph Kernels.&lt;/strong&gt;
&lt;em&gt;Rami Al-Rfou,Dustin Zelle,Bryan Perozzi.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1904.09671.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tag2Vec: Learning Tag Representations in Tag Networks.&lt;/strong&gt;
&lt;em&gt;Junshan Wang,Zhicong Lu,Guojia Song,Yue Fan,Lun Du,Wei Lin.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1905.03041.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;struc2vec: Learning Node Representations from Structural Identity.&lt;/strong&gt;
&lt;em&gt;Leonardo F. R. Ribeiro, Pedro H. P. Saverese, Daniel R. Figueiredo.&lt;/em&gt; KDD 2017. &lt;a href="https://arxiv.org/pdf/1704.03165.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inductive Representation Learning on Large Graphs.&lt;/strong&gt;
&lt;em&gt;William L. Hamilton, Rex Ying, Jure Leskovec.&lt;/em&gt; NIPS 2017. &lt;a href="https://arxiv.org/pdf/1706.02216.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Graph Embeddings with Embedding Propagation.&lt;/strong&gt;
&lt;em&gt;Alberto Garcia Duran, Mathias Niepert.&lt;/em&gt; NIPS 2017. &lt;a href="https://arxiv.org/pdf/1710.03059.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enhancing the Network Embedding Quality with Structural Similarity.&lt;/strong&gt;
&lt;em&gt;Tianshu Lyu, Yuan Zhang, Yan Zhang.&lt;/em&gt; CIKM 2017. &lt;a href="https://pdfs.semanticscholar.org/e54a/374d7e24260450e2081b93005a491d1b9116.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;An Attention-based Collaboration Framework for Multi-View Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Meng Qu, Jian Tang, Jingbo Shang, Xiang Ren, Ming Zhang, Jiawei Han.&lt;/em&gt; CIKM 2017. &lt;a href="https://arxiv.org/pdf/1709.06636.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;On Embedding Uncertain Graphs.&lt;/strong&gt;
&lt;em&gt;Jiafeng Hu, Reynold Cheng, Zhipeng Huang, Yixang Fang, Siqiang Luo.&lt;/em&gt; CIKM 2017. &lt;a href="https://i.cs.hku.hk/~zphuang/pub/CIKM17.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec.&lt;/strong&gt;
&lt;em&gt;Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang.&lt;/em&gt; WSDM 2018. &lt;a href="https://arxiv.org/pdf/1710.02971.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Conditional Network Embeddings.&lt;/strong&gt;
&lt;em&gt;Bo Kang, Jefrey Lijffijt, Tijl De Bie.&lt;/em&gt; ICLR 2019. &lt;a href="https://arxiv.org/abs/1805.07544" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Graph Infomax.&lt;/strong&gt;
&lt;em&gt;Petar Veličković, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, R Devon Hjelm.&lt;/em&gt; ICLR 2019. &lt;a href="https://arxiv.org/abs/1809.10341" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Anonymous Walk Embeddings.&lt;/strong&gt;
&lt;em&gt;Sergey Ivanov, Evgeny Burnaev.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1805.11921.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fairwalk: Towards Fair Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Tahleen Rahman, Bartlomiej Surma, Michael Backes, Yang Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://yangzhangalmo.github.io/papers/IJCAI19.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph and Autoencoder Based Feature Extraction for Zero-shot Learning.&lt;/strong&gt;
&lt;em&gt;Yang Liu, Deyan Xie, Quanxue Gao, Jungong Han, Shujian Wang, Xinbo Gao.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0421.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Space Embedding.&lt;/strong&gt;
&lt;em&gt;João Pereira, Evgeni Levin, Erik Stroes, Albert Groen.&lt;/em&gt; IJCAI 2019. &lt;a href="https://arxiv.org/abs/1907.13443" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Arbitrary-Order Proximity Preserved Network Embedding.&lt;/strong&gt;
&lt;em&gt;Ziwei Zhang, Peng Cui, Xiao Wang, Jian Pei, Xuanrong Yao, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="http://cuip.thumedialab.com/papers/NE-ArbitraryProximity.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Variational Network Embedding in Wasserstein Space.&lt;/strong&gt;
&lt;em&gt;Dingyuan Zhu, Peng Cui, Daixin Wang, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="http://cuip.thumedialab.com/papers/NE-DeepVariational.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MEGAN: A Generative Adversarial Network for Multi-View Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yiwei Sun, Suhang Wang, Tsung-Yu Hsieh, Xianfeng Tang, Vasant Honavar.&lt;/em&gt; IJCAI 2019. &lt;a href="https://arxiv.org/abs/1909.01084" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Embedding under Partial Monitoring for Evolving Networks&lt;/strong&gt;
&lt;em&gt;Yu Han, Jie Tang, Qian Chen.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0342.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Embedding with Dual Generation Tasks.&lt;/strong&gt;
&lt;em&gt;Jie Liu, Na Li, Zhicheng He.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0709.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Triplet Enhanced AutoEncoder: Model-free Discriminative Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yao Yang, Haoran Chen, Junming Shao.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0745.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Recursive Network Embedding with Regular Equivalence.&lt;/strong&gt;
&lt;em&gt;Ke Tu, Peng Cui, Xiao Wang, Philip S. Yu, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="http://cuip.thumedialab.com/papers/NE-RegularEquivalence.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Structural Node Embeddings via Diffusion Wavelets.&lt;/strong&gt;
&lt;em&gt;Claire Donnat, Marinka Zitnik, David Hallac, Jure Leskovec.&lt;/em&gt; KDD 2018. &lt;a href="https://arxiv.org/pdf/1710.10321.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Paced Network Embedding.&lt;/strong&gt;
&lt;em&gt;Hongchang Gao, Heng Huang.&lt;/em&gt; KDD 2018. &lt;a href="https://par.nsf.gov/servlets/purl/10074506" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Deep Network Representations with Adversarially Regularized Autoencoders.&lt;/strong&gt;
&lt;em&gt;Wenchao Yu, Cheng Zheng, Wei Cheng, Charu Aggarwal, Dongjin Song, Bo Zong, Haifeng Chen, Wei Wang.&lt;/em&gt; KDD 2018. &lt;a href="https://sites.cs.ucsb.edu/~bzong/doc/kdd-18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Large-Scale Learnable Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Hongyang Gao, Zhengyang Wang, Shuiwang Ji.&lt;/em&gt; KDD 2018. &lt;a href="https://arxiv.org/pdf/1808.03965" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-attributed-network" class="anchor" aria-hidden="true" href="#attributed-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Attributed Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Outlier Aware Network Embedding for Attributed Networks.&lt;/strong&gt;
&lt;em&gt;Sambaran Bandyopadhyay, N. Lokesh, M. N. Murty.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/3763" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Large-Scale Heterogeneous Feature Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Qingquan Song, Fan Yang, Xia Hu.&lt;/em&gt; AAAI 2019. &lt;a href="https://aaai.org/ojs/index.php/AAAI/article/view/4276" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Bayesian Optimization on Attributed Graphs.&lt;/strong&gt;
&lt;em&gt;Jiaxu Cui, Bo Yang, Xia Hu.&lt;/em&gt; AAAI 2019. &lt;a href="https://arxiv.org/pdf/1905.13403.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Efficient Attributed Network Embedding via Recursive Randomized Hashing.&lt;/strong&gt;
&lt;em&gt;Wei Wu, Bin Li, Ling Chen, Chengqi Zhang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0397.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Hongchang Gao, Heng Huang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0467.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ANRL: Attributed Network Representation Learning via Deep Neural Networks.&lt;/strong&gt;
&lt;em&gt;Zhen Zhang, Hongxia Yang, Jiajun Bu, Sheng Zhou, Pinggang Yu, Jianwei Zhang, Martin Ester, Can Wang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0438.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrative Network Embedding via Deep Joint Reconstruction.&lt;/strong&gt;
&lt;em&gt;Di Jin, Meng Ge, Liang Yang, Dongxiao He, Longbiao Wang, Weixiong Zhang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0473.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;node2vec: Scalable Feature Learning for Networks.&lt;/strong&gt;
&lt;em&gt;Aditya Grover, Jure Leskovec.&lt;/em&gt; KDD 2016. &lt;a href="http://www.kdd.org/kdd2016/papers/files/rfp0218-groverA.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/aditya-grover/node2vec"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Representation Learning with Rich Text Information.&lt;/strong&gt;
&lt;em&gt;Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, Edward Y. Chang.&lt;/em&gt; IJCAI 2015. &lt;a href="http://thunlp.org/~yangcheng/publications/ijcai15.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/tadw"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tri-Party Deep Network Representation.&lt;/strong&gt;
&lt;em&gt;Shirui Pan, Jia Wu, Xingquan Zhu, Chengqi Zhang, Yang Wang.&lt;/em&gt; IJCAI 2016. &lt;a href="https://www.ijcai.org/Proceedings/16/Papers/271.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TransNet: Translation-Based Network Representation Learning for Social Relation Extraction.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; IJCAI 2017. &lt;a href="http://thunlp.org/~tcc/publications/ijcai2017_transnet.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/transnet"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PRRE: Personalized Relation Ranking Embedding for Attributed Networks.&lt;/strong&gt;
&lt;em&gt;Sheng Zhou, Hongxia Yang, Xin Wang, Jiajun Bu, Martin Ester, Pinggang Yu, Jianwei Zhang, Can Wang.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3271741" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/zhoushengisnoob/PRRE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RSDNE: Exploring Relaxed Similarity and Dissimilarity from Completely-imbalanced Labels for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Zheng Wang, Xiaojun Ye, Chaokun Wang, YueXin Wu, Changping Wang, Kaiwen Liang.&lt;/em&gt; AAAI 2018. &lt;a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16062/15722" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/zhengwang100/RSDNE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised embedding in attributed networks with outliers.&lt;/strong&gt;
&lt;em&gt;Jiongqian Liang, Peter Jacobs, Jiankai Sun, and Srinivasan Parthasarathy.&lt;/em&gt; SDM 2018. &lt;a href="https://arxiv.org/pdf/1703.08100.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="http://jiongqianliang.com/SEANO/" rel="nofollow"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Representation Learning Framework for Property Graphs.&lt;/strong&gt;
&lt;em&gt;Yifan Hou,Hongzhi Chen,Changji Li,James Cheng,Ming-Chang Yang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330948/p65-hou.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning from Labeled and Unlabeled Vertices in Networks.&lt;/strong&gt;
&lt;em&gt;Wei Ye, Linfei Zhou, Dominik Mautz, Claudia Plant, Christian B?hm.&lt;/em&gt; KDD 2017. &lt;a href="https://dl.acm.org/citation.cfm?id=3098142" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Label Informed Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Jundong Li, Xia Hu.&lt;/em&gt; WSDM 2017. &lt;a href="http://www.public.asu.edu/~jundongl/paper/WSDM17_LANE.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Accelerated Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Jundong Li, Xia Hu.&lt;/em&gt; SDM 2017. &lt;a href="http://www.public.asu.edu/~jundongl/paper/SDM17_AANE.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variation Autoencoder Based Network Representation Learning for Classification.&lt;/strong&gt;
&lt;em&gt;Hang Li, Haozheng Wang, Zhenglu Yang, Masato Odagaki.&lt;/em&gt; ACL 2017. &lt;a href="https://aclweb.org/anthology/P17-3010" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Attributed Signed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Suhang Wang, Charu Aggarwal, Jiliang Tang, Huan Liu.&lt;/em&gt; CIKM 2017. &lt;a href="https://suhangwang.ist.psu.edu/publications/SNEA.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;From Properties to Links: Deep Network Embedding on Incomplete Graphs.&lt;/strong&gt;
&lt;em&gt;Dejian Yang, Senzhang Wang, Chaozhuo Li, Xiaoming Zhang, Zhoujun Li.&lt;/em&gt; CIKM 2017. &lt;a href="https://dl.acm.org/citation.cfm?id=3132847.3132975" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exploring Expert Cognition for Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Qingquan Song, Jundong Li, Xia Ben Hu.&lt;/em&gt; WSDM 2018. &lt;a href="http://www.public.asu.edu/~jundongl/paper/WSDM18_NEEC.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hierarchical Taxonomy Aware Network Embedding.&lt;/strong&gt;
&lt;em&gt;Jianxin Ma, Peng Cui, Xiao Wang, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="https://jianxinma.github.io/assets/NetHiex.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network-Specific Variational Auto-Encoder for Embedding in Attribute Networks.&lt;/strong&gt;
&lt;em&gt;Di Jin, Bingyi Li, Pengfei Jiao, Dongxiao He, Weixiong Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0370.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SPINE: Structural Identity Preserved Inductive Network Embedding.&lt;/strong&gt;
&lt;em&gt;Junliang Guo, Linli Xu, Jingchang Liu.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0333.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Content to Node: Self-translation Network Embedding.&lt;/strong&gt;
&lt;em&gt;Jie Liu, Zhicheng He, Lai Wei, Yalou Huang.&lt;/em&gt; KDD 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3219988" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-dynamic-network" class="anchor" aria-hidden="true" href="#dynamic-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Dynamic Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Network Embedding : An Extended Approach for Skip-gram based Network Embedding.&lt;/strong&gt;
&lt;em&gt;Lun Du, Yun Wang, Guojie Song, Zhicong Lu, Junshan Wang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0288.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Network Embedding by Modeling Triadic Closure Process.&lt;/strong&gt;
&lt;em&gt;Lekui Zhou, Yang Yang, Xiang Ren, Fei Wu, Yueting Zhuang.&lt;/em&gt; AAAI 2018. &lt;a href="http://yangy.org/works/dynamictriad/dynamic_triad.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DepthLGP: Learning Embeddings of Out-of-Sample Nodes in Dynamic Networks.&lt;/strong&gt;
&lt;em&gt;Jianxin Ma, Peng Cui, Wenwu Zhu.&lt;/em&gt; AAAI 2018. &lt;a href="https://jianxinma.github.io/assets/DepthLGP.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TIMERS: Error-Bounded SVD Restart on Dynamic Networks.&lt;/strong&gt;
&lt;em&gt;Ziwei Zhang, Peng Cui, Jian Pei, Xiao Wang, Wenwu Zhu.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.09541.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks.&lt;/strong&gt;
&lt;em&gt;Srijan Kumar,Xikun Zhang,Jure Leskovec.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330895/p1269-kumar.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Attributed Network Embedding for Learning in a Dynamic Environment.&lt;/strong&gt;
&lt;em&gt;Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, Huan Liu.&lt;/em&gt; CIKM 2017. &lt;a href="https://arxiv.org/pdf/1706.01860.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DyRep: Learning Representations over Dynamic Graphs.&lt;/strong&gt;
&lt;em&gt;Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, Hongyuan Zha.&lt;/em&gt; ICLR 2019. &lt;a href="https://openreview.net/forum?id=HyePrhR5KX" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedding Temporal Network via Neighborhood Formation.&lt;/strong&gt;
&lt;em&gt;Yuan Zuo, Guannan Liu, Hao Lin, Jia Guo, Xiaoqian Hu, Junjie Wu.&lt;/em&gt; KDD 2018. &lt;a href="https://zuoyuan.github.io/files/htne_kdd18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Node Embedding over Temporal Graphs.&lt;/strong&gt;
&lt;em&gt;Uriel Singer, Ido Guy, Kira Radinsky.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0640.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Embeddings for User Profiling in Twitter.&lt;/strong&gt;
&lt;em&gt;Shangsong Liang, Xiangliang Zhang, Zhaochun Ren, Evangelos Kanoulas.&lt;/em&gt; KDD 2018. &lt;a href="https://repository.kaust.edu.sa/bitstream/handle/10754/628781/p1764-liang.pdf?sequence=1&amp;amp;isAllowed=y" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NetWalk: A Flexible Deep Embedding Approach for Anomaly Detection in Dynamic Networks.&lt;/strong&gt;
&lt;em&gt;Wenchao Yu, Wei Cheng, Charu Aggarwal, Kai Zhang, Haifeng Chen, Wei Wang.&lt;/em&gt; KDD 2018. &lt;a href="http://www.shichuan.org/hin/topic/Embedding/2018.KDD%202018%20NetWalk_A%20Flexible%20Deep%20Embedding%20Approach%20for%20Anomaly%20Detection%20in%20Dynamic%20Networks.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Optimization for Embedding Highly-Dynamic and Recency-Sensitive Data.&lt;/strong&gt;
&lt;em&gt;Xumin Chen, Peng Cui, Shiqiang Yang.&lt;/em&gt; KDD 2018. &lt;a href="http://pengcui.thumedialab.com/papers/NE-ScalableOptimization.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-heterogeneous-information-network" class="anchor" aria-hidden="true" href="#heterogeneous-information-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Heterogeneous Information Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relation Structure-Aware Heterogeneous Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yuanfu Lu, Chuan Shi, Linmei Hu, Zhiyuan Liu.&lt;/em&gt; AAAI 2019. &lt;a href="https://arxiv.org/abs/1905.08027" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hyperbolic Heterogeneous Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Wang, Yiding Zhang, Chuan Shi.&lt;/em&gt; AAAI 2019. &lt;a href="http://shichuan.org/doc/65.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Latent Representations of Nodes for Classifying in Heterogeneous Social Networks.&lt;/strong&gt;
&lt;em&gt;Yann Jacob, Ludovic Denoyer, Patrick Gallinar.&lt;/em&gt; WSDM 2014. &lt;a href="http://webia.lip6.fr/~gallinar/gallinari/uploads/Teaching/WSDM2014-jacob.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Heterogeneous Network Embedding via Deep Architectures.&lt;/strong&gt;
&lt;em&gt;Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C. Aggarwal, Thomas S. Huang.&lt;/em&gt; KDD 2015. &lt;a href="http://www.ifp.illinois.edu/~chang87/papers/kdd_2015.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;metapath2vec: Scalable Representation Learning for Heterogeneous Networks.&lt;/strong&gt;
&lt;em&gt;Yuxiao Dong, Nitesh V. Chawla, Ananthram Swami.&lt;/em&gt; KDD 2017. &lt;a href="https://www3.nd.edu/~dial/publications/dong2017metapath2vec.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://ericdongyx.github.io/metapath2vec/m2v.html" rel="nofollow"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SHNE: Representation Learning for Semantic-Associated Heterogeneous Networks.&lt;/strong&gt;
&lt;em&gt;Chuxu Zhang, Ananthram Swami, Nitesh V. Chawla.&lt;/em&gt; WSDM 2019. &lt;a href="https://dl.acm.org/citation.cfm?id=3291001" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/chuxuzhang/WSDM2019_SHNE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Are Meta-Paths Necessary?: Revisiting Heterogeneous Graph Embeddings.&lt;/strong&gt;
&lt;em&gt;Rana Hussein, Dingqi Yang, Philippe Cudré-Mauroux.&lt;/em&gt; CIKM 2018. &lt;a href="https://exascale.info/assets/pdf/hussein2018cikm.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Abnormal Event Detection via Heterogeneous Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Shaohua Fan, Chuan Shi, Xiao Wang.&lt;/em&gt; CIKM 2018. &lt;a href="http://shichuan.org/doc/62.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multidimensional Network Embedding with Hierarchical Structures.&lt;/strong&gt;
&lt;em&gt;Yao Ma, Zhaochun Ren, Ziheng Jiang, Jiliang Tang, Dawei Yin.&lt;/em&gt; WSDM 2018. &lt;a href="http://cse.msu.edu/~mayao4/downloads/Multidimensional_Network_Embedding_with_Hierarchical_Structure.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Curriculum Learning for Heterogeneous Star Network Embedding via Deep Reinforcement Learning.&lt;/strong&gt;
&lt;em&gt;Meng Qu, Jian Tang, Jiawei Han.&lt;/em&gt; WSDM 2018. &lt;a href="http://delivery.acm.org/10.1145/3160000/3159711/p468-qu.pdf?ip=203.205.141.123&amp;amp;id=3159711&amp;amp;acc=ACTIVE%20SERVICE&amp;amp;key=39FCDE838982416F%2E39FCDE838982416F%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;amp;__acm__=1519788484_7383495a5c522cbe124e62e4d768f8cc" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generative Adversarial Network based Heterogeneous Bibliographic Network Representation for Personalized Citation Recommendation.&lt;/strong&gt;
&lt;em&gt;J. Han, Xiaoyan Cai, Libin Yang.&lt;/em&gt; AAAI 2018. &lt;a href="https://pdfs.semanticscholar.org/1596/d6487012696ba400fb69904a2c372a08a2be.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distance-aware DAG Embedding for Proximity Search on Heterogeneous Graphs.&lt;/strong&gt;
&lt;em&gt;Zemin Liu, Vincent W. Zheng, Zhou Zhao, Fanwei Zhu, Kevin Chen-Chuan Chang, Minghui Wu, Jing Ying.&lt;/em&gt; AAAI 2018. &lt;a href="https://pdfs.semanticscholar.org/b1cc/127a65c40e71121106d0c663f9b5baf9d6f9.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Representation Learning for Attributed Multiplex Heterogeneous Network.&lt;/strong&gt;
&lt;em&gt;Yukuo Cen,Xu Zou,Jianwei Zhang,Hongxia Yang,Jingren Zhou,Jie Tang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330964/p1358-cen.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Learning on Heterogeneous Information Networks.&lt;/strong&gt;
&lt;em&gt;Binbin Hu,Yuan Fang,Chuan Shi&lt;/em&gt; KDD 2019 &lt;a href="http://delivery.acm.org/10.1145/3340000/3330970/p120-hu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HetGNN: Heterogeneous Graph Neural Network.&lt;/strong&gt;
&lt;em&gt;Chuxu Zhang,Dongjin Song,Chao Huang,Ananthram Swami,Nitesh V. Chawla.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330961/p793-zhang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;IntentGC: a Scalable Graph Convolution Framework Fusing Heterogeneous Information for Recommendation&lt;/strong&gt;
&lt;em&gt;Jun Zhao, Zhou Zhou, Ziyu Guan, Wei Zhao, Ning Wei, Guang Qiu and Xiaofei He.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330686/p2347-zhao.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation.&lt;/strong&gt;
&lt;em&gt;Shaohua Fan, Junxiong Zhu, Xiaotian Han, Chuan Shi, Linmei Hu, Biyu Ma and Yongliang Li.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330673/p2478-fan.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Your Style Your Identity: Leveraging Writing and Photography Styles for Drug Trafficker Identification in Darknet Markets over Attributed Heterogeneous Information Network.&lt;/strong&gt;
&lt;em&gt;Yiming Zhang, Yujie Fan,Wei Song, Shifu HouYanfang Ye, Xin Li,Liang Zhao,Chuan Shi,Jiabin Wang, Qi Xiong.&lt;/em&gt; WWW 2019. &lt;a href="https://www.gwern.net/docs/sr/2019-zhang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HIN2Vec: Explore Meta-paths in Heterogeneous Information Networks for Representation Learning.&lt;/strong&gt;
&lt;em&gt;Tao-yang Fu, Wang-Chien Lee, Zhen Lei.&lt;/em&gt; CIKM 2017. &lt;a href="http://shichuan.org/hin/topic/Embedding/2017.%20CIKM%20HIN2Vec.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction.&lt;/strong&gt;
&lt;em&gt;Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, Qi Liu.&lt;/em&gt; WSDM 2018. &lt;a href="https://arxiv.org/pdf/1712.00732.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ActiveHNE: Active Heterogeneous Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xia Chen, Guoxian Yu, Jun Wang, Carlotta Domeniconi, Zhao Li, Xiangliang Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0294.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unified Embedding Model over Heterogeneous Information Network for Personalized Recommendation.&lt;/strong&gt;
&lt;em&gt;Zekai Wang, Hongzhi Liu, Yingpeng Du, Zhonghai Wu, Xing Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0529.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Easing Embedding Learning by Comprehensive Transcription of Heterogeneous Information Networks.&lt;/strong&gt;
&lt;em&gt;Yu Shi, Qi Zhu, Fang Guo, Chao Zhang, Jiawei Han.&lt;/em&gt; KDD 2018. &lt;a href="https://yu-shi-homepage.github.io/kdd18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PME: Projected Metric Embedding on Heterogeneous Networks for Link Prediction.&lt;/strong&gt;
&lt;em&gt;Hongxu Chen, Hongzhi Yin, Weiqing Wang, Hao Wang, Quoc Viet Hung Nguyen, Xue Li.&lt;/em&gt; KDD 2018. &lt;a href="http://net.pku.edu.cn/daim/hongzhi.yin/papers/KDD18-Hongxu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-bipartite-network" class="anchor" aria-hidden="true" href="#bipartite-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Bipartite Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Collaborative Similarity Embedding for Recommender Systems.&lt;/strong&gt;
&lt;em&gt;Chih-Ming Chen,Chuan-Ju Wang,Ming-Feng Tsai,Yi-Hsuan Yang.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1902.06188.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Node Embeddings in Interaction Graphs.&lt;/strong&gt;
&lt;em&gt;Yao Zhang, Yun Xiong, Xiangnan Kong, Yangyong Zhu.&lt;/em&gt; CIKM 2017. &lt;a href="https://web.cs.wpi.edu/~xkong/publications/papers/cikm17.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hierarchical Representation Learning for Bipartite Graphs.&lt;/strong&gt;
&lt;em&gt;Chong Li, Kunyang Jia, Dan Shen, C.J. Richard Shi, Hongxia Yang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0398.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-directed-network" class="anchor" aria-hidden="true" href="#directed-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Directed Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ATP: Directed Graph Embedding with Asymmetric Transitivity Preservation.&lt;/strong&gt;
&lt;em&gt;Jiankai Sun, Bortik Bandyopadhyay, Armin Bashizade, Jiongqian Liang, P. Sadayappan, Srinivasan Parthasarathy.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/3794" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Asymmetric Transitivity Preserving Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, Wenwu Zhu.&lt;/em&gt; KDD 2016. &lt;a href="http://cuip.thumedialab.com/papers/hoppe.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;"Bridge": Enhanced Signed Directed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yiqi Chen, Tieyun Qian, Huan Liu, Ke Sun.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3271738" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SIDE: Representation Learning in Signed Directed Networks.&lt;/strong&gt;
&lt;em&gt;Junghwan Kim, Haekyu Park, Ji-Eun Lee, U Kang.&lt;/em&gt; WWW 2018. &lt;a href="https://datalab.snu.ac.kr/side/resources/side.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-other-models" class="anchor" aria-hidden="true" href="#other-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Other Models&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Multiplex Network Embedding. （Multiplex Network)&lt;/strong&gt;
&lt;em&gt;Hongming Zhang, Liwei Qiu, Lingling Yi, Yangqiu Song.&lt;/em&gt; IJCAI 2018. &lt;a href="http://www.cse.ust.hk/~yqsong/papers/2018-IJCAI-MultiplexNetworkEmbedding.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structural Deep Embedding for Hyper-Networks. (Hyper-Network)&lt;/strong&gt;
&lt;em&gt;Ke Tu, Peng Cui, Xiao Wang, fei Wang, Wenwu Zhu.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.10146.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Representation Learning for Scale-free Networks. (Scale-free Network)&lt;/strong&gt;
&lt;em&gt;Rui Feng, Yang Yang, Wenjie Hu, Fei Wu, Yueting Zhuang.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.10755.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Co-Regularized Deep Multi-Network Embedding. (Multi-Network)&lt;/strong&gt;
&lt;em&gt;Jingchao Ni, Shiyu Chang, Xiao Liu, Wei Cheng, Haifeng Chen, Dongkuan Xu, Xiang Zhang.&lt;/em&gt; WWW 2018. &lt;a href="https://nijingchao.github.io/paper/www18_dmne.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Joint Link Prediction and Network Alignment via Cross-graph Embedding. (Multi-Network)&lt;/strong&gt;
&lt;em&gt;Xingbo Du, Junchi Yan, Hongyuan Zha.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0312.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DANE: Domain Adaptive Network Embedding. (Multi-Network)&lt;/strong&gt;
&lt;em&gt;Yizhou Zhang, Guojie Song, Lun Du, Shuwen Yang, Yilun Jin.&lt;/em&gt; IJCAI 2019. &lt;a href="https://arxiv.org/abs/1906.00684" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SPARC: Self-Paced Network Representation for Few-Shot Rare Category Characterization. (Few-Shot Learning)&lt;/strong&gt;
&lt;em&gt;Dawei Zhou, Jingrui He, Hongxia Yang, Wei Fan.&lt;/em&gt; KDD 2018. &lt;a href="https://dl.acm.org/authorize?N665885" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-applications" class="anchor" aria-hidden="true" href="#applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Applications&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-natural-language-processing" class="anchor" aria-hidden="true" href="#natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Natural Language Processing&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Personalized Question Routing via Heterogeneous Network Embedding.&lt;/strong&gt;
&lt;em&gt;Zeyu Li, Jyun-Yu Jiang, Yizhou Sun, Wei Wang.&lt;/em&gt; AAAI 2019. &lt;a href="http://web.cs.ucla.edu/~yzsun/papers/2019_AAAI_QR.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks.&lt;/strong&gt;
&lt;em&gt;Jian Tang, Meng Qu, Qiaozhu Mei.&lt;/em&gt; KDD 2015. &lt;a href="https://arxiv.org/pdf/1508.00200.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/mnqu/PTE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-knowledge-graph" class="anchor" aria-hidden="true" href="#knowledge-graph"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Knowledge Graph&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interaction Embeddings for Prediction and Explanation in Knowledge Graphs.&lt;/strong&gt;
&lt;em&gt;Wen Zhang, Bibek Paudel, Wei Zhang, Abraham Bernstein, Huajun Chen.&lt;/em&gt; WSDM 2019. &lt;a href="https://arxiv.org/pdf/1903.04750.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Shared Embedding Based Neural Networks for Knowledge Graph Completion.&lt;/strong&gt;
&lt;em&gt;Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, Xueqi Cheng.&lt;/em&gt; CIKM 2018 &lt;a href="https://dl.acm.org/citation.cfm?id=3271705" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Re-evaluating Embedding-Based Knowledge Graph Completion Methods.&lt;/strong&gt;
&lt;em&gt;Farahnaz Akrami, Lingbing Guo, Wei Hu, Chengkai Li.&lt;/em&gt; CIKM 2018. &lt;a href="http://ranger.uta.edu/~cli/pubs/2018/kgcompletion-cikm18short-akrami.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-social-network" class="anchor" aria-hidden="true" href="#social-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Social Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Learning for Weakly-Supervised Social Network Alignment.&lt;/strong&gt;
&lt;em&gt;Chaozhuo Li, Senzhang Wang, Yukun Wang, Philip Yu, Yanbo Liang, Yun Liu, Zhoujun Li.&lt;/em&gt; AAAI 2019. &lt;a href="https://aaai.org/ojs/index.php/AAAI/article/view/3889" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TransConv: Relationship Embedding in Social Networks.&lt;/strong&gt;
&lt;em&gt;Yi-Yu Lai, Jennifer Neville, Dan Goldwasser.&lt;/em&gt; AAAI 2019. &lt;a href="https://aaai.org/ojs/index.php/AAAI/article/view/4314" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised User Geolocation via Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Afshin Rahimi, Trevor Cohn, Timothy Baldwin.&lt;/em&gt; ACL 2018. &lt;a href="https://arxiv.org/pdf/1804.08049.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MASTER: across Multiple social networks, integrate Attribute and STructure Embedding for Reconciliation.&lt;/strong&gt;
&lt;em&gt;Sen Su, Li Sun, Zhongbao Zhang, Gen Li, Jielun Qu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0537.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MEgo2Vec: Embedding Matched Ego Networks for User Alignment Across Social Networks.&lt;/strong&gt;
&lt;em&gt;Jing Zhang, Bo Chen, Xianming Wang, Hong Chen, Cuiping Li, Fengmei Jin, Guojie Song, Yutao Zhang.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3271705" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Link Prediction via Subgraph Embedding-Based Convex Matrix Completion.&lt;/strong&gt;
&lt;em&gt;Zhu Cao, Linlin Wang, Gerard De melo.&lt;/em&gt; AAAI 2018. &lt;a href="http://iiis.tsinghua.edu.cn/~weblt/papers/link-prediction-subgraphembeddings.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;On Exploring Semantic Meanings of Links for Embedding Social Networks.&lt;/strong&gt;
&lt;em&gt;Linchuan Xu, Xiaokai Wei, Jiannong Cao, Philip S Yu.&lt;/em&gt; WWW 2018. &lt;a href="https://pdfs.semanticscholar.org/ccd3/ede78393628b5f0256ebfccbb4ac293394de.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MCNE: An End-to-End Framework for Learning Multiple Conditional Network Representations of Social Network.&lt;/strong&gt;
&lt;em&gt;Hao Wang,Tong Xu,Qi Liu,Defu Lian,Enhong Chen,Dongfang Du,Han Wu,Wen Su.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330931/p1064-wang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unsupervised Feature Selection in Signed Social Networks.&lt;/strong&gt;
&lt;em&gt;Kewei Cheng, Jundong Li, Huan Liu.&lt;/em&gt; KDD 2017. &lt;a href="http://www.public.asu.edu/~jundongl/paper/KDD17_SignedFS.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Network Embedding with Community Structural Information.&lt;/strong&gt;
&lt;em&gt;Yu Li, Ying Wang, Tingting Zhang, Jiawei Zhang, Yi Chang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0407.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-graph-clustering" class="anchor" aria-hidden="true" href="#graph-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Graph Clustering&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spectral Clustering in Heterogeneous Information Networks.&lt;/strong&gt;
&lt;em&gt;Xiang Li , Ben Kao, Zhaochun Ren, Dawei Yin.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.researchgate.net/profile/Xiang_Li238/publication/332606853_Spectral_Clustering_in_Heterogeneous_Information_Networks/links/5cc035e892851c8d2200aa29/Spectral-Clustering-in-Heterogeneous-Information-Networks.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-view Clustering with Graph Embedding for Connectome Analysis.&lt;/strong&gt;
&lt;em&gt;Guixiang Ma, Lifang He, Chun-Ta Lu, Weixiang Shao, Philip S Yu, Alex D Leow, Ann B Ragin.&lt;/em&gt; CIKM 2017. &lt;a href="https://www.cs.uic.edu/~clu/doc/cikm17_mcge.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Graph Embedding for Ensemble Clustering.&lt;/strong&gt;
&lt;em&gt;Zhiqiang Tao, Hongfu Liu, Jun Li, Zhaowen Wang, Yun Fu.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0494.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variational Graph Embedding and Clustering with Laplacian Eigenmaps.&lt;/strong&gt;
&lt;em&gt;Zitai Chen, Chuan Chen, Zong Zhang, Zibin Zheng, Qingsong Zou.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0297.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-community-detection" class="anchor" aria-hidden="true" href="#community-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Community Detection&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Incorporating Network Embedding into Markov Random Field for Better Community Detection.&lt;/strong&gt;
&lt;em&gt;Di Jin, Xinxin You, Weihao Li, Dongxiao He, Peng Cui, Francoise Fogelman-Soulie, Tanmoy Chakraborty.&lt;/em&gt; AAAI 2019. &lt;a href="http://pengcui.thumedialab.com/papers/NE-MRF.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Unified Framework for Community Detection and Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Xiangkai Zeng, Hao Wang, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun, Bo Zhang, Leyu Lin.&lt;/em&gt; TKDE 2018. &lt;a href="https://ieeexplore.ieee.org/abstract/document/8403293/" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;COSINE: Community-Preserving Social Network Embedding from Information Diffusion Cascades.&lt;/strong&gt;
&lt;em&gt;Yuan Zhang, Tianshu Lyu, Yan Zhang.&lt;/em&gt; AAAI 2018. &lt;a href="https://pdfs.semanticscholar.org/fec8/24c51b59063ba92b66bb7404010954ced5ac.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-facet Network Embedding: Beyond the General Solution of Detection and Representation.&lt;/strong&gt;
&lt;em&gt;Liang Yang, Xiaochun Cao, Yuanfang Guo.&lt;/em&gt; AAAI 2018. &lt;a href="https://yangliang.github.io/pdf/aaai18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Community Detection in Attributed Graphs: An Embedding Approach.&lt;/strong&gt;
&lt;em&gt;Ye Li, Chaofeng Sha, Xin Huang, Yanchun Zhang.&lt;/em&gt; AAAI 2018. &lt;a href="https://www.comp.hkbu.edu.hk/~xinhuang/publications/pdfs/AAAI18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Preserving Proximity and Global Ranking for Node Embedding.&lt;/strong&gt;
&lt;em&gt;Yi-An Lai, Chin-Chi Hsu, Wenhao Chen, Mi-Yen Yeh, Shou-De Lin.&lt;/em&gt; NIPS 2017. &lt;a href="https://pdfs.semanticscholar.org/b692/c82115889115ef3e63fb7e6b23c8eb9c85b3.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Community Embedding with Community Detection and Node Embedding on Graphs.&lt;/strong&gt;
&lt;em&gt;Sandro Cavallari, Vincent W. Zheng, Hongyun Cai, Kevin ChenChuan Chang, Erik Cambria.&lt;/em&gt; CIKM 2017. &lt;a href="https://sentic.net/community-embedding.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-recommendation" class="anchor" aria-hidden="true" href="#recommendation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Recommendation&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Convolutional Neural Networks for Web-Scale Recommender Systems.&lt;/strong&gt;
&lt;em&gt;Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec.&lt;/em&gt; KDD 2018. &lt;a href="https://arxiv.org/pdf/1806.01973.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Is a Single Vector Enough? Exploring Node Polysemy for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Ninghao Liu,Qiaoyu Tan,Yuening Li,Hongxia Yang,Jingren Zhou,Xia Hu.&lt;/em&gt; KDD 2019. &lt;a href="https://arxiv.org/pdf/1905.10668.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender System.&lt;/strong&gt;
&lt;em&gt;Qitian Wu,Hengrui Zhang,Xiaofeng Gao,Peng He,Paul Weng,Han Gao,Guihai Chen.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1903.10433.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-other-applications" class="anchor" aria-hidden="true" href="#other-applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Other Applications&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cash-out User Detection based on Attributed Heterogeneous Information Network with a Hierarchical Attention Mechanism. (Finance)&lt;/strong&gt;
&lt;em&gt;Binbin Hu, Zhiqiang Zhang, Chuan Shi, Jun Zhou, Xiaolong Li, Yuan Qi.&lt;/em&gt; AAAI 2019. &lt;a href="http://shichuan.org/doc/64.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Building Causal Graphs from Medical Literature and Electronic Medical Records. (Medicine)&lt;/strong&gt;
&lt;em&gt;Galia Nordon, Gideon Koren, Varda Shalev, Benny Kimelfeld, Uri Shalit, Kira Radinsky.&lt;/em&gt; AAAI 2019. &lt;a href="http://www.kiraradinsky.com/files/aaai-building-causal.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Attacks on Node Embeddings via Graph Poisoning. (Adversarial)&lt;/strong&gt;
&lt;em&gt;Aleksandar Bojchevski, Stephan Günnemann.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/bojchevski19a/bojchevski19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Compositional Fairness Constraints for Graph Embeddings. (Adversarial)&lt;/strong&gt;
&lt;em&gt;Avishek Bose, William Hamilton.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/bose19a/bose19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gromov-Wasserstein Learning for Graph Matching and Node Embedding. (Graph Matching)&lt;/strong&gt;
&lt;em&gt;Hongteng Xu, Dixin Luo, Hongyuan Zha, Lawrence Carin Duke.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/xu19b/xu19b.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Matching Networks for Learning the Similarity of Graph Structured Objects. (Graph Matching)&lt;/strong&gt;
&lt;em&gt;Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, Pushmeet Kohli.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/li19d/li19d.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MolGAN: An implicit generative model for small molecular graphs. (Molecular Generation)&lt;/strong&gt;
&lt;em&gt;Nicola De Cao, Thomas Kipf.&lt;/em&gt; ICML Workshop 2018. &lt;a href="https://arxiv.org/pdf/1805.11973.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational recurrent neural networks. (Relational Reasoning)&lt;/strong&gt;
&lt;em&gt;Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap.&lt;/em&gt; NeurIPS 2018. &lt;a href="https://arxiv.org/pdf/1806.01822.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Constructing Narrative Event Evolutionary Graph for Script Event Prediction. (Script Event Prediction)&lt;/strong&gt;
&lt;em&gt;Zhongyang Li, Xiao Ding, Ting Liu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://arxiv.org/abs/1805.05081" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/eecrazy/ConstructingNEEG_IJCAI_2018"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Network-embedding Based Method for Author Disambiguation. (Author Disambiguation)&lt;/strong&gt;
&lt;em&gt;Jun Xu, Siqi Shen, Dongsheng Li, Yongquan Fu.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3269272" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Graph Embedding for Ranking Optimization in E-commerce.(E-commerce)&lt;/strong&gt;
&lt;em&gt;Chen Chu, Zhao Li, Beibei Xin, Fengchao Peng, Chuanren Liu, Remo Rohs, Qiong Luo, Jingren Zhou.&lt;/em&gt; CIKM 2018. &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6330176/" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Network-to-Network Model for Content-rich Network Embedding.&lt;/strong&gt;
&lt;em&gt;Zhicheng He,Jie Liu,Na Li,Yalou Huang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330924/p1037-he.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unifying Inter-region Autocorrelation and Intra-region Structures for Spatial Embedding via Collective Adversarial Learning.&lt;/strong&gt;
&lt;em&gt;Yunchao Zhang,Pengyang Wang,Xiaolin Li,Yu Zheng,Yanjie Fu.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330972/p1700-zhang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Neural IR Meets Graph Embedding: A Ranking Model for Product Search.&lt;/strong&gt;
&lt;em&gt;Yuan Zhang,Dong Wang,Yan Zhang.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1901.08286.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-Network Embedding for Multi-Network Alignment.&lt;/strong&gt;
&lt;em&gt;Xiaokai Chu,Xinxin Fan,Di Yao,Zhihua Zhu,Jianhui Huang,Jingping Bi.&lt;/em&gt; WWW 2019. &lt;a href="https://sci-hub.tw/10.1145/3308558.3313499#" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Name Disambiguation in Anonymized Graphs using Network Embedding. (Name Disambiguation)&lt;/strong&gt;
&lt;em&gt;Baichuan Zhang, Mohammad Al Hasan.&lt;/em&gt; CIKM 2017. &lt;a href="https://arxiv.org/pdf/1702.02287.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NetGAN: Generating Graphs via Random Walks. (Graph Generation)&lt;/strong&gt;
&lt;em&gt;Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zügner, Stephan Günnemann.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1803.00816" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Networks as Learnable Physics Engines for Inference and Control. (Physics)&lt;/strong&gt;
&lt;em&gt;Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1806.01242.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational inductive bias for physical construction in humans and machines. (Human physical reasoning)&lt;/strong&gt;
&lt;em&gt;Jessica B. Hamrick, Kelsey R. Allen, Victor Bapst, Tina Zhu, Kevin R. McKee, Joshua B. Tenenbaum, Peter W. Battaglia.&lt;/em&gt; CogSci 2018. &lt;a href="https://arxiv.org/pdf/1806.01203.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>thunlp</author><guid isPermaLink="false">https://github.com/thunlp/NRLPapers</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>Wandmalfarbe/pandoc-latex-template #8 in TeX, This month</title><link>https://github.com/Wandmalfarbe/pandoc-latex-template</link><description>&lt;p&gt;&lt;i&gt;A pandoc LaTeX template to convert markdown files to PDF or LaTeX.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="icon.png"&gt;&lt;img src="icon.png" align="right" height="110" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-eisvogel" class="anchor" aria-hidden="true" href="#eisvogel"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eisvogel&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/Wandmalfarbe/pandoc-latex-template" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b87755780c096b231f6f2e8a6192d30318d187a3/68747470733a2f2f7472617669732d63692e6f72672f57616e646d616c66617262652f70616e646f632d6c617465782d74656d706c6174652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/Wandmalfarbe/pandoc-latex-template.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A clean &lt;strong&gt;pandoc LaTeX template&lt;/strong&gt; to convert your markdown files to PDF or LaTeX. It is designed for lecture notes and exercises with a focus on computer science. The template is compatible with pandoc 2.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A custom title page&lt;/th&gt;
&lt;th align="center"&gt;A basic example page&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/custom-titlepage/custom-titlepage.pdf"&gt;&lt;img src="examples/custom-titlepage/custom-titlepage.png" alt="A custom title page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/basic-example/basic-example.pdf"&gt;&lt;img src="examples/basic-example/basic-example.png" alt="A basic example page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install pandoc from &lt;a href="http://pandoc.org/" rel="nofollow"&gt;http://pandoc.org/&lt;/a&gt;. You also need to install &lt;a href="https://en.wikibooks.org/wiki/LaTeX/Installation#Distributions" rel="nofollow"&gt;LaTeX&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download the latest version of the Eisvogel template from &lt;a href="https://github.com/Wandmalfarbe/pandoc-latex-template/releases/latest"&gt;the release page&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extract the downloaded ZIP archive and open the folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Move the template &lt;code&gt;eisvogel.tex&lt;/code&gt; to your pandoc templates folder and rename the file to &lt;code&gt;eisvogel.latex&lt;/code&gt;. The location of the templates folder depends on your operating system:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unix, Linux, macOS: &lt;code&gt;/Users/USERNAME/.local/share/pandoc/templates/&lt;/code&gt; or &lt;code&gt;/Users/USERNAME/.pandoc/templates/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Windows Vista or later: &lt;code&gt;C:\Users\USERNAME\AppData\Roaming\pandoc\templates\&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If there are no folders called &lt;code&gt;templates&lt;/code&gt; or &lt;code&gt;pandoc&lt;/code&gt; you need to create them and put the template &lt;code&gt;eisvogel.latex&lt;/code&gt; inside. You can find the default user data directory on your system by looking at the output of &lt;code&gt;pandoc --version&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Open the terminal and navigate to the folder where your markdown file is located.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Execute the following command&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --from markdown --template eisvogel --listings&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where &lt;code&gt;example.md&lt;/code&gt; is the markdown file you want to convert to PDF.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to have nice headers and footers you need to supply metadata to your document. You can do that with a &lt;a href="http://pandoc.org/MANUAL.html#extension-yaml_metadata_block" rel="nofollow"&gt;YAML metadata block&lt;/a&gt; at the top of your markdown document (see the &lt;a href="examples/basic-example/basic-example.md"&gt;example markdown file&lt;/a&gt;). Your markdown document may look like the following:&lt;/p&gt;
&lt;div class="highlight highlight-source-gfm"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;---&lt;/span&gt;
&lt;span class="pl-ent"&gt;title&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;The Document Title&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-ent"&gt;author&lt;/span&gt;: &lt;span class="pl-s"&gt;[Example Author, Another Author]&lt;/span&gt;
&lt;span class="pl-ent"&gt;date&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;2017-02-20&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-ent"&gt;keywords&lt;/span&gt;: &lt;span class="pl-s"&gt;[Markdown, Example]&lt;/span&gt;
&lt;span class="pl-c"&gt;...&lt;/span&gt;

Here is the actual document text...&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-custom-template-variables" class="anchor" aria-hidden="true" href="#custom-template-variables"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Template Variables&lt;/h3&gt;
&lt;p&gt;This template defines some new variables to control the appearance of the resulting PDF document. The existing template variables from pandoc are all supported and their documentation can be found in &lt;a href="https://pandoc.org/MANUAL.html#variables-for-latex" rel="nofollow"&gt;the pandoc manual&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;turns on the title page when &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-color&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the background color of the title page. The color value must be given as an HTML hex color like &lt;code&gt;D8DE2C&lt;/code&gt; without the leading number sign (&lt;code&gt;#&lt;/code&gt;). When specifying the color in YAML, it is advisable to enclose it in quotes like so &lt;code&gt;titlepage-color: "D8DE2C"&lt;/code&gt; to avoid the truncation of the color (e.g. &lt;code&gt;000000&lt;/code&gt; becoming &lt;code&gt;0&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-text-color&lt;/code&gt; (defaults to &lt;code&gt;5F5F5F&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the text color of the title page&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-rule-color&lt;/code&gt; (defaults to &lt;code&gt;435488&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the color of the rule on the top of the title page&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-rule-height&lt;/code&gt; (defaults to &lt;code&gt;4&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the height of the rule on the top of the title page (in points)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;titlepage-background&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the path to a background image for the title page. The background image is scaled to cover the entire page. In the examples folder under &lt;code&gt;titlepage-background&lt;/code&gt; are a few example background images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;caption-justification&lt;/code&gt; (defaults to &lt;code&gt;raggedright&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;justification setting for captions (uses the &lt;code&gt;justification&lt;/code&gt; parameter of the &lt;a href="https://ctan.org/pkg/caption?lang=en" rel="nofollow"&gt;caption&lt;/a&gt; package)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;toc-own-page&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;begin new page after table of contents, when &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;listings-disable-line-numbers&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;disables line numbers for all listings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;listings-no-page-break&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;avoid page break inside listings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;disable-header-and-footer&lt;/code&gt; (default to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;disables the header and footer completely on all pages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;header-left&lt;/code&gt; (defaults to the title)&lt;/p&gt;
&lt;p&gt;the text on the left side of the header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;header-center&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the text in the center of the header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;header-right&lt;/code&gt; (defaults to the date)&lt;/p&gt;
&lt;p&gt;the text on the right side of the header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footer-left&lt;/code&gt; (defaults to the author)&lt;/p&gt;
&lt;p&gt;the text on the left side of the footer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footer-center&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;the text in the center of the footer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footer-right&lt;/code&gt; (defaults to the page number)&lt;/p&gt;
&lt;p&gt;the text on the right side of the footer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footnotes-pretty&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;prettifies formatting of footnotes (requires package &lt;code&gt;footmisc&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;footnotes-disable-backlinks&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;disables making the reference from the footnote at the bottom of the page into a link back to the occurence of the footnote in the main text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;book&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;typeset as book&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;logo&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;path to an image that will be displayed on the title page. The path is always relative to where pandoc is executed. The option &lt;code&gt;--resource-path&lt;/code&gt; has no effect.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;logo-width&lt;/code&gt; (defaults to &lt;code&gt;100&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;the width of the logo (in points)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;first-chapter&lt;/code&gt; (defaults to &lt;code&gt;1&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;if typesetting a book with chapter numbers, specifies the number that will be assigned to the first chapter&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;float-placement-figure&lt;/code&gt; (defaults to &lt;code&gt;H&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;Reset the default placement specifier for figure environments to the supplied value e.g. &lt;code&gt;htbp&lt;/code&gt;. The available specifiers are listed below. The first four placement specifiers can be combined.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;h&lt;/code&gt;: Place the float &lt;em&gt;here&lt;/em&gt;, i.e., approximately at the same point it occurs in the source text.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;t&lt;/code&gt;: Place the float at the &lt;em&gt;top&lt;/em&gt; of the page.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt;: Place the float at the &lt;em&gt;bottom&lt;/em&gt; of the page.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;p&lt;/code&gt;: Place the float on the next &lt;em&gt;page&lt;/em&gt; that will contain only floats like figures and tables.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;H&lt;/code&gt;: Place the float &lt;em&gt;HERE&lt;/em&gt; (exactly where it occurs in the source text). The &lt;code&gt;H&lt;/code&gt; specifier is provided by the &lt;a href="https://ctan.org/pkg/float" rel="nofollow"&gt;float package&lt;/a&gt; and may not be used in conjunction with any other placement specifiers.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;table-use-row-colors&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;enables row colors for tables. The default value is &lt;code&gt;false&lt;/code&gt; because the coloring extends beyond the edge of the table and there is currently no way to change that.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;code-block-font-size&lt;/code&gt; (defaults to &lt;code&gt;\small&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;LaTeX command to change the font size for code blocks. The available values are &lt;code&gt;\tiny&lt;/code&gt;, &lt;code&gt;\scriptsize&lt;/code&gt;, &lt;code&gt;\footnotesize&lt;/code&gt;, &lt;code&gt;\small&lt;/code&gt;, &lt;code&gt;\normalsize&lt;/code&gt;, &lt;code&gt;\large&lt;/code&gt;, &lt;code&gt;\Large&lt;/code&gt;, &lt;code&gt;\LARGE&lt;/code&gt;, &lt;code&gt;\huge&lt;/code&gt; and &lt;code&gt;\Huge&lt;/code&gt;. This option will change the font size for default code blocks using the verbatim environment and for code blocks generated with listings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-numbered-sections" class="anchor" aria-hidden="true" href="#numbered-sections"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Numbered Sections&lt;/h3&gt;
&lt;p&gt;For PDFs with &lt;a href="http://pandoc.org/MANUAL.html#options-affecting-specific-writers" rel="nofollow"&gt;numbered sections&lt;/a&gt; use the &lt;code&gt;--number-sections&lt;/code&gt; or &lt;code&gt;-N&lt;/code&gt; option.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --number-sections&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-syntax-highlighting-with-listings" class="anchor" aria-hidden="true" href="#syntax-highlighting-with-listings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syntax Highlighting with Listings&lt;/h3&gt;
&lt;p&gt;You can get syntax highlighting of delimited code blocks by using the LaTeX package listings with the option &lt;code&gt;--listings&lt;/code&gt;. This example will produce the same syntax highlighting as in the example PDF.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --listings&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-syntax-highlighting-without-listings" class="anchor" aria-hidden="true" href="#syntax-highlighting-without-listings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syntax Highlighting Without Listings&lt;/h3&gt;
&lt;p&gt;The following examples show &lt;a href="http://pandoc.org/MANUAL.html#syntax-highlighting" rel="nofollow"&gt;syntax highlighting of delimited code blocks&lt;/a&gt; without using listings. To see a list of all the supported highlight styles, type &lt;code&gt;pandoc --list-highlight-styles&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style pygments&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style kate&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style espresso&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style tango&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-standalone-latex-document" class="anchor" aria-hidden="true" href="#standalone-latex-document"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Standalone LaTeX Document&lt;/h3&gt;
&lt;p&gt;To produce a standalone LaTeX document for compiling with any LaTeX editor use &lt;code&gt;.tex&lt;/code&gt; as an output file extension.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.tex --template eisvogel&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-changing-the-document-language" class="anchor" aria-hidden="true" href="#changing-the-document-language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changing the Document Language&lt;/h3&gt;
&lt;p&gt;The default language of this template is American English. The &lt;code&gt;lang&lt;/code&gt; variable identifies the main language of the document, using a code according to &lt;a href="https://tools.ietf.org/html/bcp47" rel="nofollow"&gt;BCP 47&lt;/a&gt; (e.g. &lt;code&gt;en&lt;/code&gt; or &lt;code&gt;en-GB&lt;/code&gt;). For an incomplete list of the supported language codes see &lt;a href="http://mirrors.ctan.org/language/hyph-utf8/doc/generic/hyph-utf8/hyph-utf8.pdf" rel="nofollow"&gt;the documentation for the hyph-utf8 package (Section 2)&lt;/a&gt;. The following example changes the language to British English:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel -V lang=en-GB&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The following example changes the language to German:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pandoc example.md -o example.pdf --template eisvogel -V lang=de&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-typesetting-a-book" class="anchor" aria-hidden="true" href="#typesetting-a-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Typesetting a Book&lt;/h3&gt;
&lt;p&gt;To typeset a book supply the template variable &lt;code&gt;-V book&lt;/code&gt; from the command line or via &lt;code&gt;book: true&lt;/code&gt; in the metadata.&lt;/p&gt;
&lt;p&gt;To get the correct chapter headings you need to tell pandoc that it should convert first level headings (indicated by one &lt;code&gt;#&lt;/code&gt; in markdown) to chapters with the command line option &lt;code&gt;--top-level-division=chapter&lt;/code&gt;. Chapter numbers start at 1. If you need to change that, specify &lt;code&gt;first-chapter&lt;/code&gt; in the template variables.&lt;/p&gt;
&lt;p&gt;There will be one blank page before each chapter because the template is two-sided per default. So if you plan to publish your book as a PDF and don’t need a blank page you should add the class option &lt;code&gt;onesided&lt;/code&gt; which can be done by supplying a template variable &lt;code&gt;-V classoption=oneside&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-example-images" class="anchor" aria-hidden="true" href="#example-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example Images&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A green title page&lt;/th&gt;
&lt;th align="center"&gt;A background image on the title page&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/green-titlepage/green-titlepage.pdf"&gt;&lt;img src="examples/green-titlepage/green-titlepage.png" alt="A green title page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/titlepage-background/titlepage-background.pdf"&gt;&lt;img src="examples/titlepage-background/titlepage-background.png" alt="A background image on the title page" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;images and tables&lt;/th&gt;
&lt;th align="center"&gt;Code blocks styled without listings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/images-and-tables/images-and-tables.pdf"&gt;&lt;img src="examples/images-and-tables/images-and-tables.png" alt="images and tables" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/without-listings/without-listings.pdf"&gt;&lt;img src="examples/without-listings/without-listings.png" alt="Code blocks styled without listings" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;A book&lt;/th&gt;
&lt;th align="center"&gt;Code blocks styled with listings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="examples/book/book.pdf"&gt;&lt;img src="examples/book/book.png" alt="A book" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="examples/listings/listings.pdf"&gt;&lt;img src="examples/listings/listings.png" alt="Code blocks styled with listings" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-credits" class="anchor" aria-hidden="true" href="#credits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credits&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This template includes code for styling block quotations from &lt;a href="https://github.com/aaronwolen/pandoc-letter"&gt;pandoc-letter&lt;/a&gt; by &lt;a href="https://github.com/aaronwolen"&gt;Aaron Wolen&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This project is open source licensed under the BSD 3-Clause License. Please see the &lt;a href="LICENSE"&gt;LICENSE file&lt;/a&gt; for more information.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Wandmalfarbe</author><guid isPermaLink="false">https://github.com/Wandmalfarbe/pandoc-latex-template</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>souzatharsis/open-quant-live-book #9 in TeX, This month</title><link>https://github.com/souzatharsis/open-quant-live-book</link><description>&lt;p&gt;&lt;i&gt;An open source, hands-on and fully reproducible book in quantitative finance, data science and econophysics. Join us and help Make Wall Street Great Again!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-the-open-quant-live-book-initiative" class="anchor" aria-hidden="true" href="#the-open-quant-live-book-initiative"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Open Quant Live Book Initiative&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/souzatharsis/open-quant-live-book"&gt;&lt;img src="https://camo.githubusercontent.com/60ab6b2f3876d16ae07e3cfee664cd8d4c137f86/687474703a2f2f6769746875626261646765732e636f6d2f737461722e7376673f757365723d736f757a6174686172736973267265706f3d6f70656e2d7175616e742d6c6976652d626f6f6b267374796c653d64656661756c74" alt="star this repo" data-canonical-src="http://githubbadges.com/star.svg?user=souzatharsis&amp;amp;repo=open-quant-live-book&amp;amp;style=default" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/souzatharsis/open-quant-live-book/fork"&gt;&lt;img src="https://camo.githubusercontent.com/28abf9a95fdeed649925440adc7eba97de7f5dd7/687474703a2f2f6769746875626261646765732e636f6d2f666f726b2e7376673f757365723d736f757a6174686172736973267265706f3d6f70656e2d7175616e742d6c6976652d626f6f6b267374796c653d64656661756c74" alt="fork this repo" data-canonical-src="http://githubbadges.com/fork.svg?user=souzatharsis&amp;amp;repo=open-quant-live-book&amp;amp;style=default" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c3b9e68f54db5d8d79569ee9c9bab0ebd05d8572/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d434325323042592d2d4e432d2d5341253230342e302d6c69676874677265792e737667" alt="License: CC BY-NC-SA 4.0" data-canonical-src="https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="./fig/cover.jpg"&gt;&lt;img src="./fig/cover.jpg" alt="The Open Quant Book" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description&lt;/h3&gt;
&lt;p&gt;The book aims to be an Open Source introductory reference of the most important aspects of financial data analysis, algo trading, portfolio selection, econophysics and machine learning in finance with an emphasis in reproducibility and openness not to be found in most other typical Wall Street-like references.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contribute" class="anchor" aria-hidden="true" href="#contribute"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribute&lt;/h3&gt;
&lt;p&gt;The Book is Open and we welcome co-authors. Feel free to &lt;a href="https://www.openquants.com/contact" rel="nofollow"&gt;reach out&lt;/a&gt; or simply create a pull request with your contribution! See project structure, guidelines and how to contribute &lt;a href="https://github.com/souzatharsis/open-quant-live-book/blob/master/CONTRIBUTING.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-working-contents" class="anchor" aria-hidden="true" href="#working-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Working Contents&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The Basics&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Free Data for Markets&lt;/li&gt;
&lt;li&gt;Stylized Facts&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Algo Trading&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Investment Process&lt;/li&gt;
&lt;li&gt;Backtesting&lt;/li&gt;
&lt;li&gt;Trading Strategies&lt;/li&gt;
&lt;li&gt;Factor Investing&lt;/li&gt;
&lt;li&gt;Limit Order&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Portfolio Optimization&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Convex Optimization&lt;/li&gt;
&lt;li&gt;Risk Parity Portfolios&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Machine Learning&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Intro&lt;/li&gt;
&lt;li&gt;Agent-Based Models&lt;/li&gt;
&lt;li&gt;Binary Classifiers&lt;/li&gt;
&lt;li&gt;Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Deep Learning&lt;/li&gt;
&lt;li&gt;Hierarchical Risk Parity&lt;/li&gt;
&lt;li&gt;AutoML&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Econophysics&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Entropy, Efficiency and Bubbles&lt;/li&gt;
&lt;li&gt;Nonparametric Statistical Causality: An Information-Theoretical Approach&lt;/li&gt;
&lt;li&gt;Fractals and Scaling Laws&lt;/li&gt;
&lt;li&gt;Financial Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;Alternative Data&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;The Market, The Players, The Rules&lt;/li&gt;
&lt;li&gt;Case Studies&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-books-information" class="anchor" aria-hidden="true" href="#books-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Book's information&lt;/h3&gt;
&lt;p&gt;Website: &lt;a href="http://www.openquants.com/" rel="nofollow"&gt;http://www.openquants.com/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Licensed under &lt;a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nofollow"&gt;Attribution-NonCommercial-ShareAlike 4.0 International&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="fig/by-nc-sa.png"&gt;&lt;img src="fig/by-nc-sa.png" width="20%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Copyright (c) 2019. OpenQuants.com, New York, NY.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://patreon.com/openquants" rel="nofollow"&gt;&lt;img src="https://github.com/souzatharsis/open-quant-live-book/raw/master/fig/patreon.png" alt="Become a patron" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>souzatharsis</author><guid isPermaLink="false">https://github.com/souzatharsis/open-quant-live-book</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>riscv/riscv-isa-manual #10 in TeX, This month</title><link>https://github.com/riscv/riscv-isa-manual</link><description>&lt;p&gt;&lt;i&gt;RISC-V Instruction Set Manual&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-risc-v-instruction-set-manual-" class="anchor" aria-hidden="true" href="#risc-v-instruction-set-manual-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RISC-V Instruction Set Manual &lt;a href="https://travis-ci.org/riscv/riscv-isa-manual" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6602f9c5da5da02de1b6abd2060b3ef28fd55806/68747470733a2f2f7472617669732d63692e6f72672f72697363762f72697363762d6973612d6d616e75616c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/riscv/riscv-isa-manual.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;This repository contains the LaTeX source for the draft RISC-V Instruction Set
Manual.  At the time of this writing, none of these specifications have been
formally adopted by the RISC-V Foundation.&lt;/p&gt;
&lt;p&gt;This work is licensed under a Creative Commons Attribution 4.0 International
License.  See the LICENSE file for details.&lt;/p&gt;
&lt;p&gt;The Manual is split up into the following volumes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Volume I: User-Level ISA&lt;/li&gt;
&lt;li&gt;Volume II: Privileged Architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Official versions&lt;/strong&gt; of the specifications are available at
&lt;a href="https://riscv.org/specifications/" rel="nofollow"&gt;https://riscv.org/specifications/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Compiled versions of the most recent drafts&lt;/strong&gt; of the specifications are available at
&lt;a href="https://github.com/riscv/riscv-isa-manual/releases/latest"&gt;https://github.com/riscv/riscv-isa-manual/releases/latest&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Older official versions&lt;/strong&gt; of the specifications are available at
&lt;a href="https://github.com/riscv/riscv-isa-manual/releases/tag/archive"&gt;https://github.com/riscv/riscv-isa-manual/releases/tag/archive&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The canonical list of &lt;strong&gt;open-source RISC-V implementations' marchid CSR values&lt;/strong&gt;
is available at &lt;a href="https://github.com/riscv/riscv-isa-manual/blob/master/marchid.md"&gt;https://github.com/riscv/riscv-isa-manual/blob/master/marchid.md&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>riscv</author><guid isPermaLink="false">https://github.com/riscv/riscv-isa-manual</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>jikexueyuanwiki/tensorflow-zh #11 in TeX, This month</title><link>https://github.com/jikexueyuanwiki/tensorflow-zh</link><description>&lt;p&gt;&lt;i&gt;谷歌全新开源人工智能系统TensorFlow官方文档中文版&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-官方文档中文版" class="anchor" aria-hidden="true" href="#tensorflow-官方文档中文版"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 官方文档中文版&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="SOURCE/images/TensorFlow.jpg"&gt;&lt;img src="SOURCE/images/TensorFlow.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-你正在阅读的项目可能会比-android-系统更加深远地影响着世界" class="anchor" aria-hidden="true" href="#你正在阅读的项目可能会比-android-系统更加深远地影响着世界"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;你正在阅读的项目可能会比 Android 系统更加深远地影响着世界！&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-缘起" class="anchor" aria-hidden="true" href="#缘起"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;缘起&lt;/h2&gt;
&lt;p&gt;2015年11月9日，Google发布人工智能系统TensorFlow并宣布开源，同日，极客学院组织在线TensorFlow中文文档翻译。&lt;/p&gt;
&lt;p&gt;机器学习作为人工智能的一种类型，可以让软件根据大量的数据来对未来的情况进行阐述或预判。如今，领先的科技巨头无不在机器学习下予以极大投入。Facebook、苹果、微软，甚至国内的百度。Google 自然也在其中。「TensorFlow」是 Google 多年以来内部的机器学习系统。如今，Google 正在将此系统成为开源系统，并将此系统的参数公布给业界工程师、学者和拥有大量编程能力的技术人员，这意味着什么呢？&lt;/p&gt;
&lt;p&gt;打个不太恰当的比喻，如今 Google 对待 TensorFlow 系统，有点类似于该公司对待旗下移动操作系统 Android。如果更多的数据科学家开始使用 Google 的系统来从事机器学习方面的研究，那么这将有利于 Google 对日益发展的机器学习行业拥有更多的主导权。&lt;/p&gt;
&lt;p&gt;为了让国内的技术人员在最短的时间内迅速掌握这一世界领先的 AI 系统，极客学院 Wiki 团队发起对 TensorFlow 官方文档的中文协同翻译，一周之内，全部翻译认领完成，一个月后，全部30章节翻译校对完成，上线极客学院Wiki平台并提供下载。&lt;/p&gt;
&lt;p&gt;Google TensorFlow项目负责人Jeff Dean为该中文翻译项目回信称："&lt;em&gt;看到能够将TensorFlow翻译成中文我非常激动，我们将TensorFlow开源的主要原因之一是为了让全世界的人们能够从机器学习与人工智能中获益，类似这样的协作翻译能够让更多的人更容易地接触到TensorFlow项目，很期待接下来该项目在全球范围内的应用!&lt;/em&gt;"&lt;/p&gt;
&lt;p&gt;Jeff回信原文：&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="SOURCE/images/jeff.png"&gt;&lt;img src="SOURCE/images/jeff.png" alt="jeff" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;再次衷心感谢每一位为该翻译项目做出贡献的同学，我们会持续关注TensorFlow、AI领域以及其它最新技术的发展、持续维护该协作翻译、持续提供更多更优质的内容，为广大IT学习者们服务！&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-内容来源" class="anchor" aria-hidden="true" href="#内容来源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;内容来源&lt;/h2&gt;
&lt;p&gt;英文官方网站：&lt;br&gt;
&lt;a href="http://tensorflow.org/" rel="nofollow"&gt;http://tensorflow.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官方GitHub仓库：&lt;br&gt;
&lt;a href="https://github.com/tensorflow/tensorflow"&gt;https://github.com/tensorflow/tensorflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;中文版 GitHub 仓库：&lt;br&gt;
&lt;a href="https://github.com/jikexueyuanwiki/tensorflow-zh"&gt;https://github.com/jikexueyuanwiki/tensorflow-zh&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-参与者按认领章节排序" class="anchor" aria-hidden="true" href="#参与者按认领章节排序"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;参与者（按认领章节排序）&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-翻译" class="anchor" aria-hidden="true" href="#翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;翻译&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/PFZheng"&gt;@PFZheng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/linbojin"&gt;@Tony Jin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chenweican"&gt;@chenweican&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bingjin"&gt;@bingjin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/oskycar"&gt;@oskycar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/btpeter"&gt;@btpeter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Warln"&gt;@Warln&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ericxk"&gt;@ericxk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangaicc"&gt;@wangaicc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/TerenceCooper"&gt;@Terence Cooper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhyhooo"&gt;@zhyhooo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/thylaco1eo"&gt;@thylaco1eo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/volvet"&gt;@volvet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhangkom"&gt;@zhangkom&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/derekshang"&gt;@derekshang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lianghyv"&gt;@lianghyv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nb312"&gt;@nb312&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Jim-Zenn"&gt;@Jim-Zenn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/andyiac"&gt;@andyiac&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/TerenceCooper"&gt;@Terence Cooper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/leege100"&gt;@leege100&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-校对" class="anchor" aria-hidden="true" href="#校对"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;校对&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/sstruct"&gt;@yangtze&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ericxk"&gt;@ericxk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WangHong-yang"&gt;@HongyangWang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/LichAmnesia"&gt;@LichAmnesia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhyhooo"&gt;@zhyhooo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/waiwaizheng"&gt;@waiwaizheng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WangHong-yang"&gt;@HongyangWang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorfly"&gt;@tensorfly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lonlonago"&gt;@lonlonago&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jishaoming"&gt;@jishaoming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lucky521"&gt;@lucky521&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/allensummer"&gt;@allensummer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/volvet"&gt;@volvet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ZHNathanielLee"&gt;@ZHNathanielLee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PengFoo"&gt;@pengfoo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/qiaohaijun"&gt;@qiaohaijun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SeikaScarlet"&gt;@Seika&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-进度记录" class="anchor" aria-hidden="true" href="#进度记录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;进度记录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2015-11-10, 谷歌发布全新人工智能系统TensorFlow并宣布开源, 极客学院Wiki启动协同翻译，创建 GitHub 仓库，制定协同规范&lt;/li&gt;
&lt;li&gt;2015-11-18, 所有章节认领完毕，翻译完成18章，校对认领7章，Star数361，fork数100，协同翻译QQ群及技术交流群的TF爱好者将近300人，GitHub搜索TensorFlow排名第二&lt;/li&gt;
&lt;li&gt;2015-12-10, Star数超过500&lt;/li&gt;
&lt;li&gt;2015-12-15, 项目正式上线&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-花絮" class="anchor" aria-hidden="true" href="#花絮"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;花絮&lt;/h2&gt;
&lt;p&gt;在组织翻译的过程中，有些事情令人印象深刻，记录下来，希望以后来学习文档的同学能够明了到手中这份文档的由来：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参加翻译的有学生，也有老师；有专门研究AI/ML的，也有对此感兴趣的；有国内的，也有远在纽约的；有工程技术人员也有博士、专家&lt;/li&gt;
&lt;li&gt;其中一位，&lt;a href="http://www.longmotto.com" rel="nofollow"&gt;恩泽&lt;/a&gt;同学，为了翻译一篇文档，在前一天没有睡觉的情况下坚持翻完，20个小时没有合眼&lt;/li&gt;
&lt;li&gt;还有一位老师，刚从讲台上讲完课，就立即给我们的翻译提修改意见&lt;/li&gt;
&lt;li&gt;很多同学自发的将搭建环境中遇到的问题总结到FAQ里帮助他人&lt;/li&gt;
&lt;li&gt;为了一个翻译细节，经常是来回几次，和其他人讨论完善&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-持续改进" class="anchor" aria-hidden="true" href="#持续改进"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;持续改进&lt;/h2&gt;
&lt;p&gt;这样的一个高技术领域的文档，我们在翻译的过程中，难免会有不完善的地方，希望请大家一起帮助我们持续改进文档的翻译质量，帮助更多的人，方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在GitHub上提Issue或Pull Request，地址为: &lt;a href="https://github.com/jikexueyuanwiki/tensorflow-zh"&gt;https://github.com/jikexueyuanwiki/tensorflow-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;加入TensorFlow技术交流群，与TensorFlower们一起研究交流技术干货--TensorFlow技术交流群：782484288&lt;/li&gt;
&lt;li&gt;对翻译感兴趣？加入协同翻译群：248320884，与翻译大神一道研究TensorFlow的本地化&lt;/li&gt;
&lt;li&gt;给我们写邮件： &lt;a href="mailto:wiki@jikexueyuan.com"&gt;wiki@jikexueyuan.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-感谢支持" class="anchor" aria-hidden="true" href="#感谢支持"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;感谢支持&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://wiki.jikexueyuan.com" rel="nofollow"&gt;极客学院 Wiki&lt;/a&gt; 提供图文教程托管服务&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-离线版本" class="anchor" aria-hidden="true" href="#离线版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;离线版本&lt;/h2&gt;
&lt;p&gt;目前，离线版本(PDF、ePub)可正常下载、使用&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tex-pdf-修订版" class="anchor" aria-hidden="true" href="#tex-pdf-修订版"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tex-PDF 修订版&lt;/h2&gt;
&lt;p&gt;&lt;a href="tex_pdf"&gt;Tex-PDF 修订版&lt;/a&gt; 目前正在编订中，欢迎加入进来一起修订。您可以在此查看&lt;a href="tex_pdf/tensorflow_manual_cn.pdf"&gt;预览版&lt;/a&gt;目前最新状态。&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jikexueyuanwiki</author><guid isPermaLink="false">https://github.com/jikexueyuanwiki/tensorflow-zh</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>dart-lang/language #12 in TeX, This month</title><link>https://github.com/dart-lang/language</link><description>&lt;p&gt;&lt;i&gt;Design of the Dart language&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-dart-language-evolution" class="anchor" aria-hidden="true" href="#dart-language-evolution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dart language evolution&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/dart-lang/language" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/603c879b44e3c78bdd33b11d894682ba28e5fc1f/68747470733a2f2f7472617669732d63692e6f72672f646172742d6c616e672f6c616e67756167652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/dart-lang/language.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository is a place for the Dart language team to work on
language changes and features, and to solicit and accept feedback and requests.&lt;/p&gt;
&lt;p&gt;Issues and feature requests relevant to the language and the specification may
be filed &lt;a href="https://github.com/dart-lang/language/issues"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-organization" class="anchor" aria-hidden="true" href="#organization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Organization&lt;/h1&gt;
&lt;p&gt;We follow &lt;a href="https://github.com/dart-lang/language/blob/master/doc/life_of_a_language_feature.md"&gt;this
process&lt;/a&gt;
for planning and rolling out language changes.&lt;/p&gt;
&lt;p&gt;Features currently being worked on are listed in the
&lt;a href="https://github.com/dart-lang/language/projects/1"&gt;language funnel&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Notes from language design meetings can be
found &lt;a href="https://github.com/dart-lang/language/blob/master/minutes/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-dart-language" class="anchor" aria-hidden="true" href="#dart-language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dart Language&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://www.dartlang.org" rel="nofollow"&gt;Dart&lt;/a&gt; is an open-source, scalable programming language, with robust
libraries and runtimes, for building web, server, and mobile apps.&lt;/p&gt;
&lt;p&gt;This repository tracks the Dart language specification
and changes to the Dart language.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;Anyone can participate in the discussion about language changes
by participating on the dart language mailing list,
by replying to issues in this repository,
and by uploading documents, tests or other resources.&lt;/p&gt;
&lt;p&gt;When commenting on issues in this repository, keep in mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="+1" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png"&gt;👍&lt;/g-emoji&gt; reactions are more useful than comments to show support.&lt;/li&gt;
&lt;li&gt;Motivating examples help us understand why you want new features more than
pointers to other languages which have them. We love hearing feedback about
your experiences with other languages, but we also want to know why they are
right for Dart in particular.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license--patents" class="anchor" aria-hidden="true" href="#license--patents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License &amp;amp; patents&lt;/h2&gt;
&lt;p&gt;See &lt;a href="https://github.com/dart-lang/language/blob/master/LICENSE"&gt;LICENSE&lt;/a&gt; and &lt;a href="https://github.com/dart-lang/language/blob/master/PATENTS"&gt;PATENTS&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dart-lang</author><guid isPermaLink="false">https://github.com/dart-lang/language</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>billryan/resume #13 in TeX, This month</title><link>https://github.com/billryan/resume</link><description>&lt;p&gt;&lt;i&gt;An elegant \LaTeX\ résumé template&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-résumé" class="anchor" aria-hidden="true" href="#résumé"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Résumé&lt;/h1&gt;
&lt;p&gt;Hit branch &lt;a href="https://github.com/billryan/resume/tree/zh_CN"&gt;zh_CN&lt;/a&gt; if you want a Simplified Chinese résumé.&lt;/p&gt;
&lt;p&gt;中文用户请前往 &lt;a href="https://github.com/billryan/resume/tree/zh_CN"&gt;zh_CN&lt;/a&gt; 分支。&lt;/p&gt;
&lt;p&gt;An elegant \LaTeX\ résumé template, compiled with \XeLaTeX. Inspired by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zachscrivena/simple-resume-cv"&gt;zachscrivena/simple-resume-cv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ctan.org/pkg/res" rel="nofollow"&gt;res&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianxu.net/en/files/JianXu_CV.pdf" rel="nofollow"&gt;JianXu's CV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stat.berkeley.edu/~paciorek/computingTips/Latex_template_creating_CV_.html" rel="nofollow"&gt;paciorek's CV/Resume template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.sharelatex.com/blog/2011/03/27/how-to-write-a-latex-class-file-and-design-your-own-cv.html" rel="nofollow"&gt;How to write a LaTeX class file and design your own CV (Part 1) - ShareLaTeX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Easy to further customize or extend&lt;/li&gt;
&lt;li&gt;Full support for unicode characters (e.g. CJK) with \XeLaTeX\&lt;/li&gt;
&lt;li&gt;Perfect Simplified Chinese fonts supported with Adobefonts&lt;/li&gt;
&lt;li&gt;FontAwesome 4.6.3 support&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fork this repository&lt;/li&gt;
&lt;li&gt;Add information about you directly in GitHub&lt;/li&gt;
&lt;li&gt;Compile TeX file to PDF with &lt;a href="https://latexonline.cc/" rel="nofollow"&gt;LaTeX.Online&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Can also use Overleaf for online compilation (&lt;a href="https://www.overleaf.com/" rel="nofollow"&gt;https://www.overleaf.com/&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-sample-output" class="anchor" aria-hidden="true" href="#sample-output"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample Output&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1292567/62409353-3fecfc00-b608-11e9-8e83-84962912c956.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1292567/62409353-3fecfc00-b608-11e9-8e83-84962912c956.png" alt="English" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1292567/62409351-3f546580-b608-11e9-9f6d-d232a68c5451.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1292567/62409351-3f546580-b608-11e9-9f6d-d232a68c5451.png" alt="English with photo" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1292567/62409352-3fecfc00-b608-11e9-8d9e-76243ca3052a.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1292567/62409352-3fecfc00-b608-11e9-8d9e-76243ca3052a.png" alt="简体中文" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/billryan/resume/files/3463503/resume.pdf"&gt;English PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/billryan/resume/files/3463501/resume_photo.pdf"&gt;English with photo PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/billryan/resume/files/3463502/resume-zh_CN.pdf"&gt;简体中文 PDF&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Edit in ShareLaTeX online - &lt;a href="https://www.sharelatex.com/templates/556b27cf0d23e5a8117053d9" rel="nofollow"&gt;https://www.sharelatex.com/templates/556b27cf0d23e5a8117053d9&lt;/a&gt;, &lt;strong&gt;no TeX software install!&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Compile tex on your Computer&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you only need a résumé in English or have installed Adobe Simplified Chinese on your OS, &lt;strong&gt;It would be better to clone only the master branch,&lt;/strong&gt; since the Simplified Chinese fonts files are too large.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/billryan/resume.git --branch master --depth 1 --single-branch &amp;lt;folder&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://opensource.org/licenses/MIT" rel="nofollow"&gt;The MIT License (MIT)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Copyrighted fonts are not subjected to this License.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>billryan</author><guid isPermaLink="false">https://github.com/billryan/resume</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>xueruini/thuthesis #14 in TeX, This month</title><link>https://github.com/xueruini/thuthesis</link><description>&lt;p&gt;&lt;i&gt;LaTeX Thesis Template for Tsinghua University&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://travis-ci.org/xueruini/thuthesis" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5da8c22b95bd0e5eb940bc5fea16af8feb2ed401/68747470733a2f2f7472617669732d63692e6f72672f7875657275696e692f7468757468657369732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/xueruini/thuthesis.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/thuthesis/Lobby" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f93c05a42da86d653b4d5ad075031b2f4a9c60a1/68747470733a2f2f6261646765732e6769747465722e696d2f7468757468657369732f4c6f6262792e737667" alt="Join the chat at https://gitter.im/thuthesis/Lobby" data-canonical-src="https://badges.gitter.im/thuthesis/Lobby.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/xueruini/thuthesis/releases"&gt;&lt;img src="https://camo.githubusercontent.com/495c01faee6697531e60e3f0ca1e66743582e890/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f7875657275696e692f7468757468657369732f746f74616c2e737667" alt="Github downloads" data-canonical-src="https://img.shields.io/github/downloads/xueruini/thuthesis/total.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/xueruini/thuthesis/releases/latest"&gt;&lt;img src="https://camo.githubusercontent.com/a08197fcf9be7d3e34ac0526686304fbd4817146/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f7875657275696e692f7468757468657369732f616c6c2e737667" alt="GitHub release" data-canonical-src="https://img.shields.io/github/release/xueruini/thuthesis/all.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/xueruini/thuthesis/commits/master"&gt;&lt;img src="https://camo.githubusercontent.com/13a6034e83188b5338130101b729835496b5110f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d6974732d73696e63652f7875657275696e692f7468757468657369732f6c61746573742e737667" alt="GitHub commits" data-canonical-src="https://img.shields.io/github/commits-since/xueruini/thuthesis/latest.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-whats-thuthesis" class="anchor" aria-hidden="true" href="#whats-thuthesis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's ThuThesis?&lt;/h1&gt;
&lt;p&gt;ThuThesis is an abbreviation of &lt;b&gt;T&lt;/b&gt;sing&lt;b&gt;h&lt;/b&gt;ua &lt;b&gt;U&lt;/b&gt;niversity &lt;b&gt;Thesis&lt;/b&gt; LaTeX Template.&lt;/p&gt;
&lt;p&gt;This package establishes a simple and easy-to-use LaTeX template for Tsinghua dissertations, including general undergraduate research papers, masters theses, doctoral theses, doctoral dissertations, and post-doc reports. Additional support for other formats (what else is there?) will be added continuously. An English translation of this README follows the Chinese below.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-thuthesis是什么" class="anchor" aria-hidden="true" href="#thuthesis是什么"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ThuThesis是什么？&lt;/h1&gt;
&lt;p&gt;ThuThesis为 &lt;b&gt;T&lt;/b&gt;sing&lt;b&gt;h&lt;/b&gt;ua &lt;b&gt;U&lt;/b&gt;niversity &lt;b&gt;Thesis&lt;/b&gt; LaTeX Template之缩写。&lt;/p&gt;
&lt;p&gt;此宏包旨在建立一个简单易用的清华大学学位论文LaTeX模板，包括本科综合论文训练、硕士论文、博士论文、博士哲学论文以及博士后出站报告。现在支持本科、硕士、博士论文、博士后出站报告格式，对其它格式（还有么？）的支持会陆续加入。&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-文档" class="anchor" aria-hidden="true" href="#文档"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;文档&lt;/h1&gt;
&lt;p&gt;请&lt;a href="https://github.com/xueruini/thuthesis/releases"&gt;下载&lt;/a&gt;模板，里面包括具体使用说明以及示例文档：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模板使用说明 (thuthesis.pdf)&lt;/li&gt;
&lt;li&gt;示例文档 (main.pdf)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-下载" class="anchor" aria-hidden="true" href="#下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;发行版：&lt;a href="http://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;CTAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;开发版：&lt;a href="https://github.com/xueruini/thuthesis"&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-升级" class="anchor" aria-hidden="true" href="#升级"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;升级&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-自动更新" class="anchor" aria-hidden="true" href="#自动更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;自动更新&lt;/h2&gt;
&lt;p&gt;通过 TeX 发行版工具自动从 &lt;a href="http://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;CTAN&lt;/a&gt; 更新。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-手动更新" class="anchor" aria-hidden="true" href="#手动更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;手动更新&lt;/h2&gt;
&lt;p&gt;从 &lt;a href="https://github.com/xueruini/thuthesis"&gt;GitHub&lt;/a&gt; 下载放入论文目录，执行命令（Windows 用户在文件夹空白处按&lt;code&gt;Shift+鼠标右键&lt;/code&gt;，点击“在此处打开命令行窗口”）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xetex thuthesis.ins
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;即可得到 &lt;code&gt;thuthesis.cls&lt;/code&gt; 等模板文件。&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-提问" class="anchor" aria-hidden="true" href="#提问"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;提问&lt;/h1&gt;
&lt;p&gt;按推荐顺序排序：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先到 &lt;a href="https://github.com/xueruini/thuthesis/wiki/FAQ"&gt;FAQ&lt;/a&gt; 看看常见问题&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/xueruini/thuthesis/issues"&gt;Github Issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.newsmth.net/nForum/#!board/TeX" rel="nofollow"&gt;TeX@newsmth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://groups.google.com/group/thuthesis" rel="nofollow"&gt;ThuThesis@Google Groups&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-makefile的用法" class="anchor" aria-hidden="true" href="#makefile的用法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Makefile的用法&lt;/h1&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;make [{all&lt;span class="pl-k"&gt;|&lt;/span&gt;thesis&lt;span class="pl-k"&gt;|&lt;/span&gt;shuji&lt;span class="pl-k"&gt;|&lt;/span&gt;doc&lt;span class="pl-k"&gt;|&lt;/span&gt;clean&lt;span class="pl-k"&gt;|&lt;/span&gt;cleanall&lt;span class="pl-k"&gt;|&lt;/span&gt;distclean}]&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-目标" class="anchor" aria-hidden="true" href="#目标"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目标&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;make all&lt;/code&gt;       等于 &lt;code&gt;make thesis &amp;amp;&amp;amp; make shuji &amp;amp;&amp;amp; make doc&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make cls&lt;/code&gt;       生成模板文件；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make thesis&lt;/code&gt;    生成论文 main.pdf；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make shuji&lt;/code&gt;     生成书脊 shuji.pdf；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make doc&lt;/code&gt;       生成使用说明书 thuthesis.pdf；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;     删除示例文件的中间文件（不含 main.pdf）；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make cleanall&lt;/code&gt;  删除示例文件的中间文件和 main.pdf；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt; 删除示例文件和模板的所有中间文件和 PDF。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h1&gt;
&lt;p&gt;Download and unzip the template. Specific usage documentation and examples can be found in the files below. At present, these documents are &lt;b&gt;only available in Chinese&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Template usage (thuthesis.pdf)&lt;/li&gt;
&lt;li&gt;Template example (main.pdf)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-downloads" class="anchor" aria-hidden="true" href="#downloads"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloads&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Published version: &lt;a href="http://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;CTAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Developer version: &lt;a href="https://github.com/xueruini/thuthesis"&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-automatic" class="anchor" aria-hidden="true" href="#automatic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Automatic&lt;/h2&gt;
&lt;p&gt;Get the most up-to-date published version of the TeX tools from &lt;a href="http://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;CTAN&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-manual" class="anchor" aria-hidden="true" href="#manual"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Manual&lt;/h2&gt;
&lt;p&gt;Download the package from &lt;a href="https://github.com/xueruini/thuthesis"&gt;GitHub&lt;/a&gt; to the root directory of your thesis, then execute the command (Windows users &lt;code&gt;Shift + right click&lt;/code&gt; white area in the file window and click "Open command line window here from the popup menu"):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xetex thuthesis.ins
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You'll get &lt;code&gt;thuthesis.cls&lt;/code&gt; along with other template files.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-reporting-issues" class="anchor" aria-hidden="true" href="#reporting-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reporting Issues&lt;/h1&gt;
&lt;p&gt;Please follow the procedure below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Check the  &lt;a href="https://github.com/xueruini/thuthesis/wiki/FAQ"&gt;FAQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/xueruini/thuthesis/issues"&gt;Github Issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.newsmth.net/nForum/#!board/TeX" rel="nofollow"&gt;TeX@newsmth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://groups.google.com/group/thuthesis" rel="nofollow"&gt;ThuThesis@Google Groups&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-makefile-usage" class="anchor" aria-hidden="true" href="#makefile-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Makefile Usage&lt;/h1&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;make [{all&lt;span class="pl-k"&gt;|&lt;/span&gt;thesis&lt;span class="pl-k"&gt;|&lt;/span&gt;shuji&lt;span class="pl-k"&gt;|&lt;/span&gt;doc&lt;span class="pl-k"&gt;|&lt;/span&gt;clean&lt;span class="pl-k"&gt;|&lt;/span&gt;cleanall&lt;span class="pl-k"&gt;|&lt;/span&gt;distclean}]&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-targets" class="anchor" aria-hidden="true" href="#targets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Targets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;make all&lt;/code&gt;       same as &lt;code&gt;make thesis &amp;amp;&amp;amp; make shuji &amp;amp;&amp;amp; make doc&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make cls&lt;/code&gt;       generate template file;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make thesis&lt;/code&gt;    generate thesis main.pdf;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make shuji&lt;/code&gt;     generate book spine for printing shuji.pdf;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make doc&lt;/code&gt;       generate documentation thuthesis.pdf;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;     delete all examples' files (excluding main.pdf);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make cleanall&lt;/code&gt;  delete all examples' files and main.pdf;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt; delete all examples' and templates' files and PDFs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>xueruini</author><guid isPermaLink="false">https://github.com/xueruini/thuthesis</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>cplusplus/draft #15 in TeX, This month</title><link>https://github.com/cplusplus/draft</link><description>&lt;p&gt;&lt;i&gt;C++ standards drafts&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-c-standard-draft-sources" class="anchor" aria-hidden="true" href="#c-standard-draft-sources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;C++ Standard Draft Sources&lt;/h1&gt;
&lt;p&gt;These are the sources used to generate drafts of the C++
standard. These sources should not be considered an ISO publication,
nor should documents generated from them unless officially adopted by
the C++ working group (ISO/IEC JTC1/SC22/WG21).&lt;/p&gt;
&lt;p&gt;Get involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/cplusplus/draft/wiki/How-to-submit-an-editorial-issue"&gt;How to submit an editorial issue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cplusplus/draft/wiki/How-to-tell-if-an-issue-is-editorial"&gt;How to tell if an issue is editorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://isocpp.org/std/submit-issue" rel="nofollow"&gt;How to submit a new issue/defect report&lt;/a&gt; for non-editorial issues&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More information about the C++ standard can be found at &lt;a href="http://isocpp.org/std" rel="nofollow"&gt;isocpp.org&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-getting-started-on-mac-os-x"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-on-mac-os-x" class="anchor" aria-hidden="true" href="#getting-started-on-mac-os-x"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started on Mac OS X&lt;/h2&gt;
&lt;p&gt;Install the &lt;a href="http://tug.org/mactex/" rel="nofollow"&gt;MacTeX distribution&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are on a slow network, you'll want to get the &lt;a href="http://tug.org/mactex/morepackages.html" rel="nofollow"&gt;BasicTeX package&lt;/a&gt; instead,
then run the following command to install the other packages that the draft requires:&lt;/p&gt;
&lt;blockquote&gt;
sudo tlmgr install latexmk isodate substr relsize ulem fixme rsfs extract layouts enumitem l3packages l3kernel&lt;/blockquote&gt;
&lt;a name="user-content-getting-started-on-debian-based-systems"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-on-debian-based-systems" class="anchor" aria-hidden="true" href="#getting-started-on-debian-based-systems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started on Debian-based Systems&lt;/h2&gt;
&lt;p&gt;Install the following packages:&lt;/p&gt;
&lt;blockquote&gt;
sudo apt-get install latexmk texlive-latex-recommended texlive-latex-extra texlive-fonts-recommended&lt;/blockquote&gt;
&lt;a name="user-content-getting-started-on-fedora"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-on-fedora" class="anchor" aria-hidden="true" href="#getting-started-on-fedora"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started on Fedora&lt;/h2&gt;
&lt;p&gt;Install the following packages:&lt;/p&gt;
&lt;blockquote&gt;
dnf install latexmk texlive texlive-isodate texlive-relsize texlive-ulem texlive-fixme texlive-extract texlive-l3kernel texlive-l3packages&lt;/blockquote&gt;
&lt;a name="user-content-getting-started-on-arch-linux"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-on-arch-linux" class="anchor" aria-hidden="true" href="#getting-started-on-arch-linux"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started on Arch Linux&lt;/h2&gt;
&lt;p&gt;Install the following packages:&lt;/p&gt;
&lt;blockquote&gt;
latex-mk from the Arch User Repository.
pacman -S texlive-latexextra&lt;/blockquote&gt;
&lt;a name="user-content-instructions"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-instructions" class="anchor" aria-hidden="true" href="#instructions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Instructions&lt;/h2&gt;
&lt;p&gt;To typeset the draft document, from the &lt;code&gt;source&lt;/code&gt; directory:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;run &lt;code&gt;latexmk -pdf std&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That's it! You should now have an &lt;code&gt;std.pdf&lt;/code&gt; containing the typeset draft.&lt;/p&gt;
&lt;a name="user-content-alternative-instructions"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-alternative-instructions" class="anchor" aria-hidden="true" href="#alternative-instructions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Alternative instructions&lt;/h3&gt;
&lt;p&gt;If you can't use latexmk for some reason, you can use the Makefiles instead:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;run &lt;code&gt;make rebuild&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;make reindex&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you can't use latexmk or make for some reason, you can run LaTeX manually instead:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;run &lt;code&gt;pdflatex std&lt;/code&gt; until there are no more changed labels or changed tables&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;makeindex generalindex&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;makeindex libraryindex&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;makeindex grammarindex&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;makeindex impldefindex&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;pdflatex std&lt;/code&gt; once more.&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;makeindex -s basic.gst -o xrefindex.gls xrefindex.glo&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;makeindex -s basic.gst -o xrefdelta.gls xrefdelta.glo&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;pdflatex std&lt;/code&gt; twice more.&lt;/li&gt;
&lt;/ol&gt;
&lt;a name="user-content-generated-input-files"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-generated-input-files" class="anchor" aria-hidden="true" href="#generated-input-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generated input files&lt;/h3&gt;
&lt;p&gt;To regenerate figures from .dot files, run:&lt;/p&gt;
&lt;pre&gt;dot -o&amp;lt;pdfname&amp;gt; -Tpdf &amp;lt;dotfilename&amp;gt;
&lt;/pre&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre&gt;dot -ofigstreampos.pdf -Tpdf figstreampos.dot
&lt;/pre&gt;
&lt;a name="user-content-acknowledgements"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;A great deal of gratitude goes out to Pete Becker for his amazing work
in the original conversion of the C++ standard drafts to LaTeX, and
his subsequent maintenance of the standard drafts up to C++11. Thank
you Pete.&lt;/p&gt;
&lt;p&gt;Thanks to Walter Brown for suggesting the use of &lt;code&gt;latexmk&lt;/code&gt;.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>cplusplus</author><guid isPermaLink="false">https://github.com/cplusplus/draft</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item></channel></rss>