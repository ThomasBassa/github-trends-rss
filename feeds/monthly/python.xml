<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Python, This month</title><link>https://github.com/trending/python?since=monthly</link><description>The top repositories on GitHub for python, measured monthly</description><pubDate>Fri, 29 Nov 2019 01:05:02 GMT</pubDate><lastBuildDate>Fri, 29 Nov 2019 01:05:02 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>CorentinJ/Real-Time-Voice-Cloning #1 in Python, This month</title><link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link><description>&lt;p&gt;&lt;i&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-real-time-voice-cloning" class="anchor" aria-hidden="true" href="#real-time-voice-cloning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Real-Time Voice Cloning&lt;/h1&gt;
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;Transfer Learning from Speaker Verification to
Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. Feel free to check &lt;a href="https://matheo.uliege.be/handle/2268.2/6801" rel="nofollow"&gt;my thesis&lt;/a&gt; if you're curious or if you're looking for info I haven't documented yet (don't hesitate to make an issue for that too). Mostly I would recommend giving a quick look to the figures beyond the introduction.&lt;/p&gt;
&lt;p&gt;SV2TTS is a three-stage deep learning framework that allows to create a numerical representation of a voice from a few seconds of audio, and to use it to condition a text-to-speech model trained to generalize to new voices.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9c33f78be8afe656503da974c478ea2ba2647db7/68747470733a2f2f692e696d6775722e636f6d2f386c46556c677a2e706e67" alt="Toolbox demo" data-canonical-src="https://i.imgur.com/8lFUlgz.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-papers-implemented" class="anchor" aria-hidden="true" href="#papers-implemented"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Papers implemented&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Designation&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Implementation source&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf" rel="nofollow"&gt;1802.08435&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;WaveRNN (vocoder)&lt;/td&gt;
&lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1712.05884.pdf" rel="nofollow"&gt;1712.05884&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tacotron 2 (synthesizer)&lt;/td&gt;
&lt;td&gt;Natural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Rayhane-mamah/Tacotron-2"&gt;Rayhane-mamah/Tacotron-2&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf" rel="nofollow"&gt;1710.10467&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GE2E (encoder)&lt;/td&gt;
&lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-news" class="anchor" aria-hidden="true" href="#news"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;News&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;13/11/19&lt;/strong&gt;: I'm sorry that I can't maintain this repo as much as I wish I could. I'm working full time on improving voice cloning techniques and I don't have the time to share my improvements here. Plus this repo relies on a lot of old tensorflow code and it's hard to work with. If you're a researcher, then this repo might be of use to you. &lt;strong&gt;If you just want to clone your voice&lt;/strong&gt;, do check our demo on &lt;a href="https://www.resemble.ai/" rel="nofollow"&gt;Resemble.AI&lt;/a&gt; - it can run for free but it will be a bit slower, and it will give much better results than this repo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;20/08/19:&lt;/strong&gt; I'm working on &lt;a href="https://github.com/resemble-ai/Resemblyzer"&gt;resemblyzer&lt;/a&gt;, an independent package for the voice encoder. You can use your trained encoder models from this repo with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;06/07/19:&lt;/strong&gt; Need to run within a docker container on a remote server? See &lt;a href="https://sean.lane.sh/posts/2019/07/Running-the-Real-Time-Voice-Cloning-project-in-Docker/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;25/06/19:&lt;/strong&gt; Experimental support for low-memory GPUs (~2gb) added for the synthesizer. Pass &lt;code&gt;--low_mem&lt;/code&gt; to &lt;code&gt;demo_cli.py&lt;/code&gt; or &lt;code&gt;demo_toolbox.py&lt;/code&gt; to enable it. It adds a big overhead, so it's not recommended if you have enough VRAM.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h3&gt;
&lt;p&gt;You will need the following whether you plan to use the toolbox only or to retrain the models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Python 3.7&lt;/strong&gt;. Python 3.6 might work too, but I wouldn't go lower because I make extensive use of pathlib.&lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to install the necessary packages. Additionally you will need &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;PyTorch&lt;/a&gt; (&amp;gt;=1.0.1).&lt;/p&gt;
&lt;p&gt;A GPU is mandatory, but you don't necessarily need a high tier GPU if you only want to use the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pretrained-models" class="anchor" aria-hidden="true" href="#pretrained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained models&lt;/h3&gt;
&lt;p&gt;Download the latest &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-preliminary" class="anchor" aria-hidden="true" href="#preliminary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preliminary&lt;/h3&gt;
&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If all tests pass, you're good to go.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h3&gt;
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="http://www.openslr.org/resources/12/train-clean-100.tar.gz" rel="nofollow"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-toolbox" class="anchor" aria-hidden="true" href="#toolbox"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Toolbox&lt;/h3&gt;
&lt;p&gt;You can then try the toolbox:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br&gt;
or&lt;br&gt;
&lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wiki" class="anchor" aria-hidden="true" href="#wiki"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wiki&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How it all works&lt;/strong&gt; (WIP - &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/How-it-all-works"&gt;stub&lt;/a&gt;, you might be better off reading my thesis until it's done)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training"&gt;&lt;strong&gt;Training models yourself&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training with other data/languages&lt;/strong&gt; (WIP - see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-507864097"&gt;here&lt;/a&gt; for now)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/TODO-&amp;amp;-planned-features"&gt;&lt;strong&gt;TODO and planned features&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributions--issues" class="anchor" aria-hidden="true" href="#contributions--issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions &amp;amp; Issues&lt;/h2&gt;
&lt;p&gt;I'm working full-time as of June 2019. I don't have time to maintain this repo nor reply to issues. Sorry.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CorentinJ</author><guid isPermaLink="false">https://github.com/CorentinJ/Real-Time-Voice-Cloning</guid><pubDate>Fri, 29 Nov 2019 00:01:00 GMT</pubDate></item><item><title>openai/gpt-2 #2 in Python, This month</title><link>https://github.com/openai/gpt-2</link><description>&lt;p&gt;&lt;i&gt;Code for the paper "Language Models are Unsupervised Multitask Learners"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Archive (code is provided as-is, no updates expected)&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-gpt-2" class="anchor" aria-hidden="true" href="#gpt-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;gpt-2&lt;/h1&gt;
&lt;p&gt;Code and models from the paper &lt;a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" rel="nofollow"&gt;"Language Models are Unsupervised Multitask Learners"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can read about GPT-2 and its staged release in our &lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;original blog post&lt;/a&gt;, &lt;a href="https://openai.com/blog/gpt-2-6-month-follow-up/" rel="nofollow"&gt;6 month follow-up post&lt;/a&gt;, and &lt;a href="https://www.openai.com/blog/gpt-2-1-5b-release/" rel="nofollow"&gt;final post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We have also &lt;a href="https://github.com/openai/gpt-2-output-dataset"&gt;released a dataset&lt;/a&gt; for researchers to study their behaviors.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; &lt;em&gt;Note that our original parameter counts were wrong due to an error (in our previous blog posts and paper).  Thus you may have seen small referred to as 117M and medium referred to as 345M.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;This repository is meant to be a starting point for researchers and engineers to experiment with GPT-2.&lt;/p&gt;
&lt;p&gt;For basic information, see our &lt;a href="./model_card.md"&gt;model card&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-some-caveats" class="anchor" aria-hidden="true" href="#some-caveats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Some caveats&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GPT-2 models' robustness and worst case behaviors are not well-understood.  As with any machine-learned model, carefully evaluate GPT-2 for your use case, especially if used without fine-tuning or in safety-critical applications where reliability is important.&lt;/li&gt;
&lt;li&gt;The dataset our GPT-2 models were trained on contains many texts with &lt;a href="https://twitter.com/TomerUllman/status/1101485289720242177" rel="nofollow"&gt;biases&lt;/a&gt; and factual inaccuracies, and thus GPT-2 models are likely to be biased and inaccurate as well.&lt;/li&gt;
&lt;li&gt;To avoid having samples mistaken as human-written, we recommend clearly labeling samples as synthetic before wide dissemination.  Our models are often incoherent or inaccurate in subtle ways, which takes more than a quick read for a human to notice.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-work-with-us" class="anchor" aria-hidden="true" href="#work-with-us"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Work with us&lt;/h3&gt;
&lt;p&gt;Please &lt;a href="mailto:languagequestions@openai.com"&gt;let us know&lt;/a&gt; if you’re doing interesting research with or working on applications of GPT-2!  We’re especially interested in hearing from and potentially working with those who are studying&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Potential malicious use cases and defenses against them (e.g. the detectability of synthetic text)&lt;/li&gt;
&lt;li&gt;The extent of problematic content (e.g. bias) being baked into the models and effective mitigations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h2&gt;
&lt;p&gt;See &lt;a href="./DEVELOPERS.md"&gt;DEVELOPERS.md&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;See &lt;a href="./CONTRIBUTORS.md"&gt;CONTRIBUTORS.md&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;Please use the following bibtex entry:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-future-work" class="anchor" aria-hidden="true" href="#future-work"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future work&lt;/h2&gt;
&lt;p&gt;We may release code for evaluating the models on various benchmarks.&lt;/p&gt;
&lt;p&gt;We are still considering release of the larger models.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="./LICENSE"&gt;MIT&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>openai</author><guid isPermaLink="false">https://github.com/openai/gpt-2</guid><pubDate>Fri, 29 Nov 2019 00:02:00 GMT</pubDate></item><item><title>wangzheng0822/algo #3 in Python, This month</title><link>https://github.com/wangzheng0822/algo</link><description>&lt;p&gt;&lt;i&gt;数据结构和算法必知必会的50个代码实现&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-数据结构和算法必知必会的50个代码实现" class="anchor" aria-hidden="true" href="#数据结构和算法必知必会的50个代码实现"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数据结构和算法必知必会的50个代码实现&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-微信搜索我的公众号小争哥或者微信扫描下面二维码-获取更多压箱底的干货分享" class="anchor" aria-hidden="true" href="#微信搜索我的公众号小争哥或者微信扫描下面二维码-获取更多压箱底的干货分享"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;微信搜索我的公众号“小争哥”，或者微信扫描下面二维码, 获取更多压箱底的干货分享&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-前google工程师5万人跟着学的数据结构和算法之美专栏作者" class="anchor" aria-hidden="true" href="#前google工程师5万人跟着学的数据结构和算法之美专栏作者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;前Google工程师，5万+人跟着学的《数据结构和算法之美》专栏作者&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/wangzheng0822/markdownphotos/blob/master/pics/qrcode_for_gh_9b0e7afdff20_258.jpg"&gt;&lt;img src="https://github.com/wangzheng0822/markdownphotos/raw/master/pics/qrcode_for_gh_9b0e7afdff20_258.jpg" alt="t2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-数组" class="anchor" aria-hidden="true" href="#数组"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数组&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现一个支持动态扩容的数组&lt;/li&gt;
&lt;li&gt;实现一个大小固定的有序数组，支持动态增删改操作&lt;/li&gt;
&lt;li&gt;实现两个有序数组合并为一个有序数组&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-链表" class="anchor" aria-hidden="true" href="#链表"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;链表&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现单链表、循环链表、双向链表，支持增删操作&lt;/li&gt;
&lt;li&gt;实现单链表反转&lt;/li&gt;
&lt;li&gt;实现两个有序的链表合并为一个有序链表&lt;/li&gt;
&lt;li&gt;实现求链表的中间结点&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-栈" class="anchor" aria-hidden="true" href="#栈"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;栈&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;用数组实现一个顺序栈&lt;/li&gt;
&lt;li&gt;用链表实现一个链式栈&lt;/li&gt;
&lt;li&gt;编程模拟实现一个浏览器的前进、后退功能&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-队列" class="anchor" aria-hidden="true" href="#队列"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;队列&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;用数组实现一个顺序队列&lt;/li&gt;
&lt;li&gt;用链表实现一个链式队列&lt;/li&gt;
&lt;li&gt;实现一个循环队列&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-递归" class="anchor" aria-hidden="true" href="#递归"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;递归&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;编程实现斐波那契数列求值f(n)=f(n-1)+f(n-2)&lt;/li&gt;
&lt;li&gt;编程实现求阶乘n!&lt;/li&gt;
&lt;li&gt;编程实现一组数据集合的全排列&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-排序" class="anchor" aria-hidden="true" href="#排序"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;排序&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现归并排序、快速排序、插入排序、冒泡排序、选择排序&lt;/li&gt;
&lt;li&gt;编程实现O(n)时间复杂度内找到一组数据的第K大元素&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-二分查找" class="anchor" aria-hidden="true" href="#二分查找"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;二分查找&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现一个有序数组的二分查找算法&lt;/li&gt;
&lt;li&gt;实现模糊二分查找算法（比如大于等于给定值的第一个元素）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-散列表" class="anchor" aria-hidden="true" href="#散列表"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;散列表&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现一个基于链表法解决冲突问题的散列表&lt;/li&gt;
&lt;li&gt;实现一个LRU缓存淘汰算法&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-字符串" class="anchor" aria-hidden="true" href="#字符串"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;字符串&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现一个字符集，只包含a～z这26个英文字母的Trie树&lt;/li&gt;
&lt;li&gt;实现朴素的字符串匹配算法&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-二叉树" class="anchor" aria-hidden="true" href="#二叉树"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;二叉树&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现一个二叉查找树，并且支持插入、删除、查找操作&lt;/li&gt;
&lt;li&gt;实现查找二叉查找树中某个节点的后继、前驱节点&lt;/li&gt;
&lt;li&gt;实现二叉树前、中、后序以及按层遍历&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-堆" class="anchor" aria-hidden="true" href="#堆"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;堆&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现一个小顶堆、大顶堆、优先级队列&lt;/li&gt;
&lt;li&gt;实现堆排序&lt;/li&gt;
&lt;li&gt;利用优先级队列合并K个有序数组&lt;/li&gt;
&lt;li&gt;求一组动态数据集合的最大Top K&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-图" class="anchor" aria-hidden="true" href="#图"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;图&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现有向图、无向图、有权图、无权图的邻接矩阵和邻接表表示方法&lt;/li&gt;
&lt;li&gt;实现图的深度优先搜索、广度优先搜索&lt;/li&gt;
&lt;li&gt;实现Dijkstra算法、A*算法&lt;/li&gt;
&lt;li&gt;实现拓扑排序的Kahn算法、DFS算法&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-回溯" class="anchor" aria-hidden="true" href="#回溯"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;回溯&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;利用回溯算法求解八皇后问题&lt;/li&gt;
&lt;li&gt;利用回溯算法求解0-1背包问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-分治" class="anchor" aria-hidden="true" href="#分治"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;分治&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;利用分治算法求一组数据的逆序对个数&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-动态规划" class="anchor" aria-hidden="true" href="#动态规划"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;动态规划&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;0-1背包问题&lt;/li&gt;
&lt;li&gt;最小路径和&lt;/li&gt;
&lt;li&gt;编程实现莱文斯坦最短编辑距离&lt;/li&gt;
&lt;li&gt;编程实现查找两个字符串的最长公共子序列&lt;/li&gt;
&lt;li&gt;编程实现一个数据序列的最长递增子序列&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>wangzheng0822</author><guid isPermaLink="false">https://github.com/wangzheng0822/algo</guid><pubDate>Fri, 29 Nov 2019 00:03:00 GMT</pubDate></item><item><title>swisskyrepo/PayloadsAllTheThings #4 in Python, This month</title><link>https://github.com/swisskyrepo/PayloadsAllTheThings</link><description>&lt;p&gt;&lt;i&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-payloads-all-the-things" class="anchor" aria-hidden="true" href="#payloads-all-the-things"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Payloads All The Things&lt;/h1&gt;
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques !
I &lt;g-emoji class="g-emoji" alias="heart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2764.png"&gt;❤️&lt;/g-emoji&gt; pull requests :)&lt;/p&gt;
&lt;p&gt;You can also contribute with a &lt;g-emoji class="g-emoji" alias="beers" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f37b.png"&gt;🍻&lt;/g-emoji&gt; IRL&lt;/p&gt;
&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;README.md - vulnerability description and how to exploit it&lt;/li&gt;
&lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt;
&lt;li&gt;Images - pictures for the README.md&lt;/li&gt;
&lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You might also like the &lt;code&gt;Methodology and Resources&lt;/code&gt; folder :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/"&gt;Methodology and Resources&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Active%20Directory%20Attack.md"&gt;Active Directory Attack.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Persistence.md"&gt;Linux - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Privilege%20Escalation.md"&gt;Linux - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Metasploit%20-%20Cheatsheet.md"&gt;Metasploit - Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Methodology%20and%20enumeration.md"&gt;Methodology and enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Pivoting%20Techniques.md"&gt;Network Pivoting Techniques.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Discovery.md"&gt;Network Discovery.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Reverse%20Shell%20Cheatsheet.md"&gt;Reverse Shell Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Subdomains%20Enumeration.md"&gt;Subdomains Enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Download%20and%20Execute.md"&gt;Windows - Download and Execute.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Mimikatz.md"&gt;Windows - Mimikatz.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Persistence.md"&gt;Windows - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Post%20Exploitation%20Koadic.md"&gt;Windows - Post Exploitation Koadic.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Privilege%20Escalation.md"&gt;Windows - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Using%20credentials.md"&gt;Windows - Using credentials.md&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CVE%20Exploits"&gt;CVE Exploits&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apache Struts 2 CVE-2013-2251 CVE-2017-5638 CVE-2018-11776_.py&lt;/li&gt;
&lt;li&gt;Apache Struts 2 CVE-2017-9805.py&lt;/li&gt;
&lt;li&gt;Apache Struts 2 CVE-2018-11776.py&lt;/li&gt;
&lt;li&gt;Docker API RCE.py&lt;/li&gt;
&lt;li&gt;Drupalgeddon2 CVE-2018-7600.rb&lt;/li&gt;
&lt;li&gt;Heartbleed CVE-2014-0160.py&lt;/li&gt;
&lt;li&gt;JBoss CVE-2015-7501.py&lt;/li&gt;
&lt;li&gt;Jenkins CVE-2015-8103.py&lt;/li&gt;
&lt;li&gt;Jenkins CVE-2016-0792.py&lt;/li&gt;
&lt;li&gt;Rails CVE-2019-5420.rb&lt;/li&gt;
&lt;li&gt;Shellshock CVE-2014-6271.py&lt;/li&gt;
&lt;li&gt;Tomcat CVE-2017-12617.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2016-3510.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2017-10271.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2018-2894.py&lt;/li&gt;
&lt;li&gt;WebSphere CVE-2015-7450.py&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You want more ? Check the &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/BOOKS.md"&gt;Books&lt;/a&gt; and &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/YOUTUBE.md"&gt;Youtube videos&lt;/a&gt; selections.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>swisskyrepo</author><guid isPermaLink="false">https://github.com/swisskyrepo/PayloadsAllTheThings</guid><pubDate>Fri, 29 Nov 2019 00:04:00 GMT</pubDate></item><item><title>streamlit/streamlit #5 in Python, This month</title><link>https://github.com/streamlit/streamlit</link><description>&lt;p&gt;&lt;i&gt;Streamlit — The fastest way to build custom ML tools&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-welcome-to-streamlit-wave" class="anchor" aria-hidden="true" href="#welcome-to-streamlit-wave"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Welcome to Streamlit &lt;g-emoji class="g-emoji" alias="wave" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png"&gt;👋&lt;/g-emoji&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The fastest way to build custom ML tools.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Streamlit lets you create apps for your machine learning projects with deceptively simple Python scripts. It supports hot-reloading, so your app updates live as you edit and save your file. No need to mess with HTTP requests, HTML, JavaScript, etc. All you need is your favorite editor and a browser. Take a look at Streamlit in action:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5ae1dcfd188be26bbb0648fb62e9d6d593dbb6f5/68747470733a2f2f617773312e646973636f757273652d63646e2e636f6d2f7374616e6461726431302f75706c6f6164732f73747265616d6c69742f6f726967696e616c2f31582f323932653938356637663735656637626566386332376235383939663731663736636435373765302e676966"&gt;&lt;img src="https://camo.githubusercontent.com/5ae1dcfd188be26bbb0648fb62e9d6d593dbb6f5/68747470733a2f2f617773312e646973636f757273652d63646e2e636f6d2f7374616e6461726431302f75706c6f6164732f73747265616d6c69742f6f726967696e616c2f31582f323932653938356637663735656637626566386332376235383939663731663736636435373765302e676966" alt="Example of live coding a dashboard in Streamlit|635x380" data-canonical-src="https://aws1.discourse-cdn.com/standard10/uploads/streamlit/original/1X/292e985f7f75ef7bef8c27b5899f71f76cd577e0.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Check out our &lt;a href="https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace" rel="nofollow"&gt;launch blog post&lt;/a&gt;!!&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install streamlit
streamlit hello&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-example" class="anchor" aria-hidden="true" href="#example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h2&gt;
&lt;p&gt;Streamlit lets you build interactive apps ridiculously easily:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; streamlit &lt;span class="pl-k"&gt;as&lt;/span&gt; st

x &lt;span class="pl-k"&gt;=&lt;/span&gt; st.slider(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Select a value&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
st.write(x, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;squared is&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, x &lt;span class="pl-k"&gt;*&lt;/span&gt; x)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1e18efff3f06946e9d1559712cea0cb76364f004/68747470733a2f2f73747265616d6c69742d64656d6f2d646174612e73332d75732d776573742d322e616d617a6f6e6177732e636f6d2f737175617265642d696d6167652d666f722d6769746875622d726561646d652d322e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/1e18efff3f06946e9d1559712cea0cb76364f004/68747470733a2f2f73747265616d6c69742d64656d6f2d646174612e73332d75732d776573742d322e616d617a6f6e6177732e636f6d2f737175617265642d696d6167652d666f722d6769746875622d726561646d652d322e706e67" width="490/" data-canonical-src="https://streamlit-demo-data.s3-us-west-2.amazonaws.com/squared-image-for-github-readme-2.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-a-bigger-example" class="anchor" aria-hidden="true" href="#a-bigger-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A Bigger Example&lt;/h2&gt;
&lt;p&gt;Despite its simplicity Streamlit lets you build incredibly rich and powerful tools. &lt;a href="https://github.com/streamlit/demo-self-driving"&gt;This demo project&lt;/a&gt; lets you browse the entire &lt;a href="https://github.com/udacity/self-driving-car"&gt;Udacity self-driving-car dataset&lt;/a&gt; and run inference in real time using the &lt;a href="https://pjreddie.com/darknet/yolo" rel="nofollow"&gt;YOLO object detection net&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/streamlit/demo-self-driving/master/av_final_optimized.gif"&gt;&lt;img src="https://raw.githubusercontent.com/streamlit/demo-self-driving/master/av_final_optimized.gif" alt="Making-of Animation" title="Making-of Animation" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The complete demo is implemented in less than 300 lines of Python. In fact, the app contains &lt;a href="https://github.com/streamlit/demo-self-driving/blob/master/app.py"&gt;only 23 Streamlit calls&lt;/a&gt; which illustrates all the major building blocks of Streamlit. You can try it right now with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install --upgrade streamlit opencv-python
streamlit run https://raw.githubusercontent.com/streamlit/demo-self-driving/master/app.py&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-more-information" class="anchor" aria-hidden="true" href="#more-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Our &lt;a href="https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace" rel="nofollow"&gt;launch post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Our lovely &lt;a href="https://discuss.streamlit.io/" rel="nofollow"&gt;community&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Streamlit &lt;a href="https://streamlit.io/docs" rel="nofollow"&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;More &lt;a href="https://github.com/streamlit/"&gt;demo projects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you would like to contribute, see &lt;a href="https://github.com/streamlit/streamlit/wiki/Contributing"&gt;instructions here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-streamlit-for-teams" class="anchor" aria-hidden="true" href="#streamlit-for-teams"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Streamlit for Teams&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://streamlit.io/forteams/" rel="nofollow"&gt;Streamlit for Teams&lt;/a&gt; is our enterprise edition, with single-click deploy, authentication, web editing, versioning, and more. Please contact us if you would like to learn more.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Streamlit is completely free and open source and licensed under the &lt;a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;Apache 2.0&lt;/a&gt; license.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>streamlit</author><guid isPermaLink="false">https://github.com/streamlit/streamlit</guid><pubDate>Fri, 29 Nov 2019 00:05:00 GMT</pubDate></item><item><title>tamarott/SinGAN #6 in Python, This month</title><link>https://github.com/tamarott/SinGAN</link><description>&lt;p&gt;&lt;i&gt;Official pytorch implementation of the paper: "SinGAN: Learning a Generative Model from a Single Natural Image"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-singan" class="anchor" aria-hidden="true" href="#singan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SinGAN&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://webee.technion.ac.il/people/tomermic/SinGAN/SinGAN.htm" rel="nofollow"&gt;Project&lt;/a&gt; | &lt;a href="https://arxiv.org/pdf/1905.01164.pdf" rel="nofollow"&gt;Arxiv&lt;/a&gt; | &lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.pdf" rel="nofollow"&gt;CVF&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-official-pytorch-implementation-of-the-paper-singan-learning-a-generative-model-from-a-single-natural-image" class="anchor" aria-hidden="true" href="#official-pytorch-implementation-of-the-paper-singan-learning-a-generative-model-from-a-single-natural-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Official pytorch implementation of the paper: "SinGAN: Learning a Generative Model from a Single Natural Image"&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-iccv-2019" class="anchor" aria-hidden="true" href="#iccv-2019"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ICCV 2019&lt;/h4&gt;
&lt;h2&gt;&lt;a id="user-content-random-samples-from-a-single-image" class="anchor" aria-hidden="true" href="#random-samples-from-a-single-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Random samples from a &lt;em&gt;single&lt;/em&gt; image&lt;/h2&gt;
&lt;p&gt;With SinGAN, you can train a generative model from a single natural image, and then generate random samples form the given image, for example:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="imgs/teaser.PNG"&gt;&lt;img src="imgs/teaser.PNG" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-singans-applications" class="anchor" aria-hidden="true" href="#singans-applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SinGAN's applications&lt;/h2&gt;
&lt;p&gt;SinGAN can be also use to a line of image manipulation task, for example:
&lt;a target="_blank" rel="noopener noreferrer" href="imgs/manipulation.PNG"&gt;&lt;img src="imgs/manipulation.PNG" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
This is done by injecting an image to the already trained model. See section 4 in our &lt;a href="https://arxiv.org/pdf/1905.01164.pdf" rel="nofollow"&gt;paper&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you use this code for your research, please cite our paper:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{rottshaham2019singan,
  title={SinGAN: Learning a Generative Model from a Single Natural Image},
  author={Rott Shaham, Tamar and Dekel, Tali and Michaeli, Tomer},
  booktitle={Computer Vision (ICCV), IEEE International Conference on},
  year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-code" class="anchor" aria-hidden="true" href="#code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-install-dependencies" class="anchor" aria-hidden="true" href="#install-dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install dependencies&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;python -m pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code was tested with python 3.6&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-train" class="anchor" aria-hidden="true" href="#train"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Train&lt;/h3&gt;
&lt;p&gt;To train SinGAN model on your own image, put the desire training image under Input/Images, and run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python main_train.py --input_name &amp;lt;input_file_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will also use the resulting trained model to generate random samples starting from the coarsest scale (n=0).&lt;/p&gt;
&lt;p&gt;To run this code on a cpu machine, specify &lt;code&gt;--not_cuda&lt;/code&gt; when calling &lt;code&gt;main_train.py&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-random-samples" class="anchor" aria-hidden="true" href="#random-samples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Random samples&lt;/h3&gt;
&lt;p&gt;To generate random samples from any starting generation scale, please first train SinGAN model for the desire image (as described above), then run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python random_samples.py --input_name &amp;lt;training_image_file_name&amp;gt; --mode random_samples --gen_start_scale &amp;lt;generation start scale number&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;pay attention: for using the full model, specify the generation start scale to be 0, to start the generation from the second scale, specify it to be 1, and so on.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-random-samples-of-arbitrery-sizes" class="anchor" aria-hidden="true" href="#random-samples-of-arbitrery-sizes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Random samples of arbitrery sizes&lt;/h3&gt;
&lt;p&gt;To generate random samples of arbitrery sizes, please first train SinGAN model for the desire image (as described above), then run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python random_samples.py --input_name &amp;lt;training_image_file_name&amp;gt; --mode random_samples_arbitrary_sizes --scale_h &amp;lt;horizontal scaling factor&amp;gt; --scale_v &amp;lt;vertical scaling factor&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-animation-from-a-single-image" class="anchor" aria-hidden="true" href="#animation-from-a-single-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Animation from a single image&lt;/h3&gt;
&lt;p&gt;To generate short animation from a single image, run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python animation.py --input_name &amp;lt;input_file_name&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will automatically start a new training phase with noise padding mode.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-harmonization" class="anchor" aria-hidden="true" href="#harmonization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Harmonization&lt;/h3&gt;
&lt;p&gt;To harmonize a pasted object into an image (See example in Fig. 13 in &lt;a href="https://arxiv.org/pdf/1905.01164.pdf" rel="nofollow"&gt;our paper&lt;/a&gt;), please first train SinGAN model for the desire background image (as described above), then save the naively pasted reference image and it's binary mask under "Input/Harmonization" (see saved images for an example). Run the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python harmonization.py --input_name &amp;lt;training_image_file_name&amp;gt; --ref_name &amp;lt;naively_pasted_reference_image_file_name&amp;gt; --harmonization_start_scale &amp;lt;scale to inject&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that different injection scale will produce different harmonization effects. The coarsest injection scale equals 1.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-editing" class="anchor" aria-hidden="true" href="#editing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Editing&lt;/h3&gt;
&lt;p&gt;To edit an image, (See example in Fig. 12 in &lt;a href="https://arxiv.org/pdf/1905.01164.pdf" rel="nofollow"&gt;our paper&lt;/a&gt;), please first train SinGAN model on the desire non-edited image (as described above), then save the naive edit as a reference image under "Input/Editing" with a corresponding binary map (see saved images for an example). Run the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python editing.py --input_name &amp;lt;training_image_file_name&amp;gt; --ref_name &amp;lt;edited_image_file_name&amp;gt; --editing_start_scale &amp;lt;scale to inject&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;both the masked and unmasked output will be saved.
Here as well, different injection scale will produce different editing effects. The coarsest injection scale equals 1.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-paint-to-image" class="anchor" aria-hidden="true" href="#paint-to-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Paint to Image&lt;/h3&gt;
&lt;p&gt;To transfer a paint into a realistic image (See example in Fig. 11 in &lt;a href="https://arxiv.org/pdf/1905.01164.pdf" rel="nofollow"&gt;our paper&lt;/a&gt;), please first train SinGAN model on the desire image (as described above), then save your paint under "Input/Paint", and run the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python paint2image.py --input_name &amp;lt;training_image_file_name&amp;gt; --ref_name &amp;lt;paint_image_file_name&amp;gt; --paint_start_scale &amp;lt;scale to inject&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here as well, different injection scale will produce different editing effects. The coarsest injection scale equals 1.&lt;/p&gt;
&lt;p&gt;Advanced option: Specify quantization_flag to be True, to re-train &lt;em&gt;only&lt;/em&gt; the injection level of the model, to get a on a color-quantized version of upsamled generated images from previous scale. For some images, this might lead to more realistic results.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-super-resolution" class="anchor" aria-hidden="true" href="#super-resolution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Super Resolution&lt;/h3&gt;
&lt;p&gt;To super resolve an image, please run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python SR.py --input_name &amp;lt;LR_image_file_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will automatically train a SinGAN model correspond to 4x upsampling factor (if not exist already).
For different SR factors, please specify it using the parametr &lt;code&gt;--sr_factor&lt;/code&gt; when calling the function.
SinGAN's results on BSD100 dataset can be download from the 'Downloads' folder.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-additional-data-and-functions" class="anchor" aria-hidden="true" href="#additional-data-and-functions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Additional Data and Functions&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-single-image-fréchet-inception-distance-sifid-score" class="anchor" aria-hidden="true" href="#single-image-fréchet-inception-distance-sifid-score"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Single Image Fréchet Inception Distance (SIFID score)&lt;/h3&gt;
&lt;p&gt;To calculate the SIFID between real images and their corresponding fake samples, please run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python SIFID/sifid_score.py --path2real &amp;lt;real images path&amp;gt; --path2fake &amp;lt;fake images path&amp;gt; --images_suffix &amp;lt;e.g. jpg, png&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make sure that each of the fake images file name is identical to its cooresponding real image file name.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-super-resolution-results" class="anchor" aria-hidden="true" href="#super-resolution-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Super Resolution Results&lt;/h3&gt;
&lt;p&gt;SinGAN's SR results on BSD100 dataset can be download from the 'Downloads' folder.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-user-study" class="anchor" aria-hidden="true" href="#user-study"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;User Study&lt;/h3&gt;
&lt;p&gt;The data used for the user study can be found in the 'Downloads' folder.&lt;/p&gt;
&lt;p&gt;'real' folder: 50 real images, randomly picked from the &lt;a href="http://places.csail.mit.edu/" rel="nofollow"&gt;places databas&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;'fake_high_variance' folder: random samples starting from n=N for each of the real images&lt;/p&gt;
&lt;p&gt;'fake_mid_variance' folder: random samples starting from n=N-1 for each of the real images&lt;/p&gt;
&lt;p&gt;For additional details please see section 3.1 in our &lt;a href="https://arxiv.org/pdf/1905.01164.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tamarott</author><guid isPermaLink="false">https://github.com/tamarott/SinGAN</guid><pubDate>Fri, 29 Nov 2019 00:06:00 GMT</pubDate></item><item><title>harismuneer/Ultimate-Facebook-Scraper #7 in Python, This month</title><link>https://github.com/harismuneer/Ultimate-Facebook-Scraper</link><description>&lt;p&gt;&lt;i&gt;🤖 A bot which scrapes almost everything about a Facebook user's profile including all public posts/statuses available on the user's timeline, uploaded photos, tagged photos, videos, friends list and their profile photos (including Followers, Following, Work Friends, College Friends etc).&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;a href="#"&gt;
  &lt;div align="center"&gt;
    &lt;img src="images/ufs_icon.png" width="154" style="max-width:100%;"&gt;
  &lt;/div&gt;
&lt;/a&gt;
&lt;h1 align="center"&gt;&lt;a id="user-content-ultimate-facebook-scraper-ufs" class="anchor" aria-hidden="true" href="#ultimate-facebook-scraper-ufs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ultimate Facebook Scraper (UFS)&lt;/h1&gt;
&lt;p align="center"&gt;
  Tooling that &lt;b&gt;automates&lt;/b&gt; your social media interactions to collect posts, photos, videos, friends, followers and much more on Facebook.
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://www.codacy.com/manual/harismuneer/Ultimate-Facebook-Scraper?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=harismuneer/Ultimate-Facebook-Scraper&amp;amp;utm_campaign=Badge_Grade" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/99af5d6fbf3b3412b5d86dd7a51267cd1932f568/68747470733a2f2f6170692e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3766343137393063336333613466643239323933373737633637366338363137" data-canonical-src="https://api.codacy.com/project/badge/Grade/7f41790c3c3a4fd29293777c676c8617" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/91e0907406753ea1e935b1bad0ec812126e19fe7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4275696c642d50617373696e672d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265266c6f676f3d6170707665796f72" data-canonical-src="https://img.shields.io/badge/Build-Passing-brightgreen.svg?style=flat-square&amp;amp;logo=appveyor" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/d41b9884bd102b525c8fb9a8c3c8d3bbed2b67f0/68747470733a2f2f6261646765732e66726170736f66742e636f6d2f6f732f76312f6f70656e2d736f757263652e7376673f763d313033" data-canonical-src="https://badges.frapsoft.com/os/v1/open-source.svg?v=103" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://www.github.com/harismuneer/Ultimate-Facebook-Scraper/fork"&gt;
    &lt;img src="https://camo.githubusercontent.com/e1e2221a649724fbdb4bf206770d7f7b374b6895/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f68617269736d756e6565722f556c74696d6174652d46616365626f6f6b2d536372617065722e7376673f7374796c653d736f6369616c266c6162656c3d466f726b266d61784167653d32353932303030" data-canonical-src="https://img.shields.io/github/forks/harismuneer/Ultimate-Facebook-Scraper.svg?style=social&amp;amp;label=Fork&amp;amp;maxAge=2592000" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/e2adf2e1c6a9764f7b398c9588f8d8dd010a76c3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c6174266c6162656c3d436f6e747269627574696f6e7326636f6c6f72413d72656426636f6c6f72423d626c61636b" data-canonical-src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat&amp;amp;label=Contributions&amp;amp;colorA=red&amp;amp;colorB=black" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p align="center"&gt;
  Developers from following organizations have so far joined the quest and contributed to UFS.
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/cb47d8b8488c5a9294bb8c7bee1bbb0e55cf189b/68747470733a2f2f7777772e616462697264732e676c6f62616c2f77702d636f6e74656e742f75706c6f6164732f323031362f30372f4d6963726f736f66742d4c6f676f2d7371756172652e706e67" width="150" alt="Microsoft" data-canonical-src="https://www.adbirds.global/wp-content/uploads/2016/07/Microsoft-Logo-square.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href=""&gt;&lt;img hspace="5" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/c865e9d21e8b1f793908ef76b5a39e755995afde/68747470733a2f2f6175746f706169722e6e65742f696d616765732f65617379626c6f675f61727469636c65732f352f4d49542d4c6f676f2e706e67" width="165" alt="MIT" data-canonical-src="https://autopair.net/images/easyblog_articles/5/MIT-Logo.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/7aa438fc9aab9a289b8ca3b06e05a226ee6fb4d9/68747470733a2f2f7777772e676c6f62616c66696e616e6369616c646174612e636f6d2f77702d636f6e74656e742f75706c6f6164732f636c69656e742d6c6f676f732f486172766172642d556e69766572736974792d6c6f676f2d2e6a7067" width="163" alt="Harvard" data-canonical-src="https://www.globalfinancialdata.com/wp-content/uploads/client-logos/Harvard-University-logo-.jpg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href=""&gt;&lt;img hspace="10" style="max-width:100%;"&gt;&lt;/a&gt;  
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/e9bffdd7c658416ab6e041712e603b8efd9cf033/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f656e2f652f65342f4e6174696f6e616c5f556e69766572736974795f6f665f436f6d70757465725f616e645f456d657267696e675f536369656e6365735f6c6f676f2e706e67" width="120" alt="NUCES" data-canonical-src="https://upload.wikimedia.org/wikipedia/en/e/e4/National_University_of_Computer_and_Emerging_Sciences_logo.png" style="max-width:100%;"&gt;
  &lt;/a&gt;&lt;br&gt;      
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/056cfdefc6f173df43487548359503523340575e/68747470733a2f2f7374617469632e7769787374617469632e636f6d2f6d656469612f6537303738375f63306334386434353662306434323162396330643435343738663966343232647e6d76322e676966" width="165" alt="UCLA" data-canonical-src="https://static.wixstatic.com/media/e70787_c0c48d456b0d421b9c0d45478f9f422d~mv2.gif" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href=""&gt;&lt;img hspace="20" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/4a92f8294c47c3413d19e77d40a6fecb4f913928/687474703a2f2f7777772e6765656b7765656b2e706b2f323031362f77702d636f6e74656e742f75706c6f6164732f323031362f30312f41434d2d6c6f676f2e706e67" width="280" alt="ACM" data-canonical-src="http://www.geekweek.pk/2016/wp-content/uploads/2016/01/ACM-logo.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href=""&gt;&lt;img hspace="20" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/553f90c470ce1a5a1720febffca2f06e05b7df75/68747470733a2f2f69322e77702e636f6d2f7777772e6f7066626c6f672e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031342f31302f4c756d732d4c6f676f2e6a70673f7a6f6f6d3d312e33343939393939303436333235363834266669743d3530302532433430392673736c3d31" width="120" alt="LUMS" data-canonical-src="https://i2.wp.com/www.opfblog.com/wp-content/uploads/2014/10/Lums-Logo.jpg?zoom=1.3499999046325684&amp;amp;fit=500%2C409&amp;amp;ssl=1" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;p&gt;A bot which scrapes almost everything about a user's Facebook profile including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;uploaded photos&lt;/li&gt;
&lt;li&gt;tagged photos&lt;/li&gt;
&lt;li&gt;videos&lt;/li&gt;
&lt;li&gt;friends list and their profile photos (including Followers, Following, Work Friends, College Friends etc)&lt;/li&gt;
&lt;li&gt;and all public posts/statuses available on the user's timeline.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data is scraped in an organized format to be used for educational/research purposes by researchers. This scraper does not use Facebook's Graph API meaning there are no rate limiting issues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This tool is being used by thousands of developers weekly and we are pretty amazed at this response! Thank you guys!&lt;g-emoji class="g-emoji" alias="tada" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png"&gt;🎉&lt;/g-emoji&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;citing/referencing&lt;/strong&gt; this tool for your research, check the 'Citation' section below.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-note" class="anchor" aria-hidden="true" href="#note"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note&lt;/h2&gt;
&lt;p&gt;This tool uses xpaths of &lt;strong&gt;'divs'&lt;/strong&gt; to extract data. Since Facebook updates its site frequently, the 'divs' get changed. Consequently, we have to update the divs accordingly to correctly scrape data.&lt;/p&gt;
&lt;p&gt;The developers of this tool have devoted time and effort in developing, and maintaining this tool for a long time. &lt;strong&gt;In order to keep this amazing tool alive, we need support from you geeks.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The code is intuitive and easy to understand, so you can update the relevant xpaths in the code if you find data is not being scraped from profiles. Facebook has most likely updated their site, so please generate a pull request. Much appreciated!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sample" class="anchor" aria-hidden="true" href="#sample"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample&lt;/h2&gt;
&lt;p align="middle"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/main.png"&gt;&lt;img src="images/main.png" width="700" style="max-width:100%;"&gt;&lt;/a&gt;
 &lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-screenshot" class="anchor" aria-hidden="true" href="#screenshot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Screenshot&lt;/h2&gt;
&lt;p align="middle"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/screenshot.png"&gt;&lt;img src="images/screenshot.png" width="700" style="max-width:100%;"&gt;&lt;/a&gt;
 &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;p&gt;You will need to install latest version of &lt;a href="https://www.google.com/chrome/" rel="nofollow"&gt;Google Chrome&lt;/a&gt;. Moreover, you need to install selenium module as well using&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install selenium
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the code using Python 3. Also, the code is multi-platform and is tested on both Windows and Linux.
The tool uses latest version of &lt;a href="http://chromedriver.chromium.org/downloads" rel="nofollow"&gt;Chrome Web Driver&lt;/a&gt;. I have placed the webdriver along with the code but if that version doesn't work then replace the chrome web driver with the latest one.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-to-run" class="anchor" aria-hidden="true" href="#how-to-run"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Run&lt;/h3&gt;
&lt;p&gt;There's a file named "input.txt". You can add as many profiles as you want in the following format with each link on a new line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://www.facebook.com/andrew.ng.96
https://www.facebook.com/zuck
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make sure the link only contains the username or id number at the end and not any other stuff. Make sure its in the format mentioned above.&lt;/p&gt;
&lt;p&gt;Note: There are two modes to download Friends Profile Pics and the user's Photos: Large Size and Small Size. You can change the following variables. By default they are set to Small Sized Pics because its really quick while Large Size Mode takes time depending on the number of pictures to download&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# whether to download the full image or its thumbnail (small size)
# if small size is True then it will be very quick else if its False then it will open each photo to download it
# and it will take much more time
friends_small_size = True
photos_small_size = True
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;a href="https://zenodo.org/badge/latestdoi/145763277" rel="nofollow"&gt;
  &lt;img src="https://camo.githubusercontent.com/273879e6bcd018c853ef3a5d1a4adb87427df933/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3134353736333237372e737667" data-canonical-src="https://zenodo.org/badge/145763277.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;p&gt;If you use this tool for your research, then kindly cite it. Click the above badge for more information regarding the complete citation for this tool and diffferent citation formats like IEEE, APA etc.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-important-message" class="anchor" aria-hidden="true" href="#important-message"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Important Message&lt;/h2&gt;
&lt;p&gt;This tool is for research purposes only. Hence, the developers of this tool won't be responsible for any misuse of data collected using this tool. Used by many researchers and open source intelligence (OSINT) analysts.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;p&gt;You can get in touch with us on our LinkedIn Profiles:&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-haris-muneer" class="anchor" aria-hidden="true" href="#haris-muneer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Haris Muneer&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/harismuneer" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3c9c6ad9c5e217bfdf0540ec0d035be81117a100/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e6e6563742d68617269736d756e6565722d626c75652e7376673f6c6f676f3d6c696e6b6564696e266c6f6e6743616368653d74727565267374796c653d736f6369616c266c6162656c3d436f6e6e656374" alt="LinkedIn Link" data-canonical-src="https://img.shields.io/badge/Connect-harismuneer-blue.svg?logo=linkedin&amp;amp;longCache=true&amp;amp;style=social&amp;amp;label=Connect" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also follow my GitHub Profile to stay updated about my latest projects: &lt;a href="https://github.com/harismuneer"&gt;&lt;img src="https://camo.githubusercontent.com/f1eda7d92b24897b9917c1b16ebfc72be16e160b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e6e6563742d68617269736d756e6565722d626c75652e7376673f6c6f676f3d476974687562266c6f6e6743616368653d74727565267374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="GitHub Follow" data-canonical-src="https://img.shields.io/badge/Connect-harismuneer-blue.svg?logo=Github&amp;amp;longCache=true&amp;amp;style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-hassaan-elahi" class="anchor" aria-hidden="true" href="#hassaan-elahi"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hassaan Elahi&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/hassaan-elahi/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/016097a1448ba96812eb5abe38d730705d89f7bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e6e6563742d4861737361616e2d2d456c6168692d626c75652e7376673f6c6f676f3d6c696e6b6564696e266c6f6e6743616368653d74727565267374796c653d736f6369616c266c6162656c3d436f6e6e656374" alt="LinkedIn Link" data-canonical-src="https://img.shields.io/badge/Connect-Hassaan--Elahi-blue.svg?logo=linkedin&amp;amp;longCache=true&amp;amp;style=social&amp;amp;label=Connect" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also follow my GitHub Profile to stay updated about my latest projects:&lt;a href="https://github.com/Hassaan-Elahi"&gt;&lt;img src="https://camo.githubusercontent.com/fa3d8c58ddd3ac5746969c8f2a397de737851b0c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e6e6563742d4861737361616e2d2d456c6168692d626c75652e7376673f6c6f676f3d476974687562266c6f6e6743616368653d74727565267374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="GitHub Follow" data-canonical-src="https://img.shields.io/badge/Connect-Hassaan--Elahi-blue.svg?logo=Github&amp;amp;longCache=true&amp;amp;style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you liked the repo then kindly support it by giving it a star ⭐!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributions-welcome" class="anchor" aria-hidden="true" href="#contributions-welcome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions Welcome&lt;/h2&gt;
&lt;p&gt;&lt;a href="#"&gt;&lt;img src="https://camo.githubusercontent.com/d24f2f8414437a9491ea3145cafd373167315d50/68747470733a2f2f666f7274686562616467652e636f6d2f696d616765732f6261646765732f6275696c742d776974682d6c6f76652e737667" alt="forthebadge" data-canonical-src="https://forthebadge.com/images/badges/built-with-love.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you find any bug in the code or have any improvements in mind then feel free to generate a pull request.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-issues" class="anchor" aria-hidden="true" href="#issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Issues&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.github.com/harismuneer/Ultimate-Facebook-Scraper/issues"&gt;&lt;img src="https://camo.githubusercontent.com/b3ece733dc678aecdcf9479bb26adfeee59c963e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f68617269736d756e6565722f556c74696d6174652d46616365626f6f6b2d536372617065722e7376673f7374796c653d666c6174266c6162656c3d497373756573266d61784167653d32353932303030" alt="GitHub Issues" data-canonical-src="https://img.shields.io/github/issues/harismuneer/Ultimate-Facebook-Scraper.svg?style=flat&amp;amp;label=Issues&amp;amp;maxAge=2592000" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you face any issue, you can create a new issue in the Issues Tab and I will be glad to help you out.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="../master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/3406d0ad8f23c602988ff89dee4d145529a503ce/68747470733a2f2f696d672e736869656c64732e696f2f636f636f61706f64732f6c2f41464e6574776f726b696e672e7376673f7374796c653d7374796c65266c6162656c3d4c6963656e7365266d61784167653d32353932303030" alt="MIT" data-canonical-src="https://img.shields.io/cocoapods/l/AFNetworking.svg?style=style&amp;amp;label=License&amp;amp;maxAge=2592000" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Copyright (c) 2018-present, harismuneer, Hassaan-Elahi&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>harismuneer</author><guid isPermaLink="false">https://github.com/harismuneer/Ultimate-Facebook-Scraper</guid><pubDate>Fri, 29 Nov 2019 00:07:00 GMT</pubDate></item><item><title>ddbourgin/numpy-ml #8 in Python, This month</title><link>https://github.com/ddbourgin/numpy-ml</link><description>&lt;p&gt;&lt;i&gt;Machine learning, in numpy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-numpy-ml" class="anchor" aria-hidden="true" href="#numpy-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;numpy-ml&lt;/h1&gt;
&lt;p&gt;Ever wish you had an inefficient but somewhat legible collection of machine
learning algorithms implemented exclusively in numpy? No?&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;To see all of the available models, take a look at the &lt;a href="https://numpy-ml.readthedocs.io/" rel="nofollow"&gt;project documentation&lt;/a&gt; or see &lt;a href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;Am I missing your favorite model? Is there something that could be cleaner /
less confusing? Did I mess something up? Submit a PR! The only requirement is
that your models are written with just the &lt;a href="https://docs.python.org/3/library/" rel="nofollow"&gt;Python standard
library&lt;/a&gt; and &lt;a href="https://www.numpy.org/" rel="nofollow"&gt;NumPy&lt;/a&gt;. The
&lt;a href="https://scipy.github.io/devdocs/" rel="nofollow"&gt;SciPy library&lt;/a&gt; is also permitted under special
circumstances ;)&lt;/p&gt;
&lt;p&gt;See full contributing guidelines &lt;a href="./CONTRIBUTING.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ddbourgin</author><guid isPermaLink="false">https://github.com/ddbourgin/numpy-ml</guid><pubDate>Fri, 29 Nov 2019 00:08:00 GMT</pubDate></item><item><title>sherlock-project/sherlock #9 in Python, This month</title><link>https://github.com/sherlock-project/sherlock</link><description>&lt;p&gt;&lt;i&gt;🔎 Find usernames across social networks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/27065646/53551960-ae4dff80-3b3a-11e9-9075-cef786c69364.png"&gt;&lt;img src="https://user-images.githubusercontent.com/27065646/53551960-ae4dff80-3b3a-11e9-9075-cef786c69364.png" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;span&gt;Hunt down social media accounts by username across &lt;a href="https://github.com/theyahya/sherlock/blob/master/sites.md"&gt;social networks&lt;/a&gt;&lt;/span&gt;
  &lt;br&gt;
  &lt;a href="https://www.python.org/downloads/" title="Python version" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/392b343efc8b7ac39cdb7fd56d19a8ca6792c12b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d2533453d5f332e362d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/python-%3E=_3.6-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="LICENSE" title="License: MIT"&gt;&lt;img src="https://camo.githubusercontent.com/311762166ef25238116d3cadd22fcb6091edab98/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667" data-canonical-src="https://img.shields.io/badge/License-MIT-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://travis-ci.com/TheYahya/sherlock/" title="Build Status" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a93b7b21f175cd07941d299809dbda32764d2cff/68747470733a2f2f7472617669732d63692e636f6d2f54686559616879612f736865726c6f636b2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.com/TheYahya/sherlock.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://twitter.com/intent/tweet?text=%F0%9F%94%8E%20Find%20usernames%20across%20social%20networks%20&amp;amp;url=https://github.com/TheYahya/sherlock&amp;amp;hashtags=hacking,%20osint,%20bugbounty,%20reconnaissance" title="Share on Tweeter" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/83d4084f7b71558e33b08844da5c773a8657e271/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c" data-canonical-src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="http://sherlock-project.github.io/" rel="nofollow"&gt;&lt;img alt="Website" src="https://camo.githubusercontent.com/adc37cc0993bd76eb6e4a6e661146251f8b663af/68747470733a2f2f696d672e736869656c64732e696f2f776562736974652d75702d646f776e2d677265656e2d7265642f687474702f736865726c6f636b2d70726f6a6563742e6769746875622e696f2f2e2e737667" data-canonical-src="https://img.shields.io/website-up-down-green-red/http/sherlock-project.github.io/..svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://microbadger.com/images/theyahya/sherlock" rel="nofollow"&gt;&lt;img alt="docker image" src="https://camo.githubusercontent.com/7369ca4d589232865f5ff69b001ba2794474a285/68747470733a2f2f696d616765732e6d6963726f6261646765722e636f6d2f6261646765732f76657273696f6e2f74686579616879612f736865726c6f636b2e737667" data-canonical-src="https://images.microbadger.com/badges/version/theyahya/sherlock.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="#demo"&gt;Demo&lt;/a&gt;
     |   
  &lt;a href="#installation"&gt;Installation&lt;/a&gt;
     |   
  &lt;a href="#usage"&gt;Usage&lt;/a&gt;
     |   
  &lt;a href="#docker-notes"&gt;Docker Notes&lt;/a&gt;
     |   
  &lt;a href="#adding-new-sites"&gt;Adding New Sites&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;a href="https://asciinema.org/a/223115" rel="nofollow"&gt;
&lt;img src="./images/sherlock_preview.gif" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-demo" class="anchor" aria-hidden="true" href="#demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Demo&lt;/h2&gt;
&lt;p&gt;Use this link to test Sherlock directly in your browser:
&lt;a href="https://elody.com/scenario/plan/16/" rel="nofollow"&gt;https://elody.com/scenario/plan/16/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Python 3.6 or higher is required.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; clone the repo&lt;/span&gt;
$ git clone https://github.com/sherlock-project/sherlock.git

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; change the working directory to sherlock&lt;/span&gt;
$ &lt;span class="pl-c1"&gt;cd&lt;/span&gt; sherlock

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install python3 and python3-pip if they are not installed&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install the requirements&lt;/span&gt;
$ python3 -m pip install -r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/sherlock-project/sherlock&amp;amp;tutorial=README.md" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f4397884ebe7c8fac1cfd4433c6c78455b24a4f0/68747470733a2f2f677374617469632e636f6d2f636c6f75647373682f696d616765732f6f70656e2d62746e2e706e67" alt="Open in Cloud Shell" data-canonical-src="https://gstatic.com/cloudssh/images/open-btn.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python3 sherlock.py --help
usage: sherlock.py [-h] [--version] [--verbose] [--rank]
                   [--folderoutput FOLDEROUTPUT] [--output OUTPUT] [--tor]
                   [--unique-tor] [--csv] [--site SITE_NAME]
                   [--proxy PROXY_URL] [--json JSON_FILE]
                   [--proxy_list PROXY_LIST] [--check_proxies CHECK_PROXY]
                   [--print-found]
                   USERNAMES [USERNAMES ...]

Sherlock: Find Usernames Across Social Networks (Version 0.9.6)

positional arguments:
  USERNAMES             One or more usernames to check with social networks.

optional arguments:
  -h, --help            show this &lt;span class="pl-c1"&gt;help&lt;/span&gt; message and &lt;span class="pl-c1"&gt;exit&lt;/span&gt;
  --version             Display version information and dependencies.
  --verbose, -v, -d, --debug
                        Display extra debugging information and metrics.
  --rank, -r            Present websites ordered by their Alexa.com global
                        rank &lt;span class="pl-k"&gt;in&lt;/span&gt; popularity.
  --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT
                        If using multiple usernames, the output of the results
                        will be saved to this folder.
  --output OUTPUT, -o OUTPUT
                        If using single username, the output of the result
                        will be saved to this file.
  --tor, -t             Make requests over Tor&lt;span class="pl-k"&gt;;&lt;/span&gt; increases runtime&lt;span class="pl-k"&gt;;&lt;/span&gt; requires
                        Tor to be installed and &lt;span class="pl-k"&gt;in&lt;/span&gt; system path.
  --unique-tor, -u      Make requests over Tor with new Tor circuit after each
                        request&lt;span class="pl-k"&gt;;&lt;/span&gt; increases runtime&lt;span class="pl-k"&gt;;&lt;/span&gt; requires Tor to be
                        installed and &lt;span class="pl-k"&gt;in&lt;/span&gt; system path.
  --csv                 Create Comma-Separated Values (CSV) File.
  --site SITE_NAME      Limit analysis to just the listed sites. Add multiple
                        options to specify more than one site.
  --proxy PROXY_URL, -p PROXY_URL
                        Make requests over a proxy. e.g.
                        socks5://127.0.0.1:1080
  --json JSON_FILE, -j JSON_FILE
                        Load data from a JSON file or an online, valid, JSON
                        file.
  --proxy_list PROXY_LIST, -pl PROXY_LIST
                        Make requests over a proxy randomly chosen from a list
                        generated from a .csv file.
  --check_proxies CHECK_PROXY, -cp CHECK_PROXY
                        To be used with the &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;--proxy_list&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; parameter. The
                        script will check &lt;span class="pl-k"&gt;if&lt;/span&gt; the proxies supplied &lt;span class="pl-k"&gt;in&lt;/span&gt; the .csv
                        file are working and anonymous.Put 0 &lt;span class="pl-k"&gt;for&lt;/span&gt; no limit on
                        successfully checked proxies, or another number to
                        institute a limit.
  --print-found         Do not output sites where the username was not found.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To search for only one user:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python3 sherlock.py user123
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To search for more than one user:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python3 sherlock.py user1 user2 user3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Accounts found will be stored in an individual text file with the corresponding username (e.g &lt;code&gt;user123.txt&lt;/code&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-anaconda-windows-notes" class="anchor" aria-hidden="true" href="#anaconda-windows-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Anaconda (Windows) Notes&lt;/h2&gt;
&lt;p&gt;If you are using Anaconda in Windows, using 'python3' might not work. Use 'python' instead.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-docker-notes" class="anchor" aria-hidden="true" href="#docker-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker Notes&lt;/h2&gt;
&lt;p&gt;If docker is installed you can build an image and run this as a container.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build -t mysherlock-image .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the image is built, sherlock can be invoked by running the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -t mysherlock-image user123
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The optional &lt;code&gt;--rm&lt;/code&gt; flag removes the container filesystem on completion to prevent cruft build-up. See: &lt;a href="https://docs.docker.com/engine/reference/run/#clean-up---rm" rel="nofollow"&gt;https://docs.docker.com/engine/reference/run/#clean-up---rm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The optional &lt;code&gt;-t&lt;/code&gt; flag allocates a pseudo-TTY which allows colored output. See: &lt;a href="https://docs.docker.com/engine/reference/run/#foreground" rel="nofollow"&gt;https://docs.docker.com/engine/reference/run/#foreground&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Use the following command to access the saved results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -t -v "$PWD/results:/opt/sherlock/results" mysherlock-image -o /opt/sherlock/results/text.txt user123
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;-v "$PWD/results:/opt/sherlock/results"&lt;/code&gt; option tells docker to create (or use) the folder &lt;code&gt;results&lt;/code&gt; in the
present working directory and to mount it at &lt;code&gt;/opt/sherlock/results&lt;/code&gt; on the docker container.
The &lt;code&gt;-o /opt/sherlock/results/text.txt&lt;/code&gt; option tells &lt;code&gt;sherlock&lt;/code&gt; to output the result.&lt;/p&gt;
&lt;p&gt;Or you can use "Docker Hub" to run &lt;code&gt;sherlock&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run theyahya/sherlock user123
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-using-docker-compose" class="anchor" aria-hidden="true" href="#using-docker-compose"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using &lt;code&gt;docker-compose&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;You can use the &lt;code&gt;docker-compose.yml&lt;/code&gt; file from the repository and use this command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker-compose run sherlok -o /opt/sherlock/results/text.txt user123
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-adding-new-sites" class="anchor" aria-hidden="true" href="#adding-new-sites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding New Sites&lt;/h2&gt;
&lt;p&gt;Please look at the Wiki entry on
&lt;a href="https://github.com/TheYahya/sherlock/wiki/Adding-Sites-To-Sherlock"&gt;adding new sites&lt;/a&gt;
to understand the issues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Sherlock is not accepting adult sites in the standard list.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h2&gt;
&lt;p&gt;Thank you for contributing to Sherlock!&lt;/p&gt;
&lt;p&gt;Before creating a pull request with new development, please run the tests
to ensure that everything is working great.  It would also be a good idea to run the tests
before starting development to distinguish problems between your
environment and the Sherlock software.&lt;/p&gt;
&lt;p&gt;The following is an example of the command line to run all the tests for
Sherlock.  This invocation hides the progress text that Sherlock normally
outputs, and instead shows the verbose output of the tests.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m unittest tests.all --buffer --verbose
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we do currently have 100% test coverage.  Unfortunately, some of
the sites that Sherlock checks are not always reliable, so it is common
to get response errors.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-stargazers-over-time" class="anchor" aria-hidden="true" href="#stargazers-over-time"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stargazers over time&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://starcharts.herokuapp.com/TheYahya/sherlock" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9a700fddd7ae13c41a23e7ebd1b5eaa40a1bb549/68747470733a2f2f737461726368617274732e6865726f6b756170702e636f6d2f54686559616879612f736865726c6f636b2e737667" alt="Stargazers over time" data-canonical-src="https://starcharts.herokuapp.com/TheYahya/sherlock.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;MIT © &lt;a href="https://theyahya.com" rel="nofollow"&gt;Yahya SayadArbabi&lt;/a&gt;&lt;br&gt;
Original Creator - &lt;a href="https://github.com/sdushantha"&gt;Siddharth Dushantha&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sherlock-project</author><guid isPermaLink="false">https://github.com/sherlock-project/sherlock</guid><pubDate>Fri, 29 Nov 2019 00:09:00 GMT</pubDate></item><item><title>sundowndev/PhoneInfoga #10 in Python, This month</title><link>https://github.com/sundowndev/PhoneInfoga</link><description>&lt;p&gt;&lt;i&gt;Advanced information gathering &amp; OSINT tool for phone numbers&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d3fc46eb3ce41e8df8b75fabf670cc20eef2545a/68747470733a2f2f692e696d6775722e636f6d2f4c7455476e46332e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/d3fc46eb3ce41e8df8b75fabf670cc20eef2545a/68747470733a2f2f692e696d6775722e636f6d2f4c7455476e46332e706e67" width="500" data-canonical-src="https://i.imgur.com/LtUGnF3.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a href="https://travis-ci.org/sundowndev/PhoneInfoga" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/46d99e4427dc377be912f425a6527c6785733524/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f73756e646f776e6465762f50686f6e65496e666f67612f6d61737465722e7376673f7374796c653d666c61742d737175617265" alt="Build Status" data-canonical-src="https://img.shields.io/travis/sundowndev/PhoneInfoga/master.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://hub.docker.com/r/sundowndev/phoneinfoga/builds" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/117d6963fac80d7ad53079b83a8eb326aabb0c16/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f636c6f75642f6275696c642f73756e646f776e6465762f70686f6e65696e666f67612e7376673f7374796c653d666c61742d737175617265" alt="Build Status" data-canonical-src="https://img.shields.io/docker/cloud/build/sundowndev/phoneinfoga.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/7d89761ee8648eceb9db7eba9b49cd3c985f9c3d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d626c75652e7376673f7374796c653d666c61742d737175617265" alt="Python version" data-canonical-src="https://img.shields.io/badge/python-3.6-blue.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/sundowndev/PhoneInfoga/releases"&gt;
    &lt;img src="https://camo.githubusercontent.com/a4003bf13d0817cc46214e39aa5f5957c1ac549a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f53756e646f776e4445562f50686f6e65496e666f67612e7376673f7374796c653d666c61742d737175617265" alt="Latest version" data-canonical-src="https://img.shields.io/github/release/SundownDEV/PhoneInfoga.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/sundowndev/PhoneInfoga/blob/master/LICENSE"&gt;
    &lt;img src="https://camo.githubusercontent.com/46c222ff7fbbf26999df33d732182ec6307e69f8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f73756e646f776e6465762f50686f6e65496e666f67612e7376673f7374796c653d666c61742d737175617265" alt="License" data-canonical-src="https://img.shields.io/github/license/sundowndev/PhoneInfoga.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;h4 align="center"&gt;&lt;a id="user-content-information-gathering--osint-reconnaissance-tool-for-phone-numbers" class="anchor" aria-hidden="true" href="#information-gathering--osint-reconnaissance-tool-for-phone-numbers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Information gathering &amp;amp; OSINT reconnaissance tool for phone numbers&lt;/h4&gt;
&lt;p align="center"&gt;
  &lt;a href="https://sundowndev.github.io/PhoneInfoga/" rel="nofollow"&gt;Documentation&lt;/a&gt; •
  &lt;a href="https://sundowndev.github.io/PhoneInfoga/usage/" rel="nofollow"&gt;Basic usage&lt;/a&gt; •
  &lt;a href="https://sundowndev.github.io/PhoneInfoga/resources/" rel="nofollow"&gt;OSINT resources&lt;/a&gt; •
  &lt;a href="https://medium.com/@SundownDEV/phone-number-scanning-osint-recon-tool-6ad8f0cac27b" rel="nofollow"&gt;Related blog post&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;PhoneInfoga is one of the most advanced tools to scan phone numbers using only free resources. The goal is to first gather standard information such as country, area, carrier and line type on any international phone numbers with a very good accuracy. Then search for footprints on search engines to try to find the VoIP provider or identify the owner.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Check if phone number exists and is possible&lt;/li&gt;
&lt;li&gt;Gather standard informations such as country, line type and carrier&lt;/li&gt;
&lt;li&gt;OSINT footprinting using external APIs, Google Hacking, phone books &amp;amp; search engines&lt;/li&gt;
&lt;li&gt;Check for reputation reports, social media, disposable numbers and more&lt;/li&gt;
&lt;li&gt;Scan several numbers at once&lt;/li&gt;
&lt;li&gt;Use custom formatting for more effective OSINT reconnaissance&lt;/li&gt;
&lt;li&gt;Automatic footprinting on several custom formats&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/48694e177cacb5bdbcda70d8dcdd57c1c8e1f512/68747470733a2f2f692e696d6775722e636f6d2f71436b677a7a382e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/48694e177cacb5bdbcda70d8dcdd57c1c8e1f512/68747470733a2f2f692e696d6775722e636f6d2f71436b677a7a382e706e67" alt="Footprinting process" data-canonical-src="https://i.imgur.com/qCkgzz8.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This tool is licensed under the GNU General Public License v3.0.&lt;/p&gt;
&lt;p&gt;Some parts of this code comes from &lt;a href="https://github.com/m4ll0k/infoga"&gt;Infoga&lt;/a&gt;, another project licensed under GPLv3.0.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.flaticon.com/free-icon/fingerprint-search-symbol-of-secret-service-investigation_48838" rel="nofollow"&gt;Icon&lt;/a&gt; made by &lt;a href="https://www.freepik.com/" title="Freepik" rel="nofollow"&gt;Freepik&lt;/a&gt; from &lt;a href="https://www.flaticon.com/" title="Flaticon" rel="nofollow"&gt;flaticon.com&lt;/a&gt; is licensed by &lt;a href="http://creativecommons.org/licenses/by/3.0/" title="Creative Commons BY 3.0" rel="nofollow"&gt;CC 3.0 BY&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://app.fossa.io/projects/git%2Bgithub.com%2Fsundowndev%2FPhoneInfoga?ref=badge_large" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c1bf6c1d6350a0785cc8b07161bcc1cb587e7063/68747470733a2f2f6170702e666f7373612e696f2f6170692f70726f6a656374732f6769742532426769746875622e636f6d25324673756e646f776e64657625324650686f6e65496e666f67612e7376673f747970653d6c61726765" alt="FOSSA Status" data-canonical-src="https://app.fossa.io/api/projects/git%2Bgithub.com%2Fsundowndev%2FPhoneInfoga.svg?type=large" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sundowndev</author><guid isPermaLink="false">https://github.com/sundowndev/PhoneInfoga</guid><pubDate>Fri, 29 Nov 2019 00:10:00 GMT</pubDate></item><item><title>localstack/localstack #11 in Python, This month</title><link>https://github.com/localstack/localstack</link><description>&lt;p&gt;&lt;i&gt;💻  A fully functional local AWS cloud stack. Develop and test your cloud &amp; Serverless apps offline!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://travis-ci.org/localstack/localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9776494b9a6b388dc743ef4f1fe0f48418996403/68747470733a2f2f7472617669732d63692e6f72672f6c6f63616c737461636b2f6c6f63616c737461636b2e737667" alt="Build Status" data-canonical-src="https://travis-ci.org/localstack/localstack.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="#backers"&gt;&lt;img src="https://camo.githubusercontent.com/7503d4e605e56494b94f7e899b59c12d6869e6d4/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f6261636b6572732f62616467652e737667" alt="Backers on Open Collective" data-canonical-src="https://opencollective.com/localstack/backers/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="#sponsors"&gt;&lt;img src="https://camo.githubusercontent.com/4ce5d939a7baa05f5513a28bced276b40163e726/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f72732f62616467652e737667" alt="Sponsors on Open Collective" data-canonical-src="https://opencollective.com/localstack/sponsors/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/localstack/localstack?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a25d81482ec1f47ecef8ab8f5c6ea87316d0df71/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6c6f63616c737461636b2f6c6f63616c737461636b2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/localstack/localstack/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/localstack/Platform" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ef7f49d6c9d82d4762efd93e6c5190ed7ff070e8/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6c6f63616c737461636b2f506c6174666f726d2e737667" alt="Gitter" data-canonical-src="https://img.shields.io/gitter/room/localstack/Platform.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://badge.fury.io/py/localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3f7ff1cf090f0a7a2ca744acb42a565188b1e51f/68747470733a2f2f62616467652e667572792e696f2f70792f6c6f63616c737461636b2e737667" alt="PyPI Version" data-canonical-src="https://badge.fury.io/py/localstack.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://img.shields.io/pypi/l/localstack.svg" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8c44673b3399efcaa024ef3f64e031acd53422f5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f6c6f63616c737461636b2e737667" alt="PyPI License" data-canonical-src="https://img.shields.io/pypi/l/localstack.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://codeclimate.com/github/localstack/localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/bb304a7e024bd89e75e3ee5922f276d69b0399f0/68747470733a2f2f636f6465636c696d6174652e636f6d2f6769746875622f6c6f63616c737461636b2f6c6f63616c737461636b2f6261646765732f6770612e737667" alt="Code Climate" data-canonical-src="https://codeclimate.com/github/localstack/localstack/badges/gpa.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/_localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/83d4084f7b71558e33b08844da5c773a8657e271/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-localstack---a-fully-functional-local-aws-cloud-stack" class="anchor" aria-hidden="true" href="#localstack---a-fully-functional-local-aws-cloud-stack"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LocalStack - A fully functional local AWS cloud stack&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/localstack/localstack/raw/master/localstack/dashboard/web/img/localstack.png"&gt;&lt;img src="https://github.com/localstack/localstack/raw/master/localstack/dashboard/web/img/localstack.png" alt="LocalStack" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;LocalStack&lt;/em&gt; provides an easy-to-use test/mocking framework for developing Cloud applications.&lt;/p&gt;
&lt;p&gt;Currently, the focus is primarily on supporting the AWS cloud stack.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-announcements" class="anchor" aria-hidden="true" href="#announcements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Announcements&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;2019-10-09&lt;/strong&gt;: &lt;strong&gt;LocalStack Pro is out!&lt;/strong&gt; We're incredibly excited to announce the launch of LocalStack Pro - the enterprise version of LocalStack with additional APIs and advanced features. Check out the free trial at &lt;a href="https://localstack.cloud" rel="nofollow"&gt;https://localstack.cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2018-01-10&lt;/strong&gt;: &lt;strong&gt;Help wanted!&lt;/strong&gt; Please &lt;a href="https://lambdastudy.typeform.com/to/kDUvvy?source=localstack-github" rel="nofollow"&gt;fill out this survey&lt;/a&gt; to support a research study on the usage of Serverless and Function-as-a-Service (FaaS) services, conducted at Chalmers University of Technology. The survey only takes 5-10 minutes of your time. Many thanks for your participation!!
&lt;ul&gt;
&lt;li&gt;The result from this study can be found &lt;a href="https://research.chalmers.se/en/publication/508147" rel="nofollow"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2017-08-27&lt;/strong&gt;: &lt;strong&gt;We need your support!&lt;/strong&gt; LocalStack is growing fast, we now have thousands of developers using the platform on a regular basis. Last month we have recorded a staggering 100k test runs, with 25k+ DynamoDB tables, 20k+ SQS queues, 15k+ Kinesis streams, 13k+ S3 buckets, and 10k+ Lambda functions created locally - for 0$ costs (more details to be published soon). Bug and feature requests are pouring in, and we now need some support from &lt;em&gt;you&lt;/em&gt; to keep the open source version actively maintained. Please check out &lt;a href="https://opencollective.com/localstack" rel="nofollow"&gt;Open Collective&lt;/a&gt; and become a &lt;a href="https://github.com/localstack/localstack#backers"&gt;backer&lt;/a&gt; or &lt;a href="https://github.com/localstack/localstack#backers"&gt;supporter&lt;/a&gt; of the project today! Thanks everybody for contributing. ♥&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2017-07-20&lt;/strong&gt;: Please note: Starting with version &lt;code&gt;0.7.0&lt;/code&gt;, the Docker image will be pushed
and kept up to date under the &lt;strong&gt;new name&lt;/strong&gt; &lt;code&gt;localstack/localstack&lt;/code&gt;. (This means that you may
have to update your CI configurations.) Please refer to the updated
&lt;strong&gt;&lt;a href="doc/end_user_license_agreement"&gt;End-User License Agreement (EULA)&lt;/a&gt;&lt;/strong&gt; for the new versions.
The old Docker image (&lt;code&gt;atlassianlabs/localstack&lt;/code&gt;) is still available but will not be maintained
any longer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h1&gt;
&lt;p&gt;LocalStack spins up the following core Cloud APIs on your local machine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;API Gateway&lt;/strong&gt; at &lt;a href="http://localhost:4567" rel="nofollow"&gt;http://localhost:4567&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kinesis&lt;/strong&gt; at &lt;a href="http://localhost:4568" rel="nofollow"&gt;http://localhost:4568&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DynamoDB&lt;/strong&gt; at &lt;a href="http://localhost:4569" rel="nofollow"&gt;http://localhost:4569&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DynamoDB Streams&lt;/strong&gt; at &lt;a href="http://localhost:4570" rel="nofollow"&gt;http://localhost:4570&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Elasticsearch&lt;/strong&gt; at &lt;a href="http://localhost:4571" rel="nofollow"&gt;http://localhost:4571&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;S3&lt;/strong&gt; at &lt;a href="http://localhost:4572" rel="nofollow"&gt;http://localhost:4572&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Firehose&lt;/strong&gt; at &lt;a href="http://localhost:4573" rel="nofollow"&gt;http://localhost:4573&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lambda&lt;/strong&gt; at &lt;a href="http://localhost:4574" rel="nofollow"&gt;http://localhost:4574&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SNS&lt;/strong&gt; at &lt;a href="http://localhost:4575" rel="nofollow"&gt;http://localhost:4575&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQS&lt;/strong&gt; at &lt;a href="http://localhost:4576" rel="nofollow"&gt;http://localhost:4576&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Redshift&lt;/strong&gt; at &lt;a href="http://localhost:4577" rel="nofollow"&gt;http://localhost:4577&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ES (Elasticsearch Service)&lt;/strong&gt; at &lt;a href="http://localhost:4578" rel="nofollow"&gt;http://localhost:4578&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SES&lt;/strong&gt; at &lt;a href="http://localhost:4579" rel="nofollow"&gt;http://localhost:4579&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Route53&lt;/strong&gt; at &lt;a href="http://localhost:4580" rel="nofollow"&gt;http://localhost:4580&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CloudFormation&lt;/strong&gt; at &lt;a href="http://localhost:4581" rel="nofollow"&gt;http://localhost:4581&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CloudWatch&lt;/strong&gt; at &lt;a href="http://localhost:4582" rel="nofollow"&gt;http://localhost:4582&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSM&lt;/strong&gt; at &lt;a href="http://localhost:4583" rel="nofollow"&gt;http://localhost:4583&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SecretsManager&lt;/strong&gt; at &lt;a href="http://localhost:4584" rel="nofollow"&gt;http://localhost:4584&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;StepFunctions&lt;/strong&gt; at &lt;a href="http://localhost:4585" rel="nofollow"&gt;http://localhost:4585&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CloudWatch Logs&lt;/strong&gt; at &lt;a href="http://localhost:4586" rel="nofollow"&gt;http://localhost:4586&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EventBridge (CloudWatch Events)&lt;/strong&gt; at &lt;a href="http://localhost:4587" rel="nofollow"&gt;http://localhost:4587&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;STS&lt;/strong&gt; at &lt;a href="http://localhost:4592" rel="nofollow"&gt;http://localhost:4592&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IAM&lt;/strong&gt; at &lt;a href="http://localhost:4593" rel="nofollow"&gt;http://localhost:4593&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EC2&lt;/strong&gt; at &lt;a href="http://localhost:4597" rel="nofollow"&gt;http://localhost:4597&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to the above, the &lt;a href="https://localstack.cloud/#pricing" rel="nofollow"&gt;&lt;strong&gt;Pro version&lt;/strong&gt; of LocalStack&lt;/a&gt; supports additional APIs and advanced features, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AppSync&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Athena&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cognito&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ElastiCache&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ECS/EKS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IoT&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lambda Layers&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RDS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;XRay&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interactive UIs to manage resources&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test report dashboards&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;...and much, much more to come!&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-why-localstack" class="anchor" aria-hidden="true" href="#why-localstack"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why LocalStack?&lt;/h2&gt;
&lt;p&gt;LocalStack builds on existing best-of-breed mocking/testing tools, most notably
&lt;a href="https://github.com/mhart/kinesalite"&gt;kinesalite&lt;/a&gt;/&lt;a href="https://github.com/mhart/dynalite"&gt;dynalite&lt;/a&gt;
and &lt;a href="https://github.com/spulec/moto"&gt;moto&lt;/a&gt;. While these tools are &lt;em&gt;awesome&lt;/em&gt; (!), they lack functionality
for certain use cases. LocalStack combines the tools, makes them interoperable, and adds important
missing functionality on top of them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Error injection:&lt;/strong&gt; LocalStack allows to inject errors frequently occurring in real Cloud environments,
for instance &lt;code&gt;ProvisionedThroughputExceededException&lt;/code&gt; which is thrown by Kinesis or DynamoDB if the amount of
read/write throughput is exceeded.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Isolated processes&lt;/strong&gt;: All services in LocalStack run in separate processes. The overhead of additional
processes is negligible, and the entire stack can easily be executed on any developer machine and CI server.
In moto, components are often hard-wired in RAM (e.g., when forwarding a message on an SNS topic to an SQS queue,
the queue endpoint is looked up in a local hash map). In contrast, LocalStack services live in isolation
(separate processes available via HTTP), which fosters true decoupling and more closely resembles the real
cloud environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pluggable services&lt;/strong&gt;: All services in LocalStack are easily pluggable (and replaceable), due to the fact that
we are using isolated processes for each service. This allows us to keep the framework up-to-date and select
best-of-breed mocks for each individual service.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;python&lt;/code&gt; (both Python 2.x and 3.x supported)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pip&lt;/code&gt; (python package manager)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Docker&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installing" class="anchor" aria-hidden="true" href="#installing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing&lt;/h2&gt;
&lt;p&gt;The easiest way to install LocalStack is via &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install localstack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please do &lt;strong&gt;not&lt;/strong&gt; use &lt;code&gt;sudo&lt;/code&gt; or the &lt;code&gt;root&lt;/code&gt; user - LocalStack
should be installed and started entirely under a local non-root user. If you have problems
with permissions in MacOS X Sierra, install with &lt;code&gt;pip install --user localstack&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-in-docker" class="anchor" aria-hidden="true" href="#running-in-docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running in Docker&lt;/h2&gt;
&lt;p&gt;By default, LocalStack gets started inside a Docker container using this command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localstack start
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that on MacOS you may have to run &lt;code&gt;TMPDIR=/private$TMPDIR localstack start --docker&lt;/code&gt; if
&lt;code&gt;$TMPDIR&lt;/code&gt; contains a symbolic link that cannot be mounted by Docker.)&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-using-docker-compose" class="anchor" aria-hidden="true" href="#using-docker-compose"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using &lt;code&gt;docker-compose&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;You can also use the &lt;code&gt;docker-compose.yml&lt;/code&gt; file from the repository and use this command (currently requires &lt;code&gt;docker-compose&lt;/code&gt; version 2.1+):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker-compose up
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that on MacOS you may have to run &lt;code&gt;TMPDIR=/private$TMPDIR docker-compose up&lt;/code&gt; if
&lt;code&gt;$TMPDIR&lt;/code&gt; contains a symbolic link that cannot be mounted by Docker.)&lt;/p&gt;
&lt;p&gt;Use on existing docker-compose project. Add in existing services. The project can be found in docker hub, no need to download or clone source:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;version: '2.1'
services:
...
  localstack:
    image: localstack/localstack
    ports:
      - "4567-4584:4567-4584"
      - "${PORT_WEB_UI-8080}:${PORT_WEB_UI-8080}"
    environment:
      - SERVICES=${SERVICES- }
      - DEBUG=${DEBUG- }
      - DATA_DIR=${DATA_DIR- }
      - PORT_WEB_UI=${PORT_WEB_UI- }
      - LAMBDA_EXECUTOR=${LAMBDA_EXECUTOR- }
      - KINESIS_ERROR_PROBABILITY=${KINESIS_ERROR_PROBABILITY- }
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - "${TMPDIR:-/tmp/localstack}:/tmp/localstack"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To facilitate interoperability, configuration variables can be prefixed with &lt;code&gt;LOCALSTACK_&lt;/code&gt; in docker. For instance, setting &lt;code&gt;LOCALSTACK_SERVICES=s3&lt;/code&gt; is equivalent to &lt;code&gt;SERVICES=s3&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-starting-locally-non-docker-mode" class="anchor" aria-hidden="true" href="#starting-locally-non-docker-mode"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting locally (non-Docker mode)&lt;/h2&gt;
&lt;p&gt;Alternatively, the infrastructure can be spun up on the local host machine (without using Docker) using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localstack start --host
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that this will require &lt;a href="#Developing"&gt;additional dependencies&lt;/a&gt;, and currently is not supported on some operating systems, including Windows.)&lt;/p&gt;
&lt;p&gt;LocalStack will attempt to automatically fetch the missing dependencies when you first start it up in "host" mode; alternatively, you can use the &lt;code&gt;full&lt;/code&gt; profile to install all dependencies at &lt;code&gt;pip&lt;/code&gt; installation time:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install "localstack[full]"
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-configurations" class="anchor" aria-hidden="true" href="#configurations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configurations&lt;/h2&gt;
&lt;p&gt;You can pass the following environment variables to LocalStack:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;SERVICES&lt;/code&gt;: Comma-separated list of service names and (optional) ports they should run on.
If no port is specified, a default port is used. Service names basically correspond to the
&lt;a href="http://docs.aws.amazon.com/cli/latest/reference/#available-services" rel="nofollow"&gt;service names of the AWS CLI&lt;/a&gt;
(&lt;code&gt;kinesis&lt;/code&gt;, &lt;code&gt;lambda&lt;/code&gt;, &lt;code&gt;sqs&lt;/code&gt;, etc), although LocalStack only supports a subset of them.
Example value: &lt;code&gt;kinesis,lambda:4569,sqs:4570&lt;/code&gt; to start Kinesis on the default port,
Lambda on port 4569, and SQS on port 4570. In addition, the following shorthand values can be
specified to run a predefined ensemble of services:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;serverless&lt;/code&gt;: run services often used for Serverless apps (&lt;code&gt;iam&lt;/code&gt;, &lt;code&gt;lambda&lt;/code&gt;, &lt;code&gt;dynamodb&lt;/code&gt;, &lt;code&gt;apigateway&lt;/code&gt;, &lt;code&gt;s3&lt;/code&gt;, &lt;code&gt;sns&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DEFAULT_REGION&lt;/code&gt;: AWS region to use when talking to the API (defaults to &lt;code&gt;us-east-1&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HOSTNAME&lt;/code&gt;: Name of the host to expose the services internally (defaults to &lt;code&gt;localhost&lt;/code&gt;).
Use this to customize the framework-internal communication, e.g., if services are
started in different containers using docker-compose.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HOSTNAME_EXTERNAL&lt;/code&gt;: Name of the host to expose the services externally (defaults to &lt;code&gt;localhost&lt;/code&gt;).
This host is used, e.g., when returning queue URLs from the SQS service to the client.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;SERVICE&amp;gt;_PORT&lt;/code&gt;: Port number to bind a specific service to (defaults to service ports above).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;SERVICE&amp;gt;_PORT_EXTERNAL&lt;/code&gt;: Port number to expose a specific service externally (defaults to service ports above). &lt;code&gt;SQS_PORT_EXTERNAL&lt;/code&gt;, for example, is used when returning queue URLs from the SQS service to the client.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;USE_SSL&lt;/code&gt;: Whether to use &lt;code&gt;https://...&lt;/code&gt; URLs with SSL encryption (defaults to &lt;code&gt;false&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;KINESIS_ERROR_PROBABILITY&lt;/code&gt;: Decimal value between 0.0 (default) and 1.0 to randomly
inject &lt;code&gt;ProvisionedThroughputExceededException&lt;/code&gt; errors into Kinesis API responses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;KINESIS_SHARD_LIMIT&lt;/code&gt;: Integer value (defaults to &lt;code&gt;100&lt;/code&gt;) or &lt;code&gt;Infinity&lt;/code&gt; (to disable), in which to kinesalite will start throwing exceptions to mimick the &lt;a href="https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html" rel="nofollow"&gt;default shard limit&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;KINESIS_LATENCY&lt;/code&gt;: Integer value (defaults to &lt;code&gt;500&lt;/code&gt;) or &lt;code&gt;0&lt;/code&gt; (to disable), in which to kinesalite will delay returning a response in order to mimick latency from a live AWS call.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DYNAMODB_ERROR_PROBABILITY&lt;/code&gt;: Decimal value between 0.0 (default) and 1.0 to randomly
inject &lt;code&gt;ProvisionedThroughputExceededException&lt;/code&gt; errors into DynamoDB API responses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_EXECUTOR&lt;/code&gt;: Method to use for executing Lambda functions. Possible values are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;local&lt;/code&gt;: run Lambda functions in a temporary directory on the local machine&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker&lt;/code&gt;: run each function invocation in a separate Docker container&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker-reuse&lt;/code&gt;: create one Docker container per function and reuse it across invocations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For &lt;code&gt;docker&lt;/code&gt; and &lt;code&gt;docker-reuse&lt;/code&gt;, if LocalStack itself is started inside Docker, then
the &lt;code&gt;docker&lt;/code&gt; command needs to be available inside the container (usually requires to run the
container in privileged mode). Default is &lt;code&gt;docker&lt;/code&gt;, fallback to &lt;code&gt;local&lt;/code&gt; if Docker is not available.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_REMOTE_DOCKER&lt;/code&gt; determines whether Lambda code is copied or mounted into containers.
Possible values are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;true&lt;/code&gt; (default): your Lambda function definitions will be passed to the container by
copying the zip file (potentially slower). It allows for remote execution, where the host
and the client are not on the same machine.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;false&lt;/code&gt;: your Lambda function definitions will be passed to the container by mounting a
volume (potentially faster). This requires to have the Docker client and the Docker
host on the same machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_DOCKER_NETWORK&lt;/code&gt; Specifies the docker network for the container running your lambda function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_CONTAINER_REGISTRY&lt;/code&gt; Use an alternative docker registry to pull lambda execution containers. Default is &lt;code&gt;lambci/lambda&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DATA_DIR&lt;/code&gt;: Local directory for saving persistent data (currently only supported for these services:
Kinesis, DynamoDB, Elasticsearch, S3). Set it to &lt;code&gt;/tmp/localstack/data&lt;/code&gt; to enable persistence
(&lt;code&gt;/tmp/localstack&lt;/code&gt; is mounted into the Docker container), leave blank to disable
persistence (default).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;PORT_WEB_UI&lt;/code&gt;: Port for the Web user interface (dashboard). Default is &lt;code&gt;8080&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;SERVICE&amp;gt;_BACKEND&lt;/code&gt;: Custom endpoint URL to use for a specific service, where &lt;code&gt;&amp;lt;SERVICE&amp;gt;&lt;/code&gt; is the uppercase
service name (currently works for: &lt;code&gt;APIGATEWAY&lt;/code&gt;, &lt;code&gt;CLOUDFORMATION&lt;/code&gt;, &lt;code&gt;DYNAMODB&lt;/code&gt;, &lt;code&gt;ELASTICSEARCH&lt;/code&gt;,
&lt;code&gt;KINESIS&lt;/code&gt;, &lt;code&gt;S3&lt;/code&gt;, &lt;code&gt;SNS&lt;/code&gt;, &lt;code&gt;SQS&lt;/code&gt;). This allows to easily integrate third-party services into LocalStack.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;FORCE_NONINTERACTIVE&lt;/code&gt;: when running with Docker, disables the &lt;code&gt;--interactive&lt;/code&gt; and &lt;code&gt;--tty&lt;/code&gt; flags. Useful when running headless.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DOCKER_FLAGS&lt;/code&gt;: Allows to pass custom flags (e.g., volume mounts) to "docker run" when running LocalStack in Docker.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DOCKER_CMD&lt;/code&gt;: Shell command used to run Docker containers, e.g., set to &lt;code&gt;"sudo docker"&lt;/code&gt; to run as sudo (default: &lt;code&gt;docker&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;START_WEB&lt;/code&gt;: Flag to control whether the Web API should be started in Docker (values: &lt;code&gt;0&lt;/code&gt;/&lt;code&gt;1&lt;/code&gt;; default: &lt;code&gt;1&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_FALLBACK_URL&lt;/code&gt;: Fallback URL to use when a non-existing Lambda is invoked. Either records invocations in DynamoDB (value &lt;code&gt;dynamodb://&amp;lt;table_name&amp;gt;&lt;/code&gt;) or forwards invocations as a POST request (value &lt;code&gt;http(s)://...&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;EXTRA_CORS_ALLOWED_HEADERS&lt;/code&gt;: Comma-separated list of header names to be be added to &lt;code&gt;Access-Control-Allow-Headers&lt;/code&gt; CORS header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;EXTRA_CORS_EXPOSE_HEADERS&lt;/code&gt;: Comma-separated list of header names to be be added to &lt;code&gt;Access-Control-Expose-Headers&lt;/code&gt; CORS header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_JAVA_OPTS&lt;/code&gt;: Allow passing custom JVM options (e.g., &lt;code&gt;-Xmx512M&lt;/code&gt;) to Java Lambdas executed in Docker. Use &lt;code&gt;_debug_port_&lt;/code&gt; placeholder to configure the debug port (e.g., &lt;code&gt;-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=_debug_port_&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, the following &lt;em&gt;read-only&lt;/em&gt; environment variables are available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LOCALSTACK_HOSTNAME&lt;/code&gt;: Name of the host where LocalStack services are available.
This is needed in order to access the services from within your Lambda functions
(e.g., to store an item to DynamoDB or S3 from Lambda).
The variable &lt;code&gt;LOCALSTACK_HOSTNAME&lt;/code&gt; is available for both, local Lambda execution
(&lt;code&gt;LAMBDA_EXECUTOR=local&lt;/code&gt;) and execution inside separate Docker containers (&lt;code&gt;LAMBDA_EXECUTOR=docker&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-dynamically-updating-configuration-at-runtime" class="anchor" aria-hidden="true" href="#dynamically-updating-configuration-at-runtime"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dynamically updating configuration at runtime&lt;/h3&gt;
&lt;p&gt;Each of the service APIs listed &lt;a href="https://github.com/localstack/localstack#overview"&gt;above&lt;/a&gt; defines
a backdoor API under the path &lt;code&gt;/?_config_&lt;/code&gt; which allows to dynamically update configuration variables
defined in &lt;a href="https://github.com/localstack/localstack/blob/master/localstack/config.py"&gt;&lt;code&gt;config.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, to dynamically set &lt;code&gt;KINESIS_ERROR_PROBABILITY=1&lt;/code&gt; at runtime, use the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -v -d '{"variable":"KINESIS_ERROR_PROBABILITY","value":1}' 'http://localhost:4568/?_config_'
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-initializing-a-fresh-instance" class="anchor" aria-hidden="true" href="#initializing-a-fresh-instance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Initializing a fresh instance&lt;/h3&gt;
&lt;p&gt;When a container is started for the first time, it will execute files with extensions .sh that are found in &lt;code&gt;/docker-entrypoint-initaws.d&lt;/code&gt;. Files will be executed in alphabetical order. You can easily create aws resources on localstack using &lt;code&gt;awslocal&lt;/code&gt; (or &lt;code&gt;aws&lt;/code&gt;) cli tool in the initialization scripts.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-a-note-about-using-custom-ssl-certificates-for-use_ssl1" class="anchor" aria-hidden="true" href="#a-note-about-using-custom-ssl-certificates-for-use_ssl1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A note about using custom SSL certificates (for &lt;code&gt;USE_SSL=1&lt;/code&gt;)&lt;/h2&gt;
&lt;p&gt;If you need to use your own SSL Certificate and keep it persistent and not use the random automatic generated Certificate, you can place into the localstack temporary directory :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/tmp/localstack/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the three named files below :&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;server.test.pem
server.test.pem.crt
server.test.pem.key&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;the file &lt;code&gt;server.test.pem&lt;/code&gt; must contains your key file content, your certificate and chain certificate files contents (do a cat in this order)&lt;/li&gt;
&lt;li&gt;the file &lt;code&gt;server.test.pem.crt&lt;/code&gt; must contains your certificate and chains files contents (do a 'cat' in this order)&lt;/li&gt;
&lt;li&gt;the file server.test.pem.key must contains your key file content&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-using-use_ssl-and-own-persistent-certificate-with-docker-compose" class="anchor" aria-hidden="true" href="#using-use_ssl-and-own-persistent-certificate-with-docker-compose"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using USE_SSL and own persistent certificate with docker-compose&lt;/h3&gt;
&lt;p&gt;Typically with docker-compose you can add into docker-compose.yml this volume to the localstack services :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;volumes:
      - "${PWD}/ls_tmp:/tmp/localstack"
      - "/var/run/docker.sock:/var/run/docker.sock"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;local directory &lt;strong&gt;ls_tmp&lt;/strong&gt; must contains the three files (server.test.pem, server.test.pem.crt, server.test.pem.key)&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-accessing-the-infrastructure-via-cli-or-code" class="anchor" aria-hidden="true" href="#accessing-the-infrastructure-via-cli-or-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Accessing the infrastructure via CLI or code&lt;/h2&gt;
&lt;p&gt;You can point your &lt;code&gt;aws&lt;/code&gt; CLI to use the local infrastructure, for example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aws --endpoint-url=http://localhost:4568 kinesis list-streams
{
    "StreamNames": []
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NEW&lt;/strong&gt;: Check out &lt;a href="https://github.com/localstack/awscli-local"&gt;awslocal&lt;/a&gt;, a thin CLI wrapper
that runs commands directly against LocalStack (no need to specify &lt;code&gt;--endpoint-url&lt;/code&gt; anymore).
Install it via &lt;code&gt;pip install awscli-local&lt;/code&gt;, and then use it as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;awslocal kinesis list-streams
{
    "StreamNames": []
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: Use the environment variable &lt;code&gt;$LOCALSTACK_HOSTNAME&lt;/code&gt; to determine the target host
inside your Lambda function. See &lt;a href="#Configurations"&gt;Configurations&lt;/a&gt; section for more details.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-client-libraries" class="anchor" aria-hidden="true" href="#client-libraries"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Client Libraries&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python: &lt;a href="https://github.com/localstack/localstack-python-client"&gt;https://github.com/localstack/localstack-python-client&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;alternatively, you can also use &lt;code&gt;boto3&lt;/code&gt; and use the &lt;code&gt;endpoint_url&lt;/code&gt; parameter when creating a connection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(more coming soon...)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-integration-with-nosetests" class="anchor" aria-hidden="true" href="#integration-with-nosetests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Integration with nosetests&lt;/h2&gt;
&lt;p&gt;If you want to use LocalStack in your integration tests (e.g., nosetests), simply fire up the
infrastructure in your test setup method and then clean up everything in your teardown method:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from localstack.services import infra

def setup():
    infra.start_infra(asynchronous=True)

def teardown():
    infra.stop_infra()

def my_app_test():
    # here goes your test logic
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See the example test file &lt;code&gt;tests/integration/test_integration.py&lt;/code&gt; for more details.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-integration-with-serverless" class="anchor" aria-hidden="true" href="#integration-with-serverless"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Integration with Serverless&lt;/h2&gt;
&lt;p&gt;You can use the &lt;a href="https://www.npmjs.com/package/serverless-localstack" rel="nofollow"&gt;&lt;code&gt;serverless-localstack&lt;/code&gt;&lt;/a&gt; plugin to easily run &lt;a href="https://serverless.com/framework/" rel="nofollow"&gt;Serverless&lt;/a&gt; applications on LocalStack.
For more information, please check out the plugin repository here:
&lt;a href="https://github.com/localstack/serverless-localstack"&gt;https://github.com/localstack/serverless-localstack&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-local-code-with-lambda" class="anchor" aria-hidden="true" href="#using-local-code-with-lambda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using local code with Lambda&lt;/h2&gt;
&lt;p&gt;In order to mount a local folder, ensure that &lt;code&gt;LAMBDA_REMOTE_DOCKER&lt;/code&gt; is set to &lt;code&gt;false&lt;/code&gt; then set the S3 bucket name to &lt;code&gt;__local__&lt;/code&gt; and the S3 key to your local path:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    awslocal lambda create-function --function-name myLambda \
      --code S3Bucket="__local__",S3Key="/my/local/lambda/folder" \
      --handler index.myHandler \
      --runtime nodejs8.10 \
      --role whatever
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-integration-with-javajunit" class="anchor" aria-hidden="true" href="#integration-with-javajunit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Integration with Java/JUnit&lt;/h2&gt;
&lt;p&gt;In order to use LocalStack with Java, the project ships with a simple JUnit runner and a JUnit 5 extension. Take a look
at the example JUnit test in &lt;code&gt;ext/java&lt;/code&gt;. When you run the test, all dependencies are automatically
downloaded and installed to a temporary directory in your system.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;...
import cloud.localstack.LocalstackTestRunner;
import cloud.localstack.TestUtils;

@RunWith(LocalstackTestRunner.class)
public class MyCloudAppTest {

  @Test
  public void testLocalS3API() {
    AmazonS3 s3 = TestUtils.getClientS3()
    List&amp;lt;Bucket&amp;gt; buckets = s3.listBuckets();
    ...
  }

}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or with JUnit 5 :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@ExtendWith(LocalstackExtension.class)
public class MyCloudAppTest {
   ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, there is a version of the LocalStack Test Runner which runs in a docker container
instead of installing LocalStack on the current machine. The only dependency is to have docker
installed locally. The test runner will automatically pull the image and start the container for the
duration of the test.  The container can be configured by using the @LocalstackDockerProperties annotation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@RunWith(LocalstackDockerTestRunner.class)
@LocalstackDockerProperties(services = { "sqs", "kinesis:77077" })
public class MyDockerCloudAppTest {

  @Test
  public void testKinesis() {
    AmazonKinesis kinesis = DockerTestUtils.getClientKinesis();

    ListStreamsResult streams = kinesis.listStreams();
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or with JUnit 5 :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@ExtendWith(LocalstackDockerExtension.class)
@LocalstackDockerProperties(services = { "sqs", "kinesis:77077" })
public class MyDockerCloudAppTest {
   ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The LocalStack JUnit test runner is published as an artifact in Maven Central.
Simply add the following dependency to your &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;cloud.localstack&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;localstack-utils&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.1.22&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can configure the Docker behaviour using the &lt;code&gt;@LocalstackDockerProperties&lt;/code&gt; annotation with the following parameters:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;property&lt;/th&gt;
&lt;th&gt;usage&lt;/th&gt;
&lt;th&gt;type&lt;/th&gt;
&lt;th&gt;default value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pullNewImage&lt;/td&gt;
&lt;td&gt;Determines if a new image is pulled from the docker repo before the tests are run.&lt;/td&gt;
&lt;td&gt;boolean&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;randomizePorts&lt;/td&gt;
&lt;td&gt;Determines if the container should expose the default local stack ports (4567-4583) or if it should expose randomized ports.&lt;/td&gt;
&lt;td&gt;boolean&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;services&lt;/td&gt;
&lt;td&gt;Determines which services should be run when the localstack starts.&lt;/td&gt;
&lt;td&gt;String[]&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;imageTag&lt;/td&gt;
&lt;td&gt;Use a specific image tag for docker container&lt;/td&gt;
&lt;td&gt;String&lt;/td&gt;
&lt;td&gt;latest&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hostNameResolver&lt;/td&gt;
&lt;td&gt;Used for determining the host name of the machine running the docker containers so that the containers can be addressed.&lt;/td&gt;
&lt;td&gt;IHostNameResolver&lt;/td&gt;
&lt;td&gt;localhost&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;environmentVariableProvider&lt;/td&gt;
&lt;td&gt;Used for injecting environment variables into the container.&lt;/td&gt;
&lt;td&gt;IEnvironmentVariableProvider&lt;/td&gt;
&lt;td&gt;Empty Map&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;NB : When specifying the port in the &lt;code&gt;services&lt;/code&gt; property, you cannot use &lt;code&gt;randomizePorts = true&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-troubleshooting" class="anchor" aria-hidden="true" href="#troubleshooting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Troubleshooting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you're using AWS Java libraries with Kinesis, please, refer to &lt;a href="https://github.com/mhart/kinesalite#cbor-protocol-issues-with-the-java-sdk"&gt;CBOR protocol issues with the Java SDK guide&lt;/a&gt; how to disable CBOR protocol which is not supported by kinesalite.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accessing local S3 from Java: To avoid domain name resolution issues, you need to enable &lt;strong&gt;path style access&lt;/strong&gt; on your client:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;s3.setS3ClientOptions(S3ClientOptions.builder().setPathStyleAccess(true).build());
// There is also an option to do this if you're using any of the client builder classes:
AmazonS3ClientBuilder builder = AmazonS3ClientBuilder.standard();
builder.withPathStyleAccessEnabled(true);
...
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Mounting the temp. directory: Note that on MacOS you may have to run &lt;code&gt;TMPDIR=/private$TMPDIR docker-compose up&lt;/code&gt; if
&lt;code&gt;$TMPDIR&lt;/code&gt; contains a symbolic link that cannot be mounted by Docker.
(See details here: &lt;a href="https://bitbucket.org/atlassian/localstack/issues/40/getting-mounts-failed-on-docker-compose-up" rel="nofollow"&gt;https://bitbucket.org/atlassian/localstack/issues/40/getting-mounts-failed-on-docker-compose-up&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you run into file permission issues on &lt;code&gt;pip install&lt;/code&gt; under Mac OS (e.g., &lt;code&gt;Permission denied: '/Library/Python/2.7/site-packages/six.py'&lt;/code&gt;), then you may have to re-install &lt;code&gt;pip&lt;/code&gt; via Homebrew (see &lt;a href="https://github.com/localstack/localstack/issues/260#issuecomment-334458631"&gt;this discussion thread&lt;/a&gt;). Alternatively, try installing
with the &lt;code&gt;--user&lt;/code&gt; flag: &lt;code&gt;pip install --user localstack&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you are deploying within OpenShift, please be aware: the pod must run as &lt;code&gt;root&lt;/code&gt;, and the user must have capabilities added to the running pod, in order to allow Elasticsearch to be run as the non-root &lt;code&gt;localstack&lt;/code&gt; user.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The environment variable &lt;code&gt;no_proxy&lt;/code&gt; is rewritten by LocalStack.
(Internal requests will go straight via localhost, bypassing any proxy configuration).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For troubleshooting LocalStack start issues, you can check debug logs by running &lt;code&gt;DEBUG=1 localstack start&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In case you get errors related to node/nodejs, you may find (this issue comment: &lt;a href="https://github.com/localstack/localstack/issues/227#issuecomment-319938530"&gt;https://github.com/localstack/localstack/issues/227#issuecomment-319938530&lt;/a&gt;) helpful.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you are using AWS Java libraries and need to disable SSL certificate checking, add &lt;code&gt;-Dcom.amazonaws.sdk.disableCertChecking&lt;/code&gt; to the java invocation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-developing" class="anchor" aria-hidden="true" href="#developing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Developing&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-requirements-for-developing-or-starting-locally" class="anchor" aria-hidden="true" href="#requirements-for-developing-or-starting-locally"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements for developing or starting locally&lt;/h3&gt;
&lt;p&gt;To develop new features, or to start the stack locally (outside of Docker), the following additional tools are required:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;make&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;npm&lt;/code&gt; (node.js package manager)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;java&lt;/code&gt;/&lt;code&gt;javac&lt;/code&gt; (Java 8 runtime environment and compiler)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mvn&lt;/code&gt; (Maven, the build system for Java)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-development-environment" class="anchor" aria-hidden="true" href="#development-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development Environment&lt;/h3&gt;
&lt;p&gt;If you pull the repo in order to extend/modify LocalStack, run this command to install
all the dependencies:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will install the required pip dependencies in a local Python virtualenv directory
&lt;code&gt;.venv&lt;/code&gt; (your global python packages will remain untouched), as well as some node modules
in &lt;code&gt;./localstack/node_modules/&lt;/code&gt;. Depending on your system, some pip/npm modules may require
additional native libs installed.&lt;/p&gt;
&lt;p&gt;The Makefile contains a target to conveniently run the local infrastructure for development:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make infra
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out the
&lt;a href="https://github.com/localstack/localstack/tree/master/doc/developer_guides"&gt;developer guide&lt;/a&gt; which
contains a few instructions on how to get started with developing (and debugging) features for
LocalStack.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h2&gt;
&lt;p&gt;The project contains a set of unit and integration tests that can be kicked off via a make
target:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make test
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-web-dashboard" class="anchor" aria-hidden="true" href="#web-dashboard"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web Dashboard&lt;/h2&gt;
&lt;p&gt;The projects also comes with a simple Web dashboard that allows to view the deployed AWS
components and the relationship between them.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localstack web
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-other-ui-clients" class="anchor" aria-hidden="true" href="#other-ui-clients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other UI Clients&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://getcommandeer.com" rel="nofollow"&gt;Commandeer desktop app&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.npmjs.com/package/dynamodb-admin" rel="nofollow"&gt;DynamoDB Admin Web UI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-change-log" class="anchor" aria-hidden="true" href="#change-log"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Change Log&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;v0.10.5: Various CloudFormation fixes: deployment of API GW method integrations, properly skip resource updates, Lambda SQS event source mapping, avoid duplicate resource creation, support for ApiGateway::GatewayResponse and Events::Rule, log groups for Lambdas; support adding Lambda policies; customize Docker registry for Lambda images; support multiple configurations in S3 notifications; fix encoding of non-ASCII results from API Gateway; allow docker-reuse to use mounted volumes; support presigned S3 URL upload notifications; fix lookup of Python Lambda handler in sub directories; upgrade kinesalite; fix duplicate CORS headers; fix mapping of Lambda versions and ARNs; fix SNS x-amz-sns-message-type header; send SNS confirmation message for HTTP(S) subscriptions; fix DynamoDB local libs for Docker Alpine; add CF support for SNS subscriptions; fix RecordId for firehose put-record-batch; fix SQS messages with multi-byte characters; avoid creating multiple SNS subscriptions; add .bat script and support running under Windows; fix S3 location constraint for CF&lt;/li&gt;
&lt;li&gt;v0.10.4: Add checks for open UDP ports; fix S3 chunked encoding uploads; fix LatestStreamLabel; fix CORS headers for SQS/SNS; set Java lambda debug port only when needed; expose default region in a util function; fix MacOS tmp folder; clear tmp supervisord logs at container startup; fix signed header requests for S3; expose Web UI via HTTPS; add Timestamp to SNS messages; fix attributes for SQS queues addressed via URL&lt;/li&gt;
&lt;li&gt;v0.10.3: Allow specifying data types for CF attributes; add API for service status and starting services at runtime; support NextShardIterator in DDB streams; add mock responses for S3 encryption and replication; fix rendering of resources in web UI; custom SQS queue attributes; fix Lambda docker command and imports; fix SQS queue physical ID in CF; allow proxy listener to define custom backend per request; support Lambda event body over stdin; exclude &lt;code&gt;ingest-geoip&lt;/code&gt; ES module to optimize image size; skip checking MD5 on S3 copy; fix DynamoDB table ARN for CF; fix CF deployment of StepFunction activities; fix uploading of Java Lambda as JAR in ZIP; fix installing libs for plugins; added &lt;code&gt;LAMBDA_JAVA_OPTS&lt;/code&gt; for Java Lambda debugging; bump Maven dependency versions; refactor Lambda API; fix boolean strings in CF templates; allow overriding AWS account id with &lt;code&gt;TEST_AWS_ACCOUNT_ID&lt;/code&gt;; fix incorrect region for API GW resources created via CF; fix permissions for cache files in &lt;code&gt;/tmp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;v0.10.2: Fix logging issue with async Lambdas; fix kinesis records processing; add basic support for &lt;code&gt;Ref&lt;/code&gt; in CloudFormation; fix ddb streams uuid generation; upgrade travis CI setup; fix DynamoDB error messages; cache server processes&lt;/li&gt;
&lt;li&gt;v0.10.0: Lazy loading of libraries; fix handling of regions; add API multiserver; improve CPU profiling; fix ES xpack installation; add basic EventBridge support; refactor Lambda API and executor; add MessageAttributes on SNS payloads; tagging for SNS; ability to customize docker command&lt;/li&gt;
&lt;li&gt;v0.9.6: Add API Gateway SQS proxy; fix command to push Docker image; fix Docker bridge IP configuration; fix SSL issue in dashboard infra; updates to README&lt;/li&gt;
&lt;li&gt;v0.9.5: Reduce Docker image size by squashing; fix response body for presigned URL S3 PUT requests; fix CreateDate returned by IAM; fix account IDs for CF and SNS; fix topic checks for SMS using SNS; improve documentation around &lt;code&gt;@LocalstackDockerProperties&lt;/code&gt;; add basic EC2 support; upgrade to ElasticSearch 6.7; set Last-Modified header in S3; preserve logic with uppercase event keys in Java; add support for nodejs 10.x Lambdas&lt;/li&gt;
&lt;li&gt;v0.9.4: Fix ARNs in CloudFormation deployments; write stderr to file in supervisord; fix Lambda invocation times; fix canonicalization of service names when running in Docker; add support for &lt;code&gt;@Nested&lt;/code&gt; in Junit5; add support for batch/transaction in DynamoDB; fix output buffering for subprocesses; assign unique ports under docker-reuse; check if topic ARN exists before publish&lt;/li&gt;
&lt;li&gt;v0.9.3: Fix output buffering of child processes; new release of Java libs; add imageTag attribute for Java annotation&lt;/li&gt;
&lt;li&gt;v0.9.2: Update to Python 3 in Dockerfile; preserve attributes when SNS Subscribe; fix event source mapping in Lambda; fix CORS ExposeHeaders; set Lambda timeout in secs; add tags support for Lambda/Firehose; add message attributes for SQS/Lambda; fix shard count support for Kinesis; fix port mappings for CloudFormation&lt;/li&gt;
&lt;li&gt;v0.9.1: Define dependent and composite services in config; forward Lambda logs to CloudWatch Logs; add SQS event deserializing for Lambda; fix AWS_PROXY for JSON list payload; add START_WEB config parameter; return correct location for S3 multipart uploads; add support for Lambda custom runtime; fix account ID for IAM responses; fix using correct SSL cert; limit memory usage for Java processes; fix unicode encoding for SNS messages; allow using &lt;code&gt;LOCALSTACK_&lt;/code&gt; prefix in Docker environment variables; enable request forwarding for non-existing Lambdas; fix large downloads for S3; add API endpoint for dynamically updating config variables; fix CloudFormation stack update&lt;/li&gt;
&lt;li&gt;v0.9.0: Enhance integration with Serverless; refactor CloudFormation implementation; add support for Step Functions, IAM, STS; fix CloudFormation integration; support mounting Lambda code locally; add &lt;code&gt;docker-entrypoint-initaws.d&lt;/code&gt; dir for initializing resources; add S3Event Parser for Lambda; fix S3 chunk encoding; fix S3 multipart upload notification; add dotnetcore2.1 and ruby2.5 Lambda runtimes; fix issues with JDK 9; install ES plugins available in AWS&lt;/li&gt;
&lt;li&gt;v0.8.10: Add kclpy to pip package; fix badges in README&lt;/li&gt;
&lt;li&gt;v0.8.9: Replace moto-ext with upstream moto; fix SNS message attributes; fix swagger; make external SQS port configurable; support for SNS DeleteTopic; S3 notifications for multipart uploads; support requestContext in AWS_PROXY integration; update docs for SSL usage&lt;/li&gt;
&lt;li&gt;v0.8.8: Support Docker network config for Lambda containers; support queryStringParameters for Lambda AWS_PROXY apigateway; add AWS SecretsManager service; add SQS/Lambda integration; add support for Firehose Kinesis source; add GetAlias to Lambda API; add function properties to LambdaContext for invocations; fix extraction of Java Lambda archives; check region headers for SNS; fix Lambda output buffering; fix S3 download of gzip; bump ElasticMQ to 0.14.5; fix Lambda response codes; fix syntax issues for Python 3.7&lt;/li&gt;
&lt;li&gt;v0.8.7: Support .Net Core 2.0 and nodejs8.10 Lambdas; refactor Java libs and integrate with JUnit 5; support tags for ES domains; add CloudFormation support for SNS topics; fix kinesis error injection; fix override of &lt;code&gt;ES_JAVA_OPTS&lt;/code&gt;; fix SQS CORS preflight response; fix S3 content md5 checks and Host header; fix ES startup issue; Bump elasticmq to 0.13.10; bump kinesalite version&lt;/li&gt;
&lt;li&gt;v0.8.6: Fixes for Windows installation; bump ES to 6.2.0; support filter policy for SNS; upgrade kinesalite; refactor JUnit runner; support Lambda PutFunctionConcurrency and GetEventSourceMapping; fixes for Terraform; add golang support to Lambda; fix file permission issue in Java Lambda tests; fix S3 bucket notification config&lt;/li&gt;
&lt;li&gt;v0.8.5: Fix DDB streams event type; implement CF Fn::GetAZs; async lambda for DDB events; fix S3 content-type; fix CF deployer for SQS; fix S3 ExposePorts; fix message subject in SNS; support for Firehose -&amp;gt; ES; pass external env vars to containers from Java; add mock for list-queue-tags; enhance docker test runner; fix Windows installation issues; new version of Java libs&lt;/li&gt;
&lt;li&gt;v0.8.4: Fix &lt;code&gt;pipenv&lt;/code&gt; dependency issue; Docker JUnit test runner; POJO type for Java Lambda RequestHandler; Java Lambda DynamoDB event; reuse Docker containers for Lambda invocations; API Gateway wildcard path segments; fix SNS RawMessageDelivery&lt;/li&gt;
&lt;li&gt;v0.8.3: Fix DDB stream events for UPDATE operations; fix DDB streams sequence numbers; fix transfer-encoding for DDB; fix requests with missing content-length header; support non-ascii content in DynamoDB items; map external port for SQS queue URLs; default to LAMBDA_REMOTE_DOCKER=true if running in Docker; S3 lifecycle support; reduce Docker image size&lt;/li&gt;
&lt;li&gt;v0.8.2: Fix S3 bucket notification configuration; CORS headers for API Gateway; fix &amp;gt;128k S3 multipart uploads; return valid ShardIDs in DynamoDB Streams; fix hardcoded "ddblocal" DynamoDB TableARN; import default service ports from localstack-client; fix S3 bucket policy response; Execute lambdas asynchronously if the source is a topic&lt;/li&gt;
&lt;li&gt;v0.8.1: Improvements in Lambda API: publish-version, list-version, function aliases; use single map with Lambda function details; workaround for SQS .fifo queues; add test for S3 upload; initial support for SSM; fix regex to replace SQS queue URL hostnames; update linter (single quotes); use &lt;code&gt;docker.for.mac.localhost&lt;/code&gt; to connect to LocalStack from Docker on Mac; fix b64 encoding for Java Lambdas; fix path of moto_server command&lt;/li&gt;
&lt;li&gt;v0.8.0: Fix request data in &lt;code&gt;GenericProxyHandler&lt;/code&gt;; add &lt;code&gt;$PORT_WEB_UI&lt;/code&gt; and &lt;code&gt;$HOSTNAME_EXTERNAL&lt;/code&gt; configs; API Gateway path parameters; enable flake8 linting; add config for service backend URLs; use ElasticMQ instead of moto for SQS; expose &lt;code&gt;$LOCALSTACK_HOSTNAME&lt;/code&gt;; custom environment variable support for Lambda; improve error logging and installation for Java/JUnit; add support for S3 REST Object POST&lt;/li&gt;
&lt;li&gt;v0.7.5: Fix issue with incomplete parallel downloads; bypass http_proxy for internal requests; use native Python code to unzip archives; download KCL client libs only for testing and not on pip install&lt;/li&gt;
&lt;li&gt;v0.7.4: Refactor CLI and enable plugins; support unicode names for S3; fix SQS names containing a dot character; execute Java Lambda functions in Docker containers; fix DynamoDB error handling; update docs&lt;/li&gt;
&lt;li&gt;v0.7.3: Extract proxy listeners into (sub-)classes; put java libs into a single "fat" jar; fix issue with non-daemonized threads; refactor code to start flask services&lt;/li&gt;
&lt;li&gt;v0.7.2: Fix DATA_DIR config when running in Docker; fix Maven dependencies; return 'ConsumedCapacity' from DynamoDB get-item; use Queue ARN instead of URL for S3 bucket notifications&lt;/li&gt;
&lt;li&gt;v0.7.1: Fix S3 API to GET bucket notifications; release Java artifacts to Maven Central; fix S3 file access from Spark; create DDB stream on UpdateTable; remove AUI dependency, optimize size of Docker image&lt;/li&gt;
&lt;li&gt;v0.7.0: Support for Kinesis in CloudFormation; extend and integrate Java tests in CI; publish Docker image under new name; update READMEs and license agreements&lt;/li&gt;
&lt;li&gt;v0.6.2: Major refactoring of installation process, lazy loading of dependencies&lt;/li&gt;
&lt;li&gt;v0.6.1: Add CORS headers; platform compatibility fixes (remove shell commands and sh module); add CloudFormation validate-template; fix Lambda execution in Docker; basic domain handling in ES API; API Gateway authorizers&lt;/li&gt;
&lt;li&gt;v0.6.0: Load services as plugins; fix service default ports; fix SQS-&amp;gt;SNS and MD5 of message attributes; fix Host header for S3&lt;/li&gt;
&lt;li&gt;v0.5.5: Enable SSL encryption for all service endpoints (&lt;code&gt;USE_SSL&lt;/code&gt; config); create Docker base image; fix issue with DATA_DIR&lt;/li&gt;
&lt;li&gt;v0.5.4: Remove hardcoded /tmp/ for Windows-compat.; update CLI and docs; fix S3/SNS notifications; disable Elasticsearch compression&lt;/li&gt;
&lt;li&gt;v0.5.3: Add CloudFormation support for serverless / API Gateway deployments; fix installation via pypi; minor fix for Java (passing of environment variables)&lt;/li&gt;
&lt;li&gt;v0.5.0: Extend DynamoDB Streams API; fix keep-alive connection for S3; fix deadlock in nested Lambda executions; add integration SNS-&amp;gt;Lambda; CloudFormation serverless example; replace dynalite with DynamoDBLocal; support Lambda execution in remote Docker container; fix CloudWatch metrics for Lambda invocation errors&lt;/li&gt;
&lt;li&gt;v0.4.3: Initial support for CloudWatch metrics (for Lambda functions); HTTP forwards for API Gateway; fix S3 message body signatures; download Lambda archive from S3 bucket; fix/extend ES tests&lt;/li&gt;
&lt;li&gt;v0.4.2: Initial support for Java Lambda functions; CloudFormation deployments; API Gateway tests&lt;/li&gt;
&lt;li&gt;v0.4.1: Python 3 compatibility; data persistence; add seq. numbers in Kinesis events; limit Elasticsearch memory&lt;/li&gt;
&lt;li&gt;v0.4.0: Execute Lambda functions in Docker containers; CORS headers for S3&lt;/li&gt;
&lt;li&gt;v0.3.11: Add Route53, SES, CloudFormation; DynamoDB fault injection; UI tweaks; refactor config&lt;/li&gt;
&lt;li&gt;v0.3.10: Add initial support for S3 bucket notifications; fix subprocess32 installation&lt;/li&gt;
&lt;li&gt;v0.3.9: Make services/ports configurable via $SERVICES; add tests for Firehose+S3&lt;/li&gt;
&lt;li&gt;v0.3.8: Fix Elasticsearch via local bind and proxy; refactoring; improve error logging&lt;/li&gt;
&lt;li&gt;v0.3.5: Fix lambda handler name; fix host name for S3 API; install web libs on pip install&lt;/li&gt;
&lt;li&gt;v0.3.4: Fix file permissions in build; fix and add UI to Docker image; add stub of ES API&lt;/li&gt;
&lt;li&gt;v0.3.3: Add version tags to Docker images&lt;/li&gt;
&lt;li&gt;v0.3.2: Add support for Redshift API; code refactoring&lt;/li&gt;
&lt;li&gt;v0.3.1: Add Dockerfile and push image to Docker Hub&lt;/li&gt;
&lt;li&gt;v0.3.0: Add simple integration for JUnit; improve process signal handling&lt;/li&gt;
&lt;li&gt;v0.2.11: Refactored the AWS assume role function&lt;/li&gt;
&lt;li&gt;v0.2.10: Added AWS assume role functionality.&lt;/li&gt;
&lt;li&gt;v0.2.9: Kinesis error response formatting&lt;/li&gt;
&lt;li&gt;v0.2.7: Throw Kinesis errors randomly&lt;/li&gt;
&lt;li&gt;v0.2.6: Decouple SNS/SQS: intercept SNS calls and forward to subscribed SQS queues&lt;/li&gt;
&lt;li&gt;v0.2.5: Return error response from Kinesis if flag is set&lt;/li&gt;
&lt;li&gt;v0.2.4: Allow Lambdas to use &lt;strong&gt;file&lt;/strong&gt; (import from file instead of exec'ing)&lt;/li&gt;
&lt;li&gt;v0.2.3: Improve Kinesis/KCL auto-checkpointing (leases in DDB)&lt;/li&gt;
&lt;li&gt;v0.2.0: Speed up installation time by lazy loading libraries&lt;/li&gt;
&lt;li&gt;v0.1.19: Pass shard_id in records sent from KCL process&lt;/li&gt;
&lt;li&gt;v0.1.16: Minor restructuring and refactoring (create separate kinesis_util.py)&lt;/li&gt;
&lt;li&gt;v0.1.14: Fix AWS tokens when creating Elasticsearch client&lt;/li&gt;
&lt;li&gt;v0.1.11: Add startup/initialization notification for KCL process&lt;/li&gt;
&lt;li&gt;v0.1.10: Bump version of amazon_kclpy to 1.4.1&lt;/li&gt;
&lt;li&gt;v0.1.9: Add initial support for SQS/SNS&lt;/li&gt;
&lt;li&gt;v0.1.8: Fix installation of JARs in amazon_kclpy if localstack is installed transitively&lt;/li&gt;
&lt;li&gt;v0.1.7: Bump version of amazon_kclpy to 1.4.0&lt;/li&gt;
&lt;li&gt;v0.1.6: Add travis-ci and coveralls configuration&lt;/li&gt;
&lt;li&gt;v0.1.5: Refactor Elasticsearch utils; fix bug in method to delete all ES indexes&lt;/li&gt;
&lt;li&gt;v0.1.4: Enhance logging; extend java KCL credentials provider (support STS assumed roles)&lt;/li&gt;
&lt;li&gt;v0.1.2: Add configurable KCL log output&lt;/li&gt;
&lt;li&gt;v0.1.0: Initial release&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We welcome feedback, bug reports, and pull requests!&lt;/p&gt;
&lt;p&gt;For pull requests, please stick to the following guidelines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add tests for any new features and bug fixes. Ideally, each PR should increase the test coverage.&lt;/li&gt;
&lt;li&gt;Follow the existing code style (e.g., indents). A PEP8 code linting target is included in the Makefile.&lt;/li&gt;
&lt;li&gt;Put a reasonable amount of comments into the code.&lt;/li&gt;
&lt;li&gt;Separate unrelated changes into multiple pull requests.&lt;/li&gt;
&lt;li&gt;1 commit per PR: Please squash/rebase multiple commits into one single commit (to keep the history clean).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please note that by contributing any code or documentation to this repository (by
raising pull requests, or otherwise) you explicitly agree to
the &lt;a href="doc/contributor_license_agreement"&gt;&lt;strong&gt;Contributor License Agreement&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;This project exists thanks to all the people who contribute.
&lt;a href="https://github.com/localstack/localstack/graphs/contributors"&gt;&lt;img src="https://camo.githubusercontent.com/70c617534022062f30a0679c0c2422a23ec605ee/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f636f6e7472696275746f72732e7376673f77696474683d383930" data-canonical-src="https://opencollective.com/localstack/contributors.svg?width=890" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-backers" class="anchor" aria-hidden="true" href="#backers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Backers&lt;/h2&gt;
&lt;p&gt;Thank you to all our backers! &lt;g-emoji class="g-emoji" alias="pray" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64f.png"&gt;🙏&lt;/g-emoji&gt; [&lt;a href="https://opencollective.com/localstack#backer" rel="nofollow"&gt;Become a backer&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://opencollective.com/localstack#backers" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/baded82797613a9dfee852dc9b83e810dbb99e07/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f6261636b6572732e7376673f77696474683d383930" data-canonical-src="https://opencollective.com/localstack/backers.svg?width=890" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sponsors" class="anchor" aria-hidden="true" href="#sponsors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sponsors&lt;/h2&gt;
&lt;p&gt;Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [&lt;a href="https://opencollective.com/localstack#sponsor" rel="nofollow"&gt;Become a sponsor&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://opencollective.com/localstack/sponsor/0/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5447958470aa26c2d09b668bf4b8e3946f94908f/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f302f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/0/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/1/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0fb41b881ff9a6f2545bf210a1b5eeefe1c2f9e0/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f312f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/1/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/2/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/105ab894cafcccc622161e8a0f070bb2e1edc38f/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f322f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/2/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/3/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/12dcc5d9654145e5609701ac986ced8a34155fd9/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f332f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/3/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/4/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/99c081e4fa906ad24a9ba4a430ce1b8f6b47f9b2/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f342f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/4/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/5/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/024967ca75aecc89725c4be25d8951eb637cc30b/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f352f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/5/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/6/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2b266eeed387c32eb4e2c42dd5e5d005967c4024/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f362f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/6/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/7/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c677209769bb3f28660b6ebd0c87b2e2be8d6103/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f372f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/7/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/8/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/902262e1aeccb957ef75bcbf00de78bd0ba60fb6/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f382f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/8/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/9/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/574b420b1fe419c38dd36d0c0a2a2d78031a3af5/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f392f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/9/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-stargazers-over-time" class="anchor" aria-hidden="true" href="#stargazers-over-time"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stargazers over time&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://starchart.cc/localstack/localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9d274070627cd092149bd6ed66d1e8401cd966fa/68747470733a2f2f7374617263686172742e63632f6c6f63616c737461636b2f6c6f63616c737461636b2e737667" alt="Stargazers over time" data-canonical-src="https://starchart.cc/localstack/localstack.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright (c) 2017-2019 LocalStack maintainers and contributors.&lt;/p&gt;
&lt;p&gt;Copyright (c) 2016 Atlassian and others.&lt;/p&gt;
&lt;p&gt;This version of LocalStack is released under the Apache License, Version 2.0 (see LICENSE.txt).
By downloading and using this software you agree to the
&lt;a href="doc/end_user_license_agreement"&gt;End-User License Agreement (EULA)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We build on a number of third-party software tools, including the following:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Third-Party software&lt;/th&gt;
&lt;th&gt;License&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Python/pip modules:&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;airspeed&lt;/td&gt;
&lt;td&gt;BSD License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;amazon_kclpy&lt;/td&gt;
&lt;td&gt;Amazon Software License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;boto3&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;coverage&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;docopt&lt;/td&gt;
&lt;td&gt;MIT License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;elasticsearch&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;flask&lt;/td&gt;
&lt;td&gt;BSD License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;flask_swagger&lt;/td&gt;
&lt;td&gt;MIT License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;jsonpath-rw&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;moto&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nose&lt;/td&gt;
&lt;td&gt;GNU LGPL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pep8&lt;/td&gt;
&lt;td&gt;Expat license&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;requests&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;subprocess32&lt;/td&gt;
&lt;td&gt;PSF License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Node.js/npm modules:&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kinesalite&lt;/td&gt;
&lt;td&gt;MIT License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Other tools:&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Elasticsearch&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>localstack</author><guid isPermaLink="false">https://github.com/localstack/localstack</guid><pubDate>Fri, 29 Nov 2019 00:11:00 GMT</pubDate></item><item><title>scikit-learn/scikit-learn #12 in Python, This month</title><link>https://github.com/scikit-learn/scikit-learn</link><description>&lt;p&gt;&lt;i&gt;scikit-learn: machine learning in Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img alt="Azure" src="https://camo.githubusercontent.com/bfe67a3604768c16e941f3331709bf55507a4b57/68747470733a2f2f6465762e617a7572652e636f6d2f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f5f617069732f6275696c642f7374617475732f7363696b69742d6c6561726e2e7363696b69742d6c6561726e3f6272616e63684e616d653d6d6173746572" data-canonical-src="https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://travis-ci.org/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="Travis" src="https://camo.githubusercontent.com/590475799489c962f111c9fc5c1432ecbc577578/68747470733a2f2f6170692e7472617669732d63692e6f72672f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://api.travis-ci.org/scikit-learn/scikit-learn.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/github/scikit-learn/scikit-learn?branch=master" rel="nofollow"&gt;&lt;img alt="Codecov" src="https://camo.githubusercontent.com/58a0b06906ca5d106ec090fe8a1ac85a092b81c2/68747470733a2f2f636f6465636f762e696f2f6769746875622f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" data-canonical-src="https://codecov.io/github/scikit-learn/scikit-learn/badge.svg?branch=master&amp;amp;service=github" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="CircleCI" src="https://camo.githubusercontent.com/d2913194913f85128f908483a265e64dcd6d31e4/68747470733a2f2f636972636c6563692e636f6d2f67682f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f747265652f6d61737465722e7376673f7374796c653d736869656c6426636972636c652d746f6b656e3d3a636972636c652d746f6b656e" data-canonical-src="https://circleci.com/gh/scikit-learn/scikit-learn/tree/master.svg?style=shield&amp;amp;circle-token=:circle-token" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://img.shields.io/pypi/pyversions/scikit-learn.svg" rel="nofollow"&gt;&lt;img alt="PythonVersion" src="https://camo.githubusercontent.com/45416807fdec5b0d83acca16b2b9f08fe7d32bf1/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7363696b69742d6c6561726e2e737667" data-canonical-src="https://img.shields.io/pypi/pyversions/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://badge.fury.io/py/scikit-learn" rel="nofollow"&gt;&lt;img alt="PyPi" src="https://camo.githubusercontent.com/9f0ed32d05350afa18a801573e4da7f4a240e181/68747470733a2f2f62616467652e667572792e696f2f70792f7363696b69742d6c6561726e2e737667" data-canonical-src="https://badge.fury.io/py/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="DOI" src="https://camo.githubusercontent.com/73c63e44b8bee62df142664048c58f83ec8ad95c/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f32313336392f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e737667" data-canonical-src="https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-scikit-learn"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-scikit-learn" class="anchor" aria-hidden="true" href="#scikit-learn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;scikit-learn&lt;/h2&gt;
&lt;p&gt;scikit-learn is a Python module for machine learning built on top of
SciPy and is distributed under the 3-Clause BSD license.&lt;/p&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the &lt;a href="http://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;It is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-installation"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;a name="user-content-dependencies"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h4&gt;
&lt;p&gt;scikit-learn requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python (&amp;gt;= 3.5)&lt;/li&gt;
&lt;li&gt;NumPy (&amp;gt;= 1.11.0)&lt;/li&gt;
&lt;li&gt;SciPy (&amp;gt;= 0.17.0)&lt;/li&gt;
&lt;li&gt;joblib (&amp;gt;= 0.11)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.&lt;/strong&gt;
scikit-learn 0.21 and later require Python 3.5 or newer.&lt;/p&gt;
&lt;p&gt;Scikit-learn plotting capabilities (i.e., functions start with &lt;code&gt;plot_&lt;/code&gt;
and classes end with "Display") require Matplotlib (&amp;gt;= 1.5.1). For running the
examples Matplotlib &amp;gt;= 1.5.1 is required. A few examples require
scikit-image &amp;gt;= 0.12.3, a few examples require pandas &amp;gt;= 0.18.0.&lt;/p&gt;
&lt;a name="user-content-user-installation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-user-installation" class="anchor" aria-hidden="true" href="#user-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;User installation&lt;/h4&gt;
&lt;p&gt;If you already have a working installation of numpy and scipy,
the easiest way to install scikit-learn is using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;pip install -U scikit-learn
&lt;/pre&gt;
&lt;p&gt;or &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;conda install scikit-learn
&lt;/pre&gt;
&lt;p&gt;The documentation includes more detailed &lt;a href="http://scikit-learn.org/stable/install.html" rel="nofollow"&gt;installation instructions&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-changelog"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h3&gt;
&lt;p&gt;See the &lt;a href="http://scikit-learn.org/dev/whats_new.html" rel="nofollow"&gt;changelog&lt;/a&gt;
for a history of notable changes to scikit-learn.&lt;/p&gt;
&lt;a name="user-content-development"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h3&gt;
&lt;p&gt;We welcome new contributors of all experience levels. The scikit-learn
community goals are to be helpful, welcoming, and effective. The
&lt;a href="http://scikit-learn.org/stable/developers/index.html" rel="nofollow"&gt;Development Guide&lt;/a&gt;
has detailed information about contributing code, documentation, tests, and
more. We've included some basic information in this README.&lt;/p&gt;
&lt;a name="user-content-important-links"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-important-links" class="anchor" aria-hidden="true" href="#important-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Important links&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Official source code repo: &lt;a href="https://github.com/scikit-learn/scikit-learn"&gt;https://github.com/scikit-learn/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Download releases: &lt;a href="https://pypi.org/project/scikit-learn/" rel="nofollow"&gt;https://pypi.org/project/scikit-learn/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Issue tracker: &lt;a href="https://github.com/scikit-learn/scikit-learn/issues"&gt;https://github.com/scikit-learn/scikit-learn/issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-source-code"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-source-code" class="anchor" aria-hidden="true" href="#source-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source code&lt;/h4&gt;
&lt;p&gt;You can check the latest sources with the command:&lt;/p&gt;
&lt;pre&gt;git clone https://github.com/scikit-learn/scikit-learn.git
&lt;/pre&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h4&gt;
&lt;p&gt;To learn more about making a contribution to scikit-learn, please see our
&lt;a href="https://scikit-learn.org/dev/developers/contributing.html" rel="nofollow"&gt;Contributing guide&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-testing"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h4&gt;
&lt;p&gt;After installation, you can launch the test suite from outside the
source directory (you will need to have &lt;code&gt;pytest&lt;/code&gt; &amp;gt;= 3.3.0 installed):&lt;/p&gt;
&lt;pre&gt;pytest sklearn
&lt;/pre&gt;
&lt;p&gt;See the web page &lt;a href="http://scikit-learn.org/dev/developers/advanced_installation.html#testing" rel="nofollow"&gt;http://scikit-learn.org/dev/developers/advanced_installation.html#testing&lt;/a&gt;
for more information.&lt;/p&gt;
&lt;blockquote&gt;
Random number generation can be controlled during testing by setting
the &lt;code&gt;SKLEARN_SEED&lt;/code&gt; environment variable.&lt;/blockquote&gt;
&lt;a name="user-content-submitting-a-pull-request"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-submitting-a-pull-request" class="anchor" aria-hidden="true" href="#submitting-a-pull-request"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Submitting a Pull Request&lt;/h4&gt;
&lt;p&gt;Before opening a Pull Request, have a look at the
full Contributing page to make sure your code complies
with our guidelines: &lt;a href="http://scikit-learn.org/stable/developers/index.html" rel="nofollow"&gt;http://scikit-learn.org/stable/developers/index.html&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-project-history"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-project-history" class="anchor" aria-hidden="true" href="#project-history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Project History&lt;/h3&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the &lt;a href="http://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;The project is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: scikit-learn was previously referred to as scikits.learn.&lt;/p&gt;
&lt;a name="user-content-help-and-support"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-help-and-support" class="anchor" aria-hidden="true" href="#help-and-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Help and Support&lt;/h3&gt;
&lt;a name="user-content-documentation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;HTML documentation (stable release): &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HTML documentation (development version): &lt;a href="http://scikit-learn.org/dev/" rel="nofollow"&gt;http://scikit-learn.org/dev/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FAQ: &lt;a href="http://scikit-learn.org/stable/faq.html" rel="nofollow"&gt;http://scikit-learn.org/stable/faq.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-communication"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-communication" class="anchor" aria-hidden="true" href="#communication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Communication&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mailing list: &lt;a href="https://mail.python.org/mailman/listinfo/scikit-learn" rel="nofollow"&gt;https://mail.python.org/mailman/listinfo/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IRC channel: &lt;code&gt;#scikit-learn&lt;/code&gt; at &lt;code&gt;webchat.freenode.net&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Stack Overflow: &lt;a href="https://stackoverflow.com/questions/tagged/scikit-learn" rel="nofollow"&gt;https://stackoverflow.com/questions/tagged/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Website: &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-citation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h4&gt;
&lt;p&gt;If you use scikit-learn in a scientific publication, we would appreciate citations: &lt;a href="http://scikit-learn.org/stable/about.html#citing-scikit-learn" rel="nofollow"&gt;http://scikit-learn.org/stable/about.html#citing-scikit-learn&lt;/a&gt;&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>scikit-learn</author><guid isPermaLink="false">https://github.com/scikit-learn/scikit-learn</guid><pubDate>Fri, 29 Nov 2019 00:12:00 GMT</pubDate></item><item><title>huggingface/transformers #13 in Python, This month</title><link>https://github.com/huggingface/transformers</link><description>&lt;p&gt;&lt;i&gt;🤗 Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;br&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png"&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png" width="400" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;br&gt;
&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;&lt;p align="center"&gt;
    &lt;a href="https://circleci.com/gh/huggingface/transformers" rel="nofollow"&gt;
        &lt;img alt="Build" src="https://camo.githubusercontent.com/045b8639882280ff5cd38c403499977386c25134/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f6275696c642f6769746875622f68756767696e67666163652f7472616e73666f726d6572732f6d6173746572" data-canonical-src="https://img.shields.io/circleci/build/github/huggingface/transformers/master" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://github.com/huggingface/transformers/blob/master/LICENSE"&gt;
        &lt;img alt="GitHub" src="https://camo.githubusercontent.com/440e73b137335cc0088bb06e6c90cc7b503b14a2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f68756767696e67666163652f7472616e73666f726d6572732e7376673f636f6c6f723d626c7565" data-canonical-src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://huggingface.co/transformers/index.html" rel="nofollow"&gt;
        &lt;img alt="Documentation" src="https://camo.githubusercontent.com/b104c21f478c4d4a37f63292ab2898047f19ee24/68747470733a2f2f696d672e736869656c64732e696f2f776562736974652f687474702f68756767696e67666163652e636f2f7472616e73666f726d6572732f696e6465782e68746d6c2e7376673f646f776e5f636f6c6f723d72656426646f776e5f6d6573736167653d6f66666c696e652675705f6d6573736167653d6f6e6c696e65" data-canonical-src="https://img.shields.io/website/http/huggingface.co/transformers/index.html.svg?down_color=red&amp;amp;down_message=offline&amp;amp;up_message=online" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://github.com/huggingface/transformers/releases"&gt;
        &lt;img alt="GitHub release" src="https://camo.githubusercontent.com/8409fd8716dd1a11afa7ab38e1218b34918164eb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f68756767696e67666163652f7472616e73666f726d6572732e737667" data-canonical-src="https://img.shields.io/github/release/huggingface/transformers.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
&lt;/p&gt;
&lt;h3 align="center"&gt;&lt;a id="user-content-state-of-the-art-natural-language-processing-for-tensorflow-20-and-pytorch" class="anchor" aria-hidden="true" href="#state-of-the-art-natural-language-processing-for-tensorflow-20-and-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;p&gt;State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch
&lt;/p&gt;&lt;/h3&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers (formerly known as &lt;code&gt;pytorch-transformers&lt;/code&gt; and &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt;) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;As easy to use as pytorch-transformers&lt;/li&gt;
&lt;li&gt;As powerful and concise as Keras&lt;/li&gt;
&lt;li&gt;High performance on NLU and NLG tasks&lt;/li&gt;
&lt;li&gt;Low barrier to entry for educators and practitioners&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;State-of-the-art NLP for everyone&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep learning researchers&lt;/li&gt;
&lt;li&gt;Hands-on practitioners&lt;/li&gt;
&lt;li&gt;AI/ML/NLP teachers and educators&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lower compute costs, smaller carbon footprint&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Researchers can share trained models instead of always retraining&lt;/li&gt;
&lt;li&gt;Practitioners can reduce compute time and production costs&lt;/li&gt;
&lt;li&gt;10 architectures with over 30 pretrained models, some in more than 100 languages&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Choose the right framework for every part of a model's lifetime&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train state-of-the-art models in 3 lines of code&lt;/li&gt;
&lt;li&gt;Deep interoperability between TensorFlow 2.0 and PyTorch models&lt;/li&gt;
&lt;li&gt;Move a single model between TF2.0/PyTorch frameworks at will&lt;/li&gt;
&lt;li&gt;Seamlessly pick the right framework for training, evaluation, production&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Section&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;How to install the package&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#model-architectures"&gt;Model architectures&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Architectures (with pretrained weights)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#online-demo"&gt;Online demo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Experimenting with this repo’s text generation capabilities&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#quick-tour"&gt;Quick tour: Usage&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tokenizers &amp;amp; models usage: Bert and GPT-2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Quick-tour-TF-20-training-and-PyTorch-interoperability"&gt;Quick tour: TF 2.0 and PyTorch &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Train a TF 2.0 model in 10 lines of code, load it in PyTorch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#quick-tour-of-the-fine-tuningusage-scripts"&gt;Quick tour: Fine-tuning/usage scripts&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Using provided scripts: GLUE, SQuAD and Text generation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Migrating-from-pytorch-transformers-to-transformers"&gt;Migrating from pytorch-transformers to transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Migrating your code from pytorch-transformers to transformers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Migrating-from-pytorch-pretrained-bert-to-transformers"&gt;Migrating from pytorch-pretrained-bert to pytorch-transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Migrating your code from pytorch-pretrained-bert to transformers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[Documentation]&lt;a href="https://huggingface.co/transformers/v2.2.0" rel="nofollow"&gt;(v2.2.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v2.1.1" rel="nofollow"&gt;(v2.1.1)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v2.0.0" rel="nofollow"&gt;(v2.0.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.2.0" rel="nofollow"&gt;(v1.2.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.1.0" rel="nofollow"&gt;(v1.1.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.0.0" rel="nofollow"&gt;(v1.0.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers" rel="nofollow"&gt;(master)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Full API documentation and more&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;This repo is tested on Python 2.7 and 3.5+ (examples are tested only on python 3.5+), PyTorch 1.0.0+ and TensorFlow 2.0.0-rc1&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-with-pip" class="anchor" aria-hidden="true" href="#with-pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;With pip&lt;/h3&gt;
&lt;p&gt;First you need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to &lt;a href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available" rel="nofollow"&gt;TensorFlow installation page&lt;/a&gt; and/or &lt;a href="https://pytorch.org/get-started/locally/#start-locally" rel="nofollow"&gt;PyTorch installation page&lt;/a&gt; regarding the specific install command for your platform.&lt;/p&gt;
&lt;p&gt;When TensorFlow 2.0 and/or PyTorch has been installed, &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers can be installed using pip as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install transformers&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-from-source" class="anchor" aria-hidden="true" href="#from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;From source&lt;/h3&gt;
&lt;p&gt;Here also, you first need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to &lt;a href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available" rel="nofollow"&gt;TensorFlow installation page&lt;/a&gt; and/or &lt;a href="https://pytorch.org/get-started/locally/#start-locally" rel="nofollow"&gt;PyTorch installation page&lt;/a&gt; regarding the specific install command for your platform.&lt;/p&gt;
&lt;p&gt;When TensorFlow 2.0 and/or PyTorch has been installed, you can install from source by cloning the repository and running:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install [--editable] &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-run-the-examples" class="anchor" aria-hidden="true" href="#run-the-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run the examples&lt;/h3&gt;
&lt;p&gt;Examples are included in the repository but are not shipped with the library.
Therefore, in order to run the latest versions of the examples you also need to install from source. To do so, create a new virtual environment and follow these steps:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/huggingface/transformers
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; transformers
pip install [--editable] &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h3&gt;
&lt;p&gt;A series of tests are included for the library and the example scripts. Library tests can be found in the &lt;a href="https://github.com/huggingface/transformers/tree/master/transformers/tests"&gt;tests folder&lt;/a&gt; and examples tests in the &lt;a href="https://github.com/huggingface/transformers/tree/master/examples"&gt;examples folder&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These tests can be run using &lt;code&gt;pytest&lt;/code&gt; (install pytest if needed with &lt;code&gt;pip install pytest&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Depending on which framework is installed (TensorFlow 2.0 and/or PyTorch), the irrelevant tests will be skipped. Ensure that both frameworks are installed if you want to execute all tests.&lt;/p&gt;
&lt;p&gt;You can run the tests from the root of the cloned repository with the commands:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m pytest -sv ./transformers/tests/
python -m pytest -sv ./examples/&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-do-you-want-to-run-a-transformer-model-on-a-mobile-device" class="anchor" aria-hidden="true" href="#do-you-want-to-run-a-transformer-model-on-a-mobile-device"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Do you want to run a Transformer model on a mobile device?&lt;/h3&gt;
&lt;p&gt;You should check out our &lt;a href="https://github.com/huggingface/swift-coreml-transformers"&gt;&lt;code&gt;swift-coreml-transformers&lt;/code&gt;&lt;/a&gt; repo.&lt;/p&gt;
&lt;p&gt;It contains a set of tools to convert PyTorch or TensorFlow 2.0 trained Transformer models (currently contains &lt;code&gt;GPT-2&lt;/code&gt;, &lt;code&gt;DistilGPT-2&lt;/code&gt;, &lt;code&gt;BERT&lt;/code&gt;, and &lt;code&gt;DistilBERT&lt;/code&gt;) to CoreML models that run on iOS devices.&lt;/p&gt;
&lt;p&gt;At some point in the future, you'll be able to seamlessly move from pre-training or fine-tuning models to productizing them in CoreML, or prototype a model or an app in CoreML then research its hyperparameters or architecture from TensorFlow 2.0 and/or PyTorch. Super exciting!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-architectures" class="anchor" aria-hidden="true" href="#model-architectures"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model architectures&lt;/h2&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers currently provides 10 NLU/NLG architectures:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/google-research/bert"&gt;BERT&lt;/a&gt;&lt;/strong&gt; (from Google) released with the paper &lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt; by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/openai/finetune-transformer-lm"&gt;GPT&lt;/a&gt;&lt;/strong&gt; (from OpenAI) released with the paper &lt;a href="https://blog.openai.com/language-unsupervised/" rel="nofollow"&gt;Improving Language Understanding by Generative Pre-Training&lt;/a&gt; by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;GPT-2&lt;/a&gt;&lt;/strong&gt; (from OpenAI) released with the paper &lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt; by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/kimiyoung/transformer-xl"&gt;Transformer-XL&lt;/a&gt;&lt;/strong&gt; (from Google/CMU) released with the paper &lt;a href="https://arxiv.org/abs/1901.02860" rel="nofollow"&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/a&gt; by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/zihangdai/xlnet/"&gt;XLNet&lt;/a&gt;&lt;/strong&gt; (from Google/CMU) released with the paper &lt;a href="https://arxiv.org/abs/1906.08237" rel="nofollow"&gt;​XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/a&gt; by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/facebookresearch/XLM/"&gt;XLM&lt;/a&gt;&lt;/strong&gt; (from Facebook) released together with the paper &lt;a href="https://arxiv.org/abs/1901.07291" rel="nofollow"&gt;Cross-lingual Language Model Pretraining&lt;/a&gt; by Guillaume Lample and Alexis Conneau.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta"&gt;RoBERTa&lt;/a&gt;&lt;/strong&gt; (from Facebook), released together with the paper a &lt;a href="https://arxiv.org/abs/1907.11692" rel="nofollow"&gt;Robustly Optimized BERT Pretraining Approach&lt;/a&gt; by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/huggingface/transformers/tree/master/examples/distillation"&gt;DistilBERT&lt;/a&gt;&lt;/strong&gt; (from HuggingFace), released together with the paper &lt;a href="https://arxiv.org/abs/1910.01108" rel="nofollow"&gt;DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter&lt;/a&gt; by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into &lt;a href="https://github.com/huggingface/transformers/tree/master/examples/distillation"&gt;DistilGPT2&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/salesforce/ctrl/"&gt;CTRL&lt;/a&gt;&lt;/strong&gt; (from Salesforce) released with the paper &lt;a href="https://arxiv.org/abs/1909.05858" rel="nofollow"&gt;CTRL: A Conditional Transformer Language Model for Controllable Generation&lt;/a&gt; by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://camembert-model.fr" rel="nofollow"&gt;CamemBERT&lt;/a&gt;&lt;/strong&gt; (from Inria/Facebook/Sorbonne) released with the paper &lt;a href="https://arxiv.org/abs/1911.03894" rel="nofollow"&gt;CamemBERT: a Tasty French Language Model&lt;/a&gt; by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/google-research/google-research/tree/master/albert"&gt;ALBERT&lt;/a&gt;&lt;/strong&gt; (from Google Research and the Toyota Technological Institute at Chicago) released with the paper &lt;a href="https://arxiv.org/abs/1909.11942" rel="nofollow"&gt;ALBERT: A Lite BERT for Self-supervised Learning of Language Representations&lt;/a&gt;, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.&lt;/li&gt;
&lt;li&gt;Want to contribute a new model? We have added a &lt;strong&gt;detailed guide and templates&lt;/strong&gt; to guide you in the process of adding a new model. You can find them in the &lt;a href="./templates"&gt;&lt;code&gt;templates&lt;/code&gt;&lt;/a&gt; folder of the repository. Be sure to check the &lt;a href="./CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; and contact the maintainers or open an issue to collect feedbacks before starting your PR.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the &lt;a href="https://huggingface.co/transformers/examples.html" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-online-demo" class="anchor" aria-hidden="true" href="#online-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online demo&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://transformer.huggingface.co" rel="nofollow"&gt;Write With Transformer&lt;/a&gt;&lt;/strong&gt;, built by the Hugging Face team at transformer.huggingface.co, is the official demo of this repo’s text generation capabilities.
You can use it to experiment with completions generated by &lt;code&gt;GPT2Model&lt;/code&gt;, &lt;code&gt;TransfoXLModel&lt;/code&gt;, and &lt;code&gt;XLNetModel&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“&lt;g-emoji class="g-emoji" alias="unicorn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f984.png"&gt;🦄&lt;/g-emoji&gt; Write with transformer is to writing what calculators are to calculus.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ba91bf4a35939363eca4ca83f3ad3f83248bbc60/68747470733a2f2f7472616e73666f726d65722e68756767696e67666163652e636f2f66726f6e742f6173736574732f7468756d626e61696c2d6c617267652e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/ba91bf4a35939363eca4ca83f3ad3f83248bbc60/68747470733a2f2f7472616e73666f726d65722e68756767696e67666163652e636f2f66726f6e742f6173736574732f7468756d626e61696c2d6c617267652e706e67" alt="write_with_transformer" data-canonical-src="https://transformer.huggingface.co/front/assets/thumbnail-large.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour" class="anchor" aria-hidden="true" href="#quick-tour"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour&lt;/h2&gt;
&lt;p&gt;Let's do a very quick overview of the model architectures in &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers. Detailed examples for each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the &lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;full documentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; torch
&lt;span class="pl-k"&gt;from&lt;/span&gt; transformers &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Transformers has a unified API&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; for 8 transformer architectures and 30 pretrained weights.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;          Model          | Tokenizer          | Pretrained weights shortcut&lt;/span&gt;
&lt;span class="pl-c1"&gt;MODELS&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; [(BertModel,       BertTokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (OpenAIGPTModel,  OpenAIGPTTokenizer,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;openai-gpt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (GPT2Model,       GPT2Tokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;gpt2&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (CTRLModel,       CTRLTokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ctrl&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (TransfoXLModel,  TransfoXLTokenizer,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;transfo-xl-wt103&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (XLNetModel,      XLNetTokenizer,      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xlnet-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (XLMModel,        XLMTokenizer,        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xlm-mlm-enfr-1024&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (DistilBertModel, DistilBertTokenizer, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;distilbert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (RobertaModel,    RobertaTokenizer,    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;roberta-base&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Let's encode some text in a sequence of hidden-states using each model:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; model_class, tokenizer_class, pretrained_weights &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;MODELS&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load pretrained model/tokenizer&lt;/span&gt;
    tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer_class.from_pretrained(pretrained_weights)
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Encode text&lt;/span&gt;
    input_ids &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.tensor([tokenizer.encode(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Here is some text to encode&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)])  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Add special tokens takes care of adding [CLS], [SEP], &amp;lt;s&amp;gt;... tokens in the right way for each model.&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; torch.no_grad():
        last_hidden_states &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids)[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models outputs are now tuples&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.&lt;/span&gt;
&lt;span class="pl-c1"&gt;BERT_MODEL_CLASSES&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,
                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; All the classes for an architecture can be initiated from pretrained weights for this architecture&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Note that additional weights added for fine-tuning are only initialized&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; and need to be trained on the down-stream task&lt;/span&gt;
pretrained_weights &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(pretrained_weights)
&lt;span class="pl-k"&gt;for&lt;/span&gt; model_class &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;BERT_MODEL_CLASSES&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load pretrained model/tokenizer&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models can return full list of hidden-states &amp;amp; attentions weights at each layer&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights,
                                        &lt;span class="pl-v"&gt;output_hidden_states&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;,
                                        &lt;span class="pl-v"&gt;output_attentions&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
    input_ids &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.tensor([tokenizer.encode(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Let's see all hidden-states and attentions on this text&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)])
    all_hidden_states, all_attentions &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids)[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;:]

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models are compatible with Torchscript&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights, &lt;span class="pl-v"&gt;torchscript&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
    traced_model &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.jit.trace(model, (input_ids,))

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Simple serialization for models and tokenizers&lt;/span&gt;
    model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; save&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; re-load&lt;/span&gt;
    tokenizer.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; save&lt;/span&gt;
    tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; re-load&lt;/span&gt;

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; SOTA examples for GLUE, SQUAD, text generation...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour-tf-20-training-and-pytorch-interoperability" class="anchor" aria-hidden="true" href="#quick-tour-tf-20-training-and-pytorch-interoperability"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour TF 2.0 training and PyTorch interoperability&lt;/h2&gt;
&lt;p&gt;Let's do a quick example of how a TensorFlow 2.0 model can be trained in 12 lines of code with &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers and then loaded in PyTorch for fast inspection/tests.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow &lt;span class="pl-k"&gt;as&lt;/span&gt; tf
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow_datasets
&lt;span class="pl-k"&gt;from&lt;/span&gt; transformers &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load dataset, tokenizer, model from pretrained model/vocabulary&lt;/span&gt;
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
model &lt;span class="pl-k"&gt;=&lt;/span&gt; TFBertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
data &lt;span class="pl-k"&gt;=&lt;/span&gt; tensorflow_datasets.load(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;glue/mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Prepare dataset for GLUE as a tf.data.Dataset instance&lt;/span&gt;
train_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; glue_convert_examples_to_features(data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], tokenizer, &lt;span class="pl-v"&gt;max_length&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-v"&gt;task&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
valid_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; glue_convert_examples_to_features(data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;validation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], tokenizer, &lt;span class="pl-v"&gt;max_length&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-v"&gt;task&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
train_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; train_dataset.shuffle(&lt;span class="pl-c1"&gt;100&lt;/span&gt;).batch(&lt;span class="pl-c1"&gt;32&lt;/span&gt;).repeat(&lt;span class="pl-c1"&gt;2&lt;/span&gt;)
valid_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; valid_dataset.batch(&lt;span class="pl-c1"&gt;64&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule &lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.optimizers.Adam(&lt;span class="pl-v"&gt;learning_rate&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3e-5&lt;/span&gt;, &lt;span class="pl-v"&gt;epsilon&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1e-08&lt;/span&gt;, &lt;span class="pl-v"&gt;clipnorm&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1.0&lt;/span&gt;)
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.losses.SparseCategoricalCrossentropy(&lt;span class="pl-v"&gt;from_logits&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
metric &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.metrics.SparseCategoricalAccuracy(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;accuracy&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
model.compile(&lt;span class="pl-v"&gt;optimizer&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;optimizer, &lt;span class="pl-v"&gt;loss&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;loss, &lt;span class="pl-v"&gt;metrics&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[metric])

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train and evaluate using tf.keras.Model.fit()&lt;/span&gt;
history &lt;span class="pl-k"&gt;=&lt;/span&gt; model.fit(train_dataset, &lt;span class="pl-v"&gt;epochs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-v"&gt;steps_per_epoch&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;115&lt;/span&gt;,
                    &lt;span class="pl-v"&gt;validation_data&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;valid_dataset, &lt;span class="pl-v"&gt;validation_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;7&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load the TensorFlow model in PyTorch for inspection&lt;/span&gt;
model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
pytorch_model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;from_tf&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task&lt;/span&gt;
sentence_0 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;This research was consistent with his findings.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
sentence_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;His findings were compatible with this research.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
sentence_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;His findings were not compatible with this research.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
inputs_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer.encode_plus(sentence_0, sentence_1, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;return_tensors&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
inputs_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer.encode_plus(sentence_0, sentence_2, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;return_tensors&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

pred_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; pytorch_model(inputs_1[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;token_type_ids&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;inputs_1[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;token_type_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].argmax().item()
pred_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; pytorch_model(inputs_2[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;token_type_ids&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;inputs_2[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;token_type_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].argmax().item()

&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sentence_1 is&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; pred_1 &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;not a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;of sentence_0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sentence_2 is&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; pred_2 &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;not a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;of sentence_0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour-of-the-fine-tuningusage-scripts" class="anchor" aria-hidden="true" href="#quick-tour-of-the-fine-tuningusage-scripts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour of the fine-tuning/usage scripts&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;&lt;br&gt;
Before running the fine-tuning scripts, please read the
&lt;a href="#run-the-examples"&gt;instructions&lt;/a&gt; on how to
setup your environment to run the examples.&lt;/p&gt;
&lt;p&gt;The library comprises several example scripts with SOTA performances for NLU and NLG tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;run_glue.py&lt;/code&gt;: an example fine-tuning Bert, XLNet and XLM on nine different GLUE tasks (&lt;em&gt;sequence-level classification&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run_squad.py&lt;/code&gt;: an example fine-tuning Bert, XLNet and XLM on the question answering dataset SQuAD 2.0 (&lt;em&gt;token-level classification&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run_generation.py&lt;/code&gt;: an example using GPT, GPT-2, CTRL, Transformer-XL and XLNet for conditional language generation&lt;/li&gt;
&lt;li&gt;other model-specific examples (see the documentation).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are three quick usage examples for these scripts:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run_gluepy-fine-tuning-on-glue-tasks-for-sequence-classification" class="anchor" aria-hidden="true" href="#run_gluepy-fine-tuning-on-glue-tasks-for-sequence-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_glue.py&lt;/code&gt;: Fine-tuning on GLUE tasks for sequence classification&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://gluebenchmark.com/" rel="nofollow"&gt;General Language Understanding Evaluation (GLUE) benchmark&lt;/a&gt; is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.&lt;/p&gt;
&lt;p&gt;Before running anyone of these GLUE tasks you should download the
&lt;a href="https://gluebenchmark.com/tasks" rel="nofollow"&gt;GLUE data&lt;/a&gt; by running
&lt;a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"&gt;this script&lt;/a&gt;
and unpack it to some directory &lt;code&gt;$GLUE_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You should also install the additional packages required by the examples:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -r ./examples/requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue
&lt;span class="pl-k"&gt;export&lt;/span&gt; TASK_NAME=MRPC

python ./examples/run_glue.py \
    --model_type bert \
    --model_name_or_path bert-base-uncased \
    --task_name &lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt; \
    --do_train \
    --do_eval \
    --do_lower_case \
    --data_dir &lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/&lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt; \
    --max_seq_length 128 \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --output_dir /tmp/&lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt;/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.&lt;/p&gt;
&lt;p&gt;The dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called '/tmp/MNLI-MM/' in addition to '/tmp/MNLI/'.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-fine-tuning-xlnet-model-on-the-sts-b-regression-task" class="anchor" aria-hidden="true" href="#fine-tuning-xlnet-model-on-the-sts-b-regression-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning XLNet model on the STS-B regression task&lt;/h4&gt;
&lt;p&gt;This example code fine-tunes XLNet on the STS-B corpus using parallel training on a server with 4 V100 GPUs.
Parallel training is a simple way to use several GPUs (but is slower and less flexible than distributed training, see below).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue

python ./examples/run_glue.py \
    --model_type xlnet \
    --model_name_or_path xlnet-large-cased \
    --do_train  \
    --do_eval   \
    --task_name=sts-b     \
    --data_dir=&lt;span class="pl-smi"&gt;${GLUE_DIR}&lt;/span&gt;/STS-B  \
    --output_dir=./proc_data/sts-b-110   \
    --max_seq_length=128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --gradient_accumulation_steps=1 \
    --max_steps=1200  \
    --model_name=xlnet-large-cased   \
    --overwrite_output_dir   \
    --overwrite_cache \
    --warmup_steps=120&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On this machine we thus have a batch size of 32, please increase &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; to reach the same batch size if you have a smaller machine. These hyper-parameters should result in a Pearson correlation coefficient of &lt;code&gt;+0.917&lt;/code&gt; on the development set.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-fine-tuning-bert-model-on-the-mrpc-classification-task" class="anchor" aria-hidden="true" href="#fine-tuning-bert-model-on-the-mrpc-classification-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning Bert model on the MRPC classification task&lt;/h4&gt;
&lt;p&gt;This example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 &amp;gt; 92.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m torch.distributed.launch --nproc_per_node 8 ./examples/run_glue.py   \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --task_name MRPC \
    --do_train   \
    --do_eval   \
    --do_lower_case   \
    --data_dir &lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC/   \
    --max_seq_length 128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5   \
    --num_train_epochs 3.0  \
    --output_dir /tmp/mrpc_output/ \
    --overwrite_output_dir   \
    --overwrite_cache \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Training with these hyper-parameters gave us the following results:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;  acc = 0.8823529411764706
  acc_and_f1 = 0.901702786377709
  eval_loss = 0.3418912578906332
  f1 = 0.9210526315789473
  global_step = 174
  loss = 0.07231863956341798&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-run_squadpy-fine-tuning-on-squad-for-question-answering" class="anchor" aria-hidden="true" href="#run_squadpy-fine-tuning-on-squad-for-question-answering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_squad.py&lt;/code&gt;: Fine-tuning on SQuAD for question-answering&lt;/h3&gt;
&lt;p&gt;This example code fine-tunes BERT on the SQuAD dataset using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 &amp;gt; 93 on SQuAD:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m torch.distributed.launch --nproc_per_node=8 ./examples/run_squad.py \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --do_train \
    --do_eval \
    --do_lower_case \
    --train_file &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
    --predict_file &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
    --learning_rate 3e-5 \
    --num_train_epochs 2 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir ../models/wwm_uncased_finetuned_squad/ \
    --per_gpu_eval_batch_size=3   \
    --per_gpu_train_batch_size=3   \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Training with these hyper-parameters gave us the following results:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/evaluate-v1.1.py &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json
{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 86.91579943235573, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 93.1532499015869}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the model provided as &lt;code&gt;bert-large-uncased-whole-word-masking-finetuned-squad&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run_generationpy-text-generation-with-gpt-gpt-2-ctrl-transformer-xl-and-xlnet" class="anchor" aria-hidden="true" href="#run_generationpy-text-generation-with-gpt-gpt-2-ctrl-transformer-xl-and-xlnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_generation.py&lt;/code&gt;: Text generation with GPT, GPT-2, CTRL, Transformer-XL and XLNet&lt;/h3&gt;
&lt;p&gt;A conditional generation script is also included to generate text from a prompt.
The generation script includes the &lt;a href="https://github.com/rusiaaman/XLNet-gen#methodology"&gt;tricks&lt;/a&gt; proposed by Aman Rusia to get high-quality generation with memory models like Transformer-XL and XLNet (include a predefined text to make short inputs longer).&lt;/p&gt;
&lt;p&gt;Here is how to run the script with the small version of OpenAI GPT-2 model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python ./examples/run_generation.py \
    --model_type=gpt2 \
    --length=20 \
    --model_name_or_path=gpt2 \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and from the Salesforce CTRL model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python ./examples/run_generation.py \
    --model_type=ctrl \
    --length=20 \
    --model_name_or_path=ctrl \
    --temperature=0 \
    --repetition_penalty=1.2 \&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-migrating-from-pytorch-transformers-to-transformers" class="anchor" aria-hidden="true" href="#migrating-from-pytorch-transformers-to-transformers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Migrating from pytorch-transformers to transformers&lt;/h2&gt;
&lt;p&gt;Here is a quick summary of what you should take care of when migrating from &lt;code&gt;pytorch-transformers&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-positional-order-of-some-models-keywords-inputs-attention_mask-token_type_ids-changed" class="anchor" aria-hidden="true" href="#positional-order-of-some-models-keywords-inputs-attention_mask-token_type_ids-changed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Positional order of some models' keywords inputs (&lt;code&gt;attention_mask&lt;/code&gt;, &lt;code&gt;token_type_ids&lt;/code&gt;...) changed&lt;/h3&gt;
&lt;p&gt;To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models &lt;strong&gt;keywords inputs&lt;/strong&gt; (&lt;code&gt;attention_mask&lt;/code&gt;, &lt;code&gt;token_type_ids&lt;/code&gt;...) has been changed.&lt;/p&gt;
&lt;p&gt;If you used to call the models with keyword names for keyword arguments, e.g. &lt;code&gt;model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)&lt;/code&gt;, this should not cause any change.&lt;/p&gt;
&lt;p&gt;If you used to call the models with positional inputs for keyword arguments, e.g. &lt;code&gt;model(inputs_ids, attention_mask, token_type_ids)&lt;/code&gt;, you may have to double check the exact order of input arguments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-migrating-from-pytorch-pretrained-bert-to-transformers" class="anchor" aria-hidden="true" href="#migrating-from-pytorch-pretrained-bert-to-transformers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Migrating from pytorch-pretrained-bert to transformers&lt;/h2&gt;
&lt;p&gt;Here is a quick summary of what you should take care of when migrating from &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-models-always-output-tuples" class="anchor" aria-hidden="true" href="#models-always-output-tuples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models always output &lt;code&gt;tuples&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The main breaking change when migrating from &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt; is that every model's forward method always outputs a &lt;code&gt;tuple&lt;/code&gt; with various elements depending on the model and the configuration parameters.&lt;/p&gt;
&lt;p&gt;The exact content of the tuples for each model is detailed in the models' docstrings and the &lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is a &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt; conversion example for a &lt;code&gt;BertForSequenceClassification&lt;/code&gt; classification model:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Let's load our model&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; If you used to have this line in pytorch-pretrained-bert:&lt;/span&gt;
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Now just use this line in transformers to extract the loss from the output tuple:&lt;/span&gt;
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; In transformers you can also have access to the logits:&lt;/span&gt;
loss, logits &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs[:&lt;span class="pl-c1"&gt;2&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;output_attentions&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)
loss, logits, attentions &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-using-hidden-states" class="anchor" aria-hidden="true" href="#using-hidden-states"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using hidden states&lt;/h3&gt;
&lt;p&gt;By enabling the configuration option &lt;code&gt;output_hidden_states&lt;/code&gt;, it was possible to retrieve the last hidden states of the encoder. In &lt;code&gt;pytorch-transformers&lt;/code&gt; as well as &lt;code&gt;transformers&lt;/code&gt; the return value has changed slightly: &lt;code&gt;all_hidden_states&lt;/code&gt; now also includes the hidden state of the embeddings in addition to those of the encoding layers. This allows users to easily access the embeddings final state.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-serialization" class="anchor" aria-hidden="true" href="#serialization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Serialization&lt;/h3&gt;
&lt;p&gt;Breaking change in the &lt;code&gt;from_pretrained()&lt;/code&gt; method:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Models are now set in evaluation mode by default when instantiated with the &lt;code&gt;from_pretrained()&lt;/code&gt; method. To train them, don't forget to set them back in training mode (&lt;code&gt;model.train()&lt;/code&gt;) to activate the dropout modules.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The additional &lt;code&gt;*input&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt; arguments supplied to the &lt;code&gt;from_pretrained()&lt;/code&gt; method used to be directly passed to the underlying model's class &lt;code&gt;__init__()&lt;/code&gt; method. They are now used to update the model configuration attribute instead, which can break derived model classes built based on the previous &lt;code&gt;BertForSequenceClassification&lt;/code&gt; examples. We are working on a way to mitigate this breaking change in &lt;a href="https://github.com/huggingface/transformers/pull/866"&gt;#866&lt;/a&gt; by forwarding the the model's &lt;code&gt;__init__()&lt;/code&gt; method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method &lt;code&gt;save_pretrained(save_directory)&lt;/code&gt; if you were using any other serialization method before.&lt;/p&gt;
&lt;p&gt;Here is an example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Let's load a model and tokenizer&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Do some stuff to our model and tokenizer&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Ex: add new tokens to the vocabulary and embeddings of our model&lt;/span&gt;
tokenizer.add_tokens([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;[SPECIAL_TOKEN_1]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;[SPECIAL_TOKEN_2]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])
model.resize_token_embeddings(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(tokenizer))
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train our model&lt;/span&gt;
train(model)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Now let's save our model and tokenizer to a directory&lt;/span&gt;
model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Reload the model and the tokenizer&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-optimizers-bertadam--openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules" class="anchor" aria-hidden="true" href="#optimizers-bertadam--openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimizers: BertAdam &amp;amp; OpenAIAdam are now AdamW, schedules are standard PyTorch schedules&lt;/h3&gt;
&lt;p&gt;The two optimizers previously included, &lt;code&gt;BertAdam&lt;/code&gt; and &lt;code&gt;OpenAIAdam&lt;/code&gt;, have been replaced by a single &lt;code&gt;AdamW&lt;/code&gt; optimizer which has a few differences:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it only implements weights decay correction,&lt;/li&gt;
&lt;li&gt;schedules are now externals (see below),&lt;/li&gt;
&lt;li&gt;gradient clipping is now also external (see below).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The new optimizer &lt;code&gt;AdamW&lt;/code&gt; matches PyTorch &lt;code&gt;Adam&lt;/code&gt; optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping.&lt;/p&gt;
&lt;p&gt;The schedules are now standard &lt;a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate" rel="nofollow"&gt;PyTorch learning rate schedulers&lt;/a&gt; and not part of the optimizer anymore.&lt;/p&gt;
&lt;p&gt;Here is a conversion examples from &lt;code&gt;BertAdam&lt;/code&gt; with a linear warmup and decay schedule to &lt;code&gt;AdamW&lt;/code&gt; and the same schedule:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Parameters:&lt;/span&gt;
lr &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1e-3&lt;/span&gt;
max_grad_norm &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1.0&lt;/span&gt;
num_training_steps &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1000&lt;/span&gt;
num_warmup_steps &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt;
warmup_proportion &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;float&lt;/span&gt;(num_warmup_steps) &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;float&lt;/span&gt;(num_training_steps)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 0.1&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Previously BertAdam optimizer was instantiated like this:&lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertAdam(model.parameters(), &lt;span class="pl-v"&gt;lr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;lr, &lt;span class="pl-v"&gt;schedule&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;warmup_linear&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;warmup&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;warmup_proportion, &lt;span class="pl-v"&gt;t_total&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_training_steps)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## and used like this:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; batch &lt;span class="pl-k"&gt;in&lt;/span&gt; train_data:
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(batch)
    loss.backward()
    optimizer.step()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## In Transformers, optimizer and schedules are splitted and instantiated like this:&lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; AdamW(model.parameters(), &lt;span class="pl-v"&gt;lr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;lr, &lt;span class="pl-v"&gt;correct_bias&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To reproduce BertAdam specific behavior set correct_bias=False&lt;/span&gt;
scheduler &lt;span class="pl-k"&gt;=&lt;/span&gt; get_linear_schedule_with_warmup(optimizer, &lt;span class="pl-v"&gt;num_warmup_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_warmup_steps, &lt;span class="pl-v"&gt;num_training_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_training_steps)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; PyTorch scheduler&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## and used like this:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; batch &lt;span class="pl-k"&gt;in&lt;/span&gt; train_data:
    model.train()
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(batch)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Gradient clipping is not in AdamW anymore (so you can use amp without issue)&lt;/span&gt;
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;We now have a paper you can cite for the &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers library:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>huggingface</author><guid isPermaLink="false">https://github.com/huggingface/transformers</guid><pubDate>Fri, 29 Nov 2019 00:13:00 GMT</pubDate></item><item><title>pytorch/examples #14 in Python, This month</title><link>https://github.com/pytorch/examples</link><description>&lt;p&gt;&lt;i&gt;A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytorch-examples" class="anchor" aria-hidden="true" href="#pytorch-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Examples&lt;/h1&gt;
&lt;p&gt;A repository showcasing examples of using &lt;a href="https://github.com/pytorch/pytorch"&gt;PyTorch&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mnist"&gt;Image classification (MNIST) using Convnets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="word_language_model"&gt;Word level Language Modeling using LSTM RNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="imagenet"&gt;Training Imagenet Classifiers with Residual Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="dcgan"&gt;Generative Adversarial Networks (DCGAN)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vae"&gt;Variational Auto-Encoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="super_resolution"&gt;Superresolution using an efficient sub-pixel convolutional neural network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mnist_hogwild"&gt;Hogwild training of shared ConvNets across multiple processes on MNIST&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning"&gt;Training a CartPole to balance in OpenAI Gym with actor-critic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="snli"&gt;Natural Language Inference (SNLI) with GloVe vectors, LSTMs, and torchtext&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="time_sequence_prediction"&gt;Time sequence prediction - use an LSTM to learn Sine waves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="fast_neural_style"&gt;Implement the Neural Style Transfer algorithm on images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="cpp"&gt;Several examples illustrating the C++ Frontend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, a list of good examples hosted in their own repositories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/OpenNMT/OpenNMT-py"&gt;Neural Machine Translation using sequence-to-sequence RNN with attention (OpenNMT)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pytorch</author><guid isPermaLink="false">https://github.com/pytorch/examples</guid><pubDate>Fri, 29 Nov 2019 00:14:00 GMT</pubDate></item><item><title>fighting41love/funNLP #15 in Python, This month</title><link>https://github.com/fighting41love/funNLP</link><description>&lt;p&gt;&lt;i&gt;中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&amp;摘要相关工具、cocoNLP信息抽取工具、国内电话号码正则匹配、清华大学XLORE:中英文跨语言百科知识图谱、清华大学人工智能技术系列报告、自然语言生成、NLU太难了系列、自动对联数据及机器人、用户名黑名单列表、罪名法务名词及分类模型、微信公众号语料、cs224n深度学习自然语言处理课程、中文手写汉字识别、中文自然语言处理 语料/数据集、变量命名神器、分词语料库+代码、任务型对话英文数据集、ASR 语音数据集 + 基于深度学习的中文语音识别系统、笑声检测器、Microsoft多语言数字/单位/如日期时间识别包、中华新华字典数据库及api(包括常用歇后语、成语、词语和汉字)、文档图谱自动生成、SpaCy 中文模型、Common Voice语音识别数据集新版、神经网络关系抽取、基于bert的命名实体识别、关键词(Keyphrase)抽取包pke、基于医疗领域知识图谱的问答系统、基于依存句法与语义角色标注的事件三元组抽取、依存句法分析4万句高质量标注数据、cnocr：用来做中文OCR的Python3包、中文人物关系知识图谱项目、中文nlp竞赛项目及代码汇总、中文字符数据、speech-aligner: 从“人声语音”及其“语言文本”产生音素级别时间对齐标注的工具、AmpliGraph: 知识图谱表示学习(Python)库：知识图谱概念链接预测、Scattertext 文本可视化(python)、语言/知识表示工具：BERT &amp; ERNIE、中文对比英文自然语言处理NLP的区别综述、Synonyms中文近义词工具包、HarvestText领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）、word2word：(Python)方便易用的多语言词-词对集：62种语言/3,564个多语言对、语音识别语料生成工具：从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库、构建医疗实体识别的模型（包含词典和语料标注）、单文档非监督的关键词抽取、Kashgari中使用gpt-2语言模型、开源的金融投资数据提取工具、文本自动摘要库TextTeaser: 仅支持英文、人民日报语料处理工具集、一些关于自然语言的基本模型、基于14W歌曲知识库的问答尝试--功能包括歌词接龙and已知歌词找歌曲以及歌曲歌手歌词三角关系的问答、基于Siamese bilstm模型的相似句子判定模型并提供训练数据集和测试数据集、用Transformer编解码模型实现的根据Hacker News文章标题自动生成评论、用BERT进行序列标记和文本分类的模板代码、LitBank：NLP数据集——支持自然语言处理和计算人文学科任务的100部带标记英文小说语料、百度开源的基准信息抽取系统、虚假新闻数据集、Facebook: LAMA语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口、CommonsenseQA：面向常识的英文QA挑战、中文知识图谱资料、数据及工具、各大公司内部里大牛分享的技术文档 PDF 或者 PPT、自然语言生成SQL语句（英文）、中文NLP数据增强（EDA）工具、英文NLP数据增强工具 、基于医药知识图谱的智能问答系统、京东商品知识图谱、基于mongodb存储的军事领域知识图谱问答项目、基于远监督的中文关系抽取、语音情感分析、中文ULMFiT-情感分析-文本分类-语料及模型、一个拍照做题程序、世界各国大规模人名库、一个利用有趣中文语料库 qingyun 训练出来的中文聊天机器人、中文聊天机器人seqGAN、省市区镇行政区划数据带拼音标注、教育行业新闻语料库包含自动文摘功能、开放了对话机器人-知识图谱-语义理解-自然语言处理工具及数据、中文知识图谱：基于百度百科中文页面-抽取三元组信息-构建中文知识图谱、masr: 中文语音识别-提供预训练模型-高识别率、Python音频数据增广库、中文全词覆盖BERT及两份阅读理解数据、ConvLab：开源多域端到端对话系统平台、中文自然语言处理数据集、基于最新版本rasa搭建的对话系统、基于TensorFlow和BERT的管道式实体及关系抽取、一个小型的证券知识图谱/知识库、复盘所有NLP比赛的TOP方案、OpenCLaP：多领域开源中文预训练语言模型仓库、中文自然语言处理向量合集、基于金融-司法领域(兼有闲聊性质)的聊天机器人、g2pC：基于上下文的汉语读音自动标记模块、Zincbase 知识图谱构建工具包、诗歌质量评价/细粒度情感诗歌语料库。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;最近需要从文本中抽取结构化信息，用到了很多github上的包，遂整理了一下，后续会不断更新。&lt;/p&gt;
&lt;p&gt;很多包非常有趣，值得收藏，满足大家的收集癖！
如果觉得有用，请分享并star，谢谢！&lt;/p&gt;
&lt;p&gt;涉及内容包括：&lt;strong&gt;中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&amp;amp;摘要相关工具、cocoNLP信息抽取工具、国内电话号码正则匹配、清华大学XLORE:中英文跨语言百科知识图谱、清华大学人工智能技术系列报告、自然语言生成、NLU太难了系列、自动对联数据及机器人、用户名黑名单列表、罪名法务名词及分类模型、微信公众号语料、cs224n深度学习自然语言处理课程、中文手写汉字识别、中文自然语言处理 语料/数据集、变量命名神器、分词语料库+代码、任务型对话英文数据集、ASR 语音数据集 + 基于深度学习的中文语音识别系统、笑声检测器、Microsoft多语言数字/单位/如日期时间识别包、中华新华字典数据库及api(包括常用歇后语、成语、词语和汉字)、文档图谱自动生成、SpaCy 中文模型、Common Voice语音识别数据集新版、神经网络关系抽取、基于bert的命名实体识别、关键词(Keyphrase)抽取包pke、基于医疗领域知识图谱的问答系统、基于依存句法与语义角色标注的事件三元组抽取、依存句法分析4万句高质量标注数据、cnocr：用来做中文OCR的Python3包、中文人物关系知识图谱项目、中文nlp竞赛项目及代码汇总、中文字符数据、speech-aligner: 从“人声语音”及其“语言文本”产生音素级别时间对齐标注的工具、AmpliGraph: 知识图谱表示学习(Python)库：知识图谱概念链接预测、Scattertext 文本可视化(python)、语言/知识表示工具：BERT &amp;amp; ERNIE、中文对比英文自然语言处理NLP的区别综述、Synonyms中文近义词工具包、HarvestText领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）、word2word：(Python)方便易用的多语言词-词对集：62种语言/3,564个多语言对、语音识别语料生成工具：从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库、构建医疗实体识别的模型（包含词典和语料标注）、单文档非监督的关键词抽取、Kashgari中使用gpt-2语言模型、开源的金融投资数据提取工具、文本自动摘要库TextTeaser: 仅支持英文、人民日报语料处理工具集、一些关于自然语言的基本模型、基于14W歌曲知识库的问答尝试--功能包括歌词接龙and已知歌词找歌曲以及歌曲歌手歌词三角关系的问答、基于Siamese bilstm模型的相似句子判定模型并提供训练数据集和测试数据集、用Transformer编解码模型实现的根据Hacker News文章标题自动生成评论、用BERT进行序列标记和文本分类的模板代码、LitBank：NLP数据集——支持自然语言处理和计算人文学科任务的100部带标记英文小说语料、百度开源的基准信息抽取系统、虚假新闻数据集、Facebook: LAMA语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口、CommonsenseQA：面向常识的英文QA挑战、中文知识图谱资料、数据及工具、各大公司内部里大牛分享的技术文档 PDF 或者 PPT、自然语言生成SQL语句（英文）、中文NLP数据增强（EDA）工具、英文NLP数据增强工具 、基于医药知识图谱的智能问答系统、京东商品知识图谱、基于mongodb存储的军事领域知识图谱问答项目、基于远监督的中文关系抽取、语音情感分析、中文ULMFiT-情感分析-文本分类-语料及模型、一个拍照做题程序、世界各国大规模人名库、一个利用有趣中文语料库 qingyun 训练出来的中文聊天机器人、中文聊天机器人seqGAN、省市区镇行政区划数据带拼音标注、教育行业新闻语料库包含自动文摘功能、开放了对话机器人-知识图谱-语义理解-自然语言处理工具及数据、中文知识图谱：基于百度百科中文页面-抽取三元组信息-构建中文知识图谱、masr: 中文语音识别-提供预训练模型-高识别率、Python音频数据增广库、中文全词覆盖BERT及两份阅读理解数据、ConvLab：开源多域端到端对话系统平台、中文自然语言处理数据集、基于最新版本rasa搭建的对话系统、基于TensorFlow和BERT的管道式实体及关系抽取、一个小型的证券知识图谱/知识库、复盘所有NLP比赛的TOP方案、OpenCLaP：多领域开源中文预训练语言模型仓库、UER：基于不同语料+编码器+目标任务的中文预训练模型仓库、中文自然语言处理向量合集、基于金融-司法领域(兼有闲聊性质)的聊天机器人、g2pC：基于上下文的汉语读音自动标记模块、Zincbase 知识图谱构建工具包、诗歌质量评价/细粒度情感诗歌语料库、快速转化「中文数字」和「阿拉伯数字」、百度知道问答语料库、基于知识图谱的问答系统、jieba_fast 加速版的jieba、正则表达式教程、中文阅读理解数据集、基于BERT等最新语言模型的抽取式摘要提取、Python利用深度学习进行文本摘要的综合指南、知识图谱深度学习相关资料整理、维基大规模平行文本语料、StanfordNLP 0.2.0：纯Python版自然语言处理包、NeuralNLP-NeuralClassifier：腾讯开源深度学习文本分类工具、端到端的封闭域对话系统、中文命名实体识别：NeuroNER vs. BertNER、新闻事件线索抽取、2019年百度的三元组抽取比赛：“科学空间队”源码、基于依存句法的开放域文本知识三元组抽取和知识库构建、中文的GPT2训练代码、ML-NLP - 机器学习(Machine Learning)NLP面试中常考到的知识点和代码实现、nlp4han:中文自然语言处理工具集(断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检查、XLM：Facebook的跨语言预训练语言模型、用基于BERT的微调和特征提取方法来进行知识图谱百度百科人物词条属性抽取、中文自然语言处理相关的开放任务-数据集-当前最佳结果、CoupletAI - 基于CNN+Bi-LSTM+Attention 的自动对对联系统、抽象知识图谱、MiningZhiDaoQACorpus - 580万百度知道问答数据挖掘项目&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. textfilter: 中英文敏感词过滤&lt;/strong&gt;  &lt;a href="https://github.com/observerss/textfilter"&gt;observerss/textfilter&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; &amp;gt;&amp;gt;&amp;gt; f = DFAFilter()
 &amp;gt;&amp;gt;&amp;gt; f.add("sexy")
 &amp;gt;&amp;gt;&amp;gt; f.filter("hello sexy baby")
 hello **** baby
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;敏感词包括政治、脏话等话题词汇。其原理主要是基于词典的查找（项目中的keyword文件），内容很劲爆。。。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. langid：97种语言检测&lt;/strong&gt; &lt;a href="https://github.com/saffsd/langid.py"&gt;https://github.com/saffsd/langid.py&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install langid&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import langid
&amp;gt;&amp;gt;&amp;gt; langid.classify("This is a test")
('en', -54.41310358047485)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. langdetect：另一个语言检测&lt;/strong&gt;&lt;a href="https://code.google.com/archive/p/language-detection/" rel="nofollow"&gt;https://code.google.com/archive/p/language-detection/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install langdetect&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;from langdetect import detect
from langdetect import detect_langs

s1 = "本篇博客主要介绍两款语言探测工具，用于区分文本到底是什么语言，"
s2 = 'We are pleased to introduce today a new technology'
print(detect(s1))
print(detect(s2))
print(detect_langs(s3))    # detect_langs()输出探测出的所有语言类型及其所占的比例
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出结果如下： 注：语言类型主要参考的是ISO 639-1语言编码标准，详见&lt;a href="https://baike.baidu.com/item/ISO%20639-1" rel="nofollow"&gt;ISO 639-1百度百科&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;跟上一个语言检测比较，准确率低，效率高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. phone 中国手机归属地查询：&lt;/strong&gt; &lt;a href="https://github.com/ls0f/phone"&gt;ls0f/phone&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;已集成到 python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;中，欢迎试用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;from phone import Phone
p  = Phone()
p.find(18100065143)
#return {'phone': '18100065143', 'province': '上海', 'city': '上海', 'zip_code': '200000', 'area_code': '021', 'phone_type': '电信'}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;支持号段: 13*,15*,18*,14[5,7],17[0,6,7,8]&lt;/p&gt;
&lt;p&gt;记录条数: 360569 (updated:2017年4月)&lt;/p&gt;
&lt;p&gt;作者提供了数据&lt;a href="https://github.com/lovedboy/phone/raw/master/phone/phone.dat"&gt;phone.dat&lt;/a&gt; 方便非python用户Load数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. phone国际手机、电话归属地查询：&lt;/strong&gt;&lt;a href="https://github.com/AfterShip/phone"&gt;AfterShip/phone&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;npm install phone&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;import phone from 'phone';
phone('+852 6569-8900'); // return ['+85265698900', 'HKG']
phone('(817) 569-8900'); // return ['+18175698900, 'USA']
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;6. ngender 根据名字判断性别：&lt;/strong&gt;&lt;a href="https://github.com/observerss/ngender"&gt;observerss/ngender&lt;/a&gt; 基于朴素贝叶斯计算的概率&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install ngender&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import ngender
&amp;gt;&amp;gt;&amp;gt; ngender.guess('赵本山')
('male', 0.9836229687547046)
&amp;gt;&amp;gt;&amp;gt; ngender.guess('宋丹丹')
('female', 0.9759486128949907)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;7. 抽取email的正则表达式&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;已集成到 python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;中，欢迎试用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;email_pattern = '^[*#\u4e00-\u9fa5 a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+(\.[a-zA-Z0-9-]+)*\.[a-zA-Z0-9]{2,6}$'
emails = re.findall(email_pattern, text, flags=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;8. 抽取phone_number的正则表达式&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;已集成到 python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;中，欢迎试用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;cellphone_pattern = '^((13[0-9])|(14[0-9])|(15[0-9])|(17[0-9])|(18[0-9]))\d{8}$'
phoneNumbers = re.findall(cellphone_pattern, text, flags=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;9. 抽取身份证号的正则表达式&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;IDCards_pattern = r'^([1-9]\d{5}[12]\d{3}(0[1-9]|1[012])(0[1-9]|[12][0-9]|3[01])\d{3}[0-9xX])$'
IDs = re.findall(IDCards_pattern, text, flags=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;10.  人名语料库：&lt;/strong&gt; &lt;a href="https://github.com/wainshine/Chinese-Names-Corpus"&gt;wainshine/Chinese-Names-Corpus&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;人名抽取功能 python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;，欢迎试用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;中文（现代、古代）名字、日文名字、中文的姓和名、称呼（大姨妈、小姨妈等）、英文-&amp;gt;中文名字（李约翰）、成语词典
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;（可用于中文分词、姓名识别）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;11. 中文缩写库：&lt;/strong&gt;&lt;a href="https://github.com/zhangyics/Chinese-abbreviation-dataset/blob/master/dev_set.txt"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;全国人大: 全国/n 人民/n 代表大会/n
中国: 中华人民共和国/ns
女网赛: 女子/n 网球/n 比赛/vn
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;12. 汉语拆字词典：&lt;/strong&gt;&lt;a href="https://github.com/kfcd/chaizi"&gt;kfcd/chaizi&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;漢字	拆法 (一)	拆法 (二)	拆法 (三)
拆	手 斥	扌 斥	才 斥
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;13. 词汇情感值：&lt;/strong&gt;&lt;a href="https://github.com/rainarch/SentiBridge/blob/master/Entity_Emotion_Express/CCF_data/pair_mine_result"&gt;rainarch/SentiBridge&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;山泉水	充沛	0.400704566541	0.370067395878
视野	        宽广	0.305762728932	0.325320747491
大峡谷	惊险	0.312137906517	0.378594957281
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;14. 中文词库、停用词、敏感词&lt;/strong&gt; &lt;a href="https://github.com/fighting41love/Chinese_from_dongxiexidian"&gt;dongxiexidian/Chinese&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;此package的敏感词库分类更细：&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;反动词库&lt;/a&gt;， &lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;敏感词库表统计&lt;/a&gt;， &lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;暴恐词库&lt;/a&gt;， &lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;民生词库&lt;/a&gt;， &lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;色情词库&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;15. 汉字转拼音：&lt;/strong&gt;&lt;a href="https://github.com/mozillazg/python-pinyin"&gt;mozillazg/python-pinyin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文本纠错会用到&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;16. 中文繁简体互转：&lt;/strong&gt;&lt;a href="https://github.com/skydark/nstools/tree/master/zhtools"&gt;skydark/nstools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;17. 英文模拟中文发音引擎&lt;/strong&gt; funny chinese text to speech enginee：&lt;a href="https://github.com/tinyfool/ChineseWithEnglish"&gt;tinyfool/ChineseWithEnglish&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;say wo i ni
#说：我爱你
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;相当于用英文音标，模拟中文发音。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;18. 汪峰歌词生成器：&lt;/strong&gt;&lt;a href="https://github.com/phunterlau/wangfeng-rnn"&gt;phunterlau/wangfeng-rnn&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;我在这里中的夜里
就像一场是一种生命的意旪
就像我的生活变得在我一样
可我们这是一个知道
我只是一天你会怎吗
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;19. 同义词库、反义词库、否定词库：&lt;/strong&gt;&lt;a href="https://github.com/guotong1988/chinese_dictionary"&gt;guotong1988/chinese_dictionary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;20. 无空格英文串分割、抽取单词：&lt;/strong&gt;&lt;a href="https://github.com/keredson/wordninja"&gt;wordinja&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import wordninja
&amp;gt;&amp;gt;&amp;gt; wordninja.split('derekanderson')
['derek', 'anderson']
&amp;gt;&amp;gt;&amp;gt; wordninja.split('imateapot')
['im', 'a', 'teapot']
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;21. IP地址正则表达式：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;22. 腾讯QQ号正则表达式：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[1-9]([0-9]{5,11})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;23. 国内固话号码正则表达式：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[0-9-()（）]{7,18}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;24. 用户名正则表达式：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[A-Za-z0-9_\-\u4e00-\u9fa5]+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;25. 汽车品牌、汽车零件相关词汇：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;见本repo的data文件 [data](https://github.com/fighting41love/funNLP/tree/master/data)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;26. 时间抽取：&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;已集成到 python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;中，欢迎试用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;在2016年6月7日9:44执行測試，结果如下

Hi，all。下周一下午三点开会

&amp;gt;&amp;gt; 2016-06-13 15:00:00-false

周一开会

&amp;gt;&amp;gt; 2016-06-13 00:00:00-true

下下周一开会

&amp;gt;&amp;gt; 2016-06-20 00:00:00-true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://github.com/shinyke/Time-NLP"&gt;java version&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanzecheng/Time_NLP"&gt;python version&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;27. 各种中文词向量：&lt;/strong&gt; &lt;a href="https://github.com/Embedding/Chinese-Word-Vectors"&gt;github repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;中文词向量大全&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;28. 公司名字大全：&lt;/strong&gt; &lt;a href="https://github.com/wainshine/Company-Names-Corpus"&gt;github repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;29. 古诗词库：&lt;/strong&gt; &lt;a href="https://github.com/panhaiqi/AncientPoetry"&gt;github repo&lt;/a&gt; &lt;a href="https://github.com/chinese-poetry/chinese-poetry"&gt;更全的古诗词库&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;30. THU整理的词库：&lt;/strong&gt; &lt;a href="http://thuocl.thunlp.org/" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;已整理到本repo的data文件夹中.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;31. 中文聊天语料&lt;/strong&gt; &lt;a href="https://github.com/codemayq/chaotbot_corpus_Chinese"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;该库搜集了包含:豆瓣多轮, PTT八卦语料, 青云语料, 电视剧对白语料, 贴吧论坛回帖语料,微博语料,小黄鸡语料
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;32. 中文谣言数据:&lt;/strong&gt; &lt;a href="https://github.com/thunlp/Chinese_Rumor_Dataset"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;该数据文件中，每一行为一条json格式的谣言数据，字段释义如下：

rumorCode: 该条谣言的唯一编码，可以通过该编码直接访问该谣言举报页面。
title: 该条谣言被举报的标题内容
informerName: 举报者微博名称
informerUrl: 举报者微博链接
rumormongerName: 发布谣言者的微博名称
rumormongerUr: 发布谣言者的微博链接
rumorText: 谣言内容
visitTimes: 该谣言被访问次数
result: 该谣言审查结果
publishTime: 该谣言被举报时间
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;33. 情感波动分析：&lt;/strong&gt;&lt;a href="https://github.com/CasterWx/python-girlfriend-mood/"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;词库已整理到本repo的data文件夹中.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;本repo项目是一个通过与人对话获得其情感值波动图谱, 内用词库在data文件夹中.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;34. 百度中文问答数据集&lt;/strong&gt;：&lt;a href="https://pan.baidu.com/s/1QUsKcFWZ7Tg1dk_AbldZ1A" rel="nofollow"&gt;链接&lt;/a&gt; 提取码: 2dva&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;35. 句子、QA相似度匹配:MatchZoo&lt;/strong&gt;  &lt;a href="https://github.com/NTMC-Community/MatchZoo"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文本相似度匹配算法的集合，包含多个深度学习的方法，值得尝试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;36. bert资源：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bert论文中文翻译: &lt;a href="https://github.com/yuanxiaosc/BERT_Paper_Chinese_Translation"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;bert原作者的slides: &lt;a href="https://pan.baidu.com/s/1OSPsIu2oh1iJ-bcXoDZpJQ" rel="nofollow"&gt;link&lt;/a&gt;
提取码: iarj&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;文本分类实践: &lt;a href="https://github.com/NLPScott/bert-Chinese-classification-task"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert tutorial文本分类教程: &lt;a href="https://github.com/Socialbird-AILab/BERT-Classification-Tutorial"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert pytorch实现:  &lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert用于中文命名实体识别 tensorflow版本: &lt;a href="https://github.com/macanv/BERT-BiLSTM-CRF-NER"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BERT生成句向量，BERT做文本分类、文本相似度计算&lt;a href="https://github.com/terrifyzhao/bert-utils"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert 基于 keras 的封装分类标注框架 Kashgari，几分钟即可搭建一个分类或者序列标注模型: &lt;a href="https://github.com/BrikerMan/Kashgari"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert、ELMO的图解： &lt;a href="https://jalammar.github.io/illustrated-bert/" rel="nofollow"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BERT: Pre-trained models and downstream applications: &lt;a href="https://github.com/asyml/texar/tree/master/examples/bert"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;37. Texar - Toolkit for Text Generation and Beyond&lt;/strong&gt;: &lt;a href="https://github.com/asyml/texar"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;基于Tensorflow的开源工具包，旨在支持广泛的机器学习，特别是文本生成任务，如机器翻译、对话、摘要、内容处置、语言建模等&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;38. 中文事件抽取：&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/ComplexEventExtraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;中文复合事件抽取，包括条件事件、因果事件、顺承事件、反转事件等事件抽取，并形成事理图谱。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;39. cocoNLP:&lt;/strong&gt; &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;人名、地址、邮箱、手机号、手机归属地 等信息的抽取，rake短语抽取算法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install cocoNLP&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from cocoNLP.extractor import extractor

&amp;gt;&amp;gt;&amp;gt; ex = extractor()

&amp;gt;&amp;gt;&amp;gt; text = '急寻特朗普，男孩，于2018年11月27号11时在陕西省安康市汉滨区走失。丢失发型短发，...如有线索，请迅速与警方联系：18100065143，132-6156-2938，baizhantang@sina.com.cn 和yangyangfuture at gmail dot com'

# 抽取邮箱
&amp;gt;&amp;gt;&amp;gt; emails = ex.extract_email(text)
&amp;gt;&amp;gt;&amp;gt; print(emails)

['baizhantang@sina.com.cn', 'yangyangfuture@gmail.com.cn']
# 抽取手机号
&amp;gt;&amp;gt;&amp;gt; cellphones = ex.extract_cellphone(text,nation='CHN')
&amp;gt;&amp;gt;&amp;gt; print(cellphones)

['18100065143', '13261562938']
# 抽取手机归属地、运营商
&amp;gt;&amp;gt;&amp;gt; cell_locs = [ex.extract_cellphone_location(cell,'CHN') for cell in cellphones]
&amp;gt;&amp;gt;&amp;gt; print(cell_locs)

cellphone_location [{'phone': '18100065143', 'province': '上海', 'city': '上海', 'zip_code': '200000', 'area_code': '021', 'phone_type': '电信'}]
# 抽取地址信息
&amp;gt;&amp;gt;&amp;gt; locations = ex.extract_locations(text)
&amp;gt;&amp;gt;&amp;gt; print(locations)
['陕西省安康市汉滨区', '安康市汉滨区', '汉滨区']
# 抽取时间点
&amp;gt;&amp;gt;&amp;gt; times = ex.extract_time(text)
&amp;gt;&amp;gt;&amp;gt; print(times)
time {"type": "timestamp", "timestamp": "2018-11-27 11:00:00"}
# 抽取人名
&amp;gt;&amp;gt;&amp;gt; name = ex.extract_name(text)
&amp;gt;&amp;gt;&amp;gt; print(name)
特朗普

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;40. 国内电话号码正则匹配（三大运营商+虚拟等）:&lt;/strong&gt; &lt;a href="https://github.com/VincentSit/ChinaMobilePhoneNumberRegex"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;41. 清华大学XLORE:中英文跨语言百科知识图谱:&lt;/strong&gt; &lt;a href="https://xlore.org/download.html" rel="nofollow"&gt;link&lt;/a&gt;&lt;br&gt;
上述链接中包含了所有实体及关系的TTL文件，更多数据将在近期发布。
概念，实例，属性和上下位关系数目&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;百度&lt;/th&gt;
&lt;th&gt;中文维基&lt;/th&gt;
&lt;th&gt;英文维基&lt;/th&gt;
&lt;th&gt;总数&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;概念数量&lt;/td&gt;
&lt;td&gt;32,009&lt;/td&gt;
&lt;td&gt;150,241&lt;/td&gt;
&lt;td&gt;326,518&lt;/td&gt;
&lt;td&gt;508,768&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;实例数量&lt;/td&gt;
&lt;td&gt;1,629,591&lt;/td&gt;
&lt;td&gt;640,622&lt;/td&gt;
&lt;td&gt;1,235,178&lt;/td&gt;
&lt;td&gt;3,505,391&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;属性数量&lt;/td&gt;
&lt;td&gt;157,370&lt;/td&gt;
&lt;td&gt;45,190&lt;/td&gt;
&lt;td&gt;26,723&lt;/td&gt;
&lt;td&gt;229.283&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;InstanceOf&lt;/td&gt;
&lt;td&gt;7,584,931&lt;/td&gt;
&lt;td&gt;1,449,925&lt;/td&gt;
&lt;td&gt;3,032,515&lt;/td&gt;
&lt;td&gt;12,067,371&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SubClassOf&lt;/td&gt;
&lt;td&gt;2,784&lt;/td&gt;
&lt;td&gt;191,577&lt;/td&gt;
&lt;td&gt;555,538&lt;/td&gt;
&lt;td&gt;749,899&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;跨语言连接（概念/实例）&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;百度&lt;/th&gt;
&lt;th&gt;中文维基&lt;/th&gt;
&lt;th&gt;英文维基&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;百度&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;10,216/336,890&lt;/td&gt;
&lt;td&gt;4,846/303,108&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;中文维基&lt;/td&gt;
&lt;td&gt;10,216/336,890&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;28,921/454,579&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;英文维基&lt;/td&gt;
&lt;td&gt;4,846/303,108&lt;/td&gt;
&lt;td&gt;28,921/454,579&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;42. 清华大学人工智能技术系列报告：&lt;/strong&gt; &lt;a href="https://reports.aminer.cn" rel="nofollow"&gt;link&lt;/a&gt;&lt;br&gt;
每年会出AI领域相关的报告，内容包含&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自然语言处理 &lt;a href="https://static.aminer.cn/misc/article/nlp.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;知识图谱 &lt;a href="https://www.aminer.cn/research_report/5c3d5a8709%20e961951592a49d?download=true&amp;amp;pathname=knowledgegraph.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;数据挖掘 &lt;a href="https://www.aminer.cn/research_report/5c3d5a5cecb160952fa10b76?download=true&amp;amp;pathname=datamining.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;自动驾驶 &lt;a href="https://static.aminer.cn/misc/article/selfdriving.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;机器翻译 &lt;a href="https://static.aminer.cn/misc/article/translation.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;区块链 &lt;a href="https://static.aminer.cn/misc/article/blockchain_public.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;机器人 &lt;a href="https://static.aminer.cn/misc/article/robotics_beta.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;计算机图形学 &lt;a href="https://static.aminer.cn/misc/article/cg.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3D打印 &lt;a href="https://static.aminer.cn/misc/article/3d.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;人脸识别 &lt;a href="https://static.aminer.cn/misc/article/facerecognition.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;人工智能芯片 &lt;a href="https://static.aminer.cn/misc/article/aichip.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;等等&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;43.自然语言生成方面:&lt;/strong&gt;&lt;br&gt;
&lt;a href="https://ehudreiter.com" rel="nofollow"&gt;Ehud Reiter教授的博客&lt;/a&gt;  北大万小军教授强力推荐，该博客对NLG技术、评价与应用进行了深入的探讨与反思。&lt;br&gt;
&lt;a href="https://github.com/ChenChengKuan/awesome-text-generation"&gt;文本生成相关资源大列表&lt;/a&gt;&lt;br&gt;
&lt;a href="https://drive.google.com/file/d/1Mdna3q986k6OoJNsfAHznTtnMAEVzv5z/view" rel="nofollow"&gt;自然语言生成：让机器掌握自动创作的本领 - 开放域对话生成及在微软小冰中的实践&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/harvardnlp/Talk-Latent/blob/master/main.pdf"&gt;文本生成控制&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;44.:&lt;/strong&gt;
&lt;a href="https://github.com/fxsjy/jieba"&gt;jieba&lt;/a&gt;和&lt;a href="https://github.com/hankcs/pyhanlp"&gt;hanlp&lt;/a&gt;就不必介绍了吧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;45.NLP太难了系列:&lt;/strong&gt; &lt;a href="https://github.com/fighting41love/hardNLP"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;来到杨过曾经生活过的地方，小龙女动情地说：“我也想过过过儿过过的生活。” ​​​&lt;/li&gt;
&lt;li&gt;来到儿子等校车的地方，邓超对孙俪说：“我也想等等等等等过的那辆车。”&lt;/li&gt;
&lt;li&gt;赵敏说：我也想控忌忌己不想无忌。&lt;/li&gt;
&lt;li&gt;你也想犯范范范玮琪犯过的错吗&lt;/li&gt;
&lt;li&gt;对叙打击是一次性行为？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;46.自动对联数据及机器人:&lt;/strong&gt;&lt;br&gt;
&lt;a href="https://github.com/wb14123/couplet-dataset"&gt;70万对联数据 link&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/wb14123/seq2seq-couplet"&gt;代码 link&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;上联&lt;/th&gt;
&lt;th&gt;下联&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;殷勤怕负三春意&lt;/td&gt;
&lt;td&gt;潇洒难书一字愁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;如此清秋何吝酒&lt;/td&gt;
&lt;td&gt;这般明月不须钱&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;47.用户名黑名单列表：&lt;/strong&gt; &lt;a href="https://github.com/marteinn/The-Big-Username-Blacklist"&gt;github&lt;/a&gt;
包含了用户名禁用列表，比如: &lt;a href="https://github.com/marteinn/The-Big-Username-Blacklist/blob/master/list_raw.txt"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;administrator
administration
autoconfig
autodiscover
broadcasthost
domain
editor
guest
host
hostmaster
info
keybase.txt
localdomain
localhost
master
mail
mail0
mail1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;48.罪名法务名词及分类模型:&lt;/strong&gt;   &lt;a href="https://github.com/liuhuanyong/CrimeKgAssitant"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;包含856项罪名知识图谱, 基于280万罪名训练库的罪名预测,基于20W法务问答对的13类问题分类与法律资讯问答功能
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;49.微信公众号语料:&lt;/strong&gt; &lt;a href="https://github.com/nonamestreet/weixin_public_corpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3G语料，包含部分网络抓取的微信公众号的文章，已经去除HTML，只包含了纯文本。每行一篇，是JSON格式，name是微信公众号名字，account是微信公众号ID，title是题目，content是正文&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;50.cs224n深度学习自然语言处理课程：&lt;/strong&gt;&lt;a href="http://web.stanford.edu/class/cs224n/" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;课程中模型的pytorch实现 &lt;a href="https://github.com/DSKSD/DeepNLP-models-Pytorch"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;面向深度学习研究人员的自然语言处理实例教程 &lt;a href="https://github.com/graykode/nlp-tutorial"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;51.中文手写汉字识别：&lt;/strong&gt;&lt;a href="https://github.com/chizhanyuefeng/Chinese_OCR_CNN-RNN-CTC"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;52.中文自然语言处理 语料/数据集：&lt;/strong&gt;&lt;a href="https://github.com/SophonPlus/ChineseNlpCorpus"&gt;github&lt;/a&gt;
&lt;a href="https://github.com/thunlp/THUOCL"&gt;竞品：THUOCL（THU Open Chinese Lexicon）中文词库&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;53.变量命名神器：&lt;/strong&gt;&lt;a href="https://github.com/unbug/codelf"&gt;github&lt;/a&gt; &lt;a href="https://unbug.github.io/codelf/" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;54.分词语料库+代码：&lt;/strong&gt;&lt;a href="https://pan.baidu.com/s/1MXZONaLgeaw0_TxZZDAIYQ" rel="nofollow"&gt;百度网盘链接&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提取码: pea6&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/GlassyWing/bi-lstm-crf"&gt;keras实现的基于Bi-LSTM + CRF的中文分词+词性标注&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/GlassyWing/transformer-word-segmenter"&gt;基于Universal Transformer + CRF 的中文分词和词性标注&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/yaoguangluo/NeroParser"&gt;快速神经网络分词包 java version&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;55. NLP新书推荐《Natural Language Processing》by Jacob Eisenstein：&lt;/strong&gt; &lt;a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;56. 任务型对话英文数据集：&lt;/strong&gt;   &lt;a href="https://github.com/AtmaHou/Task-Oriented-Dialogue-Dataset-Survey"&gt;github&lt;/a&gt;&lt;br&gt;
【最全任务型对话数据集】主要介绍了一份任务型对话数据集大全，这份数据集大全涵盖了到目前在任务型对话领域的所有常用数据集的主要信息。此外，为了帮助研究者更好的把握领域进展的脉络，我们以Leaderboard的形式给出了几个数据集上的State-of-the-art实验结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;57. ASR 语音数据集 + 基于深度学习的中文语音识别系统：&lt;/strong&gt;  &lt;a href="https://github.com/nl8590687/ASRT_SpeechRecognition"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Data Sets 数据集&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;清华大学THCHS30中文语音数据集&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;data_thchs30.tgz
&lt;a href="http://cn-mirror.openslr.org/resources/18/data_thchs30.tgz" rel="nofollow"&gt;OpenSLR国内镜像&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/18/data_thchs30.tgz" rel="nofollow"&gt;OpenSLR国外镜像&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;test-noise.tgz
&lt;a href="http://cn-mirror.openslr.org/resources/18/test-noise.tgz" rel="nofollow"&gt;OpenSLR国内镜像&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/18/test-noise.tgz" rel="nofollow"&gt;OpenSLR国外镜像&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;resource.tgz
&lt;a href="http://cn-mirror.openslr.org/resources/18/resource.tgz" rel="nofollow"&gt;OpenSLR国内镜像&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/18/resource.tgz" rel="nofollow"&gt;OpenSLR国外镜像&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Free ST Chinese Mandarin Corpus&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ST-CMDS-20170001_1-OS.tar.gz
&lt;a href="http://cn-mirror.openslr.org/resources/38/ST-CMDS-20170001_1-OS.tar.gz" rel="nofollow"&gt;OpenSLR国内镜像&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/38/ST-CMDS-20170001_1-OS.tar.gz" rel="nofollow"&gt;OpenSLR国外镜像&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AIShell-1 开源版数据集&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;data_aishell.tgz
&lt;a href="http://cn-mirror.openslr.org/resources/33/data_aishell.tgz" rel="nofollow"&gt;OpenSLR国内镜像&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/33/data_aishell.tgz" rel="nofollow"&gt;OpenSLR国外镜像&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注：数据集解压方法&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ tar xzf data_aishell.tgz
$ cd data_aishell/wav
$ for tar in *.tar.gz;  do tar xvf $tar; done
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Primewords Chinese Corpus Set 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;primewords_md_2018_set1.tar.gz
&lt;a href="http://cn-mirror.openslr.org/resources/47/primewords_md_2018_set1.tar.gz" rel="nofollow"&gt;OpenSLR国内镜像&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/47/primewords_md_2018_set1.tar.gz" rel="nofollow"&gt;OpenSLR国外镜像&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;58. 笑声检测器：&lt;/strong&gt;  &lt;a href="https://github.com/ideo/LaughDetection"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;59. Microsoft多语言数字/单位/如日期时间识别包：&lt;/strong&gt; [github](&lt;a href="https://github.com/Microsoft/Recognizers-Text"&gt;https://github.com/Microsoft/Recognizers-Text&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;60. chinese-xinhua 中华新华字典数据库及api，包括常用歇后语、成语、词语和汉字&lt;/strong&gt; &lt;a href="https://github.com/pwxcoo/chinese-xinhua"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;61. 文档图谱自动生成&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/TextGrapher"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TextGrapher - Text Content Grapher based on keyinfo extraction by NLP method。输入一篇文档，将文档进行关键信息提取，进行结构化，并最终组织成图谱组织形式，形成对文章语义信息的图谱化展示&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;62. SpaCy 中文模型&lt;/strong&gt; &lt;a href="https://github.com/howl-anderson/Chinese_models_for_SpaCy"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包含Parser, NER, 语法树等功能。有一些英文package使用spacy的英文模型的，如果要适配中文，可能需要使用spacy中文模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;63. Common Voice语音识别数据集新版&lt;/strong&gt;  &lt;a href="https://voice.mozilla.org/en/datasets" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包括来自42,000名贡献者超过1,400小时的语音样本，涵github&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;64. 神经网络关系抽取 pytorch&lt;/strong&gt;  &lt;a href="https://github.com/ShulinCao/OpenNRE-PyTorch"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;暂不支持中文&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;65. 基于bert的命名实体识别 pytorch&lt;/strong&gt;  &lt;a href="https://github.com/Kyubyong/bert_ner"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;暂不支持中文&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;66. 关键词(Keyphrase)抽取包 pke&lt;/strong&gt;  &lt;a href="https://github.com/boudinfl/pke"&gt;github&lt;/a&gt;&lt;br&gt;
&lt;a href="http://aclweb.org/anthology/C16-2015" rel="nofollow"&gt;pke: an open source python-based keyphrase extraction toolkit&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;暂不支持中文，我于近期对其进行修改，使其适配中文。
请关注我的github动态，谢谢！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;67. 基于医疗领域知识图谱的问答系统&lt;/strong&gt;  &lt;a href="https://github.com/zhihao-chen/QASystemOnMedicalGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该repo参考了&lt;a href="https://github.com/liuhuanyong/QASystemOnMedicalKG"&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;68. 基于依存句法与语义角色标注的事件三元组抽取&lt;/strong&gt;  &lt;a href="https://github.com/liuhuanyong/EventTriplesExtraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;69. 依存句法分析4万句高质量标注数据&lt;/strong&gt; by 苏州大学汉语依存树库（SUCDT）
&lt;a href="http://hlt.suda.edu.cn/index.php/Nlpcc-2019-shared-task" rel="nofollow"&gt;Homepage&lt;/a&gt;
数据下载详见homepage底部，需要签署协议，需要邮件接收解压密码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;70. cnocr：用来做中文OCR的Python3包，自带了训练好的识别模型&lt;/strong&gt; &lt;a href="https://github.com/breezedeus/cnocr"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;71. 中文人物关系知识图谱项目&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/PersonRelationKnowledgeGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中文人物关系图谱构建&lt;/li&gt;
&lt;li&gt;基于知识库的数据回标&lt;/li&gt;
&lt;li&gt;基于远程监督与bootstrapping方法的人物关系抽取&lt;/li&gt;
&lt;li&gt;基于知识图谱的知识问答等应用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;72. 中文nlp竞赛项目及代码汇总&lt;/strong&gt; &lt;a href="https://github.com/geekinglcq/CDCS"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本生成、文本摘要：Byte Cup 2018 国际机器学习竞赛&lt;/li&gt;
&lt;li&gt;知识图谱：瑞金医院MMC人工智能辅助构建知识图谱大赛&lt;/li&gt;
&lt;li&gt;视频识别 问答：2018之江杯全球人工智能大赛 ：视频识别&amp;amp;问答&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;73. 中文字符数据&lt;/strong&gt; &lt;a href="https://github.com/skishore/makemeahanzi"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;简/繁体汉字笔顺&lt;/li&gt;
&lt;li&gt;矢量笔画&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;74. speech-aligner: 从“人声语音”及其“语言文本”，产生音素级别时间对齐标注的工具&lt;/strong&gt; &lt;a href="https://github.com/open-speech/speech-aligner"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;75. AmpliGraph: 知识图谱表示学习(Python)库：知识图谱概念链接预测&lt;/strong&gt; &lt;a href="https://github.com/Accenture/AmpliGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;埃森哲出品，目前尚不支持中文&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;76. Scattertext 文本可视化(python)&lt;/strong&gt; &lt;a href="https://github.com/JasonKessler/scattertext"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;很好用的工具包，简单修改后可支持中文&lt;/li&gt;
&lt;li&gt;能否分析出某个类别的文本与其他文本的用词差异&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;77. 语言/知识表示工具：BERT &amp;amp; ERNIE&lt;/strong&gt; &lt;a href="https://github.com/PaddlePaddle/LARK"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;百度出品，ERNIE也号称在多项nlp任务中击败了bert&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;78. 中文对比英文自然语言处理NLP的区别综述&lt;/strong&gt; &lt;a href="https://mp.weixin.qq.com/s/LQU_HJ4q74lL5oCIk7w5RA" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;79. Synonyms中文近义词工具包&lt;/strong&gt; &lt;a href="https://github.com/huyingxi/Synonyms"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Synonyms 中文近义词工具包，可以用于自然语言理解的很多任务：文本对齐，推荐算法，相似度计算，语义偏移，关键字提取，概念提取，自动摘要，搜索引擎等&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;80. HarvestText领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）&lt;/strong&gt; &lt;a href="https://github.com/blmoistawinde/HarvestText"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;81. word2word：(Python)方便易用的多语言词-词对集：62种语言/3,564个多语言对&lt;/strong&gt; &lt;a href="https://github.com/Kyubyong/word2word"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;82. 语音识别语料生成工具：从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库&lt;/strong&gt; &lt;a href="https://github.com/yc9701/pansori"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;83. ASR语音大辞典/词典：&lt;/strong&gt; github&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;84. 构建医疗实体识别的模型，包含词典和语料标注，基于python:&lt;/strong&gt; &lt;a href="https://github.com/yixiu00001/LSTM-CRF-medical"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;85. 单文档非监督的关键词抽取：&lt;/strong&gt; &lt;a href="https://github.com/LIAAD/yake"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;86. Kashgari中使用gpt-2语言模型&lt;/strong&gt; &lt;a href="https://github.com/BrikerMan/Kashgari"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;87.  开源的金融投资数据提取工具&lt;/strong&gt; &lt;a href="https://github.com/PKUJohnson/OpenData"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;88. 文本自动摘要库TextTeaser: 仅支持英文&lt;/strong&gt; &lt;a href="https://github.com/IndigoResearch/textteaser"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;89. 人民日报语料处理工具集&lt;/strong&gt; &lt;a href="https://github.com/howl-anderson/tools_for_corpus_of_people_daily"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;90. 一些关于自然语言的基本模型&lt;/strong&gt; &lt;a href="https://github.com/lpty/nlp_base"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;91. 基于14W歌曲知识库的问答尝试，功能包括歌词接龙，已知歌词找歌曲以及歌曲歌手歌词三角关系的问答&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/MusicLyricChatbot"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;92. 基于Siamese bilstm模型的相似句子判定模型,提供训练数据集和测试数据集&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/SiameseSentenceSimilarity"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提供了10万个训练样本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;93. 用Transformer编解码模型实现的根据Hacker News文章标题自动生成评论&lt;/strong&gt; &lt;a href="https://github.com/leod/hncynic"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;94. 用BERT进行序列标记和文本分类的模板代码&lt;/strong&gt; &lt;a href="https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;95. LitBank：NLP数据集——支持自然语言处理和计算人文学科任务的100部带标记英文小说语料&lt;/strong&gt; &lt;a href="https://github.com/dbamman/litbank"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;96. 百度开源的基准信息抽取系统&lt;/strong&gt; &lt;a href="https://github.com/baidu/information-extraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;97. 虚假新闻数据集 fake news corpus&lt;/strong&gt; &lt;a href="https://github.com/several27/FakeNewsCorpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;98. Facebook: LAMA语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口&lt;/strong&gt; &lt;a href="https://github.com/facebookresearch/LAMA"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;99. CommonsenseQA：面向常识的英文QA挑战&lt;/strong&gt; &lt;a href="https://www.tau-nlp.org/commonsenseqa" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;100. 中文知识图谱资料、数据及工具&lt;/strong&gt; &lt;a href="https://github.com/husthuke/awesome-knowledge-graph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;101. 各大公司内部里大牛分享的技术文档 PDF 或者 PPT&lt;/strong&gt; &lt;a href="https://github.com/0voice/from_coder_to_expert"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;102. 自然语言生成SQL语句（英文）&lt;/strong&gt; &lt;a href="https://github.com/paulfitz/mlsql"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;103. 中文NLP数据增强（EDA）工具&lt;/strong&gt; &lt;a href="https://github.com/zhanlaoban/eda_nlp_for_Chinese"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 英文NLP数据增强工具 &lt;a href="https://github.com/makcedward/nlpaug"&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;104. 基于医药知识图谱的智能问答系统&lt;/strong&gt; &lt;a href="https://github.com/YeYzheng/KGQA-Based-On-medicine"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;105. 京东商品知识图谱&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/ProductKnowledgeGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于京东网站的1300种商品上下级概念，约10万商品品牌，约65万品牌销售关系，商品描述维度等知识库，基于该知识库可以支持商品属性库构建，商品销售问答，品牌物品生产等知识查询服务，也可用于情感分析等下游应用．&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;106. 基于mongodb存储的军事领域知识图谱问答项目&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/QAonMilitaryKG"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于mongodb存储的军事领域知识图谱问答项目，包括飞行器、太空装备等8大类，100余小类，共计5800项的军事武器知识库，该项目不使用图数据库进行存储，通过jieba进行问句解析，问句实体项识别，基于查询模板完成多类问题的查询，主要是提供一种工业界的问答思想demo。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;107. 基于远监督的中文关系抽取&lt;/strong&gt; &lt;a href="https://github.com/xiaolalala/Distant-Supervised-Chinese-Relation-Extraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;108. 语音情感分析&lt;/strong&gt; &lt;a href="https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;109. 中文ULMFiT 情感分析 文本分类 语料及模型&lt;/strong&gt; &lt;a href="https://github.com/bigboNed3/chinese_ulmfit"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;110. 一个拍照做题程序。输入一张包含数学计算题的图片，输出识别出的数学计算式以及计算结果&lt;/strong&gt; &lt;a href="https://github.com/Roujack/mathAI"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;111. 世界各国大规模人名库&lt;/strong&gt; &lt;a href="https://github.com/philipperemy/name-dataset"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;112. 一个利用有趣中文语料库 qingyun 训练出来的中文聊天机器人&lt;/strong&gt; &lt;a href="https://github.com/Doragd/Chinese-Chatbot-PyTorch-Implementation"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用了青云语料10万语料，本repo中也有该语料的链接&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;113. 中文聊天机器人， 根据自己的语料训练出自己想要的聊天机器人，可以用于智能客服、在线问答、智能聊天等场景&lt;/strong&gt; &lt;a href="https://github.com/zhaoyingjun/chatbot"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据自己的语料训练出自己想要的聊天机器人，可以用于智能客服、在线问答、智能聊天等场景。加入seqGAN版本。&lt;/li&gt;
&lt;li&gt;repo中提供了一份质量不太高的语料&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;114. 省市区镇行政区划数据带拼音标注&lt;/strong&gt; &lt;a href="https://github.com/xiangyuecn/AreaCity-JsSpider-StatsGov"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;国家统计局中的省市区镇行政区划数据带拼音标注，高德地图的坐标和行政区域边界范围，在浏览器里面运行js代码采集的2019年发布的最新数据，含采集源码，提供csv格式数据，支持csv转成省市区多级联动js代码&lt;/li&gt;
&lt;li&gt;坐标、边界范围、名称、拼音、行政区等多级地址&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;115. 教育行业新闻 自动文摘 语料库&lt;/strong&gt; &lt;a href="https://github.com/wonderfulsuccess/chinese_abstractive_corpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;116. 开放了对话机器人、知识图谱、语义理解、自然语言处理工具及数据&lt;/strong&gt; &lt;a href="https://www.ownthink.com/#header-n30" rel="nofollow"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;另一个qa对的机器人 &lt;a href="https://github.com/WenRichard/QAmodel-for-Retrievalchatbot"&gt;Amodel-for-Retrivalchatbot - 客服机器人，Chinese Retreival chatbot（中文检索式机器人）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;117. 中文知识图谱：基于百度百科中文页面，抽取三元组信息，构建中文知识图谱&lt;/strong&gt; &lt;a href="https://github.com/lixiang0/WEB_KG"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;118. masr: 中文语音识别，提供预训练模型，高识别率&lt;/strong&gt; &lt;a href="https://github.com/lukhy/masr"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;119. Python音频数据增广库&lt;/strong&gt; &lt;a href="https://github.com/iver56/audiomentations"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;120. 中文全词覆盖BERT及两份阅读理解数据&lt;/strong&gt; &lt;a href="https://github.com/ymcui/Chinese-BERT-wwm"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DRCD数据集&lt;/strong&gt;由中国台湾台达研究院发布，其形式与SQuAD相同，是基于繁体中文的抽取式阅读理解数据集。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CMRC 2018数据集&lt;/strong&gt;是哈工大讯飞联合实验室发布的中文机器阅读理解数据。根据给定问题，系统需要从篇章中抽取出片段作为答案，形式与SQuAD相同。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;121. ConvLab：开源多域端到端对话系统平台&lt;/strong&gt; &lt;a href="https://github.com/ConvLab/ConvLab"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;122. 中文自然语言处理数据集&lt;/strong&gt; &lt;a href="https://github.com/InsaneLife/ChineseNLPCorpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;123. 基于最新版本rasa搭建的对话系统&lt;/strong&gt; &lt;a href="https://github.com/GaoQ1/rasa_chatbot_cn"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;124. 基于TensorFlow和BERT的管道式实体及关系抽取&lt;/strong&gt; &lt;a href="https://github.com/yuanxiaosc/Entity-Relation-Extraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entity and Relation Extraction Based on TensorFlow and BERT. 基于TensorFlow和BERT的管道式实体及关系抽取，2019语言与智能技术竞赛信息抽取任务解决方案。Schema based Knowledge Extraction, SKE 2019&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;125. 一个小型的证券知识图谱/知识库&lt;/strong&gt; &lt;a href="https://github.com/lemonhu/stock-knowledge-graph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;126. 复盘所有NLP比赛的TOP方案&lt;/strong&gt; &lt;a href="https://github.com/zhpmatrix/nlp-competitions-list-review"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;127. OpenCLaP：多领域开源中文预训练语言模型仓库&lt;/strong&gt; &lt;a href="https://github.com/thunlp/OpenCLaP"&gt;github&lt;/a&gt;
包含如下语言模型及百度百科数据&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;民事文书BERT	bert-base	全部民事文书	2654万篇文书	22554词	370MB&lt;/li&gt;
&lt;li&gt;刑事文书BERT	bert-base	全部刑事文书	663万篇文书	22554词	370MB&lt;/li&gt;
&lt;li&gt;百度百科BERT	bert-base	百度百科	903万篇词条	22166词	367MB&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;128. UER：基于不同语料、编码器、目标任务的中文预训练模型仓库（包括BERT、GPT、ELMO等）&lt;/strong&gt; &lt;a href="https://github.com/dbiir/UER-py"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于PyTorch的预训练模型框架，支持对编码器，目标任务等进行任意的组合，从而复现已有的预训练模型，或在已有的预训练模型上进一步改进。基于UER训练了不同性质的预训练模型（不同语料、编码器、目标任务），构成了中文预训练模型仓库，适用于不同的场景。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;129. 中文自然语言处理向量合集&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/ChineseEmbedding"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包括字向量,拼音向量,词向量,词性向量,依存关系向量.共5种类型的向量&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;130. 基于金融-司法领域(兼有闲聊性质)的聊天机器人&lt;/strong&gt; &lt;a href="https://github.com/charlesXu86/Chatbot_CN"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中的主要模块有信息抽取、NLU、NLG、知识图谱等，并且利用Django整合了前端展示,目前已经封装了nlp和kg的restful接口&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;131. g2pC：基于上下文的汉语读音自动标记模块&lt;/strong&gt; &lt;a href="https://github.com/Kyubyong/g2pC"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;132. Zincbase 知识图谱构建工具包&lt;/strong&gt; &lt;a href="https://github.com/tomgrek/zincbase"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;133. 诗歌质量评价/细粒度情感诗歌语料库&lt;/strong&gt; &lt;a href="https://github.com/THUNLP-AIPoet/Datasets"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;134. 快速转化「中文数字」和「阿拉伯数字」&lt;/strong&gt; &lt;a href="https://github.com/HaveTwoBrush/cn2an"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中文、阿拉伯数字互转&lt;/li&gt;
&lt;li&gt;中文与阿拉伯数字混合的情况，在开发中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;135. 百度知道问答语料库&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/MiningZhiDaoQACorpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;超过580万的问题，938万的答案，5800个分类标签。基于该问答语料库，可支持多种应用，如闲聊问答，逻辑挖掘&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;136. 基于知识图谱的问答系统&lt;/strong&gt; &lt;a href="https://github.com/WenRichard/KBQA-BERT"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BERT做命名实体识别和句子相似度，分为online和outline模式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;137. jieba_fast 加速版的jieba&lt;/strong&gt; &lt;a href="https://github.com/deepcs233/jieba_fast"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用cpython重写了jieba分词库中计算DAG和HMM中的vitrebi函数，速度得到大幅提升&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;138. 正则表达式教程&lt;/strong&gt; &lt;a href="https://github.com/ziishaned/learn-regex/blob/master/translations/README-cn.md"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;139. 中文阅读理解数据集&lt;/strong&gt; &lt;a href="https://github.com/ymcui/Chinese-RC-Datasets"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;140. 基于BERT等最新语言模型的抽取式摘要提取&lt;/strong&gt; &lt;a href="https://github.com/Hellisotherpeople/CX_DB8"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;141. Python利用深度学习进行文本摘要的综合指南&lt;/strong&gt; &lt;a href="https://mp.weixin.qq.com/s/gDZyTbM1nw3fbEnU--y3nQ" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;142. 知识图谱深度学习相关资料整理&lt;/strong&gt; &lt;a href="https://github.com/lihanghang/Knowledge-Graph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;深度学习与自然语言处理、知识图谱、对话系统。包括知识获取、知识库构建、知识库应用三大技术研究与应用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;143. 维基大规模平行文本语料&lt;/strong&gt; &lt;a href="https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;85种语言、1620种语言对、135M对照句&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;144. StanfordNLP 0.2.0：纯Python版自然语言处理包&lt;/strong&gt; &lt;a href="https://stanfordnlp.github.io/stanfordnlp/" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;145. NeuralNLP-NeuralClassifier：腾讯开源深度学习文本分类工具&lt;/strong&gt; &lt;a href="https://github.com/Tencent/NeuralNLP-NeuralClassifier"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;146. 端到端的封闭域对话系统&lt;/strong&gt; &lt;a href="https://github.com/cdqa-suite/cdQA"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;147. 中文命名实体识别：NeuroNER vs. BertNER&lt;/strong&gt; &lt;a href="https://github.com/EOA-AILab/NER-Chinese"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;148. 新闻事件线索抽取&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/ImportantEventExtractor"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An exploration for Eventline (important news Rank organized by pulic time)，针对某一事件话题下的新闻报道集合，通过使用docrank算法，对新闻报道进行重要性识别，并通过新闻报道时间挑选出时间线上重要新闻&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;149. 2019年百度的三元组抽取比赛，“科学空间队”源码(第7名)&lt;/strong&gt; &lt;a href="https://github.com/bojone/kg-2019"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;150. 基于依存句法的开放域文本知识三元组抽取和知识库构建&lt;/strong&gt; &lt;a href="https://github.com/lemonhu/open-entity-relation-extraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;151. 中文的GPT2训练代码&lt;/strong&gt; &lt;a href="https://github.com/Morizeyao/GPT2-Chinese"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;152. ML-NLP - 机器学习(Machine Learning)、NLP面试中常考到的知识点和代码实现&lt;/strong&gt; &lt;a href="https://github.com/NLP-LOVE/ML-NLP"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;153. nlp4han:中文自然语言处理工具集(断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检查&lt;/strong&gt; &lt;a href="https://github.com/kidden/nlp4han"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;154. XLM：Facebook的跨语言预训练语言模型&lt;/strong&gt; &lt;a href="https://github.com/facebookresearch/XLM"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;155. 用基于BERT的微调和特征提取方法来进行知识图谱百度百科人物词条属性抽取&lt;/strong&gt; &lt;a href="https://github.com/sakuranew/BERT-AttributeExtraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;156. 中文自然语言处理相关的开放任务，数据集, 以及当前最佳结果&lt;/strong&gt; &lt;a href="https://github.com/didi/ChineseNLP"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;157. CoupletAI - 基于CNN+Bi-LSTM+Attention 的自动对对联系统&lt;/strong&gt; &lt;a href="https://github.com/WiseDoge/CoupletAI"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;158. 抽象知识图谱，目前规模50万，支持名词性实体、状态性描述、事件性动作进行抽象&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/AbstractKnowledgeGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;159. MiningZhiDaoQACorpus - 580万百度知道问答数据挖掘项目&lt;/strong&gt; &lt;a href="%E7%99%BE%E5%BA%A6%E7%9F%A5%E9%81%93%E9%97%AE%E7%AD%94%E8%AF%AD%E6%96%99%E5%BA%93%EF%BC%8C%E5%8C%85%E6%8B%AC%E8%B6%85%E8%BF%87580%E4%B8%87%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%8C%E6%AF%8F%E4%B8%AA%E9%97%AE%E9%A2%98%E5%B8%A6%E6%9C%89%E9%97%AE%E9%A2%98%E6%A0%87%E7%AD%BE%E3%80%82%E5%9F%BA%E4%BA%8E%E8%AF%A5%E9%97%AE%E7%AD%94%E8%AF%AD%E6%96%99%E5%BA%93%EF%BC%8C%E5%8F%AF%E6%94%AF%E6%8C%81%E5%A4%9A%E7%A7%8D%E5%BA%94%E7%94%A8%EF%BC%8C%E5%A6%82%E9%80%BB%E8%BE%91%E6%8C%96%E6%8E%98"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;160. brat rapid annotation tool: 序列标注工具&lt;/strong&gt; &lt;a href="http://brat.nlplab.org/index.html" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;161. 大规模中文知识图谱数据：：1.4亿实体&lt;/strong&gt; &lt;a href="https://github.com/ownthink/KnowledgeGraphData"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;162. 数据增强在机器翻译及其他nlp任务中的应用及效果&lt;/strong&gt; &lt;a href="https://mp.weixin.qq.com/s/_aVwSWuYho_7MUT0LuFgVA" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;163. allennlp阅读理解:支持多种数据和模型&lt;/strong&gt; &lt;a href="https://github.com/allenai/allennlp-reading-comprehension"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;164. PDF表格数据提取工具&lt;/strong&gt; &lt;a href="https://github.com/camelot-dev/camelot"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;165. Graphbrain：AI开源软件库和科研工具，目的是促进自动意义提取和文本理解以及知识的探索和推断&lt;/strong&gt; &lt;a href="https://github.com/graphbrain/graphbrain"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;166. 简历自动筛选系统&lt;/strong&gt; &lt;a href="https://github.com/JAIJANYANI/Automated-Resume-Screening-System"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;167. 基于命名实体识别的简历自动摘要&lt;/strong&gt; &lt;a href="https://github.com/DataTurks-Engg/Entity-Recognition-In-Resumes-SpaCy"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;168. 中文语言理解测评基准，包括代表性的数据集&amp;amp;基准模型&amp;amp;语料库&amp;amp;排行榜&lt;/strong&gt; &lt;a href="https://github.com/brightmart/ChineseGLUE"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;169. 树洞 OCR 文字识别&lt;/strong&gt; &lt;a href="https://github.com/AnyListen/tools-ocr"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个c++ OCR &lt;a href="https://github.com/myhub/tr"&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;170. 从包含表格的扫描图片中识别表格和文字&lt;/strong&gt; &lt;a href="https://github.com/bitdata/ocrtable"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;171. 语声迁移&lt;/strong&gt; &lt;a href="https://github.com/fighting41love/become-yukarin"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;172. Python口语自然语言处理工具集(英文)&lt;/strong&gt; &lt;a href="https://github.com/gooofy/py-nltools"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;173. similarity：相似度计算工具包，java编写&lt;/strong&gt; &lt;a href="https://github.com/shibing624/similarity"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用于词语、短语、句子、词法分析、情感分析、语义分析等相关的相似度计算&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;174. 海量中文预训练ALBERT模型&lt;/strong&gt; &lt;a href="https://github.com/brightmart/albert_zh"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;175. Transformers 2.0&lt;/strong&gt; &lt;a href="https://github.com/huggingface/transformers"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持TensorFlow 2.0 和 PyTorch 的自然语言处理预训练语言模型(BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) 8种架构/33种预训练模型/102种语言&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;176. 基于大规模音频数据集Audioset的音频增强&lt;/strong&gt; &lt;a href="https://github.com/AppleHolic/audioset_augmentor"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;177. Poplar：网页版自然语言标注工具&lt;/strong&gt; &lt;a href="https://github.com/synyi/poplar"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;178. 图片文字去除，可用于漫画翻译&lt;/strong&gt; &lt;a href="https://github.com/yu45020/Text_Segmentation_Image_Inpainting"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;179. 186种语言的数字叫法库&lt;/strong&gt; &lt;a href="https://github.com/google/UniNum"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;180. Amazon发布基于知识的人-人开放领域对话数据集&lt;/strong&gt; &lt;a href="https://github.com/alexa/alexa-prize-topical-chat-dataset/"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;181. 中文文本纠错模块代码&lt;/strong&gt; &lt;a href="https://github.com/zedom1/error-detection"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;182. 繁简体转换&lt;/strong&gt; &lt;a href="https://github.com/berniey/hanziconv"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;183. Python实现的多种文本可读性评价指标&lt;/strong&gt; &lt;a href="https://github.com/cdimascio/py-readability-metrics"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;184. 类似于人名/地名/组织机构名的命名体识别数据集&lt;/strong&gt; &lt;a href="https://github.com/LG-1/video_music_book_datasets"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;185. 东南大学《知识图谱》研究生课程(资料)&lt;/strong&gt; &lt;a href="https://github.com/npubird/KnowledgeGraphCourse"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;186. 英文拼写检查库&lt;/strong&gt; &lt;a href="https://github.com/barrust/pyspellchecker"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from spellchecker import SpellChecker

spell = SpellChecker()

# find those words that may be misspelled
misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])

for word in misspelled:
    # Get the one `most likely` answer
    print(spell.correction(word))

    # Get a list of `likely` options
    print(spell.candidates(word))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;187. wwsearch是企业微信后台自研的全文检索引擎&lt;/strong&gt; &lt;a href="https://github.com/Tencent/wwsearch"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;188. CHAMELEON：深度学习新闻推荐系统元架构&lt;/strong&gt; &lt;a href="https://github.com/gabrielspmoreira/chameleon_recsys"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;189. 8篇论文梳理BERT相关模型进展与反思&lt;/strong&gt; &lt;a href="https://www.msra.cn/zh-cn/news/features/bert" rel="nofollow"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;190. DocSearch：免费文档搜索引擎&lt;/strong&gt; &lt;a href="https://github.com/algolia/docsearch"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;191. LIDA：轻量交互式对话标注工具&lt;/strong&gt; &lt;a href="https://github.com/Wluper/lida"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;192. aili - the fastest in-memory index in the East 东半球最快并发索引&lt;/strong&gt; &lt;a href="https://github.com/UncP/aili"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fighting41love</author><guid isPermaLink="false">https://github.com/fighting41love/funNLP</guid><pubDate>Fri, 29 Nov 2019 00:15:00 GMT</pubDate></item><item><title>shenweichen/DeepCTR #16 in Python, This month</title><link>https://github.com/shenweichen/DeepCTR</link><description>&lt;p&gt;&lt;i&gt;Easy-to-use,Modular and Extendible package of deep-learning based CTR models.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deepctr" class="anchor" aria-hidden="true" href="#deepctr"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepCTR&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://pypi.org/project/deepctr" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/38f6b7c09c84023c8f16959b9e1e125e4492391e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f646565706374722e737667" alt="Python Versions" data-canonical-src="https://img.shields.io/pypi/pyversions/deepctr.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/deepctr" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b7daf212d80a5608e2e73bb73fa55c4c456dea48/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f54656e736f72466c6f772d312e342b2f322e302b2d626c75652e737667" alt="TensorFlow Versions" data-canonical-src="https://img.shields.io/badge/TensorFlow-1.4+/2.0+-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pepy.tech/project/deepctr" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ad4b1fbbf29c164f17f96bfd951491458aac48e4/68747470733a2f2f706570792e746563682f62616467652f64656570637472" alt="Downloads" data-canonical-src="https://pepy.tech/badge/deepctr" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/deepctr" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/36c0fda3ff6ea8dccb874b60f3c01621f7bebaeb/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f646565706374722e737667" alt="PyPI Version" data-canonical-src="https://img.shields.io/pypi/v/deepctr.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/shenweichen/deepctr/issues"&gt;&lt;img src="https://camo.githubusercontent.com/e005422bb71732e1e12418055087779fe439dbb6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7368656e7765696368656e2f646565706374722e737667" alt="GitHub Issues" data-canonical-src="https://img.shields.io/github/issues/shenweichen/deepctr.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://deepctr-doc.readthedocs.io/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3d8a05c75e963e1e331ac3aec8d13e26c1420d89/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f646565706374722d646f632f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/deepctr-doc/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/shenweichen/DeepCTR" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/7b8574e67f6c966b9e8e3dec97f01879ba45cb1a/68747470733a2f2f7472617669732d63692e6f72672f7368656e7765696368656e2f446565704354522e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/shenweichen/DeepCTR.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/shenweichen/DeepCTR?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/67ebf7bfb66af5cc411b2877ff5615df8af8b2cb/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f7368656e7765696368656e2f446565704354522f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/shenweichen/DeepCTR/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.codacy.com/app/wcshen1994/DeepCTR?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=shenweichen/DeepCTR&amp;amp;utm_campaign=Badge_Grade" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/af110566b709b9ce1fc1d5253ebc29cf197faf92/68747470733a2f2f6170692e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f6434303939373334646330653462616239316433333265616438633062646430" alt="Codacy Badge" data-canonical-src="https://api.codacy.com/project/badge/Grade/d4099734dc0e4bab91d332ead8c0bdd0" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="./README.md#disscussiongroup"&gt;&lt;img src="https://camo.githubusercontent.com/a96f982a1eb95a7e530af1a25cf593d1659f8b5c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d7765636861742d627269676874677265656e3f7374796c653d666c6174" alt="Disscussion" data-canonical-src="https://img.shields.io/badge/chat-wechat-brightgreen?style=flat" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/shenweichen/deepctr/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/fb3394a39cec3c6da0046710bd963ea7702d4bdf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7368656e7765696368656e2f646565706374722e737667" alt="License" data-canonical-src="https://img.shields.io/github/license/shenweichen/deepctr.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;DeepCTR is a &lt;strong&gt;Easy-to-use&lt;/strong&gt;,&lt;strong&gt;Modular&lt;/strong&gt; and &lt;strong&gt;Extendible&lt;/strong&gt; package of deep-learning based CTR models along with lots of core components layers which can be used to easily build custom models.It is compatible with &lt;strong&gt;tensorflow 1.4+ and 2.0+&lt;/strong&gt;.You can use any complex model with &lt;code&gt;model.fit()&lt;/code&gt;and &lt;code&gt;model.predict()&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;Let's &lt;a href="https://deepctr-doc.readthedocs.io/en/latest/Quick-Start.html" rel="nofollow"&gt;&lt;strong&gt;Get Started!&lt;/strong&gt;&lt;/a&gt;(&lt;a href="https://zhuanlan.zhihu.com/p/53231955" rel="nofollow"&gt;Chinese Introduction&lt;/a&gt;)&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-models-list" class="anchor" aria-hidden="true" href="#models-list"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models List&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Model&lt;/th&gt;
&lt;th align="left"&gt;Paper&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Convolutional Click Prediction Model&lt;/td&gt;
&lt;td align="left"&gt;[CIKM 2015]&lt;a href="http://ir.ia.ac.cn/bitstream/173211/12337/1/A%20Convolutional%20Click%20Prediction%20Model.pdf" rel="nofollow"&gt;A Convolutional Click Prediction Model&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Factorization-supported Neural Network&lt;/td&gt;
&lt;td align="left"&gt;[ECIR 2016]&lt;a href="https://arxiv.org/pdf/1601.02376.pdf" rel="nofollow"&gt;Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Product-based Neural Network&lt;/td&gt;
&lt;td align="left"&gt;[ICDM 2016]&lt;a href="https://arxiv.org/pdf/1611.00144.pdf" rel="nofollow"&gt;Product-based neural networks for user response prediction&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Wide &amp;amp; Deep&lt;/td&gt;
&lt;td align="left"&gt;[DLRS 2016]&lt;a href="https://arxiv.org/pdf/1606.07792.pdf" rel="nofollow"&gt;Wide &amp;amp; Deep Learning for Recommender Systems&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;DeepFM&lt;/td&gt;
&lt;td align="left"&gt;[IJCAI 2017]&lt;a href="http://www.ijcai.org/proceedings/2017/0239.pdf" rel="nofollow"&gt;DeepFM: A Factorization-Machine based Neural Network for CTR Prediction&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Piece-wise Linear Model&lt;/td&gt;
&lt;td align="left"&gt;[arxiv 2017]&lt;a href="https://arxiv.org/abs/1704.05194" rel="nofollow"&gt;Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Deep &amp;amp; Cross Network&lt;/td&gt;
&lt;td align="left"&gt;[ADKDD 2017]&lt;a href="https://arxiv.org/abs/1708.05123" rel="nofollow"&gt;Deep &amp;amp; Cross Network for Ad Click Predictions&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Attentional Factorization Machine&lt;/td&gt;
&lt;td align="left"&gt;[IJCAI 2017]&lt;a href="http://www.ijcai.org/proceedings/2017/435" rel="nofollow"&gt;Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Neural Factorization Machine&lt;/td&gt;
&lt;td align="left"&gt;[SIGIR 2017]&lt;a href="https://arxiv.org/pdf/1708.05027.pdf" rel="nofollow"&gt;Neural Factorization Machines for Sparse Predictive Analytics&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;xDeepFM&lt;/td&gt;
&lt;td align="left"&gt;[KDD 2018]&lt;a href="https://arxiv.org/pdf/1803.05170.pdf" rel="nofollow"&gt;xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;AutoInt&lt;/td&gt;
&lt;td align="left"&gt;[arxiv 2018]&lt;a href="https://arxiv.org/abs/1810.11921" rel="nofollow"&gt;AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Deep Interest Network&lt;/td&gt;
&lt;td align="left"&gt;[KDD 2018]&lt;a href="https://arxiv.org/pdf/1706.06978.pdf" rel="nofollow"&gt;Deep Interest Network for Click-Through Rate Prediction&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Deep Interest Evolution Network&lt;/td&gt;
&lt;td align="left"&gt;[AAAI 2019]&lt;a href="https://arxiv.org/pdf/1809.03672.pdf" rel="nofollow"&gt;Deep Interest Evolution Network for Click-Through Rate Prediction&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;NFFM&lt;/td&gt;
&lt;td align="left"&gt;[arxiv 2019]&lt;a href="https://arxiv.org/pdf/1904.12579.pdf" rel="nofollow"&gt;Operation-aware Neural Networks for User Response Prediction&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;FGCNN&lt;/td&gt;
&lt;td align="left"&gt;[WWW 2019]&lt;a href="https://arxiv.org/pdf/1904.04447" rel="nofollow"&gt;Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Deep Session Interest Network&lt;/td&gt;
&lt;td align="left"&gt;[IJCAI 2019]&lt;a href="https://arxiv.org/abs/1905.06482" rel="nofollow"&gt;Deep Session Interest Network for Click-Through Rate Prediction &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;FiBiNET&lt;/td&gt;
&lt;td align="left"&gt;[RecSys 2019]&lt;a href="https://arxiv.org/pdf/1905.09433.pdf" rel="nofollow"&gt;FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-disscussiongroup" class="anchor" aria-hidden="true" href="#disscussiongroup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DisscussionGroup&lt;/h2&gt;
&lt;p&gt;Please follow our wechat to join group:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;公众号：&lt;strong&gt;浅梦的学习笔记&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;wechat ID: &lt;strong&gt;deepctrbot&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./docs/pics/weichennote.png"&gt;&lt;img src="./docs/pics/weichennote.png" alt="wechat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>shenweichen</author><guid isPermaLink="false">https://github.com/shenweichen/DeepCTR</guid><pubDate>Fri, 29 Nov 2019 00:16:00 GMT</pubDate></item><item><title>HelloZeroNet/ZeroNet #17 in Python, This month</title><link>https://github.com/HelloZeroNet/ZeroNet</link><description>&lt;p&gt;&lt;i&gt;ZeroNet - Decentralized websites using Bitcoin crypto and BitTorrent network&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-zeronet---" class="anchor" aria-hidden="true" href="#zeronet---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ZeroNet &lt;a href="https://travis-ci.org/HelloZeroNet/ZeroNet" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/bc91fbfda3d81f05104f7bb2cc92876e7be10559/68747470733a2f2f7472617669732d63692e6f72672f48656c6c6f5a65726f4e65742f5a65726f4e65742e7376673f6272616e63683d707933" alt="Build Status" data-canonical-src="https://travis-ci.org/HelloZeroNet/ZeroNet.svg?branch=py3" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://zeronet.io/docs/faq/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/dccbace744b6629415fd2d8df335b15a25e884e1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6661712d627269676874677265656e2e737667" alt="Documentation" data-canonical-src="https://img.shields.io/badge/docs-faq-brightgreen.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://zeronet.io/docs/help_zeronet/donate/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/17bb0834a6aa5c6ac8b78fffb0c10aa955cedc01/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6b6565705f746869735f70726f6a6563745f616c6976652d646f6e6174652d79656c6c6f772e737667" alt="Help" data-canonical-src="https://img.shields.io/badge/keep_this_project_alive-donate-yellow.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Decentralized websites using Bitcoin crypto and the BitTorrent network - &lt;a href="https://zeronet.io" rel="nofollow"&gt;https://zeronet.io&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-why" class="anchor" aria-hidden="true" href="#why"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We believe in open, free, and uncensored network and communication.&lt;/li&gt;
&lt;li&gt;No single point of failure: Site remains online so long as at least 1 peer is
serving it.&lt;/li&gt;
&lt;li&gt;No hosting costs: Sites are served by visitors.&lt;/li&gt;
&lt;li&gt;Impossible to shut down: It's nowhere because it's everywhere.&lt;/li&gt;
&lt;li&gt;Fast and works offline: You can access the site even if Internet is
unavailable.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Real-time updated sites&lt;/li&gt;
&lt;li&gt;Namecoin .bit domains support&lt;/li&gt;
&lt;li&gt;Easy to setup: unpack &amp;amp; run&lt;/li&gt;
&lt;li&gt;Clone websites in one click&lt;/li&gt;
&lt;li&gt;Password-less &lt;a href="https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki"&gt;BIP32&lt;/a&gt;
based authorization: Your account is protected by the same cryptography as your Bitcoin wallet&lt;/li&gt;
&lt;li&gt;Built-in SQL server with P2P data synchronization: Allows easier site development and faster page load times&lt;/li&gt;
&lt;li&gt;Anonymity: Full Tor network support with .onion hidden services instead of IPv4 addresses&lt;/li&gt;
&lt;li&gt;TLS encrypted connections&lt;/li&gt;
&lt;li&gt;Automatic uPnP port opening&lt;/li&gt;
&lt;li&gt;Plugin for multiuser (openproxy) support&lt;/li&gt;
&lt;li&gt;Works with any browser/OS&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-how-does-it-work" class="anchor" aria-hidden="true" href="#how-does-it-work"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How does it work?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;After starting &lt;code&gt;zeronet.py&lt;/code&gt; you will be able to visit zeronet sites using
&lt;code&gt;http://127.0.0.1:43110/{zeronet_address}&lt;/code&gt; (eg.
&lt;code&gt;http://127.0.0.1:43110/1HeLLo4uzjaLetFx6NH3PMwFP3qbRbTf3D&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;When you visit a new zeronet site, it tries to find peers using the BitTorrent
network so it can download the site files (html, css, js...) from them.&lt;/li&gt;
&lt;li&gt;Each visited site is also served by you.&lt;/li&gt;
&lt;li&gt;Every site contains a &lt;code&gt;content.json&lt;/code&gt; file which holds all other files in a sha512 hash
and a signature generated using the site's private key.&lt;/li&gt;
&lt;li&gt;If the site owner (who has the private key for the site address) modifies the
site, then he/she signs the new &lt;code&gt;content.json&lt;/code&gt; and publishes it to the peers.
Afterwards, the peers verify the &lt;code&gt;content.json&lt;/code&gt; integrity (using the
signature), they download the modified files and publish the new content to
other peers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-slideshow-about-zeronet-cryptography-site-updates-multi-user-sites-" class="anchor" aria-hidden="true" href="#slideshow-about-zeronet-cryptography-site-updates-multi-user-sites-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://docs.google.com/presentation/d/1_2qK1IuOKJ51pgBvllZ9Yu7Au2l551t3XBgyTSvilew/pub?start=false&amp;amp;loop=false&amp;amp;delayms=3000" rel="nofollow"&gt;Slideshow about ZeroNet cryptography, site updates, multi-user sites »&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-frequently-asked-questions-" class="anchor" aria-hidden="true" href="#frequently-asked-questions-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zeronet.io/docs/faq/" rel="nofollow"&gt;Frequently asked questions »&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-zeronet-developer-documentation-" class="anchor" aria-hidden="true" href="#zeronet-developer-documentation-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zeronet.io/docs/site_development/getting_started/" rel="nofollow"&gt;ZeroNet Developer Documentation »&lt;/a&gt;&lt;/h4&gt;
&lt;h2&gt;&lt;a id="user-content-screenshots" class="anchor" aria-hidden="true" href="#screenshots"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Screenshots&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/4629a7d44a828f5bb20cedd90522ae62f0947b35/68747470733a2f2f692e696d6775722e636f6d2f4836304f4148592e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/4629a7d44a828f5bb20cedd90522ae62f0947b35/68747470733a2f2f692e696d6775722e636f6d2f4836304f4148592e706e67" alt="Screenshot" data-canonical-src="https://i.imgur.com/H60OAHY.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/825299a2fdb6ec4665e4633a2ddf06cb2712b426/68747470733a2f2f7a65726f6e65742e696f2f646f63732f696d672f7a65726f74616c6b2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/825299a2fdb6ec4665e4633a2ddf06cb2712b426/68747470733a2f2f7a65726f6e65742e696f2f646f63732f696d672f7a65726f74616c6b2e706e67" alt="ZeroTalk" data-canonical-src="https://zeronet.io/docs/img/zerotalk.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-more-screenshots-in-zeronet-docs-" class="anchor" aria-hidden="true" href="#more-screenshots-in-zeronet-docs-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zeronet.io/docs/using_zeronet/sample_sites/" rel="nofollow"&gt;More screenshots in ZeroNet docs »&lt;/a&gt;&lt;/h4&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-join" class="anchor" aria-hidden="true" href="#how-to-join"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to join&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-windows" class="anchor" aria-hidden="true" href="#windows"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Windows&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Download &lt;a href="https://github.com/HelloZeroNet/ZeroNet-win/archive/dist-win64/ZeroNet-py3-win64.zip"&gt;ZeroNet-py3-win64.zip&lt;/a&gt; (18MB)&lt;/li&gt;
&lt;li&gt;Unpack anywhere&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;ZeroNet.exe&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-macos" class="anchor" aria-hidden="true" href="#macos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;macOS&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Download &lt;a href="https://github.com/HelloZeroNet/ZeroNet-dist/archive/mac/ZeroNet-dist-mac.zip"&gt;ZeroNet-dist-mac.zip&lt;/a&gt; (13.2MB)&lt;/li&gt;
&lt;li&gt;Unpack anywhere&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;ZeroNet.app&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-linux-x86-64bit" class="anchor" aria-hidden="true" href="#linux-x86-64bit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linux (x86-64bit)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;wget https://github.com/HelloZeroNet/ZeroNet-linux/archive/dist-linux64/ZeroNet-py3-linux64.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tar xvpfz ZeroNet-py3-linux64.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cd ZeroNet-linux-dist-linux64/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start with: &lt;code&gt;./ZeroNet.sh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Open the ZeroHello landing page in your browser by navigating to: &lt;a href="http://127.0.0.1:43110/" rel="nofollow"&gt;http://127.0.0.1:43110/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; Start with &lt;code&gt;./ZeroNet.sh --ui_ip '*' --ui_restrict your.ip.address&lt;/code&gt; to allow remote connections on the web interface.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-install-from-source" class="anchor" aria-hidden="true" href="#install-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install from source&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;wget https://github.com/HelloZeroNet/ZeroNet/archive/py3/ZeroNet-py3.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tar xvpfz ZeroNet-py3.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cd ZeroNet-py3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo apt-get update&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo apt-get install python3-pip&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo python3 -m pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start with: &lt;code&gt;python3 zeronet.py&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Open the ZeroHello landing page in your browser by navigating to: &lt;a href="http://127.0.0.1:43110/" rel="nofollow"&gt;http://127.0.0.1:43110/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-current-limitations" class="anchor" aria-hidden="true" href="#current-limitations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Current limitations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;del&gt;No torrent-like file splitting for big file support&lt;/del&gt; (big file support added)&lt;/li&gt;
&lt;li&gt;&lt;del&gt;No more anonymous than Bittorrent&lt;/del&gt; (built-in full Tor support added)&lt;/li&gt;
&lt;li&gt;File transactions are not compressed &lt;del&gt;or encrypted yet&lt;/del&gt; (TLS encryption added)&lt;/li&gt;
&lt;li&gt;No private sites&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-how-can-i-create-a-zeronet-site" class="anchor" aria-hidden="true" href="#how-can-i-create-a-zeronet-site"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How can I create a ZeroNet site?&lt;/h2&gt;
&lt;p&gt;Shut down zeronet if you are running it already&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ zeronet.py siteCreate
...
- Site private key: 23DKQpzxhbVBrAtvLEc2uvk7DZweh4qL3fn3jpM3LgHDczMK2TtYUq
- Site address: 13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2
...
- Site created&lt;span class="pl-k"&gt;!&lt;/span&gt;
$ zeronet.py
...&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Congratulations, you're finished! Now anyone can access your site using
&lt;code&gt;http://127.0.0.1:43110/13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Next steps: &lt;a href="https://zeronet.io/docs/site_development/getting_started/" rel="nofollow"&gt;ZeroNet Developer Documentation&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-can-i-modify-a-zeronet-site" class="anchor" aria-hidden="true" href="#how-can-i-modify-a-zeronet-site"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How can I modify a ZeroNet site?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Modify files located in data/13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2 directory.
After you're finished:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ zeronet.py siteSign 13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2
- Signing site: 13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2...
Private key (input hidden):&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Enter the private key you got when you created the site, then:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ zeronet.py sitePublish 13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2
...
Site:13DNDk..bhC2 Publishing to 3/10 peers...
Site:13DNDk..bhC2 Successfuly published to 3 peers
- Serving files....&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;That's it! You've successfully signed and published your modifications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-help-keep-this-project-alive" class="anchor" aria-hidden="true" href="#help-keep-this-project-alive"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Help keep this project alive&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bitcoin: 1QDhxQ6PraUZa21ET5fYUCPgdrwBomnFgX&lt;/li&gt;
&lt;li&gt;Paypal: &lt;a href="https://zeronet.io/docs/help_zeronet/donate/" rel="nofollow"&gt;https://zeronet.io/docs/help_zeronet/donate/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-sponsors" class="anchor" aria-hidden="true" href="#sponsors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sponsors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Better macOS/Safari compatibility made possible by &lt;a href="https://www.browserstack.com" rel="nofollow"&gt;BrowserStack.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-thank-you" class="anchor" aria-hidden="true" href="#thank-you"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Thank you!&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;More info, help, changelog, zeronet sites: &lt;a href="https://www.reddit.com/r/zeronet/" rel="nofollow"&gt;https://www.reddit.com/r/zeronet/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Come, chat with us: &lt;a href="https://kiwiirc.com/client/irc.freenode.net/zeronet" rel="nofollow"&gt;#zeronet @ FreeNode&lt;/a&gt; or on &lt;a href="https://gitter.im/HelloZeroNet/ZeroNet" rel="nofollow"&gt;gitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Email: &lt;a href="mailto:hello@zeronet.io"&gt;hello@zeronet.io&lt;/a&gt; (PGP: CB9613AE)&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>HelloZeroNet</author><guid isPermaLink="false">https://github.com/HelloZeroNet/ZeroNet</guid><pubDate>Fri, 29 Nov 2019 00:17:00 GMT</pubDate></item><item><title>home-assistant/home-assistant #18 in Python, This month</title><link>https://github.com/home-assistant/home-assistant</link><description>&lt;p&gt;&lt;i&gt;:house_with_garden: Open source home automation that puts local control and privacy first&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-home-assistant-" class="anchor" aria-hidden="true" href="#home-assistant-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Home Assistant &lt;a href="https://discord.gg/c5DvZ4e" rel="nofollow"&gt;&lt;img alt="Chat Status" src="https://camo.githubusercontent.com/599662b08725231a9f847723b9bc4f6dc4757d0c/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3333303934343233383931303936333731342e737667" data-canonical-src="https://img.shields.io/discord/330944238910963714.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Home Assistant is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control.&lt;/p&gt;
&lt;p&gt;To get started:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 -m pip install homeassistant
hass --open-ui&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Check out &lt;a href="https://home-assistant.io" rel="nofollow"&gt;home-assistant.io&lt;/a&gt; for &lt;a href="https://home-assistant.io/demo/" rel="nofollow"&gt;a
demo&lt;/a&gt;, &lt;a href="https://home-assistant.io/getting-started/" rel="nofollow"&gt;installation instructions&lt;/a&gt;,
&lt;a href="https://home-assistant.io/getting-started/automation-2/" rel="nofollow"&gt;tutorials&lt;/a&gt; and &lt;a href="https://home-assistant.io/docs/" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://home-assistant.io/demo/" rel="nofollow"&gt;&lt;img alt="screenshot-states" src="https://camo.githubusercontent.com/99578d7bca06d9c2973c2564e06f1ca444a4cce1/68747470733a2f2f7261772e6769746875622e636f6d2f686f6d652d617373697374616e742f686f6d652d617373697374616e742f6d61737465722f646f63732f73637265656e73686f74732e706e67" data-canonical-src="https://raw.github.com/home-assistant/home-assistant/master/docs/screenshots.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-featured-integrations"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-featured-integrations" class="anchor" aria-hidden="true" href="#featured-integrations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Featured integrations&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://home-assistant.io/integrations/" rel="nofollow"&gt;&lt;img alt="screenshot-components" src="https://camo.githubusercontent.com/ba21a6029ccb81d4a26b1ad9c198e61d01a07e7a/68747470733a2f2f7261772e6769746875622e636f6d2f686f6d652d617373697374616e742f686f6d652d617373697374616e742f6465762f646f63732f73637265656e73686f742d636f6d706f6e656e74732e706e67" data-canonical-src="https://raw.github.com/home-assistant/home-assistant/dev/docs/screenshot-components.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The system is built using a modular approach so support for other devices or actions can be implemented easily. See also the &lt;a href="https://developers.home-assistant.io/docs/en/architecture_index.html" rel="nofollow"&gt;section on architecture&lt;/a&gt; and the &lt;a href="https://developers.home-assistant.io/docs/en/creating_component_index.html" rel="nofollow"&gt;section on creating your own
components&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you run into issues while using Home Assistant or during development
of a component, check the &lt;a href="https://home-assistant.io/help/" rel="nofollow"&gt;Home Assistant help section&lt;/a&gt; of our website for further help and information.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>home-assistant</author><guid isPermaLink="false">https://github.com/home-assistant/home-assistant</guid><pubDate>Fri, 29 Nov 2019 00:18:00 GMT</pubDate></item><item><title>facebookresearch/detectron2 #19 in Python, This month</title><link>https://github.com/facebookresearch/detectron2</link><description>&lt;p&gt;&lt;i&gt;Detectron2 is FAIR's next-generation research platform for object detection and segmentation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/Detectron2-Logo-Horz.svg"&gt;&lt;img src=".github/Detectron2-Logo-Horz.svg" width="300" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Detectron2 is Facebook AI Research's next generation software system
that implements state-of-the-art object detection algorithms.
It is a ground-up rewrite of the previous version,
&lt;a href="https://github.com/facebookresearch/Detectron/"&gt;Detectron&lt;/a&gt;,
and it originates from &lt;a href="https://github.com/facebookresearch/maskrcnn-benchmark/"&gt;maskrcnn-benchmark&lt;/a&gt;.&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-whats-new" class="anchor" aria-hidden="true" href="#whats-new"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's New&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It is powered by the &lt;a href="https://pytorch.org" rel="nofollow"&gt;PyTorch&lt;/a&gt; deep learning framework.&lt;/li&gt;
&lt;li&gt;Includes more features such as panoptic segmentation, densepose, Cascade R-CNN, rotated bounding boxes, etc.&lt;/li&gt;
&lt;li&gt;Can be used as a library to support &lt;a href="projects/"&gt;different projects&lt;/a&gt; on top of it.
We'll open source more research projects in this way.&lt;/li&gt;
&lt;li&gt;It &lt;a href="https://detectron2.readthedocs.io/notes/benchmarks.html" rel="nofollow"&gt;trains much faster&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See our &lt;a href="https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/" rel="nofollow"&gt;blog post&lt;/a&gt;
to see more demos and learn about detectron2.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;See &lt;a href="INSTALL.md"&gt;INSTALL.md&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;See &lt;a href="GETTING_STARTED.md"&gt;GETTING_STARTED.md&lt;/a&gt;,
or the &lt;a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5" rel="nofollow"&gt;Colab Notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Learn more at our &lt;a href="https://detectron2.readthedocs.org" rel="nofollow"&gt;documentation&lt;/a&gt;.
And see &lt;a href="projects/"&gt;projects/&lt;/a&gt; for some projects that are built on top of detectron2.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-zoo-and-baselines" class="anchor" aria-hidden="true" href="#model-zoo-and-baselines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model Zoo and Baselines&lt;/h2&gt;
&lt;p&gt;We provide a large set of baseline results and trained models available for download in the &lt;a href="MODEL_ZOO.md"&gt;Detectron2 Model Zoo&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Detectron2 is released under the &lt;a href="LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citing-detectron" class="anchor" aria-hidden="true" href="#citing-detectron"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing Detectron&lt;/h2&gt;
&lt;p&gt;If you use Detectron2 in your research or wish to refer to the baseline results published in the &lt;a href="MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt;, please use the following BibTeX entry.&lt;/p&gt;
&lt;div class="highlight highlight-text-bibtex"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;@misc&lt;/span&gt;{&lt;span class="pl-en"&gt;wu2019detectron2&lt;/span&gt;,
  &lt;span class="pl-s"&gt;author&lt;/span&gt; =       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;Yuxin Wu and Alexander Kirillov and Francisco Massa and&lt;/span&gt;
&lt;span class="pl-s"&gt;                  Wan-Yen Lo and Ross Girshick&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;title&lt;/span&gt; =        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;Detectron2&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;howpublished&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;\url{https://github.com/facebookresearch/detectron2}&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;year&lt;/span&gt; =         &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;2019&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>facebookresearch</author><guid isPermaLink="false">https://github.com/facebookresearch/detectron2</guid><pubDate>Fri, 29 Nov 2019 00:19:00 GMT</pubDate></item><item><title>getsentry/sentry #20 in Python, This month</title><link>https://github.com/getsentry/sentry</link><description>&lt;p&gt;&lt;i&gt;Sentry is cross-platform application monitoring, with a focus on error reporting.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;/p&gt;&lt;p align="center"&gt;
    &lt;a href="https://sentry.io/?utm_source=github&amp;amp;utm_medium=logo" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/2dfeafbee0904d6df16ddf7200993dace1629e60/68747470733a2f2f73656e7472792d6272616e642e73746f726167652e676f6f676c65617069732e636f6d2f73656e7472792d6c6f676f2d626c61636b2e706e67" alt="Sentry" height="72" data-canonical-src="https://sentry-brand.storage.googleapis.com/sentry-logo-black.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p align="center"&gt;
    Users and logs provide clues. Sentry provides answers.
  &lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;a name="user-content-what-s-sentry"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-whats-sentry" class="anchor" aria-hidden="true" href="#whats-sentry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's Sentry?&lt;/h2&gt;
&lt;p&gt;Sentry fundamentally is a service that helps you monitor and fix crashes in realtime.
The server is in Python, but it contains a full API for sending events from any
language, in any application.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-1.png"&gt;&lt;img src="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-1.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-2.png"&gt;&lt;img src="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-2.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-3.png"&gt;&lt;img src="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-3.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;&lt;a name="user-content-official-sentry-sdks"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-official-sentry-sdks" class="anchor" aria-hidden="true" href="#official-sentry-sdks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Official Sentry SDKs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-javascript"&gt;JavaScript&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/react-native-sentry"&gt;React-Native&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-python"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/raven-ruby"&gt;Ruby&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-php"&gt;PHP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-go"&gt;Go&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-java"&gt;Java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-cocoa"&gt;Objective-C/Swift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-dotnet"&gt;C#&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/perl-raven"&gt;Perl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-elixir"&gt;Elixir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-laravel"&gt;Laravel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-resources"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.sentry.io/" rel="nofollow"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://forum.sentry.io/" rel="nofollow"&gt;Community&lt;/a&gt; (Bugs, feature requests, general questions)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.sentry.io/internal/contributing/" rel="nofollow"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry/issues"&gt;Bug Tracker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://discord.gg/ez5KZN7" rel="nofollow"&gt;Discord&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.transifex.com/getsentry/sentry/" rel="nofollow"&gt;Transifex&lt;/a&gt; (Translate Sentry!)&lt;/li&gt;
&lt;/ul&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>getsentry</author><guid isPermaLink="false">https://github.com/getsentry/sentry</guid><pubDate>Fri, 29 Nov 2019 00:20:00 GMT</pubDate></item><item><title>dbolya/yolact #21 in Python, This month</title><link>https://github.com/dbolya/yolact</link><description>&lt;p&gt;&lt;i&gt;A simple, fully convolutional model for real-time instance segmentation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-you-only-look-at-coefficients" class="anchor" aria-hidden="true" href="#you-only-look-at-coefficients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Y&lt;/strong&gt;ou &lt;strong&gt;O&lt;/strong&gt;nly &lt;strong&gt;L&lt;/strong&gt;ook &lt;strong&gt;A&lt;/strong&gt;t &lt;strong&gt;C&lt;/strong&gt;oefficien&lt;strong&gt;T&lt;/strong&gt;s&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;    ██╗   ██╗ ██████╗ ██╗      █████╗  ██████╗████████╗
    ╚██╗ ██╔╝██╔═══██╗██║     ██╔══██╗██╔════╝╚══██╔══╝
     ╚████╔╝ ██║   ██║██║     ███████║██║        ██║   
      ╚██╔╝  ██║   ██║██║     ██╔══██║██║        ██║   
       ██║   ╚██████╔╝███████╗██║  ██║╚██████╗   ██║   
       ╚═╝    ╚═════╝ ╚══════╝╚═╝  ╚═╝ ╚═════╝   ╚═╝ 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A simple, fully convolutional model for real-time instance segmentation. This is the code for &lt;a href="https://arxiv.org/abs/1904.02689" rel="nofollow"&gt;our paper&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-iccv-update-v11-released-check-out-the-iccv-trailer-here" class="anchor" aria-hidden="true" href="#iccv-update-v11-released-check-out-the-iccv-trailer-here"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ICCV update (v1.1) released! Check out the ICCV trailer here:&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=0pMfmo8qfpQ" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c9f0f1403e25276c0beea78732b5cec6c9b610ab/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f30704d666d6f38716670512f302e6a7067" alt="IMAGE ALT TEXT HERE" data-canonical-src="https://img.youtube.com/vi/0pMfmo8qfpQ/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Read &lt;a href="CHANGELOG.md"&gt;the changelog&lt;/a&gt; for details on, well, what changed. Oh, and the paper got updated too with pascal results and an appendix with box mAP.&lt;/p&gt;
&lt;p&gt;Some examples from our base model (33.5 fps on a Titan Xp and 29.8 mAP on COCO's &lt;code&gt;test-dev&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_0.png"&gt;&lt;img src="data/yolact_example_0.png" alt="Example 0" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_1.png"&gt;&lt;img src="data/yolact_example_1.png" alt="Example 1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_2.png"&gt;&lt;img src="data/yolact_example_2.png" alt="Example 2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Set up a Python3 environment.&lt;/li&gt;
&lt;li&gt;Install &lt;a href="http://pytorch.org/" rel="nofollow"&gt;Pytorch&lt;/a&gt; 1.0.1 (or higher) and TorchVision.&lt;/li&gt;
&lt;li&gt;Install some other packages:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Cython needs to be installed before pycocotools&lt;/span&gt;
pip install cython
pip install opencv-python pillow pycocotools matplotlib &lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Clone this repository and enter it:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/dbolya/yolact.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; yolact&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you'd like to train YOLACT, download the COCO dataset and the 2014/2017 annotations. Note that this script will take a while and dump 21gb of files into &lt;code&gt;./data/coco&lt;/code&gt;.
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh data/scripts/COCO.sh&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you'd like to evaluate YOLACT on &lt;code&gt;test-dev&lt;/code&gt;, download &lt;code&gt;test-dev&lt;/code&gt; with this script.
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh data/scripts/COCO_test.sh&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-evaluation" class="anchor" aria-hidden="true" href="#evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evaluation&lt;/h1&gt;
&lt;p&gt;As of April 5th, 2019 here are our latest models along with their FPS on a Titan Xp and mAP on &lt;code&gt;test-dev&lt;/code&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Image Size&lt;/th&gt;
&lt;th align="center"&gt;Backbone&lt;/th&gt;
&lt;th align="center"&gt;FPS&lt;/th&gt;
&lt;th align="center"&gt;mAP&lt;/th&gt;
&lt;th&gt;Weights&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Resnet50-FPN&lt;/td&gt;
&lt;td align="center"&gt;42.5&lt;/td&gt;
&lt;td align="center"&gt;28.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1yp7ZbbDwvMiFJEq4ptVKTYTI2VeRDXl0/view?usp=sharing" rel="nofollow"&gt;yolact_resnet50_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EUVpxoSXaqNIlssoLKOEoCcB1m0RpzGq_Khp5n1VX3zcUw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Darknet53-FPN&lt;/td&gt;
&lt;td align="center"&gt;40.0&lt;/td&gt;
&lt;td align="center"&gt;28.7&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1dukLrTzZQEuhzitGkHaGjphlmRJOjVnP/view?usp=sharing" rel="nofollow"&gt;yolact_darknet53_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/ERrao26c8llJn25dIyZPhwMBxUp2GdZTKIMUQA3t0djHLw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Resnet101-FPN&lt;/td&gt;
&lt;td align="center"&gt;33.0&lt;/td&gt;
&lt;td align="center"&gt;29.8&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1UYy3dMapbH1BnmtZU4WH1zbYgOzzHHf_/view?usp=sharing" rel="nofollow"&gt;yolact_base_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EYRWxBEoKU9DiblrWx2M89MBGFkVVB_drlRd_v5sdT3Hgg" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;700&lt;/td&gt;
&lt;td align="center"&gt;Resnet101-FPN&lt;/td&gt;
&lt;td align="center"&gt;23.6&lt;/td&gt;
&lt;td align="center"&gt;31.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1lE4Lz5p25teiXV-6HdTiOJSnS7u7GBzg/view?usp=sharing" rel="nofollow"&gt;yolact_im700_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/Eagg5RSc5hFEhp7sPtvLNyoBjhlf2feog7t8OQzHKKphjw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To evalute the model, put the corresponding weights file in the &lt;code&gt;./weights&lt;/code&gt; directory and run one of the following commands.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quantitative-results-on-coco" class="anchor" aria-hidden="true" href="#quantitative-results-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quantitative Results on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Quantitatively evaluate a trained model on the entire validation set. Make sure you have COCO downloaded as above.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This should get 29.92 validation mask mAP last time I checked.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Output a COCOEval json to submit to the website or to use the run_coco_eval.py script.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This command will create './results/bbox_detections.json' and './results/mask_detections.json' for detection and instance segmentation respectively.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; You can run COCOEval on the files created in the previous command. The performance should match my implementation in eval.py.&lt;/span&gt;
python run_coco_eval.py

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To output a coco json file for test-dev, make sure you have test-dev downloaded from above and go&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json --dataset=coco2017_testdev_dataset&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-qualitative-results-on-coco" class="anchor" aria-hidden="true" href="#qualitative-results-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Qualitative Results on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display qualitative results on COCO. From here on I'll use a confidence threshold of 0.15.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --display&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-benchmarking-on-coco" class="anchor" aria-hidden="true" href="#benchmarking-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmarking on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run just the raw model on the first 1k images of the validation set&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --benchmark --max_images=1000&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-images" class="anchor" aria-hidden="true" href="#images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Images&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display qualitative results on the specified image.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=my_image.png

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process an image and save it to another file.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=input_image.png:output_image.png

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process a whole folder of images.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --images=path/to/input/folder:path/to/output/folder&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-video" class="anchor" aria-hidden="true" href="#video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display a video in real-time. "--video_multiframe" will process that many frames at once for improved performance.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; If you want, use "--display_fps" to draw the FPS directly on the frame.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=my_video.mp4

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display a webcam feed in real-time. If you have multiple webcams pass the index of the webcam you want instead of 0.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=0

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process a video and save it to another file. This uses the same pipeline as the ones above now, so it's fast!&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=input_video.mp4:output_video.mp4&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can tell, &lt;code&gt;eval.py&lt;/code&gt; can do a ton of stuff. Run the &lt;code&gt;--help&lt;/code&gt; command to see everything it can do.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python eval.py --help&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h1&gt;
&lt;p&gt;By default, we train on COCO. Make sure to download the entire dataset using the commands above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To train, grab an imagenet-pretrained model and put it in &lt;code&gt;./weights&lt;/code&gt;.
&lt;ul&gt;
&lt;li&gt;For Resnet101, download &lt;code&gt;resnet101_reducedfc.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/1tvqFPd4bJtakOlmn-uIA492g2qurRChj/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Resnet50, download &lt;code&gt;resnet50-19c8e357.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/1Jy3yCdbatgXa5YYIdTCRrSV0S9V5g1rn/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Darknet53, download &lt;code&gt;darknet53.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/17Y431j4sagFpSReuPNoFcj9h7azDTZFf/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run one of the training commands below.
&lt;ul&gt;
&lt;li&gt;Note that you can press ctrl+c while training and it will save an &lt;code&gt;*_interrupt.pth&lt;/code&gt; file at the current iteration.&lt;/li&gt;
&lt;li&gt;All weights are saved in the &lt;code&gt;./weights&lt;/code&gt; directory by default with the file name &lt;code&gt;&amp;lt;config&amp;gt;_&amp;lt;epoch&amp;gt;_&amp;lt;iter&amp;gt;.pth&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trains using the base config with a batch size of 8 (the default).&lt;/span&gt;
python train.py --config=yolact_base_config

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trains yolact_base_config with a batch_size of 5. For the 550px models, 1 batch takes up around 1.5 gigs of VRAM, so specify accordingly.&lt;/span&gt;
python train.py --config=yolact_base_config --batch_size=5

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Resume training yolact_base with a specific weight file and start from the iteration specified in the weight file's name.&lt;/span&gt;
python train.py --config=yolact_base_config --resume=weights/yolact_base_10_32100.pth --start_iter=-1

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Use the help option to see a description of all available command line arguments&lt;/span&gt;
python train.py --help&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-multi-gpu-support" class="anchor" aria-hidden="true" href="#multi-gpu-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-GPU Support&lt;/h2&gt;
&lt;p&gt;YOLACT now supports multiple GPUs seamlessly during training:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before running any of the scripts, run: &lt;code&gt;export CUDA_VISIBLE_DEVICES=[gpus]&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Where you should replace [gpus] with a comma separated list of the index of each GPU you want to use (e.g., 0,1,2,3).&lt;/li&gt;
&lt;li&gt;You should still do this if only using 1 GPU.&lt;/li&gt;
&lt;li&gt;You can check the indices of your GPUs with &lt;code&gt;nvidia-smi&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Then, simply set the batch size to &lt;code&gt;8*num_gpus&lt;/code&gt; with the training commands above. The training script will automatically scale the hyperparameters to the right values.
&lt;ul&gt;
&lt;li&gt;If you have memory to spare you can increase the batch size further, but keep it a multiple of the number of GPUs you're using.&lt;/li&gt;
&lt;li&gt;If you want to allocate the images per GPU specific for different GPUs, you can use &lt;code&gt;--batch_alloc=[alloc]&lt;/code&gt; where [alloc] is a comma seprated list containing the number of images on each GPU. This must sum to &lt;code&gt;batch_size&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-logging" class="anchor" aria-hidden="true" href="#logging"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Logging&lt;/h2&gt;
&lt;p&gt;YOLACT now logs training and validation information by default. You can disable this with &lt;code&gt;--no_log&lt;/code&gt;. A guide on how to visualize these logs is coming soon, but now you can look at &lt;code&gt;LogVizualizer&lt;/code&gt; in &lt;code&gt;utils/logger.py&lt;/code&gt; for help.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pascal-sbd" class="anchor" aria-hidden="true" href="#pascal-sbd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pascal SBD&lt;/h2&gt;
&lt;p&gt;We also include a config for training on Pascal SBD annotations (for rapid experimentation or comparing with other methods). To train on Pascal SBD, proceed with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download the dataset from &lt;a href="http://home.bharathh.info/pubs/codes/SBD/download.html" rel="nofollow"&gt;here&lt;/a&gt;. It's the first link in the top "Overview" section (and the file is called &lt;code&gt;benchmark.tgz&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Extract the dataset somewhere. In the dataset there should be a folder called &lt;code&gt;dataset/img&lt;/code&gt;. Create the directory &lt;code&gt;./data/sbd&lt;/code&gt; (where &lt;code&gt;.&lt;/code&gt; is YOLACT's root) and copy &lt;code&gt;dataset/img&lt;/code&gt; to &lt;code&gt;./data/sbd/img&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Download the COCO-style annotations from &lt;a href="https://drive.google.com/open?id=1ExrRSPVctHW8Nxrn0SofU1lVhK5Wn0_S" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Extract the annotations into &lt;code&gt;./data/sbd/&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Now you can train using &lt;code&gt;--config=yolact_resnet50_pascal_config&lt;/code&gt;. Check that config to see how to extend it to other models.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will automate this all with a script soon, don't worry. Also, if you want the script I used to convert the annotations, I put it in &lt;code&gt;./scripts/convert_sbd.py&lt;/code&gt;, but you'll have to check how it works to be able to use it because I don't actually remember at this point.&lt;/p&gt;
&lt;p&gt;If you want to verify our results, you can download our &lt;code&gt;yolact_resnet50_pascal_config&lt;/code&gt; weights from &lt;a href="https://drive.google.com/open?id=1yLVwtkRtNxyl0kxeMCtPXJsXFFyc_FHe" rel="nofollow"&gt;here&lt;/a&gt;. This model should get 72.3 mask AP_50 and 56.2 mask AP_70. Note that the "all" AP isn't the same as the "vol" AP reported in others papers for pascal (they use an averages of the thresholds from &lt;code&gt;0.1 - 0.9&lt;/code&gt; in increments of &lt;code&gt;0.1&lt;/code&gt; instead of what COCO uses).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-custom-datasets" class="anchor" aria-hidden="true" href="#custom-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h2&gt;
&lt;p&gt;You can also train on your own dataset by following these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a COCO-style Object Detection JSON annotation file for your dataset. The specification for this can be found &lt;a href="http://cocodataset.org/#format-data" rel="nofollow"&gt;here&lt;/a&gt;. Note that we don't use some fields, so the following may be omitted:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;info&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;liscense&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Under &lt;code&gt;image&lt;/code&gt;: &lt;code&gt;license, flickr_url, coco_url, date_captured&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;categories&lt;/code&gt; (we use our own format for categories, see below)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create a definition for your dataset under &lt;code&gt;dataset_base&lt;/code&gt; in &lt;code&gt;data/config.py&lt;/code&gt; (see the comments in &lt;code&gt;dataset_base&lt;/code&gt; for an explanation of each field):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;my_custom_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; dataset_base.copy({
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;name&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;My Dataset&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_training_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_info&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:   &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_training_annotation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_validation_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid_info&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:   &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_validation_annotation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;has_gt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;True&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;class_names&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_1&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_2&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_3&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;...&lt;/span&gt;)
})&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;A couple things to note:
&lt;ul&gt;
&lt;li&gt;Class IDs in the annotation file should start at 1 and increase sequentially on the order of &lt;code&gt;class_names&lt;/code&gt;. If this isn't the case for your annotation file (like in COCO), see the field &lt;code&gt;label_map&lt;/code&gt; in &lt;code&gt;dataset_base&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you do not want to create a validation split, use the same image path and annotations file for validation. By default (see &lt;code&gt;python train.py --help&lt;/code&gt;), &lt;code&gt;train.py&lt;/code&gt; will output validation mAP for the first 5000 images in the dataset every 2 epochs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Finally, in &lt;code&gt;yolact_base_config&lt;/code&gt; in the same file, change the value for &lt;code&gt;'dataset'&lt;/code&gt; to &lt;code&gt;'my_custom_dataset'&lt;/code&gt; or whatever you named the config object above. Then you can use any of the training commands in the previous section.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-creating-a-custom-dataset-from-scratch" class="anchor" aria-hidden="true" href="#creating-a-custom-dataset-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Creating a Custom Dataset from Scratch&lt;/h4&gt;
&lt;p&gt;See &lt;a href="https://github.com/dbolya/yolact/issues/70#issuecomment-504283008"&gt;this nice post by @Amit12690&lt;/a&gt; for tips on how to annotate a custom dataset and prepare it for use with YOLACT.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;p&gt;If you use YOLACT or this code base in your work, please cite&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{bolya-iccv2019,
  author    = {Daniel Bolya and Chong Zhou and Fanyi Xiao and Yong Jae Lee},
  title     = {YOLACT: {Real-time} Instance Segmentation},
  booktitle = {ICCV},
  year      = {2019},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h1&gt;
&lt;p&gt;For questions about our paper or code, please contact &lt;a href="mailto:dbolya@ucdavis.edu"&gt;Daniel Bolya&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dbolya</author><guid isPermaLink="false">https://github.com/dbolya/yolact</guid><pubDate>Fri, 29 Nov 2019 00:21:00 GMT</pubDate></item><item><title>dagster-io/dagster #22 in Python, This month</title><link>https://github.com/dagster-io/dagster</link><description>&lt;p&gt;&lt;i&gt;A Python library for building data applications: ETL, ML, Data Pipelines, and more.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/609349/57987382-7e294500-7a35-11e9-9c6a-f73e0f1d3a1c.png"&gt;&lt;img src="https://user-images.githubusercontent.com/609349/57987382-7e294500-7a35-11e9-9c6a-f73e0f1d3a1c.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;br&gt;&lt;br&gt;
&lt;a href="https://badge.fury.io/py/dagster" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/1e6441ac0fa4d3b9e968e8fef89d18a684fa183b/68747470733a2f2f62616467652e667572792e696f2f70792f646167737465722e737667" data-canonical-src="https://badge.fury.io/py/dagster.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;a href="https://coveralls.io/github/dagster-io/dagster?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0297bd3b6e7c67862d727860f76241f95b6f305a/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f646167737465722d696f2f646167737465722f62616467652e7376673f6272616e63683d6d6173746572" data-canonical-src="https://coveralls.io/repos/github/dagster-io/dagster/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://buildkite.com/dagster/dagster" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ed41dd566dc2f127467ea249a2b539ee42c7e692/68747470733a2f2f62616467652e6275696c646b6974652e636f6d2f38383835343562656162383239653431653564373330336462313535323561326263336230663065333361373237353961632e7376673f6272616e63683d6d6173746572" data-canonical-src="https://badge.buildkite.com/888545beab829e41e5d7303db15525a2bc3b0f0e33a72759ac.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://dagster.readthedocs.io/en/master/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e81711b6e8b9cbc5c0509a009136cfa7488aa786/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f646167737465722f62616467652f3f76657273696f6e3d6d6173746572" data-canonical-src="https://readthedocs.org/projects/dagster/badge/?version=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Dagster is a system for building modern data applications.&lt;/p&gt;
&lt;p&gt;Combining an elegant programming model and beautiful tools, Dagster allows infrastructure engineers, data engineers, and data scientists to seamlessly collaborate to process and produce the trusted, reliable data needed in today's world.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-install" class="anchor" aria-hidden="true" href="#install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install&lt;/h3&gt;
&lt;p&gt;To get started:
&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;code&gt;pip install dagster dagit&lt;/code&gt;
&lt;/p&gt;
&lt;br&gt;
This installs two modules:
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;dagster&lt;/strong&gt; | The core programming model and abstraction stack; stateless, single-node,
single-process and multi-process execution engines; and a CLI tool for driving those engines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dagit&lt;/strong&gt; | A UI and rich development environment for Dagster, including a DAG browser, a type-aware config editor, and a streaming execution interface.
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-learn" class="anchor" aria-hidden="true" href="#learn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learn&lt;/h3&gt;
&lt;p&gt;Next, jump right into our &lt;a href="https://dagster.readthedocs.io/en/stable/sections/learn/tutorial/index.html" rel="nofollow"&gt;tutorial&lt;/a&gt;, or read our &lt;a href="https://dagster.readthedocs.io" rel="nofollow"&gt;complete documentation&lt;/a&gt;. If you're actively using Dagster or have questions on getting started, we'd love to hear from you:&lt;/p&gt;
&lt;br&gt;
&lt;p align="center"&gt;
&lt;a href="https://join.slack.com/t/dagster/shared_invite/enQtNjEyNjkzNTA2OTkzLTI0MzdlNjU0ODVhZjQyOTMyMGM1ZDUwZDQ1YjJmYjI3YzExZGViMDI1ZDlkNTY5OThmYWVlOWM1MWVjN2I3NjU" rel="nofollow"&gt;&lt;img src="https://user-images.githubusercontent.com/609349/63558739-f60a7e00-c502-11e9-8434-c8a95b03ce62.png" width="160px;" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h3&gt;
&lt;p&gt;For details on contributing or running the project for development, check out our &lt;a href="https://dagster.readthedocs.io/en/stable/sections/community/contributing.html" rel="nofollow"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-integrations" class="anchor" aria-hidden="true" href="#integrations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Integrations&lt;/h1&gt;
&lt;p&gt;Dagster works with the tools and systems that you're already using with your data, including:&lt;/p&gt;
&lt;table&gt;
	&lt;thead&gt;
		&lt;tr align="center"&gt;
			&lt;td colspan="2"&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/td&gt;
			&lt;td&gt;&lt;b&gt;Dagster Library&lt;/b&gt;&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/thead&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
			&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/609349/57987547-a7e36b80-7a37-11e9-95ae-4c4de2618e87.png"&gt;&lt;img src="https://user-images.githubusercontent.com/609349/57987547-a7e36b80-7a37-11e9-95ae-4c4de2618e87.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt; &lt;b&gt;Apache Airflow&lt;/b&gt;&lt;/td&gt;
			&lt;td&gt;&lt;a href="https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-airflow"&gt;dagster-airflow&lt;/a&gt;&lt;br&gt;Allows Dagster pipelines to be scheduled and executed, either containerized or uncontainerized, as &lt;a href="https://github.com/apache/airflow"&gt;Apache Airflow DAGs&lt;/a&gt;.&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/609349/57987976-5ccc5700-7a3d-11e9-9fa5-1a51299b1ccb.png"&gt;&lt;img src="https://user-images.githubusercontent.com/609349/57987976-5ccc5700-7a3d-11e9-9fa5-1a51299b1ccb.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt; &lt;b&gt;Apache Spark&lt;/b&gt;&lt;/td&gt;
			&lt;td&gt;&lt;a href="https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-spark"&gt;dagster-spark&lt;/a&gt; · &lt;a href="https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-pyspark"&gt;dagster-pyspark&lt;/a&gt;
			&lt;br&gt;Libraries for interacting with Apache Spark and Pyspark.
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/609349/58348728-48f66b80-7e16-11e9-9e9f-1a0fea9a49b4.png"&gt;&lt;img src="https://user-images.githubusercontent.com/609349/58348728-48f66b80-7e16-11e9-9e9f-1a0fea9a49b4.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt; &lt;b&gt;Dask&lt;/b&gt;&lt;/td&gt;
			&lt;td&gt;&lt;a href="https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-dask"&gt;dagster-dask&lt;/a&gt;
			&lt;br&gt;Provides a Dagster integration with Dask / Dask.Distributed.
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/609349/58349731-f36f8e00-7e18-11e9-8a2e-86e086caab66.png"&gt;&lt;img src="https://user-images.githubusercontent.com/609349/58349731-f36f8e00-7e18-11e9-8a2e-86e086caab66.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt; &lt;b&gt;DataDog&lt;/b&gt;&lt;/td&gt;
			&lt;td&gt;&lt;a href="https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-datadog"&gt;dagster-datadog&lt;/a&gt;
			&lt;br&gt;Provides a Dagster resource for publishing metrics to DataDog.
			&lt;/td&gt;
		&lt;/tr&gt;
        
		&lt;tr&gt;
			&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/609349/57987809-bf245800-7a3b-11e9-8905-494ed99d0852.png"&gt;&lt;img src="https://user-images.githubusercontent.com/609349/57987809-bf245800-7a3b-11e9-8905-494ed99d0852.png" style="max-width:100%;"&gt;&lt;/a&gt;
			 /  &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/609349/57987827-fa268b80-7a3b-11e9-8a18-b675d76c19aa.png"&gt;&lt;img src="https://user-images.githubusercontent.com/609349/57987827-fa268b80-7a3b-11e9-8a18-b675d76c19aa.png" style="max-width:100%;"&gt;&lt;/a&gt;
			&lt;/td&gt;
			&lt;td&gt; &lt;b&gt;Jupyter / Papermill&lt;/b&gt;&lt;/td&gt;
			&lt;td&gt;&lt;a href="https://github.com/dagster-io/dagster/tree/master/python_modules/dagstermill"&gt;dagstermill&lt;/a&gt;&lt;br&gt;Built on the &lt;a href="https://github.com/nteract/papermill"&gt;papermill library&lt;/a&gt;, dagstermill is meant for integrating productionized Jupyter notebooks into dagster pipelines.&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/609349/57988016-f431aa00-7a3d-11e9-8cb6-1309d4246b27.png"&gt;&lt;img src="https://user-images.githubusercontent.com/609349/57988016-f431aa00-7a3d-11e9-8cb6-1309d4246b27.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt; &lt;b&gt;PagerDuty&lt;/b&gt;&lt;/td&gt;
			&lt;td&gt;&lt;a href="https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-pagerduty"&gt;dagster-pagerduty&lt;/a&gt;
			&lt;br&gt;A library for creating PagerDuty alerts from Dagster workflows.
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/609349/58349397-fcac2b00-7e17-11e9-900c-9ab8cf7cb64a.png"&gt;&lt;img src="https://user-images.githubusercontent.com/609349/58349397-fcac2b00-7e17-11e9-900c-9ab8cf7cb64a.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
			&lt;td&gt; &lt;b&gt;Snowflake&lt;/b&gt;&lt;/td&gt;
			&lt;td&gt;&lt;a href="https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-snowflake"&gt;dagster-snowflake&lt;/a&gt;
			&lt;br&gt;A library for interacting with the Snowflake Data Warehouse.
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td colspan="2" align="center"&gt;&lt;b&gt;Cloud Providers&lt;/b&gt;&lt;/td&gt;
			&lt;td&gt;&lt;b&gt;&lt;/b&gt;&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/609349/57987557-c2b5e000-7a37-11e9-9310-c274481a4682.png"&gt;&lt;img src="https://user-images.githubusercontent.com/609349/57987557-c2b5e000-7a37-11e9-9310-c274481a4682.png" style="max-width:100%;"&gt;&lt;/a&gt; &lt;/td&gt;
			&lt;td&gt;&lt;b&gt;AWS&lt;/b&gt;&lt;/td&gt;
			&lt;td&gt;&lt;a href="https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-aws"&gt;dagster-aws&lt;/a&gt;
			&lt;br&gt;A library for interacting with Amazon Web Services. Provides integrations with S3, EMR, and (coming soon!) Redshift.
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/609349/57987566-f98bf600-7a37-11e9-81fa-b8ca1ea6cc1e.png"&gt;&lt;img src="https://user-images.githubusercontent.com/609349/57987566-f98bf600-7a37-11e9-81fa-b8ca1ea6cc1e.png" style="max-width:100%;"&gt;&lt;/a&gt; &lt;/td&gt;
			&lt;td&gt;&lt;b&gt;GCP&lt;/b&gt;&lt;/td&gt;
			&lt;td&gt;&lt;a href="https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-gcp"&gt;dagster-gcp&lt;/a&gt;
			&lt;br&gt;A library for interacting with Google Cloud Platform. Provides integrations with BigQuery and Cloud Dataproc.
			&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This list is growing as we are actively building more integrations, and we welcome contributions!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-example-projects" class="anchor" aria-hidden="true" href="#example-projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example Projects&lt;/h1&gt;
&lt;p&gt;Several example projects are provided under the examples folder demonstrating how to use Dagster, including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/airline_demo"&gt;&lt;strong&gt;examples/airline-demo&lt;/strong&gt;&lt;/a&gt;: A substantial demo project illustrating how these tools can be used together to manage a realistic data pipeline.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/event_pipeline_demo"&gt;&lt;strong&gt;examples/event-pipeline-demo&lt;/strong&gt;&lt;/a&gt;: An example illustrating a typical web event processing pipeline with S3, Scala Spark, and Snowflake.&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dagster-io</author><guid isPermaLink="false">https://github.com/dagster-io/dagster</guid><pubDate>Fri, 29 Nov 2019 00:22:00 GMT</pubDate></item><item><title>zhaoolee/ChromeAppHeroes #23 in Python, This month</title><link>https://github.com/zhaoolee/ChromeAppHeroes</link><description>&lt;p&gt;&lt;i&gt;🌈谷粒-Chrome插件英雄榜, 为优秀的Chrome插件写一本中文说明书, 让Chrome插件英雄们造福人类~  ChromePluginHeroes, Write a Chinese manual for the excellent Chrome plugin, let the Chrome plugin heroes benefit the human~&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9ac21b8aea054eb48fc404fd429638bf.jpeg"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9ac21b8aea054eb48fc404fd429638bf.jpeg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/996icu/996.ICU/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/41215df7ff78cefe41536bf897fe1c7e55b10bd2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d416e74692532303939362d626c75652e737667" alt="LICENSE" data-canonical-src="https://img.shields.io/badge/license-Anti%20996-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://996.icu" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/13ac320a9a774e316fe72ffb1eaacf09b01b59a3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c696e6b2d3939362e6963752d7265642e737667" alt="996.icu" data-canonical-src="https://img.shields.io/badge/link-996.icu-red.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1f62c412c50e5397395878c4da31205080db55ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265"&gt;&lt;img src="https://camo.githubusercontent.com/1f62c412c50e5397395878c4da31205080db55ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265" alt="https://img.shields.io/github/issues/zhaoolee/ChromeAppHeroes.svg?style=popout-square" data-canonical-src="https://img.shields.io/github/issues/zhaoolee/ChromeAppHeroes.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/eae70f04ac75459320f0ec7397f12bded49476bd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265"&gt;&lt;img src="https://camo.githubusercontent.com/eae70f04ac75459320f0ec7397f12bded49476bd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265" alt="https://img.shields.io/github/stars/zhaoolee/ChromeAppHeroes.svg?style=popout-square" data-canonical-src="https://img.shields.io/github/stars/zhaoolee/ChromeAppHeroes.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-谷粒-chrome插件英雄榜" class="anchor" aria-hidden="true" href="#谷粒-chrome插件英雄榜"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;谷粒-Chrome插件英雄榜&lt;/h1&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="rainbow" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f308.png"&gt;🌈&lt;/g-emoji&gt;谷粒-Chrome插件英雄榜, 为优秀的Chrome插件写一本中文说明书, 让Chrome插件英雄们造福人类~
ChromeAppHeroes, Write a Chinese manual for the excellent Chrome plugin, let the Chrome plugin heroes benefit the human~&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/5ecd2856f287477c89c20efb7de11a9b.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/5ecd2856f287477c89c20efb7de11a9b.png" alt="谷粒VI设计.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;感谢&lt;a href="https://github.com/LuoJiangYong"&gt;老罗巴扎嘿&lt;/a&gt;为本项目设计的新的Logo | &lt;a href="https://zhaoolee.gitbooks.io/chrome/content/gu-li-qu-yi.html" rel="nofollow"&gt;谷粒文化(老罗巴扎嘿语录)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-相关项目推广-用chrome学编程" class="anchor" aria-hidden="true" href="#相关项目推广-用chrome学编程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;相关项目推广: &lt;a href="https://github.com/zhaoolee/ProgrammingWithChrome"&gt;用Chrome学编程&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;《用Chrome学编程(如何用Chrome优雅装B)》, 用Gif图展示Chrome的骚操作, 充分挖掘Chrome的编程潜力! &lt;a href="https://github.com/zhaoolee/ProgrammingWithChrome"&gt;https://github.com/zhaoolee/ProgrammingWithChrome&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-项目新增cn服务器" class="anchor" aria-hidden="true" href="#项目新增cn服务器"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目新增CN服务器&lt;/h2&gt;
&lt;p&gt;本项目使用了大量Gif图片, 而且github在国内的访问速度非常不稳定,导致文章页面打开稍慢, 为了解决大陆用户访问项目速度慢的问题, zhaoolee在阿里云买了一台5M的VPS, 已将所有文章链接替换到v2fy.com域名下, 访问速度会非常快, 而且图片支持懒加载, 可以节省下载gif图的流量,入口为&lt;a href="https://www.v2fy.com/ChromeAppHeroes/" rel="nofollow"&gt;https://www.v2fy.com/ChromeAppHeroes/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;海外用户:&lt;a href="https://zhaoolee.com/ChromeAppHeroes/" rel="nofollow"&gt;备用入口&lt;/a&gt;依然保留&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-目录点击以下标题-可以进入文章页" class="anchor" aria-hidden="true" href="#目录点击以下标题-可以进入文章页"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录(点击以下标题, 可以进入文章页~)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/061-image-assistant/" rel="nofollow"&gt;061《ImageAssistant》图片助手批量图片下载器&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/060_tabagotchi/" rel="nofollow"&gt;060《Tabagotchi》为减缓全球变暖做出贡献&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/059_page_speed_insight_and_check_list/" rel="nofollow"&gt;059《PageSpeed Insight and CheckList》为网页优化提供建议和量化指标&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/058_ip_address/" rel="nofollow"&gt;058《IP-Address》快速查看当前设备IP&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/057_webp_save_as_png/" rel="nofollow"&gt;057《图片另存为JPG/PNG/WebP》让WebP图片下载为PNG格式&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/056_search/" rel="nofollow"&gt;056《Search》为Chrome设置搜索引擎关键词&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/055_keylines/" rel="nofollow"&gt;055《Keylines》为网页元素添加随机描边颜色&lt;/a&gt; | &lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/055_keylines.html" rel="nofollow"&gt;备用链接&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/054_er_xiang_yi_tu_sou_tu/" rel="nofollow"&gt;054《二箱 以图搜图》让你在搜图方面随心所欲（为所欲为）&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/053_shu_biao_dian_ji_te_xiao/" rel="nofollow"&gt;053《鼠标点击特效 (๑•́ ∀ •̀๑)》为鼠标点击添加有趣的特效&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/052_site_palette/" rel="nofollow"&gt;052《Site Palette》自动提取网站配色&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/051_custom_cursor_for_chrome/" rel="nofollow"&gt;051《Custom Cursor for Chrome™》为Chrome换上可爱初音光标&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/050_google_results_previewer/" rel="nofollow"&gt;050《Google Results Previewer》无点击查看谷歌搜索结果&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/049_web_server_for_chrome/" rel="nofollow"&gt;049《Web Server for Chrome》搭建本地Web服务器, 实现局域网共享文件夹&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/048_words_discoverer/" rel="nofollow"&gt;048《Words Discoverer》高亮标注单词,提升你的词汇量&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/047_go_to_tab/" rel="nofollow"&gt;047《Go to Tab》快速跳转到打开的网页&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/046_whatfont/" rel="nofollow"&gt;046《WhatFont》字体爱好者优雅查看网页字体&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/045_restlet_client/" rel="nofollow"&gt;045《Restlet Client》优秀的Api测试工具&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/044_gu_ge_fang_wen_zhu_shou/" rel="nofollow"&gt;044《谷歌访问助手》访问Chrome商店 Gmail 谷歌搜索&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/043_dream_afar_new_tab/" rel="nofollow"&gt;043《Dream Afar New Tab》探索世界的新方式&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/042_edge/" rel="nofollow"&gt;042 在Edge中安装Chrome扩展程序&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/041_copy_all_urls/" rel="nofollow"&gt;041《Copy All Urls》优雅地保存-开启多个标签页&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/040_gitzip_for_github/" rel="nofollow"&gt;040《GitZip for github》从Github批量下载表情包&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/039_simplify_gmail/" rel="nofollow"&gt;039《Simplify Gmail》让网页版Gmail更清爽&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/038_alexa_traffic_rank/" rel="nofollow"&gt;038《Alexa Traffic Rank》一键查看网站全球排名&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/037_saladict/" rel="nofollow"&gt;037《Saladict》谷歌!有道!我全都要! 聚合词典, 并行翻译&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/036_screen_shader/" rel="nofollow"&gt;036《Screen Shader》把网页调成暖色，你的眼睛会感谢你&lt;g-emoji class="g-emoji" alias="pray" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64f.png"&gt;🙏&lt;/g-emoji&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/035_print_friendly_and_pdf/" rel="nofollow"&gt;035《Print Friendly &amp;amp; PDF》让你拥有最佳的打印阅读体验&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/034_astro_bot/" rel="nofollow"&gt;034《Astro Bot》用新标签页刷编程题&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/033_yi_ye/" rel="nofollow"&gt;033《一叶》在任意网页开启实时弹幕 聊天窗口 留言板&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/032_smallpdf/" rel="nofollow"&gt;032《Smallpdf》简单好用的线上PDF工具&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/031_onetab/" rel="nofollow"&gt;031《OneTab》把多个Tab转换为一个列表&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/030_jue_jin/" rel="nofollow"&gt;030《掘金》相信优质技术内容的力量&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/029_simread/" rel="nofollow"&gt;029 《SimpRead》为任意网页开启阅读模式&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/028_adblock/" rel="nofollow"&gt;028《AdBlock》Adblock自定义屏蔽简书广告&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/027_text/" rel="nofollow"&gt;027《Text》来自Chrome实验室的跨平台记事本&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/026_quickey_launcher/" rel="nofollow"&gt;026《Quickey Launcher》打开网站只需一键&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/025_console/" rel="nofollow"&gt;025《Console》Chrome自带好用的计算器&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/024_dark_reader/" rel="nofollow"&gt;024《Dark Reader》为任意网站启用夜间模式&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/023_fireshot/" rel="nofollow"&gt;023《FireShot》一键滚动截屏整个网页&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/022kuo_zhan_guan_li_qi/" rel="nofollow"&gt;022《扩展管理器》管理你的Chrome扩展&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/021_bi_li_bi_li_zhu_shou/" rel="nofollow"&gt;021《哔哩哔哩助手》助你快速成为B站老司机&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/020_boxel_rebound/" rel="nofollow"&gt;020《Boxel Rebound》“嗨到中毒”的弹跳小方块(附自制赛道分享方法)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/019_mega/" rel="nofollow"&gt;019《MEGA》网盘可以良心到什么程度? 试试MEGA吧!&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/018_enhanced_github/" rel="nofollow"&gt;018《Enhanced Github》从“冰柜”到“冰棍儿”,下载Github单个文件&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/017_xin_lang_wei_bo_tu_chuang/" rel="nofollow"&gt;017《新浪微博图床》本地Markdown编写更流畅, 新浪微博图床来帮忙&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/016_jie_chu_b_zhan_qu_yu_xian_zhi/" rel="nofollow"&gt;016《解除B站区域限制》查看进击的巨人第三季&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/015_xpath_helper/" rel="nofollow"&gt;015 《XPath Helper》完成Bing每日壁纸的小爬虫&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/014_chao_ji_ma_li_ao_you_xi/" rel="nofollow"&gt;014《超级马里奥游戏》Chrome变身小霸王&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/013_quick_qr/" rel="nofollow"&gt;013《Quick QR》用二维码实现云粘贴&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/012_ourstickys/" rel="nofollow"&gt;012《OurStickys》Chrome特色网页便签纸&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/011_whatruns/" rel="nofollow"&gt;011 《whatruns》一键分析网站技术栈&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/010_speedtest/" rel="nofollow"&gt;010《speedtest》网络测速插件speedtest&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/009_vimium/" rel="nofollow"&gt;009《vimium》Chrome与vim双神器融合&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/008_chrome_cleaner_pro/" rel="nofollow"&gt;008《Chrome Cleaner Pro》为Chrome加速&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/007_loom/" rel="nofollow"&gt;007《loom》 Chrome翻录网页视频神器&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/006_similarsites/" rel="nofollow"&gt;006《SimilarSites》 一键查找姊妹网站 SimilarSites&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/005_video_speed_controller/" rel="nofollow"&gt;005《Video Speed Controller》 刷课（刷剧）神器！给网页视频加个速(最快可达16倍!)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/004_tampermonkey/" rel="nofollow"&gt;004《Tampermonkey》 油猴子! 给浏览器开个挂&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/003_secure_shell_app/" rel="nofollow"&gt;003《Secure Shell App》 Chrome中开启ssh一种什么体验&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/002_chrono/" rel="nofollow"&gt;002《chrono》 让Chrome下载资源更容易&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.v2fy.com/p/001_markdown_here/" rel="nofollow"&gt;001《markdown-here》 Markdown一键转换到"富文本格式"&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-开源插件推广作者自荐" class="anchor" aria-hidden="true" href="#开源插件推广作者自荐"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;开源插件推广(作者自荐)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;作者主页&lt;/th&gt;
&lt;th&gt;开源信息&lt;/th&gt;
&lt;th&gt;简介&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/the-fucking-github/agajobpbaphiohkbkjigcalebbfmofdo" rel="nofollow"&gt;The Fucking Github&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/lvxianchao"&gt;lvxianchao&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/lvxianchao/the-fucking-github"&gt;Github仓库地址&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;很方便地查看、整理、搜索你已经 Star 过的项目和搜索 Github 上的项目。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/hitup/eiokaohkigpbonodjcbjpecbnccijkjb" rel="nofollow"&gt;HitUP&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wonderbeyond"&gt;wonderbeyond&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wonderbeyond/HitUP"&gt;Github仓库地址&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;利用 New Tab “空白页” 助您保持对流行技术趋势的跟进，附带其它福利。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/gitako-github-file-tree/giljefjcheohhamkjphiebfjnlphnokk" rel="nofollow"&gt;Gitako - Github file tree&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/EnixCoda"&gt;EnixCoda&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/EnixCoda/Gitako"&gt;Github仓库地址&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;功能上类似于大名鼎鼎的 Octotree ，但是用了更现代化的前端工具，性能好很多。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/githuber/janmcneaglgklfljjcpihkkomeghljnf" rel="nofollow"&gt;GITHUBER&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/zhuowenli"&gt;zhuowenli&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/zhuowenli/githuber"&gt;Github仓库地址&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;这是一个帮助 GitHub 开发者每日发现优质内容的 Chrome 主页拓展。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/60c92f0de3d44bb7a612d08e2e1f3d18.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/60c92f0de3d44bb7a612d08e2e1f3d18.png" alt="造福人类.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-咦微信打赏" class="anchor" aria-hidden="true" href="#咦微信打赏"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;咦?(微信打赏)&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c4fdea49e11241e392d6bcaa33855897.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c4fdea49e11241e392d6bcaa33855897.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;赞赏金额&lt;/th&gt;
&lt;th&gt;赞赏者(微信名)&lt;/th&gt;
&lt;th&gt;赞赏时间&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;20.00&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年8月2日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20.00&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年7月11日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12.34&lt;/td&gt;
&lt;td&gt;张明辉&lt;/td&gt;
&lt;td&gt;2019年8月20日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;六小登登&lt;/td&gt;
&lt;td&gt;2019年9月5日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;云淡风晴&lt;/td&gt;
&lt;td&gt;2019年7月24日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;金三古月&lt;/td&gt;
&lt;td&gt;2019年6月2日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;Azuno&lt;/td&gt;
&lt;td&gt;2019年6月1日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;邦妥&lt;/td&gt;
&lt;td&gt;2019年5月22日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;enjoy life&lt;/td&gt;
&lt;td&gt;2019年9月20日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;L__hoo原&lt;/td&gt;
&lt;td&gt;2019年9月20日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;梦想旅程(公众号:苏生不惑)&lt;/td&gt;
&lt;td&gt;2019年9月14日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;1111&lt;/td&gt;
&lt;td&gt;2019年7月27日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;那都不重要&lt;/td&gt;
&lt;td&gt;2019年5月19日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;Lismg&lt;/td&gt;
&lt;td&gt;2019年6月5日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;small胖&lt;/td&gt;
&lt;td&gt;2019年7月9日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.00&lt;/td&gt;
&lt;td&gt;良辰美&lt;/td&gt;
&lt;td&gt;2019年7月20日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.00&lt;/td&gt;
&lt;td&gt;@Coolstar&lt;/td&gt;
&lt;td&gt;2019年7月6日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年9月26日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;夏天的小虫子&lt;/td&gt;
&lt;td&gt;2019年9月23日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年7月26日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;2019年7月12日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年6月13日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;Walter Wu&lt;/td&gt;
&lt;td&gt;2019年6月1日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;Joseph&lt;/td&gt;
&lt;td&gt;2019年4月24日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年4月12日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;于云鹏Edward&lt;/td&gt;
&lt;td&gt;2019年4月12日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;黄金星&lt;/td&gt;
&lt;td&gt;2019年4月11日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;Cloud 9&lt;/td&gt;
&lt;td&gt;2019年4月5日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.20&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年7月25日&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;感谢以上赞赏者对本开源项目的支持[手动滑稽]&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-061imageassistant图片助手批量图片下载器" class="anchor" aria-hidden="true" href="#061imageassistant图片助手批量图片下载器"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/061-image-assistant/" rel="nofollow"&gt;061《ImageAssistant》图片助手批量图片下载器&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/69475211-6cba5e80-0e05-11ea-8364-2fdaf073cdb0.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/69475211-6cba5e80-0e05-11ea-8364-2fdaf073cdb0.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;《ImageAssistant》图片助手批量图片下载器,在提取网页图片的方面,功能非常全面, 能提取绝大多数图片网站的资源, 如果你经常为无法提取网页图片资源发愁, 相信这款扩展程序能为你带来惊喜&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-060tabagotchi为减缓全球变暖做出贡献" class="anchor" aria-hidden="true" href="#060tabagotchi为减缓全球变暖做出贡献"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/060_tabagotchi/" rel="nofollow"&gt;060《Tabagotchi》为减缓全球变暖做出贡献&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63478935-7b1f7400-c4be-11e9-8679-5f4a6a56c89c.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63478935-7b1f7400-c4be-11e9-8679-5f4a6a56c89c.gif" alt="tabagotchi" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tabagotchi扩展以一种有趣的方式, 提醒我们减少标签页数量, 减少了计算机产生的热量, 为阻止全球变暖做出了贡献~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-059pagespeed-insight-and-checklist为网页优化提供建议和量化指标" class="anchor" aria-hidden="true" href="#059pagespeed-insight-and-checklist为网页优化提供建议和量化指标"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/059_page_speed_insight_and_check_list/" rel="nofollow"&gt;059《PageSpeed Insight and CheckList》为网页优化提供建议和量化指标&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63309328-f818e500-c328-11e9-8f1a-68fed13a4015.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63309328-f818e500-c328-11e9-8f1a-68fed13a4015.gif" alt="pag_speed" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63309327-f7804e80-c328-11e9-8eab-9055db8a5d2c.png"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63309327-f7804e80-c328-11e9-8eab-9055db8a5d2c.png" alt="001" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PageSpeed Insight and CheckList 和 Google Page Speed 结合使用, 能够为网页质量评分,量化网页优化的效果,也为优化网页指明了方向,对前端工程师而言,是非常重要的工具&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-058ip-address快速查看当前设备ip" class="anchor" aria-hidden="true" href="#058ip-address快速查看当前设备ip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/058_ip_address/" rel="nofollow"&gt;058《IP-Address》快速查看当前设备IP&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63222725-ee369b00-c1dd-11e9-986e-cbc002168db8.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63222725-ee369b00-c1dd-11e9-986e-cbc002168db8.gif" alt="ip_address" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;获取当前设备的IP地址,对于开发者而言,是一个经常遇到的问题,而《IP-Address》这款简洁小巧的软件, 能满足我们的需求&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-057图片另存为jpgpngwebp让webp图片下载为png格式" class="anchor" aria-hidden="true" href="#057图片另存为jpgpngwebp让webp图片下载为png格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/057_webp_save_as_png/" rel="nofollow"&gt;057《图片另存为JPG/PNG/WebP》让WebP图片下载为PNG格式&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63221240-ce48ac80-c1c8-11e9-9860-376fedc0845e.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63221240-ce48ac80-c1c8-11e9-9860-376fedc0845e.gif" alt="save_as_png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;WebP是非常先进的格式, 但由于Photoshop这类元老级图像编辑软件不支持, 我们只能将图片为png格式,再进行编辑, 先进技术改变世界, 需要一个过程, 而在过程中提供一个折中的方案(把WebP装换为png, 再将png图片装换为WebP), 也是一件有价值的事~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-056search为chrome设置搜索引擎关键词" class="anchor" aria-hidden="true" href="#056search为chrome设置搜索引擎关键词"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/056_search/" rel="nofollow"&gt;056《Search》为Chrome设置搜索引擎关键词&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/62503773-3c37c000-b828-11e9-9605-4ecce76830ec.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/62503773-3c37c000-b828-11e9-9605-4ecce76830ec.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在早期的网址导航主页上, 可以通过点击选择不同的搜索引擎进行搜索(数量有限, 而且不支持自定义), 而Chrome搜索更极客一些, 通过&lt;strong&gt;自定义关键词加空格&lt;/strong&gt;的方法, 在搜索引擎之间自由切换, 是一种兼具扩展性与易用性的做法&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-055keylines为网页元素添加随机描边颜色-" class="anchor" aria-hidden="true" href="#055keylines为网页元素添加随机描边颜色-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/055_keylines/" rel="nofollow"&gt;055《Keylines》为网页元素添加随机描边颜色 &lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61917657-dbcf9580-af80-11e9-87d3-528609ab85b0.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61917657-dbcf9580-af80-11e9-87d3-528609ab85b0.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Keylines的实现原理非常简单(为网页dom元素添加了outline属性), 但展示的效果却非常惊艳, 这应该归功于Keylines作者优秀的想法, 很多时候, 优秀的软件并不一定使用了很难掌握的技术, 而是包含了作者优秀的想法~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-054二箱以图搜图让你在搜图方面随心所欲为所欲为" class="anchor" aria-hidden="true" href="#054二箱以图搜图让你在搜图方面随心所欲为所欲为"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/054_er_xiang_yi_tu_sou_tu/" rel="nofollow"&gt;054《二箱+以图搜图》让你在搜图方面随心所欲（为所欲为）&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61757068-93ce3880-adf1-11e9-8903-ebf313fb6098.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61757068-93ce3880-adf1-11e9-8903-ebf313fb6098.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;《二箱 以图搜图》是一款简单实用的搜图小工具，如果你是一名设计师, 可以帮你快速查找他人设计作品中所用的素材来源, 提升你的工作效率~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-053鼠标点击特效-๑́--̀๑为鼠标点击添加有趣的特效" class="anchor" aria-hidden="true" href="#053鼠标点击特效-๑́--̀๑为鼠标点击添加有趣的特效"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/053_shu_biao_dian_ji_te_xiao/" rel="nofollow"&gt;053《鼠标点击特效 (๑•́ ∀ •̀๑)》为鼠标点击添加有趣的特效&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61600040-04921b00-ac61-11e9-8446-533752d71de1.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61600040-04921b00-ac61-11e9-8446-533752d71de1.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;《鼠标点击特效 (๑•́ ∀ •̀๑)》是一款为鼠标点击添加有趣的特效的扩展程序,虽然没啥实际用途,但很好玩, 录制一些有趣的网页小程序时, 会非常出彩~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-052site-palette自动提取网站配色" class="anchor" aria-hidden="true" href="#052site-palette自动提取网站配色"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/052_site_palette/" rel="nofollow"&gt;052《Site Palette》自动提取网站配色&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61169390-2f101400-a58f-11e9-8769-4d62b7b64f37.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61169390-2f101400-a58f-11e9-8769-4d62b7b64f37.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Site Palette使用简单, 功能实用, 没有广告, 是典型的小而美的扩展程序, 这类扩展程序越多, Chrome的用户体验也就越好~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-051custom-cursor-for-chrome为chrome换上可爱初音光标" class="anchor" aria-hidden="true" href="#051custom-cursor-for-chrome为chrome换上可爱初音光标"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/051_custom_cursor_for_chrome/" rel="nofollow"&gt;051《Custom Cursor for Chrome™》为Chrome换上可爱初音光标&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61166967-d0846f00-a569-11e9-9141-15cef4983098.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61166967-d0846f00-a569-11e9-9141-15cef4983098.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;早期的QQ空间和个人博客, 我们会给页面加各种各样的装饰, 连鼠标指针也要定制一下, 当时感觉乐趣无穷, 后面就失去了兴趣, 对于个人博客, 感觉越简洁越好, 于是就有了Next这些大量留白的博客主题,但我感觉在Next这类主题中加一些定制化的小物件也是不错的, 在简洁与花哨之间找到平衡, 不正是生活的乐趣之源么~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-050google-results-previewer无点击查看谷歌搜索结果" class="anchor" aria-hidden="true" href="#050google-results-previewer无点击查看谷歌搜索结果"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/050_google_results_previewer/" rel="nofollow"&gt;050《Google Results Previewer》无点击查看谷歌搜索结果&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/9219a092f0f4eb1c6f614c1667b316d1.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/9219a092f0f4eb1c6f614c1667b316d1.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Google Results Previewer的功能简单实用, 也没有多余的设置, 属于新手友好型工具&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-049web-server-for-chrome搭建本地web服务器-实现局域网共享文件夹" class="anchor" aria-hidden="true" href="#049web-server-for-chrome搭建本地web服务器-实现局域网共享文件夹"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/049_web_server_for_chrome/" rel="nofollow"&gt;049《Web Server for Chrome》搭建本地Web服务器, 实现局域网共享文件夹&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/74d3eb882b103e0fb1e5e5dd651c052f.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/74d3eb882b103e0fb1e5e5dd651c052f.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Web Server for Chrome可以帮我们在本地快速开启http服务,让开发和测试变得更加简单, 如果你想和同处某个局域网的小伙伴, 建立一个共享文件夹, Web Server for Chrome或许是你最简单的实现方法~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-048words-discoverer背单词新姿势提升你的词汇量" class="anchor" aria-hidden="true" href="#048words-discoverer背单词新姿势提升你的词汇量"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/048_words_discoverer/" rel="nofollow"&gt;048《Words Discoverer》背单词新姿势,提升你的词汇量&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/305439fdd84017da654e00f16aaee752.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/305439fdd84017da654e00f16aaee752.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Words Discoverer(中文译名: 单词发现者),&lt;strong&gt;可以突出显示网页上罕见的英语字典词汇和惯用语。促进英语语言学习并扩大词汇量&lt;/strong&gt;,通过自动高亮网页单词, 辅助单词记忆是一个很好的路子, 建议过一段时间,就稍微调高&lt;strong&gt;不突出显示 最常用的英语单词&lt;/strong&gt;的数量, 比如从默认的15%调整到16%,  单词发现者与沙拉查词结合使用, 真的是体验极佳~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-047go-to-tab快速跳转到打开的网页" class="anchor" aria-hidden="true" href="#047go-to-tab快速跳转到打开的网页"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/047_go_to_tab/" rel="nofollow"&gt;047《Go to Tab》快速跳转到打开的网页&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/59550928-2a623b00-8fa4-11e9-8525-8e830907463b.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/59550928-2a623b00-8fa4-11e9-8525-8e830907463b.gif" alt="2019-06-15-18 54 23" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Go to Tab对于工作期间大量打开页面, 又长时间不关机的程序员们, 是非常有帮助的&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-046whatfont字体爱好者优雅查看网页字体" class="anchor" aria-hidden="true" href="#046whatfont字体爱好者优雅查看网页字体"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/046_whatfont/" rel="nofollow"&gt;046《WhatFont》字体爱好者优雅查看网页字体&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/59549312-4529b500-8f8e-11e9-8107-004486a02258.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/59549312-4529b500-8f8e-11e9-8107-004486a02258.gif" alt="font 2019-06-15 16_04_10" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;WhatFont属于功能非常单一的小工具, 让字体爱好者优雅查看网页字体属性, 如果你对漂亮字体有一份执念, 推荐到&lt;a href="https://fonts.google.com/" rel="nofollow"&gt;https://fonts.google.com/&lt;/a&gt;, &lt;a href="https://www.myfonts.com/" rel="nofollow"&gt;https://www.myfonts.com/&lt;/a&gt;
等字体网站,找寻更多可爱的字体~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-045restlet-client优秀的api测试工具" class="anchor" aria-hidden="true" href="#045restlet-client优秀的api测试工具"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/045_restlet_client/" rel="nofollow"&gt;045《Restlet Client》优秀的Api测试工具&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/89ea1e51dab48d5a84f089adf33eb274.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/89ea1e51dab48d5a84f089adf33eb274.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Restlet Client是一款开发实用工具, 支持一键导入Postman等api测试工具的测试用例&lt;/li&gt;
&lt;li&gt;近来, Postman开始主推自己的70M左右的客户端安装包, 功能没什么改进, 体积却变得超大,而且Postman的Chrome扩展程序, 对macOS的支持不太好(每次打开, 都会弹窗报一个错)&lt;/li&gt;
&lt;li&gt;Restlet Client依然只是一个开箱即用的Chrome扩展程序, 非常适合硬盘空间有限的小伙伴使用(软件功能够用就可以了~)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-044谷歌访问助手访问chrome商店-gmail-谷歌搜索" class="anchor" aria-hidden="true" href="#044谷歌访问助手访问chrome商店-gmail-谷歌搜索"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/044_gu_ge_fang_wen_zhu_shou/" rel="nofollow"&gt;044《谷歌访问助手》访问Chrome商店 Gmail 谷歌搜索&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/deff71a536ba4027a01fe3c7a558c277.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/deff71a536ba4027a01fe3c7a558c277.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;《谷歌访问助手》可以让我们访问Chrome商店, 以及谷歌搜索, 谷歌Gmail等服务
&lt;code&gt;仅为香港地区用户提，供方便科研,外贸提供帮助,不良用户,将封锁访问IP,后果自负&lt;/code&gt;, 谷歌访问助手需要你设置主页为&lt;code&gt;https://2018.hao245.com/&lt;/code&gt;才能使用, 有百度全家桶, 360全家桶的流氓内涵~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-043dream-afar-new-tab探索世界的新方式" class="anchor" aria-hidden="true" href="#043dream-afar-new-tab探索世界的新方式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/043_dream_afar_new_tab/" rel="nofollow"&gt;043《Dream Afar New Tab》探索世界的新方式&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e40b7bec41ce4ac892578bc88a03d25c.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e40b7bec41ce4ac892578bc88a03d25c.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;《Dream Afar New Tab》的设计非常漂亮, 功能调节也非常简单, 只有两级菜单, 壁纸也非常精美, 对浏览器颜值有要求的小伙伴, 可以试一试~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-042-在edge中安装chrome扩展程序" class="anchor" aria-hidden="true" href="#042-在edge中安装chrome扩展程序"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/042_edge/" rel="nofollow"&gt;042 在Edge中安装Chrome扩展程序&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/a131b9833d20424ab93cb258ab8542e8.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/a131b9833d20424ab93cb258ab8542e8.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Edge可以安装绝大多数Chrome商店中的扩展, 但Chrome中的谷歌开发App程序, 类似&lt;a href="https://chrome.google.com/webstore/detail/secure-shell-app/pnhechapfaindjhompbnflcldabbghjo" rel="nofollow"&gt;Secure Shell App&lt;/a&gt;, 目前是无法安装的, 新版Edge使用了Chrome的Chromium内核, 可以兼容安装Chrome生态中的各种应用程序,为Edge未来的发展带来了无限可能~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-041copy-all-urls优雅地保存-开启多个标签页" class="anchor" aria-hidden="true" href="#041copy-all-urls优雅地保存-开启多个标签页"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/041_copy_all_urls/" rel="nofollow"&gt;041《Copy All Urls》优雅地保存-开启多个标签页&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/eac219ff189a4295bbf88974b045ba5b.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/eac219ff189a4295bbf88974b045ba5b.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Copy All Urls属于小而美地工具，如果你每天都需要查看几个固定的网页, Copy All Urls能帮你省很多时间~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-040gitzip-for-github从github批量下载表情包" class="anchor" aria-hidden="true" href="#040gitzip-for-github从github批量下载表情包"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/040_gitzip_for_github/" rel="nofollow"&gt;040《GitZip for github》从Github批量下载表情包&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/f5b923dc4a21437484e90859342ed366.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/f5b923dc4a21437484e90859342ed366.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;以前介绍过Github快速下载单个文件的扩展工具&lt;a href="https://zhaoolee.gitbooks.io/chrome/content/018enhanced-github300b-cong-201c-bing-gui-201d-dao-201c-bing-gun-er-201d2c-xia-zai-github-dan-ge-wen-jian.html" rel="nofollow"&gt;《Enhanced Github》&lt;/a&gt; , 《Enhanced Github》 和 《GitZip for github》 结合到一起, 就可以让我们快速下载, github任意仓库任意文件夹的优质资源了~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-039simplify-gmail让网页版gmail更清爽" class="anchor" aria-hidden="true" href="#039simplify-gmail让网页版gmail更清爽"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/039_simplify_gmail/" rel="nofollow"&gt;039《Simplify Gmail》让网页版Gmail更清爽&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c9b1aa8201c24208b0e0aedfcdbdc992.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c9b1aa8201c24208b0e0aedfcdbdc992.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;好的扩展程序就应该这样, 让人见到后耳目一新, 使用的方法却非常简单。
如果你并没有注册过Gmail邮箱, 可以尝试注册一个, Gmail是非常好用的, 拥有规范的接口, 不会随便拦截邮件, 也不会在页面铺满广告&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-038alexa-traffic-rank一键查看网站全球排名" class="anchor" aria-hidden="true" href="#038alexa-traffic-rank一键查看网站全球排名"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/038_alexa_traffic_rank/" rel="nofollow"&gt;038《Alexa Traffic Rank》一键查看网站全球排名&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fcefd45a5cc74e4c824f567535f79c5c.webp"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fcefd45a5cc74e4c824f567535f79c5c.webp" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Alexa给出的网站排名, 是目前公认最具参考价值的排名, 打开一个新站点, 查一下新站点的Alexa排名, 以及与它类似的站点, 让我们很快对新站点的定位, 有一个大致的认知~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-037saladict谷歌有道我全都要-聚合词典-并行翻译" class="anchor" aria-hidden="true" href="#037saladict谷歌有道我全都要-聚合词典-并行翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/037_saladict/" rel="nofollow"&gt;037《Saladict》谷歌!有道!我全都要! 聚合词典, 并行翻译&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/07322f3c4b13484a8a048194558cec5c.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/07322f3c4b13484a8a048194558cec5c.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;沙拉查词(Saladict)是一款非常优秀的查词扩展, 上文只是提及了它最常用的一些功能, 沙拉查词的后台管理选项非常丰富, 感兴趣的小伙伴可以慢慢探索&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-036screen-shader把屏幕调成暖色你的眼睛会感谢你" class="anchor" aria-hidden="true" href="#036screen-shader把屏幕调成暖色你的眼睛会感谢你"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/036_screen_shader/" rel="nofollow"&gt;036《Screen Shader》把屏幕调成暖色，你的眼睛会感谢你&lt;g-emoji class="g-emoji" alias="pray" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64f.png"&gt;🙏&lt;/g-emoji&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/3a94a283267047c39114694706de7293.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/3a94a283267047c39114694706de7293.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;对于长时间看电脑的办公人员, 可以尝试吧屏幕调成暖色, 开始可能会不习惯, 但后面会感觉眼睛会舒服很多, 你的眼睛也会感谢你的~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-035print-friendly--pdf让你拥有最佳的打印阅读体验" class="anchor" aria-hidden="true" href="#035print-friendly--pdf让你拥有最佳的打印阅读体验"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/035_print_friendly_and_pdf/" rel="nofollow"&gt;035《Print Friendly &amp;amp; PDF》让你拥有最佳的打印阅读体验&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/a71d2b280298482ba2408482c1537bf9.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/a71d2b280298482ba2408482c1537bf9.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;《Print Friendly &amp;amp; PDF》是一款文件打印chrome插件，会在打印之前删除垃圾广告，导航和无用浮窗从而实现页面优化，让你拥有最佳的打印阅读体验, 如果你经常需要打印网页, 可以通过《Print Friendly &amp;amp; PDF》让你的打印工作变得省时省力~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-034astro-bot用新标签页刷编程题" class="anchor" aria-hidden="true" href="#034astro-bot用新标签页刷编程题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/034_astro_bot/" rel="nofollow"&gt;034《Astro Bot》用新标签页刷编程题&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/671d39ca714f437fa1d287bfb988724e.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/671d39ca714f437fa1d287bfb988724e.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Astro Bot可以在新标签页,展示一道与程序相关的问题或相关新闻&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-033一叶在任意网页开启实时弹幕-聊天窗口-留言板" class="anchor" aria-hidden="true" href="#033一叶在任意网页开启实时弹幕-聊天窗口-留言板"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/033_yi_ye/" rel="nofollow"&gt;033《一叶》在任意网页开启实时弹幕 聊天窗口 留言板&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6a328e8eb9984f5abea5816c681b8e4e.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6a328e8eb9984f5abea5816c681b8e4e.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;一叶是一款很有想法的产品,但目前用户量还是很少, 对此,我个人也有一些想法,如果官方可以效仿pokemongo这类寻宝游戏,在各大网站的主页对应的留言板内,埋下一些有意思的彩蛋,让用户去寻宝,或许会有利于产品的推广~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-032smallpdf简单好用的线上pdf工具" class="anchor" aria-hidden="true" href="#032smallpdf简单好用的线上pdf工具"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/032_smallpdf/" rel="nofollow"&gt;032《Smallpdf》简单好用的线上PDF工具&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/2c00d25291db4750963c60e78344d4cc.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/2c00d25291db4750963c60e78344d4cc.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Smallpdf是一个非常好用的PDF工具,可以收藏起来,作为日常办公的工具, Smallpdf可以进行多份pdf在线合并, pdf在线编辑, 如果你是一个经常和PDF打交道的人, 可不要错过它~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-031onetab把多个tab转换为一个列表" class="anchor" aria-hidden="true" href="#031onetab把多个tab转换为一个列表"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/031_onetab/" rel="nofollow"&gt;031《OneTab》把多个Tab转换为一个列表&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/93781d48870742e08dc68fa17e79169e.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/93781d48870742e08dc68fa17e79169e.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;当你发现自己有太多的标签页时,单击OneTab图标,所有标签页会转换成一个列表,当你需要再次访问这些标签页时,点击OneTab图标唤出列表,点击列表恢复标签页&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-030掘金相信优质技术内容的力量" class="anchor" aria-hidden="true" href="#030掘金相信优质技术内容的力量"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/030_jue_jin/" rel="nofollow"&gt;030《掘金》相信优质技术内容的力量&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fcca47d65f2542808281c17ec379d7d9.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fcca47d65f2542808281c17ec379d7d9.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;如果你想对 程序员, 产品经理, 设计师的行业知识有所了解, 可以没事儿打开掘金插件看一看, 如果你感觉很喜欢里面的内容, 可以到掘金官网 &lt;a href="https://juejin.im/" rel="nofollow"&gt;https://juejin.im/&lt;/a&gt; 逛一逛&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-029-simpread为任意网页开启阅读模式" class="anchor" aria-hidden="true" href="#029-simpread为任意网页开启阅读模式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/029_simread/" rel="nofollow"&gt;029 《SimpRead》为任意网页开启阅读模式&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/0f9aa9ca332c4325806f92784af9f9ac.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/0f9aa9ca332c4325806f92784af9f9ac.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
为网页开启阅读模式, 能让我们更专注于内容, 不会被花花绿绿的广告推广分散精力, 而SimpRead就是一歀为网页开启&lt;strong&gt;阅读模式&lt;/strong&gt;的插件&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-028adblockadblock屏蔽简书广告" class="anchor" aria-hidden="true" href="#028adblockadblock屏蔽简书广告"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/028_adblock/" rel="nofollow"&gt;028《AdBlock》Adblock屏蔽简书广告&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e149c42ac1f343b88f50e522cba9ad64.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e149c42ac1f343b88f50e522cba9ad64.gif" alt="屏蔽简书广告" style="max-width:100%;"&gt;&lt;/a&gt;
Adblock的功能非常丰富, 但很多功能基本用不到, 普通用户只需要开启Adblock, 能使用右键工具屏蔽不喜欢的广告, 也就够用了~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-027text来自chrome实验室的跨平台记事本" class="anchor" aria-hidden="true" href="#027text来自chrome实验室的跨平台记事本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/027_text/" rel="nofollow"&gt;027《Text》来自Chrome实验室的跨平台记事本&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6e287798ca1d4b939705447d4b8b2b3b.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6e287798ca1d4b939705447d4b8b2b3b.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Text由谷歌Chrome实验室研发并开源, 开源地址&lt;a href="https://github.com/GoogleChromeLabs/text-app"&gt;https://github.com/GoogleChromeLabs/text-app&lt;/a&gt; , Text属于小而美的产品, 功能不算强大, 但是够用, 而且借助Chrome完成了跨平台(在Linux也可以使用哦~)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-026quickey-launcher打开网站只需一键" class="anchor" aria-hidden="true" href="#026quickey-launcher打开网站只需一键"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/026_quickey_launcher/" rel="nofollow"&gt;026《Quickey Launcher》打开网站只需一键&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/322a82d214b34ff2ba70d9c1cd71d276.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/322a82d214b34ff2ba70d9c1cd71d276.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
Quickey Launcher以优雅的方式, 为任意网页绑定一个快捷键, 绑定完成后, 即可通过快捷键,打开网页&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-025consolechrome自带好用的计算器" class="anchor" aria-hidden="true" href="#025consolechrome自带好用的计算器"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/025_console/" rel="nofollow"&gt;025《Console》Chrome自带好用的计算器&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c7bc7cabd06a453dbed2bae0a2bf08d5.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c7bc7cabd06a453dbed2bae0a2bf08d5.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Chrome计算机的好用之处: 既可以看到加数字的记录,也可以实时预览运算的结果, 输入完成后还可以很方便的核查一遍, 还有一点: Chrome计算器观赏性强(逼格很高)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-024dark-reader为任意网站启用夜间模式" class="anchor" aria-hidden="true" href="#024dark-reader为任意网站启用夜间模式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/024_dark_reader/" rel="nofollow"&gt;024《Dark Reader》为任意网站启用夜间模式&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/35e84f58945d4775a31154ea4dc51cac.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/35e84f58945d4775a31154ea4dc51cac.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;喜欢夜间模式的小伙伴, Dark Reader应该可以满足你了~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5&gt;&lt;a id="user-content-023fireshot一键滚动截屏整个网页" class="anchor" aria-hidden="true" href="#023fireshot一键滚动截屏整个网页"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/023_fireshot/" rel="nofollow"&gt;023《FireShot》一键滚动截屏整个网页&lt;/a&gt;&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/81ac43fe1d6e454b93dc7f3ae57d96cd.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/81ac43fe1d6e454b93dc7f3ae57d96cd.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
总体来讲, FireShot是一款不错的软件, 免费且功能够用, 滚动截图的功能比同类软件做的都要好&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-022扩展管理器管理你的chrome扩展" class="anchor" aria-hidden="true" href="#022扩展管理器管理你的chrome扩展"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/022kuo_zhan_guan_li_qi/" rel="nofollow"&gt;022《扩展管理器》管理你的Chrome扩展&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/0480fffebb10437c8d5555f085de9006.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/0480fffebb10437c8d5555f085de9006.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
如果Chrome安装的插件很多, 我们可以对插件进行分组, 按照场景,启用不同组的插件&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-021哔哩哔哩助手助你快速成为b站老司机" class="anchor" aria-hidden="true" href="#021哔哩哔哩助手助你快速成为b站老司机"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/021_bi_li_bi_li_zhu_shou/" rel="nofollow"&gt;021《哔哩哔哩助手》助你快速成为B站老司机&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6ccb9837b60d4d79814a8add20723d97.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6ccb9837b60d4d79814a8add20723d97.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;哔哩哔哩助手, 功能实用,开发者也一直保持着较高频率的更新,可以放心食用~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-020boxel-rebound嗨到中毒的弹跳小方块附自制赛道分享方法" class="anchor" aria-hidden="true" href="#020boxel-rebound嗨到中毒的弹跳小方块附自制赛道分享方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/020_boxel_rebound/" rel="nofollow"&gt;020《Boxel Rebound》“嗨到中毒”的弹跳小方块(附自制赛道分享方法)&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/dbc83cc53c26492db8843ff3e35fc75d.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/dbc83cc53c26492db8843ff3e35fc75d.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
Boxel Rebound是一个偏极客的小游戏, 玩法简单, 可以自由创建赛道, 分享赛道, 获取别人的赛道进行二次开发; 无论你是Mac用户,Windows用户,Linux用户, 只要安装了Chrome浏览器, 就可以玩耍Boxel Rebound&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-019mega网盘可以良心到什么程度-试试mega吧" class="anchor" aria-hidden="true" href="#019mega网盘可以良心到什么程度-试试mega吧"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/019_mega/" rel="nofollow"&gt;019《MEGA》网盘可以良心到什么程度? 试试MEGA吧!&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b5aea0b5e3c54f0a9a050a754a67093d.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b5aea0b5e3c54f0a9a050a754a67093d.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;没有限速的概念(真的被百度盘的限速策略恶心到了)&lt;/li&gt;
&lt;li&gt;在国内可用(google虽好, 但国内用不了, MEGAsync亲测国内可用)&lt;/li&gt;
&lt;li&gt;云端加密, 资源不会被封杀&lt;/li&gt;
&lt;li&gt;官方提供了Linux客户端&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-018enhanced-github从冰柜到冰棍儿下载github单个文件" class="anchor" aria-hidden="true" href="#018enhanced-github从冰柜到冰棍儿下载github单个文件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/018_enhanced_github/" rel="nofollow"&gt;018《Enhanced Github》从“冰柜”到“冰棍儿”,下载Github单个文件&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/769a22f995d74226ba4104aba7e8ab59.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/769a22f995d74226ba4104aba7e8ab59.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/00541b7bd6954f8ea2a6a1beaebbb79b.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/00541b7bd6954f8ea2a6a1beaebbb79b.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
我需要Github给我一根冰棍解暑,Github却坚持把装有冰棍的冰柜也送给我（哥们儿真够意思）... 有了Enhanced Github这款插件, 我们可以下载Github优秀项目中最核心的代码文件进行学习, 而不是 下载 整个仓库作为藏品&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-017新浪微博图床本地markdown编写更流畅-新浪微博图床来帮忙" class="anchor" aria-hidden="true" href="#017新浪微博图床本地markdown编写更流畅-新浪微博图床来帮忙"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/017_xin_lang_wei_bo_tu_chuang/" rel="nofollow"&gt;017《新浪微博图床》本地Markdown编写更流畅, 新浪微博图床来帮忙&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/48c12b3864f84e988e073209fd7cf8e4.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/48c12b3864f84e988e073209fd7cf8e4.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
用Markdown写文章, 如果文章中使用了本地配图, 那本地配图就要和文章一起打包,否则别人是看不到图片的,如果把本地图片放到网络服务器, 然后直接把图片的url粘贴到文章里面, 就可以免除图片打包的步骤&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-016解除b站区域限制查看进击的巨人第三季" class="anchor" aria-hidden="true" href="#016解除b站区域限制查看进击的巨人第三季"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/016_jie_chu_b_zhan_qu_yu_xian_zhi/" rel="nofollow"&gt;016《解除B站区域限制》查看进击的巨人第三季&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/34d50d4d15094ca08e1bbd76c477122a.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/34d50d4d15094ca08e1bbd76c477122a.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/99fd518796894945aa87225a5022c453.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/99fd518796894945aa87225a5022c453.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
解除B站区域限制,B站老司机必备技能&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-015xpath-helper完成bing每日壁纸的小爬虫" class="anchor" aria-hidden="true" href="#015xpath-helper完成bing每日壁纸的小爬虫"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/015_xpath_helper/" rel="nofollow"&gt;015《XPath Helper》完成Bing每日壁纸的小爬虫&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/308bec78f4674130b85a5852f0b25a88.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/308bec78f4674130b85a5852f0b25a88.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;XPath是一个辅助我们写爬虫的小插件, 我们可以用XPath辅助我们完成一个Bing壁纸的小爬虫~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-014超级马里奥游戏chrome变身小霸王" class="anchor" aria-hidden="true" href="#014超级马里奥游戏chrome变身小霸王"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/014_chao_ji_ma_li_ao_you_xi/" rel="nofollow"&gt;014《超级马里奥游戏》Chrome变身小霸王&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/008f3bd3c8b8483b9d70be5d5ed4f9ee.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/008f3bd3c8b8483b9d70be5d5ed4f9ee.gif" alt="超级玛丽.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;用Chrome玩超级马里奥是一种什么体验? 哈哈, 好玩! 《超级马里奥游戏》这款插件,可以让你打开Chrome, 随时玩一局超级玛丽, 嘿嘿&lt;g-emoji class="g-emoji" alias="yum" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60b.png"&gt;😋&lt;/g-emoji&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-013quick-qr用二维码实现云粘贴" class="anchor" aria-hidden="true" href="#013quick-qr用二维码实现云粘贴"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/013_quick_qr/" rel="nofollow"&gt;013《Quick QR》用二维码实现云粘贴&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b59f299316624e86aa7cdd379a02aac4.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b59f299316624e86aa7cdd379a02aac4.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;通过Quick QR, 我们可以不借助任何通讯软件,通过手机扫码,获取PC浏览器上任意一段文字信息(云粘贴板哦~)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-012ourstickyschrome特色网页便签纸" class="anchor" aria-hidden="true" href="#012ourstickyschrome特色网页便签纸"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/012_ourstickys/" rel="nofollow"&gt;012《OurStickys》Chrome特色网页便签纸&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/62597d60ffd6443396725c9677951221.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/62597d60ffd6443396725c9677951221.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;向众人介绍喜欢的网页功能时,可以边讲,边向网页打便签,这样既能让人眼前一亮,也让听众容易抓住重点~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-011-whatruns一键分析网站技术栈" class="anchor" aria-hidden="true" href="#011-whatruns一键分析网站技术栈"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/011_whatruns/" rel="nofollow"&gt;011 《whatruns》一键分析网站技术栈&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/28cc002358c647878b54f9bcaaf67a0a.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/28cc002358c647878b54f9bcaaf67a0a.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;如果你对当前浏览的网站非常感兴趣, 可以通过whatruns了解软件的技术栈, 比如看看这个名为facebook用了什么技术&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-010speedtest网络测速插件speedtest" class="anchor" aria-hidden="true" href="#010speedtest网络测速插件speedtest"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/010_speedtest/" rel="nofollow"&gt;010《speedtest》网络测速插件speedtest&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9aa1e5323a6a4cbcb96304b33a5261c8.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9aa1e5323a6a4cbcb96304b33a5261c8.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;当上网速度很慢的时候, 人们想到的第一件事就进行网络测速,在window上, 只要你安装了360全家桶, 测速功能就是默认安装的, 但测速这种功能根本不需要安装到本地, 交给浏览器就好了&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-009vimiumchrome与vim双神器融合" class="anchor" aria-hidden="true" href="#009vimiumchrome与vim双神器融合"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/009_vimium/" rel="nofollow"&gt;009《vimium》Chrome与vim双神器融合&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/7d6e9fadef3f48409c81a8c76d24e0cc.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/7d6e9fadef3f48409c81a8c76d24e0cc.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;vimium可以让我们只使用键盘就可以浏览网页, 如果你第一次看到有人使用vimium, 它的操作方式绝对能让你感到惊艳~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-008chrome-cleaner-pro为chrome加速" class="anchor" aria-hidden="true" href="#008chrome-cleaner-pro为chrome加速"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/008_chrome_cleaner_pro/" rel="nofollow"&gt;008《Chrome Cleaner Pro》为Chrome加速&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/30899ae22f644a9bb62eb8b24d75c884.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/30899ae22f644a9bb62eb8b24d75c884.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Chrome经过最近几年的发展, 强力的扩展越来越多, 离Chrome OS的目标也越来越近, 软件做大了就会有类似Windows的通病, 软件会变慢, 让Chrome变快的最简单方式就是清理垃圾, 而Chrome Cleaner Pro走的是一键清理的路子~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-007loom-chrome翻录网页视频神器" class="anchor" aria-hidden="true" href="#007loom-chrome翻录网页视频神器"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/007_loom/" rel="nofollow"&gt;007《loom》 Chrome翻录网页视频神器&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/4058cf0008074c5f86b8eb1684e7a1a0.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/4058cf0008074c5f86b8eb1684e7a1a0.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Loom可以一键录制浏览器的单个标签页(盗版翻录视频的神器), 录制完成后自动生成在线网页,进行视频播放, 可以下载刚刚录制的视频, 也可以为刚刚生成的在线视频设置密码(盗版录屏加发布一条龙服务~)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-006similarsites-一键查找姊妹网站-similarsites" class="anchor" aria-hidden="true" href="#006similarsites-一键查找姊妹网站-similarsites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/006_similarsites/" rel="nofollow"&gt;006《SimilarSites》 一键查找姊妹网站 SimilarSites&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/26c6c604be1c41e88ebfe79c733173b0.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/26c6c604be1c41e88ebfe79c733173b0.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;当你浏览一个很棒的站点的时候, 或许你会想到, 和它"差不多"的站点有哪些, 尤其是针对一些资源站点, 这个站点没有, 而它同类的站点"往往有"! SimilarSites, 它的作用只有一个, 发现同类站点!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-005video-speed-controller-刷课刷剧神器给网页视频加个速最快可达16倍" class="anchor" aria-hidden="true" href="#005video-speed-controller-刷课刷剧神器给网页视频加个速最快可达16倍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/005_video_speed_controller/" rel="nofollow"&gt;005《Video Speed Controller》 刷课（刷剧）神器！给网页视频加个速(最快可达16倍!)&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/083c51a1c32a4ad6931646bb005fd5a3.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/083c51a1c32a4ad6931646bb005fd5a3.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;刷一些没营养视频的时候, 我们会有倍速播放视频的需求, 而网站的在线播放器一般只提供不高于4倍的播放速度, 而Video Speed Controller可以将视频播放速度提高到16倍速~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-004tampermonkey-油猴子-给浏览器开个挂" class="anchor" aria-hidden="true" href="#004tampermonkey-油猴子-给浏览器开个挂"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/004_tampermonkey/" rel="nofollow"&gt;004《Tampermonkey》 油猴子! 给浏览器开个挂&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e87601eb459549b3b8e33994fc3fdfb4.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e87601eb459549b3b8e33994fc3fdfb4.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;油猴子必备成为Chrome的第二应用商店, 有了油猴子, 你可以免费查看VIP视频, 清除各种网页广告, 在豆瓣影评页面显示电影资源的下载地址~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-003secure-shell-app-chrome中开启ssh一种什么体验" class="anchor" aria-hidden="true" href="#003secure-shell-app-chrome中开启ssh一种什么体验"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/003_secure_shell_app/" rel="nofollow"&gt;003《Secure Shell App》 Chrome中开启ssh一种什么体验&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/87b66b4cbd12426bbab65a3443f1f1ec.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/87b66b4cbd12426bbab65a3443f1f1ec.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;很多小白想要通过购买服务器搭建自己的VPN, 购买服务器后, 第一步就是要通过ssh登录服务器, 而Windows并没有自带ssh软件,现在你无需下载putty或xshell ,可以通过这款Secure Shell App在chrome直接实现ssh登录服务器了&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-002-chrono-让chrome下载资源更容易" class="anchor" aria-hidden="true" href="#002-chrono-让chrome下载资源更容易"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/002_chrono/" rel="nofollow"&gt;002 《chrono》 让Chrome下载资源更容易&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b574ee1798984ff49396837b620f51ef.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b574ee1798984ff49396837b620f51ef.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;chrono可以非常方便的嗅探识别网页中的资源, 然后一键下载所有资源(收图喽!)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-001markdown-here-markdown一键转换到富文本格式" class="anchor" aria-hidden="true" href="#001markdown-here-markdown一键转换到富文本格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.v2fy.com/p/001_markdown_here/" rel="nofollow"&gt;001《markdown-here》 Markdown一键转换到"富文本格式"&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fc5de2eb22184a138c618728cfb40ede.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fc5de2eb22184a138c618728cfb40ede.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;有了markdown-here这个插件, 可以在网页版 QQ邮箱, Gmail, 新浪头条文章, 里面使用mardown格式进行书写,然后一键转换为富文本&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-他人眼中的-chrome插件英雄榜商业互吹模块" class="anchor" aria-hidden="true" href="#他人眼中的-chrome插件英雄榜商业互吹模块"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;他人眼中的 Chrome插件英雄榜(商业互吹模块)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/88386634" rel="nofollow"&gt;《这份“插件英雄榜Top20”才是Chrome的正确打开方式！》&lt;/a&gt; 作者: &lt;a href="https://me.csdn.net/dQCFKyQDXYm3F8rB0" rel="nofollow"&gt;AI科技大本营&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58636515" rel="nofollow"&gt;《Chrome 插件英雄榜》&lt;/a&gt; 作者: &lt;a href="https://www.zhihu.com/people/loonggg/activities" rel="nofollow"&gt;非著名程序员&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openingsource.org/6190/zh-tw/" rel="nofollow"&gt;《開源日報第363期》&lt;/a&gt; 作者: &lt;a href="https://openingsource.org/" rel="nofollow"&gt;开源工厂&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/Y-9ht-E7-OdJOEDDb3yyWw" rel="nofollow"&gt;《一根火柴的N种打开方式》&lt;/a&gt; 作者: &lt;a href="https://github.com/LuoJiangYong"&gt;老罗巴扎嘿&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-名字起啥好" class="anchor" aria-hidden="true" href="#名字起啥好"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;名字起啥好?&lt;/h2&gt;
&lt;p&gt;将这个仓库命名为&lt;strong&gt;Chrome扩展英雄榜&lt;/strong&gt;可能更准确些,但&lt;strong&gt;插件&lt;/strong&gt;这个名词, 更通俗易懂, 所以就使用了&lt;strong&gt;Chrome插件英雄榜&lt;/strong&gt;这个命名 ,感谢@&lt;a href="https://github.com/hjthjthjt"&gt;hjthjthjt&lt;/a&gt; 给出的&lt;a href="https://github.com/zhaoolee/ChromeAppHeroes/issues/14"&gt;issue&lt;/a&gt;纠正&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-推荐姊妹仓库" class="anchor" aria-hidden="true" href="#推荐姊妹仓库"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/zhaoolee/StarsAndClown"&gt;推荐姊妹仓库&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本仓库的姊妹篇:**&lt;a href="https://github.com/zhaoolee/StarsAndClown"&gt;《Github星聚弃疗榜》&lt;/a&gt;**为Github创意项目写一本推荐书，让Github优秀项目造福人类~ 已开源到Github: &lt;a href="https://github.com/zhaoolee/StarsAndClown"&gt;https://github.com/zhaoolee/StarsAndClown&lt;/a&gt; 同样有趣有料哦~&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-感谢" class="anchor" aria-hidden="true" href="#感谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;感谢&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;感谢 掘金沸点运营 &lt;a href="https://juejin.im/user/5b39bd7de51d4558d43ff06d" rel="nofollow"&gt;@清蒸不是水煮&lt;/a&gt; 给出的 &lt;strong&gt;正面最开始放个索引目录比较好&lt;/strong&gt; 的小建议&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;感谢&lt;a href="https://www.jianshu.com/" rel="nofollow"&gt;简书&lt;/a&gt;社区提供超棒的Markdown编辑器,&lt;strong&gt;Chrome插件英雄榜&lt;/strong&gt;的编辑工作,几乎全部由通过简书编辑器完成&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;** emm... &lt;a href="https://zhaoolee.com/ChromeAppHeroes/download_the_chrome_extension_from_the_store.html" rel="nofollow"&gt;从官方商店下载Chrome插件的方法&lt;/a&gt;**&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Chrome插件英雄榜&lt;/strong&gt; Github地址: &lt;a href="https://github.com/zhaoolee/ChromeAppHeroes"&gt;https://github.com/zhaoolee/ChromeAppHeroes&lt;/a&gt;
我需要你的支持, 希望你能为本项目填加一个 &lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;🌟&lt;/g-emoji&gt;星.
I need your support, I hope you can add a star &lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;🌟&lt;/g-emoji&gt; to this project.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-一根火柴的n种打开方式谷粒文化" class="anchor" aria-hidden="true" href="#一根火柴的n种打开方式谷粒文化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/meaning_of_gu_li.html" rel="nofollow"&gt;一根火柴的N种打开方式(谷粒文化)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9ac21b8aea054eb48fc404fd429638bf.jpeg"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9ac21b8aea054eb48fc404fd429638bf.jpeg" alt="smartmockups_juunlhbe.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/dc9ab48d958843c98f2a4c9336cff748.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/dc9ab48d958843c98f2a4c9336cff748.png" alt="2.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-项目相关阅读" class="anchor" aria-hidden="true" href="#项目相关阅读"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目相关阅读&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/chrome_extended_resources_site.html" rel="nofollow"&gt;Chrome扩展资源站点推荐&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zhaoolee</author><guid isPermaLink="false">https://github.com/zhaoolee/ChromeAppHeroes</guid><pubDate>Fri, 29 Nov 2019 00:23:00 GMT</pubDate></item><item><title>YunYang1994/tensorflow-yolov3 #24 in Python, This month</title><link>https://github.com/YunYang1994/tensorflow-yolov3</link><description>&lt;p&gt;&lt;i&gt;🔥 pure tensorflow Implement of YOLOv3 with support to train your own dataset &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content--are-you-looking-for-a-new-yolov3-implemented-by-tf20-" class="anchor" aria-hidden="true" href="#-are-you-looking-for-a-new-yolov3-implemented-by-tf20-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="new" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f195.png"&gt;🆕&lt;/g-emoji&gt; Are you looking for a new YOLOv3 implemented by TF2.0 ?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;If you hate the fucking tensorflow1.x very much, no worries! I have implemented &lt;strong&gt;a new YOLOv3 repo with TF2.0&lt;/strong&gt;, and also made a chinese blog on how to implement YOLOv3 object detector from scratch. &lt;br&gt;
&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/YOLOV3"&gt;code&lt;/a&gt; | &lt;a href="https://github.com/YunYang1994/cv-notebooks/blob/master/ai_algorithm/YOLOv3.md"&gt;blog&lt;/a&gt;  | &lt;a href="https://github.com/YunYang1994/tensorflow-yolov3/issues/39"&gt;issue&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-part-1-quick-start" class="anchor" aria-hidden="true" href="#part-1-quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;part 1. Quick start&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Clone this file&lt;/li&gt;
&lt;/ol&gt;
&lt;pre lang="bashrc"&gt;&lt;code&gt;$ git clone https://github.com/YunYang1994/tensorflow-yolov3.git
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;You are supposed  to install some dependencies before getting out hands with these codes.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre lang="bashrc"&gt;&lt;code&gt;$ cd tensorflow-yolov3
$ pip install -r ./docs/requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Exporting loaded COCO weights as TF checkpoint(&lt;code&gt;yolov3_coco.ckpt&lt;/code&gt;)【&lt;a href="https://pan.baidu.com/s/11mwiUy8KotjUVQXqkGGPFQ&amp;amp;shfl=sharepset" rel="nofollow"&gt;BaiduCloud&lt;/a&gt;】&lt;/li&gt;
&lt;/ol&gt;
&lt;pre lang="bashrc"&gt;&lt;code&gt;$ cd checkpoint
$ wget https://github.com/YunYang1994/tensorflow-yolov3/releases/download/v1.0/yolov3_coco.tar.gz
$ tar -xvf yolov3_coco.tar.gz
$ cd ..
$ python convert_weight.py
$ python freeze_graph.py
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Then you will get some &lt;code&gt;.pb&lt;/code&gt; files in the root path.,  and run the demo script&lt;/li&gt;
&lt;/ol&gt;
&lt;pre lang="bashrc"&gt;&lt;code&gt;$ python image_demo.py
$ python video_demo.py # if use camera, set video_path = 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/68088581-9255e700-fe9b-11e9-8672-2672ab398abe.jpg"&gt;&lt;img width="100%" src="https://user-images.githubusercontent.com/30433053/68088581-9255e700-fe9b-11e9-8672-2672ab398abe.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-part-2-train-your-own-dataset" class="anchor" aria-hidden="true" href="#part-2-train-your-own-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;part 2. Train your own dataset&lt;/h2&gt;
&lt;p&gt;Two files are required as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/YunYang1994/tensorflow-yolov3/master/data/dataset/voc_train.txt" rel="nofollow"&gt;&lt;code&gt;dataset.txt&lt;/code&gt;&lt;/a&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;xxx/xxx.jpg 18.19,6.32,424.13,421.83,20 323.86,2.65,640.0,421.94,20 
xxx/xxx.jpg 48,240,195,371,11 8,12,352,498,14
# image_path x_min, y_min, x_max, y_max, class_id  x_min, y_min ,..., class_id 
# make sure that x_max &amp;lt; width and y_max &amp;lt; height
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/YunYang1994/tensorflow-yolov3/blob/master/data/classes/coco.names"&gt;&lt;code&gt;class.names&lt;/code&gt;&lt;/a&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;person
bicycle
car
...
toothbrush
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-21-train-on-voc-dataset" class="anchor" aria-hidden="true" href="#21-train-on-voc-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.1 Train on VOC dataset&lt;/h3&gt;
&lt;p&gt;Download VOC PASCAL trainval  and test data&lt;/p&gt;
&lt;pre lang="bashrc"&gt;&lt;code&gt;$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar
$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extract all of these tars into one directory and rename them, which should have the following basic structure.&lt;/p&gt;
&lt;pre lang="bashrc"&gt;&lt;code&gt;
VOC           # path:  /home/yang/dataset/VOC
├── test
|    └──VOCdevkit
|        └──VOC2007 (from VOCtest_06-Nov-2007.tar)
└── train
     └──VOCdevkit
         └──VOC2007 (from VOCtrainval_06-Nov-2007.tar)
         └──VOC2012 (from VOCtrainval_11-May-2012.tar)
                     
$ python scripts/voc_annotation.py --data_path /home/yang/test/VOC
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then edit your &lt;code&gt;./core/config.py&lt;/code&gt; to make some necessary configurations&lt;/p&gt;
&lt;pre lang="bashrc"&gt;&lt;code&gt;__C.YOLO.CLASSES                = "./data/classes/voc.names"
__C.TRAIN.ANNOT_PATH            = "./data/dataset/voc_train.txt"
__C.TEST.ANNOT_PATH             = "./data/dataset/voc_test.txt"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are two kinds of training method:&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-1-train-from-scratch" class="anchor" aria-hidden="true" href="#1-train-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(1) train from scratch:&lt;/h5&gt;
&lt;pre lang="bashrc"&gt;&lt;code&gt;$ python train.py
$ tensorboard --logdir ./data
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;&lt;a id="user-content-2-train-from-coco-weightsrecommend" class="anchor" aria-hidden="true" href="#2-train-from-coco-weightsrecommend"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(2) train from COCO weights(recommend):&lt;/h5&gt;
&lt;pre lang="bashrc"&gt;&lt;code&gt;$ cd checkpoint
$ wget https://github.com/YunYang1994/tensorflow-yolov3/releases/download/v1.0/yolov3_coco.tar.gz
$ tar -xvf yolov3_coco.tar.gz
$ cd ..
$ python convert_weight.py --train_from_coco
$ python train.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-22-evaluate-on-voc-dataset" class="anchor" aria-hidden="true" href="#22-evaluate-on-voc-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.2 Evaluate on VOC dataset&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python evaluate.py
$ cd mAP
$ python main.py -na
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the mAP on the VOC2012 dataset:&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/33013904/58227054-dd4fc800-7d5b-11e9-85aa-67854292fbe0.png"&gt;&lt;img width="50%" src="https://user-images.githubusercontent.com/33013904/58227054-dd4fc800-7d5b-11e9-85aa-67854292fbe0.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-part-3-stargazers-over-time" class="anchor" aria-hidden="true" href="#part-3-stargazers-over-time"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;part 3. Stargazers over time&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://starcharts.herokuapp.com/YunYang1994/tensorflow-yolov3" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c1d56bad1c55921043331554fca992db0c373f7d/68747470733a2f2f737461726368617274732e6865726f6b756170702e636f6d2f59756e59616e67313939342f74656e736f72666c6f772d796f6c6f76332e737667" alt="Stargazers over time" data-canonical-src="https://starcharts.herokuapp.com/YunYang1994/tensorflow-yolov3.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-part-4-other-implementations" class="anchor" aria-hidden="true" href="#part-4-other-implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;part 4. Other Implementations&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/cq7g1-4oFTftLbmKcpi_aQ" rel="nofollow"&gt;-&lt;strong&gt;&lt;code&gt;YOLOv3目标检测有了TensorFlow实现，可用自己的数据来训练&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/Stinky-Tofu/Stronger-yolo"&gt;-&lt;strong&gt;&lt;code&gt;Stronger-yolo&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://itnext.io/implementing-yolo-v3-in-tensorflow-tf-slim-c3c55ff59dbe" rel="nofollow"&gt;- &lt;strong&gt;&lt;code&gt;Implementing YOLO v3 in Tensorflow (TF-Slim)&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/wizyoung/YOLOv3_TensorFlow"&gt;- &lt;strong&gt;&lt;code&gt;YOLOv3_TensorFlow&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html" rel="nofollow"&gt;- &lt;strong&gt;&lt;code&gt;Object Detection using YOLOv2 on Pascal VOC2012&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hackernoon.com/understanding-yolo-f5a74bbc7967" rel="nofollow"&gt;-&lt;strong&gt;&lt;code&gt;Understanding YOLO&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>YunYang1994</author><guid isPermaLink="false">https://github.com/YunYang1994/tensorflow-yolov3</guid><pubDate>Fri, 29 Nov 2019 00:24:00 GMT</pubDate></item><item><title>ansible/ansible #25 in Python, This month</title><link>https://github.com/ansible/ansible</link><description>&lt;p&gt;&lt;i&gt;Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy. Avoid writing scripts or custom code to deploy and update your applications — automate in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com/ansible/&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/ansible" rel="nofollow"&gt;&lt;img alt="PyPI version" src="https://camo.githubusercontent.com/1700ed8e65665052f4e72ba6ae9e1f1d7fddc6c6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f616e7369626c652e737667" data-canonical-src="https://img.shields.io/pypi/v/ansible.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/" rel="nofollow"&gt;&lt;img alt="Docs badge" src="https://camo.githubusercontent.com/dc37b81ae5ef1245837ee1f1547892e8345ccd4b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/docs-latest-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html" rel="nofollow"&gt;&lt;img alt="Chat badge" src="https://camo.githubusercontent.com/a36ab54aea33fa40f9d063c8804b9bbf1b6fbd47/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d4952432d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/chat-IRC-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://app.shippable.com/projects/573f79d02a8192902e20e34b" rel="nofollow"&gt;&lt;img alt="Build Status" src="https://camo.githubusercontent.com/c4dd185960fb101604717a4c8965ac9ba2725e69/68747470733a2f2f6170692e736869707061626c652e636f6d2f70726f6a656374732f3537336637396430326138313932393032653230653334622f62616467653f6272616e63683d646576656c" data-canonical-src="https://api.shippable.com/projects/573f79d02a8192902e20e34b/badge?branch=devel" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/code_of_conduct.html" rel="nofollow"&gt;&lt;img alt="Ansible Code of Conduct" src="https://camo.githubusercontent.com/412f4c0b8d7289e25a69d8568bd02c1bf976f9fd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532306f66253230636f6e647563742d416e7369626c652d73696c7665722e737667" data-canonical-src="https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html#mailing-list-information" rel="nofollow"&gt;&lt;img alt="Ansible mailing lists" src="https://camo.githubusercontent.com/74dd4958c493abf9d3105cbcd020e55aa5df90c9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61696c696e672532306c697374732d416e7369626c652d6f72616e67652e737667" data-canonical-src="https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="COPYING"&gt;&lt;img alt="Repository License" src="https://camo.githubusercontent.com/0ac7552afd56fbe0c2ce6722d54f68857aa92b82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c25323076332e302d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-ansible"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-ansible" class="anchor" aria-hidden="true" href="#ansible"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ansible&lt;/h2&gt;
&lt;p&gt;Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on &lt;a href="https://ansible.com/" rel="nofollow"&gt;the Ansible website&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-design-principles"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-design-principles" class="anchor" aria-hidden="true" href="#design-principles"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Design Principles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Have a dead simple setup process and a minimal learning curve.&lt;/li&gt;
&lt;li&gt;Manage machines very quickly and in parallel.&lt;/li&gt;
&lt;li&gt;Avoid custom-agents and additional open ports, be agentless by
leveraging the existing SSH daemon.&lt;/li&gt;
&lt;li&gt;Describe infrastructure in a language that is both machine and human
friendly.&lt;/li&gt;
&lt;li&gt;Focus on security and easy auditability/review/rewriting of content.&lt;/li&gt;
&lt;li&gt;Manage new remote machines instantly, without bootstrapping any
software.&lt;/li&gt;
&lt;li&gt;Allow module development in any dynamic language, not just Python.&lt;/li&gt;
&lt;li&gt;Be usable as non-root.&lt;/li&gt;
&lt;li&gt;Be the easiest IT automation system to use, ever.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-use-ansible"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-use-ansible" class="anchor" aria-hidden="true" href="#use-ansible"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use Ansible&lt;/h3&gt;
&lt;p&gt;You can install a released version of Ansible via &lt;code&gt;pip&lt;/code&gt;, a package manager, or
our &lt;a href="https://releases.ansible.com/ansible/" rel="nofollow"&gt;release repository&lt;/a&gt;. See our
&lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html" rel="nofollow"&gt;installation guide&lt;/a&gt; for details on installing Ansible
on a variety of platforms.&lt;/p&gt;
&lt;p&gt;Red Hat offers supported builds of &lt;a href="https://www.ansible.com/ansible-engine" rel="nofollow"&gt;Ansible Engine&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Power users and developers can run the &lt;code&gt;devel&lt;/code&gt; branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the &lt;code&gt;devel&lt;/code&gt; branch. We recommend getting involved
in the Ansible community if you want to run the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/p&gt;
&lt;a name="user-content-get-involved"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-get-involved" class="anchor" aria-hidden="true" href="#get-involved"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Get Involved&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Read &lt;a href="https://docs.ansible.com/ansible/latest/community" rel="nofollow"&gt;Community
Information&lt;/a&gt; for all
kinds of ways to contribute to and interact with the project,
including mailing list information and how to submit bug reports and
code to Ansible.&lt;/li&gt;
&lt;li&gt;Join a &lt;a href="https://github.com/ansible/community/wiki"&gt;Working Group&lt;/a&gt;, an organized community devoted to a specific technology domain or platform.&lt;/li&gt;
&lt;li&gt;Submit a proposed code update through a pull request to the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/li&gt;
&lt;li&gt;Talk to us before making larger changes
to avoid duplicate efforts. This not only helps everyone
know what is going on, it also helps save time and effort if we decide
some changes are needed.&lt;/li&gt;
&lt;li&gt;For a list of email lists, IRC channels and Working Groups, see the
&lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html" rel="nofollow"&gt;Communication page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-branch-info"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-branch-info" class="anchor" aria-hidden="true" href="#branch-info"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Branch Info&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;devel&lt;/code&gt; branch corresponds to the release actively under development.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;stable-2.X&lt;/code&gt; branches correspond to stable releases.&lt;/li&gt;
&lt;li&gt;Create a branch based on &lt;code&gt;devel&lt;/code&gt; and set up a &lt;a href="https://docs.ansible.com/ansible/latest/dev_guide/developing_modules_general.html#common-environment-setup" rel="nofollow"&gt;dev environment&lt;/a&gt; if you want to open a PR.&lt;/li&gt;
&lt;li&gt;See the &lt;a href="https://docs.ansible.com/ansible/latest/reference_appendices/release_and_maintenance.html" rel="nofollow"&gt;Ansible release and maintenance&lt;/a&gt; page for information about active branches.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-roadmap"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-roadmap" class="anchor" aria-hidden="true" href="#roadmap"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Roadmap&lt;/h3&gt;
&lt;p&gt;Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The &lt;a href="https://docs.ansible.com/ansible/devel/roadmap/" rel="nofollow"&gt;Ansible Roadmap page&lt;/a&gt; details what is planned and how to influence the roadmap.&lt;/p&gt;
&lt;a name="user-content-authors"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h3&gt;
&lt;p&gt;Ansible was created by &lt;a href="https://github.com/mpdehaan"&gt;Michael DeHaan&lt;/a&gt;
and has contributions from over 4700 users (and growing). Thanks everyone!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ansible.com" rel="nofollow"&gt;Ansible&lt;/a&gt; is sponsored by &lt;a href="https://www.redhat.com" rel="nofollow"&gt;Red Hat, Inc.&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-license"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;GNU General Public License v3.0 or later&lt;/p&gt;
&lt;p&gt;See &lt;a href="COPYING"&gt;COPYING&lt;/a&gt; to see the full text.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>ansible</author><guid isPermaLink="false">https://github.com/ansible/ansible</guid><pubDate>Fri, 29 Nov 2019 00:25:00 GMT</pubDate></item></channel></rss>